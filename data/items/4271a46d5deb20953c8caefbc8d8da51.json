{
  "title": "Manifold clustering in the embedding space using UMAP and GMM",
  "link": "https://datasciencevademecum.com/2021/01/02/manifold-clustering-in-the-embedding-space-using-umap-and-gmm/",
  "dc:creator": "Gianmario",
  "pubDate": "Sat, 02 Jan 2021 10:09:44 +0000",
  "category": [
    "Clustering",
    "Data Munging",
    "Data Visualization",
    "Embedding",
    "clustering",
    "dimensionality reduction",
    "embedding",
    "gmm"
  ],
  "guid": "https://datasciencevademecum.com/?p=2605",
  "description": "<p>In the previous article Extracting rich embedding features from pictures using PyTorch and ResNeXt-WSL we have seen how to represent pictures into a multi-dimensional numerical embedding space. We have also seen the effectiveness of the embedding space to represent similar pictures closely to each other. In this tutorial, we will see a few clustering techniques &#8230; <a href=\"https://datasciencevademecum.com/2021/01/02/manifold-clustering-in-the-embedding-space-using-umap-and-gmm/\" class=\"more-link\">Continue reading <span class=\"screen-reader-text\">Manifold clustering in the embedding space using UMAP and GMM</span></a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://datasciencevademecum.com/2021/01/02/manifold-clustering-in-the-embedding-space-using-umap-and-gmm/\">Manifold clustering in the embedding space using UMAP and GMM</a> appeared first on <a rel=\"nofollow\" href=\"https://datasciencevademecum.com\">Vademecum of Practical Data Science</a>.</p>\n",
  "content:encoded": "\n<p>In the previous article <a href=\"https://datasciencevademecum.com/2020/12/02/extracting-rich-embedding-features-from-pictures-using-pytorch-and-resnext-wsl/\">Extracting rich embedding features from pictures using PyTorch and ResNeXt-WSL</a> we have seen how to represent pictures into a multi-dimensional numerical embedding space. We have also seen the effectiveness of the embedding space to represent similar pictures closely to each other. In this tutorial, we will see a few clustering techniques that are suitable for discovering and identifying the manifolds in our dataset. <br>Moreover, the proposed clustering technique is also motivated by the &#8220;document embedding averaging&#8221; that will be described in the next article.</p>\n\n\n\n<h2>Dimensionality reduction via Uniform Manifold Approximation and Projection (UMAP)</h2>\n\n\n\n<p>Dimensionality reduction is not just used for data visualization, but it is a fundamental step for clustering algorithms due to the &#8220;<a href=\"https://en.wikipedia.org/wiki/Curse_of_dimensionality\">curse of dimensionality</a>&#8220;. In other words, if the number of dimensions increases then most of the points will start to look as similar and as different from each other across at least a few of those dimensions. The effect is that there is no clear structure to follow resulting in a random grouping of the data points.</p>\n\n\n\n<p>The unsupervised dimensionality reduction techniques are divided into two families: Linear Projection and Manifold Learning. </p>\n\n\n\n<p>The main difference of manifold learning with linear projections (e.g. PCA, SVD) is that it can handle non-linear relationships in the data and it is very effective for clustering groups of similar data points preserving their relative proximities. For our purposes and given the nature of our data (the embedding space generated by a deep convolutional neural network), we do not consider any linear projection technique as suitable.</p>\n\n\n\n<p>In the manifold learning family, there are two main competitors: t-SNE and UMAP.</p>\n\n\n\n<figure class=\"wp-block-jetpack-image-compare\"><div class=\"juxtapose\" data-mode=\"horizontal\"><img decoding=\"async\" loading=\"lazy\" id=\"2540\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/1_image-13.png?resize=660%2C406&#038;ssl=1\" alt=\"\" width=\"660\" height=\"406\" class=\"image-compare__image-before\" data-recalc-dims=\"1\"/><img decoding=\"async\" loading=\"lazy\" id=\"2539\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/1_image-12.png?resize=660%2C406&#038;ssl=1\" alt=\"\" width=\"660\" height=\"406\" class=\"image-compare__image-after\" data-recalc-dims=\"1\"/></div><figcaption>t-SNE vs UMAP 3d projection of COCO detection 2017 pictures colored by supercategory</figcaption></figure>\n\n\n\n<p>Uniform Manifold Approximation and Projection (UMAP) is a general-purpose manifold learning and dimension reduction algorithm.<br>T-distributed Stochastic Neighbour Embedding (t-SNE) is an algorithm that generates a low-dimensional graph trying to keep similar instances close and dissimilar instances apart.</p>\n\n\n\n<p>They sound similar, and in fact from a lot of aspects they are. Nevertheless, let&#8217;s summarize a few reasons to prefer UMAP over t-SNE for clustering purposes:</p>\n\n\n\n<figure class=\"wp-block-table\"><table><tbody><tr><td><strong>Property</strong></td><td><strong>t-SNE</strong></td><td><strong>UMAP</strong></td></tr><tr><td><strong>Scalability</strong></td><td>It suffers from high dimensionalities due to its tree structure and often requires a pre-processing step such as PCA or auto-encoders.</td><td>It scales to any dimensional space due to an adaptive exponential kernel that connects sparse regions based on the local connectivity.</td></tr><tr><td><strong>Speed</strong></td><td>Uses plain gradient descent, making it very slow.</td><td>Stochastic gradient descent and the omitted normalization steps make it faster to converge.</td></tr><tr><td><strong>Global structure</strong></td><td>Only locally preserved. The within-cluster distances are informative.</td><td>Preserved globally across the whole space making between-clusters distance meaningful.</td></tr><tr><td><strong>Hold-out transformation</strong></td><td>Not supported. It can only transform the datapoints used during training.</td><td>Can be used as a transformer in a machine learning pipeline on a new set of data.</td></tr><tr><td><strong>Data visualization</strong></td><td>It was designed to only embed 2 or 3 dimensions due to its tree structure. It is specific for visualization purposes.<br>It often results in a bunch of spherical local clusters placed inside a larger sphere.</td><td>Can be used for data visualization but the clusters tend to have irregular shapes.</td></tr></tbody></table><figcaption>Comparison of t-SNE and UMAP</figcaption></figure>\n\n\n\n<p>For a more comprehensive comparison of t-SNE vs. UMAP please refer to the following article: <a href=\"https://towardsdatascience.com/how-exactly-umap-works-13e3040e1668\">How exactly UMAP works</a>.</p>\n\n\n\n<p>For the reasons discussed above, we can conclude that t-SNE is a great visualization tool but UMAP is a more suitable technique for clustering purposes in the case of manifold structures.</p>\n\n\n\n<h2>Clustering with Gaussian Mixture Model (GMM)</h2>\n\n\n\n<p>GMM is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. It can be seen as a generalization of the more popular k-means model. The advantage of using GMM over k-means is that it can represent clusters of different sizes and shapes based on a parametric covariance matrix.<br>In k-means the clusters are spherical over all of the dimensions and share the same diameter. This is a big limitation when considering a manifold feature space as is the case of transforming a deep convolutional neural network embedding space with UMAP. </p>\n\n\n\n<figure class=\"wp-block-image\"><img decoding=\"async\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/1ZaapeE-NBoVZYibjVJkKVA.jpg?w=660&#038;ssl=1\" alt=\"Image for post\" data-recalc-dims=\"1\"/><figcaption>Different clustering output using the two models on the iris dataset<br>Source: https://towardsdatascience.com/mixture-modelling-from-scratch-in-r-5ab7bfc83eef</figcaption></figure>\n\n\n\n<p>Another distinction is in the interpretation of the clustering output. k-means divide the space into voronoi cells and hard assign each point to the cluster of the closest centroid. GMM, on the other hand, gives us an interpretable output modeling the probability that each data point belong to each cluster. The latter is a desired probability for dealing with fuzzy situations in presence of overlapping clusters or outliers. </p>\n\n\n\n<h3>GMM parameters</h3>\n\n\n\n<p>As with k-means, also GMM requires the number of clusters k to be specified.</p>\n\n\n\n<p>Moreover, in order for GMM to be able to model arbitrary elliptic shapes in the feature space, the covariance matrix should be &#8220;full&#8221;. The problem with &#8220;full&#8221; GMM models is that the degrees of freedom increase quadratically with the dimension of the feature space, risking to overfit the data. There are a few constrained versions of GMM that impose certain properties to the covariance matrix., namely: spherical, diagonal, and tied.</p>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter\"><img decoding=\"async\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/sphx_glr_plot_gmm_covariances_001.png?w=660&#038;ssl=1\" alt=\"spherical, diag, tied, full\" data-recalc-dims=\"1\"/><figcaption>Cluster shapes obtained using different covariance types <br>Source: https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_covariances.html</figcaption></figure></div>\n\n\n\n<ul><li><strong>Spherical</strong>&nbsp;is a &#8220;diagonal&#8221; situation with circular contours (spherical in higher dimensions, whence the name).</li><li><strong>Diagonal</strong>&nbsp;means the contour axes are oriented along the coordinate axes, but otherwise the eccentricities may vary between components.</li><li><strong>Full</strong>&nbsp;means the components may independently adopt any position and shape.</li><li><strong>Tied</strong>&nbsp;means they have the same shape, but the shape may be anything.</li></ul>\n\n\n\n<p>We are now left with two major parameters to tune: the number of clusters k and the covariance type among the 4 options listed above.</p>\n\n\n\n<h2>Model selection using BIC and Silhouette scores</h2>\n\n\n\n<p>Likely, since GMM is a probabilistic model we can calculate the Bayesian Information Criterion (BIC)  that is a statistics calculated as the sum of the negative log-likelihood of the model and a penalty term that is a function of the number of data samples and the number of free parameters of the model. The smaller the BIC value the more preferable is the model. Nonetheless, searching for the minimum BIC score may suggest selecting a model with a lot of clusters in front of tiny decreases of the score. That is why a preferred approach is to identify the elbow of the curve that corresponds to the minimum of the second derivative. <br>The BIC score is comparable among different clustering outputs only if they are representing the same points in the same feature space. That is, we cannot compare data points reduced via PCA with data points reduced via UMAP.</p>\n\n\n\n<p>Another technique is the <a href=\"https://en.wikipedia.org/wiki/Silhouette_(clustering)\">Silhouette score</a> that is an empirical method measuring the consistency of the clusters by comparing how much a point is similar to its cluster (cohesion) compared to the other clusters (separation). The silhouette ranges from −1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate, otherwise the clustering configuration may have too many or too few clusters.</p>\n\n\n\n<p>If we plot both the BIC and Silhouette curves as function of the number of clusters k for the 4 different covariance types we obtain the following graph:</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"660\" height=\"424\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-16.png?resize=660%2C424&#038;ssl=1\" alt=\"\" class=\"wp-image-2620\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-16.png?w=772&ssl=1 772w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-16.png?resize=300%2C193&ssl=1 300w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-16.png?resize=768%2C493&ssl=1 768w\" sizes=\"(max-width: 660px) 100vw, 660px\" data-recalc-dims=\"1\" /><figcaption>GMM model selection using both the BIC and Silhouette scores</figcaption></figure>\n\n\n\n<p>The dot points correspond, respectively, to the elbow and the maximum of the BIC and Silhouette curves.</p>\n\n\n\n<p>We can conclude that the ideal number of clusters should be between 30 and 50. <br>In terms of the covariance type, the tied type minimizes the BIC while there is not strong evidence of worsening results in the Silhouette curve. <br>The lower BIC score can be explained by the good trade-off between low model complexity and the high likelihood of the points. Moreover, given the nature of the feature space, it does make sense to consider manifolds of irregular but similar shapes.</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"660\" height=\"481\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-17.png?resize=660%2C481&#038;ssl=1\" alt=\"\" class=\"wp-image-2621\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-17.png?w=666&ssl=1 666w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-17.png?resize=300%2C218&ssl=1 300w\" sizes=\"(max-width: 660px) 100vw, 660px\" data-recalc-dims=\"1\" /><figcaption>Tied covariance matrix of the GMM model trained with 40 clusters</figcaption></figure>\n\n\n\n<p>The selected configuration for us will be tied covariance type and 40 clusters.</p>\n\n\n\n<h2>Visualizing the clusters</h2>\n\n\n\n<p>In order to visualize the clusters we will re-use the 3D projections that were calculated in <a href=\"https://datasciencevademecum.com/2020/12/02/extracting-rich-embedding-features-from-pictures-using-pytorch-and-resnext-wsl/\">Extracting rich embedding features from pictures using PyTorch and ResNeXt-WSL</a> but we will color the points based on the assigned clusters rather than the COCO super categories of the pictures.</p>\n\n\n\n<figure class=\"wp-block-jetpack-image-compare\"><div class=\"juxtapose\" data-mode=\"vertical\"><img decoding=\"async\" loading=\"lazy\" id=\"2538\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-11.png?resize=660%2C406&#038;ssl=1\" alt=\"\" width=\"660\" height=\"406\" class=\"image-compare__image-before\" data-recalc-dims=\"1\"/><img decoding=\"async\" loading=\"lazy\" id=\"2626\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/pca-clusters.png?resize=660%2C453&#038;ssl=1\" alt=\"\" width=\"660\" height=\"453\" class=\"image-compare__image-after\" data-recalc-dims=\"1\"/></div><figcaption>PCA 3D projection colored by COCO categories (on the left) or the GMM clusters (on the right)</figcaption></figure>\n\n\n\n<figure class=\"wp-block-jetpack-image-compare\"><div class=\"juxtapose\" data-mode=\"vertical\"><img decoding=\"async\" loading=\"lazy\" id=\"2616\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/1_1_image-13.png?resize=660%2C406&#038;ssl=1\" alt=\"\" width=\"660\" height=\"406\" class=\"image-compare__image-before\" data-recalc-dims=\"1\"/><img decoding=\"async\" loading=\"lazy\" id=\"2625\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/tsne-clusters.png?resize=660%2C453&#038;ssl=1\" alt=\"\" width=\"660\" height=\"453\" class=\"image-compare__image-after\" data-recalc-dims=\"1\"/></div><figcaption>UMAP 3D projection colored by COCO categories (on the left) or the GMM clusters (on the right)</figcaption></figure>\n\n\n\n<figure class=\"wp-block-jetpack-image-compare\"><div class=\"juxtapose\" data-mode=\"vertical\"><img decoding=\"async\" loading=\"lazy\" id=\"2615\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/1_1_image-12.png?resize=660%2C406&#038;ssl=1\" alt=\"\" width=\"660\" height=\"406\" class=\"image-compare__image-before\" data-recalc-dims=\"1\"/><img decoding=\"async\" loading=\"lazy\" id=\"2627\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/1_umap-clusters.png?resize=660%2C453&#038;ssl=1\" alt=\"\" width=\"660\" height=\"453\" class=\"image-compare__image-after\" data-recalc-dims=\"1\"/></div></figure>\n\n\n\n<h2>Comparison with the COCO taxonomy</h2>\n\n\n\n<h3>Predicting COCO annotations via supervised learning</h3>\n\n\n\n<p>In order to measure the predictive power of the embedding space, let&#8217;s try to predict the COCO annotations. We can train a random forest classifier with default parameters on a multi-labels task. Since each picture can have none, one, or many categories annotated, the task consists of predicting whether a given label is present or not in the picture.</p>\n\n\n\n<p>I have used 4000 pictures for training and 1000 for the test stratifying on the most frequent label in each picture. The multilabel accuracy (<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\">exact subset of labels match as defined in scikit-learn</a>) would be 16.7% which is not bad considering the high cardinality of the task. If we micro-average all of the label predictions we can perform a binary evaluation:</p>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"353\" height=\"294\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-5.png?resize=353%2C294&#038;ssl=1\" alt=\"\" class=\"wp-image-2527\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-5.png?w=353&ssl=1 353w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-5.png?resize=300%2C250&ssl=1 300w\" sizes=\"(max-width: 353px) 100vw, 353px\" data-recalc-dims=\"1\" /><figcaption>Confusion matrix for the multi-label random forest classifier</figcaption></figure></div>\n\n\n\n<figure class=\"wp-block-image size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"474\" height=\"180\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-6.png?resize=474%2C180&#038;ssl=1\" alt=\"\" class=\"wp-image-2528\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-6.png?w=474&ssl=1 474w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-6.png?resize=300%2C114&ssl=1 300w\" sizes=\"(max-width: 474px) 100vw, 474px\" data-recalc-dims=\"1\" /><figcaption>Binary classification report</figcaption></figure>\n\n\n\n<p>We have achieved 89% precision and 31% recall on all of the possible picture annotations, not bad and not great. We should consider that we only trained on a very small sample of pictures and a few labels had very few occurrences. Nonetheless, the purpose of this tutorial is not to predict COCO categories but rather to show the effectiveness of the embedding features in identifying correct manifolds.</p>\n\n\n\n<h3>Clusters similarity and consistency</h3>\n\n\n\n<p>Since we have grouped the COCO pictures into 40 unsupervised clusters, let&#8217;s compare our grouping with the categories provided in the COCO taxonomy. We can use the <a href=\"https://en.wikipedia.org/wiki/Rand_index#:~:text=(true%20negatives).-,Adjusted%20Rand%20index,specified%20by%20a%20random%20model.\">Adjusted Rand Index</a> that is a measure of similarity between two clustering outputs. The higher the score, the higher the consistency between the two groupings.</p>\n\n\n\n<p>We obtained the following results:</p>\n\n\n\n<ul><li>AdjustedRand(GMM_clusters, COCO_supercategories) = 0.09955</li><li>AdjustedRand(GMM_clusters, COCO_categories) = 0.08844</li></ul>\n\n\n\n<p>As we could have already observed from the 3D projections, we can conclude that the discovered topicality defined by the manifolds in the data does not match the COCO taxonomy.</p>\n\n\n\n<h2>Pictures in the cluster</h2>\n\n\n\n<p>What topic is each cluster representing then? <br>Let&#8217;s print the pictures closest to the centroid in a few sample clusters.</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"660\" height=\"653\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-18.png?resize=660%2C653&#038;ssl=1\" alt=\"\" class=\"wp-image-2641\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-18.png?resize=1024%2C1013&ssl=1 1024w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-18.png?resize=300%2C297&ssl=1 300w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-18.png?resize=768%2C759&ssl=1 768w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-18.png?resize=1536%2C1519&ssl=1 1536w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-18.png?w=1716&ssl=1 1716w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-18.png?w=1320&ssl=1 1320w\" sizes=\"(max-width: 660px) 100vw, 660px\" data-recalc-dims=\"1\" /><figcaption>Cluster 0: horses</figcaption></figure>\n\n\n\n<figure class=\"wp-block-image size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"660\" height=\"640\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-20.png?resize=660%2C640&#038;ssl=1\" alt=\"\" class=\"wp-image-2643\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-20.png?resize=1024%2C993&ssl=1 1024w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-20.png?resize=300%2C291&ssl=1 300w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-20.png?resize=768%2C745&ssl=1 768w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-20.png?resize=1536%2C1490&ssl=1 1536w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-20.png?w=1709&ssl=1 1709w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-20.png?w=1320&ssl=1 1320w\" sizes=\"(max-width: 660px) 100vw, 660px\" data-recalc-dims=\"1\" /><figcaption>Cluster 3: dining</figcaption></figure>\n\n\n\n<figure class=\"wp-block-image size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"660\" height=\"632\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-21.png?resize=660%2C632&#038;ssl=1\" alt=\"\" class=\"wp-image-2644\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-21.png?resize=1024%2C981&ssl=1 1024w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-21.png?resize=300%2C287&ssl=1 300w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-21.png?resize=768%2C736&ssl=1 768w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-21.png?resize=1536%2C1472&ssl=1 1536w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-21.png?w=1719&ssl=1 1719w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-21.png?w=1320&ssl=1 1320w\" sizes=\"(max-width: 660px) 100vw, 660px\" data-recalc-dims=\"1\" /><figcaption>Cluster 10: sea and watersports</figcaption></figure>\n\n\n\n<figure class=\"wp-block-image size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"660\" height=\"639\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-19.png?resize=660%2C639&#038;ssl=1\" alt=\"\" class=\"wp-image-2642\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-19.png?resize=1024%2C992&ssl=1 1024w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-19.png?resize=300%2C291&ssl=1 300w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-19.png?resize=768%2C744&ssl=1 768w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-19.png?resize=1536%2C1488&ssl=1 1536w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-19.png?w=1720&ssl=1 1720w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-19.png?w=1320&ssl=1 1320w\" sizes=\"(max-width: 660px) 100vw, 660px\" data-recalc-dims=\"1\" /><figcaption>Cluster 18: bears</figcaption></figure>\n\n\n\n<figure class=\"wp-block-image size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"660\" height=\"660\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-22.png?resize=660%2C660&#038;ssl=1\" alt=\"\" class=\"wp-image-2645\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-22.png?resize=1024%2C1024&ssl=1 1024w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-22.png?resize=300%2C300&ssl=1 300w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-22.png?resize=150%2C150&ssl=1 150w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-22.png?resize=768%2C768&ssl=1 768w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-22.png?resize=1536%2C1536&ssl=1 1536w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-22.png?w=1696&ssl=1 1696w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-22.png?w=1320&ssl=1 1320w\" sizes=\"(max-width: 660px) 100vw, 660px\" data-recalc-dims=\"1\" /><figcaption>Cluster 22: towers</figcaption></figure>\n\n\n\n<h2>Conclusions</h2>\n\n\n\n<p>In this tutorial, we have learned how to cluster pictures in their latent embedding space. We first have used UMAP for isolating manifolds and projecting them into a lower-dimensional space. We then used GMM for discovering the high-density areas in the UMAP space. The BIC elbow and Silhouette techniques were used to find the ideal number of clusters as well as the constraints to the covariance matrix. Through the AdjustedRand test, we demonstrated that the data is intrinsically organized into major topics that do not match with the COCO taxonomy. For instance, we found clusters for horses, bears, towers, watersports, people&#8217;s dining, and more.</p>\n\n\n\n<p>The presented methodology can be used to cluster any dataset that presents high-dimensional manifolds, not just pictures. It is in general suitable for embeddings produced by neural network models.</p>\n\n\n\n<p>If instead of using a pre-trained network, you are training your own, you may want to consider a small dimensionality (below 50) such that you may not need any dimensionality reduction before clustering. Other clustering algorithms that work with any kind of shapes, and are not constrained by the Gaussian mixture assumption, are the hierarchical density-based models such as <a href=\"https://hdbscan.readthedocs.io/en/latest/\">HDBSCAN</a>.</p>\n\n\n\n<p>Stay tuned for the next article on how to exploit embedding features and the manifolds clusters to average and represent collections of datapoints (documents) into the same latent space.</p>\n\n\n\n<p>You can find the code and the notebooks at <a href=\"https://github.com/gm-spacagna/docem\" rel=\"noreferrer noopener\" target=\"_blank\">https://github.com/gm-spacagna/docem</a>.</p>\n\n\n\n<p>If you want to learn more about tuning techniques for data clustering algorithms you can read those articles: <a href=\"https://datasciencevademecum.com/2014/02/27/data-clustering-dont-worry-about-the-algorithm/\">Data Clustering? don’t worry about the algorithm</a> and <a href=\"https://datasciencevademecum.com/2014/02/27/a-distributed-genetic-evolutionary-tuning-for-data-clustering/\">A Distributed Genetic Evolutionary Tuning for Data Clustering: Part 1</a>.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://datasciencevademecum.com/2021/01/02/manifold-clustering-in-the-embedding-space-using-umap-and-gmm/\">Manifold clustering in the embedding space using UMAP and GMM</a> appeared first on <a rel=\"nofollow\" href=\"https://datasciencevademecum.com\">Vademecum of Practical Data Science</a>.</p>\n",
  "post-id": 2605
}