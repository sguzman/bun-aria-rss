{
  "title": "Intelligent document processing with AWS AI and Analytics services in the insurance industry: Part 2",
  "link": "https://aws.amazon.com/blogs/machine-learning/part-2-intelligent-document-processing-with-aws-ai-and-analytics-services-in-the-insurance-industry/",
  "dc:creator": "Chinmayee Rane",
  "pubDate": "Thu, 03 Nov 2022 19:04:37 +0000",
  "category": [
    "Amazon Comprehend",
    "Amazon Comprehend Medical",
    "Amazon Machine Learning",
    "Amazon Textract",
    "Analytics",
    "Artificial Intelligence",
    "Uncategorized"
  ],
  "guid": "c1bc1ed0750387506c14ae0c2184353a95dd67e2",
  "description": "In Part 1 of this series, we discussed intelligent document processing (IDP), and how IDP can accelerate claims processing use cases in the insurance industry. We discussed how we can use AWS AI services to accurately categorize claims documents along with supporting documents. We also discussed how to extract various types of documents in an […]",
  "content:encoded": "<p>In <a href=\"https://aws.amazon.com/blogs/machine-learning/part-1-intelligent-document-processing-with-aws-ai-services-in-the-insurance-industry/\">Part 1</a> of this series, we discussed intelligent document processing (IDP), and how IDP can accelerate claims processing use cases in the insurance industry. We discussed how we can use AWS AI services to accurately categorize claims documents along with supporting documents. We also discussed how to extract various types of documents in an insurance claims package, such as forms, tables, or specialized documents such as invoices, receipts, or ID documents. We looked into the challenges in legacy document processes, which is time-consuming, error-prone, expensive, and difficult to process at scale, and how you can use AWS AI services to help implement your IDP pipeline.</p> \n<p>In this post, we walk you through advanced IDP features for document extraction, querying, and enrichment. We also look into how to further use the extracted structured information from claims data to get insights using AWS Analytics and visualization services. We highlight on how extracted structured data from IDP can help against fraudulent claims using AWS Analytics services.</p> \n<table style=\"border-color: #ff9900;height: 140px\" border=\"2px\" width=\"718\" cellpadding=\"10px\"> \n <tbody> \n  <tr> \n   <td><strong>Intelligent document processing with AWS AI and Analytics services in the insurance industry</strong><p></p> \n    <ul> \n     <li><a href=\"https://aws.amazon.com/blogs/machine-learning/part-1-intelligent-document-processing-with-aws-ai-services-in-the-insurance-industry/\" target=\"_blank\" rel=\"noopener\">Part 1: Classification and extraction of documents</a></li> \n     <li><strong>Part 2: Data enrichment and insights</strong></li> \n    </ul> </td> \n  </tr> \n </tbody> \n</table> \n<h2>Solution overview</h2> \n<p>The following diagram illustrates the phases if IDP using AWS AI services. In Part 1, we discussed the first three phases of the IDP workflow. In this post, we expand on the extraction step and the remaining phases, which include integrating IDP with AWS Analytics services.</p> \n<p><img loading=\"lazy\" class=\"alignnone wp-image-43818\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/05/insurance-phases-idp.png\" alt=\"The different phases of intelligent document processing in insurance industry\" width=\"800\" height=\"221\"></p> \n<p>We use these analytics services for further insights and visualizations, and to detect fraudulent claims using structured, normalized data from IDP. The following diagram illustrates the solution architecture.</p> \n<p><img loading=\"lazy\" class=\"alignnone wp-image-43819\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/05/IDP_Insurance_Architecture.png\" alt=\"IDP architecture diagram\" width=\"800\" height=\"514\"></p> \n<p>The phases we discuss in this post use the following key services:</p> \n<ul> \n <li><a href=\"https://aws.amazon.com/comprehend/medical/\">Amazon Comprehend Medical</a> is a HIPAA-eligible natural language processing (NLP) service that uses machine learning (ML) models that have been pre-trained to understand and extract health data from medical text, such as prescriptions, procedures, or diagnoses.</li> \n <li><a href=\"https://aws.amazon.com/glue/?whats-new-cards.sort-by=item.additionalFields.postDateTime&amp;whats-new-cards.sort-order=desc\">AWS Glue</a> is a part of the AWS Analytics services stack, and is a serverless data integration service that makes it easy to discover, prepare, and combine data for analytics, ML, and application development.</li> \n <li><a href=\"https://aws.amazon.com/redshift/\">Amazon Redshift</a> is another service in the Analytics stack. Amazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud.</li> \n</ul> \n<h2>Prerequisites</h2> \n<p>Before you get started, refer to <a href=\"https://aws.amazon.com/blogs/machine-learning/part-1-intelligent-document-processing-with-aws-ai-services-in-the-insurance-industry/\">Part 1</a> for a high-level overview of the insurance use case with IDP and details about the data capture and classification stages.</p> \n<p>For more information regarding the code samples, refer to our <a href=\"https://github.com/aws-samples/aws-ai-intelligent-document-processing/tree/main/industry/insurance\">GitHub repo.</a></p> \n<h2>Extraction phase</h2> \n<p>In Part 1, we saw how to use Amazon Textract APIs to extract information like forms and tables from documents, and how to analyze invoices and identity documents. In this post, we enhance the extraction phase with Amazon Comprehend to extract default and custom entities specific to custom use cases.</p> \n<p>Insurance carriers often come across dense text in insurance claims applications, such a patient’s discharge summary letter (see the following example image). It can be difficult to automatically extract information from such types of documents where there is no definite structure. To address this, we can use the following methods to extract key business information from the document:</p> \n<ul> \n <li>Amazon Comprehend <a href=\"https://docs.aws.amazon.com/comprehend/latest/dg/API_DetectEntities.html\">default entity recognition</a></li> \n <li>Amazon Comprehend <a href=\"https://docs.aws.amazon.com/comprehend/latest/dg/custom-entity-recognition.html\">custom entity recognizer</a></li> \n</ul> \n<p><img loading=\"lazy\" class=\"alignnone wp-image-43832\" style=\"margin: 10px 0px 10px 0px;border: 1px solid #CCCCCC\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/05/discharge-summary-1.png\" alt=\"Discharge summary sample\" width=\"500\" height=\"649\"></p> \n<h3>Extract default entities with the Amazon Comprehend DetectEntities API</h3> \n<p>We run the following code on the sample medical transcription document:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">comprehend = boto3.client('comprehend') \n\nresponse = comprehend.detect_entities( Text=text, LanguageCode='en')\n\n#print enitities from the response JSON\n\nfor entity in response['Entities']:\n    print(f'{entity[\"Type\"]} : {entity[\"Text\"]}')\n</code></pre> \n</div> \n<p>The following screenshot shows a collection of entities identified in the input text. The output has been shortened for the purposes of this post. Refer to the <a href=\"https://github.com/aws-samples/aws-ai-intelligent-document-processing/blob/main/industry/insurance/03-document-extraction-2.ipynb\">GitHub repo</a> for a detailed list of entities.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/18/ML-10191-detectentities-response-1.png\"><img loading=\"lazy\" class=\"alignnone wp-image-44375 size-full\" style=\"margin: 10px 0px 10px 0px;border: 1px solid #CCCCCC\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/18/ML-10191-detectentities-response-1.png\" alt=\"\" width=\"500\" height=\"533\"></a></p> \n<h3>Extract custom entities with Amazon Comprehend custom entity recognition</h3> \n<p>The response from the <code>DetectEntities</code> API includes the default entities. However, we’re interested in knowing specific entity values, such as the patient’s name (denoted by the default entity <code>PERSON</code>), or the patient’s ID (denoted by the default entity <code>OTHER</code>). To recognize these custom entities, we train an Amazon Comprehend custom entity recognizer model. We recommend following the comprehensive steps on how to train and deploy a custom entity recognition model in the <a href=\"https://github.com/aws-samples/aws-ai-intelligent-document-processing/blob/main/industry/insurance/03-document-extraction-2.ipynb\">GitHub repo.</a></p> \n<p>After we deploy the custom model, we can use the helper function <code>get_entities()</code> to retrieve custom entities like <code>PATIENT_NAME</code> and <code>PATIENT_D</code> from the API response:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">def get_entities(text):\ntry:\n    #detect entities\n    entities_custom = comprehend.detect_entities(LanguageCode=\"en\",\n                      Text=text, EndpointArn=ER_ENDPOINT_ARN) \n    df_custom = pd.DataFrame(entities_custom[\"Entities\"], columns = ['Text',  \n                'Type', 'Score'])\n    df_custom = df_custom.drop_duplicates(subset=['Text']).reset_index()\n    return df_custom\nexcept Exception as e:\n    print(e)\n\n# call the get_entities() function \nresponse = get_entities(text) \n#print the response from the get_entities() function\nprint(response)</code></pre> \n</div> \n<p>The following screenshot shows our results.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/18/ML-10191-detectentities-response-2-1.png\"><img loading=\"lazy\" class=\"alignnone wp-image-44376 size-full\" style=\"margin: 10px 0px 10px 0px;border: 1px solid #CCCCCC\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/18/ML-10191-detectentities-response-2-1.png\" alt=\"\" width=\"300\" height=\"110\"></a></p> \n<h2>Enrichment phase</h2> \n<p>In the document enrichment phase, we perform enrichment functions on healthcare-related documents to draw valuable insights. We look at the following types of enrichment:</p> \n<ul> \n <li><strong>Extract domain-specific language</strong> – We use Amazon Comprehend Medical to extract medical-specific ontologies like ICD-10-CM, RxNorm, and SNOMED CT</li> \n <li><strong>Redact sensitive information</strong> – We use Amazon Comprehend to redact personally identifiable information (PII), and Amazon Comprehend Medical for protected health information (PHI) redaction</li> \n</ul> \n<h3>Extract medical information from unstructured medical text</h3> \n<p>Documents such as medical providers’ notes and clinical trial reports include dense medical text. Insurance claims carriers need to identify the relationships among the extracted health information from this dense text and link them to medical ontologies like ICD-10-CM, RxNorm, and SNOMED CT codes. This is very valuable in automating claim capture, validation, and approval workflows for insurance companies to accelerate and simplify claim processing. Let’s look at how we can use the Amazon Comprehend Medical <code>InferICD10CM</code> API to detect possible medical conditions as entities and link them to their codes:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">cm_json_data = comprehend_med.infer_icd10_cm(Text=text)\n\nprint(\"\\nMedical coding\\n========\")\n\nfor entity in cm_json_data[\"Entities\"]:\n      for icd in entity[\"ICD10CMConcepts\"]:\n           description = icd['Description']\n           code = icd[\"Code\"]\n           print(f'{description}: {code}')\n</code></pre> \n</div> \n<p>For the input text, which we can pass in from the Amazon Textract <code>DetectDocumentText</code> API, the <code>InferICD10CM</code> API returns the following output (the output has been abbreviated for brevity).</p> \n<p><img loading=\"lazy\" class=\"alignnone wp-image-43839\" style=\"margin: 10px 0px 10px 0px;border: 1px solid #CCCCCC\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/05/ML-10191-InferICD10CM-response.png\" alt=\"Extract medical information from unstructured medical text\" width=\"500\" height=\"166\"></p> \n<p>Similarly, we can use the Amazon Comprehend Medical <a href=\"https://docs.aws.amazon.com/comprehend-medical/latest/dev/ontology-RxNorm.html\"><code>InferRxNorm</code></a> API to identify medications and the <a href=\"https://docs.aws.amazon.com/comprehend-medical/latest/dev/ontology-linking-snomed.html\"><code>InferSNOMEDCT</code></a> API to detect medical entities within healthcare-related insurance documents.</p> \n<h3>Perform PII and PHI redaction</h3> \n<p>Insurance claims packages require a lot of privacy compliance and regulations because they contain both PII and PHI data. Insurance carriers can reduce compliance risk by redacting information like policy numbers or the patient’s name.</p> \n<p>Let’s look at an example of a patient’s discharge summary. We use the Amazon Comprehend <code>DetectPiiEntities</code> API to detect PII entities within the document and protect the patient’s privacy by redacting these entities:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">resp = call_textract(input_document = f's3://{data_bucket}/idp/textract/dr-note-sample.png')\ntext = get_string(textract_json=resp, output_type=[Textract_Pretty_Print.LINES])\n\n# call Amazon Comprehend Detect PII Entities API\nentity_resp = comprehend.detect_pii_entities(Text=text, LanguageCode=\"en\") \n\npii = []\nfor entity in entity_resp['Entities']:\n      pii_entity={}\n      pii_entity['Type'] = entity['Type']\n      pii_entity['Text'] = text[entity['BeginOffset']:entity['EndOffset']]\n      pii.append(pii_entity)\nprint(pii)\n</code></pre> \n</div> \n<p>We get the following PII entities in the response from the <code>detect_pii_entities()</code> API :</p> \n<p><img loading=\"lazy\" class=\"alignnone size-full wp-image-43842\" style=\"margin: 10px 0px 10px 0px;border: 1px solid #CCCCCC\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/05/ML-10191-detectpiientities-response.png\" alt=\"response from the detect_pii_entities() API\" width=\"500\" height=\"105\"></p> \n<p>We can then redact the PII entities that were detected from the documents by utilizing the bounding box geometry of the entities from the document. For that, we use a helper tool called <code>amazon-textract-overlayer</code>. For more information, refer to <a href=\"https://github.com/aws-samples/amazon-textract-textractor/tree/master/overlayer\">Textract-Overlayer</a>. The following screenshots compare a document before and after redaction.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/18/ML-10191-dr-note-sample-1.png\"><img loading=\"lazy\" class=\"alignnone wp-image-44377 size-full\" style=\"margin: 10px 0px 10px 0px;border: 1px solid #CCCCCC\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/18/ML-10191-dr-note-sample-1.png\" alt=\"\" width=\"500\" height=\"329\"></a><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/18/ML-10191-redacted-dr-note-1-1.png\"><img loading=\"lazy\" class=\"alignnone wp-image-44378 size-full\" style=\"margin: 10px 0px 10px 0px;border: 1px solid #CCCCCC\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/18/ML-10191-redacted-dr-note-1-1.png\" alt=\"\" width=\"500\" height=\"340\"></a></p> \n<p>Similar to the Amazon Comprehend <code>DetectPiiEntities</code> API, we can also use the <code>DetectPHI</code> API to detect PHI data in the clinical text being examined. For more information, refer to <a href=\"https://docs.aws.amazon.com/comprehend-medical/latest/dev/textanalysis-phi.html\">Detect PHI.</a></p> \n<h2>Review and validation phase</h2> \n<p>In the document review and validation phase, we can now verify if the claim package meets the business’s requirements, because we have all the information collected from the documents in the package from earlier stages. We can do this by introducing a human in the loop that can review and validate all the fields or just an auto-approval process for low dollar claims before sending the package to downstream applications. We can use <a href=\"https://aws.amazon.com/augmented-ai/\">Amazon Augmented AI</a> (Amazon A2I) to automate the human review process for insurance claims processing.</p> \n<p><img loading=\"lazy\" class=\"alignnone wp-image-43848\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/05/ML10191-review-validation-phase.png\" alt=\"\" width=\"800\" height=\"298\"></p> \n<p>Now that we have all required data extracted and normalized from claims processing using AI services for IDP, we can extend the solution to integrate with AWS Analytics services such as AWS Glue and Amazon Redshift to solve additional use cases and provide further analytics and visualizations.</p> \n<h2>Detect fraudulent insurance claims</h2> \n<p>In this post, we implement a serverless architecture where the extracted and processed data is stored in a data lake and is used to detect fraudulent insurance claims using ML. We use <a href=\"http://aws.amazon.com/s3\">Amazon Simple Storage Service</a> (Amazon S3) to store the processed data. We can then use <a href=\"https://aws.amazon.com/glue/\">AWS Glue</a> or <a href=\"http://aws.amazon.com/emr\">Amazon EMR</a> to cleanse the data and add additional fields to make it consumable for reporting and ML. After that, we use <a href=\"https://aws.amazon.com/redshift/features/redshift-ml/\">Amazon Redshift ML</a> to build a fraud detection ML model. Finally, we build reports using <a href=\"https://aws.amazon.com/quicksight\">Amazon QuickSight</a> to get insights into the data.</p> \n<h3>Setup Amazon Redshift external schema</h3> \n<p>For the purpose of this example, we have created a <a href=\"https://idp-assets-wwso.s3.us-east-2.amazonaws.com/workshop-data/output.csv\">sample dataset</a> the emulates the output of an ETL (extract, transform, and load) process, and use AWS Glue Data Catalog as the metadata catalog. First, we create a database named <code>idp_demo</code> in the Data Catalog and an external schema in Amazon Redshift called <code>idp_insurance_demo</code> (see the following code). We use an <a href=\"http://aws.amazon.com/iam\">AWS Identity and Access Management</a> (IAM) role to grant permissions to the Amazon Redshift cluster to access Amazon S3 and <a href=\"https://aws.amazon.com/sagemaker/\">Amazon SageMaker</a>. For more information about how to set up this IAM role with least privilege, refer to <a href=\"https://docs.aws.amazon.com/redshift/latest/dg/admin-setup.html\">Cluster and configure setup for Amazon Redshift ML administration</a>.</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-sql\">CREATE EXTERNAL SCHEMA idp_insurance_demo\nFROM DATA CATALOG\nDATABASE 'idp_demo' \nIAM_ROLE '<em>&lt;&lt;&lt;your IAM Role here&gt;&gt;&gt;</em>'\nCREATE EXTERNAL DATABASE IF NOT EXISTS;\n</code></pre> \n</div> \n<h3>Create Amazon Redshift external table</h3> \n<p>The next step is to create an external table in Amazon Redshift referencing the S3 location where the file is located. In this case, our file is a comma-separated text file. We also want to skip the header row from the file, which can be configured in the table properties section. See the following code:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-sql\">create external table idp_insurance_demo.claims(id INTEGER,\ndate_of_service date,\npatients_address_city VARCHAR,\npatients_address_state VARCHAR,\npatients_address_zip VARCHAR,\npatient_status VARCHAR,\ninsured_address_state VARCHAR,\ninsured_address_zip VARCHAR,\ninsured_date_of_birth date,\ninsurance_plan_name VARCHAR,\ntotal_charges DECIMAL(14,4),\nfraud VARCHAR,\nduplicate varchar,\ninvalid_claim VARCHAR\n)\nrow format delimited\nfields terminated by ','\nstored as textfile\nlocation '<em>&lt;&lt;&lt;S3 path where file is located&gt;&gt;&gt;</em>'\ntable properties ( 'skip.header.line.count'='1');</code></pre> \n</div> \n<h3>Create training and test datasets</h3> \n<p>After we create the external table, we prepare our dataset for ML by splitting it into training set and test set. We create a new external table called <code>claim_train</code>, which consists of all records with ID &lt;= 85000 from the claims table. This is the training set that we train our ML model on.</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-sql\">CREATE EXTERNAL TABLE\nidp_insurance_demo.claims_train\nrow format delimited\nfields terminated by ','\nstored as textfile\nlocation '<em>&lt;&lt;&lt;S3 path where file is located&gt;&gt;&gt;</em>/train'\ntable properties ( 'skip.header.line.count'='1')\nAS select * from idp_insurance_demo.claims where id &lt;= 850000</code></pre> \n</div> \n<p>We create another external table called <code>claim_test</code> that consists of all records with ID &gt;85000 to be the test set that we test the ML model on:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-sql\">CREATE EXTERNAL TABLE\nidp_insurance_demo.claims_test\nrow format delimited\nfields terminated by ','\nstored as textfile\nlocation '<em>&lt;&lt;&lt;S3 path where file is located&gt;&gt;&gt;</em>/test'\ntable properties ( 'skip.header.line.count'='1')\nAS select * from idp_insurance_demo.claims where id &gt; 850000</code></pre> \n</div> \n<h3>Create an ML model with Amazon Redshift ML</h3> \n<p>Now we create the model using the <a href=\"https://docs.aws.amazon.com/redshift/latest/dg/r_CREATE_MODEL.html\">CREATE MODEL</a> command (see the following code). We select the relevant columns from the <code>claims_train</code> table that can determine a fraudulent transaction. The goal of this model is to predict the value of the <code>fraud</code> column; therefore, <code>fraud</code> is added as the prediction target. After the model is trained, it creates a function named <code>insurance_fraud_model</code>. This function is used for inference while running SQL statements to predict the value of the <code>fraud</code> column for new records.</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-sql\">CREATE MODEL idp_insurance_demo.insurance_fraud_model\nFROM (SELECT \ntotal_charges ,\nfraud ,\nduplicate,\ninvalid_claim\nFROM idp_insurance_demo.claims_train\n)\nTARGET fraud\nFUNCTION insurance_fraud_model\nIAM_ROLE '<em>&lt;&lt;&lt;your IAM Role here&gt;&gt;&gt;</em>'\nSETTINGS (\nS3_BUCKET '<em>&lt;&lt;&lt;S3 bucket where model artifacts will be stored&gt;&gt;&gt;</em>'\n);</code></pre> \n</div> \n<h3>Evaluate ML model metrics</h3> \n<p>After we create the model, we can run queries to check the accuracy of the model. We use the <code>insurance_fraud_model</code> function to predict the value of the <code>fraud</code> column for new records. Run the following query on the <code>claims_test</code> table to create a confusion matrix:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-sql\">SELECT \nfraud,\nidp_insurance_demo.insurance_fraud_model (total_charges ,duplicate,invalid_claim ) as fraud_calculcated,\ncount(1)\nFROM idp_insurance_demo.claims_test\nGROUP BY fraud , fraud_calculcated;</code></pre> \n</div> \n<h3>Detect fraud using the ML model</h3> \n<p>After we create the new model, as new claims data is inserted into the data warehouse or data lake, we can use the <code>insurance_fraud_model</code> function to calculate the fraudulent transactions. We do this by first loading the new data into a temporary table. Then we use the <code>insurance_fraud_model</code> function to calculate the <code>fraud</code> flag for each new transaction and insert the data along with the flag into the final table, which in this case is the <code>claims</code> table.</p> \n<h2>Visualize the claims data</h2> \n<p>When the data is available in Amazon Redshift, we can create visualizations using QuickSight. We can then share the QuickSight dashboards with business users and analysts. To create the QuickSight dashboard, you first need to create an Amazon Redshift dataset in QuickSight. For instructions, refer to <a href=\"https://docs.aws.amazon.com/quicksight/latest/user/create-a-database-data-set.html\">Creating a dataset from a database</a>.</p> \n<p>After you create the dataset, you can create a new analysis in QuickSight using the dataset. The following are some sample reports we created:</p> \n<ul> \n <li><strong>Total number of claims by state, grouped by the <code>fraud</code> field</strong> – This chart shows us the proportion of fraudulent transactions compared to the total number of transactions in a particular state.</li> \n <li><strong>Sum of the total dollar value of the claims, grouped by the <code>fraud</code> field</strong> – This chart shows us the proportion of dollar amount of fraudulent transactions compared to the total dollar amount of transactions in a particular state.</li> \n <li><strong>Total number of transactions per insurance company, grouped by the <code>fraud</code> field</strong> – This chart shows us how many claims were filed for each insurance company and how many of them are fraudulent.</li> \n</ul> \n<p><img loading=\"lazy\" class=\"alignnone wp-image-43889\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/06/ML-10191-visual1-1.png\" alt=\"• Total number of transactions per insurance company, grouped by the fraud field \" width=\"500\" height=\"257\"></p> \n<ul> \n <li><strong>Total sum of fraudulent transactions by state displayed on a US map</strong> – This chart just shows the fraudulent transactions and displays the total charges for those transactions by state on the map. The darker shade of blue indicates higher total charges. We can further analyze this by city within that state and zip codes with the city to better understand the trends.</li> \n</ul> \n<p><img loading=\"lazy\" class=\"alignnone wp-image-45362 size-full\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/11/03/fraudulent-by-state.jpg\" alt=\"\" width=\"1000\" height=\"500\"></p> \n<h2>Clean up</h2> \n<p>To prevent incurring future charges to your AWS account, delete the resources that you provisioned in the setup by following the instructions in the <a href=\"https://github.com/aws-samples/aws-ai-intelligent-document-processing/blob/main/industry/insurance/04-document-enrichment.ipynb\">Cleanup section</a> in our repo.</p> \n<h2>Conclusion</h2> \n<p>In this two-part series, we saw how to build an end-to-end IDP pipeline with little or no ML experience. We explored a claims processing use case in the insurance industry and how IDP can help automate this use case using services such as Amazon Textract, Amazon Comprehend, Amazon Comprehend Medical, and Amazon A2I. In Part 1, we demonstrated how to use AWS AI services for document extraction. In Part 2, we extended the extraction phase and performed data enrichment. Finally, we extended the structured data extracted from IDP for further analytics, and created visualizations to detect fraudulent claims using AWS Analytics services.</p> \n<p>We recommend reviewing the security sections of the <a href=\"https://docs.aws.amazon.com/textract/latest/dg/security.html\">Amazon Textract</a>, <a href=\"https://docs.aws.amazon.com/comprehend/latest/dg/comp-security.html\">Amazon Comprehend</a>, and <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/a2i-permissions-security.html\">Amazon A2I</a> documentation and following the guidelines provided. To learn more about the pricing of the solution, review the pricing details of <a href=\"https://aws.amazon.com/textract/pricing/\">Amazon Textract</a>, <a href=\"https://aws.amazon.com/comprehend/pricing/\">Amazon Comprehend</a>, and <a href=\"https://aws.amazon.com/augmented-ai/pricing/\">Amazon A2I</a>.</p> \n<hr> \n<h3>About the Authors</h3> \n<p style=\"clear: both\"><strong><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/08/02/chinrane-1.png\"><img loading=\"lazy\" class=\"size-full wp-image-40413 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/08/02/chinrane-1.png\" alt=\"author\" width=\"100\" height=\"127\"></a></strong><strong>Chinmayee Rane</strong> is an AI/ML Specialist Solutions Architect at Amazon Web Services. She is passionate about applied mathematics and machine learning. She focuses on designing intelligent document processing solutions for AWS customers. Outside of work, she enjoys salsa and bachata dancing.</p> \n<p style=\"clear: both\"><strong><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/06/uday-narayanan.png\"><br> <img loading=\"lazy\" class=\"wp-image-43891 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/06/uday-narayanan.png\" alt=\"\" width=\"100\" height=\"116\"></a><strong>Uday Narayanan </strong></strong>is an Analytics Specialist Solutions Architect at AWS. He enjoys helping customers find innovative solutions to complex business challenges. His core areas of focus are data analytics, big data systems, and machine learning. In his spare time, he enjoys playing sports, binge-watching TV shows, and traveling.</p> \n<p style=\"clear: both\"><strong><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/09/21/sonali-headshot-1.png\"><br> <img loading=\"lazy\" class=\"size-full wp-image-43006 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/09/21/sonali-headshot-1.png\" alt=\"\" width=\"100\" height=\"148\"></a><strong>Sonali Sahu </strong></strong>is leading the Intelligent Document Processing AI/ML Solutions Architect team at Amazon Web Services. She is a passionate technophile and enjoys working with customers to solve complex problems using innovation. Her core area of focus is artificial intelligence and machine learning for intelligent document processing.</p>"
}