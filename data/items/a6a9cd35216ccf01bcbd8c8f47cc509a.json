{
  "title": "Learning safety in model-based Reinforcement Learning using MPC and Gaussian Processes. (arXiv:2211.01860v1 [eess.SY])",
  "link": "http://arxiv.org/abs/2211.01860",
  "description": "<p>In this work, we propose a method to encourage safety in a Model Predictive\nControl (MPC)-based Reinforcement Learning (RL) agent via Gaussian Process (GP)\nregression. This framework consists of 1) a parametric MPC scheme that is\nemployed as model-based controller with approximate knowledge on the real\nsystem's dynamics, 2) an episodic RL algorithm tasked with adjusting the MPC\nparametrization in order to increase its performance, and lastly, 3) GP\nregressors used to estimate, directly from data, constraints on the MPC\nparameters capable of predicting, up to some probability, whether the\nparametrization is likely to yield a safe or unsafe policy. These constraints\nare then enforced onto the RL updates in an effort to enhance the learning\nmethod with a probabilistic safety mechanism. Compared to other recent\npublications combining safe RL with MPC, our method does not require further\nassumptions on, e.g., the prediction model in order to retain computational\ntractability. We illustrate the results of our method in a numerical example on\nthe control of a quadrotor drone in a safety-critical environment.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/eess/1/au:+Airaldi_F/0/1/0/all/0/1\">Filippo Airaldi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schutter_B/0/1/0/all/0/1\">Bart De Schutter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dabiri_A/0/1/0/all/0/1\">Azita Dabiri</a>"
}