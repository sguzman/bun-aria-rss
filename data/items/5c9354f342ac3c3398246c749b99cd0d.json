{
  "title": "Dask Distributed Release 1.13.0",
  "link": "",
  "updated": "2016-09-12T00:00:00+00:00",
  "id": "https://mrocklin.github.io/blog/work/2016/09/12/dask-distributed-release-1.13.0",
  "content": "<p>I’m pleased to announce a release of\n<a href=\"http://dask.readthedocs.io/en/latest/\">Dask</a>’s distributed scheduler,\n<a href=\"http://distributed.readthedocs.io/en/latest/\">dask.distributed</a>, version\n1.13.0.</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>conda install dask distributed -c conda-forge\nor\npip install dask distributed --upgrade\n</code></pre></div></div>\n\n<p>The last few months have seen a number of important user-facing features:</p>\n\n<ul>\n  <li>Executor is renamed to Client</li>\n  <li>Workers can spill excess data to disk when they run out of memory</li>\n  <li>The Client.compute and Client.persist methods for dealing with dask\ncollections (like dask.dataframe or dask.delayed) gain the ability to\nrestrict sub-components of the computation to different parts of the\ncluster with a <code class=\"language-plaintext highlighter-rouge\">workers=</code> keyword argument.</li>\n  <li>IPython kernels can be deployed on the worker and schedulers for\ninteractive debugging.</li>\n  <li>The Bokeh web interface has gained new plots and improve the visual styling\nof old ones.</li>\n</ul>\n\n<p>Additionally there are beta features in current development.  These features\nare available now, but may change without warning in future versions.\nExperimentation and feedback by users comfortable with living on the bleeding\nedge is most welcome:</p>\n\n<ul>\n  <li>Clients can publish named datasets on the scheduler to share between them</li>\n  <li>Tasks can launch other tasks</li>\n  <li>Workers can restart themselves in new software environments provided by the\nuser</li>\n</ul>\n\n<p>There have also been significant internal changes.  Other than increased\nperformance these changes should not be directly apparent.</p>\n\n<ul>\n  <li>The scheduler was refactored to a more state-machine like architecture.\n<a href=\"http://distributed.readthedocs.io/en/latest/scheduling-state.html\">Doc page</a></li>\n  <li>Short-lived connections are now managed by a connection pool</li>\n  <li>Work stealing has changed and grown more responsive:\n<a href=\"http://distributed.readthedocs.io/en/latest/work-stealing.html\">Doc page</a></li>\n  <li>General resilience improvements</li>\n</ul>\n\n<p>The rest of this post will contain very brief explanations of the topics above.\nSome of these topics may become blogposts of their own at some point.  Until\nthen I encourage people to look at the <a href=\"http://distributed.readthedocs.io/en/latest\">distributed scheduler’s\ndocumentation</a> which is separate\nfrom <a href=\"http://dask.readthedocs.io/en/latest/\">dask’s normal documentation</a> and\nso may contain new information for some readers (Google Analytics reports about\n5-10x the readership on\n<a href=\"http://dask.readthedocs.org\">http://dask.readthedocs.org</a> than on\n<a href=\"http://distributed.readthedocs.org\">http://distributed.readthedocs.org</a>.</p>\n\n<h2 id=\"major-changes-and-features\">Major Changes and Features</h2>\n\n<h3 id=\"rename-executor-to-client\">Rename Executor to Client</h3>\n\n<p><a href=\"http://distributed.readthedocs.io/en/latest/api.html\">http://distributed.readthedocs.io/en/latest/api.html</a></p>\n\n<p>The term <em>Executor</em> was originally chosen to coincide with the\n<a href=\"https://docs.python.org/3/library/concurrent.futures.html\">concurrent.futures</a>\nExecutor interface, which is what defines the behavior for the <code class=\"language-plaintext highlighter-rouge\">.submit</code>,\n<code class=\"language-plaintext highlighter-rouge\">.map</code>, <code class=\"language-plaintext highlighter-rouge\">.result</code> methods and <code class=\"language-plaintext highlighter-rouge\">Future</code> object used as the primary interface.</p>\n\n<p>Unfortunately, this is the same term used by projects like Spark and Mesos for\n“the low-level thing that executes tasks on each of the workers” causing\nsignificant confusion when communicating with other communities or for\ntransitioning users.</p>\n\n<p>In response we rename <em>Executor</em> to a somewhat more generic term, <em>Client</em> to\ndesignate its role as <em>the thing users interact with to control their\ncomputations</em>.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">distributed</span> <span class=\"kn\">import</span> <span class=\"n\">Executor</span>  <span class=\"c1\"># Old\n</span><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">e</span> <span class=\"o\">=</span> <span class=\"n\">Executor</span><span class=\"p\">()</span>                    <span class=\"c1\"># Old\n</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">distributed</span> <span class=\"kn\">import</span> <span class=\"n\">Client</span>    <span class=\"c1\"># New\n</span><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">c</span> <span class=\"o\">=</span> <span class=\"n\">Client</span><span class=\"p\">()</span>                      <span class=\"c1\"># New\n</span></code></pre></div></div>\n\n<p>Executor remains an alias for Client and will continue to be valid for some\ntime, but there may be some backwards incompatible changes for internal use of\n<code class=\"language-plaintext highlighter-rouge\">executor=</code> keywords within methods.  Newer examples and materials will all use\nthe term <code class=\"language-plaintext highlighter-rouge\">Client</code>.</p>\n\n<h3 id=\"workers-spill-excess-data-to-disk\">Workers Spill Excess Data to Disk</h3>\n\n<p><a href=\"http://distributed.readthedocs.io/en/latest/worker.html#spill-excess-data-to-disk\">http://distributed.readthedocs.io/en/latest/worker.html#spill-excess-data-to-disk</a></p>\n\n<p>When workers get close to running out of memory they can send excess data to\ndisk.  This is not on by default and instead requires adding the\n<code class=\"language-plaintext highlighter-rouge\">--memory-limit=auto</code> option to <code class=\"language-plaintext highlighter-rouge\">dask-worker</code>.</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>dask-worker scheduler:8786                      # Old\ndask-worker scheduler:8786 --memory-limit=auto  # New\n</code></pre></div></div>\n\n<p>This will eventually become the default (and is now when using\n<a href=\"http://distributed.readthedocs.io/en/latest/local-cluster.html\">LocalCluster</a>)\nbut we’d like to see how things progress and phase it in slowly.</p>\n\n<p>Generally this feature should improve robustness and allow the solution of\nlarger problems on smaller clusters, although with a performance cost.  Dask’s\npolicies to reduce memory use through clever scheduling remain in place, so in\nthe common case you should never need this feature, but it’s nice to have as a\nfailsafe.</p>\n\n<h3 id=\"enable-restriction-of-valid-workers-for-compute-and-persist-methods\">Enable restriction of valid workers for compute and persist methods</h3>\n\n<p><a href=\"http://distributed.readthedocs.io/en/latest/locality.html#user-control\">http://distributed.readthedocs.io/en/latest/locality.html#user-control</a></p>\n\n<p>Expert users of the distributed scheduler will be aware of the ability to\nrestrict certain tasks to run only on certain computers.  This tends to be\nuseful when dealing with GPUs or  with special databases or instruments only\navailable on some machines.</p>\n\n<p>Previously this option was available only on the <code class=\"language-plaintext highlighter-rouge\">submit</code>, <code class=\"language-plaintext highlighter-rouge\">map</code>, and <code class=\"language-plaintext highlighter-rouge\">scatter</code>\nmethods, forcing people to use the more immedate interface.  Now the dask\ncollection interface functions <code class=\"language-plaintext highlighter-rouge\">compute</code> and <code class=\"language-plaintext highlighter-rouge\">persist</code> support this keyword as\nwell.</p>\n\n<h3 id=\"ipython-integration\">IPython Integration</h3>\n\n<p><a href=\"http://distributed.readthedocs.io/en/latest/ipython.html\">http://distributed.readthedocs.io/en/latest/ipython.html</a></p>\n\n<p>You can start IPython kernels on the workers or scheduler and then access them\ndirectly using either IPython magics or the QTConsole.  This tends to be\nvaluable when things go wrong and you want to interactively debug on the worker\nnodes themselves.</p>\n\n<p><strong>Start IPython on the Scheduler</strong></p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">client</span><span class=\"p\">.</span><span class=\"n\">start_ipython_scheduler</span><span class=\"p\">()</span>  <span class=\"c1\"># Start IPython kernel on the scheduler\n</span><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"o\">%</span><span class=\"n\">scheduler</span> <span class=\"n\">scheduler</span><span class=\"p\">.</span><span class=\"n\">processing</span>   <span class=\"c1\"># Use IPython magics to inspect scheduler\n</span><span class=\"p\">{</span><span class=\"s\">'127.0.0.1:3595'</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s\">'inc-1'</span><span class=\"p\">,</span> <span class=\"s\">'inc-2'</span><span class=\"p\">],</span>\n <span class=\"s\">'127.0.0.1:53589'</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s\">'inc-2'</span><span class=\"p\">,</span> <span class=\"s\">'add-5'</span><span class=\"p\">]}</span>\n</code></pre></div></div>\n\n<p><strong>Start IPython on the Workers</strong></p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">info</span> <span class=\"o\">=</span> <span class=\"n\">e</span><span class=\"p\">.</span><span class=\"n\">start_ipython_workers</span><span class=\"p\">()</span>  <span class=\"c1\"># Start IPython kernels on all workers\n</span><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"n\">info</span><span class=\"p\">)</span>\n<span class=\"p\">[</span><span class=\"s\">'127.0.0.1:4595'</span><span class=\"p\">,</span> <span class=\"s\">'127.0.0.1:53589'</span><span class=\"p\">]</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"o\">%</span><span class=\"n\">remote</span> <span class=\"n\">info</span><span class=\"p\">[</span><span class=\"s\">'127.0.0.1:3595'</span><span class=\"p\">]</span> <span class=\"n\">worker</span><span class=\"p\">.</span><span class=\"n\">active</span>  <span class=\"c1\"># Use IPython magics\n</span><span class=\"p\">{</span><span class=\"s\">'inc-1'</span><span class=\"p\">,</span> <span class=\"s\">'inc-2'</span><span class=\"p\">}</span>\n</code></pre></div></div>\n\n<h3 id=\"bokeh-interface\">Bokeh Interface</h3>\n\n<p><a href=\"http://distributed.readthedocs.io/en/latest/web.html\">http://distributed.readthedocs.io/en/latest/web.html</a></p>\n\n<p>The Bokeh web interface to the cluster continues to evolve both by improving\nexisting plots and by adding new plots and new pages.</p>\n\n<p><img src=\"https://raw.githubusercontent.com/dask/dask-org/master/images/bokeh-progress-large.gif\" alt=\"dask progress bar\" width=\"60%\" align=\"right\" /></p>\n\n<p>For example the progress bars have become more compact and shrink down\ndynamically to respond to addiional bars.</p>\n\n<p>And we’ve added in extra tables and plots to monitor workers, such as their\nmemory use and current backlog of tasks.</p>\n\n<h2 id=\"experimental-features\">Experimental Features</h2>\n\n<p>The features described below are experimental and may change without warning.\nPlease do not depend on them in stable code.</p>\n\n<h3 id=\"publish-datasets\">Publish Datasets</h3>\n\n<p><a href=\"http://distributed.readthedocs.io/en/latest/publish.html\">http://distributed.readthedocs.io/en/latest/publish.html</a></p>\n\n<p>You can now save collections on the scheduler, allowing you to come back to the\nsame computations later or allow collaborators to see and work off of your\nresults.  This can be useful in the following cases:</p>\n\n<ol>\n  <li>There is a dataset from which you frequently base all computations, and you\nwant that dataset always in memory and easy to access without having to\nrecompute it each time you start work, even if you disconnect.</li>\n  <li>You want to send results to a colleague working on the same Dask cluster and\nhave them get immediate access to your computations without having to send\nthem a script and without them having to repeat the work on the cluster.</li>\n</ol>\n\n<p><strong>Example: Client One</strong></p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">dask.distributed</span> <span class=\"kn\">import</span> <span class=\"n\">Client</span>\n<span class=\"n\">client</span> <span class=\"o\">=</span> <span class=\"n\">Client</span><span class=\"p\">(</span><span class=\"s\">'scheduler-address:8786'</span><span class=\"p\">)</span>\n\n<span class=\"kn\">import</span> <span class=\"nn\">dask.dataframe</span> <span class=\"k\">as</span> <span class=\"n\">dd</span>\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">dd</span><span class=\"p\">.</span><span class=\"n\">read_csv</span><span class=\"p\">(</span><span class=\"s\">'s3://my-bucket/*.csv'</span><span class=\"p\">)</span>\n<span class=\"n\">df2</span> <span class=\"o\">=</span> <span class=\"n\">df</span><span class=\"p\">[</span><span class=\"n\">df</span><span class=\"p\">.</span><span class=\"n\">balance</span> <span class=\"o\">&lt;</span> <span class=\"mi\">0</span><span class=\"p\">]</span>\n<span class=\"n\">df2</span> <span class=\"o\">=</span> <span class=\"n\">client</span><span class=\"p\">.</span><span class=\"n\">persist</span><span class=\"p\">(</span><span class=\"n\">df2</span><span class=\"p\">)</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">df2</span><span class=\"p\">.</span><span class=\"n\">head</span><span class=\"p\">()</span>\n      <span class=\"n\">name</span>  <span class=\"n\">balance</span>\n<span class=\"mi\">0</span>    <span class=\"n\">Alice</span>     <span class=\"o\">-</span><span class=\"mi\">100</span>\n<span class=\"mi\">1</span>      <span class=\"n\">Bob</span>     <span class=\"o\">-</span><span class=\"mi\">200</span>\n<span class=\"mi\">2</span>  <span class=\"n\">Charlie</span>     <span class=\"o\">-</span><span class=\"mi\">300</span>\n<span class=\"mi\">3</span>   <span class=\"n\">Dennis</span>     <span class=\"o\">-</span><span class=\"mi\">400</span>\n<span class=\"mi\">4</span>    <span class=\"n\">Edith</span>     <span class=\"o\">-</span><span class=\"mi\">500</span>\n\n<span class=\"n\">client</span><span class=\"p\">.</span><span class=\"n\">publish_dataset</span><span class=\"p\">(</span><span class=\"n\">accounts</span><span class=\"o\">=</span><span class=\"n\">df2</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p><strong>Example: Client Two</strong></p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">dask.distributed</span> <span class=\"kn\">import</span> <span class=\"n\">Client</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">client</span> <span class=\"o\">=</span> <span class=\"n\">Client</span><span class=\"p\">(</span><span class=\"s\">'scheduler-address:8786'</span><span class=\"p\">)</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">client</span><span class=\"p\">.</span><span class=\"n\">list_datasets</span><span class=\"p\">()</span>\n<span class=\"p\">[</span><span class=\"s\">'accounts'</span><span class=\"p\">]</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">client</span><span class=\"p\">.</span><span class=\"n\">get_dataset</span><span class=\"p\">(</span><span class=\"s\">'accounts'</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">df</span><span class=\"p\">.</span><span class=\"n\">head</span><span class=\"p\">()</span>\n      <span class=\"n\">name</span>  <span class=\"n\">balance</span>\n<span class=\"mi\">0</span>    <span class=\"n\">Alice</span>     <span class=\"o\">-</span><span class=\"mi\">100</span>\n<span class=\"mi\">1</span>      <span class=\"n\">Bob</span>     <span class=\"o\">-</span><span class=\"mi\">200</span>\n<span class=\"mi\">2</span>  <span class=\"n\">Charlie</span>     <span class=\"o\">-</span><span class=\"mi\">300</span>\n<span class=\"mi\">3</span>   <span class=\"n\">Dennis</span>     <span class=\"o\">-</span><span class=\"mi\">400</span>\n<span class=\"mi\">4</span>    <span class=\"n\">Edith</span>     <span class=\"o\">-</span><span class=\"mi\">500</span>\n</code></pre></div></div>\n\n<h3 id=\"launch-tasks-from-tasks\">Launch Tasks from tasks</h3>\n\n<p><a href=\"http://distributed.readthedocs.io/en/latest/task-launch.html\">http://distributed.readthedocs.io/en/latest/task-launch.html</a></p>\n\n<p>You can now submit tasks to the cluster that themselves submit more tasks.\nThis allows the submission of highly dynamic workloads that can shape\nthemselves depending on future computed values without ever checking back in\nwith the original client.</p>\n\n<p>This is accomplished by starting new local <code class=\"language-plaintext highlighter-rouge\">Client</code>s within the task that can\ninteract with the scheduler.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">def</span> <span class=\"nf\">func</span><span class=\"p\">():</span>\n    <span class=\"kn\">from</span> <span class=\"nn\">distributed</span> <span class=\"kn\">import</span> <span class=\"n\">local_client</span>\n    <span class=\"k\">with</span> <span class=\"n\">local_client</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"n\">c2</span><span class=\"p\">:</span>\n        <span class=\"n\">future</span> <span class=\"o\">=</span> <span class=\"n\">c2</span><span class=\"p\">.</span><span class=\"n\">submit</span><span class=\"p\">(...)</span>\n\n<span class=\"n\">c</span> <span class=\"o\">=</span> <span class=\"n\">Client</span><span class=\"p\">(...)</span>\n<span class=\"n\">future</span> <span class=\"o\">=</span> <span class=\"n\">c</span><span class=\"p\">.</span><span class=\"n\">submit</span><span class=\"p\">(</span><span class=\"n\">func</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>There are a few straightforward use cases for this, like iterative algorithms\nwith stoping criteria, but also many novel use cases including streaming\nand monitoring systems.</p>\n\n<h3 id=\"restart-workers-in-redeployable-python-environments\">Restart Workers in Redeployable Python Environments</h3>\n\n<p>You can now zip up and distribute full Conda environments, and ask\ndask-workers to restart themselves, live, in that environment.  This involves\nthe following:</p>\n\n<ol>\n  <li>Create a conda environment locally (or any redeployable directory including\na <code class=\"language-plaintext highlighter-rouge\">python</code> executable)</li>\n  <li>Zip up that environment and use the existing dask.distributed network\nto copy it to all of the workers</li>\n  <li>Shut down all of the workers and restart them within the new environment</li>\n</ol>\n\n<p>This helps users to experiment with different software environments with a much\nfaster turnaround time (typically tens of seconds) than asking IT to install\nlibraries or building and deploying Docker containers (which is also a fine\nsolution).  Note that they typical solution of uploading individual python\nscripts or egg files has been around for a while, <a href=\"http://distributed.readthedocs.io/en/latest/api.html#distributed.client.Client.upload_file\">see API docs for\nupload_file</a></p>\n\n<h2 id=\"acknowledgements\">Acknowledgements</h2>\n\n<p>Since version 1.12.0 on August 18th the following people have contributed\ncommits to the <a href=\"https://github.com/dask/distributed\">dask/distributed repository</a></p>\n\n<ul>\n  <li>Dave Hirschfeld</li>\n  <li>dsidi</li>\n  <li>Jim Crist</li>\n  <li>Joseph Crail</li>\n  <li>Loïc Estève</li>\n  <li>Martin Durant</li>\n  <li>Matthew Rocklin</li>\n  <li>Min RK</li>\n  <li>Scott Sievert</li>\n</ul>"
}