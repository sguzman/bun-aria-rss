{
  "title": "A Million Text Files And A Single Laptop",
  "description": "<p><img src=\"/wp-content/uploads/2016/01/million-files-size.png\" alt=\"GNU Parallel Cat Unix\" /></p>",
  "pubDate": "Thu, 28 Jan 2016 09:53:42 +0000",
  "link": "http://randyzwitch.com/gnu-parallel-medium-data/",
  "guid": "http://randyzwitch.com/gnu-parallel-medium-data/",
  "content": "<p><img src=\"/wp-content/uploads/2016/01/million-files-size.png\" alt=\"GNU Parallel Cat Unix\" /></p>\n\n<p>More often that I would like, I receive datasets where the data has only been partially cleaned, such as the picture on the right: hundreds, thousands…even millions of tiny files. Usually when this happens, the data all have the same format (such as having being generated by sensors or other memory-constrained devices).</p>\n\n<p>The problem with data like this is that 1) it’s inconvenient to think about a dataset as a million individual pieces 2) the data in aggregate are too large to hold in RAM but 3) the data are small enough where using Hadoop or even a relational database seems like overkill.</p>\n\n<p>Surprisingly, with judicious use of <a href=\"http://www.gnu.org/software/parallel/\">GNU Parallel</a>, stream processing and a relatively modern computer, you can efficiently process annoying, “medium-sized” data as described above.</p>\n\n<h2 id=\"data-generation\">Data Generation</h2>\n\n<p>For this blog post, I used a combination of R and Python to generate the data: the “Groceries” dataset from the <a href=\"https://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\">arules</a> package for sampling transactions (with replacement), and the Python <a href=\"https://github.com/joke2k/faker\">Faker (fake-factory)</a> package to generate fake customer profiles and for creating the 1MM+ text files:</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-python\" data-lang=\"python\"><table class=\"rouge-table\"><tbody><tr><td class=\"gutter gl\"><pre class=\"lineno\">1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n</pre></td><td class=\"code\"><pre><span class=\"c1\">#R Code\n</span><span class=\"n\">library</span><span class=\"p\">(</span><span class=\"n\">arules</span><span class=\"p\">)</span>\n<span class=\"n\">data</span><span class=\"p\">(</span><span class=\"s\">\"Groceries\"</span><span class=\"p\">)</span>\n<span class=\"n\">write</span><span class=\"p\">(</span><span class=\"n\">Groceries</span><span class=\"p\">,</span> <span class=\"s\">\"groceries.txt\"</span><span class=\"p\">,</span> <span class=\"n\">sep</span> <span class=\"o\">=</span> <span class=\"s\">\",\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\">#Python Code\n</span><span class=\"kn\">import</span> <span class=\"nn\">random</span><span class=\"p\">,</span> <span class=\"n\">csv</span>\n<span class=\"kn\">from</span> <span class=\"nn\">faker</span> <span class=\"kn\">import</span> <span class=\"n\">Faker</span>\n<span class=\"n\">fake</span> <span class=\"o\">=</span> <span class=\"n\">Faker</span><span class=\"p\">()</span>\n<span class=\"kn\">from</span> <span class=\"nn\">pandas</span> <span class=\"kn\">import</span> <span class=\"n\">DataFrame</span>\n<span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"n\">pd</span>\n\n<span class=\"c1\"># Create customer file of 1,234,567 customers with fake data\n# Use dataframe index as a way to generate unique customer id\n</span><span class=\"n\">customers</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">fake</span><span class=\"p\">.</span><span class=\"n\">simple_profile</span><span class=\"p\">()</span> <span class=\"k\">for</span> <span class=\"n\">x</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">1234567</span><span class=\"p\">)]</span>\n<span class=\"n\">customer_df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"p\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">customers</span><span class=\"p\">)</span>\n<span class=\"n\">customer_df</span><span class=\"p\">[</span><span class=\"s\">\"cust_id\"</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">customer_df</span><span class=\"p\">.</span><span class=\"n\">index</span>\n\n<span class=\"c1\">#Read in transactions file from arules package\n</span><span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s\">\"grocerydata.txt\"</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n    <span class=\"n\">transactions</span> <span class=\"o\">=</span> <span class=\"n\">f</span><span class=\"p\">.</span><span class=\"n\">readlines</span><span class=\"p\">()</span>\n\n<span class=\"c1\">#Remove new line character\n</span><span class=\"n\">transactions</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">x</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">:</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">x</span> <span class=\"ow\">in</span> <span class=\"n\">transactions</span><span class=\"p\">]</span>\n\n<span class=\"c1\">#Generate transactions by cust_id\n</span>\n<span class=\"c1\">#file format:\n#cust_id::int\n#store_id::int\n#transaction_datetime::string/datetime\n#items::string\n</span>\n<span class=\"c1\">#for each customer...\n</span><span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">1234567</span><span class=\"p\">):</span>\n    <span class=\"c1\">#...create a file...\n</span>    <span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s\">'/transactions/custfile_%s'</span> <span class=\"o\">%</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"s\">'w'</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">csvfile</span><span class=\"p\">:</span>\n        <span class=\"n\">trans</span> <span class=\"o\">=</span> <span class=\"n\">csv</span><span class=\"p\">.</span><span class=\"n\">writer</span><span class=\"p\">(</span><span class=\"n\">csvfile</span><span class=\"p\">,</span> <span class=\"n\">delimiter</span><span class=\"o\">=</span><span class=\"s\">' '</span><span class=\"p\">,</span> <span class=\"n\">quotechar</span><span class=\"o\">=</span><span class=\"s\">'\"'</span><span class=\"p\">,</span> <span class=\"n\">quoting</span><span class=\"o\">=</span><span class=\"n\">csv</span><span class=\"p\">.</span><span class=\"n\">QUOTE_MINIMAL</span><span class=\"p\">)</span>\n        <span class=\"c1\">#...that contains all of the transactions they've ever made\n</span>        <span class=\"k\">for</span> <span class=\"n\">j</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">randint</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">365</span><span class=\"p\">)):</span>\n            <span class=\"n\">trans</span><span class=\"p\">.</span><span class=\"n\">writerow</span><span class=\"p\">([</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">fake</span><span class=\"p\">.</span><span class=\"n\">zipcode</span><span class=\"p\">(),</span> <span class=\"n\">fake</span><span class=\"p\">.</span><span class=\"n\">date_time_this_decade</span><span class=\"p\">(</span><span class=\"n\">before_now</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"n\">after_now</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">),</span> <span class=\"n\">transactions</span><span class=\"p\">[</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">randint</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">transactions</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"p\">)]])</span>\n</pre></td></tr></tbody></table></code></pre></figure>\n\n<h2 id=\"problem-1-concatenating-cat---outtxt-\">Problem 1: Concatenating (<code class=\"language-plaintext highlighter-rouge\">cat * &gt;&gt; out.txt</code> ?!)</h2>\n\n<p>The <a href=\"http://man7.org/linux/man-pages/man1/cat.1.html\">cat</a> utility in Unix-y systems is familiar to most anyone who has ever opened up a Terminal window. Take some or all of the files in a folder, concatenate them together….one big file. But something funny happens once you get enough files…</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"nv\">$ </span><span class=\"nb\">cat</span> <span class=\"k\">*</span> <span class=\"o\">&gt;&gt;</span> out.txt\n\n<span class=\"nt\">-bash</span>: /bin/cat: Argument list too long</code></pre></figure>\n\n<p>That’s a fun thought…too many files for the computer to keep track of. As it turns out, many Unix tools will only accept about 10,000 arguments; the use of the asterisk in the <code class=\"language-plaintext highlighter-rouge\">cat</code> command gets expanded before running, so the above statement passes 1,234,567 arguments to <code class=\"language-plaintext highlighter-rouge\">cat</code> and you get an error message.</p>\n\n<p>One (naive) solution would be to loop over every file (a completely serial operation):</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"k\">for </span>f <span class=\"k\">in</span> <span class=\"k\">*</span><span class=\"p\">;</span> <span class=\"k\">do </span><span class=\"nb\">cat</span> <span class=\"s2\">\"</span><span class=\"nv\">$f</span><span class=\"s2\">\"</span> <span class=\"o\">&gt;&gt;</span> ../transactions_cat/transactions.csv<span class=\"p\">;</span> <span class=\"k\">done</span></code></pre></figure>\n\n<p>Roughly <strong>10,093 seconds</strong> later, you’ll have your concatenated file. Three hours is quite a coffee break…</p>\n\n<h2 id=\"solution-1-gnu-parallel--concatenation\">Solution 1: GNU Parallel &amp; Concatenation</h2>\n\n<p>Above, I mentioned that looping over each file gets you past the error condition of too many arguments, but it is a serial operation. If you look at your computer usage during that operation, you’ll likely see that only a fraction of a core of your computer’s CPU is being utilized. We can greatly improve that through the use of GNU Parallel:</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"nb\">ls</span> | parallel <span class=\"nt\">-m</span> <span class=\"nt\">-j</span> <span class=\"nv\">$f</span> <span class=\"s2\">\"cat {} &gt;&gt; ../transactions_cat/transactions.csv\"</span></code></pre></figure>\n\n<p>The <code class=\"language-plaintext highlighter-rouge\">$f</code> argument in the code is to highlight that you can choose the level of parallelism; however, you will not get infinitely linear scaling, as shown below (<a href=\"https://gist.github.com/randyzwitch/ee0f738b5895e059fa2a\">graph code, Julia</a>):</p>\n\n<div id=\"cat\">\n</div>\n\n<p>Given that the graph represents a single run at each level of parallelism, it’s a bit difficult to say <em>exactly</em> where the parallelism gets maxed out, but at roughly 10 concurrent jobs, there’s no additional benefit. It’s also interesting to point out what the <code class=\"language-plaintext highlighter-rouge\">-m</code> argument represents; by specifying <code class=\"language-plaintext highlighter-rouge\">m</code>, you allow multiple arguments (i.e. multiple text files) to be passed as inputs into parallel. This <em>alone</em> leads to an 8x speedup over the naive loop solution.</p>\n\n<h2 id=\"problem-2-data--ram\">Problem 2: Data &gt; RAM</h2>\n\n<p>Now that we have a single file, we’ve removed the “one million files” cognitive dissonance, but now we have a second problem: at 19.93GB, the amount of data exceeds the RAM in my laptop (2014 MBP, 16GB of RAM). So in order to do analysis, either a bigger machine is needed or processing has to be done in a streaming or “chunked” manner (such as using the <a href=\"http://pandas.pydata.org/pandas-docs/stable/io.html#iterating-through-files-chunk-by-chunk\">“chunksize” keyword in pandas</a>).</p>\n\n<p>But continuing on with our use of GNU Parallel, suppose we wanted to answer the following types of questions about our transactions data:</p>\n\n<ol>\n  <li>How many unique products were sold?</li>\n  <li>How many transactions were there per day?</li>\n  <li>How many total items were sold per store, per month?</li>\n</ol>\n\n<p>If it’s not clear from the list above, in all three questions there is an “embarrassingly parallel” portion of the computation. Let’s take a look at how to answer all three of these questions in a time- and RAM-efficient manner:</p>\n\n<h5 id=\"q1-unique-products\">Q1: Unique Products</h5>\n\n<p>Given the format of the data file (transactions in a single column array), this question is the hardest to parallelize, but using a neat trick with the <code class=\"language-plaintext highlighter-rouge\">[tr](http://www.linfo.org/tr.html)</code> (transliterate) utility, we can map our data to one product per row as we stream over the file:</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><table class=\"rouge-table\"><tbody><tr><td class=\"gutter gl\"><pre class=\"lineno\">1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n</pre></td><td class=\"code\"><pre><span class=\"c\"># Serial method (i.e. no parallelism)</span>\n<span class=\"c\"># This is a simple implementation of map &amp; reduce; tr statements represent one map, sort -u statements one reducer</span>\n\n<span class=\"c\"># cut -d ' ' -f 5- transactions.csv | \\     - Using cut, take everything from the 5th column and over from the transactions.csv file</span>\n<span class=\"c\"># tr -d \\\" | \\                              - Using tr, trim off double-quotes. This leaves us with a comma-delimited string of products representing a transaction</span>\n<span class=\"c\"># sort -u | \\                               - Using sort, put similar items together, but only output the unique values</span>\n<span class=\"c\"># wc -l                                     - Count number of unique lines, which after de-duping, represents number of unique products</span>\n\n<span class=\"nv\">$ </span><span class=\"nb\">time cut</span> <span class=\"nt\">-d</span> <span class=\"s1\">' '</span> <span class=\"nt\">-f</span> 5- transactions.csv | <span class=\"nb\">tr</span> <span class=\"nt\">-d</span> <span class=\"se\">\\\"</span> | <span class=\"nb\">tr</span> <span class=\"s1\">','</span> <span class=\"s1\">'\\n'</span> | <span class=\"nb\">sort</span> <span class=\"nt\">-u</span> | <span class=\"nb\">wc</span> <span class=\"nt\">-l</span>\n331\n\nreal\t292m7.116s\n\n<span class=\"c\"># Parallelized version, default chunk size of 1MB. This will use 100% of all CPUs (real and virtual)</span>\n<span class=\"c\"># Also map &amp; reduce; tr statements a single map, sort -u statements multiple reducers (8 by default)</span>\n\n<span class=\"nv\">$ </span><span class=\"nb\">time cut</span> <span class=\"nt\">-d</span> <span class=\"s1\">' '</span> <span class=\"nt\">-f</span> 5- transactions.csv | <span class=\"nb\">tr</span> <span class=\"nt\">-d</span> <span class=\"se\">\\\"</span> | <span class=\"nb\">tr</span> <span class=\"s1\">','</span> <span class=\"s1\">'\\n'</span> | parallel <span class=\"nt\">--pipe</span> <span class=\"nt\">--block</span> 1M <span class=\"nb\">sort</span> <span class=\"nt\">-u</span> | <span class=\"nb\">sort</span> <span class=\"nt\">-u</span> | <span class=\"nb\">wc</span> <span class=\"nt\">-l</span>\n331\n\n<span class=\"c\"># block size performance - Making block size smaller might improve performance</span>\n<span class=\"c\"># Number of jobs can also be manipulated (not evaluated)</span>\n<span class=\"c\"># --500K:               73m57.232s</span>\n<span class=\"c\"># --Default 1M:         75m55.268s (3.84x faster than serial)</span>\n<span class=\"c\"># --2M:                 79m30.950s</span>\n<span class=\"c\"># --3M:                 80m43.311s</span>\n</pre></td></tr></tbody></table></code></pre></figure>\n\n<p>The trick here is that we swap the comma-delimited transactions with the newline character; the effect of this is taking a single transaction row and returning multiple rows, one for each product. Then we pass that down the line, eventually using <code class=\"language-plaintext highlighter-rouge\">sort -u</code> to de-dup the list and <code class=\"language-plaintext highlighter-rouge\">wc -l</code> to count the number of unique lines (i.e. products).</p>\n\n<p>In a serial fashion, it takes quite some time to calculate the number of unique products. Incorporating GNU Parallel, just using the defaults, gives nearly a 4x speedup!</p>\n\n<h5 id=\"q2-transactions-by-day\">Q2. Transactions By Day</h5>\n\n<p>If the file format could be considered undesirable in question 1, for question 2 the format is perfect. Since each row represents a transaction, all we need to do is perform the equivalent of a SQL <code class=\"language-plaintext highlighter-rouge\">Group By</code> on the date and sum the rows:</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><table class=\"rouge-table\"><tbody><tr><td class=\"gutter gl\"><pre class=\"lineno\">1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n</pre></td><td class=\"code\"><pre><span class=\"c\"># Data is at transaction level, so just need to do equivalent of 'group by' operation</span>\n<span class=\"c\"># Using cut again, we choose field 3, which is the date part of the timestamp</span>\n<span class=\"c\"># sort | uniq -c is a common pattern for doing a 'group by' count operation</span>\n<span class=\"c\"># Final tr step is to trim the leading quotation mark from date string</span>\n\n<span class=\"nb\">time cut</span> <span class=\"nt\">-d</span> <span class=\"s1\">' '</span> <span class=\"nt\">-f</span> 3 transactions.csv | <span class=\"nb\">sort</span> | <span class=\"nb\">uniq</span> <span class=\"nt\">-c</span> | <span class=\"nb\">tr</span> <span class=\"nt\">-d</span> <span class=\"se\">\\\"</span>\n\nreal\t76m51.223s\n\n<span class=\"c\"># Parallelized version</span>\n<span class=\"c\"># Quoting can be annoying when using parallel, so writing a Bash function is often much easier than dealing with escaping quotes</span>\n<span class=\"c\"># To do 'group by' operation using awk, need to use an associative array</span>\n<span class=\"c\"># Because we are doing parallel operations, need to pass awk output to awk again to return final counts</span>\n\nawksub <span class=\"o\">()</span> <span class=\"o\">{</span> <span class=\"nb\">awk</span> <span class=\"s1\">'{a[$3]+=1;}END{for(i in a)print i\" \"a[i];}'</span><span class=\"p\">;</span><span class=\"o\">}</span>\n<span class=\"nb\">export</span> <span class=\"nt\">-f</span> awksub\n<span class=\"nb\">time </span>parallel <span class=\"nt\">--pipe</span> awksub &lt; transactions.csv | <span class=\"nb\">awk</span> <span class=\"s1\">'{a[$1]+=$2;}END{for(i in a)print i\" \"a[i];}'</span> | <span class=\"nb\">tr</span> <span class=\"nt\">-d</span> <span class=\"se\">\\\"</span> | <span class=\"nb\">sort\n\n</span>real\t8m22.674s <span class=\"o\">(</span>9.05x faster than serial<span class=\"o\">)</span>\n</pre></td></tr></tbody></table></code></pre></figure>\n\n<p>Using GNU Parallel starts to become complicated here, but you do get a 9x speed-up by calculating rows by date in chunks, then “reducing” again by calculating total rows by date (a trick I picked up at this <a href=\"http://www.rankfocus.com/use-cpu-cores-linux-commands/\">blog post</a>.</p>\n\n<h5 id=\"q3-total-items-per-store-per-month\">Q3. Total items Per store, Per month</h5>\n\n<p>For this example, it could be that my command-line fu is weak, but the serial method actually turns out to be the fastest. Of course, at a 14 minute run time, the real-time benefits to parallelization aren’t that great.</p>\n\n<p>It may be possible that one of you out there knows how to do this correctly, but an interesting thing to note is that the serial version already uses 40-50% of the available CPU available. So parallelization might yield a 2x speedup, but seven minutes extra per run isn’t worth spending hours trying to the optimal settings.</p>\n\n<h2 id=\"but-ive-got-multiple-files\">But, I’ve got MULTIPLE files…</h2>\n\n<p>The three examples above showed that it’s possible to process datasets larger than RAM in a realistic amount of time using GNU Parallel. However, the examples also showed that working with Unix utilities can become complicated rather quickly. Shell scripts can help move beyond the “one-liner” syndrome, when the pipeline gets so long you lose track of the logic, but eventually problems are more easily solved using other tools.</p>\n\n<p>The data that I generated at the beginning of this post represented two concepts: transactions and customers. Once you get to the point where you want to do joins, summarize by multiple columns, estimate models, etc., loading data into a database or an analytics environment like R or Python makes sense. But hopefully this post has shown that a laptop is capable of analyzing WAY more data than most people believe, using many tools written decades ago.</p>"
}