{
  "title": "Generalized Estimators, Slope, Efficiency, and Fisher Information Bounds. (arXiv:2208.03630v2 [math.ST] UPDATED)",
  "link": "http://arxiv.org/abs/2208.03630",
  "description": "<p>Point estimators may not exist, need not be unique, and their distributions\nare not parameter invariant. Generalized estimators provide distributions that\nare parameter invariant, unique, and exist when point estimates do not.\nComparing point estimators using variance is less useful when estimators are\nbiased. A squared slope $\\Lambda$ is defined that can be used to compare both\npoint and generalized estimators and is unaffected by bias. Fisher information\n$I$ and variance are fundamentally different quantities: the latter is defined\nat a distribution that need not belong to a family, while the former cannot be\ndefined without a family of distributions, $M$. Fisher information and\n$\\Lambda$ are similar quantities as both are defined on the tangent bundle\n$T\\!M$ and $I$ provides an upper bound, $\\Lambda\\le I$, that holds for all\nsample sizes -- asymptotics are not required. Comparing estimators using\n$\\Lambda$ rather than variance supports Fisher's claim that $I$ provides a\nbound even in small samples. $\\Lambda$-efficiency is defined that extends the\nefficiency of unbiased estimators based on variance. While defined by the\nslope, $\\Lambda$-efficiency is simply $\\rho^{2}$, the square of the correlation\nbetween estimator and score function.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/math/1/au:+Vos_P/0/1/0/all/0/1\">Paul W. Vos</a>"
}