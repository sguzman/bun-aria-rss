{
  "id": "tag:blogger.com,1999:blog-15418143.post-5986936212375969343",
  "published": "2015-03-20T19:58:00.000-05:00",
  "updated": "2016-06-14T03:43:35.565-05:00",
  "category": [
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    ""
  ],
  "title": "Deep Learning vs Machine Learning vs Pattern Recognition",
  "content": "<div class=\"p1\">Lets take a close look at three related terms (Deep Learning vs Machine Learning vs Pattern Recognition), and see how they relate to some of the hottest tech-themes in 2015 (namely Robotics and Artificial Intelligence). In our short journey through jargon, you should acquire a better understanding of how computer vision fits in, as well as gain an intuitive feel for how the machine learning zeitgeist has slowly evolved over time.<br /><br /></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"http://1.bp.blogspot.com/-A8krme3OoTg/VT3_afoWesI/AAAAAAAAOCk/A1CztkiMC2A/s1600/unknown.jpeg\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"240\" src=\"https://1.bp.blogspot.com/-A8krme3OoTg/VT3_afoWesI/AAAAAAAAOCk/A1CztkiMC2A/s1600/unknown.jpeg\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\">Fig 1. Putting a human inside a computer is not Artificial Intelligence</div><div class=\"separator\" style=\"clear: both; text-align: center;\">(Photo from&nbsp;<a href=\"http://crowdcomputingblog.com/2013/11/01/whats-the-true-definition-of-a-platform/\">WorkFusion Blog</a>)</div><div class=\"p2\"><span class=\"s1\"></span><br />If you look around, you'll see no shortage of jobs at high-tech startups looking for machine learning experts. While only a fraction of them are looking for Deep Learning experts, I bet most of these startups can benefit from even the most elementary kind of data scientist. So how do you spot a future data-scientist? You learn how they think.&nbsp;</div><div class=\"p3\"><br /></div><div class=\"p3\"><b>The three highly-related \"learning\" buzz words</b></div><div class=\"p3\"><b><br /></b></div><div class=\"p3\">“Pattern recognition,” “machine learning,” and “deep learning” represent three different schools of thought.&nbsp; Pattern recognition is the oldest (and as a term is quite outdated). Machine Learning is the most fundamental (one of the hottest areas for startups and research labs as of today, early 2015). And <i>Deep Learning is the new, the big, the bleeding-edge -- we’re not even close to thinking about the post-deep-learning era</i>. &nbsp;Just take a look at the following Google Trends graph. &nbsp;You'll see that a) Machine Learning is rising like a true champion, b) Pattern Recognition started as synonymous with Machine Learning, c) Pattern Recognition is dying, and d) Deep Learning is new and rising fast.</div><br /><script src=\"//www.google.com/trends/embed.js?hl=en-US&amp;q=machine+learning,+pattern+recognition,+deep+learning&amp;cmpt=q&amp;tz&amp;tz&amp;content=1&amp;cid=TIMESERIES_GRAPH_0&amp;export=5&amp;w=500&amp;h=330\" type=\"text/javascript\"></script> <br /><div class=\"p2\"><span class=\"s1\"></span><br /></div><div class=\"p3\"><span class=\"s1\"><b>1. Pattern Recognition: The birth of smart programs</b></span></div><div class=\"p3\"><span class=\"s1\"><b><br /></b></span></div><div class=\"p3\"><span class=\"s1\">Pattern recognition was a term popular in the 70s and 80s. The emphasis was on getting a computer program to do something “smart” like recognize the character \"3\". And it really took a lot of cleverness and intuition to build such a program. Just think of \"3\" vs \"B\" and \"3\" vs \"8\". &nbsp;Back in the day, i</span>t didn’t really matter how you did it as long as there was no human-in-a-box pretending to be a machine. (See Figure 1) &nbsp;So if your algorithm would apply some filters to an image, localize some edges, and apply morphological operators, it was definitely of interest to the pattern recognition community.&nbsp; Optical Character Recognition grew out of this community and it is fair to call “Pattern Recognition” as the “Smart\" Signal Processing of the 70s, 80s, and early 90s. Decision trees, heuristics, quadratic discriminant analysis, etc all came out of this era. Pattern Recognition become something CS folks did, and not EE folks. &nbsp;One of the most popular books from that time period is the <strike>infamous</strike> invaluable Duda &amp; Hart \"Pattern Classification\" book and is still a great starting point for young researchers. &nbsp;But don't get too caught up in the vocabulary, it's a bit dated.</div><div class=\"p3\"><span class=\"s1\"><br /></span></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><br /></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"http://3.bp.blogspot.com/-uxnx5mRW1k4/VR9MI0PK0KI/AAAAAAAAN6U/HfjMzLEzK4g/s1600/315.png\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"314\" src=\"https://3.bp.blogspot.com/-uxnx5mRW1k4/VR9MI0PK0KI/AAAAAAAAN6U/HfjMzLEzK4g/s1600/315.png\" width=\"320\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><br /></div><div class=\"separator\" style=\"clear: both; text-align: center;\">The character \"3\" partitioned into 16 sub-matrices. Custom rules, custom decisions, and custom \"smart\" programs used to be all the rage.&nbsp;</div><div class=\"p3\"><br /></div><div class=\"p3\"><span class=\"s1\"><br /></span><span class=\"s1\"><b>Quiz</b>:&nbsp;</span>The most popular Computer Vision conference is called CVPR and the PR stands for Pattern Recognition. &nbsp;Can you guess the year of the first CVPR conference?</div><div class=\"p2\"><span class=\"s1\"></span><br /></div><div class=\"p3\"><span class=\"s1\"><b>2. Machine Learning: Smart programs can learn from examples</b></span><br /><span class=\"s1\"><b><br /></b></span></div><div class=\"p3\"><span class=\"s1\">Sometime in the early 90s people started realizing that a more powerful way to build pattern recognition algorithms is to replace an expert (who probably knows way too much about pixels) with data (which can be mined from cheap laborers).&nbsp; So you collect a bunch of face images and non-face images, choose an algorithm, and wait for the computations to finish.&nbsp; This is the spirit of machine learning. &nbsp;\"Machine Learning\" emphasizes that the computer program (or machine) must do some work after it is given data. &nbsp;The Learning step is made explicit. &nbsp;And believe me, waiting 1 day for your computations to finish scales better than inviting your academic colleagues to your home institution to design some classification rules by hand.</span></div><div class=\"p3\"><span class=\"s1\"><br /></span></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"http://1.bp.blogspot.com/-Br9BfM-VV1s/VT3_qrSBiiI/AAAAAAAAOCs/oirhh9wS0SA/s1600/ml-eng.png\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"255\" src=\"https://1.bp.blogspot.com/-Br9BfM-VV1s/VT3_qrSBiiI/AAAAAAAAOCs/oirhh9wS0SA/s1600/ml-eng.png\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><br /></div><div class=\"separator\" style=\"clear: both; text-align: center;\">\"What is Machine Learning\" from <a href=\"http://nkonst.com/machine-learning-explained-simple-words/\">Dr Natalia Konstantinova's Blog</a>. The most important part of this diagram are the \"Gears\" which suggests that crunching/working/computing is an important step in the ML pipeline.</div><div class=\"p3\"><br /></div><div class=\"p3\"><span class=\"s1\">As Machine Learning grew into a major research topic in the mid 2000s, computer scientists began applying these ideas to a wide array of problems.&nbsp; No longer was it only character recognition, cat vs. dog recognition, and other “recognize a pattern inside an array of pixels” problems.&nbsp; Researchers started applying Machine Learning to Robotics (reinforcement learning, manipulation, motion planning, grasping), to genome data, as well as to predict financial markets.&nbsp; Machine Learning was married with Graph Theory under the brand “Graphical Models,” every robotics expert had no choice but to become a Machine Learning Expert, and <b>Machine Learning quickly became one of the most desired and versatile computing skills</b>. &nbsp;However \"Machine Learning\" says nothing about the underlying algorithm. &nbsp;We've seen convex optimization, Kernel-based methods, Support Vector Machines, as well as Boosting have their winning days. &nbsp;Together with some custom manually engineered features, we had lots of recipes, lots of different schools of thought, and it wasn't entirely clear how a newcomer should select features and algorithms. &nbsp;But that was all about to change...</span><br /><span class=\"s1\"><br /></span><span class=\"s1\">Further reading: To learn more about the kinds of features that were used in Computer Vision research see my blog post: <a href=\"http://www.computervisionblog.com/2015/01/from-feature-descriptors-to-deep.html\">From feature descriptors to deep learning: 20 years of computer vision</a>.</span></div><div class=\"p2\"><span class=\"s1\"></span><br /></div><div class=\"p3\"><span class=\"s1\"><b>3. Deep Learning: one architecture to rule them all</b></span><br /><span class=\"s1\"><b><br /></b></span></div><div class=\"p3\"><span class=\"s1\">Fast forward to today and what we’re seeing is a large interest in something called Deep Learning. The most popular kinds of Deep Learning models, as they are using in large scale image recognition tasks, are known as Convolutional Neural Nets, or simply ConvNets.&nbsp;</span></div><div class=\"p3\"><span class=\"s1\"><br /></span></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"http://4.bp.blogspot.com/-uHgA5CQk22A/VT3_7lLLXtI/AAAAAAAAOC0/YMAyuywwaQ0/s1600/convnet.png\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"138\" src=\"https://4.bp.blogspot.com/-uHgA5CQk22A/VT3_7lLLXtI/AAAAAAAAOC0/YMAyuywwaQ0/s400/convnet.png\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><br /></div><div class=\"separator\" style=\"clear: both; text-align: center;\">ConvNet diagram from <a href=\"http://torch.cogbits.com/doc/tutorials_supervised/\">Torch Tutorial</a></div><div class=\"p3\"><span class=\"s1\"><br /></span></div><div class=\"p2\"><span class=\"s1\"></span>Deep Learning emphasizes the kind of model you might want to use (e.g., a deep convolutional multi-layer neural network) and that you can use data fill in the missing parameters.&nbsp; But with deep-learning comes great responsibility.&nbsp; Because you are starting with a model of the world which has a high dimensionality, you really need a lot of data (big data) and a lot of crunching power (GPUs). Convolutions are used extensively in deep learning (especially computer vision applications), and the architectures are far from shallow.<br /><br />If you're starting out with Deep Learning, simply brush up on some elementary Linear Algebra and start coding. &nbsp;I highly recommend Andrej Karpathy's <a href=\"http://karpathy.github.io/neuralnets/\">Hacker's guide to Neural Networks</a>. Implementing your own CPU-based backpropagation algorithm on a non-convolution based problem is a good place to start.</div><div class=\"p2\"><br />There are still lots of unknowns. The theory of why deep learning works is incomplete, and no single guide or book is better than true machine learning experience. &nbsp;There are lots of reasons why Deep Learning is gaining popularity, but Deep Learning is not going to take over the world. &nbsp;As long as you continue brushing up on your machine learning skills, your job is safe. But don't be afraid to chop these networks in half, slice 'n dice at will, and build software architectures that work in tandem with your learning algorithm. &nbsp;The Linux Kernel of tomorrow might run on <a href=\"http://caffe.berkeleyvision.org/\">Caffe</a> (one of the most popular deep learning frameworks), but great products will always need great vision, domain expertise, market development, and most importantly: human creativity.</div><div class=\"p2\"><br /></div><div class=\"p3\"><span class=\"s1\"><b>Other related buzz-words</b></span></div><div class=\"p3\"><br /></div><div class=\"p3\"><span class=\"s1\"><b>Big-data</b> is the philosophy of measuring all sorts of things, saving that data, and looking through it for information.&nbsp; For business, this big-data approach can give you actionable insights.&nbsp; In the context of learning algorithms, we’ve only started seeing the marriage of big-data and machine learning within the past few years.&nbsp; <b>Cloud-computing</b>, <b>GPUs</b>, <b>DevOps</b>, and <b>PaaS</b> providers have made large scale computing within reach of the researcher and ambitious \"everyday\" developer.&nbsp;</span><br /><span class=\"s1\"><b><br /></b></span><span class=\"s1\"><b>Artificial Intelligence</b> is perhaps the oldest term, the most vague, and the one that was gone through the most ups and downs in the past 50 years. When somebody says they work on Artificial Intelligence, you are either going to want to laugh at them or take out a piece of paper and write down everything they say.</span><br /><span class=\"s1\"><br /></span><span class=\"s1\">Further reading: My 2011 Blog post <a href=\"http://www.computervisionblog.com/2011/03/computer-vision-is-artificial.html\">Computer Vision is Artificial Intelligence</a>.</span><br /><span class=\"s1\"><br /></span><span class=\"s1\"><b>Conclusion</b></span><br /><span class=\"s1\"><b><br /></b></span>Machine Learning is here to stay. Don't think about it as Pattern Recognition vs Machine Learning vs Deep Learning, just realize that each term emphasizes something a little bit different. &nbsp;But the search continues. &nbsp;Go ahead and explore. Break something. We will continue building smarter software and our algorithms will continue to learn, but we've only begun to explore the kinds of architectures that can truly rule-them-all.</div><div class=\"p2\"><br /><span class=\"s1\"></span></div><div class=\"p2\"><span class=\"s1\"></span></div><div class=\"p2\">If you're interested in real-time vision applications of deep learning, namely those suitable for robotic and home automation applications, then you should check out what we've been building at <a href=\"http://vision.ai/\">vision.ai</a>. Hopefully in a few days, I'll be able to say a little bit more. :-)<br /><br />Until next time.<br /><span class=\"s1\"></span></div><div class=\"p3\"><br /></div><div class=\"p3\"><br /></div><div class=\"p2\"><span class=\"s1\"></span>See <a href=\"https://news.ycombinator.com/item?id=9247851\">discussion about this blog post on Hacker News</a>.</div><br /><div class=\"p2\"><span class=\"s1\"></span><br /></div>",
  "link": [
    "",
    "",
    "",
    "",
    ""
  ],
  "author": {
    "name": "Tomasz Malisiewicz",
    "uri": "http://www.blogger.com/profile/17507234774392358321",
    "email": "noreply@blogger.com",
    "gd:image": ""
  },
  "media:thumbnail": "",
  "thr:total": 25
}