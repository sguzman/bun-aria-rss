{
  "title": "Monocular Visual Odometry using OpenCV",
  "description": "<script type=\"text/javascript\" src=\"//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"></script>\n\n<p>Last month, I made a <a href=\"/vision/visual-odometry-full/\">post</a> on Stereo Visual Odometry and its implementation in MATLAB. \nThis post would be focussing on <strong>Monocular Visual Odometry</strong>, and how we can implement it in <strong>OpenCV/C++</strong>.\nThe implementation that I describe in this post is once again freely available on <a href=\"https://github.com/avisingh599/mono-vo\">github</a>.\nIt is also simpler to understand, and runs at 5fps, which is much faster than my older stereo implementation.</p>\n\n<p>If you are new to Visual Odometry, I suggest having a look at the first few paragraphs (before all the math starts) of my \n<a href=\"/vision/visual-odometry-full/\">old post</a>. It talks about what Visual Odometry is, why we \nneed it, and also compares the monocular and stereo approaches.</p>\n\n<p>Acquanted with all the basics of visual odometry? Cool. Let’s go ahead.</p>\n\n<h2 id=\"demo\">Demo</h2>\n<p>Before I move onto describing the implementation, have a look at the algorithm in action!</p>\n\n<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>\n<div class=\"embed-container\"><iframe src=\"https://www.youtube.com/embed/homos4vd_Zs\" frameborder=\"0\" allowfullscreen=\"\"></iframe></div>\n\n<p>Pretty cool, eh? Let’s dive into implementing it in OpenCV now.</p>\n\n<h3 id=\"formulation-of-the-problem\">Formulation of the problem</h3>\n\n<h4 id=\"input\">Input</h4>\n<p>We have a stream of gray scale images coming from a camera. Let the frames, captured at time \\(t\\) and \\(t+1\\) be referred to as\n\\(\\mathit{I}^{t}\\), \\(\\mathit{I}^{t+1}\\). We have prior knowledge of all the intrinsic parameters, obtained via calibration, \nwhich can also be done in <a href=\"http://docs.opencv.org/3.0.0/d9/d0c/group__calib3d.html\">OpenCV</a>.</p>\n\n<h4 id=\"output\">Output</h4>\n<p>For every pair of images, we need to find the rotation matrix \\(R\\) and the translation vector \\(t\\), which describes the motion of the vehicle between the two frames. The vector \\(t\\) can only be computed upto a scale factor in our monocular scheme.</p>\n\n<h3 id=\"algorithm-outline\">Algorithm Outline</h3>\n\n<ol>\n  <li>Capture images: \\(\\mathit{I}^t\\), \\(\\mathit{I}^{t+1}\\),</li>\n  <li>Undistort the above images.</li>\n  <li>Use FAST algorithm to detect features in  \\(\\mathit{I}^t\\), and track those features to \\({I}^{t+1}\\). A new detection is triggered if the number of features drop below a certain threshold.</li>\n  <li>Use Nister’s 5-point alogirthm with RANSAC to compute the essential matrix.</li>\n  <li>Estimate \\(R, t\\) from the essential matrix that was computed in the previous step.</li>\n  <li>Take scale information from some external source (like a speedometer), and concatenate the translation vectors, and rotation matrices.</li>\n</ol>\n\n<p>You may or may not understand all the steps that have been metioned above, but don’t worry. All the points\nabove will be explained in great detail in the text to follow.</p>\n\n<h3 id=\"undistortion\">Undistortion</h3>\n\n<p>Distortion happens when lines that are straight in the real world become curved in the images. T\nhis step compensates for this lens distortion. It is performed with the help of the distortion parameters \nthat were obtained during calibration. Since the KITTI dataset that I’m using already comes with \nundistorted images, I won’t write the code about it here. However, it is relatively straightforward to \n<a href=\"http://docs.opencv.org/modules/imgproc/doc/geometric_transformations.html#undistort\">undistort</a> with OpenCV.</p>\n\n<h3 id=\"feature-detection\">Feature Detection</h3>\n<p>My approach uses the FAST corner detector, just like my stereo implementation. I’ll now explain in brief how the detector works, though you must have a look at the <a href=\"http://www.edwardrosten.com/work/fast.html\">original paper and source code</a> if you want to really understand how it works. Suppose there is a point \\(\\mathbf{P}\\) which we want to test if it is a corner or not. We draw a circle of 16px circumference around this point as shown in figure below. For every pixel which lies on the circumference of this circle, we see if there exits a continuous set of pixels whose intensity exceed the intensity of the original pixel by a certain factor \\(\\mathbf{I}\\) and for another set of contiguous pixels if the intensity is less by at least the same factor \\(\\mathbf{I}\\). If yes, then we mark this point as a corner. A heuristic for rejecting the vast majority of non-corners is used, in which the pixel at 1,9,5,13 are examined first, and atleast three of them must have a higher intensity be amount at least \\(\\mathbf{I}\\), or must have an intensity lower by the same amount \\(\\mathbf{I}\\) for the point to be a corner. This particular approach is selected due to its computational efficiency as compared to other popular interest point detectors such as SIFT.</p>\n\n<figure>\n  <img src=\"/images/visodo/fast.png\" />\n  <figcaption>Image from the original FAST feature detection paper</figcaption>\n</figure>\n\n<p>Using OpenCV, detecting features is trivial, and here is the code that does it.</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-c--\" data-lang=\"c++\"><span class=\"kt\">void</span> <span class=\"nf\">featureDetection</span><span class=\"p\">(</span><span class=\"n\">Mat</span> <span class=\"n\">img_1</span><span class=\"p\">,</span> <span class=\"n\">vector</span><span class=\"o\">&lt;</span><span class=\"n\">Point2f</span><span class=\"o\">&gt;&amp;</span> <span class=\"n\">points1</span><span class=\"p\">)</span>\t<span class=\"p\">{</span> \n  <span class=\"n\">vector</span><span class=\"o\">&lt;</span><span class=\"n\">KeyPoint</span><span class=\"o\">&gt;</span> <span class=\"n\">keypoints_1</span><span class=\"p\">;</span>\n  <span class=\"kt\">int</span> <span class=\"n\">fast_threshold</span> <span class=\"o\">=</span> <span class=\"mi\">20</span><span class=\"p\">;</span>\n  <span class=\"kt\">bool</span> <span class=\"n\">nonmaxSuppression</span> <span class=\"o\">=</span> <span class=\"nb\">true</span><span class=\"p\">;</span>\n  <span class=\"n\">FAST</span><span class=\"p\">(</span><span class=\"n\">img_1</span><span class=\"p\">,</span> <span class=\"n\">keypoints_1</span><span class=\"p\">,</span> <span class=\"n\">fast_threshold</span><span class=\"p\">,</span> <span class=\"n\">nonmaxSuppression</span><span class=\"p\">);</span>\n  <span class=\"n\">KeyPoint</span><span class=\"o\">::</span><span class=\"n\">convert</span><span class=\"p\">(</span><span class=\"n\">keypoints_1</span><span class=\"p\">,</span> <span class=\"n\">points1</span><span class=\"p\">,</span> <span class=\"n\">vector</span><span class=\"o\">&lt;</span><span class=\"kt\">int</span><span class=\"o\">&gt;</span><span class=\"p\">());</span>\n<span class=\"p\">}</span></code></pre></figure>\n\n<p>The parameters in the code above are set such that it gives ~4000 features on one image from the KITTI dataset. You may want\ntune these parameters so as to obtain the best performance on your own data.\nNote that the code above also converts the datatype of the detected feature points from KeyPoints to a vector of Point2f, so \nthat we can directly pass it to the feature tracking step, described below:</p>\n\n<h3 id=\"feature-tracking\">Feature Tracking</h3>\n\n<p>The fast corners detected in the previous step are fed to the next step, which uses a <a href=\"https://www.ces.clemson.edu/~stb/klt/\">KLT tracker</a>. The KLT tracker basically looks around every corner to be tracked, and uses this local information to find the corner in the next image. You are welcome to look into the KLT link to know more. The corners detected in \\(\\mathit{I}^{t}\\) are tracked in \\(\\mathit{I}^{t+1}\\). Let the set of features detected in \\(\\mathit{I}^{t}\\) be \\(\\mathcal{F}^{t}\\) , and the set of corresponding features in \\(\\mathit{I}^{t+1}\\) be \\(\\mathcal{F}^{t+1}\\). Here is the function that does feature tracking in OpenCV using the KLT tracker:</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-c--\" data-lang=\"c++\"><span class=\"kt\">void</span> <span class=\"nf\">featureTracking</span><span class=\"p\">(</span><span class=\"n\">Mat</span> <span class=\"n\">img_1</span><span class=\"p\">,</span> <span class=\"n\">Mat</span> <span class=\"n\">img_2</span><span class=\"p\">,</span> <span class=\"n\">vector</span><span class=\"o\">&lt;</span><span class=\"n\">Point2f</span><span class=\"o\">&gt;&amp;</span> <span class=\"n\">points1</span><span class=\"p\">,</span> <span class=\"n\">vector</span><span class=\"o\">&lt;</span><span class=\"n\">Point2f</span><span class=\"o\">&gt;&amp;</span> <span class=\"n\">points2</span><span class=\"p\">,</span> <span class=\"n\">vector</span><span class=\"o\">&lt;</span><span class=\"n\">uchar</span><span class=\"o\">&gt;&amp;</span> <span class=\"n\">status</span><span class=\"p\">)</span>\t<span class=\"p\">{</span> \n\n<span class=\"c1\">//this function automatically gets rid of points for which tracking fails</span>\n\n  <span class=\"n\">vector</span><span class=\"o\">&lt;</span><span class=\"kt\">float</span><span class=\"o\">&gt;</span> <span class=\"n\">err</span><span class=\"p\">;</span>\t\t\t\t\t\n  <span class=\"n\">Size</span> <span class=\"n\">winSize</span><span class=\"o\">=</span><span class=\"n\">Size</span><span class=\"p\">(</span><span class=\"mi\">21</span><span class=\"p\">,</span><span class=\"mi\">21</span><span class=\"p\">);</span>\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n  <span class=\"n\">TermCriteria</span> <span class=\"n\">termcrit</span><span class=\"o\">=</span><span class=\"n\">TermCriteria</span><span class=\"p\">(</span><span class=\"n\">TermCriteria</span><span class=\"o\">::</span><span class=\"n\">COUNT</span><span class=\"o\">+</span><span class=\"n\">TermCriteria</span><span class=\"o\">::</span><span class=\"n\">EPS</span><span class=\"p\">,</span> <span class=\"mi\">30</span><span class=\"p\">,</span> <span class=\"mf\">0.01</span><span class=\"p\">);</span>\n\n  <span class=\"n\">calcOpticalFlowPyrLK</span><span class=\"p\">(</span><span class=\"n\">img_1</span><span class=\"p\">,</span> <span class=\"n\">img_2</span><span class=\"p\">,</span> <span class=\"n\">points1</span><span class=\"p\">,</span> <span class=\"n\">points2</span><span class=\"p\">,</span> <span class=\"n\">status</span><span class=\"p\">,</span> <span class=\"n\">err</span><span class=\"p\">,</span> <span class=\"n\">winSize</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">termcrit</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mf\">0.001</span><span class=\"p\">);</span>\n\n  <span class=\"c1\">//getting rid of points for which the KLT tracking failed or those who have gone outside the frame</span>\n  <span class=\"kt\">int</span> <span class=\"n\">indexCorrection</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span>\n  <span class=\"k\">for</span><span class=\"p\">(</span> <span class=\"kt\">int</span> <span class=\"n\">i</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">;</span> <span class=\"n\">i</span><span class=\"o\">&lt;</span><span class=\"n\">status</span><span class=\"p\">.</span><span class=\"n\">size</span><span class=\"p\">();</span> <span class=\"n\">i</span><span class=\"o\">++</span><span class=\"p\">)</span>\n     <span class=\"p\">{</span>  <span class=\"n\">Point2f</span> <span class=\"n\">pt</span> <span class=\"o\">=</span> <span class=\"n\">points2</span><span class=\"p\">.</span><span class=\"n\">at</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"o\">-</span> <span class=\"n\">indexCorrection</span><span class=\"p\">);</span>\n     \t<span class=\"k\">if</span> <span class=\"p\">((</span><span class=\"n\">status</span><span class=\"p\">.</span><span class=\"n\">at</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">)</span><span class=\"o\">||</span><span class=\"p\">(</span><span class=\"n\">pt</span><span class=\"p\">.</span><span class=\"n\">x</span><span class=\"o\">&lt;</span><span class=\"mi\">0</span><span class=\"p\">)</span><span class=\"o\">||</span><span class=\"p\">(</span><span class=\"n\">pt</span><span class=\"p\">.</span><span class=\"n\">y</span><span class=\"o\">&lt;</span><span class=\"mi\">0</span><span class=\"p\">))</span>\t<span class=\"p\">{</span>\n     \t\t  <span class=\"k\">if</span><span class=\"p\">((</span><span class=\"n\">pt</span><span class=\"p\">.</span><span class=\"n\">x</span><span class=\"o\">&lt;</span><span class=\"mi\">0</span><span class=\"p\">)</span><span class=\"o\">||</span><span class=\"p\">(</span><span class=\"n\">pt</span><span class=\"p\">.</span><span class=\"n\">y</span><span class=\"o\">&lt;</span><span class=\"mi\">0</span><span class=\"p\">))</span>\t<span class=\"p\">{</span>\n     \t\t  \t<span class=\"n\">status</span><span class=\"p\">.</span><span class=\"n\">at</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span>\n     \t\t  <span class=\"p\">}</span>\n     \t\t  <span class=\"n\">points1</span><span class=\"p\">.</span><span class=\"n\">erase</span> <span class=\"p\">(</span><span class=\"n\">points1</span><span class=\"p\">.</span><span class=\"n\">begin</span><span class=\"p\">()</span> <span class=\"o\">+</span> <span class=\"n\">i</span> <span class=\"o\">-</span> <span class=\"n\">indexCorrection</span><span class=\"p\">);</span>\n     \t\t  <span class=\"n\">points2</span><span class=\"p\">.</span><span class=\"n\">erase</span> <span class=\"p\">(</span><span class=\"n\">points2</span><span class=\"p\">.</span><span class=\"n\">begin</span><span class=\"p\">()</span> <span class=\"o\">+</span> <span class=\"n\">i</span> <span class=\"o\">-</span> <span class=\"n\">indexCorrection</span><span class=\"p\">);</span>\n     \t\t  <span class=\"n\">indexCorrection</span><span class=\"o\">++</span><span class=\"p\">;</span>\n     \t<span class=\"p\">}</span>\n\n     <span class=\"p\">}</span>\n\n<span class=\"p\">}</span></code></pre></figure>\n\n<h4 id=\"feature-re-detection\">Feature Re-Detection</h4>\n<p>Note that while doing KLT tracking, we will eventually lose some points (as they move out of the field of view of the car), and \nwe thus trigger a redetection whenver the total number of features go below a certain threshold (2000 in my implementation).</p>\n\n<h3 id=\"essential-matrix-estimation\">Essential Matrix Estimation</h3>\n<p>Once we have point-correspondences, we have several techniques for the computation of an essential matrix. The essential matrix is defined as follows:\n\\(\\begin{equation}\ny_{1}^{T}Ey_{2} = 0\n\\end{equation}\\)\nHere, \\(y_{1}\\), \\(y_{2}\\) are homogenous normalised image coordinates. \nWhile a simple algorithm requiring eight point correspondences exists\\cite{Higgins81}, a more recent approach that is shown to give better results is the five point algorithm<sup id=\"fnref:1\" role=\"doc-noteref\"><a href=\"#fn:1\" class=\"footnote\" rel=\"footnote\">1</a></sup>. It solves a number of non-linear equations, and requires the minimum number of points possible, since the Essential Matrix has only five degrees of freedom.</p>\n\n<h4 id=\"ransac\">RANSAC</h4>\n<p>If all of our point correspondences were perfect, then we would have need only \nfive feature correspondences between two successive frames to estimate motion accurately. \nHowever, the feature tracking algorithms are not perfect, and therefore we have several \nerroneous correspondence. A standard technique of handling outliers when doing model estimation\nis RANSAC. It is an iterative algorithm. At every iteration, it randomly samples five \npoints from out set of correspondences, estimates the Essential Matrix, and then checks\nif the other points are inliers when using this essential matrix. The algorithm terminates\nafter a fixed number of iterations, and the Essential matrix with which the maximum number of points agree, is used.</p>\n\n<p>Using the above in OpenCV is again pretty straightforward, and all you need is one line:</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-c--\" data-lang=\"c++\"><span class=\"n\">E</span> <span class=\"o\">=</span> <span class=\"n\">findEssentialMat</span><span class=\"p\">(</span><span class=\"n\">points2</span><span class=\"p\">,</span> <span class=\"n\">points1</span><span class=\"p\">,</span> <span class=\"n\">focal</span><span class=\"p\">,</span> <span class=\"n\">pp</span><span class=\"p\">,</span> <span class=\"n\">RANSAC</span><span class=\"p\">,</span> <span class=\"mf\">0.999</span><span class=\"p\">,</span> <span class=\"mf\">1.0</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"p\">);</span></code></pre></figure>\n\n<h3 id=\"computing-r-t-from-the-essential-matrix\">Computing R, t from the Essential Matrix</h3>\n<p>Another definition of the Essential Matrix (consistent) with the definition mentioned earlier is as follows:\n\\(\\begin{equation}\nE = R[t]_{x}\n\\end{equation}\\)\nHere, \\(R\\) is the rotation matrix, while \\([t]_{x}\\) is  the matrix representation of a cross product with \\(t\\). Taking the SVD of the essential matrix, and then exploiting the constraints on the rotation matrix, we get the following:</p>\n\n\\[E = U\\Sigma V^{T}\\]\n\n\\[[t]_{x} = VW\\Sigma V^{T}\\]\n\n\\[R = UW^{-1}V^{T}\\]\n\n<p>Here’s the one-liner that implements it in OpenCV:</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-c--\" data-lang=\"c++\"><span class=\"n\">recoverPose</span><span class=\"p\">(</span><span class=\"n\">E</span><span class=\"p\">,</span> <span class=\"n\">points2</span><span class=\"p\">,</span> <span class=\"n\">points1</span><span class=\"p\">,</span> <span class=\"n\">R</span><span class=\"p\">,</span> <span class=\"n\">t</span><span class=\"p\">,</span> <span class=\"n\">focal</span><span class=\"p\">,</span> <span class=\"n\">pp</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"p\">);</span></code></pre></figure>\n\n<h3 id=\"constructing-trajectory\">Constructing Trajectory</h3>\n<p>Let the pose of the camera be denoted by \\(R_{pos}\\), \\(t_{pos}\\). We can then track the trajectory using the following equation:</p>\n\n\\[R_{pos} = R R_{pos}\\]\n\n\\[t_{pos} = t_{pos} + t R_{pos}\\]\n\n<p>Note that the scale information of the translation vector \\(t\\) has to be obtained from some other source before concatenating.\nIn my implementation, I extract this information from the ground truth that is supplied by the KITTI dataset.</p>\n\n<h3 id=\"heuristics\">Heuristics</h3>\n<p>Most Computer Vision algorithms are not complete without a few heuristics thrown in, and Visual Odometry is not an exception. The\nheuristive that we use is explained below:</p>\n\n<h4 id=\"dominant-motion-is-forward\">Dominant Motion is Forward</h4>\n<p>The entire visual odometry algorithm makes the assumption that most of the points in its environment are rigid. However, if we are in a scenario where the vehicle is at a stand still, and a buss passes by (on a road intersection, for example), it would lead the algorithm to believe that the car has moved sideways, which is physically impossible. As a result, if we ever find the translation is dominant in a direction other than forward, we simply ignore that motion.</p>\n\n<h2 id=\"results\">Results</h2>\n<p>So, how good is the performance of the algorithm on the KITTI dataset? See for yourself.</p>\n\n<figure>\n  <img src=\"/images/visodo/2K.png\" />\n  <figcaption> Computed Trajectory vs Ground Truth for 2000 frames</figcaption>\n</figure>\n\n<h2 id=\"what-next\">What next?</h2>\n<p>A major limitation of my implementation is that it cannot evaluate relative scale. I did try implementing some methods, but I \nencountered the problem which is known as “scale drift” i.e. small errors accumulate, leading to bad odometry estimates.\nI hope I’ll soon implement a more robust relative scale computation pipeline, and write a post about it!</p>\n\n<hr />\n\n<div class=\"footnotes\" role=\"doc-endnotes\">\n  <ol>\n    <li id=\"fn:1\" role=\"doc-endnote\">\n      <p>David Nister An efficient solution to the five-point relative pose problem (2004) <a href=\"#fnref:1\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>\n    </li>\n  </ol>\n</div>",
  "pubDate": "Mon, 08 Jun 2015 00:00:00 +0000",
  "link": "https://avisingh599.github.io/vision/monocular-vo/",
  "guid": "https://avisingh599.github.io/vision/monocular-vo/"
}