{
  "title": "Summer Student Projects 2018",
  "link": "",
  "updated": "2018-03-20T00:00:00+00:00",
  "id": "https://mrocklin.github.io/blog/work/2018/03/20/summer-projects",
  "content": "<p>Around this time of year students look for Summer projects.\nOften they get internships at potential future employers.\nSometimes they become more engaged in open source software.</p>\n\n<p>This blogpost contains some projects that I think are appropriate for a summer\nstudent in a computational field.  They reflect my biases (which, assuming you\nread my blog, you’re ok with) and are by no means comprehensive of\nopportunities within the Scientific Python ecosystem.  To be perfectly clear\nI’m only providing ideas and context here,\nI offer neither funding nor mentorship.</p>\n\n<h3 id=\"criteria-for-a-good-project\">Criteria for a good project</h3>\n\n<ol>\n  <li>Is well defined and tightly scoped\nto reduce uncertainty about what a successful outcome looks like,\nand to reduce the necessity for high-level advising</li>\n  <li>Is calibrated so that an industrious student can complete it in a few months</li>\n  <li>It’s useful, but also peripheral.\nIt has value to the ecosystem but is not critical enough\nthat a core devs is likely to complete the task in the next few months,\nor be overly picky about the implementation.</li>\n  <li>It’s interesting, and is likely to stimulate thought within the student</li>\n  <li>It teaches valuable skills that will help the student in a future job search</li>\n  <li>It can lead to future work, if the student makes a strong connection</li>\n</ol>\n\n<p>The projects listed here target someone who already has decent knowledge of the\nfundamentals PyData or SciPy ecosystem (numpy, pandas, general understanding of\nprogramming, etc..).\nThey are somewhat focused around Dask and other projects that I personally work on.</p>\n\n<h3 id=\"distributed-gpu-ndarrays-with-cupy-bohrium-or-other\">Distributed GPU NDArrays with CuPy, Bohrium, or other</h3>\n\n<p>Dask arrays coordinate many NumPy arrays to operate in parallel.\nIt includes all of the parallel algorithms,\nleaving the in-memory implementation to NumPy chunks.</p>\n\n<p>But the chunk arrays don’t actually have to be NumPy arrays,\nthey just have to look similar enough to NumPy arrays to fool Dask Array.\nWe’ve done this before with <a href=\"http://dask.pydata.org/en/latest/array-sparse.html\">sparse arrays</a>\nwhich implement a subset of the <code class=\"language-plaintext highlighter-rouge\">numpy.ndarray</code> API, but with sparse storage,\nand it has worked nicely.</p>\n\n<p>There are a few GPU NDArray projects out there that satisfy much of the NumPy interface:</p>\n\n<ul>\n  <li><a href=\"https://cupy.chainer.org/\">CuPy</a></li>\n  <li><a href=\"http://bohrium.readthedocs.io/\">Bohrium</a></li>\n  <li>…</li>\n</ul>\n\n<p>It would be valuable to do the same thing with Dask Array with them.\nThis might give us a decent general purpose distributed GPU array relatively cheaply.\nThis would engage the following:</p>\n\n<ol>\n  <li>Knowledge of GPUs and performance implications of using them</li>\n  <li>NumPy protocols (assuming that the GPU library will still need some changes to make it fully compatible)</li>\n  <li>Distributed performance, focusing on bandwidths between various parts of the architecture</li>\n  <li>Profiling and benchmarking</li>\n</ol>\n\n<p>Github issue for conversation is here: <a href=\"https://github.com/dask/dask/issues/3007\">dask/dask #3007</a></p>\n\n<h3 id=\"use-numba-and-dask-for-numerical-simulations\">Use Numba and Dask for Numerical Simulations</h3>\n\n<p>While Python is very popular in data analytics\nit has been less successful in hard-core numeric algorithms and simulation,\nwhich are typically done in C++/Fortran and MPI.\nThis is because Python is perceived to be too slow for serious numerical computing.</p>\n\n<p>Yet with recent advances in Numba for fast in-core computing and Dask for parallel computing things may be changing.\nCertainly fine-tuned C++/Fortran + MPI can out-perform Numba and Dask,\nbut by how much?\nIf the answer is only 10% or so then it could be that the lower barrier to entry of Numba,\nor the dynamic scaling of Dask,\ncan make them competitive in fields where Python has not previously had a major impact.</p>\n\n<p>For which kinds of problems is a dynamic JITted language almost-as-good as C++/MPI?\nFor which kinds of problems is the dynamic nature of these tools valuable,\neither due to more rapid development,\ngreater flexibility in accepting community created modules,\ndynamic load balancing,\nor other reasons?</p>\n\n<p>This project would require the student to come in with an understanding of their own field,\nthe kinds of computational problems that are relevant there,\nand an understanding of the performance characteristics\nthat might make dynamic systems tolerable.\nThey would learn about optimization and profiling,\nand would characterize the relevant costs of dynamic languages in a slightly more modern era.</p>\n\n<h3 id=\"blocked-numerical-linear-algebra\">Blocked Numerical Linear Algebra</h3>\n\n<p>Dask arrays contain some algorithms for blocked linear algebra,\nlike least squares, QR, LU, Cholesky, etc..,\nbut no particular attention has been paid to them.</p>\n\n<p>It would be interesting to investigate the performance of these algorithms\nand compare them to proper distributed BLAS/LAPACK implementations.\nThis will very likely lead to opportunities to improve the algorithms\nand possibly some of Dask’s internal machinery.</p>\n\n<h3 id=\"dask-r-and-dask-julia\">Dask-R and Dask-Julia</h3>\n\n<p>Someone with understanding of R’s or Julia’s networking stack\ncould adapt Dask’s distributed scheduler for those languages.\nRecall that the dask.distributed network consists of a central scheduler,\nmany distributed workers, one or more user-facing clients.\nCurrently these are all written in Python and only really useful from that language.</p>\n\n<p>Making this system useful in another language would require rewriting the client and worker code,\nbut would not require rewriting the scheduler code, which is intentionally language agnostic.\nFortunately the client and worker are both relatively simple codebases (relative to the scheduler at least)\nand minimal implementations could probably be written in around 1-2k lines each.</p>\n\n<p>This would not provide the high-level collections like dask.array or dask.dataframe,\nbut would provide all of the distributed networking, load balancing, resilience, etc..\nthat is necessary to build a distributed computing stack.\nIt would also allow others to come later and build the high level collections that\nwould be appropriate for that language\n(presumably R and Julia user communities don’t want exactly Pandas-style dataframe semantics anyway).</p>\n\n<p>This is discussed further in <a href=\"https://github.com/dask/distributed/issues/586\">dask/distributed #586</a>\nand has actually been partially implemented in Julia in the <a href=\"https://github.com/invenia/DaskDistributedDispatcher.jl\">Invenia project</a>.</p>\n\n<p>This would require some knowledge of network programming and,\nideally, async programming in either R or Julia.</p>\n\n<h3 id=\"high-level-numpy-optimizations\">High-Level NumPy Optimizations</h3>\n\n<p>Projects like Numpy and Dask array compute what the user says,\neven if a more efficient solution exists.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">(</span><span class=\"n\">x</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">)[:</span><span class=\"mi\">5</span><span class=\"p\">]</span>  <span class=\"c1\"># what user said\n</span></code></pre></div></div>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">x</span><span class=\"p\">[:</span><span class=\"mi\">5</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"mi\">1</span>    <span class=\"c1\"># faster and equivalent solution\n</span></code></pre></div></div>\n\n<p>It would be useful to have a project that exactly copies the Numpy API,\nbut constructs a symbolic representation of that computation instead of performs work.\nThis would enable a few important use cases that we’ve seen arise recently.\nThese include both applications from just analyzing the symbolic representation\nand also applications from changing it to a more optimal form:</p>\n\n<ol>\n  <li>You could analyze this representation and warn users\nabout intermediate stages that require a lot of RAM or compute time</li>\n  <li>You could suggest ideal chunking patterns based on the full computation</li>\n  <li>You could communicate this computation over the network to a remote server to perform the computation</li>\n  <li>You could visualize the computation to help users or students understand what they’re computing</li>\n  <li>You could manipulate the representation into more efficient forms (such as what is shown above)</li>\n</ol>\n\n<p>The first part of this would be to construct a class that behaves like a Numpy array\nbut constructs a symbolic tree representation instead.\nThis would be similar to Sympy, Theano, Tensorflow, Blaze.expr or similar projects,\nbut it would have much smaller scope and would not be at all creative in\ndesigning new APIs.  I suspect that you could bootstrap this project quickly\nusing systems like dask.array, which already do all of the shape and dtype computations for you.\nThis is also a good opportunity to connect to recent and ongoing work in Numpy\nto establish protocols that allow other array libraries (like this one) to\nwork smoothly with existing Numpy code.</p>\n\n<p>Then you would start to build some of the analyses listed above on top of this representation.\nSome of these are harder than others to do robustly,\nbut presumably they would get easier in time.</p>"
}