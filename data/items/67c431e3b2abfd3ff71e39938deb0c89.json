{
  "title": "Parts But No Car",
  "link": "",
  "updated": "2014-10-02T10:45:00+02:00",
  "published": "2014-10-02T10:45:00+02:00",
  "author": {
    "name": "Mikio L. Braun",
    "uri": "http://mikiobraun.de",
    "email": "mikiobraun@gmail.com"
  },
  "id": "http://blog.mikiobraun.de/2014/10/parts-bug-no-car-big-data-infrastructure",
  "content": "<p><p>One question which pops up again and again when I talk about streamdrill is\nwhether that cannot be done by X, where X is one of Hadoop, Spark, Go, or some\nother piece of Big Data infrastructure.</p>\n\n<p>Of course, the reason why I find it hard to respond that question is that the\nengineer in my is tempted to say “in principle, yes” which sort of questions\nwhy I put all that work to rebuild something which apparently already exists.\nBut the truth is that there’s a huge gap between “in principle” and “in\nreality”, and I’d like to spell this difference out in this post.</p>\n\n<p>The bottom line is that all those pieces of Big Data infrastructure which\nexists today provide you with a lot of pretty impressive functionality,\ndistributed storage, scalable computing, resilience, and so on, but not in a\nway which solves your data analysis problems out of the box. The analogy I\nlike is that Big Data is a lot like providing you with an engine, a\ntransmission, some tires, a gearbox, and so on, but no car.</p>\n\n<p>So let us consider an example where you have some clickstream and you want to\nextract some information about your users. Think, for example, recommendation,\nor churn prediction. So what steps are actually involved in putting together\nsuch a system?</p>\n\n<p>First comes the hardware, either on the cloud or by buying or finding some\nspare machines, and then setting up the basic infrastructure. Nowadays, this\nwould mean installing Linux, HDFS, the distributed filesystem of Hadoop, and\nYARN, the resource manager which allows you to run different kind of compute\njobs on the cluster. Especially when you go for the raw Open Source version of\nHadoop, this step requires a lot of manual configuration, and unless you\nalready did this a few times, this might take a while to get to work.</p>\n\n<p>Then, you need to take in the data in some way, for example, by something\nlike <a href=\"http://kafka.apache.org/\">Apache Kafka</a>, which is essentially a mixture of a distributed log\nstorage and an event transport plattform.</p>\n\n<p>Next, you need to process the data, which could either be done by a system\nlike <a href=\"http://storm.incubator.apache.org/\">Apache Storm</a>, a stream processing framework which lets you distribute\ncomputing once you have it broken down to pieces of computation taking in\nan event at a time. Or you use <a href=\"http://spark.apache.org/\">Apache Spark</a> which let’s you describe\ncomputation on a higher level with something like a functional collection API\nand can also be fed a stream of data.</p>\n\n<p>Unfortunately, this still does nothing useful out of the box. Both Storm and\nSpark are just frameworks for distributed computing, meaning that they allow\nyou to scale computation, but you need to tell them what you want to compute.\nSo you first need to figure out what to do with your data and this involves\nlooking at data, identifying the kind of statistical analysis which is suited\nto solve your problem, and so on, and probably requires a skilled data\nscientist to spend one to two month working on the data. There are projects\nlike <a href=\"http://spark.apache.org/mllib\">mllib</a> which provide more advanced analytics, but again these projects\ndon’t provide full solutions to application problems but are tools for a data\nscientist to work with (And they are still somewhat early stage IMHO.)</p>\n\n<p>Still, there’s more work to do. One thing people are often unaware of is that\nStorm and Spark have no <a href=\"/2013/03/stream-processing-has-no-\nquery-layer.html\">storage layer</a>. This means that they both perform computation, but to get\nto the result of the computation, you have to store it somewhere and have some\nmeans to query it. This means usually to store the result in a database,\nsomething like <a href=\"http://redis.io\">redis</a>, if you want the speed of a memory based data storage, or\nin some other way.</p>\n\n<p>So by now we have taken care of how to get the data in, what to do with it and\nhow, and how to store the result such that we can query it while the\ncomputation is going on. Conservatively talking, we’re already down six man\nmonths, probably less if you have done it before and/or are lucky. Finally,\nyou also need to have some way to visualize the results, or if your main\naccess is via an API, to monitor what the system is doing. For this, more\ncoding is required, to create a web backend with graphs written in  <a href=\"http://d3js.org\">d3.js</a> in\nJavaScript.</p>\n\n<p>The resulting system probably looks a bit like this.</p>\n\n<div class=\"figure\">\n\t<img src=\"/images/big-data-parts-fig.png\" />\n</div>\n\n<p>Lots of moving parts which need to be deployed and maintained. Contrast this\nwith an integrated solution. To me this is difference between a bunch of parts\nand a car.</p>\n\n</p>\n   <p><a href=\"http://blog.mikiobraun.de/2014/10/parts-bug-no-car-big-data-infrastructure.html\">Click here for the full article</a>"
}