{
  "title": "Pretty fast word2vec with Numba",
  "link": "",
  "published": "2016-05-03T00:00:00+00:00",
  "updated": "2016-05-03T00:00:00+00:00",
  "id": "https://wcbeard.github.io/blog/2016/05/03/word2vec",
  "content": "<script src=\"//ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js\"></script>\n\n<link href=\"https://fonts.googleapis.com/css?family=Source+Sans+Pro\" rel=\"stylesheet\" type=\"text/css\" />\n\n<script type=\"text/x-mathjax-config\">\n  MathJax.Hub.Config({\n    tex2jax: {inlineMath: [[\"$\",\"$\"],[\"\\\\(\",\"\\\\)\"]]}\n  });\n</script>\n\n<!-- <script type=\"text/javascript\" src=\"/MathJax/MathJax.js?config=TeX-AMS_HTML-full\"></script> -->\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML-full\"> </script>\n\n<style type=\"text/css\">\n  .input_hidden {\n    display: none;\n    //  margin-top: 5px;\n  }\n  .container {\n    /*width:140%; //!important;*/\n    width: none; //!important;\n    border: none !important;\n  }\n  div.prompt {\n    display: none;\n  }\n  .CodeMirror{\n    /*font-family: \"Consolas\", sans-serif;*/\n    font-size:18px;\n    line-height: 30px;\n  }\n  pre, kbd, samp {\n    /*font-family: 'Source Sans Pro', Helvetica, Arial, sans-serif;*/\n    /*line-height: 30px;*/\n    font-size:18px;\n  }\n  pre.code {\n    font-family: Consolas, monospace;\n    font-size: 12px;\n  }*/\n  /*p {\n    font-size:20px;\n    font-family: 'Source Sans Pro', Helvetica, Arial, sans-serif;\n    line-height: 30px;\n    text-align: left;\n  }*/\n  div.cell{\n    max-width:100%;\n    margin-left:auto;\n    margin-right:auto;\n  }\n  div.text_cell_render{\n    max-width:100%;\n    margin-left:auto;\n    margin-right:auto;\n  }\n  h2,h3,h4{\n    text-align: left;\n  }\n  </style>\n\n<script>\n  $(document).ready(function(){\n    $(\".output_wrapper\").click(function(){\n      $(this).prev('.input_hidden').slideToggle();\n    });\n  })\n  </script>\n\n<div class=\"cell border-box-sizing text_cell rendered\">\n<div class=\"prompt input_prompt\">\n</div>\n<div class=\"inner_cell\">\n<div class=\"text_cell_render border-box-sizing rendered_html\">\n<p>A difficulty of working with text data is that using each word of a large vocabulary as a feature requires working with large dimensions. While sparse matrix data structures make manipulating these vectors tractable, there are whole classes of algorithms that do not work well or at all on sparse high dimensional data.</p>\n<p>A somewhat recent development called word2vec allows words to be represented as dense vectors of much smaller dimensions (on the order of $\\mathbb{R}^{100}$). This algorithm yields representations of words such that words appearing in similar contexts will lie close to one another in this low dimensional vector space. Another interesting feature of this algorithm is that the representation does a good job at inferring analogies. <a href=\"http://rare-technologies.com/word2vec-tutorial/#app\">This gensim demo</a> of the algorithm shows word2vec inferring that <em>dogs</em> is to <em>puppies</em> as <em>cats</em> is to <em>kittens</em>:</p>\n\n</div>\n</div>\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[1]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"kn\">from</span> <span class=\"nn\">IPython.display</span> <span class=\"k\">import</span> <span class=\"n\">Image</span>\n<span class=\"n\">Image</span><span class=\"p\">(</span><span class=\"n\">filename</span><span class=\"o\">=</span><span class=\"s\">&#39;kittens.png&#39;</span><span class=\"p\">)</span>\n</pre></div>\n\n</div>\n</div>\n</div>\n\n<div class=\"output_wrapper\">\n<div class=\"output\">\n\n\n<div class=\"output_area\"><div class=\"prompt output_prompt\">Out[1]:</div>\n\n\n<div class=\"output_png output_subarea output_execute_result\">\n<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAcQAAACKCAYAAAA0ejq3AAAKomlDQ1BJQ0MgUHJvZmlsZQAASImV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\n1I8g608CJEACJOAXAv8fb348jLwvW9sAAAAASUVORK5CYII=\n\" />\n</div>\n\n</div>\n\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing text_cell rendered\">\n<div class=\"prompt input_prompt\">\n</div>\n<div class=\"inner_cell\">\n<div class=\"text_cell_render border-box-sizing rendered_html\">\n<p>At a high level, the skip-gram flavor of this algorithm looks at a word and its surrounding  words, and tries to maximize the probability that the word's vector representation predicts those actual words occurring around it. If it is trained on the phrase <em>the quick brown fox jumps</em>, the word2vec input representation of the word <em>brown</em> would yield a high dot product with the output vectors for the words <em>the, quick, fox</em> and <em>jumps</em>. And if the algorithm is trained on a lot more text, there's a good chance that it will start to learn that the words $brown$ and $red$ appear in similar contexts, so that their vector representations will be pretty close to one another's.</p>\n<h2 id=\"Speed\">Speed<a class=\"anchor-link\" href=\"#Speed\">&#182;</a></h2><p>I'm constantly trying to navigate the trade-off from simultaneously maximizing my laziness and the speed of my code. Aiming for the lazy side, I originally wanted to experiment with using <a href=\"https://github.com/HIPS/autograd\">autograd</a> to write my gradient updates for me, and as nice as it was to not have to write out the gradient calculation, this pretty quickly bubbled to the top as one of the major bottlenecks, leaving me to write the gradients manually by faster means.</p>\n<p>While numpy gives a big speed boost over plain python, cython (used by the go-to word2vec implementation, <a href=\"https://github.com/piskvorky/gensim\">gensim</a>), gives a major performance improvement beyond numpy, with speeds often comparable to code written in C. Part of the cost of this speed up is that writing in Cython, while more pythonic than C, seems to require additional type annotations and syntactic elements, making it less readable and less interesting to write (at least for me). My goal here has been to make word2vec as close to gensim's cython performance as possible while sticking to Python, so I settled on Numba, a numeric JIT compiler that uses LLVM and supports a subset of Python and numpy.</p>\n<p>While I ran into some limitations of numba, rewriting the inner loops in Numba functions ended up giving a significant speed-boost. I did a lot of profiling and iterating to find the major bottlenecks of the gradient descent learning, and while I don't know of a good way to communicate the iterative stone-smoothing process that is program optimization,  I've left the original numpy and improved numba versions of some of these functions for microbenchmark comparison.</p>\n\n</div>\n</div>\n</div>\n<div class=\"cell border-box-sizing text_cell rendered\">\n<div class=\"prompt input_prompt\">\n</div>\n<div class=\"inner_cell\">\n<div class=\"text_cell_render border-box-sizing rendered_html\">\n<h1 id=\"Let's-get-started\">Let's get started<a class=\"anchor-link\" href=\"#Let's-get-started\">&#182;</a></h1>\n</div>\n</div>\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[2]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"o\">%%</span><span class=\"nx\">javascript</span>\n<span class=\"kd\">var</span> <span class=\"nx\">csc</span> <span class=\"o\">=</span> <span class=\"nx\">IPython</span><span class=\"p\">.</span><span class=\"nx\">keyboard_manager</span><span class=\"p\">.</span><span class=\"nx\">command_shortcuts</span>\n<span class=\"nx\">csc</span><span class=\"p\">.</span><span class=\"nx\">add_shortcut</span><span class=\"p\">(</span><span class=\"s1\">&#39;Ctrl-k&#39;</span><span class=\"p\">,</span><span class=\"s1\">&#39;ipython.move-selected-cell-up&#39;</span><span class=\"p\">)</span>\n<span class=\"nx\">csc</span><span class=\"p\">.</span><span class=\"nx\">add_shortcut</span><span class=\"p\">(</span><span class=\"s1\">&#39;Ctrl-j&#39;</span><span class=\"p\">,</span><span class=\"s1\">&#39;ipython.move-selected-cell-down&#39;</span><span class=\"p\">)</span>\n<span class=\"nx\">csc</span><span class=\"p\">.</span><span class=\"nx\">add_shortcut</span><span class=\"p\">(</span><span class=\"s1\">&#39;Shift-m&#39;</span><span class=\"p\">,</span><span class=\"s1\">&#39;ipython.merge-selected-cell-with-cell-after&#39;</span><span class=\"p\">)</span>\n</pre></div>\n\n</div>\n</div>\n</div>\n\n<div class=\"output_wrapper\">\n<div class=\"output\">\n\n\n<div class=\"output_area\"><div class=\"prompt\"></div>\n\n\n<div class=\"output_subarea output_javascript \">\n<script type=\"text/javascript\">\nvar csc = IPython.keyboard_manager.command_shortcuts\ncsc.add_shortcut('Ctrl-k','ipython.move-selected-cell-up')\ncsc.add_shortcut('Ctrl-j','ipython.move-selected-cell-down')\ncsc.add_shortcut('Shift-m','ipython.merge-selected-cell-with-cell-after')\n</script>\n</div>\n\n</div>\n\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[1]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"kn\">from</span> <span class=\"nn\">project_imports</span> <span class=\"k\">import</span> <span class=\"o\">*</span>\n<span class=\"o\">%</span><span class=\"k\">matplotlib</span> inline\n</pre></div>\n\n</div>\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[2]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"kn\">import</span> <span class=\"nn\">wordvec_utils</span> <span class=\"k\">as</span> <span class=\"nn\">wut</span>\n<span class=\"kn\">from</span> <span class=\"nn\">wordvec_utils</span> <span class=\"k\">import</span> <span class=\"n\">Cat</span><span class=\"p\">,</span> <span class=\"n\">update</span><span class=\"p\">,</span> <span class=\"n\">Conf</span><span class=\"p\">,</span> <span class=\"n\">NegSampler</span>\n<span class=\"kn\">import</span> <span class=\"nn\">utils</span> <span class=\"k\">as</span> <span class=\"nn\">ut</span>\n<span class=\"kn\">from</span> <span class=\"nn\">utils</span> <span class=\"k\">import</span> <span class=\"n\">take</span><span class=\"p\">,</span> <span class=\"n\">ilen</span><span class=\"p\">,</span> <span class=\"n\">timer</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numba_utils</span> <span class=\"k\">as</span> <span class=\"nn\">nbu</span>\n<span class=\"kn\">from</span> <span class=\"nn\">numba_utils</span> <span class=\"k\">import</span> <span class=\"n\">ns_grad</span> <span class=\"k\">as</span> <span class=\"n\">ns_grad_jit</span><span class=\"p\">,</span> <span class=\"n\">nseed</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">autograd</span> <span class=\"k\">import</span> <span class=\"n\">numpy</span> <span class=\"k\">as</span> <span class=\"n\">npa</span><span class=\"p\">,</span> <span class=\"n\">grad</span>\n<span class=\"kn\">from</span> <span class=\"nn\">numba</span> <span class=\"k\">import</span> <span class=\"n\">jit</span><span class=\"p\">,</span> <span class=\"n\">njit</span>\n<span class=\"kn\">from</span> <span class=\"nn\">numpy</span> <span class=\"k\">import</span> <span class=\"n\">array</span><span class=\"p\">,</span> <span class=\"n\">ndarray</span> <span class=\"k\">as</span> <span class=\"n\">Array</span>\n<span class=\"kn\">from</span> <span class=\"nn\">numpy.linalg</span> <span class=\"k\">import</span> <span class=\"n\">norm</span>\n</pre></div>\n\n</div>\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing text_cell rendered\">\n<div class=\"prompt input_prompt\">\n</div>\n<div class=\"inner_cell\">\n<div class=\"text_cell_render border-box-sizing rendered_html\">\n<h2 id=\"Objective-functions\">Objective functions<a class=\"anchor-link\" href=\"#Objective-functions\">&#182;</a></h2><p>The standard skip-gram objective function comes from taking the softmax probability of each of the actual context word vectors dotted with the input ($u_{c,j^*_c}$) and multiplying them together:</p>\n\\begin{align}\nE &amp; = -\\log \\prod_{c=1} ^{C}\n    \\frac {\\exp (u_{c,j^*_c})}\n          {\\sum_{j'=1} ^ V \\exp(u_{j'})} \\\\\n  &amp; = -\\sum^C_{c=1} u_{j^*_c} + C \\cdot \\log \\sum ^ V _{j'=1} \\exp(u_j')\n\\end{align}<p>See <a href=\"http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf\">word2vec Parameter Learning Explained</a> for a detailed explanation, and note that the algorithm keeps 2 representations for each word, an input vector from $W$, and an output vector from $W'$ (<code>W1</code> and <code>W2</code> in the code).\nThough I ended up using a different objective called negative sampling, I kept this original objective below as <code>skipgram_likelihood</code>, written as a nested function since autograd requires single-argument functions to differentiate:</p>\n\n</div>\n</div>\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[7]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"k\">def</span> <span class=\"nf\">skipgram_likelihood</span><span class=\"p\">(</span><span class=\"n\">wi</span><span class=\"p\">,</span> <span class=\"n\">cwds</span><span class=\"p\">,</span> <span class=\"n\">dv</span><span class=\"o\">=</span><span class=\"k\">None</span><span class=\"p\">):</span>\n    <span class=\"n\">wix</span> <span class=\"o\">=</span> <span class=\"n\">dv</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"n\">wi</span><span class=\"p\">)</span>\n    <span class=\"n\">cixs</span> <span class=\"o\">=</span> <span class=\"n\">dv</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"n\">cwds</span><span class=\"p\">)</span>\n    <span class=\"n\">C</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">cwds</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">logloss</span><span class=\"p\">(</span><span class=\"n\">Wall</span><span class=\"p\">):</span>\n        <span class=\"n\">W1</span><span class=\"p\">,</span> <span class=\"n\">W2</span> <span class=\"o\">=</span> <span class=\"n\">Cat</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"n\">Wall</span><span class=\"p\">)</span>\n        <span class=\"n\">h</span> <span class=\"o\">=</span> <span class=\"n\">W1</span><span class=\"p\">[</span><span class=\"n\">wix</span><span class=\"p\">,</span> <span class=\"p\">:]</span>  <span class=\"c\"># ∈ ℝⁿ</span>\n        <span class=\"n\">u</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">h</span><span class=\"p\">,</span> <span class=\"n\">W2</span><span class=\"p\">)</span>\n        <span class=\"n\">ucs</span> <span class=\"o\">=</span> <span class=\"n\">u</span><span class=\"p\">[</span><span class=\"n\">cixs</span><span class=\"p\">]</span>\n        <span class=\"k\">return</span> <span class=\"o\">-</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">ucs</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">C</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">u</span><span class=\"p\">)))</span>\n    <span class=\"k\">return</span> <span class=\"n\">logloss</span>\n</pre></div>\n\n</div>\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing text_cell rendered\">\n<div class=\"prompt input_prompt\">\n</div>\n<div class=\"inner_cell\">\n<div class=\"text_cell_render border-box-sizing rendered_html\">\n<p>The return value of <code>logloss</code> should pretty clearly resemble the equation above.</p>\n<h1 id=\"Negative-sampling\">Negative sampling<a class=\"anchor-link\" href=\"#Negative-sampling\">&#182;</a></h1><h2 id=\"Gradient\">Gradient<a class=\"anchor-link\" href=\"#Gradient\">&#182;</a></h2><p>After reading a bit more about word2vec, I found out about an extension to the skip-gram model called negative sampling that efficiently generates better word vectors. The basic idea is that in addition to training a word vector with $C$ words that <em>do</em> appear around it, the vector should also be trained with $K$ words randomly chosen from the rest of the text, as negative examples of what the vector should <em>not</em> predict in its context.</p>\n<p>As a side-note to keep up with the notation taken from <a href=\"http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf\">word2vec Parameter Learning Explained</a>, the term $\\boldsymbol v_{w_O}'$ refers to a word vector ($\\boldsymbol v$) that is from the output vector matrix ($\\boldsymbol v'$) representing an output word $w_O$. Notation aside, the gradient for the negative sampling extension is relatively straightforward. For each true context word vector $\\boldsymbol v_{w_O}'$ appearing close to the input word vector $h$, we'll draw $K$ word vectors $\\boldsymbol v_{w_i}$ at random from the corpus. The objective is computed by adding the log of the sigmoid of the word vector dotted with either the true or false context word (the negative samples are negated):</p>\n\n</div>\n</div>\n</div>\n<div class=\"cell border-box-sizing text_cell rendered\">\n<div class=\"prompt input_prompt\">\n</div>\n<div class=\"inner_cell\">\n<div class=\"text_cell_render border-box-sizing rendered_html\">\n$$\nE = -\\log \\sigma(\\boldsymbol v_{w_O}^{\\prime T} \\boldsymbol h)\n    - \\sum^K _{i=1} \\log \\sigma (-\\boldsymbol v_{w_i} ^{\\prime T} \\boldsymbol h)\n$$<p>The gradient with respect to $\\boldsymbol v_{w_j}^{\\prime T} \\boldsymbol h$ is then as follows, where $t_j$ is an indicator for whether $w_j$ actually appears in the context:</p>\n$$\n    \\frac{\\partial E}\n         {\\partial \\boldsymbol v_{w_j}^{\\prime T} \\boldsymbol h}\n         = \\sigma(\\boldsymbol v_{w_j}^{\\prime T} \\boldsymbol h) -t_j.\n$$<p>The numpy gradient is written below as <code>ns_grad</code>; the numba gradient <code>ns_grad_jit</code> is similar, but all of the functions used in a JIT'd function must also be numba-JIT'd, so I moved them all out to the <code>numba_utils</code> module.</p>\n<p>The schema I adopted to calculate gradients takes a subset of the matrix formed by concatenating the input and output parameter matrices together ($W || W'$). If the vector dimension $N$ is 100, and there are 1000 words in the vocabulary, this matrix will be in $\\mathbb{R}^{1000 \\times 200}$. The gradient function takes a subset of this matrix with $K + 2$ rows. The first row represents the input word, the next represents the true context word, and the rest represent the $K$ negative samples.</p>\n<p>Note that since the first vector is from the input parameter matrix $W$, and these functions are operating on the input and output matrices concatenated, we only care about the first $N$ entries of the first row. Similarly, we only care about the last $N$ entries for the rest of the output vectors. This schema should be more clear from the <code>get_vecs1</code> function below that extracts the vectors from the relevant subset of the concatenated parameter matrices. This also means that for the gradients of the concatenated matrix, half of the entries in each row will be zero.</p>\n\n</div>\n</div>\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[8]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"n\">getNall</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">W</span><span class=\"p\">:</span> <span class=\"n\">W</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">//</span> <span class=\"mi\">2</span>\n<span class=\"n\">gen_labels</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">negsamps</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">negsamps</span><span class=\"p\">)</span>\n<span class=\"n\">sig</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"mi\">1</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">+</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"n\">x</span><span class=\"p\">))</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">get_vecs1</span><span class=\"p\">(</span><span class=\"n\">Wsub</span><span class=\"p\">):</span>\n    <span class=\"n\">N</span> <span class=\"o\">=</span> <span class=\"n\">getNall</span><span class=\"p\">(</span><span class=\"n\">Wsub</span><span class=\"p\">)</span>\n    <span class=\"n\">h</span> <span class=\"o\">=</span> <span class=\"n\">Wsub</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"p\">:</span><span class=\"n\">N</span><span class=\"p\">]</span>  <span class=\"c\"># ∈ ℝⁿ</span>\n    <span class=\"n\">vwo</span> <span class=\"o\">=</span> <span class=\"n\">Wsub</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">:]</span>\n    <span class=\"n\">negsamps</span> <span class=\"o\">=</span> <span class=\"n\">Wsub</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">:,</span> <span class=\"n\">N</span><span class=\"p\">:]</span>\n    <span class=\"k\">return</span> <span class=\"n\">h</span><span class=\"p\">,</span> <span class=\"n\">vwo</span><span class=\"p\">,</span> <span class=\"n\">negsamps</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">ns_loss_grads</span><span class=\"p\">(</span><span class=\"n\">h</span><span class=\"p\">:</span> <span class=\"s\">&#39;v[n]&#39;</span><span class=\"p\">,</span> <span class=\"n\">vout</span><span class=\"p\">:</span> <span class=\"s\">&#39;[v[n]]&#39;</span><span class=\"p\">,</span> <span class=\"n\">label</span><span class=\"p\">:</span> <span class=\"s\">&#39;v[n]&#39;</span><span class=\"p\">):</span>\n    <span class=\"n\">dot</span> <span class=\"o\">=</span> <span class=\"n\">sig</span><span class=\"p\">(</span><span class=\"n\">vout</span> <span class=\"err\">@</span> <span class=\"n\">h</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"n\">label</span>\n    <span class=\"k\">return</span> <span class=\"n\">dot</span> <span class=\"o\">*</span> <span class=\"n\">vout</span><span class=\"p\">,</span> <span class=\"n\">dot</span> <span class=\"o\">*</span> <span class=\"n\">h</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">ns_grad</span><span class=\"p\">(</span><span class=\"n\">Wsub</span><span class=\"p\">):</span>\n    <span class=\"n\">h</span><span class=\"p\">,</span> <span class=\"n\">vwo</span><span class=\"p\">,</span> <span class=\"n\">negsamps</span> <span class=\"o\">=</span> <span class=\"n\">get_vecs1</span><span class=\"p\">(</span><span class=\"n\">Wsub</span><span class=\"p\">)</span>\n    <span class=\"n\">N</span> <span class=\"o\">=</span> <span class=\"n\">getNall</span><span class=\"p\">(</span><span class=\"n\">Wsub</span><span class=\"p\">)</span>\n    <span class=\"n\">Wsub_grad</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">Wsub</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n\n    <span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">vout</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"ow\">in</span> <span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">count</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">it</span><span class=\"o\">.</span><span class=\"n\">chain</span><span class=\"p\">([</span><span class=\"n\">vwo</span><span class=\"p\">],</span> <span class=\"n\">negsamps</span><span class=\"p\">),</span>\n                              <span class=\"n\">gen_labels</span><span class=\"p\">(</span><span class=\"n\">negsamps</span><span class=\"p\">)):</span>\n        <span class=\"n\">hgrad</span><span class=\"p\">,</span> <span class=\"n\">vgrad</span> <span class=\"o\">=</span> <span class=\"n\">ns_loss_grads</span><span class=\"p\">(</span><span class=\"n\">h</span><span class=\"p\">,</span> <span class=\"n\">vout</span><span class=\"p\">,</span> <span class=\"n\">label</span><span class=\"p\">)</span>\n        <span class=\"n\">Wsub_grad</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"p\">:</span><span class=\"n\">N</span><span class=\"p\">]</span> <span class=\"o\">+=</span> <span class=\"n\">hgrad</span>\n        <span class=\"n\">Wsub_grad</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">:]</span> <span class=\"o\">+=</span> <span class=\"n\">vgrad</span>\n    <span class=\"k\">return</span> <span class=\"n\">Wsub_grad</span>\n</pre></div>\n\n</div>\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing text_cell rendered\">\n<div class=\"prompt input_prompt\">\n</div>\n<div class=\"inner_cell\">\n<div class=\"text_cell_render border-box-sizing rendered_html\">\n<h2 id=\"Gradient-check\">Gradient check<a class=\"anchor-link\" href=\"#Gradient-check\">&#182;</a></h2><p>The following gradient checking functionality based on <a href=\"http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/\">the UFLDL tutorial</a> uses simple calculus to ensure that the gradients are working as expected.</p>\n<h3 id=\"Finite-difference-check\">Finite difference check<a class=\"anchor-link\" href=\"#Finite-difference-check\">&#182;</a></h3>\n</div>\n</div>\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[9]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"k\">def</span> <span class=\"nf\">ns_loss</span><span class=\"p\">(</span><span class=\"n\">h</span><span class=\"p\">,</span> <span class=\"n\">vwo</span><span class=\"p\">,</span> <span class=\"n\">vwi_negs</span><span class=\"p\">):</span>\n    <span class=\"sd\">&quot;&quot;&quot;This should be called on the subset of the matrix (Win || Wout&#39;)</span>\n<span class=\"sd\">    determined by row indices `wi, win_ix, negwds`.</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n    <span class=\"n\">negsum</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n    <span class=\"k\">for</span> <span class=\"n\">j</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">vwi_negs</span><span class=\"p\">)):</span>\n        <span class=\"n\">negsum</span> <span class=\"o\">+=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"n\">sig</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"n\">vwi_negs</span><span class=\"p\">[</span><span class=\"n\">j</span><span class=\"p\">]</span> <span class=\"err\">@</span> <span class=\"n\">h</span><span class=\"p\">))</span>\n        \n    <span class=\"k\">return</span> <span class=\"o\">-</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"n\">sig</span><span class=\"p\">(</span><span class=\"n\">vwo</span> <span class=\"err\">@</span> <span class=\"n\">h</span><span class=\"p\">))</span> <span class=\"o\">-</span> <span class=\"n\">negsum</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">ns_loss_vec</span><span class=\"p\">(</span><span class=\"n\">h</span><span class=\"p\">,</span> <span class=\"n\">vwo</span><span class=\"p\">,</span> <span class=\"n\">vwi_negs</span><span class=\"p\">):</span>\n    <span class=\"sd\">&quot;&quot;&quot;This should be called on the subset of the matrix (Win || Wout&#39;)</span>\n<span class=\"sd\">    determined by row indices `wi, win_ix, negwds`.</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n    <span class=\"k\">return</span> <span class=\"o\">-</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"n\">sig</span><span class=\"p\">(</span><span class=\"n\">vwo</span> <span class=\"err\">@</span> <span class=\"n\">h</span><span class=\"p\">))</span> <span class=\"o\">-</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"n\">sig</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"n\">vwi_negs</span> <span class=\"err\">@</span> <span class=\"n\">h</span> <span class=\"p\">)))</span>\n</pre></div>\n\n</div>\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[10]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"k\">def</span> <span class=\"nf\">J</span><span class=\"p\">(</span><span class=\"n\">Wsub</span><span class=\"p\">,</span> <span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"n\">ns_loss</span><span class=\"p\">):</span>\n    <span class=\"n\">N</span> <span class=\"o\">=</span> <span class=\"n\">getNall</span><span class=\"p\">(</span><span class=\"n\">Wsub</span><span class=\"p\">)</span>\n    <span class=\"n\">h</span><span class=\"p\">,</span> <span class=\"n\">vwo</span><span class=\"p\">,</span> <span class=\"n\">vwi_negs</span> <span class=\"o\">=</span> <span class=\"n\">get_vecs1</span><span class=\"p\">(</span><span class=\"n\">Wsub</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">loss</span><span class=\"p\">(</span><span class=\"n\">h</span><span class=\"p\">,</span> <span class=\"n\">vwo</span><span class=\"p\">,</span> <span class=\"n\">vwi_negs</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">check_grad_</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"nb\">int</span><span class=\"o\">=</span><span class=\"k\">None</span><span class=\"p\">,</span> <span class=\"n\">j</span><span class=\"p\">:</span> <span class=\"nb\">int</span><span class=\"o\">=</span><span class=\"k\">None</span><span class=\"p\">,</span> <span class=\"n\">eps</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"n\">e</span><span class=\"o\">-</span><span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"n\">J</span><span class=\"p\">:</span> <span class=\"s\">&#39;fn&#39;</span><span class=\"o\">=</span><span class=\"k\">None</span><span class=\"p\">):</span>\n    <span class=\"sd\">&quot;&quot;&quot;From eqn at http://ufldl.stanford.edu/tutorial/supervised</span>\n<span class=\"sd\">    /DebuggingGradientChecking/&quot;&quot;&quot;</span>\n    <span class=\"n\">Wneg</span><span class=\"p\">,</span> <span class=\"n\">Wpos</span> <span class=\"o\">=</span> <span class=\"n\">W</span><span class=\"o\">.</span><span class=\"n\">copy</span><span class=\"p\">(),</span> <span class=\"n\">W</span><span class=\"o\">.</span><span class=\"n\">copy</span><span class=\"p\">()</span>\n    <span class=\"n\">Wneg</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">j</span><span class=\"p\">]</span> <span class=\"o\">-=</span> <span class=\"n\">eps</span>\n    <span class=\"n\">Wpos</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">j</span><span class=\"p\">]</span> <span class=\"o\">+=</span> <span class=\"n\">eps</span>\n    <span class=\"k\">return</span> <span class=\"p\">(</span><span class=\"n\">J</span><span class=\"p\">(</span><span class=\"n\">Wpos</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"n\">J</span><span class=\"p\">(</span><span class=\"n\">Wneg</span><span class=\"p\">))</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">eps</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">approx_grad</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">J</span><span class=\"o\">=</span><span class=\"n\">J</span><span class=\"p\">):</span>\n    <span class=\"n\">n</span><span class=\"p\">,</span> <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">W</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n    <span class=\"n\">grad</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros_like</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">):</span>\n        <span class=\"k\">for</span> <span class=\"n\">j</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">):</span>\n            <span class=\"n\">grad</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">j</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">check_grad_</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">i</span><span class=\"o\">=</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">j</span><span class=\"o\">=</span><span class=\"n\">j</span><span class=\"p\">,</span> <span class=\"n\">eps</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"n\">e</span><span class=\"o\">-</span><span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"n\">J</span><span class=\"o\">=</span><span class=\"n\">J</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">grad</span>\n</pre></div>\n\n</div>\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing text_cell rendered\">\n<div class=\"prompt input_prompt\">\n</div>\n<div class=\"inner_cell\">\n<div class=\"text_cell_render border-box-sizing rendered_html\">\n<h3 id=\"Autograd-checking\">Autograd checking<a class=\"anchor-link\" href=\"#Autograd-checking\">&#182;</a></h3>\n</div>\n</div>\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[11]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"k\">def</span> <span class=\"nf\">mk_ns_loss_a</span><span class=\"p\">(</span><span class=\"n\">N</span><span class=\"p\">):</span>\n    <span class=\"sd\">&quot;&quot;&quot;Return a loss function that works on an N-dimensional</span>\n<span class=\"sd\">    representation. This takes a single argument, the subset of</span>\n<span class=\"sd\">    the input and output matrices that correspond to the input</span>\n<span class=\"sd\">    word (first row), true contest word (second row) and K</span>\n<span class=\"sd\">    negative samples (rest of the rows). Since it takes a</span>\n<span class=\"sd\">    single argument, the gradient can automatically be</span>\n<span class=\"sd\">    calculated by autograd&quot;</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n    <span class=\"k\">def</span> <span class=\"nf\">σ</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"mi\">1</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">+</span> <span class=\"n\">npa</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"n\">x</span><span class=\"p\">))</span>\n    \n    <span class=\"k\">def</span> <span class=\"nf\">ns_loss_a</span><span class=\"p\">(</span><span class=\"n\">Wsub</span><span class=\"p\">):</span>\n        <span class=\"n\">h</span> <span class=\"o\">=</span> <span class=\"n\">Wsub</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"p\">:</span><span class=\"n\">N</span><span class=\"p\">]</span>\n        <span class=\"n\">vwo</span> <span class=\"o\">=</span> <span class=\"n\">Wsub</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">:]</span>\n        <span class=\"n\">vwi_negs</span> <span class=\"o\">=</span> <span class=\"n\">Wsub</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">:,</span> <span class=\"n\">N</span><span class=\"p\">:]</span>\n        <span class=\"n\">vwo_h</span> <span class=\"o\">=</span> <span class=\"n\">npa</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">vwo</span><span class=\"p\">,</span> <span class=\"n\">h</span><span class=\"p\">)</span>\n        <span class=\"n\">vwi_negs_h</span> <span class=\"o\">=</span> <span class=\"n\">npa</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">vwi_negs</span><span class=\"p\">,</span> <span class=\"n\">h</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"o\">-</span><span class=\"n\">npa</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"n\">σ</span><span class=\"p\">(</span><span class=\"n\">vwo_h</span><span class=\"p\">))</span> <span class=\"o\">-</span> <span class=\"n\">npa</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">npa</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"n\">σ</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"n\">vwi_negs_h</span><span class=\"p\">)))</span>\n    <span class=\"k\">return</span> <span class=\"n\">ns_loss_a</span>\n\n<span class=\"n\">mk_ns_grad_a</span> <span class=\"o\">=</span> <span class=\"n\">z</span><span class=\"o\">.</span><span class=\"n\">compose</span><span class=\"p\">(</span><span class=\"n\">grad</span><span class=\"p\">,</span> <span class=\"n\">mk_ns_loss_a</span><span class=\"p\">)</span>\n</pre></div>\n\n</div>\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[12]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"n\">N_</span> <span class=\"o\">=</span> <span class=\"mi\">50</span><span class=\"p\">;</span> <span class=\"n\">W</span> <span class=\"o\">=</span> <span class=\"n\">wut</span><span class=\"o\">.</span><span class=\"n\">init_w</span><span class=\"p\">(</span><span class=\"mi\">1000</span><span class=\"p\">,</span> <span class=\"n\">N_</span><span class=\"p\">,</span> <span class=\"n\">seed</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">);</span> <span class=\"n\">Wsub</span> <span class=\"o\">=</span> <span class=\"n\">W</span><span class=\"p\">[:</span><span class=\"mi\">8</span><span class=\"p\">]</span>\n</pre></div>\n\n</div>\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing text_cell rendered\">\n<div class=\"prompt input_prompt\">\n</div>\n<div class=\"inner_cell\">\n<div class=\"text_cell_render border-box-sizing rendered_html\">\n<h3 id=\"Compare-gradients\">Compare gradients<a class=\"anchor-link\" href=\"#Compare-gradients\">&#182;</a></h3>\n</div>\n</div>\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[13]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"n\">np_check</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">approx_grad</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">partial</span><span class=\"p\">(</span><span class=\"n\">J</span><span class=\"p\">,</span> <span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"n\">ns_loss</span><span class=\"p\">))</span>\n<span class=\"n\">np_vec_check</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">approx_grad</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">partial</span><span class=\"p\">(</span><span class=\"n\">J</span><span class=\"p\">,</span> <span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"n\">ns_loss_vec</span><span class=\"p\">))</span>\n<span class=\"n\">ns_grad_auto</span> <span class=\"o\">=</span> <span class=\"n\">mk_ns_grad_a</span><span class=\"p\">(</span><span class=\"n\">N_</span><span class=\"p\">)</span>\n</pre></div>\n\n</div>\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[14]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"k\">def</span> <span class=\"nf\">grad_close</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">,</span> <span class=\"n\">grd</span><span class=\"o\">=</span><span class=\"n\">ns_grad</span><span class=\"p\">(</span><span class=\"n\">Wsub</span><span class=\"p\">)):</span>\n    <span class=\"sd\">&quot;&quot;&quot;Check that a given gradient function result agrees</span>\n<span class=\"sd\">    with ns_grad. Print out norm of the difference.&quot;&quot;&quot;</span>\n    <span class=\"n\">grd2</span> <span class=\"o\">=</span> <span class=\"n\">f</span><span class=\"p\">(</span><span class=\"n\">Wsub</span><span class=\"p\">)</span>\n    <span class=\"n\">close</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">allclose</span><span class=\"p\">(</span><span class=\"n\">grd</span><span class=\"p\">,</span> <span class=\"n\">grd2</span><span class=\"p\">)</span>\n    <span class=\"n\">close_</span> <span class=\"o\">=</span> <span class=\"s\">&#39;√&#39;</span> <span class=\"k\">if</span> <span class=\"n\">close</span> <span class=\"k\">else</span> <span class=\"s\">&#39;x&#39;</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s\">&#39;{} Diff: {}&#39;</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">close_</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">norm</span><span class=\"p\">(</span><span class=\"n\">grd</span> <span class=\"o\">-</span> <span class=\"n\">grd2</span><span class=\"p\">)))</span>\n    <span class=\"k\">return</span> <span class=\"n\">close</span>\n</pre></div>\n\n</div>\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[15]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"k\">assert</span> <span class=\"n\">grad_close</span><span class=\"p\">(</span><span class=\"n\">np_check</span><span class=\"p\">)</span>\n<span class=\"k\">assert</span> <span class=\"n\">grad_close</span><span class=\"p\">(</span><span class=\"n\">np_vec_check</span><span class=\"p\">)</span>\n<span class=\"k\">assert</span> <span class=\"n\">grad_close</span><span class=\"p\">(</span><span class=\"n\">ns_grad_auto</span><span class=\"p\">)</span>\n<span class=\"k\">assert</span> <span class=\"n\">grad_close</span><span class=\"p\">(</span><span class=\"n\">ns_grad</span><span class=\"p\">)</span>\n<span class=\"k\">assert</span> <span class=\"n\">grad_close</span><span class=\"p\">(</span><span class=\"n\">ns_grad_jit</span><span class=\"p\">)</span>\n</pre></div>\n\n</div>\n</div>\n</div>\n\n<div class=\"output_wrapper\">\n<div class=\"output\">\n\n\n<div class=\"output_area\"><div class=\"prompt\"></div>\n<div class=\"output_subarea output_stream output_stdout output_text\">\n<pre>√ Diff: 4.3962387780340825e-09\n√ Diff: 4.3962387780340825e-09\n√ Diff: 1.1173464828107622e-17\n√ Diff: 0.0\n√ Diff: 0.0\n</pre>\n</div>\n</div>\n\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[16]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"o\">%</span><span class=\"k\">timeit</span> np_check(Wsub)      #  48.9 ms per loop\n<span class=\"o\">%</span><span class=\"k\">timeit</span> np_vec_check(Wsub)  #  33.8 ms per loop\n<span class=\"o\">%</span><span class=\"k\">timeit</span> ns_grad_auto(Wsub)  #     856 µs per loop\n<span class=\"o\">%</span><span class=\"k\">timeit</span> ns_grad(Wsub)       #      53.8 µs per loop\n<span class=\"o\">%</span><span class=\"k\">timeit</span> ns_grad_jit(Wsub)   #       6.14 µs per loop\n</pre></div>\n\n</div>\n</div>\n</div>\n\n<div class=\"output_wrapper\">\n<div class=\"output\">\n\n\n<div class=\"output_area\"><div class=\"prompt\"></div>\n<div class=\"output_subarea output_stream output_stdout output_text\">\n<pre>10 loops, best of 3: 48.2 ms per loop\n10 loops, best of 3: 32 ms per loop\n1000 loops, best of 3: 845 µs per loop\n10000 loops, best of 3: 54.7 µs per loop\nThe slowest run took 4.31 times longer than the fastest. This could mean that an intermediate result is being cached \n100000 loops, best of 3: 6.17 µs per loop\n</pre>\n</div>\n</div>\n\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing text_cell rendered\">\n<div class=\"prompt input_prompt\">\n</div>\n<div class=\"inner_cell\">\n<div class=\"text_cell_render border-box-sizing rendered_html\">\n<p>The first two gradient checking versions are extremely slow as expected from the naive and unpythonic style. The numpy implementation gives a huge improvement over these, and numba gives us another order of magnitude speedup over this. The autograd version looks like it falls at about the logarithmic midpoint between the naive checking and handwritten implementations.</p>\n<h3 id=\"Draw-negative-samples\">Draw negative samples<a class=\"anchor-link\" href=\"#Draw-negative-samples\">&#182;</a></h3><p>As a foreshadowing of performance bottlenecks that my original implementation ran into, I have a few versions of a function that chooses words from the text at random, that increase in performance. They all draw words randomly according to the unigram$^{3/4}$ distribution, which is similar to the unigram distribution, but boosts the probability of less frequent items:</p>\n\n</div>\n</div>\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[17]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"n\">uni</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linspace</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">)</span>\n<span class=\"n\">uni34</span> <span class=\"o\">=</span> <span class=\"n\">uni</span> <span class=\"o\">**</span> <span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"o\">/</span><span class=\"mi\">4</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">uni</span><span class=\"p\">,</span> <span class=\"n\">uni34</span> <span class=\"o\">-</span> <span class=\"n\">uni</span><span class=\"p\">);</span>\n</pre></div>\n\n</div>\n</div>\n</div>\n\n<div class=\"output_wrapper\">\n<div class=\"output\">\n\n\n<div class=\"output_area\"><div class=\"prompt\"></div>\n\n\n<div class=\"output_png output_subarea \">\n<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAX4AAAECCAYAAAD5OrxGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIYQIMRL8QggRYiT4hRAixEjwCyFEiPn/QwKarduYbi0AAAAASUVORK5CYII=\n\" />\n</div>\n\n</div>\n\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing text_cell rendered\">\n<div class=\"prompt input_prompt\">\n</div>\n<div class=\"inner_cell\">\n<div class=\"text_cell_render border-box-sizing rendered_html\">\n<p>Most of the samplers were written as generators, except for the last one, <code>neg_sampler_inplace</code>, which modifies an array in-place with random samples. As much as this in-place modification conflicts with my usual approach, it ended up being necessary for a final numba performance boost.</p>\n<p>The generator sampler I ended up going with, <code>neg_sampler_jit_pad</code>, is padded with a couple of dummy entries. I originally drew some negative samples <code>negsamps</code> from one of these generators, and then created the array to index $W$ by prepending <code>negsamps</code> with <code>w</code> and <code>c</code>, copying these to a new array. Usually this is ok, but I found that copying <code>negsamps</code> to a new array in the inner loop caused a noticeable delay, so I wrote <code>neg_sampler_jit_pad</code> to return 2 extra empty elements at the beginning to be able to insert <code>w</code> and <code>c</code> inplace without copying to a new array.</p>\n\n</div>\n</div>\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[18]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"k\">def</span> <span class=\"nf\">unigram</span><span class=\"p\">(</span><span class=\"n\">txt</span><span class=\"p\">,</span> <span class=\"nb\">pow</span><span class=\"o\">=.</span><span class=\"mi\">75</span><span class=\"p\">):</span>\n    <span class=\"s\">&quot;Unigram^(3/4) model&quot;</span>\n    <span class=\"n\">cts</span> <span class=\"o\">=</span> <span class=\"n\">Series</span><span class=\"p\">(</span><span class=\"n\">Counter</span><span class=\"p\">(</span><span class=\"n\">txt</span><span class=\"p\">))</span>\n    \n    <span class=\"c\"># If txt is integers, fill in missing values (likely for</span>\n    <span class=\"c\"># unknown token) with 0 probability to reliably use</span>\n    <span class=\"c\"># index to identify token</span>\n    <span class=\"n\">int_txt</span> <span class=\"o\">=</span> <span class=\"n\">cts</span><span class=\"o\">.</span><span class=\"n\">index</span><span class=\"o\">.</span><span class=\"n\">dtype</span> <span class=\"o\">==</span> <span class=\"nb\">int</span>\n    <span class=\"k\">if</span> <span class=\"n\">int_txt</span><span class=\"p\">:</span>\n        <span class=\"n\">missing_tokens</span> <span class=\"o\">=</span> <span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">cts</span><span class=\"o\">.</span><span class=\"n\">index</span><span class=\"o\">.</span><span class=\"n\">max</span><span class=\"p\">()))</span> <span class=\"o\">-</span> <span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">cts</span><span class=\"o\">.</span><span class=\"n\">index</span><span class=\"p\">)</span>\n        <span class=\"k\">for</span> <span class=\"n\">msg</span> <span class=\"ow\">in</span> <span class=\"n\">missing_tokens</span><span class=\"p\">:</span>\n            <span class=\"n\">cts</span><span class=\"o\">.</span><span class=\"n\">loc</span><span class=\"p\">[</span><span class=\"n\">msg</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n        <span class=\"n\">cts</span> <span class=\"o\">=</span> <span class=\"n\">cts</span><span class=\"o\">.</span><span class=\"n\">sort_index</span><span class=\"p\">()</span>\n        \n    <span class=\"n\">N</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">txt</span><span class=\"p\">)</span>\n    <span class=\"n\">ctsdf</span> <span class=\"o\">=</span> <span class=\"p\">((</span><span class=\"n\">cts</span> <span class=\"o\">/</span> <span class=\"n\">cts</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">())</span> <span class=\"o\">**</span> <span class=\"nb\">pow</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">reset_index</span><span class=\"p\">(</span><span class=\"n\">drop</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n    <span class=\"n\">ctsdf</span><span class=\"o\">.</span><span class=\"n\">columns</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s\">&#39;Word&#39;</span><span class=\"p\">,</span> <span class=\"s\">&#39;Prob&#39;</span><span class=\"p\">]</span>\n    <span class=\"k\">if</span> <span class=\"n\">int_txt</span><span class=\"p\">:</span>\n        <span class=\"k\">assert</span> <span class=\"p\">(</span><span class=\"n\">ctsdf</span><span class=\"o\">.</span><span class=\"n\">Word</span> <span class=\"o\">==</span> <span class=\"n\">ctsdf</span><span class=\"o\">.</span><span class=\"n\">index</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">all</span><span class=\"p\">()</span>\n    <span class=\"k\">return</span> <span class=\"n\">ctsdf</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">cum_prob_uni</span><span class=\"p\">(</span><span class=\"n\">xs</span><span class=\"p\">,</span> <span class=\"nb\">pow</span><span class=\"o\">=.</span><span class=\"mi\">75</span><span class=\"p\">):</span>\n    <span class=\"n\">ug</span> <span class=\"o\">=</span> <span class=\"n\">unigram</span><span class=\"p\">(</span><span class=\"n\">xs</span><span class=\"p\">,</span> <span class=\"nb\">pow</span><span class=\"o\">=</span><span class=\"nb\">pow</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">ug</span><span class=\"o\">.</span><span class=\"n\">Prob</span><span class=\"o\">.</span><span class=\"n\">cumsum</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">div</span><span class=\"p\">(</span><span class=\"n\">ug</span><span class=\"o\">.</span><span class=\"n\">Prob</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">())</span><span class=\"o\">.</span><span class=\"n\">values</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">neg_sampler_pd</span><span class=\"p\">(</span><span class=\"n\">xs</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"nb\">pow</span><span class=\"o\">=.</span><span class=\"mi\">75</span><span class=\"p\">):</span>\n    <span class=\"s\">&quot;Simplest, but slowest sampler using built-in pandas sample function&quot;</span>\n    <span class=\"n\">ug</span> <span class=\"o\">=</span> <span class=\"n\">unigram</span><span class=\"p\">(</span><span class=\"n\">xs</span><span class=\"p\">,</span> <span class=\"nb\">pow</span><span class=\"o\">=</span><span class=\"nb\">pow</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">seed</span> <span class=\"ow\">in</span> <span class=\"n\">count</span><span class=\"p\">():</span>\n        <span class=\"k\">yield</span> <span class=\"n\">ug</span><span class=\"o\">.</span><span class=\"n\">Word</span><span class=\"o\">.</span><span class=\"n\">sample</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"o\">=</span><span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">weights</span><span class=\"o\">=</span><span class=\"n\">ug</span><span class=\"o\">.</span><span class=\"n\">Prob</span><span class=\"p\">,</span> <span class=\"n\">random_state</span><span class=\"o\">=</span><span class=\"n\">seed</span><span class=\"p\">,</span>\n                             <span class=\"n\">replace</span><span class=\"o\">=</span><span class=\"k\">True</span><span class=\"p\">)</span>\n        \n        \n<span class=\"k\">def</span> <span class=\"nf\">neg_sampler_np</span><span class=\"p\">(</span><span class=\"n\">xs</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">cache_len</span><span class=\"o\">=</span><span class=\"mi\">1000</span><span class=\"p\">,</span> <span class=\"n\">use_seed</span><span class=\"o\">=</span><span class=\"k\">False</span><span class=\"p\">,</span> <span class=\"nb\">pow</span><span class=\"o\">=.</span><span class=\"mi\">75</span><span class=\"p\">,</span>\n                   <span class=\"n\">ret_type</span><span class=\"o\">=</span><span class=\"n\">Array</span><span class=\"p\">):</span>\n    <span class=\"s\">&quot;Faster neg. sampler without the pandas overhead&quot;</span>\n    <span class=\"n\">ug</span> <span class=\"o\">=</span> <span class=\"n\">unigram</span><span class=\"p\">(</span><span class=\"n\">xs</span><span class=\"p\">,</span> <span class=\"nb\">pow</span><span class=\"o\">=</span><span class=\"nb\">pow</span><span class=\"p\">)</span>\n    <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">ug</span><span class=\"o\">.</span><span class=\"n\">Prob</span><span class=\"o\">.</span><span class=\"n\">values</span> <span class=\"o\">/</span> <span class=\"n\">ug</span><span class=\"o\">.</span><span class=\"n\">Prob</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">()</span>\n    <span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">ug</span><span class=\"o\">.</span><span class=\"n\">Word</span><span class=\"o\">.</span><span class=\"n\">values</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">neg_sampler_np_</span><span class=\"p\">():</span>\n        <span class=\"k\">for</span> <span class=\"n\">seed</span> <span class=\"ow\">in</span> <span class=\"n\">count</span><span class=\"p\">():</span>\n            <span class=\"k\">if</span> <span class=\"n\">use_seed</span><span class=\"p\">:</span>\n                <span class=\"n\">nr</span><span class=\"o\">.</span><span class=\"n\">seed</span><span class=\"p\">(</span><span class=\"n\">seed</span><span class=\"p\">)</span>\n            <span class=\"n\">Wds</span> <span class=\"o\">=</span> <span class=\"n\">nr</span><span class=\"o\">.</span><span class=\"n\">choice</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">cache_len</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">),</span> <span class=\"n\">p</span><span class=\"o\">=</span><span class=\"n\">p</span><span class=\"p\">)</span>\n            <span class=\"k\">for</span> <span class=\"n\">wds</span> <span class=\"ow\">in</span> <span class=\"n\">Wds</span><span class=\"p\">:</span>\n                <span class=\"k\">yield</span> <span class=\"n\">wds</span>\n                \n    <span class=\"n\">agen</span> <span class=\"o\">=</span> <span class=\"n\">neg_sampler_np_</span><span class=\"p\">()</span>            \n    <span class=\"k\">if</span> <span class=\"n\">ret_type</span> <span class=\"o\">==</span> <span class=\"nb\">list</span><span class=\"p\">:</span>\n        <span class=\"k\">return</span> <span class=\"nb\">map</span><span class=\"p\">(</span><span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">agen</span><span class=\"p\">)</span>\n    <span class=\"k\">assert</span> <span class=\"n\">ret_type</span> <span class=\"o\">==</span> <span class=\"n\">Array</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"s\">&#39;Only defined for type &#39;</span>\n                                    <span class=\"s\">&#39;in {Array, list}&#39;</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">agen</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">neg_sampler_jit</span><span class=\"p\">(</span><span class=\"n\">xs</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"nb\">pow</span><span class=\"o\">=.</span><span class=\"mi\">75</span><span class=\"p\">,</span> <span class=\"n\">ret_type</span><span class=\"o\">=</span><span class=\"nb\">list</span><span class=\"p\">):</span>\n    <span class=\"n\">cum_prob</span> <span class=\"o\">=</span> <span class=\"n\">cum_prob_uni</span><span class=\"p\">(</span><span class=\"n\">xs</span><span class=\"p\">,</span> <span class=\"nb\">pow</span><span class=\"o\">=</span><span class=\"nb\">pow</span><span class=\"p\">)</span>\n    <span class=\"n\">sampler</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n        <span class=\"nb\">list</span><span class=\"p\">:</span>  <span class=\"n\">nbu</span><span class=\"o\">.</span><span class=\"n\">neg_sampler_jitl_</span><span class=\"p\">,</span>\n        <span class=\"n\">Array</span><span class=\"p\">:</span> <span class=\"n\">nbu</span><span class=\"o\">.</span><span class=\"n\">neg_sampler_jita_</span><span class=\"p\">,</span>\n    <span class=\"p\">}[</span><span class=\"n\">ret_type</span><span class=\"p\">]</span>\n    <span class=\"k\">return</span> <span class=\"n\">sampler</span><span class=\"p\">(</span><span class=\"n\">cum_prob</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">neg_sampler_jit_pad</span><span class=\"p\">(</span><span class=\"n\">xs</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"nb\">pow</span><span class=\"o\">=.</span><span class=\"mi\">75</span><span class=\"p\">,</span> <span class=\"n\">ret_type</span><span class=\"o\">=</span><span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">pad</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">):</span>\n    <span class=\"n\">cum_prob</span> <span class=\"o\">=</span> <span class=\"n\">cum_prob_uni</span><span class=\"p\">(</span><span class=\"n\">xs</span><span class=\"p\">,</span> <span class=\"nb\">pow</span><span class=\"o\">=</span><span class=\"nb\">pow</span><span class=\"p\">)</span>\n    <span class=\"n\">sampler</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n        <span class=\"nb\">list</span><span class=\"p\">:</span>  <span class=\"n\">nbu</span><span class=\"o\">.</span><span class=\"n\">neg_sampler_jitl_pad</span><span class=\"p\">,</span>\n        <span class=\"n\">Array</span><span class=\"p\">:</span> <span class=\"n\">nbu</span><span class=\"o\">.</span><span class=\"n\">neg_sampler_jita_pad</span><span class=\"p\">,</span>\n    <span class=\"p\">}[</span><span class=\"n\">ret_type</span><span class=\"p\">]</span>\n    <span class=\"k\">return</span> <span class=\"n\">sampler</span><span class=\"p\">(</span><span class=\"n\">cum_prob</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">pad</span><span class=\"o\">=</span><span class=\"n\">pad</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">neg_sampler_inplace</span><span class=\"p\">(</span><span class=\"n\">xs</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"nb\">pow</span><span class=\"o\">=.</span><span class=\"mi\">75</span><span class=\"p\">,</span> <span class=\"n\">pad</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">ret_type</span><span class=\"o\">=</span><span class=\"n\">Array</span><span class=\"p\">,</span>\n                        <span class=\"n\">seed</span><span class=\"o\">=</span><span class=\"k\">None</span><span class=\"p\">):</span>\n    <span class=\"n\">use_seed</span> <span class=\"o\">=</span> <span class=\"n\">seed</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"k\">None</span> <span class=\"ow\">and</span> <span class=\"k\">False</span>\n    <span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"n\">seed</span> <span class=\"ow\">or</span> <span class=\"mi\">0</span>\n    <span class=\"n\">cum_prob</span> <span class=\"o\">=</span> <span class=\"n\">cum_prob_uni</span><span class=\"p\">(</span><span class=\"n\">xs</span><span class=\"p\">,</span> <span class=\"nb\">pow</span><span class=\"o\">=</span><span class=\"nb\">pow</span><span class=\"p\">)</span>\n    <span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">(</span><span class=\"n\">K</span> <span class=\"o\">+</span> <span class=\"n\">pad</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">int64</span><span class=\"p\">)</span>\n    \n    <span class=\"nd\">@njit</span>\n    <span class=\"k\">def</span> <span class=\"nf\">neg_sampler_jit_pad_arr_</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">):</span>\n        <span class=\"k\">if</span> <span class=\"n\">use_seed</span><span class=\"p\">:</span>\n            <span class=\"n\">nr</span><span class=\"o\">.</span><span class=\"n\">seed</span><span class=\"p\">(</span><span class=\"n\">seed</span><span class=\"p\">)</span>\n        <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">pad</span><span class=\"p\">,</span> <span class=\"n\">K</span> <span class=\"o\">+</span> <span class=\"n\">pad</span><span class=\"p\">):</span>\n            <span class=\"n\">a</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">nbu</span><span class=\"o\">.</span><span class=\"n\">bisect_left_jit</span><span class=\"p\">(</span><span class=\"n\">cum_prob</span><span class=\"p\">,</span> <span class=\"n\">nr</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">())</span>\n    <span class=\"k\">return</span> <span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">neg_sampler_jit_pad_arr_</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">neg_sampler_inplace_gen</span><span class=\"p\">(</span><span class=\"n\">xs</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"nb\">pow</span><span class=\"o\">=.</span><span class=\"mi\">75</span><span class=\"p\">,</span> <span class=\"n\">pad</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">ret_type</span><span class=\"o\">=</span><span class=\"n\">Array</span><span class=\"p\">):</span>\n    <span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">neg_sampler_jit_pad_arr_</span> <span class=\"o\">=</span> <span class=\"n\">neg_sampler_inplace</span><span class=\"p\">(</span><span class=\"n\">xs</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"nb\">pow</span><span class=\"o\">=</span><span class=\"nb\">pow</span><span class=\"p\">,</span>\n                                                      <span class=\"n\">pad</span><span class=\"o\">=</span><span class=\"n\">pad</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"n\">it</span><span class=\"o\">.</span><span class=\"n\">repeat</span><span class=\"p\">(</span><span class=\"k\">None</span><span class=\"p\">):</span>\n        <span class=\"n\">neg_sampler_jit_pad_arr_</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">)</span>\n        <span class=\"k\">yield</span> <span class=\"n\">a</span><span class=\"p\">[</span><span class=\"n\">pad</span><span class=\"p\">:]</span><span class=\"o\">.</span><span class=\"n\">copy</span><span class=\"p\">()</span>\n</pre></div>\n\n</div>\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing text_cell rendered\">\n<div class=\"prompt input_prompt\">\n</div>\n<div class=\"inner_cell\">\n<div class=\"text_cell_render border-box-sizing rendered_html\">\n<h3 id=\"Check-distributions\">Check distributions<a class=\"anchor-link\" href=\"#Check-distributions\">&#182;</a></h3><p>Just as a sanity check that the different implementations do the same thing, I randomly generate words according to how frequently they occur in the text with each of the samplers and scatter-plot them against the actual word frequency to check that they mostly lie on $y=x$.</p>\n\n</div>\n</div>\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[19]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"kn\">from</span> <span class=\"nn\">sklearn.preprocessing</span> <span class=\"k\">import</span> <span class=\"n\">LabelEncoder</span>\n<span class=\"kn\">from</span> <span class=\"nn\">nltk.corpus</span> <span class=\"k\">import</span> <span class=\"n\">brown</span>\n<span class=\"n\">some_text</span> <span class=\"o\">=</span> <span class=\"n\">take</span><span class=\"p\">(</span><span class=\"n\">brown</span><span class=\"o\">.</span><span class=\"n\">words</span><span class=\"p\">(),</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"n\">e6</span><span class=\"p\">))</span>\n</pre></div>\n\n</div>\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[20]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"n\">le</span> <span class=\"o\">=</span> <span class=\"n\">LabelEncoder</span><span class=\"p\">()</span>\n<span class=\"n\">smtok</span> <span class=\"o\">=</span> <span class=\"n\">le</span><span class=\"o\">.</span><span class=\"n\">fit_transform</span><span class=\"p\">(</span><span class=\"n\">some_text</span><span class=\"p\">)</span>\n</pre></div>\n\n</div>\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[21]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"n\">gen_pd</span> <span class=\"o\">=</span>       <span class=\"n\">NegSampler</span><span class=\"p\">(</span><span class=\"n\">neg_sampler_pd</span><span class=\"p\">,</span> <span class=\"n\">smtok</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">)</span>\n<span class=\"n\">gen_npl</span> <span class=\"o\">=</span>      <span class=\"n\">NegSampler</span><span class=\"p\">(</span><span class=\"n\">neg_sampler_np</span><span class=\"p\">,</span> <span class=\"n\">smtok</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"n\">ret_type</span><span class=\"o\">=</span><span class=\"nb\">list</span><span class=\"p\">)</span>\n<span class=\"n\">gen_npa</span> <span class=\"o\">=</span>      <span class=\"n\">NegSampler</span><span class=\"p\">(</span><span class=\"n\">neg_sampler_np</span><span class=\"p\">,</span> <span class=\"n\">smtok</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"n\">ret_type</span><span class=\"o\">=</span><span class=\"n\">Array</span><span class=\"p\">)</span>\n<span class=\"n\">gen_jitl</span> <span class=\"o\">=</span>     <span class=\"n\">NegSampler</span><span class=\"p\">(</span><span class=\"n\">neg_sampler_jit</span><span class=\"p\">,</span> <span class=\"n\">smtok</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"n\">ret_type</span><span class=\"o\">=</span><span class=\"nb\">list</span><span class=\"p\">)</span>\n<span class=\"n\">gen_jitl_pad</span> <span class=\"o\">=</span> <span class=\"n\">NegSampler</span><span class=\"p\">(</span><span class=\"n\">neg_sampler_jit_pad</span><span class=\"p\">,</span> <span class=\"n\">smtok</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"n\">ret_type</span><span class=\"o\">=</span><span class=\"nb\">list</span><span class=\"p\">,</span>\n                          <span class=\"n\">pad</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"n\">gen_jita</span> <span class=\"o\">=</span>     <span class=\"n\">NegSampler</span><span class=\"p\">(</span><span class=\"n\">neg_sampler_jit</span><span class=\"p\">,</span> <span class=\"n\">smtok</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"n\">ret_type</span><span class=\"o\">=</span><span class=\"n\">Array</span><span class=\"p\">)</span>\n<span class=\"n\">gen_jita_pad</span> <span class=\"o\">=</span> <span class=\"n\">NegSampler</span><span class=\"p\">(</span><span class=\"n\">neg_sampler_jit_pad</span><span class=\"p\">,</span> <span class=\"n\">smtok</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">,</span>\n                          <span class=\"n\">ret_type</span><span class=\"o\">=</span><span class=\"n\">Array</span><span class=\"p\">,</span> <span class=\"n\">pad</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"n\">gen_jita_padi</span> <span class=\"o\">=</span> <span class=\"n\">NegSampler</span><span class=\"p\">(</span><span class=\"n\">neg_sampler_inplace_gen</span><span class=\"p\">,</span> <span class=\"n\">smtok</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">,</span>\n                           <span class=\"n\">ret_type</span><span class=\"o\">=</span><span class=\"n\">Array</span><span class=\"p\">,</span> <span class=\"n\">pad</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n</pre></div>\n\n</div>\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[22]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"n\">run_n</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">gen</span><span class=\"p\">,</span> <span class=\"n\">n</span><span class=\"o\">=</span><span class=\"mi\">10000</span><span class=\"p\">:</span> <span class=\"n\">ilen</span><span class=\"p\">(</span><span class=\"n\">it</span><span class=\"o\">.</span><span class=\"n\">islice</span><span class=\"p\">(</span><span class=\"n\">gen</span><span class=\"p\">,</span> <span class=\"n\">n</span><span class=\"p\">))</span>\n<span class=\"o\">%</span><span class=\"k\">timeit</span> run_n(gen_pd, n=1000)  # 3.41 s per loop\n<span class=\"o\">%</span><span class=\"k\">timeit</span> run_n(gen_npl)         #    31.4 ms per loop\n<span class=\"o\">%</span><span class=\"k\">timeit</span> run_n(gen_npa)         #    19.1 ms per loop\n<span class=\"o\">%</span><span class=\"k\">timeit</span> run_n(gen_jitl)        #    18.6 ms per loop\n<span class=\"o\">%</span><span class=\"k\">timeit</span> run_n(gen_jitl_pad)    #    19.2 ms per loop\n<span class=\"o\">%</span><span class=\"k\">timeit</span> run_n(gen_jita)        #    14.7 ms per loop\n<span class=\"o\">%</span><span class=\"k\">timeit</span> run_n(gen_jita_pad)    #    19.2 ms per loop\n<span class=\"o\">%</span><span class=\"k\">timeit</span> run_n(gen_jita_padi)   #    20.9 ms per loop\n</pre></div>\n\n</div>\n</div>\n</div>\n\n<div class=\"output_wrapper\">\n<div class=\"output\">\n\n\n<div class=\"output_area\"><div class=\"prompt\"></div>\n<div class=\"output_subarea output_stream output_stdout output_text\">\n<pre>1 loops, best of 3: 3.42 s per loop\n10 loops, best of 3: 32.4 ms per loop\n10 loops, best of 3: 19.9 ms per loop\n100 loops, best of 3: 18.7 ms per loop\n100 loops, best of 3: 19.4 ms per loop\n100 loops, best of 3: 14.5 ms per loop\n100 loops, best of 3: 14.9 ms per loop\n10 loops, best of 3: 20.6 ms per loop\n</pre>\n</div>\n</div>\n\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[23]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"n\">n</span> <span class=\"o\">=</span> <span class=\"mi\">100000</span>\n<span class=\"n\">csp</span> <span class=\"o\">=</span> <span class=\"n\">Series</span><span class=\"p\">(</span><span class=\"n\">Counter</span><span class=\"p\">(</span><span class=\"n\">x</span> <span class=\"k\">for</span> <span class=\"n\">xs</span> <span class=\"ow\">in</span> <span class=\"n\">it</span><span class=\"o\">.</span><span class=\"n\">islice</span><span class=\"p\">(</span><span class=\"n\">gen_pd</span><span class=\"p\">,</span> <span class=\"n\">n</span> <span class=\"o\">//</span> <span class=\"mi\">100</span><span class=\"p\">)</span>\n                     <span class=\"k\">for</span> <span class=\"n\">x</span> <span class=\"ow\">in</span> <span class=\"n\">xs</span><span class=\"p\">))</span>\n<span class=\"n\">csnp</span> <span class=\"o\">=</span> <span class=\"n\">Series</span><span class=\"p\">(</span><span class=\"n\">Counter</span><span class=\"p\">(</span><span class=\"n\">x</span> <span class=\"k\">for</span> <span class=\"n\">xs</span> <span class=\"ow\">in</span> <span class=\"n\">it</span><span class=\"o\">.</span><span class=\"n\">islice</span><span class=\"p\">(</span><span class=\"n\">gen_npl</span><span class=\"p\">,</span> <span class=\"n\">n</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">x</span> <span class=\"ow\">in</span> <span class=\"n\">xs</span><span class=\"p\">))</span>\n<span class=\"n\">csj</span> <span class=\"o\">=</span> <span class=\"n\">Series</span><span class=\"p\">(</span><span class=\"n\">Counter</span><span class=\"p\">(</span><span class=\"n\">x</span> <span class=\"k\">for</span> <span class=\"n\">xs</span> <span class=\"ow\">in</span> <span class=\"n\">it</span><span class=\"o\">.</span><span class=\"n\">islice</span><span class=\"p\">(</span><span class=\"n\">gen_jita_pad</span><span class=\"p\">,</span> <span class=\"n\">n</span><span class=\"p\">)</span>\n                     <span class=\"k\">for</span> <span class=\"n\">x</span> <span class=\"ow\">in</span> <span class=\"n\">xs</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">:]))</span>\n<span class=\"n\">cs_ip</span> <span class=\"o\">=</span> <span class=\"n\">Series</span><span class=\"p\">(</span><span class=\"n\">Counter</span><span class=\"p\">(</span><span class=\"n\">x</span> <span class=\"k\">for</span> <span class=\"n\">xs</span> <span class=\"ow\">in</span> <span class=\"n\">it</span><span class=\"o\">.</span><span class=\"n\">islice</span><span class=\"p\">(</span><span class=\"n\">gen_jita_padi</span><span class=\"p\">,</span> <span class=\"n\">n</span><span class=\"p\">)</span>\n                       <span class=\"k\">for</span> <span class=\"n\">x</span> <span class=\"ow\">in</span> <span class=\"n\">xs</span><span class=\"p\">))</span>\n\n<span class=\"n\">ug</span> <span class=\"o\">=</span> <span class=\"n\">unigram</span><span class=\"p\">(</span><span class=\"n\">smtok</span><span class=\"p\">,</span> <span class=\"nb\">pow</span><span class=\"o\">=.</span><span class=\"mi\">75</span><span class=\"p\">)</span>\n<span class=\"n\">cts</span> <span class=\"o\">=</span> <span class=\"n\">DataFrame</span><span class=\"p\">({</span><span class=\"s\">&#39;Numba&#39;</span><span class=\"p\">:</span> <span class=\"n\">csj</span><span class=\"p\">,</span> <span class=\"s\">&#39;Numpy&#39;</span><span class=\"p\">:</span> <span class=\"n\">csnp</span><span class=\"p\">,</span> <span class=\"s\">&#39;Pandas&#39;</span><span class=\"p\">:</span> <span class=\"n\">csp</span><span class=\"p\">,</span>\n                 <span class=\"s\">&#39;Inplace&#39;</span><span class=\"p\">:</span> <span class=\"n\">cs_ip</span><span class=\"p\">})</span><span class=\"o\">.</span><span class=\"n\">fillna</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"n\">probs</span> <span class=\"o\">=</span> <span class=\"n\">cts</span> <span class=\"o\">/</span> <span class=\"n\">cts</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">()</span>\n<span class=\"n\">probs</span><span class=\"p\">[</span><span class=\"s\">&#39;Probs&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">ug</span><span class=\"o\">.</span><span class=\"n\">Prob</span> <span class=\"o\">/</span> <span class=\"n\">ug</span><span class=\"o\">.</span><span class=\"n\">Prob</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">()</span>\n</pre></div>\n\n</div>\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[24]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"k\">def</span> <span class=\"nf\">plot_dist</span><span class=\"p\">(</span><span class=\"n\">xcol</span><span class=\"o\">=</span><span class=\"k\">None</span><span class=\"p\">,</span> <span class=\"n\">subplt</span><span class=\"o\">=</span><span class=\"k\">None</span><span class=\"p\">):</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplot</span><span class=\"p\">(</span><span class=\"n\">subplt</span><span class=\"p\">)</span>\n    <span class=\"n\">probs</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">=</span><span class=\"n\">xcol</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"o\">=</span><span class=\"s\">&#39;Probs&#39;</span><span class=\"p\">,</span> <span class=\"n\">ax</span><span class=\"o\">=</span><span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">gca</span><span class=\"p\">(),</span> <span class=\"n\">kind</span><span class=\"o\">=</span><span class=\"s\">&#39;scatter&#39;</span><span class=\"p\">,</span>\n               <span class=\"n\">alpha</span><span class=\"o\">=.</span><span class=\"mi\">25</span><span class=\"p\">)</span>\n    <span class=\"n\">_</span><span class=\"p\">,</span> <span class=\"n\">xi</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">xlim</span><span class=\"p\">(</span><span class=\"k\">None</span><span class=\"p\">)</span>\n    <span class=\"n\">_</span><span class=\"p\">,</span> <span class=\"n\">yi</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">ylim</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"k\">None</span><span class=\"p\">)</span>\n    <span class=\"n\">end</span> <span class=\"o\">=</span> <span class=\"nb\">min</span><span class=\"p\">(</span><span class=\"n\">xi</span><span class=\"p\">,</span> <span class=\"n\">yi</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">end</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">end</span><span class=\"p\">],</span> <span class=\"n\">alpha</span><span class=\"o\">=.</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">xticks</span><span class=\"p\">(</span><span class=\"n\">rotation</span><span class=\"o\">=</span><span class=\"mi\">70</span><span class=\"p\">)</span>\n    \n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">figure</span><span class=\"p\">(</span><span class=\"n\">figsize</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">16</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">))</span>\n<span class=\"n\">plot_dist</span><span class=\"p\">(</span><span class=\"n\">xcol</span><span class=\"o\">=</span><span class=\"s\">&#39;Numba&#39;</span><span class=\"p\">,</span> <span class=\"n\">subplt</span><span class=\"o\">=</span><span class=\"mi\">141</span><span class=\"p\">)</span>\n<span class=\"n\">plot_dist</span><span class=\"p\">(</span><span class=\"n\">xcol</span><span class=\"o\">=</span><span class=\"s\">&#39;Numpy&#39;</span><span class=\"p\">,</span> <span class=\"n\">subplt</span><span class=\"o\">=</span><span class=\"mi\">142</span><span class=\"p\">)</span>\n<span class=\"n\">plot_dist</span><span class=\"p\">(</span><span class=\"n\">xcol</span><span class=\"o\">=</span><span class=\"s\">&#39;Pandas&#39;</span><span class=\"p\">,</span> <span class=\"n\">subplt</span><span class=\"o\">=</span><span class=\"mi\">143</span><span class=\"p\">)</span>\n<span class=\"n\">plot_dist</span><span class=\"p\">(</span><span class=\"n\">xcol</span><span class=\"o\">=</span><span class=\"s\">&#39;Inplace&#39;</span><span class=\"p\">,</span> <span class=\"n\">subplt</span><span class=\"o\">=</span><span class=\"mi\">144</span><span class=\"p\">)</span>\n</pre></div>\n\n</div>\n</div>\n</div>\n\n<div class=\"output_wrapper\">\n<div class=\"output\">\n\n\n<div class=\"output_area\"><div class=\"prompt\"></div>\n\n\n<div class=\"output_png output_subarea \">\n<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA8IAAAJ3CAYAAABfm+jfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n\" />\n</div>\n\n</div>\n\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[25]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"k\">del</span> <span class=\"n\">smtok</span><span class=\"p\">,</span> <span class=\"n\">le</span>\n</pre></div>\n\n</div>\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing text_cell rendered\">\n<div class=\"prompt input_prompt\">\n</div>\n<div class=\"inner_cell\">\n<div class=\"text_cell_render border-box-sizing rendered_html\">\n<p>As these plots show, the samplers seem to be drawing words according to the expected distribution. The pandas sampler was so slow that I had to reduce the number of draws by a couple orders of magnitude, so the plot is a lot noisier than the others, but still roughly on track. The in-place sampler appears to be slower on the microbenchmarks, but gives an improvement when restructuring the gradient update routine.</p>\n<h3 id=\"Sliding-window\">Sliding window<a class=\"anchor-link\" href=\"#Sliding-window\">&#182;</a></h3><p>It turned out another significant bottleneck of the gradient descent routine was the sliding window code that iterates over the entire corpus, yielding a word and its context words at each point.</p>\n<p>For the first few words in the text sequence, there are less than $C$ surrounding context words, so some checking is required. For the numba function, I used a specialized iterator at the beginning and end to avoid checking the max and min indices at every step.\nJust like in the negative sampler section, I first wrote the sliding window functions in a generator style, and converted the fastest version to an inplace modification style.</p>\n\n</div>\n</div>\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[26]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"k\">def</span> <span class=\"nf\">sliding_window</span><span class=\"p\">(</span><span class=\"n\">xs</span><span class=\"p\">,</span> <span class=\"n\">C</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">start_pos</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">):</span>\n    <span class=\"sd\">&quot;&quot;&quot;Iterate through corpus, yielding input word</span>\n<span class=\"sd\">    and surrounding context words&quot;&quot;&quot;</span>\n    <span class=\"n\">winsize</span> <span class=\"o\">=</span> <span class=\"n\">C</span> <span class=\"o\">//</span> <span class=\"mi\">2</span>\n    <span class=\"n\">N</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">xs</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">x</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">xs</span><span class=\"p\">,</span> <span class=\"n\">start_pos</span><span class=\"p\">):</span>\n        <span class=\"n\">ix1</span> <span class=\"o\">=</span> <span class=\"nb\">max</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">i</span><span class=\"o\">-</span><span class=\"n\">winsize</span><span class=\"p\">)</span>\n        <span class=\"n\">ix2</span> <span class=\"o\">=</span> <span class=\"nb\">min</span><span class=\"p\">(</span><span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">i</span><span class=\"o\">+</span><span class=\"n\">winsize</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n        <span class=\"k\">yield</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">xs</span><span class=\"p\">[</span><span class=\"n\">ix1</span><span class=\"p\">:</span><span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"n\">xs</span><span class=\"p\">[</span><span class=\"n\">i</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">:</span><span class=\"n\">ix2</span><span class=\"p\">]</span>\n\n<span class=\"nd\">@njit</span>\n<span class=\"k\">def</span> <span class=\"nf\">sliding_window_jit</span><span class=\"p\">(</span><span class=\"n\">xs</span><span class=\"p\">,</span> <span class=\"n\">C</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">):</span>\n    <span class=\"sd\">&quot;&quot;&quot;Iterates through corpus, yielding input word</span>\n<span class=\"sd\">    and surrounding context words&quot;&quot;&quot;</span>\n    <span class=\"n\">winsize</span> <span class=\"o\">=</span> <span class=\"n\">C</span> <span class=\"o\">//</span> <span class=\"mi\">2</span>\n    <span class=\"n\">N</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">xs</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">winsize</span><span class=\"p\">):</span>\n        <span class=\"k\">yield</span> <span class=\"n\">nbu</span><span class=\"o\">.</span><span class=\"n\">bounds_check_window</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">xs</span><span class=\"p\">,</span> <span class=\"n\">winsize</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">winsize</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"o\">-</span><span class=\"n\">winsize</span><span class=\"p\">):</span>\n        <span class=\"n\">context</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n        <span class=\"k\">for</span> <span class=\"n\">j</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"o\">-</span><span class=\"n\">winsize</span><span class=\"p\">,</span> <span class=\"n\">i</span><span class=\"o\">+</span><span class=\"n\">winsize</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">):</span>\n            <span class=\"k\">if</span> <span class=\"n\">j</span> <span class=\"o\">!=</span> <span class=\"n\">i</span><span class=\"p\">:</span>\n                <span class=\"n\">context</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">xs</span><span class=\"p\">[</span><span class=\"n\">j</span><span class=\"p\">])</span>\n        <span class=\"k\">yield</span> <span class=\"n\">xs</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">],</span> <span class=\"n\">context</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">N</span><span class=\"o\">-</span><span class=\"n\">winsize</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">):</span>\n        <span class=\"k\">yield</span> <span class=\"n\">nbu</span><span class=\"o\">.</span><span class=\"n\">bounds_check_window</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">xs</span><span class=\"p\">,</span> <span class=\"n\">winsize</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">)</span>\n\n<span class=\"nd\">@njit</span>\n<span class=\"k\">def</span> <span class=\"nf\">sliding_window_jit_arr</span><span class=\"p\">(</span><span class=\"n\">xs</span><span class=\"p\">,</span> <span class=\"n\">C</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">):</span>\n    <span class=\"sd\">&quot;&quot;&quot;Iterates through corpus, yielding input word</span>\n<span class=\"sd\">    and surrounding context words&quot;&quot;&quot;</span>\n    <span class=\"n\">winsize</span> <span class=\"o\">=</span> <span class=\"n\">C</span> <span class=\"o\">//</span> <span class=\"mi\">2</span>\n    <span class=\"n\">N</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">xs</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">winsize</span><span class=\"p\">):</span>\n        <span class=\"k\">yield</span> <span class=\"n\">nbu</span><span class=\"o\">.</span><span class=\"n\">bounds_check_window_arr</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">xs</span><span class=\"p\">,</span> <span class=\"n\">winsize</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">winsize</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"o\">-</span><span class=\"n\">winsize</span><span class=\"p\">):</span>\n        <span class=\"n\">context</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">(</span><span class=\"n\">C</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">int64</span><span class=\"p\">)</span>\n        <span class=\"k\">for</span> <span class=\"n\">ci</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">winsize</span><span class=\"p\">):</span>\n            <span class=\"n\">context</span><span class=\"p\">[</span><span class=\"n\">ci</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">xs</span><span class=\"p\">[</span><span class=\"n\">i</span> <span class=\"o\">-</span> <span class=\"n\">winsize</span> <span class=\"o\">+</span> <span class=\"n\">ci</span><span class=\"p\">]</span>\n            <span class=\"n\">context</span><span class=\"p\">[</span><span class=\"n\">winsize</span> <span class=\"o\">+</span> <span class=\"n\">ci</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">xs</span><span class=\"p\">[</span><span class=\"n\">i</span> <span class=\"o\">+</span> <span class=\"mi\">1</span> <span class=\"o\">+</span> <span class=\"n\">ci</span><span class=\"p\">]</span>\n        <span class=\"k\">yield</span> <span class=\"n\">xs</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">],</span> <span class=\"n\">context</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">N</span><span class=\"o\">-</span><span class=\"n\">winsize</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">):</span>\n        <span class=\"k\">yield</span> <span class=\"n\">nbu</span><span class=\"o\">.</span><span class=\"n\">bounds_check_window_arr</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">xs</span><span class=\"p\">,</span> <span class=\"n\">winsize</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">)</span>\n</pre></div>\n\n</div>\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[27]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"nd\">@njit</span>\n<span class=\"k\">def</span> <span class=\"nf\">sliding_window_inplace</span><span class=\"p\">(</span><span class=\"n\">xs</span><span class=\"p\">,</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">C</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">):</span>\n    <span class=\"n\">winsize</span> <span class=\"o\">=</span> <span class=\"n\">C</span> <span class=\"o\">//</span> <span class=\"mi\">2</span>\n    <span class=\"n\">N</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">xs</span><span class=\"p\">)</span>\n    <span class=\"k\">if</span> <span class=\"n\">i</span> <span class=\"o\">&lt;</span> <span class=\"n\">winsize</span><span class=\"p\">:</span>\n        <span class=\"k\">return</span> <span class=\"n\">nbu</span><span class=\"o\">.</span><span class=\"n\">bounds_check_window_arr</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">xs</span><span class=\"p\">,</span> <span class=\"n\">winsize</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">)</span>\n    <span class=\"k\">elif</span> <span class=\"n\">i</span> <span class=\"o\">&lt;</span> <span class=\"n\">N</span><span class=\"o\">-</span><span class=\"n\">winsize</span><span class=\"p\">:</span>\n        <span class=\"n\">context</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">(</span><span class=\"n\">C</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">int64</span><span class=\"p\">)</span>\n        <span class=\"k\">for</span> <span class=\"n\">ci</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">winsize</span><span class=\"p\">):</span>\n            <span class=\"n\">context</span><span class=\"p\">[</span><span class=\"n\">ci</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">xs</span><span class=\"p\">[</span><span class=\"n\">i</span> <span class=\"o\">-</span> <span class=\"n\">winsize</span> <span class=\"o\">+</span> <span class=\"n\">ci</span><span class=\"p\">]</span>\n            <span class=\"n\">context</span><span class=\"p\">[</span><span class=\"n\">winsize</span> <span class=\"o\">+</span> <span class=\"n\">ci</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">xs</span><span class=\"p\">[</span><span class=\"n\">i</span> <span class=\"o\">+</span> <span class=\"mi\">1</span> <span class=\"o\">+</span> <span class=\"n\">ci</span><span class=\"p\">]</span>\n        <span class=\"k\">return</span> <span class=\"n\">xs</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">],</span> <span class=\"n\">context</span>\n    <span class=\"k\">elif</span> <span class=\"n\">i</span> <span class=\"o\">&lt;</span> <span class=\"n\">N</span><span class=\"p\">:</span>\n        <span class=\"k\">return</span> <span class=\"n\">nbu</span><span class=\"o\">.</span><span class=\"n\">bounds_check_window_arr</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">xs</span><span class=\"p\">,</span> <span class=\"n\">winsize</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">)</span>\n    <span class=\"k\">raise</span> <span class=\"ne\">ValueError</span><span class=\"p\">(</span><span class=\"s\">&#39;Out of bounds&#39;</span><span class=\"p\">)</span>\n    \n<span class=\"k\">def</span> <span class=\"nf\">sliding_window_ix_</span><span class=\"p\">(</span><span class=\"n\">xs</span><span class=\"p\">,</span> <span class=\"n\">C</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"p\">(</span><span class=\"n\">sliding_window_inplace</span><span class=\"p\">(</span><span class=\"n\">xs</span><span class=\"p\">,</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">C</span><span class=\"o\">=</span><span class=\"n\">C</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">xs</span><span class=\"p\">)))</span>\n</pre></div>\n\n</div>\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[28]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"n\">samp_toks</span> <span class=\"o\">=</span> <span class=\"n\">nr</span><span class=\"o\">.</span><span class=\"n\">randint</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"n\">e6</span><span class=\"p\">,</span> <span class=\"n\">size</span><span class=\"o\">=</span><span class=\"mi\">100005</span><span class=\"p\">)</span>\n<span class=\"n\">samp_toksl</span> <span class=\"o\">=</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"n\">samp_toks</span><span class=\"p\">)</span>\n<span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"n\">sliding_window_jit</span><span class=\"p\">(</span><span class=\"n\">samp_toksl</span><span class=\"p\">[:</span><span class=\"mi\">10</span><span class=\"p\">],</span> <span class=\"n\">C</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">))</span>\n<span class=\"n\">run_window</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">f</span><span class=\"p\">,</span> <span class=\"n\">toks</span><span class=\"o\">=</span><span class=\"n\">samp_toksl</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"n\">ut</span><span class=\"o\">.</span><span class=\"n\">ilen</span><span class=\"p\">(</span><span class=\"n\">xs</span><span class=\"p\">)</span>\n                                         <span class=\"k\">for</span> <span class=\"n\">xs</span> <span class=\"ow\">in</span> <span class=\"n\">f</span><span class=\"p\">(</span><span class=\"n\">toks</span><span class=\"p\">,</span> <span class=\"n\">C</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">)]</span>\n</pre></div>\n\n</div>\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[29]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"n\">to_lst</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">xs</span><span class=\"p\">:</span> <span class=\"p\">[(</span><span class=\"n\">c</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"n\">ys</span><span class=\"p\">))</span> <span class=\"k\">for</span> <span class=\"n\">c</span><span class=\"p\">,</span> <span class=\"n\">ys</span> <span class=\"ow\">in</span> <span class=\"n\">xs</span><span class=\"p\">]</span>\n<span class=\"k\">assert</span> <span class=\"n\">to_lst</span><span class=\"p\">(</span><span class=\"n\">sliding_window_ix_</span><span class=\"p\">(</span><span class=\"n\">samp_toks</span><span class=\"p\">[:</span><span class=\"mi\">10</span><span class=\"p\">],</span> <span class=\"n\">C</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">)</span>\n             <span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"n\">to_lst</span><span class=\"p\">(</span><span class=\"n\">sliding_window_jit_arr</span><span class=\"p\">(</span><span class=\"n\">samp_toks</span><span class=\"p\">[:</span><span class=\"mi\">10</span><span class=\"p\">],</span> <span class=\"n\">C</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">))</span>\n</pre></div>\n\n</div>\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[30]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"o\">%</span><span class=\"k\">timeit</span> run_window(sliding_window)\n<span class=\"o\">%</span><span class=\"k\">timeit</span> run_window(sliding_window_jit)\n<span class=\"o\">%</span><span class=\"k\">timeit</span> run_window(sliding_window_jit_arr, toks=samp_toks)\n<span class=\"o\">%</span><span class=\"k\">timeit</span> run_window(sliding_window_ix_, toks=samp_toks)\n</pre></div>\n\n</div>\n</div>\n</div>\n\n<div class=\"output_wrapper\">\n<div class=\"output\">\n\n\n<div class=\"output_area\"><div class=\"prompt\"></div>\n<div class=\"output_subarea output_stream output_stdout output_text\">\n<pre>10 loops, best of 3: 178 ms per loop\n10 loops, best of 3: 133 ms per loop\n10 loops, best of 3: 109 ms per loop\n10 loops, best of 3: 167 ms per loop\n</pre>\n</div>\n</div>\n\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing text_cell rendered\">\n<div class=\"prompt input_prompt\">\n</div>\n<div class=\"inner_cell\">\n<div class=\"text_cell_render border-box-sizing rendered_html\">\n<h3 id=\"Norm\">Norm<a class=\"anchor-link\" href=\"#Norm\">&#182;</a></h3><p>Since I'm using the concatenated input and output matrix, half of the <code>w, c, negsamps</code> submatrix entries are 0. On each iteration I check the gradient matrix's norm to clip the gradient if necessary. A specialized norm function taking this structure into account gives about an order of magnitude speedup over the numpy one.</p>\n\n</div>\n</div>\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[31]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"nd\">@njit</span>\n<span class=\"k\">def</span> <span class=\"nf\">grad_norm</span><span class=\"p\">(</span><span class=\"n\">Wsub</span><span class=\"p\">):</span>\n    <span class=\"sd\">&quot;&quot;&quot;Calculate norm of gradient, where first row</span>\n<span class=\"sd\">    is input vector, rest are output vectors. For any row,</span>\n<span class=\"sd\">    half of the entries are zeros, which allows a lot of</span>\n<span class=\"sd\">    skipping for a faster computation&quot;&quot;&quot;</span>\n    <span class=\"n\">n</span> <span class=\"o\">=</span> <span class=\"n\">Wsub</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">//</span> <span class=\"mi\">2</span>\n    <span class=\"n\">sm</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">):</span>\n        <span class=\"n\">sm</span> <span class=\"o\">+=</span> <span class=\"n\">Wsub</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"o\">**</span> <span class=\"mi\">2</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">Wsub</span><span class=\"p\">)):</span>\n        <span class=\"k\">for</span> <span class=\"n\">j</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">,</span> <span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">n</span><span class=\"p\">):</span>\n            <span class=\"n\">sm</span> <span class=\"o\">+=</span> <span class=\"n\">Wsub</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">j</span><span class=\"p\">]</span> <span class=\"o\">**</span> <span class=\"mi\">2</span>\n    <span class=\"k\">return</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">sm</span><span class=\"p\">)</span>\n</pre></div>\n\n</div>\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[32]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"n\">grd</span> <span class=\"o\">=</span> <span class=\"n\">ns_grad_jit</span><span class=\"p\">(</span><span class=\"n\">Wsub</span><span class=\"p\">)</span>\n<span class=\"k\">assert</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">isclose</span><span class=\"p\">(</span><span class=\"n\">grad_norm</span><span class=\"p\">(</span><span class=\"n\">grd</span><span class=\"p\">),</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">norm</span><span class=\"p\">(</span><span class=\"n\">grad_norm</span><span class=\"p\">(</span><span class=\"n\">grd</span><span class=\"p\">)))</span>\n\n<span class=\"o\">%</span><span class=\"k\">timeit</span> np.linalg.norm(grad_norm(grd))\n<span class=\"o\">%</span><span class=\"k\">timeit</span> grad_norm(grd)\n</pre></div>\n\n</div>\n</div>\n</div>\n\n<div class=\"output_wrapper\">\n<div class=\"output\">\n\n\n<div class=\"output_area\"><div class=\"prompt\"></div>\n<div class=\"output_subarea output_stream output_stdout output_text\">\n<pre>The slowest run took 5.81 times longer than the fastest. This could mean that an intermediate result is being cached \n100000 loops, best of 3: 4.69 µs per loop\nThe slowest run took 4.25 times longer than the fastest. This could mean that an intermediate result is being cached \n1000000 loops, best of 3: 659 ns per loop\n</pre>\n</div>\n</div>\n\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing text_cell rendered\">\n<div class=\"prompt input_prompt\">\n</div>\n<div class=\"inner_cell\">\n<div class=\"text_cell_render border-box-sizing rendered_html\">\n<h2 id=\"Gradient-descent\">Gradient descent<a class=\"anchor-link\" href=\"#Gradient-descent\">&#182;</a></h2><p>That does it for optimizing most of the individual pieces.\nFor the gradient descent routine, I'm passing all the hyper-parameters through a configuration dictionary, validated by <a href=\"https://pypi.python.org/pypi/voluptuous\">voluptuous</a>. This lets me specify of things like the context window size, learning rate, number of negative samples and size of the word vectors. Check the utility file <code>wordvec_utils.py</code> where it's defined for details on the meanings of the parameters.</p>\n\n</div>\n</div>\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[33]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"n\">all_text</span> <span class=\"o\">=</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"n\">brown</span><span class=\"o\">.</span><span class=\"n\">words</span><span class=\"p\">())</span>\n<span class=\"n\">use_words_</span> <span class=\"o\">=</span> <span class=\"n\">z</span><span class=\"o\">.</span><span class=\"n\">valfilter</span><span class=\"p\">(</span><span class=\"k\">lambda</span> <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">x</span> <span class=\"o\">&gt;=</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">Counter</span><span class=\"p\">(</span><span class=\"n\">all_text</span><span class=\"p\">))</span>\n<span class=\"n\">use_words</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">w</span> <span class=\"k\">for</span> <span class=\"n\">w</span> <span class=\"ow\">in</span> <span class=\"n\">all_text</span> <span class=\"k\">if</span> <span class=\"n\">w</span> <span class=\"ow\">in</span> <span class=\"n\">use_words_</span><span class=\"p\">]</span>\n<span class=\"n\">le</span> <span class=\"o\">=</span> <span class=\"n\">LabelEncoder</span><span class=\"p\">()</span>\n<span class=\"n\">toka</span> <span class=\"o\">=</span> <span class=\"n\">le</span><span class=\"o\">.</span><span class=\"n\">fit_transform</span><span class=\"p\">(</span><span class=\"n\">use_words</span><span class=\"p\">)</span>\n<span class=\"n\">tokl</span> <span class=\"o\">=</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"n\">toka</span><span class=\"p\">)</span>\n<span class=\"n\">vc</span> <span class=\"o\">=</span> <span class=\"n\">le</span><span class=\"o\">.</span><span class=\"n\">classes_</span>\n</pre></div>\n\n</div>\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[34]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"n\">cnf</span> <span class=\"o\">=</span> <span class=\"n\">ut</span><span class=\"o\">.</span><span class=\"n\">AttrDict</span><span class=\"p\">(</span>\n    <span class=\"n\">eta</span><span class=\"o\">=.</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">min_eta</span><span class=\"o\">=.</span><span class=\"mi\">0001</span><span class=\"p\">,</span> <span class=\"n\">accumsec</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n    <span class=\"n\">N</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"n\">C</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"o\">=</span><span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"nb\">iter</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">thresh</span><span class=\"o\">=</span><span class=\"mi\">15</span><span class=\"p\">,</span> <span class=\"n\">epoch</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n    <span class=\"n\">pad</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n    <span class=\"n\">term</span><span class=\"o\">=</span><span class=\"nb\">dict</span><span class=\"p\">(</span><span class=\"n\">iters</span><span class=\"o\">=</span><span class=\"k\">None</span><span class=\"p\">,</span>\n              <span class=\"n\">secs</span><span class=\"o\">=</span><span class=\"k\">None</span>\n    <span class=\"p\">),</span>\n    <span class=\"nb\">dir</span><span class=\"o\">=</span><span class=\"s\">&#39;cache&#39;</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n<span class=\"n\">cnf</span> <span class=\"o\">=</span> <span class=\"n\">Conf</span><span class=\"p\">(</span><span class=\"n\">cnf</span><span class=\"p\">)</span>\n<span class=\"n\">cnf_</span> <span class=\"o\">=</span> <span class=\"n\">cnf</span>\n<span class=\"k\">del</span> <span class=\"n\">cnf</span>\n<span class=\"n\">cfp</span> <span class=\"o\">=</span> <span class=\"n\">update</span><span class=\"p\">(</span><span class=\"n\">cnf_</span><span class=\"p\">,</span> <span class=\"n\">pad</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">term</span><span class=\"o\">=</span><span class=\"p\">{})</span>\n\n<span class=\"n\">W</span> <span class=\"o\">=</span> <span class=\"n\">wut</span><span class=\"o\">.</span><span class=\"n\">init_w</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">vc</span><span class=\"p\">),</span> <span class=\"n\">cnf_</span><span class=\"o\">.</span><span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">seed</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"n\">We</span> <span class=\"o\">=</span> <span class=\"n\">W</span><span class=\"o\">.</span><span class=\"n\">copy</span><span class=\"p\">()</span>\n</pre></div>\n\n</div>\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing text_cell rendered\">\n<div class=\"prompt input_prompt\">\n</div>\n<div class=\"inner_cell\">\n<div class=\"text_cell_render border-box-sizing rendered_html\">\n<h3 id=\"Sgd\">Sgd<a class=\"anchor-link\" href=\"#Sgd\">&#182;</a></h3><p>One final speedup I got was from putting the entire inner loop routine into a numba JIT'd function, instead of calling each JIT'd function separately. Numba seems to reduce some of python's function call overhead, as well as the numpy indexing. For some reason, merely indexing into a numpy array (to calculate the gradient and then update the original matrix) still ended up being a significant bottleneck, though numba seemed to reduce it.</p>\n<p>Since Numba doesn't allow passing functions in as parameters, <code>mk_grad_update</code> is a higher order vanilla Python function that returns either a regular numpy function, or the same function with a numba decorator and a numba gradient function in the closure.</p>\n\n</div>\n</div>\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[35]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"k\">def</span> <span class=\"nf\">mk_grad_update</span><span class=\"p\">(</span><span class=\"n\">jit</span><span class=\"o\">=</span><span class=\"k\">False</span><span class=\"p\">,</span> <span class=\"n\">grad_func</span><span class=\"o\">=</span><span class=\"n\">ns_grad</span><span class=\"p\">):</span>\n    <span class=\"k\">if</span> <span class=\"n\">jit</span><span class=\"p\">:</span>\n        <span class=\"n\">grad_func</span> <span class=\"o\">=</span> <span class=\"n\">ns_grad_jit</span>\n        <span class=\"n\">norm_func</span> <span class=\"o\">=</span> <span class=\"n\">grad_norm</span>\n        <span class=\"n\">deco</span> <span class=\"o\">=</span> <span class=\"n\">njit</span>\n    <span class=\"k\">else</span><span class=\"p\">:</span>\n        <span class=\"n\">norm_func</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">norm</span>\n        <span class=\"n\">deco</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">x</span>\n        \n    <span class=\"k\">def</span> <span class=\"nf\">grad_update</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">sub_ixs</span><span class=\"p\">,</span> <span class=\"n\">eta</span><span class=\"p\">):</span>\n        <span class=\"n\">Wsub</span> <span class=\"o\">=</span> <span class=\"n\">W</span><span class=\"p\">[</span><span class=\"n\">sub_ixs</span><span class=\"p\">]</span>\n        <span class=\"n\">grad</span> <span class=\"o\">=</span> <span class=\"n\">grad_func</span><span class=\"p\">(</span><span class=\"n\">Wsub</span><span class=\"p\">)</span>\n        <span class=\"n\">gnorm</span> <span class=\"o\">=</span> <span class=\"n\">norm_func</span><span class=\"p\">(</span><span class=\"n\">grad</span><span class=\"p\">)</span>\n        <span class=\"k\">if</span> <span class=\"n\">gnorm</span> <span class=\"o\">&gt;</span> <span class=\"mi\">5</span><span class=\"p\">:</span>  <span class=\"c\"># clip gradient</span>\n            <span class=\"n\">grad</span> <span class=\"o\">/=</span> <span class=\"n\">gnorm</span>\n        <span class=\"n\">W</span><span class=\"p\">[</span><span class=\"n\">sub_ixs</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">Wsub</span> <span class=\"o\">-</span> <span class=\"n\">eta</span> <span class=\"o\">*</span> <span class=\"n\">grad</span>\n        \n    <span class=\"k\">return</span> <span class=\"n\">deco</span><span class=\"p\">(</span><span class=\"n\">grad_update</span><span class=\"p\">)</span>\n\n<span class=\"n\">grad_update</span> <span class=\"o\">=</span> <span class=\"n\">mk_grad_update</span><span class=\"p\">(</span><span class=\"n\">jit</span><span class=\"o\">=</span><span class=\"k\">False</span><span class=\"p\">)</span>\n<span class=\"n\">grad_update_jit</span> <span class=\"o\">=</span> <span class=\"n\">mk_grad_update</span><span class=\"p\">(</span><span class=\"n\">jit</span><span class=\"o\">=</span><span class=\"k\">True</span><span class=\"p\">)</span>\n</pre></div>\n\n</div>\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[36]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"nd\">@njit</span>\n<span class=\"k\">def</span> <span class=\"nf\">grad_update_jit_pad</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">sub_ixs</span><span class=\"p\">,</span> <span class=\"n\">eta</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">):</span>\n    <span class=\"sd\">&quot;&quot;&quot;If focus or context word are contained in negative samples,</span>\n<span class=\"sd\">    drop them before performing the grad update.</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n    <span class=\"n\">sub_ixs</span> <span class=\"o\">=</span> <span class=\"n\">nbu</span><span class=\"o\">.</span><span class=\"n\">remove_dupes</span><span class=\"p\">(</span><span class=\"n\">sub_ixs</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"p\">)</span>\n    <span class=\"n\">grad_update_jit</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">sub_ixs</span><span class=\"p\">,</span> <span class=\"n\">eta</span><span class=\"p\">)</span>\n    \n<span class=\"k\">def</span> <span class=\"nf\">inner_update</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">negsamps</span><span class=\"p\">,</span> <span class=\"n\">eta</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">):</span>\n    <span class=\"sd\">&quot;&quot;&quot;Numpy inner loop gradient update function.</span>\n<span class=\"sd\">    I.e., slow version of `grad_update_jit_pad`&quot;&quot;&quot;</span>\n    <span class=\"c\">#print(negsamps, eta, w, c)</span>\n    <span class=\"k\">if</span> <span class=\"p\">(</span><span class=\"n\">w</span> <span class=\"ow\">in</span> <span class=\"n\">negsamps</span><span class=\"p\">)</span> <span class=\"ow\">or</span> <span class=\"p\">(</span><span class=\"n\">c</span> <span class=\"ow\">in</span> <span class=\"n\">negsamps</span><span class=\"p\">):</span>\n        <span class=\"n\">negsamps</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">x</span> <span class=\"k\">for</span> <span class=\"n\">x</span> <span class=\"ow\">in</span> <span class=\"n\">negsamps</span> <span class=\"k\">if</span> <span class=\"n\">x</span> <span class=\"ow\">not</span> <span class=\"ow\">in</span> <span class=\"p\">{</span><span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"p\">}]</span>\n    <span class=\"n\">sub_ixs</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"n\">negsamps</span><span class=\"p\">)</span> <span class=\"c\"># list(negsamps)</span>\n    <span class=\"n\">grad_update</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">sub_ixs</span><span class=\"p\">,</span> <span class=\"n\">eta</span><span class=\"p\">)</span>\n    \n<span class=\"k\">def</span> <span class=\"nf\">check_padding</span><span class=\"p\">(</span><span class=\"n\">sampler</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">grad_update</span><span class=\"p\">,</span> <span class=\"n\">dims</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">):</span>\n    <span class=\"s\">&quot;Poor dependent type checker&quot;</span>\n    <span class=\"n\">_samp</span> <span class=\"o\">=</span> <span class=\"nb\">next</span><span class=\"p\">(</span><span class=\"n\">sampler</span><span class=\"p\">)[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"k\">if</span> <span class=\"n\">dims</span> <span class=\"o\">==</span> <span class=\"mi\">2</span> <span class=\"k\">else</span> <span class=\"nb\">next</span><span class=\"p\">(</span><span class=\"n\">sampler</span><span class=\"p\">)</span>\n    <span class=\"n\">k_</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">_samp</span><span class=\"p\">)</span>\n    <span class=\"n\">padded</span> <span class=\"o\">=</span> <span class=\"n\">k_</span> <span class=\"o\">==</span> <span class=\"mi\">2</span> <span class=\"o\">+</span> <span class=\"n\">k</span>\n    <span class=\"k\">assert</span> <span class=\"n\">k_</span> <span class=\"o\">==</span> <span class=\"n\">sampler</span><span class=\"o\">.</span><span class=\"n\">pad</span> <span class=\"o\">+</span> <span class=\"n\">sampler</span><span class=\"o\">.</span><span class=\"n\">K</span>\n    <span class=\"k\">assert</span> <span class=\"n\">k</span> <span class=\"o\">==</span> <span class=\"n\">sampler</span><span class=\"o\">.</span><span class=\"n\">K</span>\n    <span class=\"k\">assert</span> <span class=\"n\">k_</span> <span class=\"ow\">in</span> <span class=\"p\">(</span><span class=\"mi\">2</span> <span class=\"o\">+</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s\">&#39;Length of samples&#39;</span>\n        <span class=\"s\">&#39; should be either `k` or `k` + 2 if padded&#39;</span><span class=\"p\">)</span>\n    <span class=\"k\">assert</span> <span class=\"n\">padded</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"n\">grad_update</span> <span class=\"ow\">in</span> <span class=\"n\">padded_updates</span><span class=\"p\">),</span> <span class=\"p\">(</span>\n        <span class=\"s\">&#39;Make sure sampler size agrees with grad_update&#39;</span><span class=\"p\">)</span>\n\n<span class=\"n\">padded_updates</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"n\">grad_update_jit_pad</span><span class=\"p\">}</span>\n\n<span class=\"n\">gen_npl</span> <span class=\"o\">=</span> <span class=\"n\">NegSampler</span><span class=\"p\">(</span><span class=\"n\">neg_sampler_np</span><span class=\"p\">,</span> <span class=\"n\">tokl</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"o\">=</span><span class=\"n\">cnf_</span><span class=\"o\">.</span><span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">ret_type</span><span class=\"o\">=</span><span class=\"nb\">list</span><span class=\"p\">)</span>\n<span class=\"n\">numpy_opts</span> <span class=\"o\">=</span> <span class=\"nb\">dict</span><span class=\"p\">(</span><span class=\"n\">ns_grad_</span><span class=\"o\">=</span><span class=\"n\">ns_grad</span><span class=\"p\">,</span> <span class=\"n\">neg_sampler</span><span class=\"o\">=</span><span class=\"n\">gen_npl</span><span class=\"p\">,</span>\n                  <span class=\"n\">sliding_window</span><span class=\"o\">=</span><span class=\"n\">sliding_window</span><span class=\"p\">,</span>\n                  <span class=\"n\">grad_update</span><span class=\"o\">=</span><span class=\"n\">inner_update</span><span class=\"p\">)</span>\n\n<span class=\"n\">ngsamp_pad</span> <span class=\"o\">=</span> <span class=\"n\">NegSampler</span><span class=\"p\">(</span><span class=\"n\">neg_sampler_jit_pad</span><span class=\"p\">,</span> <span class=\"n\">tokl</span><span class=\"p\">,</span> <span class=\"n\">cfp</span><span class=\"o\">.</span><span class=\"n\">K</span><span class=\"p\">,</span>\n                        <span class=\"n\">ret_type</span><span class=\"o\">=</span><span class=\"n\">Array</span><span class=\"p\">,</span> <span class=\"n\">pad</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"n\">fast_opts</span> <span class=\"o\">=</span> <span class=\"nb\">dict</span><span class=\"p\">(</span><span class=\"n\">ns_grad_</span><span class=\"o\">=</span><span class=\"n\">ns_grad_jit</span><span class=\"p\">,</span> <span class=\"n\">neg_sampler</span><span class=\"o\">=</span><span class=\"n\">ngsamp_pad</span><span class=\"p\">,</span>\n                 <span class=\"n\">sliding_window</span><span class=\"o\">=</span><span class=\"n\">sliding_window_jit</span><span class=\"p\">,</span>\n                 <span class=\"n\">grad_update</span><span class=\"o\">=</span><span class=\"n\">grad_update_jit_pad</span><span class=\"p\">)</span>\n</pre></div>\n\n</div>\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing text_cell rendered\">\n<div class=\"prompt input_prompt\">\n</div>\n<div class=\"inner_cell\">\n<div class=\"text_cell_render border-box-sizing rendered_html\">\n<p>And finally, here is the full gradient descent function put together. The numpy version, <code>sgd_np</code>, has some vestigial code and options from earlier iterations where I was doing more debugging. <code>mk_sgd_inplace</code> returns a more straightforward Numba sgd function.</p>\n\n</div>\n</div>\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[37]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"k\">def</span> <span class=\"nf\">sgd_np</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"o\">=</span><span class=\"k\">None</span><span class=\"p\">,</span> <span class=\"n\">corp</span><span class=\"o\">=</span><span class=\"k\">None</span><span class=\"p\">,</span> <span class=\"n\">cf</span><span class=\"o\">=</span><span class=\"p\">{},</span> <span class=\"n\">ns_grad_</span><span class=\"o\">=</span><span class=\"n\">ns_grad</span><span class=\"p\">,</span> <span class=\"n\">neg_sampler</span><span class=\"o\">=</span><span class=\"k\">None</span><span class=\"p\">,</span>\n        <span class=\"n\">vc</span><span class=\"o\">=</span><span class=\"k\">None</span><span class=\"p\">,</span> <span class=\"n\">sliding_window</span><span class=\"o\">=</span><span class=\"n\">sliding_window</span><span class=\"p\">,</span>\n        <span class=\"n\">grad_update</span><span class=\"o\">=</span><span class=\"n\">grad_update_jit_pad</span><span class=\"p\">):</span>\n    <span class=\"n\">check_padding</span><span class=\"p\">(</span><span class=\"n\">neg_sampler</span><span class=\"p\">,</span> <span class=\"n\">cf</span><span class=\"o\">.</span><span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">grad_update</span><span class=\"p\">)</span>\n    <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">exists</span><span class=\"p\">(</span><span class=\"n\">cf</span><span class=\"o\">.</span><span class=\"n\">dir</span><span class=\"p\">):</span>\n        <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">mkdir</span><span class=\"p\">(</span><span class=\"n\">cf</span><span class=\"o\">.</span><span class=\"n\">dir</span><span class=\"p\">)</span>\n    <span class=\"n\">st</span> <span class=\"o\">=</span> <span class=\"n\">time</span><span class=\"o\">.</span><span class=\"n\">time</span><span class=\"p\">();</span> <span class=\"n\">cf</span> <span class=\"o\">=</span> <span class=\"n\">Conf</span><span class=\"p\">(</span><span class=\"n\">cf</span><span class=\"p\">)</span>  <span class=\"c\">#.copy()</span>\n    <span class=\"k\">assert</span> <span class=\"n\">cf</span><span class=\"o\">.</span><span class=\"n\">N</span> <span class=\"o\">==</span> <span class=\"n\">W</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">/</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"s\">&#39;shape of W disagrees with conf&#39;</span>\n    \n    <span class=\"n\">iter_corpus</span> <span class=\"o\">=</span> <span class=\"n\">corp</span><span class=\"p\">[</span><span class=\"n\">cf</span><span class=\"o\">.</span><span class=\"n\">iter</span><span class=\"p\">:]</span> <span class=\"k\">if</span> <span class=\"n\">cf</span><span class=\"o\">.</span><span class=\"n\">iter</span> <span class=\"k\">else</span> <span class=\"n\">corp</span>\n    <span class=\"n\">learning_rates</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linspace</span><span class=\"p\">(</span><span class=\"n\">cf</span><span class=\"o\">.</span><span class=\"n\">eta</span><span class=\"p\">,</span> <span class=\"n\">cf</span><span class=\"o\">.</span><span class=\"n\">min_eta</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">iter_corpus</span><span class=\"p\">))</span>\n    <span class=\"k\">assert</span> <span class=\"n\">neg_sampler</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"k\">None</span><span class=\"p\">,</span> <span class=\"s\">&quot;Give me a negative sampler!&quot;</span>\n    \n    <span class=\"n\">iters_</span> <span class=\"o\">=</span> <span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">count</span><span class=\"p\">(</span><span class=\"n\">cf</span><span class=\"o\">.</span><span class=\"n\">iter</span><span class=\"p\">),</span>\n                  <span class=\"n\">sliding_window</span><span class=\"p\">(</span><span class=\"n\">iter_corpus</span><span class=\"p\">,</span> <span class=\"n\">C</span><span class=\"o\">=</span><span class=\"n\">cf</span><span class=\"o\">.</span><span class=\"n\">C</span><span class=\"p\">),</span>\n                  <span class=\"n\">z</span><span class=\"o\">.</span><span class=\"n\">partition</span><span class=\"p\">(</span><span class=\"n\">cf</span><span class=\"o\">.</span><span class=\"n\">C</span><span class=\"p\">,</span> <span class=\"n\">neg_sampler</span><span class=\"p\">),</span>\n                  <span class=\"n\">learning_rates</span><span class=\"p\">,</span>\n                 <span class=\"p\">)</span>\n    <span class=\"n\">iters</span> <span class=\"o\">=</span> <span class=\"n\">ut</span><span class=\"o\">.</span><span class=\"n\">timeloop</span><span class=\"p\">(</span><span class=\"n\">iters_</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">cf</span><span class=\"o\">.</span><span class=\"n\">term</span><span class=\"p\">)</span>\n\n    <span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">cont_</span><span class=\"p\">),</span> <span class=\"n\">negsamp_lst</span><span class=\"p\">,</span> <span class=\"n\">eta</span> <span class=\"ow\">in</span> <span class=\"n\">iters</span><span class=\"p\">:</span>\n        <span class=\"n\">cont</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">x</span> <span class=\"k\">for</span> <span class=\"n\">x</span> <span class=\"ow\">in</span> <span class=\"n\">cont_</span> <span class=\"k\">if</span> <span class=\"n\">x</span> <span class=\"o\">!=</span> <span class=\"n\">w</span><span class=\"p\">]</span> <span class=\"k\">if</span> <span class=\"n\">w</span> <span class=\"ow\">in</span> <span class=\"n\">cont_</span> <span class=\"k\">else</span> <span class=\"n\">cont_</span>\n        <span class=\"k\">for</span> <span class=\"n\">c</span><span class=\"p\">,</span> <span class=\"n\">negsamps</span> <span class=\"ow\">in</span> <span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">cont</span><span class=\"p\">,</span> <span class=\"n\">negsamp_lst</span><span class=\"p\">):</span>\n            <span class=\"n\">grad_update</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">negsamps</span><span class=\"p\">,</span> <span class=\"n\">eta</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"o\">=</span><span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"o\">=</span><span class=\"n\">c</span><span class=\"p\">)</span>\n\n    <span class=\"n\">tdur</span> <span class=\"o\">=</span> <span class=\"n\">time</span><span class=\"o\">.</span><span class=\"n\">time</span><span class=\"p\">()</span> <span class=\"o\">-</span> <span class=\"n\">st</span>\n    <span class=\"n\">cf2</span> <span class=\"o\">=</span> <span class=\"n\">update</span><span class=\"p\">(</span><span class=\"n\">cf</span><span class=\"p\">,</span> <span class=\"nb\">iter</span><span class=\"o\">=</span><span class=\"n\">i</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)</span>  <span class=\"c\"># , norms=norms, gradnorms=gradnorms</span>\n    <span class=\"n\">cf2</span><span class=\"p\">[</span><span class=\"s\">&#39;accumsec&#39;</span><span class=\"p\">]</span> <span class=\"o\">+=</span> <span class=\"n\">tdur</span>\n    <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">cf2</span><span class=\"o\">.</span><span class=\"n\">term</span> <span class=\"ow\">and</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n        <span class=\"n\">fn</span> <span class=\"o\">=</span> <span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">cf2</span><span class=\"o\">.</span><span class=\"n\">dir</span><span class=\"p\">,</span> <span class=\"s\">&#39;n{}_e{}.csv&#39;</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">cf</span><span class=\"o\">.</span><span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">cf</span><span class=\"o\">.</span><span class=\"n\">epoch</span><span class=\"p\">))</span>\n        <span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">index</span><span class=\"o\">=</span><span class=\"n\">vc</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to_csv</span><span class=\"p\">(</span><span class=\"n\">fn</span><span class=\"p\">)</span>\n        <span class=\"n\">cf2</span><span class=\"p\">[</span><span class=\"s\">&#39;epoch&#39;</span><span class=\"p\">]</span> <span class=\"o\">+=</span> <span class=\"mi\">1</span>\n        <span class=\"n\">cf2</span> <span class=\"o\">=</span> <span class=\"n\">update</span><span class=\"p\">(</span><span class=\"n\">cf2</span><span class=\"p\">,</span> <span class=\"nb\">iter</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n    <span class=\"k\">else</span><span class=\"p\">:</span>\n        <span class=\"k\">pass</span>\n    <span class=\"c\">#     print(i, &#39;iters&#39;)</span>\n    <span class=\"k\">return</span> <span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">cf2</span>\n\n<span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">sgd_np</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"o\">=</span><span class=\"n\">We</span><span class=\"o\">.</span><span class=\"n\">copy</span><span class=\"p\">(),</span> <span class=\"n\">corp</span><span class=\"o\">=</span><span class=\"n\">tokl</span><span class=\"p\">,</span> <span class=\"n\">cf</span><span class=\"o\">=</span><span class=\"n\">update</span><span class=\"p\">(</span><span class=\"n\">cnf_</span><span class=\"p\">,</span> <span class=\"n\">term</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s\">&#39;iters&#39;</span><span class=\"p\">:</span> <span class=\"mi\">1000</span><span class=\"p\">}),</span>\n        <span class=\"o\">**</span><span class=\"n\">fast_opts</span><span class=\"p\">)</span>\n</pre></div>\n\n</div>\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[38]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"k\">def</span> <span class=\"nf\">mk_sgd_inplace</span><span class=\"p\">(</span><span class=\"n\">cf</span><span class=\"o\">=</span><span class=\"k\">None</span><span class=\"p\">,</span> <span class=\"n\">sampler</span><span class=\"o\">=</span><span class=\"k\">None</span><span class=\"p\">):</span>\n    <span class=\"n\">iters</span> <span class=\"o\">=</span> <span class=\"n\">cf</span><span class=\"o\">.</span><span class=\"n\">term</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s\">&#39;iters&#39;</span><span class=\"p\">)</span> <span class=\"ow\">or</span> <span class=\"mi\">0</span>\n    \n    <span class=\"nd\">@njit</span>\n    <span class=\"k\">def</span> <span class=\"nf\">loop</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">toka</span><span class=\"p\">,</span> <span class=\"n\">eta</span><span class=\"o\">=</span><span class=\"k\">None</span><span class=\"p\">,</span> <span class=\"n\">min_eta</span><span class=\"o\">=</span><span class=\"k\">None</span><span class=\"p\">,</span> <span class=\"n\">C</span><span class=\"o\">=</span><span class=\"k\">None</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"o\">=</span><span class=\"k\">None</span><span class=\"p\">,</span> <span class=\"n\">pad</span><span class=\"o\">=</span><span class=\"k\">None</span><span class=\"p\">):</span>\n        <span class=\"n\">N</span> <span class=\"o\">=</span> <span class=\"n\">iters</span> <span class=\"ow\">or</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">toka</span><span class=\"p\">)</span>\n        <span class=\"n\">sub_ixs</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">(</span><span class=\"n\">K</span> <span class=\"o\">+</span> <span class=\"n\">pad</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">int64</span><span class=\"p\">)</span>\n        <span class=\"n\">etas</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linspace</span><span class=\"p\">(</span><span class=\"n\">eta</span><span class=\"p\">,</span> <span class=\"n\">min_eta</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">)</span>\n        <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">N</span><span class=\"p\">):</span>\n            <span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">conts</span> <span class=\"o\">=</span> <span class=\"n\">sliding_window_inplace</span><span class=\"p\">(</span><span class=\"n\">toka</span><span class=\"p\">,</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">C</span><span class=\"o\">=</span><span class=\"n\">C</span><span class=\"p\">)</span>\n            <span class=\"n\">eta_</span> <span class=\"o\">=</span> <span class=\"n\">etas</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span>\n            <span class=\"k\">for</span> <span class=\"n\">c</span> <span class=\"ow\">in</span> <span class=\"n\">conts</span><span class=\"p\">:</span>\n                <span class=\"n\">sampler</span><span class=\"p\">(</span><span class=\"n\">sub_ixs</span><span class=\"p\">)</span>\n                <span class=\"n\">grad_update_jit_pad</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">sub_ixs</span><span class=\"p\">,</span> <span class=\"n\">eta_</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"o\">=</span><span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"o\">=</span><span class=\"n\">c</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"n\">W</span>\n    <span class=\"n\">kwa</span> <span class=\"o\">=</span> <span class=\"n\">z</span><span class=\"o\">.</span><span class=\"n\">keyfilter</span><span class=\"p\">(</span><span class=\"k\">lambda</span> <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">x</span> <span class=\"ow\">in</span> <span class=\"s\">&#39;eta min_eta C K pad&#39;</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">(),</span> <span class=\"n\">cf</span><span class=\"p\">)</span>\n    <span class=\"n\">looper</span> <span class=\"o\">=</span> <span class=\"n\">partial</span><span class=\"p\">(</span><span class=\"n\">loop</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwa</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">looper</span>\n\n<span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">inplace_sampler</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">neg_sampler_inplace</span><span class=\"p\">(</span>\n        <span class=\"n\">toka</span><span class=\"p\">,</span> <span class=\"n\">cfp</span><span class=\"o\">.</span><span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"nb\">pow</span><span class=\"o\">=.</span><span class=\"mi\">75</span><span class=\"p\">,</span> <span class=\"n\">pad</span><span class=\"o\">=</span><span class=\"n\">cfp</span><span class=\"o\">.</span><span class=\"n\">pad</span><span class=\"p\">,</span> <span class=\"n\">ret_type</span><span class=\"o\">=</span><span class=\"n\">Array</span><span class=\"p\">,</span> <span class=\"n\">seed</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">))</span>\n<span class=\"n\">sgd_inplace</span> <span class=\"o\">=</span> <span class=\"n\">mk_sgd_inplace</span><span class=\"p\">(</span><span class=\"n\">cfp</span><span class=\"p\">,</span> <span class=\"n\">sampler</span><span class=\"o\">=</span><span class=\"n\">inplace_sampler</span><span class=\"p\">)</span>\n\n<span class=\"n\">w1</span> <span class=\"o\">=</span> <span class=\"n\">We</span><span class=\"o\">.</span><span class=\"n\">copy</span><span class=\"p\">()</span>\n<span class=\"n\">sgd_inplace</span><span class=\"p\">(</span><span class=\"n\">w1</span><span class=\"p\">,</span> <span class=\"n\">toka</span><span class=\"p\">[:</span><span class=\"mi\">100</span><span class=\"p\">])</span>  <span class=\"c\"># warmup the JIT compiler;</span>\n</pre></div>\n\n</div>\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing text_cell rendered\">\n<div class=\"prompt input_prompt\">\n</div>\n<div class=\"inner_cell\">\n<div class=\"text_cell_render border-box-sizing rendered_html\">\n<h3 id=\"Benchmarks\">Benchmarks<a class=\"anchor-link\" href=\"#Benchmarks\">&#182;</a></h3><p>Now that everything is coded up, it's time to do some benchmarks. I'm curious about how well this numba code compares to both the original numpy version I wrote and the uber-optimized gensim Cython code. For the comparison, I ran the algorithms on the Brown corpus <a href=\"http://www.nltk.org/nltk_data/\">packaged with NLTK</a> (loaded above as an array of integer tokens to variable <code>toka</code> in the <a href=\"#Gradient-descent\">Gradient descent</a> section). After running these, I realized that as close as I tried to follow the papers, there's still some interesting implementation differences between gensim and what I came up with, as far as the way the negative sampling gradients are applied to the word vectors. So even after running the algorithms over the same corpus, there's a pretty sizable gap in the word vector norm.</p>\n<p>To get an idea of how comparable the word vectors are, I compare the accuracy of completing analogies from the respective word vectors, using a file that came with gensim. The file has a bunch of analogies like 'brother' is to 'brothers' as 'sister' is to 'sisters', allowing you to evaluate how well a set of word vectors can complete the analogy. In this case it can be scored on whether it guesses that the vector for 'sisters' ($v_{sisters}$) is the closest to the vector that results from $v_{brothers} - v_{brother} + v_{sister}$. While writing the evaluation code I noticed that some of the completions, while not technically correct may reveal unexpected relationships within the corpus it was trained on. For example, given the relationship of Tokyo being the capital of Japan, word2vec predicted that Warsaw was the capital of the USSR, and Seattle is the capital of the AFL-CIO.</p>\n<p>(To reproduce the gensim vectors, you'll need to run <code>export PYTHONHASHSEED=42</code> in the terminal before even starting the ipython notebook, though I'm not even sure how well python's randomness can be reproduced on different systems and versions.)</p>\n<h4 id=\"Numba\">Numba<a class=\"anchor-link\" href=\"#Numba\">&#182;</a></h4>\n</div>\n</div>\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[39]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s\">&#39;PYTHONHASHSEED={}&#39;</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s\">&#39;PYTHONHASHSEED&#39;</span><span class=\"p\">)))</span>\n<span class=\"n\">nseed</span><span class=\"p\">(</span><span class=\"mi\">41</span><span class=\"p\">)</span>\n<span class=\"n\">w1</span><span class=\"p\">,</span> <span class=\"n\">t_numba</span> <span class=\"o\">=</span> <span class=\"n\">timer</span><span class=\"p\">(</span><span class=\"n\">sgd_inplace</span><span class=\"p\">)(</span><span class=\"n\">We</span><span class=\"o\">.</span><span class=\"n\">copy</span><span class=\"p\">(),</span> <span class=\"n\">toka</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s\">&#39;Numba score: {}&#39;</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">wut</span><span class=\"o\">.</span><span class=\"n\">score_wv</span><span class=\"p\">(</span><span class=\"n\">w1</span><span class=\"p\">,</span> <span class=\"n\">vc</span><span class=\"p\">)))</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s\">&#39;Numba wordvec norm: {:.2f}&#39;</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">norm</span><span class=\"p\">(</span><span class=\"n\">w1</span><span class=\"p\">)))</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s\">&#39;=&gt; {:.2f}s&#39;</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">t_numba</span><span class=\"p\">))</span>\n</pre></div>\n\n</div>\n</div>\n</div>\n\n<div class=\"output_wrapper\">\n<div class=\"output\">\n\n\n<div class=\"output_area\"><div class=\"prompt\"></div>\n<div class=\"output_subarea output_stream output_stdout output_text\">\n<pre>PYTHONHASHSEED=42\nNumba score: 182\nNumba wordvec norm: 351.30\n=&gt; 53.53s\n</pre>\n</div>\n</div>\n\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing text_cell rendered\">\n<div class=\"prompt input_prompt\">\n</div>\n<div class=\"inner_cell\">\n<div class=\"text_cell_render border-box-sizing rendered_html\">\n<h4 id=\"Numpy\">Numpy<a class=\"anchor-link\" href=\"#Numpy\">&#182;</a></h4>\n</div>\n</div>\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[40]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"n\">nr</span><span class=\"o\">.</span><span class=\"n\">seed</span><span class=\"p\">(</span><span class=\"mi\">41</span><span class=\"p\">)</span>  <span class=\"c\"># 182</span>\n<span class=\"n\">w2</span> <span class=\"o\">=</span> <span class=\"n\">We</span><span class=\"o\">.</span><span class=\"n\">copy</span><span class=\"p\">()</span>\n<span class=\"p\">(</span><span class=\"n\">w2</span><span class=\"p\">,</span> <span class=\"n\">_</span><span class=\"p\">),</span> <span class=\"n\">t_np</span> <span class=\"o\">=</span> <span class=\"n\">timer</span><span class=\"p\">(</span><span class=\"n\">sgd_np</span><span class=\"p\">)(</span><span class=\"n\">W</span><span class=\"o\">=</span><span class=\"n\">w2</span><span class=\"p\">,</span> <span class=\"n\">corp</span><span class=\"o\">=</span><span class=\"n\">tokl</span><span class=\"p\">,</span> <span class=\"n\">cf</span><span class=\"o\">=</span><span class=\"n\">update</span><span class=\"p\">(</span><span class=\"n\">cnf_</span><span class=\"p\">,</span> <span class=\"n\">term</span><span class=\"o\">=</span><span class=\"p\">{}),</span>\n                           <span class=\"o\">**</span><span class=\"n\">numpy_opts</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s\">&#39;Numpy score: {}&#39;</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">wut</span><span class=\"o\">.</span><span class=\"n\">score_wv</span><span class=\"p\">(</span><span class=\"n\">w2</span><span class=\"p\">,</span> <span class=\"n\">vc</span><span class=\"p\">)))</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s\">&#39;Numpy wordvec norm: {:.2f}&#39;</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">norm</span><span class=\"p\">(</span><span class=\"n\">w2</span><span class=\"p\">)))</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s\">&#39;=&gt; {:.2f}s&#39;</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">t_np</span><span class=\"p\">))</span>\n</pre></div>\n\n</div>\n</div>\n</div>\n\n<div class=\"output_wrapper\">\n<div class=\"output\">\n\n\n<div class=\"output_area\"><div class=\"prompt\"></div>\n<div class=\"output_subarea output_stream output_stdout output_text\">\n<pre>Numpy score: 182\nNumpy wordvec norm: 349.22\n=&gt; 368.91s\n</pre>\n</div>\n</div>\n\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing text_cell rendered\">\n<div class=\"prompt input_prompt\">\n</div>\n<div class=\"inner_cell\">\n<div class=\"text_cell_render border-box-sizing rendered_html\">\n<h4 id=\"Gensim-(Cython)\">Gensim (Cython)<a class=\"anchor-link\" href=\"#Gensim-(Cython)\">&#182;</a></h4>\n</div>\n</div>\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[41]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"kn\">import</span> <span class=\"nn\">gensim</span>\n<span class=\"kn\">from</span> <span class=\"nn\">gensim.models.word2vec</span> <span class=\"k\">import</span> <span class=\"n\">Word2Vec</span>\n<span class=\"k\">assert</span> <span class=\"n\">gensim</span><span class=\"o\">.</span><span class=\"n\">models</span><span class=\"o\">.</span><span class=\"n\">word2vec</span><span class=\"o\">.</span><span class=\"n\">FAST_VERSION</span> <span class=\"o\">==</span> <span class=\"mi\">1</span>\n<span class=\"n\">gparams</span> <span class=\"o\">=</span> <span class=\"n\">ut</span><span class=\"o\">.</span><span class=\"n\">to_gensim_params</span><span class=\"p\">(</span><span class=\"n\">cnf_</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">gensim_eval</span><span class=\"p\">(</span><span class=\"n\">mod</span><span class=\"p\">):</span>\n    <span class=\"n\">ans</span> <span class=\"o\">=</span> <span class=\"n\">mod</span><span class=\"o\">.</span><span class=\"n\">accuracy</span><span class=\"p\">(</span><span class=\"s\">&#39;src/questions-words.txt&#39;</span><span class=\"p\">)</span>\n    <span class=\"n\">sect</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">d</span> <span class=\"k\">for</span> <span class=\"n\">d</span> <span class=\"ow\">in</span> <span class=\"n\">ans</span> <span class=\"k\">if</span> <span class=\"n\">d</span><span class=\"p\">[</span><span class=\"s\">&#39;section&#39;</span><span class=\"p\">]</span> <span class=\"o\">==</span> <span class=\"s\">&#39;total&#39;</span><span class=\"p\">]</span>\n    <span class=\"k\">return</span> <span class=\"nb\">sum</span><span class=\"p\">([</span><span class=\"mi\">1</span> <span class=\"k\">for</span> <span class=\"n\">d</span> <span class=\"ow\">in</span> <span class=\"n\">sect</span> <span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"n\">d</span><span class=\"p\">[</span><span class=\"s\">&#39;correct&#39;</span><span class=\"p\">]])</span>\n</pre></div>\n\n</div>\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing text_cell rendered\">\n<div class=\"prompt input_prompt\">\n</div>\n<div class=\"inner_cell\">\n<div class=\"text_cell_render border-box-sizing rendered_html\">\n<p>1 epoch</p>\n\n</div>\n</div>\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[42]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"c\"># %time </span>\n<span class=\"n\">gen_fast</span><span class=\"p\">,</span> <span class=\"n\">t_gen</span> <span class=\"o\">=</span> <span class=\"n\">timer</span><span class=\"p\">(</span><span class=\"n\">Word2Vec</span><span class=\"p\">)(</span><span class=\"n\">brown</span><span class=\"o\">.</span><span class=\"n\">sents</span><span class=\"p\">(),</span> <span class=\"n\">seed</span><span class=\"o\">=</span><span class=\"mi\">42</span><span class=\"p\">,</span> <span class=\"nb\">iter</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span>\n                                  <span class=\"n\">workers</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">gparams</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s\">&#39;Gensim score: {}&#39;</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">gensim_eval</span><span class=\"p\">(</span><span class=\"n\">gen_fast</span><span class=\"p\">)))</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s\">&#39;Gensim wordvec norm: {:.2f}&#39;</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">norm</span><span class=\"p\">(</span><span class=\"n\">gen_fast</span><span class=\"o\">.</span><span class=\"n\">syn0</span><span class=\"p\">)))</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s\">&#39;=&gt; {:.2f}s&#39;</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">t_gen</span><span class=\"p\">))</span>\n</pre></div>\n\n</div>\n</div>\n</div>\n\n<div class=\"output_wrapper\">\n<div class=\"output\">\n\n\n<div class=\"output_area\"><div class=\"prompt\"></div>\n<div class=\"output_subarea output_stream output_stdout output_text\">\n<pre>Gensim score: 109\nGensim wordvec norm: 209.85\n=&gt; 10.50s\n</pre>\n</div>\n</div>\n\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing text_cell rendered\">\n<div class=\"prompt input_prompt\">\n</div>\n<div class=\"inner_cell\">\n<div class=\"text_cell_render border-box-sizing rendered_html\">\n<p>2 epochs</p>\n\n</div>\n</div>\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[43]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"n\">gen_fast2</span><span class=\"p\">,</span> <span class=\"n\">t_gen2</span> <span class=\"o\">=</span> <span class=\"n\">timer</span><span class=\"p\">(</span><span class=\"n\">Word2Vec</span><span class=\"p\">)(</span><span class=\"n\">brown</span><span class=\"o\">.</span><span class=\"n\">sents</span><span class=\"p\">(),</span> <span class=\"n\">seed</span><span class=\"o\">=</span><span class=\"mi\">42</span><span class=\"p\">,</span>\n                                    <span class=\"nb\">iter</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">workers</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">gparams</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s\">&#39;Gensim score: {}&#39;</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">gensim_eval</span><span class=\"p\">(</span><span class=\"n\">gen_fast2</span><span class=\"p\">)))</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s\">&#39;Gensim wordvec norm: {:.2f}&#39;</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">norm</span><span class=\"p\">(</span><span class=\"n\">gen_fast2</span><span class=\"o\">.</span><span class=\"n\">syn0</span><span class=\"p\">)))</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s\">&#39;=&gt; {:.2f}s&#39;</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">t_gen2</span><span class=\"p\">))</span>\n</pre></div>\n\n</div>\n</div>\n</div>\n\n<div class=\"output_wrapper\">\n<div class=\"output\">\n\n\n<div class=\"output_area\"><div class=\"prompt\"></div>\n<div class=\"output_subarea output_stream output_stdout output_text\">\n<pre>Gensim score: 181\nGensim wordvec norm: 243.99\n=&gt; 17.21s\n</pre>\n</div>\n</div>\n\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing text_cell rendered\">\n<div class=\"prompt input_prompt\">\n</div>\n<div class=\"inner_cell\">\n<div class=\"text_cell_render border-box-sizing rendered_html\">\n<h5 id=\"Timing-Ratios\">Timing Ratios<a class=\"anchor-link\" href=\"#Timing-Ratios\">&#182;</a></h5><p>Divide the row function by the column to see the time ratio. An entry of 5 at row <code>Numba</code> and column <code>Gensim</code> would mean that the Numba function takes 5 times as long as the Gensim one.</p>\n\n</div>\n</div>\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[44]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span class=\"n\">ts</span> <span class=\"o\">=</span> <span class=\"n\">Series</span><span class=\"p\">(</span><span class=\"nb\">dict</span><span class=\"p\">(</span><span class=\"n\">Gensim</span><span class=\"o\">=</span><span class=\"n\">t_gen</span><span class=\"p\">,</span> <span class=\"n\">Gensim2</span><span class=\"o\">=</span><span class=\"n\">t_gen2</span><span class=\"p\">,</span> <span class=\"n\">Numba</span><span class=\"o\">=</span><span class=\"n\">t_numba</span><span class=\"p\">,</span> <span class=\"n\">Numpy</span><span class=\"o\">=</span><span class=\"n\">t_np</span><span class=\"p\">))</span>\n<span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">ts</span><span class=\"p\">[:,</span> <span class=\"k\">None</span><span class=\"p\">]</span> <span class=\"o\">/</span> <span class=\"n\">ts</span><span class=\"p\">[:,</span> <span class=\"k\">None</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"n\">index</span><span class=\"o\">=</span><span class=\"n\">ts</span><span class=\"o\">.</span><span class=\"n\">index</span><span class=\"p\">,</span>\n          <span class=\"n\">columns</span><span class=\"o\">=</span><span class=\"n\">ts</span><span class=\"o\">.</span><span class=\"n\">index</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">apply</span><span class=\"p\">(</span><span class=\"n\">mc</span><span class=\"p\">(</span><span class=\"s\">&#39;round&#39;</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">))</span>\n</pre></div>\n\n</div>\n</div>\n</div>\n\n<div class=\"output_wrapper\">\n<div class=\"output\">\n\n\n<div class=\"output_area\"><div class=\"prompt output_prompt\">Out[44]:</div>\n\n\n<div class=\"output_text output_subarea output_execute_result\">\n<pre>         Gensim  Gensim2  Numba  Numpy\nGensim     1.00     0.61   0.20   0.03\nGensim2    1.64     1.00   0.32   0.05\nNumba      5.10     3.11   1.00   0.15\nNumpy     35.13    21.44   6.89   1.00</pre>\n</div>\n\n</div>\n\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing text_cell rendered\">\n<div class=\"prompt input_prompt\">\n</div>\n<div class=\"inner_cell\">\n<div class=\"text_cell_render border-box-sizing rendered_html\">\n<h2 id=\"Conclusion\">Conclusion<a class=\"anchor-link\" href=\"#Conclusion\">&#182;</a></h2><p>It looks like with all my numba optimizations, I'm able to get about 7x speedup over numpy, for about the same word vectors (the norms are close and the analogy accuracy score is the same). But despite the numba optimizations, my implementation is still about 5x slower than gensim's Cython version. With the possible implementation differences mentioned above, though, it looks like gensim produces word vectors more similar to mine after 2 epochs, at least in terms of word analogy accuracy. Running 2 gensim epochs gets me down to 'only' 3 times slower, but even this is by limiting gensim to a single worker. I should probably be happy with these numbers, since the gensim contributors really know what they're doing, but going through this has given me a greater appreciation of the dark cython magic they're using behind the scenes.</p>\n<p>Along with word2vec, this was also my first dive into programming with numba. It was a bit tricky navigating around some restrictions like higher-order functions, and I missed being able to avoid lower level for-loop style programming. Interpreting numba error messages was also a pain point, but I assume that's something I'll get better at the more I use it and the more it's developed. But overall I'm really happy with the speed increases it gives, as well as being able to stick to 'pretty much Python.' Numeric computing projects from the python community haven't ceased to impress me.</p>\n\n</div>\n</div>\n</div>",
  "author": {
    "name": ""
  },
  "category": "",
  "summary": "A difficulty of working with text data is that using each word of a large vocabulary as a feature requires working with large dimensions. While sparse matrix data structures make manipulating these vectors tractable, there are whole classes of algorithms that do not work well or at all on sparse high dimensional data. A somewhat recent development called word2vec allows words to be represented as dense vectors of much smaller dimensions (on the order of $\\mathbb{R}^{100}$). This algorithm yields representations of words such that words appearing in similar contexts will lie close to one another in this low dimensional vector space. Another interesting feature of this algorithm is that the representation does a good job at inferring analogies. This gensim demo of the algorithm shows word2vec inferring that dogs is to puppies as cats is to kittens: In&nbsp;[1]: from IPython.display import Image Image(filename=&#39;kittens.png&#39;) Out[1]: At a high level, the skip-gram flavor of this algorithm looks at a word and its surrounding words, and tries to maximize the probability that the word's vector representation predicts those actual words occurring around it. If it is trained on the phrase the quick brown fox jumps, the word2vec input representation of the word brown would yield a high dot product with the output vectors for the words the, quick, fox and jumps. And if the algorithm is trained on a lot more text, there's a good chance that it will start to learn that the words $brown$ and $red$ appear in similar contexts, so that their vector representations will be pretty close to one another's. Speed&#182;I'm constantly trying to navigate the trade-off from simultaneously maximizing my laziness and the speed of my code. Aiming for the lazy side, I originally wanted to experiment with using autograd to write my gradient updates for me, and as nice as it was to not have to write out the gradient calculation, this pretty quickly bubbled to the top as one of the major bottlenecks, leaving me to write the gradients manually by faster means. While numpy gives a big speed boost over plain python, cython (used by the go-to word2vec implementation, gensim), gives a major performance improvement beyond numpy, with speeds often comparable to code written in C. Part of the cost of this speed up is that writing in Cython, while more pythonic than C, seems to require additional type annotations and syntactic elements, making it less readable and less interesting to write (at least for me). My goal here has been to make word2vec as close to gensim's cython performance as possible while sticking to Python, so I settled on Numba, a numeric JIT compiler that uses LLVM and supports a subset of Python and numpy. While I ran into some limitations of numba, rewriting the inner loops in Numba functions ended up giving a significant speed-boost. I did a lot of profiling and iterating to find the major bottlenecks of the gradient descent learning, and while I don't know of a good way to communicate the iterative stone-smoothing process that is program optimization, I've left the original numpy and improved numba versions of some of these functions for microbenchmark comparison. Let's get started&#182; In&nbsp;[2]: %%javascript var csc = IPython.keyboard_manager.command_shortcuts csc.add_shortcut(&#39;Ctrl-k&#39;,&#39;ipython.move-selected-cell-up&#39;) csc.add_shortcut(&#39;Ctrl-j&#39;,&#39;ipython.move-selected-cell-down&#39;) csc.add_shortcut(&#39;Shift-m&#39;,&#39;ipython.merge-selected-cell-with-cell-after&#39;) In&nbsp;[1]: from project_imports import * %matplotlib inline In&nbsp;[2]: import wordvec_utils as wut from wordvec_utils import Cat, update, Conf, NegSampler import utils as ut from utils import take, ilen, timer import numba_utils as nbu from numba_utils import ns_grad as ns_grad_jit, nseed from autograd import numpy as npa, grad from numba import jit, njit from numpy import array, ndarray as Array from numpy.linalg import norm Objective functions&#182;The standard skip-gram objective function comes from taking the softmax probability of each of the actual context word vectors dotted with the input ($u_{c,j^*_c}$) and multiplying them together: \\begin{align} E &amp; = -\\log \\prod_{c=1} ^{C} \\frac {\\exp (u_{c,j^*_c})} {\\sum_{j'=1} ^ V \\exp(u_{j'})} \\\\ &amp; = -\\sum^C_{c=1} u_{j^*_c} + C \\cdot \\log \\sum ^ V _{j'=1} \\exp(u_j') \\end{align}See word2vec Parameter Learning Explained for a detailed explanation, and note that the algorithm keeps 2 representations for each word, an input vector from $W$, and an output vector from $W'$ (W1 and W2 in the code). Though I ended up using a different objective called negative sampling, I kept this original objective below as skipgram_likelihood, written as a nested function since autograd requires single-argument functions to differentiate: In&nbsp;[7]: def skipgram_likelihood(wi, cwds, dv=None): wix = dv.get(wi) cixs = dv.get(cwds) C = len(cwds) def logloss(Wall): W1, W2 = Cat.split(Wall) h = W1[wix, :] # ∈ ℝⁿ u = np.dot(h, W2) ucs = u[cixs] return -np.sum(ucs) + C * np.log(np.sum(np.exp(u))) return logloss The return value of logloss should pretty clearly resemble the equation above. Negative sampling&#182;Gradient&#182;After reading a bit more about word2vec, I found out about an extension to the skip-gram model called negative sampling that efficiently generates better word vectors. The basic idea is that in addition to training a word vector with $C$ words that do appear around it, the vector should also be trained with $K$ words randomly chosen from the rest of the text, as negative examples of what the vector should not predict in its context. As a side-note to keep up with the notation taken from word2vec Parameter Learning Explained, the term $\\boldsymbol v_{w_O}'$ refers to a word vector ($\\boldsymbol v$) that is from the output vector matrix ($\\boldsymbol v'$) representing an output word $w_O$. Notation aside, the gradient for the negative sampling extension is relatively straightforward. For each true context word vector $\\boldsymbol v_{w_O}'$ appearing close to the input word vector $h$, we'll draw $K$ word vectors $\\boldsymbol v_{w_i}$ at random from the corpus. The objective is computed by adding the log of the sigmoid of the word vector dotted with either the true or false context word (the negative samples are negated): $$ E = -\\log \\sigma(\\boldsymbol v_{w_O}^{\\prime T} \\boldsymbol h) - \\sum^K _{i=1} \\log \\sigma (-\\boldsymbol v_{w_i} ^{\\prime T} \\boldsymbol h) $$The gradient with respect to $\\boldsymbol v_{w_j}^{\\prime T} \\boldsymbol h$ is then as follows, where $t_j$ is an indicator for whether $w_j$ actually appears in the context: $$ \\frac{\\partial E} {\\partial \\boldsymbol v_{w_j}^{\\prime T} \\boldsymbol h} = \\sigma(\\boldsymbol v_{w_j}^{\\prime T} \\boldsymbol h) -t_j. $$The numpy gradient is written below as ns_grad; the numba gradient ns_grad_jit is similar, but all of the functions used in a JIT'd function must also be numba-JIT'd, so I moved them all out to the numba_utils module. The schema I adopted to calculate gradients takes a subset of the matrix formed by concatenating the input and output parameter matrices together ($W || W'$). If the vector dimension $N$ is 100, and there are 1000 words in the vocabulary, this matrix will be in $\\mathbb{R}^{1000 \\times 200}$. The gradient function takes a subset of this matrix with $K + 2$ rows. The first row represents the input word, the next represents the true context word, and the rest represent the $K$ negative samples. Note that since the first vector is from the input parameter matrix $W$, and these functions are operating on the input and output matrices concatenated, we only care about the first $N$ entries of the first row. Similarly, we only care about the last $N$ entries for the rest of the output vectors. This schema should be more clear from the get_vecs1 function below that extracts the vectors from the relevant subset of the concatenated parameter matrices. This also means that for the gradients of the concatenated matrix, half of the entries in each row will be zero. In&nbsp;[8]: getNall = lambda W: W.shape[1] // 2 gen_labels = lambda negsamps: [1] + [0] * len(negsamps) sig = lambda x: 1 / (1 + np.exp(-x)) def get_vecs1(Wsub): N = getNall(Wsub) h = Wsub[0, :N] # ∈ ℝⁿ vwo = Wsub[1, N:] negsamps = Wsub[2:, N:] return h, vwo, negsamps def ns_loss_grads(h: &#39;v[n]&#39;, vout: &#39;[v[n]]&#39;, label: &#39;v[n]&#39;): dot = sig(vout @ h) - label return dot * vout, dot * h def ns_grad(Wsub): h, vwo, negsamps = get_vecs1(Wsub) N = getNall(Wsub) Wsub_grad = np.zeros(Wsub.shape) for i, vout, label in zip(count(1), it.chain([vwo], negsamps), gen_labels(negsamps)): hgrad, vgrad = ns_loss_grads(h, vout, label) Wsub_grad[0, :N] += hgrad Wsub_grad[i, N:] += vgrad return Wsub_grad Gradient check&#182;The following gradient checking functionality based on the UFLDL tutorial uses simple calculus to ensure that the gradients are working as expected. Finite difference check&#182; In&nbsp;[9]: def ns_loss(h, vwo, vwi_negs): &quot;&quot;&quot;This should be called on the subset of the matrix (Win || Wout&#39;) determined by row indices `wi, win_ix, negwds`. &quot;&quot;&quot; negsum = 0 for j in range(len(vwi_negs)): negsum += np.log(sig(-vwi_negs[j] @ h)) return -np.log(sig(vwo @ h)) - negsum def ns_loss_vec(h, vwo, vwi_negs): &quot;&quot;&quot;This should be called on the subset of the matrix (Win || Wout&#39;) determined by row indices `wi, win_ix, negwds`. &quot;&quot;&quot; return -np.log(sig(vwo @ h)) - np.sum(np.log(sig(-vwi_negs @ h ))) In&nbsp;[10]: def J(Wsub, loss=ns_loss): N = getNall(Wsub) h, vwo, vwi_negs = get_vecs1(Wsub) return loss(h, vwo, vwi_negs) def check_grad_(W, i: int=None, j: int=None, eps=1e-6, J: &#39;fn&#39;=None): &quot;&quot;&quot;From eqn at http://ufldl.stanford.edu/tutorial/supervised /DebuggingGradientChecking/&quot;&quot;&quot; Wneg, Wpos = W.copy(), W.copy() Wneg[i, j] -= eps Wpos[i, j] += eps return (J(Wpos) - J(Wneg)) / (2 * eps) def approx_grad(W, J=J): n, m = W.shape grad = np.zeros_like(W) for i in range(n): for j in range(m): grad[i, j] = check_grad_(W, i=i, j=j, eps=1e-6, J=J) return grad Autograd checking&#182; In&nbsp;[11]: def mk_ns_loss_a(N): &quot;&quot;&quot;Return a loss function that works on an N-dimensional representation. This takes a single argument, the subset of the input and output matrices that correspond to the input word (first row), true contest word (second row) and K negative samples (rest of the rows). Since it takes a single argument, the gradient can automatically be calculated by autograd&quot; &quot;&quot;&quot; def σ(x): return 1 / (1 + npa.exp(-x)) def ns_loss_a(Wsub): h = Wsub[0, :N] vwo = Wsub[1, N:] vwi_negs = Wsub[2:, N:] vwo_h = npa.dot(vwo, h) vwi_negs_h = npa.dot(vwi_negs, h) return -npa.log(σ(vwo_h)) - npa.sum(npa.log(σ(-vwi_negs_h))) return ns_loss_a mk_ns_grad_a = z.compose(grad, mk_ns_loss_a) In&nbsp;[12]: N_ = 50; W = wut.init_w(1000, N_, seed=1); Wsub = W[:8] Compare gradients&#182; In&nbsp;[13]: np_check = lambda x: approx_grad(x, partial(J, loss=ns_loss)) np_vec_check = lambda x: approx_grad(x, partial(J, loss=ns_loss_vec)) ns_grad_auto = mk_ns_grad_a(N_) In&nbsp;[14]: def grad_close(f, grd=ns_grad(Wsub)): &quot;&quot;&quot;Check that a given gradient function result agrees with ns_grad. Print out norm of the difference.&quot;&quot;&quot; grd2 = f(Wsub) close = np.allclose(grd, grd2) close_ = &#39;√&#39; if close else &#39;x&#39; print(&#39;{} Diff: {}&#39;.format(close_, np.linalg.norm(grd - grd2))) return close In&nbsp;[15]: assert grad_close(np_check) assert grad_close(np_vec_check) assert grad_close(ns_grad_auto) assert grad_close(ns_grad) assert grad_close(ns_grad_jit) √ Diff: 4.3962387780340825e-09 √ Diff: 4.3962387780340825e-09 √ Diff: 1.1173464828107622e-17 √ Diff: 0.0 √ Diff: 0.0 In&nbsp;[16]: %timeit np_check(Wsub) # 48.9 ms per loop %timeit np_vec_check(Wsub) # 33.8 ms per loop %timeit ns_grad_auto(Wsub) # 856 µs per loop %timeit ns_grad(Wsub) # 53.8 µs per loop %timeit ns_grad_jit(Wsub) # 6.14 µs per loop 10 loops, best of 3: 48.2 ms per loop 10 loops, best of 3: 32 ms per loop 1000 loops, best of 3: 845 µs per loop 10000 loops, best of 3: 54.7 µs per loop The slowest run took 4.31 times longer than the fastest. This could mean that an intermediate result is being cached 100000 loops, best of 3: 6.17 µs per loop The first two gradient checking versions are extremely slow as expected from the naive and unpythonic style. The numpy implementation gives a huge improvement over these, and numba gives us another order of magnitude speedup over this. The autograd version looks like it falls at about the logarithmic midpoint between the naive checking and handwritten implementations. Draw negative samples&#182;As a foreshadowing of performance bottlenecks that my original implementation ran into, I have a few versions of a function that chooses words from the text at random, that increase in performance. They all draw words randomly according to the unigram$^{3/4}$ distribution, which is similar to the unigram distribution, but boosts the probability of less frequent items: In&nbsp;[17]: uni = np.linspace(0, 1, 100) uni34 = uni ** (3/4) plt.plot(uni, uni34 - uni); Most of the samplers were written as generators, except for the last one, neg_sampler_inplace, which modifies an array in-place with random samples. As much as this in-place modification conflicts with my usual approach, it ended up being necessary for a final numba performance boost. The generator sampler I ended up going with, neg_sampler_jit_pad, is padded with a couple of dummy entries. I originally drew some negative samples negsamps from one of these generators, and then created the array to index $W$ by prepending negsamps with w and c, copying these to a new array. Usually this is ok, but I found that copying negsamps to a new array in the inner loop caused a noticeable delay, so I wrote neg_sampler_jit_pad to return 2 extra empty elements at the beginning to be able to insert w and c inplace without copying to a new array. In&nbsp;[18]: def unigram(txt, pow=.75): &quot;Unigram^(3/4) model&quot; cts = Series(Counter(txt)) # If txt is integers, fill in missing values (likely for # unknown token) with 0 probability to reliably use # index to identify token int_txt = cts.index.dtype == int if int_txt: missing_tokens = set(range(cts.index.max())) - set(cts.index) for msg in missing_tokens: cts.loc[msg] = 0 cts = cts.sort_index() N = len(txt) ctsdf = ((cts / cts.sum()) ** pow).reset_index(drop=0) ctsdf.columns = [&#39;Word&#39;, &#39;Prob&#39;] if int_txt: assert (ctsdf.Word == ctsdf.index).all() return ctsdf def cum_prob_uni(xs, pow=.75): ug = unigram(xs, pow=pow) return ug.Prob.cumsum().div(ug.Prob.sum()).values def neg_sampler_pd(xs, K, pow=.75): &quot;Simplest, but slowest sampler using built-in pandas sample function&quot; ug = unigram(xs, pow=pow) for seed in count(): yield ug.Word.sample(n=K, weights=ug.Prob, random_state=seed, replace=True) def neg_sampler_np(xs, K, cache_len=1000, use_seed=False, pow=.75, ret_type=Array): &quot;Faster neg. sampler without the pandas overhead&quot; ug = unigram(xs, pow=pow) p = ug.Prob.values / ug.Prob.sum() a = ug.Word.values def neg_sampler_np_(): for seed in count(): if use_seed: nr.seed(seed) Wds = nr.choice(a, size=(cache_len, K), p=p) for wds in Wds: yield wds agen = neg_sampler_np_() if ret_type == list: return map(list, agen) assert ret_type == Array, (&#39;Only defined for type &#39; &#39;in {Array, list}&#39;) return agen def neg_sampler_jit(xs, K, pow=.75, ret_type=list): cum_prob = cum_prob_uni(xs, pow=pow) sampler = { list: nbu.neg_sampler_jitl_, Array: nbu.neg_sampler_jita_, }[ret_type] return sampler(cum_prob, K) def neg_sampler_jit_pad(xs, K, pow=.75, ret_type=list, pad=0): cum_prob = cum_prob_uni(xs, pow=pow) sampler = { list: nbu.neg_sampler_jitl_pad, Array: nbu.neg_sampler_jita_pad, }[ret_type] return sampler(cum_prob, K, pad=pad) def neg_sampler_inplace(xs, K, pow=.75, pad=0, ret_type=Array, seed=None): use_seed = seed is not None and False seed = seed or 0 cum_prob = cum_prob_uni(xs, pow=pow) a = np.empty(K + pad, dtype=np.int64) @njit def neg_sampler_jit_pad_arr_(a): if use_seed: nr.seed(seed) for i in range(pad, K + pad): a[i] = nbu.bisect_left_jit(cum_prob, nr.rand()) return a, neg_sampler_jit_pad_arr_ def neg_sampler_inplace_gen(xs, K, pow=.75, pad=0, ret_type=Array): a, neg_sampler_jit_pad_arr_ = neg_sampler_inplace(xs, K, pow=pow, pad=pad) for _ in it.repeat(None): neg_sampler_jit_pad_arr_(a) yield a[pad:].copy() Check distributions&#182;Just as a sanity check that the different implementations do the same thing, I randomly generate words according to how frequently they occur in the text with each of the samplers and scatter-plot them against the actual word frequency to check that they mostly lie on $y=x$. In&nbsp;[19]: from sklearn.preprocessing import LabelEncoder from nltk.corpus import brown some_text = take(brown.words(), int(1e6)) In&nbsp;[20]: le = LabelEncoder() smtok = le.fit_transform(some_text) In&nbsp;[21]: gen_pd = NegSampler(neg_sampler_pd, smtok, 8) gen_npl = NegSampler(neg_sampler_np, smtok, 8, ret_type=list) gen_npa = NegSampler(neg_sampler_np, smtok, 8, ret_type=Array) gen_jitl = NegSampler(neg_sampler_jit, smtok, 8, ret_type=list) gen_jitl_pad = NegSampler(neg_sampler_jit_pad, smtok, 8, ret_type=list, pad=2) gen_jita = NegSampler(neg_sampler_jit, smtok, 8, ret_type=Array) gen_jita_pad = NegSampler(neg_sampler_jit_pad, smtok, 8, ret_type=Array, pad=2) gen_jita_padi = NegSampler(neg_sampler_inplace_gen, smtok, 8, ret_type=Array, pad=2) In&nbsp;[22]: run_n = lambda gen, n=10000: ilen(it.islice(gen, n)) %timeit run_n(gen_pd, n=1000) # 3.41 s per loop %timeit run_n(gen_npl) # 31.4 ms per loop %timeit run_n(gen_npa) # 19.1 ms per loop %timeit run_n(gen_jitl) # 18.6 ms per loop %timeit run_n(gen_jitl_pad) # 19.2 ms per loop %timeit run_n(gen_jita) # 14.7 ms per loop %timeit run_n(gen_jita_pad) # 19.2 ms per loop %timeit run_n(gen_jita_padi) # 20.9 ms per loop 1 loops, best of 3: 3.42 s per loop 10 loops, best of 3: 32.4 ms per loop 10 loops, best of 3: 19.9 ms per loop 100 loops, best of 3: 18.7 ms per loop 100 loops, best of 3: 19.4 ms per loop 100 loops, best of 3: 14.5 ms per loop 100 loops, best of 3: 14.9 ms per loop 10 loops, best of 3: 20.6 ms per loop In&nbsp;[23]: n = 100000 csp = Series(Counter(x for xs in it.islice(gen_pd, n // 100) for x in xs)) csnp = Series(Counter(x for xs in it.islice(gen_npl, n) for x in xs)) csj = Series(Counter(x for xs in it.islice(gen_jita_pad, n) for x in xs[2:])) cs_ip = Series(Counter(x for xs in it.islice(gen_jita_padi, n) for x in xs)) ug = unigram(smtok, pow=.75) cts = DataFrame({&#39;Numba&#39;: csj, &#39;Numpy&#39;: csnp, &#39;Pandas&#39;: csp, &#39;Inplace&#39;: cs_ip}).fillna(0) probs = cts / cts.sum() probs[&#39;Probs&#39;] = ug.Prob / ug.Prob.sum() In&nbsp;[24]: def plot_dist(xcol=None, subplt=None): plt.subplot(subplt) probs.plot(x=xcol, y=&#39;Probs&#39;, ax=plt.gca(), kind=&#39;scatter&#39;, alpha=.25) _, xi = plt.xlim(None) _, yi = plt.ylim(0, None) end = min(xi, yi) plt.plot([0, end], [0, end], alpha=.2) plt.xticks(rotation=70) plt.figure(figsize=(16, 10)) plot_dist(xcol=&#39;Numba&#39;, subplt=141) plot_dist(xcol=&#39;Numpy&#39;, subplt=142) plot_dist(xcol=&#39;Pandas&#39;, subplt=143) plot_dist(xcol=&#39;Inplace&#39;, subplt=144) In&nbsp;[25]: del smtok, le As these plots show, the samplers seem to be drawing words according to the expected distribution. The pandas sampler was so slow that I had to reduce the number of draws by a couple orders of magnitude, so the plot is a lot noisier than the others, but still roughly on track. The in-place sampler appears to be slower on the microbenchmarks, but gives an improvement when restructuring the gradient update routine. Sliding window&#182;It turned out another significant bottleneck of the gradient descent routine was the sliding window code that iterates over the entire corpus, yielding a word and its context words at each point. For the first few words in the text sequence, there are less than $C$ surrounding context words, so some checking is required. For the numba function, I used a specialized iterator at the beginning and end to avoid checking the max and min indices at every step. Just like in the negative sampler section, I first wrote the sliding window functions in a generator style, and converted the fastest version to an inplace modification style. In&nbsp;[26]: def sliding_window(xs, C=4, start_pos=0): &quot;&quot;&quot;Iterate through corpus, yielding input word and surrounding context words&quot;&quot;&quot; winsize = C // 2 N = len(xs) for i, x in enumerate(xs, start_pos): ix1 = max(0, i-winsize) ix2 = min(N, i+winsize+1) yield x, xs[ix1:i] + xs[i + 1:ix2] @njit def sliding_window_jit(xs, C=4): &quot;&quot;&quot;Iterates through corpus, yielding input word and surrounding context words&quot;&quot;&quot; winsize = C // 2 N = len(xs) for i in range(winsize): yield nbu.bounds_check_window(i, xs, winsize, N) for i in range(winsize, N-winsize): context = [] for j in range(i-winsize, i+winsize+1): if j != i: context.append(xs[j]) yield xs[i], context for i in range(N-winsize, N): yield nbu.bounds_check_window(i, xs, winsize, N) @njit def sliding_window_jit_arr(xs, C=4): &quot;&quot;&quot;Iterates through corpus, yielding input word and surrounding context words&quot;&quot;&quot; winsize = C // 2 N = len(xs) for i in range(winsize): yield nbu.bounds_check_window_arr(i, xs, winsize, N) for i in range(winsize, N-winsize): context = np.empty(C, dtype=np.int64) for ci in range(winsize): context[ci] = xs[i - winsize + ci] context[winsize + ci] = xs[i + 1 + ci] yield xs[i], context for i in range(N-winsize, N): yield nbu.bounds_check_window_arr(i, xs, winsize, N) In&nbsp;[27]: @njit def sliding_window_inplace(xs, i, C=4): winsize = C // 2 N = len(xs) if i &lt; winsize: return nbu.bounds_check_window_arr(i, xs, winsize, N) elif i &lt; N-winsize: context = np.empty(C, dtype=np.int64) for ci in range(winsize): context[ci] = xs[i - winsize + ci] context[winsize + ci] = xs[i + 1 + ci] return xs[i], context elif i &lt; N: return nbu.bounds_check_window_arr(i, xs, winsize, N) raise ValueError(&#39;Out of bounds&#39;) def sliding_window_ix_(xs, C=4): return (sliding_window_inplace(xs, i, C=C) for i in range(len(xs))) In&nbsp;[28]: samp_toks = nr.randint(0, 1e6, size=100005) samp_toksl = list(samp_toks) list(sliding_window_jit(samp_toksl[:10], C=4)) run_window = lambda f, toks=samp_toksl: [ut.ilen(xs) for xs in f(toks, C=4)] In&nbsp;[29]: to_lst = lambda xs: [(c, list(ys)) for c, ys in xs] assert to_lst(sliding_window_ix_(samp_toks[:10], C=4) ) == to_lst(sliding_window_jit_arr(samp_toks[:10], C=4)) In&nbsp;[30]: %timeit run_window(sliding_window) %timeit run_window(sliding_window_jit) %timeit run_window(sliding_window_jit_arr, toks=samp_toks) %timeit run_window(sliding_window_ix_, toks=samp_toks) 10 loops, best of 3: 178 ms per loop 10 loops, best of 3: 133 ms per loop 10 loops, best of 3: 109 ms per loop 10 loops, best of 3: 167 ms per loop Norm&#182;Since I'm using the concatenated input and output matrix, half of the w, c, negsamps submatrix entries are 0. On each iteration I check the gradient matrix's norm to clip the gradient if necessary. A specialized norm function taking this structure into account gives about an order of magnitude speedup over the numpy one. In&nbsp;[31]: @njit def grad_norm(Wsub): &quot;&quot;&quot;Calculate norm of gradient, where first row is input vector, rest are output vectors. For any row, half of the entries are zeros, which allows a lot of skipping for a faster computation&quot;&quot;&quot; n = Wsub.shape[1] // 2 sm = 0 for i in range(n): sm += Wsub[0, i] ** 2 for i in range(1, len(Wsub)): for j in range(n, 2 * n): sm += Wsub[i, j] ** 2 return np.sqrt(sm) In&nbsp;[32]: grd = ns_grad_jit(Wsub) assert np.isclose(grad_norm(grd), np.linalg.norm(grad_norm(grd))) %timeit np.linalg.norm(grad_norm(grd)) %timeit grad_norm(grd) The slowest run took 5.81 times longer than the fastest. This could mean that an intermediate result is being cached 100000 loops, best of 3: 4.69 µs per loop The slowest run took 4.25 times longer than the fastest. This could mean that an intermediate result is being cached 1000000 loops, best of 3: 659 ns per loop Gradient descent&#182;That does it for optimizing most of the individual pieces. For the gradient descent routine, I'm passing all the hyper-parameters through a configuration dictionary, validated by voluptuous. This lets me specify of things like the context window size, learning rate, number of negative samples and size of the word vectors. Check the utility file wordvec_utils.py where it's defined for details on the meanings of the parameters. In&nbsp;[33]: all_text = list(brown.words()) use_words_ = z.valfilter(lambda x: x &gt;= 5, Counter(all_text)) use_words = [w for w in all_text if w in use_words_] le = LabelEncoder() toka = le.fit_transform(use_words) tokl = list(toka) vc = le.classes_ In&nbsp;[34]: cnf = ut.AttrDict( eta=.1, min_eta=.0001, accumsec=0, N=100, C=4, K=6, iter=0, thresh=15, epoch=0, pad=0, term=dict(iters=None, secs=None ), dir=&#39;cache&#39;, ) cnf = Conf(cnf) cnf_ = cnf del cnf cfp = update(cnf_, pad=2, term={}) W = wut.init_w(len(vc), cnf_.N, seed=1) We = W.copy() Sgd&#182;One final speedup I got was from putting the entire inner loop routine into a numba JIT'd function, instead of calling each JIT'd function separately. Numba seems to reduce some of python's function call overhead, as well as the numpy indexing. For some reason, merely indexing into a numpy array (to calculate the gradient and then update the original matrix) still ended up being a significant bottleneck, though numba seemed to reduce it. Since Numba doesn't allow passing functions in as parameters, mk_grad_update is a higher order vanilla Python function that returns either a regular numpy function, or the same function with a numba decorator and a numba gradient function in the closure. In&nbsp;[35]: def mk_grad_update(jit=False, grad_func=ns_grad): if jit: grad_func = ns_grad_jit norm_func = grad_norm deco = njit else: norm_func = np.linalg.norm deco = lambda x: x def grad_update(W, sub_ixs, eta): Wsub = W[sub_ixs] grad = grad_func(Wsub) gnorm = norm_func(grad) if gnorm &gt; 5: # clip gradient grad /= gnorm W[sub_ixs] = Wsub - eta * grad return deco(grad_update) grad_update = mk_grad_update(jit=False) grad_update_jit = mk_grad_update(jit=True) In&nbsp;[36]: @njit def grad_update_jit_pad(W, sub_ixs, eta, w=0, c=0): &quot;&quot;&quot;If focus or context word are contained in negative samples, drop them before performing the grad update. &quot;&quot;&quot; sub_ixs = nbu.remove_dupes(sub_ixs, w, c) grad_update_jit(W, sub_ixs, eta) def inner_update(W, negsamps, eta, w=0, c=0): &quot;&quot;&quot;Numpy inner loop gradient update function. I.e., slow version of `grad_update_jit_pad`&quot;&quot;&quot; #print(negsamps, eta, w, c) if (w in negsamps) or (c in negsamps): negsamps = [x for x in negsamps if x not in {w, c}] sub_ixs = np.array([w, c] + negsamps) # list(negsamps) grad_update(W, sub_ixs, eta) def check_padding(sampler, k, grad_update, dims=1): &quot;Poor dependent type checker&quot; _samp = next(sampler)[0] if dims == 2 else next(sampler) k_ = len(_samp) padded = k_ == 2 + k assert k_ == sampler.pad + sampler.K assert k == sampler.K assert k_ in (2 + k, k), (&#39;Length of samples&#39; &#39; should be either `k` or `k` + 2 if padded&#39;) assert padded == (grad_update in padded_updates), ( &#39;Make sure sampler size agrees with grad_update&#39;) padded_updates = {grad_update_jit_pad} gen_npl = NegSampler(neg_sampler_np, tokl, K=cnf_.K, ret_type=list) numpy_opts = dict(ns_grad_=ns_grad, neg_sampler=gen_npl, sliding_window=sliding_window, grad_update=inner_update) ngsamp_pad = NegSampler(neg_sampler_jit_pad, tokl, cfp.K, ret_type=Array, pad=2) fast_opts = dict(ns_grad_=ns_grad_jit, neg_sampler=ngsamp_pad, sliding_window=sliding_window_jit, grad_update=grad_update_jit_pad) And finally, here is the full gradient descent function put together. The numpy version, sgd_np, has some vestigial code and options from earlier iterations where I was doing more debugging. mk_sgd_inplace returns a more straightforward Numba sgd function. In&nbsp;[37]: def sgd_np(W=None, corp=None, cf={}, ns_grad_=ns_grad, neg_sampler=None, vc=None, sliding_window=sliding_window, grad_update=grad_update_jit_pad): check_padding(neg_sampler, cf.K, grad_update) if not os.path.exists(cf.dir): os.mkdir(cf.dir) st = time.time(); cf = Conf(cf) #.copy() assert cf.N == W.shape[1] / 2, &#39;shape of W disagrees with conf&#39; iter_corpus = corp[cf.iter:] if cf.iter else corp learning_rates = np.linspace(cf.eta, cf.min_eta, len(iter_corpus)) assert neg_sampler is not None, &quot;Give me a negative sampler!&quot; iters_ = zip(count(cf.iter), sliding_window(iter_corpus, C=cf.C), z.partition(cf.C, neg_sampler), learning_rates, ) iters = ut.timeloop(iters_, **cf.term) for i, (w, cont_), negsamp_lst, eta in iters: cont = [x for x in cont_ if x != w] if w in cont_ else cont_ for c, negsamps in zip(cont, negsamp_lst): grad_update(W, negsamps, eta, w=w, c=c) tdur = time.time() - st cf2 = update(cf, iter=i+1) # , norms=norms, gradnorms=gradnorms cf2[&#39;accumsec&#39;] += tdur if not cf2.term and 0: fn = join(cf2.dir, &#39;n{}_e{}.csv&#39;.format(cf.N, cf.epoch)) DataFrame(W, index=vc).to_csv(fn) cf2[&#39;epoch&#39;] += 1 cf2 = update(cf2, iter=0) else: pass # print(i, &#39;iters&#39;) return W, cf2 _ = sgd_np(W=We.copy(), corp=tokl, cf=update(cnf_, term={&#39;iters&#39;: 1000}), **fast_opts) In&nbsp;[38]: def mk_sgd_inplace(cf=None, sampler=None): iters = cf.term.get(&#39;iters&#39;) or 0 @njit def loop(W, toka, eta=None, min_eta=None, C=None, K=None, pad=None): N = iters or len(toka) sub_ixs = np.empty(K + pad, np.int64) etas = np.linspace(eta, min_eta, N) for i in range(N): w, conts = sliding_window_inplace(toka, i, C=C) eta_ = etas[i] for c in conts: sampler(sub_ixs) grad_update_jit_pad(W, sub_ixs, eta_, w=w, c=c) return W kwa = z.keyfilter(lambda x: x in &#39;eta min_eta C K pad&#39;.split(), cf) looper = partial(loop, **kwa) return looper a, inplace_sampler = (neg_sampler_inplace( toka, cfp.K, pow=.75, pad=cfp.pad, ret_type=Array, seed=2)) sgd_inplace = mk_sgd_inplace(cfp, sampler=inplace_sampler) w1 = We.copy() sgd_inplace(w1, toka[:100]) # warmup the JIT compiler; Benchmarks&#182;Now that everything is coded up, it's time to do some benchmarks. I'm curious about how well this numba code compares to both the original numpy version I wrote and the uber-optimized gensim Cython code. For the comparison, I ran the algorithms on the Brown corpus packaged with NLTK (loaded above as an array of integer tokens to variable toka in the Gradient descent section). After running these, I realized that as close as I tried to follow the papers, there's still some interesting implementation differences between gensim and what I came up with, as far as the way the negative sampling gradients are applied to the word vectors. So even after running the algorithms over the same corpus, there's a pretty sizable gap in the word vector norm. To get an idea of how comparable the word vectors are, I compare the accuracy of completing analogies from the respective word vectors, using a file that came with gensim. The file has a bunch of analogies like 'brother' is to 'brothers' as 'sister' is to 'sisters', allowing you to evaluate how well a set of word vectors can complete the analogy. In this case it can be scored on whether it guesses that the vector for 'sisters' ($v_{sisters}$) is the closest to the vector that results from $v_{brothers} - v_{brother} + v_{sister}$. While writing the evaluation code I noticed that some of the completions, while not technically correct may reveal unexpected relationships within the corpus it was trained on. For example, given the relationship of Tokyo being the capital of Japan, word2vec predicted that Warsaw was the capital of the USSR, and Seattle is the capital of the AFL-CIO. (To reproduce the gensim vectors, you'll need to run export PYTHONHASHSEED=42 in the terminal before even starting the ipython notebook, though I'm not even sure how well python's randomness can be reproduced on different systems and versions.) Numba&#182; In&nbsp;[39]: print(&#39;PYTHONHASHSEED={}&#39;.format(os.environ.get(&#39;PYTHONHASHSEED&#39;))) nseed(41) w1, t_numba = timer(sgd_inplace)(We.copy(), toka) print(&#39;Numba score: {}&#39;.format(wut.score_wv(w1, vc))) print(&#39;Numba wordvec norm: {:.2f}&#39;.format(norm(w1))) print(&#39;=&gt; {:.2f}s&#39;.format(t_numba)) PYTHONHASHSEED=42 Numba score: 182 Numba wordvec norm: 351.30 =&gt; 53.53s Numpy&#182; In&nbsp;[40]: nr.seed(41) # 182 w2 = We.copy() (w2, _), t_np = timer(sgd_np)(W=w2, corp=tokl, cf=update(cnf_, term={}), **numpy_opts) print(&#39;Numpy score: {}&#39;.format(wut.score_wv(w2, vc))) print(&#39;Numpy wordvec norm: {:.2f}&#39;.format(norm(w2))) print(&#39;=&gt; {:.2f}s&#39;.format(t_np)) Numpy score: 182 Numpy wordvec norm: 349.22 =&gt; 368.91s Gensim (Cython)&#182; In&nbsp;[41]: import gensim from gensim.models.word2vec import Word2Vec assert gensim.models.word2vec.FAST_VERSION == 1 gparams = ut.to_gensim_params(cnf_) def gensim_eval(mod): ans = mod.accuracy(&#39;src/questions-words.txt&#39;) sect = [d for d in ans if d[&#39;section&#39;] == &#39;total&#39;] return sum([1 for d in sect for _ in d[&#39;correct&#39;]]) 1 epoch In&nbsp;[42]: # %time gen_fast, t_gen = timer(Word2Vec)(brown.sents(), seed=42, iter=1, workers=1, **gparams) print(&#39;Gensim score: {}&#39;.format(gensim_eval(gen_fast))) print(&#39;Gensim wordvec norm: {:.2f}&#39;.format(norm(gen_fast.syn0))) print(&#39;=&gt; {:.2f}s&#39;.format(t_gen)) Gensim score: 109 Gensim wordvec norm: 209.85 =&gt; 10.50s 2 epochs In&nbsp;[43]: gen_fast2, t_gen2 = timer(Word2Vec)(brown.sents(), seed=42, iter=2, workers=1, **gparams) print(&#39;Gensim score: {}&#39;.format(gensim_eval(gen_fast2))) print(&#39;Gensim wordvec norm: {:.2f}&#39;.format(norm(gen_fast2.syn0))) print(&#39;=&gt; {:.2f}s&#39;.format(t_gen2)) Gensim score: 181 Gensim wordvec norm: 243.99 =&gt; 17.21s Timing Ratios&#182;Divide the row function by the column to see the time ratio. An entry of 5 at row Numba and column Gensim would mean that the Numba function takes 5 times as long as the Gensim one. In&nbsp;[44]: ts = Series(dict(Gensim=t_gen, Gensim2=t_gen2, Numba=t_numba, Numpy=t_np)) DataFrame(ts[:, None] / ts[:, None].T, index=ts.index, columns=ts.index).apply(mc(&#39;round&#39;, 2)) Out[44]: Gensim Gensim2 Numba Numpy Gensim 1.00 0.61 0.20 0.03 Gensim2 1.64 1.00 0.32 0.05 Numba 5.10 3.11 1.00 0.15 Numpy 35.13 21.44 6.89 1.00 Conclusion&#182;It looks like with all my numba optimizations, I'm able to get about 7x speedup over numpy, for about the same word vectors (the norms are close and the analogy accuracy score is the same). But despite the numba optimizations, my implementation is still about 5x slower than gensim's Cython version. With the possible implementation differences mentioned above, though, it looks like gensim produces word vectors more similar to mine after 2 epochs, at least in terms of word analogy accuracy. Running 2 gensim epochs gets me down to 'only' 3 times slower, but even this is by limiting gensim to a single worker. I should probably be happy with these numbers, since the gensim contributors really know what they're doing, but going through this has given me a greater appreciation of the dark cython magic they're using behind the scenes. Along with word2vec, this was also my first dive into programming with numba. It was a bit tricky navigating around some restrictions like higher-order functions, and I missed being able to avoid lower level for-loop style programming. Interpreting numba error messages was also a pain point, but I assume that's something I'll get better at the more I use it and the more it's developed. But overall I'm really happy with the speed increases it gives, as well as being able to stick to 'pretty much Python.' Numeric computing projects from the python community haven't ceased to impress me."
}