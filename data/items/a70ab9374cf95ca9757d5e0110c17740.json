{
  "title": "Bridging Information Theory and Machine Learning",
  "link": "https://medium.com/@chris_bour/bridging-information-theory-and-machine-learning-8bb8109db58d?source=rss-90ca8bc640f4------2",
  "guid": "https://medium.com/p/8bb8109db58d",
  "category": [
    "information-theory",
    "statistical-learning",
    "machine-learning",
    "digital-communication"
  ],
  "dc:creator": "Christophe Bourguignat",
  "pubDate": "Sat, 26 Sep 2020 09:27:24 GMT",
  "atom:updated": "2020-09-26T09:27:52.690Z",
  "content:encoded": "<h3>Information Theory</h3><p>Between 1997 and 2000, I learned the fascinating domain of information theory and digital communications:</p><ul><li>source coding: entropy, compression, Huffman, …</li><li>noisy communication channels and their capacity: Shannon limit, …</li><li>error detection and correction codes: Hamming, Hadamard, BCH, convolutional, turbocodes, …</li><li>optimal receiving and decoding: sequential, Viterbi, …</li></ul><p>I applied this knowledge between 2000 and 2013, on applications that were at this time 2G/3G mobile networks, modems (V90, ADSL, …), satellite communications, digital microwave radio links, …</p><p>My reference book was <a href=\"https://www.amazon.com/Digital-Communications-John-Proakis-Fourth/dp/B004IOVT9A\"><strong>Digital Communications</strong></a>, by John G. Proakis.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/404/0*uBZfmhU5Lmh9sMJW.jpg\" /></figure><h3>Machine Learning</h3><p>In 2013, I decided to radically change my field, and started to learn machine learning, and apply it to enterprise data.</p><p>My first source of inspiration has been <a href=\"https://www.coursera.org/learn/machine-learning\"><strong>Standford’s Andrew Ng course</strong></a>, and <a href=\"https://www.amazon.fr/Elements-Statistical-Learning-Inference-Prediction/dp/0387848576\"><strong>Elements of Statistical Learning</strong></a>, by Hastie, Tibshirani, Friedman.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/333/0*7xZazcMCnz5Xayqe.jpg\" /></figure><h3>Bridging the gap</h3><p>In my mind, those two disciplines — information theory on one side, and machine learning on the other side — were two separated domains, corresponding to two distinct periods of my engineering career (2000–2013, and then 2014–2020+).</p><p>But I then recently discovered a new book: <a href=\"https://www.amazon.fr/Information-Theory-Inference-Learning-Algorithms/dp/0521642981\"><strong>Information Theory, Inference and Learning Algorithms</strong></a>, by MacKay, and it made me change my mind.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/400/0*95OPJmz2Fyf05GKx.jpg\" /></figure><p>This book first adresses information theory (data compression, noisy-channel coding, …), and then neural networks. And suddenly, Chapter 40 page 483, <strong>the two fields are reunified</strong>:</p><blockquote>Neural network models involve the adaptation of a set of weights <strong>w</strong> in response to a set of data points, for example a set DN of N target values t1, …tN at given locations <strong>x</strong>1, …<strong>x</strong>N. The adapted weights are then used to process subsequent data. This process can be viewed as a communication process, in which the sender examines the data DN and creates a message <strong>w</strong> that depends on those data. The receiver then uses <strong>w</strong>; for example, the receiver might use the weights to try to reconstruct what the DN was. This is using the neuron for ‘memory’ rather than for ‘generalization’, ie extrapolating from the the observed data to the value of tN+1 at some new location xN+1. The adapted network weights <strong>w</strong> therefore play the role of a communication channel, conveying information about the training data to a future user of that neural net. The question we now address is, ‘what is the capacity of this channel?’ — that is, ‘how much information can be stored by training a neural network?’</blockquote><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*iA4XKZte5Y_JgvKjPihBHg.jpeg\" /></figure><p>How magic is it, to understand that two domains you love, and thought different, are in fact linked.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=8bb8109db58d\" width=\"1\" height=\"1\" alt=\"\">"
}