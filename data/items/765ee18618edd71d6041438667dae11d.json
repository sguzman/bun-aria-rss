{
  "id": "tag:blogger.com,1999:blog-15418143.post-5529343758137673151",
  "published": "2016-06-01T04:21:00.000-05:00",
  "updated": "2016-06-14T03:38:45.285-05:00",
  "category": [
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    ""
  ],
  "title": "Deep Learning Trends @ ICLR 2016",
  "content": "Started by the youngest members of the Deep Learning Mafia [1], namely&nbsp;<a href=\"http://yann.lecun.com/\">Yann LeCun</a> and <a href=\"http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html\">Yoshua Bengio</a>,&nbsp;the ICLR conference is quickly becoming a strong contender for the single <i>most important venue in the Deep Learning space</i>. More intimate than NIPS and less benchmark-driven than CVPR, the world of ICLR is arXiv-based and moves fast.<br /><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://3.bp.blogspot.com/-kVgTFEwiQ-0/V06d4M-6i7I/AAAAAAAAOqQ/TxxSFstFdpw8Y3H9Q3SiRBBRvhXRY5VfwCLcB/s1600/deep_learning_machine_learning_conference_iclr_2016.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"261\" src=\"https://3.bp.blogspot.com/-kVgTFEwiQ-0/V06d4M-6i7I/AAAAAAAAOqQ/TxxSFstFdpw8Y3H9Q3SiRBBRvhXRY5VfwCLcB/s400/deep_learning_machine_learning_conference_iclr_2016.png\" width=\"400\" /></a></div><br /><br />Today's post is all about ICLR 2016. I’ll highlight new strategies for building deeper and more powerful neural networks, ideas for compressing big networks into smaller ones, as well as techniques for building “deep learning calculators.” A host of new artificial intelligence problems is being hit hard with the newest wave of deep learning techniques, and from a computer vision point of view, there's no doubt that <i>deep convolutional neural networks are today's \"master algorithm\" for dealing with perceptual data</i>.<br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><br /></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><b>Deep Powwow in Paradise?</b> <a href=\"http://www.iclr.cc/doku.php?id=iclr2016:main\">ICLR 2016</a> was held in Puerto Rico.&nbsp;</div><br />Whether you're working in Robotics, Augmented Reality, or dealing with a computer vision-related problem, the following summary of ICLR research trends will give you a taste of what's possible on top of today's Deep Learning stack. Consider today's blog post a reading group conversation-starter.<br /><br /><b>Part I: ICLR vs CVPR</b><br /><b>Part II: ICLR 2016 Deep Learning Trends</b><br /><b>Part III: Quo Vadis Deep Learning?</b><br /><h3><br />Part I: ICLR vs CVPR</h3>Last month's International Conference of Learning Representations, known briefly as ICLR 2016, and commonly pronounced as “eye-clear,” could more appropriately be called the <i>International Conference on Deep Learning</i>. The ICLR 2016 conference was held May 2nd-4th 2016 in lovely Puerto Rico. This year was the 4th installment of the conference -- the first was in 2013 and it was initially so small that it had to be co-located with another conference. Because it was started by none other than the Deep Learning Mafia, it should be no surprise that just about everybody at the conference was studying and/or applying Deep Learning Methods. Convolutional Neural Networks (which dominate image recognition tasks) were all over the place, with LSTMs and other Recurrent Neural Networks (used to model sequences and build \"deep learning calculators\") in second place. Most of my own research conference experiences come from CVPR (Computer Vision and Pattern Recognition), and I've been a regular CVPR attendee since 2004. Compared to ICLR, CVPR has a somewhat colder, more-emprical feel. To describe the difference between ICLR and CVPR, Yan LeCun, quoting <a href=\"http://www.cs.toronto.edu/~urtasun/\">Raquel Urtasun</a> (who got the original saying from <a href=\"http://www.cs.utoronto.ca/~fidler/\">Sanja Fidler</a>), put it best on Facebook.<br /><div style=\"text-align: center;\"><br /></div><div style=\"text-align: center;\"><span style=\"font-size: large;\"><b>CVPR:</b> What can Deep Nets do for me?</span></div><div style=\"text-align: center;\"><span style=\"font-size: large;\"><b>ICLR:</b> What can I do for Deep Nets?</span></div><br />The ICLR 2016 conference was my first official powwow that truly felt like a close-knit \"let's share knowledge\" event. 3 days of the main conference, plenty of evening networking events, and no workshops. With a total attendance of about 500, ICLR is about 1/4 the size of CVPR. In fact, CVPR 2004 in D.C. was my first conference ever, and CVPRs are infamous for their packed poster sessions, multiple sessions, and enough workshops/tutorials to make CVPRs last an entire week. At the end of CVPR, you'll have a research hangover and will need a few days to recuperate. I prefer the size and length of ICLR.<br /><br />CVPR and NIPS, like many other top-tier conferences heavily utilizing machine learning techniques, have grown to gargantuan sizes, and paper acceptance rates at these mega conferences are close to 20%. It not necessarily true that the research papers at ICLR were any more half-baked than some CVPR papers, but the amount of experimental validation for an ICLR paper makes it a different kind of beast than CVPR. CVPR’s main focus is to produce papers that are ‘state-of-the-art’ and this essentially means you have to run your algorithm on a benchmark and beat last season’s leading technique. ICLR’s main focus it to highlight new and promising techniques in the analysis and design of deep convolutional neural networks, initialization schemes for such models, and the training algorithms to learn such models from raw data.<br /><br /><b>Deep Learning is Learning Representations</b><br />Yann LeCun and Yoshua Bengio started this conference in 2013 because there was a need to a new, small, high-quality venue with an explicit focus on deep methods. Why is the conference called “Learning Representations?” Because the typical deep neural networks that are trained in an end-to-end fashion actually learn such intermediate representations. Traditional shallow methods are based on manually-engineered features on top of a trainable classifier, but deep methods learn a network of layers which learns those highly-desired features as well as the classifier. So what do you get when you blur the line between features and classifiers? You get representation learning. And this is what Deep Learning is all about.<br /><br /><b>ICLR Publishing Model: arXiv or bust</b><br />At ICLR, papers get posted on arXiv directly. And if you had any doubts that arXiv is just about the single awesomest thing to hit the research publication model since the Gutenberg press, let the success of ICLR be one more data point towards enlightenment. ICLR has essentially bypassed the old-fashioned publishing model where some third party like Elsevier says “you can publish with us and we’ll put our logo on your papers and then charge regular people $30 for each paper they want to read.” Sorry Elsevier, research doesn’t work that way. Most research papers aren’t good enough to be worth $30 for a copy. It is <b>the entire body of academic research that provides true value, for which a single paper just a mere door</b>. You see, <i>Elsevier</i>, if you actually gave the world an exceptional research paper search engine, together with the ability to have 10-20 papers printed on decent quality paper for a $30/month subscription, then you would make a killing on researchers and I would endorse such a subscription. So ICLR, rightfully so, just said fuck it, we’ll use arXiv as the method for disseminating our ideas. <b>All future research conferences should use arXiv to disseminate papers</b>. Anybody can download the papers, see when newer versions with corrections are posted, and they can print their own physical copies. But be warned: <b>Deep Learning moves so fast, that you’ve gotta be hitting refresh or arXiv on a weekly basis or you’ll be schooled by some grad students in Canada.</b><br /><br /><b>Attendees of ICLR</b><br />Google DeepMind and Facebook’s FAIR constituted a large portion of the attendees. A lot of startups, researchers from the Googleplex, Twitter, NVIDIA, and startups such as Clarifai and Magic Leap. Overall a very young and vibrant crowd, and a very solid representation by super-smart 28-35 year olds.<br /><br /><h3>Part II: Deep Learning Themes @ ICLR 2016</h3><b>Incorporating Structure into Deep Learning</b><br /><a href=\"http://www.cs.toronto.edu/~urtasun/\">Raquel Urtasun</a>&nbsp;from the University of Toronto gave a talk about Incorporating Structure in Deep Learning. See <a href=\"http://videolectures.net/iclr2016_urtasun_incoporating_structure/\">Raquel's Keynote video</a> here. Many ideas from structure learning and graphical models were presented in her keynote. Raquel’s computer vision focus makes her work stand out, and she additionally showed some recent research snapshots from her upcoming CVPR 2016 work.<br /><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"http://www.cs.toronto.edu/~fidler/courses/tutorialCVPR15.jpg\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" src=\"http://www.cs.toronto.edu/~fidler/courses/tutorialCVPR15.jpg\" height=\"55\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\">Raquel gave a wonderful&nbsp;<a href=\"http://www.cs.toronto.edu/~fidler/3DsceneTutorialCVPR15.html\">3D Indoor Understanding Tutorial</a>&nbsp;at last year's CVPR 2015.</div><div class=\"separator\" style=\"clear: both; text-align: center;\"><br /></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><br /></div>One of Raquel's strengths is her strong command of geometry, and her work covers both learning-based methods as well as multiple-view geometry. I strongly recommend keeping a close look at her upcoming research ideas. Below are two bleeding edge papers from Raquel's group -- the first one focuses on soccer field localization from a broadcast of such a game using&nbsp;branch and bound inference in a MRF.<br /><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://4.bp.blogspot.com/-tnT9Yh6Q1Fk/Vz4RWM1UsDI/AAAAAAAAOpk/NZzwDimA0I8QQXMi8MZO69_y1De03j55wCLcB/s1600/soccer_field.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"126\" src=\"https://4.bp.blogspot.com/-tnT9Yh6Q1Fk/Vz4RWM1UsDI/AAAAAAAAOpk/NZzwDimA0I8QQXMi8MZO69_y1De03j55wCLcB/s400/soccer_field.png\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\">Raquel's new work. Soccer Field Localization from Single Image. Homayounfar et al, 2016.</div><div class=\"separator\" style=\"clear: both; text-align: center;\"><br /></div><a href=\"http://arxiv.org/abs/1604.02715\">Soccer Field Localization from a Single Image</a>. <a href=\"http://www.cs.toronto.edu/~namdar/\">Namdar Homayounfar</a>, Sanja Fidler, Raquel Urtasun. in arXiv:1604.02715.<br /><br />The second upcoming paper from Raquel's group is on using Deep Learning for Dense Optical Flow, in the spirit of <a href=\"http://arxiv.org/abs/1504.06852\">FlowNet</a>, which I discussed in my <a href=\"http://www.computervisionblog.com/2015/12/iccv-2015-twenty-one-hottest-research.html\">ICCV 2015 hottest papers blog post</a>.&nbsp;The technique is built on the observation that the scene is typically composed of a static background, as well as a relatively small number of traffic participants which move rigidly in 3D. The dense optical flow technique is applied to autonomous driving.<br /><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://3.bp.blogspot.com/-d2rhi4k0TkQ/Vz4STflNjhI/AAAAAAAAOps/R03T3ZfjrOIo_sl6ojUpb0gV9Jw6fVJQwCLcB/s1600/optical_flow_raquel.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"230\" src=\"https://3.bp.blogspot.com/-d2rhi4k0TkQ/Vz4STflNjhI/AAAAAAAAOps/R03T3ZfjrOIo_sl6ojUpb0gV9Jw6fVJQwCLcB/s400/optical_flow_raquel.png\" width=\"400\" /></a></div><br /><a href=\"http://arxiv.org/abs/1604.01827\">Deep Semantic Matching for Optical Flow</a>.&nbsp;Min Bai, Wenjie Luo, Kaustav Kundu, Raquel Urtasun. In arXiv:1604.01827.<br /><br /><b>Reinforcement Learning</b><br /><a href=\"http://homes.cs.washington.edu/~svlevine/\">Sergey Levine</a>&nbsp;gave an excellent Keynote on deep reinforcement learning and its application to Robotics[3]. See <a href=\"http://videolectures.net/iclr2016_levine_deep_learning/\">Sergey's Keynote video</a> here. This kind of work is still the future, and there was very little robotics-related research in the main conference. It might not be surprising, because having an assembly of robotic arms is not cheap, and such gear is simply not present in most grad student research labs. Most ICLR work is pure software and some math theory, so a single GPU is all that is needed to start with a typical Deep Learning pipeline.<br /><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://1.bp.blogspot.com/-q3ZcSqgSTQY/VzuSF9yDc0I/AAAAAAAAOos/CAvlFLniqsg2m7JqHpCG9mQV3erpuHytgCLcB/s1600/robotarms.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"197\" src=\"https://1.bp.blogspot.com/-q3ZcSqgSTQY/VzuSF9yDc0I/AAAAAAAAOos/CAvlFLniqsg2m7JqHpCG9mQV3erpuHytgCLcB/s400/robotarms.png\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\">An army of robot arms jointly learning to grasp somewhere inside Google.</div><br />Take a look at the following interesting work which shows what Alex&nbsp;<a href=\"https://www.cs.toronto.edu/~kriz/\">Krizhevsky</a>, the author of the legendary 2012 AlexNet paper which rocked the world of object recognition, is currently doing. And it has to do with Deep Learning for Robotics, currently at Google.<br /><br /><a href=\"http://arxiv.org/abs/1603.02199\">Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection</a>&nbsp;Sergey Levine, Peter Pastor, Alex Krizhevsky, Deirdre Quillen. In arXiv:1603.02199.<br /><b><br /></b> For those of you who want to learn more about Reinforcement Learning, perhaps it is time to check out <a href=\"http://karpathy.github.io/2016/05/31/rl/\">Andrej Karpathy's Deep Reinforcement Learning: Pong From Pixels</a> tutorial. One thing is for sure: when it comes to deep reinforcement learning, OpenAI is all-in.<br /><b><br /></b> <b>Compressing Networks</b><br /><table cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"float: right; margin-left: 1em; text-align: right;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://3.bp.blogspot.com/hyhK-UtSvzNbxzc_ub-RqRgyoghf4IA5DL4cuxspUGWOyHxdD4YO9Ckxsym7mDFxLGqg=w300\" style=\"clear: right; margin-bottom: 1em; margin-left: auto; margin-right: auto;\"><img border=\"0\" height=\"200\" src=\"https://3.bp.blogspot.com/hyhK-UtSvzNbxzc_ub-RqRgyoghf4IA5DL4cuxspUGWOyHxdD4YO9Ckxsym7mDFxLGqg=w300\" width=\"200\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"font-size: 12.8px; text-align: center;\">Model Compression: The WinZip of<br />Neural Nets?</td></tr></tbody></table>While NVIDIA might be today’s king of Deep Learning Hardware, I can’t help the feeling that there is a new player lurking in the shadows. You see, GPU-based mining of bitcoin didn’t last very long once people realized the economic value of owning bitcoins. Bitcoin very quickly transitioned into specialized FPGA hardware for running the underlying bitcoin computations, and the FPGAs of Deep Learning are right around the corner. Will NVIDIA remain the King? I see a fork in NVIDIA's future. You can continue producing hardware which pleases both gamers and machine learning researchers, or you can specialize. There is a plethora of interesting companies like Nervana Systems, Movidius, and most importantly Google, that don’t want to rely on power-hungry heatboxes known as GPUs, especially when it comes to scaling already trained deep learning models. Just take a look at <a href=\"http://www.movidius.com/solutions/machine-vision-algorithms/machine-learning\">Fathom by Movidius</a> or the <a href=\"https://cloudplatform.googleblog.com/2016/05/Google-supercharges-machine-learning-tasks-with-custom-chip.html\">Google TPU</a>.<br /><br /><br />But the world has already seen the economic value of Deep Nets, and the “software” side of deep nets isn't waiting for the FPGAs of neural nets. <b>The software version of compressing neural networks is a very trendy topic.</b> You basically want to take a beefy neural network and compress it down into smaller, more efficient model. Binarizing the weights is one such strategy. Student-Teacher networks where a smaller network is trained to mimic the larger network are already here. And don’t be surprised if within the next year we’ll see 1MB sized networks performing at the level of Oxford’s VGGNet on the ImageNet 1000-way classification task.<br /><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://4.bp.blogspot.com/-h73q0MJPdZQ/VzuH6aglbtI/AAAAAAAAOnQ/Df8Uqn343ZwrE_VPWFNL0Ccb_eQTZMMEgCLcB/s1600/deep_compress.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"283\" src=\"https://4.bp.blogspot.com/-h73q0MJPdZQ/VzuH6aglbtI/AAAAAAAAOnQ/Df8Uqn343ZwrE_VPWFNL0Ccb_eQTZMMEgCLcB/s400/deep_compress.png\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\">Summary from ICLR 2016's Deep Compression paper by Han et al.</div><br />This year's ICLR brought a slew of Compression papers, the three which stood out are listed below.<br /><br /><a href=\"http://arxiv.org/abs/1510.00149\">Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding</a>.&nbsp;Song Han, Huizi Mao, and Bill Dally. In ICLR 2016. This paper won the Best Paper Award. See Han give the <a href=\"http://videolectures.net/iclr2016_han_deep_compression/\">Deep Compression</a>&nbsp;talk.<br /><br /><a href=\"https://arxiv.org/abs/1510.03009\">Neural Networks with Few Multiplications</a>. Zhouhan Lin, Matthieu Courbariaux, Roland Memisevic, Yoshua Bengio. In ICLR 2016.<br /><br /><a href=\"http://arxiv.org/abs/1511.04561\">8-Bit Approximations for Parallelism in Deep Learning</a>. Tim Dettmers. In ICLR 2016.<br /><b><br /></b> <b>Unsupervised Learning</b><br />Philip Isola presented a very Efrosian paper on using Siamese Networks defined on patches to learn a patch similarity function in an unsupervised way. This patch-patch similarity function was used to create a local similarity graph defined over an image which can be used to discover the extent of objects. This reminds me of the Object Discovery line of research started by Alyosha Efros and the MIT group, where the basic idea is to abstain from using class labels in learning a similarity function.<br /><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://3.bp.blogspot.com/-8a73VKcm2WI/VzuEfhXpKWI/AAAAAAAAOmk/iu50QDSoGx8hejfW_bUmpDwdhsyyBAOqQCLcB/s1600/isola.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"118\" src=\"https://3.bp.blogspot.com/-8a73VKcm2WI/VzuEfhXpKWI/AAAAAAAAOmk/iu50QDSoGx8hejfW_bUmpDwdhsyyBAOqQCLcB/s400/isola.png\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\">Isola et al: A Siamese network has shared weights and can be used for learning embeddings or \"similarity functions.\"</div><br /><a href=\"http://arxiv.org/pdf/1511.06811.pdf\"><br /></a> <a href=\"http://arxiv.org/pdf/1511.06811.pdf\">Learning visual groups from co-occurrences in space and time</a> <a href=\"http://web.mit.edu/phillipi/\">Phillip Isola</a>, Daniel Zoran, Dilip Krishnan, Edward H. Adelson. In ICLR 2016.<br /><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://1.bp.blogspot.com/-ALMDlo_icyA/VzuFbnYbjVI/AAAAAAAAOms/ND7_YlEBIwEW22pPDFO1cUpzIL0PuUAugCLcB/s1600/isola2.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"211\" src=\"https://1.bp.blogspot.com/-ALMDlo_icyA/VzuFbnYbjVI/AAAAAAAAOms/ND7_YlEBIwEW22pPDFO1cUpzIL0PuUAugCLcB/s400/isola2.png\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\">Isola et al: Visual groupings applied to image patches, frames of a video, and a large scene dataset.</div><br /><br /><b>Initializing Networks: And why BatchNorm matters&nbsp;</b><br />Getting a neural network up and running is more difficult than it seems. Several papers in ICLR 2016 suggested new ways of initializing networks. But practically speaking, deep net initialization is “essentially solved.” Initialization seems to be&nbsp;an area of research that truly became more of a “science” than an “art” once researchers introduced <a href=\"http://arxiv.org/abs/1502.03167\">BatchNorm</a> into their neural networks. <i>BatchNorm is the butter of Deep Learning -- add it to everything and everything will taste better.&nbsp;</i>But this wasn’t always the case!<br /><br />In the early days, researchers had lots of problems with constructing an initial set of weights of a deep neural network such that the back propagation could learn anything. In fact, one of the reasons why the Neural Networks of the 90s died as a research program, is precisely because it was well-known that a handful of top researchers knew how to tune their networks so that they could start automatically learning from data, but the other research didn’t know all of the right initialization tricks. It was as if the “black magic” inside the 90s NNs was just too intense. At some point, convex methods and kernel SVMs because the tools of choice — with no need to initialize in a convex optimization setting, for almost a decade (1995 to 2005) researchers just ran away from deep methods. Once 2006 hit, Deep Architectures were working again with Hinton’s magical deep Boltzmann Machines and unsupervised pretraining. Unsupervised pretaining didn’t last long, as researchers got GPUs and found that once your data set is large enough (think ~2 million images in ImageNet), that simple discriminative back-propagation does work. Random weight initialization strategies and cleverly tuned learning rates were quickly shared amongst researchers once 100s of them jumped on the ImageNet dataset. People started sharing code, and wonderful things happened!<br /><br />But designing new neural networks for new problems was still problematic -- one wouldn't know exactly the best way to set multiple learning rates and random initialization magnitudes. But researchers got to work, and a handful of solid hackers from Google found out that the key problem was that poorly initialized networks were having a hard time flowing information through the networks. It’s as if layer N was producing activations in one range and the subsequent layers were expecting information to be of another order of magnitude. So Szegedy and Ioffe from Google proposed a simple “trick” to whiten the flow of data as it passes through the network. Their trick, called “BatchNorm” involves using a normalization layer after each convolutional and/or fully-connected layer in a deep network. This normalization layer whitens the data by subtracting a mean and dividing by a standard deviation, thus producing roughly gaussian numbers as information flows through the network. So simple, yet so sweet. <i>The idea of whitening data is so prevalent in all of machine learning, that it’s silly that it took deep learning researchers so long to re-discover the trick in the context of deep nets.</i><br /><br /><a href=\"https://arxiv.org/abs/1511.06856\">Data-dependent Initializations of Convolutional Neural Networks</a> Philipp Krähenbühl, Carl Doersch, Jeff Donahue, Trevor Darrell. In ICLR 2016. Carl Doersch, a fellow CMU PhD, is going to DeepMind, so there goes another point for DeepMind.<br /><br /><br /><b>Backprop Tricks</b><br />Injecting noise into the gradient seems to work. And this reminds me of the common grad student dilemma where you fix a bug in your gradient calculation, and your learning algorithm does worse. You see, when you were computing the derivative on the white board, you probably made a silly mistake like messing up a coefficient that balances two terms or forgetting an additive / multiplicative term somewhere. &nbsp;However, with a high probability, your “buggy gradient” was actually correlated with the true “gradient”. And in many scenarios, a quantity correlated with the true gradient is better than the true gradient. &nbsp;It is a certain form of regularization that hasn’t been adequately addressed in the research community. <b>What kinds of “buggy gradients” are actually good for learning?</b> And is there a space of “buggy gradients” that are cheaper to compute than “true gradients”? These “FastGrad” methods could speed up training deep networks, at least for the first several epochs. Maybe by ICLR 2017 somebody will decide to pursue this research track.<br /><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://3.bp.blogspot.com/-aNwn04qZ7Go/VzuHQd9OyaI/AAAAAAAAOnE/wDc-Pg1xrCQPn_BmvXalZx-f8y_MEzqpgCLcB/s1600/noise.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"205\" src=\"https://3.bp.blogspot.com/-aNwn04qZ7Go/VzuHQd9OyaI/AAAAAAAAOnE/wDc-Pg1xrCQPn_BmvXalZx-f8y_MEzqpgCLcB/s400/noise.png\" width=\"400\" /></a></div><br /><a href=\"http://arxiv.org/abs/1511.06807\">Adding Gradient Noise Improves Learning for Very Deep Networks</a>. Arvind Neelakantan, Luke Vilnis, Quoc V. Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach, James Martens. In ICLR 2016.<br /><br /><a href=\"http://arxiv.org/abs/1511.06306\">Robust Convolutional Neural Networks under Adversarial Noise</a> Jonghoon Jin, Aysegul Dundar, Eugenio Culurciello. In ICLR 2016.<br /><br /><b>Attention: Focusing Computations</b><br />Attention-based methods are all about treating different \"interesting\" areas with more care than the \"boring\" areas. Not all pixels are equal, and people are able to quickly focus on the interesting bits of a static picture. ICLR 2016's most interesting \"attention\" paper was the Dynamic Capacity Networks paper from <a href=\"https://aaroncourville.wordpress.com/\">Aaron Courville</a>'s group at the University of Montreal. <a href=\"http://www.dmi.usherb.ca/~larocheh/index_en.html\">Hugo Larochelle</a>, another key researcher with strong ties to the Deep Learning mafia, is now a Research Scientist at Twitter.<br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://4.bp.blogspot.com/-x8A-5ffHjAk/Vz4P6Mugm7I/AAAAAAAAOpY/dhgutS6Alv8Ra8bDaF7uIe96GQ-7fQ80ACLcB/s1600/dcn.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"167\" src=\"https://4.bp.blogspot.com/-x8A-5ffHjAk/Vz4P6Mugm7I/AAAAAAAAOpY/dhgutS6Alv8Ra8bDaF7uIe96GQ-7fQ80ACLcB/s400/dcn.png\" width=\"400\" /></a></div><a href=\"http://arxiv.org/abs/1511.07838\">Dynamic Capacity Networks</a> Amjad Almahairi, Nicolas Ballas, Tim Cooijmans, Yin Zheng, Hugo Larochelle, Aaron Courville. In ICLR 2016.<br /><br /><br /><b>The “ResNet trick”: Going Mega Deep because it's Mega Fun</b><br />We saw some new papers on the new “ResNet” trick which emerged within the last few months in the Deep Learning Community. The ResNet trick is the “Residual Net” trick that gives us a rule for creating a deep stack of layers. Because each residual layer essentially learns to either pass the raw data through or mix in some combination of a non-linear transformation, the flow of information is much smoother. This “control of flow” that comes with residual blocks, lets you build VGG-style networks that are quite deep.<br /><br /><a href=\"http://arxiv.org/abs/1602.07261\">Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</a>&nbsp;Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke. In ICLR 2016.<br /><br /><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://4.bp.blogspot.com/-t5YKN7kWXUk/VzuPH1020KI/AAAAAAAAOoU/aGQSTQJoZFMPzWNC0t0Le5zgEHZbDQD4QCLcB/s1600/rir.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"147\" src=\"https://4.bp.blogspot.com/-t5YKN7kWXUk/VzuPH1020KI/AAAAAAAAOoU/aGQSTQJoZFMPzWNC0t0Le5zgEHZbDQD4QCLcB/s400/rir.png\" width=\"400\" /></a></div><br /><a href=\"https://arxiv.org/abs/1603.08029\">Resnet in Resnet: Generalizing Residual Architectures</a>&nbsp;Sasha Targ, Diogo Almeida, Kevin Lyman. In ICLR 2016.<br /><b><br /></b><b>Deep Metric Learning and Learning Subcategories</b><br />A great paper, presented by Manohar Paluri of Facebook, focused on a new way to think about deep metric learning. The paper is “Metric Learning with Adaptive Density Discrimination” and reminds me of my own research from CMU. Their key idea can be distilled to the “anti-category” argument. Basically, you build into your algorithm the intuition that not all elements of a category C1 should collapse into a single unique representation. Due to the visual variety within a category, you only make the assumption that an element X of category C is going to be similar to a subset of other Cs, and not all of them. In their paper, they make the assumption that all members of category C belong to a set of latent subcategories, and EM-like learning alternates between finding subcategory assignments and updating the distance metric. During my PhD, we took this idea even further and build Exemplar-SVMs which were the smallest possible subcategories with a single positive “exemplar” member.<br /><br />Manohar started his research as a member of the FAIR team, which focuses more on R&amp;D work, but metric learning ideas are very product-focused, and the paper is a great example of a technology that seems to be \"product-ready.\" I envision dozens of Facebook products that can benefit from such data-derived adaptive deep distance metrics.<br /><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://3.bp.blogspot.com/-gTAhKIRASiM/VzuOHfb5YiI/AAAAAAAAOoM/lQEaQntSn9sx1gk89oL4ItZb0FXCplRhACLcB/s1600/magnet.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"171\" src=\"https://3.bp.blogspot.com/-gTAhKIRASiM/VzuOHfb5YiI/AAAAAAAAOoM/lQEaQntSn9sx1gk89oL4ItZb0FXCplRhACLcB/s400/magnet.png\" width=\"400\" /></a></div><a href=\"http://arxiv.org/abs/1511.05939\">Metric Learning with Adaptive Density Discrimination</a>. Oren Rippel, Manohar Paluri, Piotr Dollar, Lubomir Bourdev. In ICLR 2016.<br /><br /><br /><b>Deep Learning Calculators</b><br />LSTMs, Deep Neural Turing Machines, and what I call “Deep Learning Calculators” were big at the conference. Some people say, “Just because you can use deep learning to build a calculator, it doesn’t mean you should.\" And for some people, Deep Learning is the Holy-Grail-Titan-Power-Hammer, and everything that can be described with words should be built using deep learning components. Nevertheless, it's an exciting time for Deep Turing Machines.<br /><br />The winner of the Best Paper Award was the paper, Neural Programmer-Interpreters by Scott Reed and Nando de Freitas. An interesting way to blend deep learning with the theory of computation. If you’re wondering what it would look like to use Deep Learning to learn quicksort, then check out their paper. And it seems like Scott Reed is going to Google DeepMind, so you can tell where they’re placing their bets.<br /><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://1.bp.blogspot.com/-0LB4ZLszSOo/VzuI9FcyjOI/AAAAAAAAOnY/2FLtPToVyNstjBdPbLoIavWiNU5B7G7RACLcB/s1600/npi.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"167\" src=\"https://1.bp.blogspot.com/-0LB4ZLszSOo/VzuI9FcyjOI/AAAAAAAAOnY/2FLtPToVyNstjBdPbLoIavWiNU5B7G7RACLcB/s400/npi.png\" width=\"400\" /></a></div><a href=\"http://arxiv.org/abs/1511.06279\">Neural Programmer-Interpreters</a>. Scott Reed, Nando de Freitas. In ICLR 2016.<br /><br />Another interesting paper by some OpenAI guys is “Neural Random-Access Machines” which is going to be another fan favorite for those who love Deep Learning Calculators.<br /><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://2.bp.blogspot.com/-rPhlusUS8BQ/VzuJcpzPGII/AAAAAAAAOnc/PQh4xKXy9_42k9UkiaWh6ig13Cuot58BQCLcB/s1600/nram.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"165\" src=\"https://2.bp.blogspot.com/-rPhlusUS8BQ/VzuJcpzPGII/AAAAAAAAOnc/PQh4xKXy9_42k9UkiaWh6ig13Cuot58BQCLcB/s400/nram.png\" width=\"400\" /></a></div><a href=\"http://arxiv.org/abs/1511.06392\">Neural Random-Access Machines</a>. Karol Kurach, Marcin Andrychowicz, Ilya Sutskever. In ICLR 2016.<br /><b><br /></b> <b>Computer Vision Applications</b><br />Boundary detection is a common computer vision task, where the goal is to predict boundaries between objects. CV folks have been using image pyramids, or multi-level processing, for quite some time. Check out the following Deep Boundary paper which aggregates information across multiple spatial resolutions.<br /><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://2.bp.blogspot.com/-ueMFLkPpDWg/Vz4Nr6MCqBI/AAAAAAAAOpA/nUDIxovuT6QRsYfNazo1DPfmHCvD30MjgCLcB/s1600/segmentation.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"268\" src=\"https://2.bp.blogspot.com/-ueMFLkPpDWg/Vz4Nr6MCqBI/AAAAAAAAOpA/nUDIxovuT6QRsYfNazo1DPfmHCvD30MjgCLcB/s400/segmentation.png\" width=\"400\" /></a></div><a href=\"http://arxiv.org/abs/1511.07386\">Pushing the Boundaries of Boundary Detection using Deep Learning</a> Iasonas Kokkinos, In ICLR 2016.<br /><br />A great application for RNNs is to \"unfold\" an image into multiple layers. In the context of object detection, the goal is to decompose an image into its parts. The following figure explains it best, but if you've been wondering where to use RNNs in your computer vision pipeline, check out their paper.<br /><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://2.bp.blogspot.com/-yRIrE3-QqJk/Vz4OHzSgxUI/AAAAAAAAOpE/AhGwpsQdQE8vswQ59Kss2r1xb8bbhcVEgCLcB/s1600/decompnet.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"225\" src=\"https://2.bp.blogspot.com/-yRIrE3-QqJk/Vz4OHzSgxUI/AAAAAAAAOpE/AhGwpsQdQE8vswQ59Kss2r1xb8bbhcVEgCLcB/s400/decompnet.png\" width=\"400\" /></a></div><a href=\"http://arxiv.org/abs/1511.06449\">Learning to decompose for object detection and instance segmentation</a> Eunbyung Park, Alexander C. Berg. In ICLR 2016.<br /><br />Dilated convolutions are a \"trick\" which allows you to increase your network's receptive field size and scene segmentation is one of the best application domains for such dilations.<br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://4.bp.blogspot.com/-2LqF23x7i7w/Vz4OpSVHzzI/AAAAAAAAOpM/tnydeP_1zLQ7BmuW7_ndoNUcJ0JjrQLOgCLcB/s1600/dilated_convolutions.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"211\" src=\"https://4.bp.blogspot.com/-2LqF23x7i7w/Vz4OpSVHzzI/AAAAAAAAOpM/tnydeP_1zLQ7BmuW7_ndoNUcJ0JjrQLOgCLcB/s400/dilated_convolutions.png\" width=\"400\" /></a></div><br /><a href=\"https://arxiv.org/abs/1511.07122\">Multi-Scale Context Aggregation by Dilated Convolutions</a> Fisher Yu, Vladlen Koltun. In ICLR 2016.<br /><br /><b><br /></b> <b><br /></b> <b>Visualizing Networks</b><br />Two of the best “visualization” papers were “Do Neural Networks Learn the same thing?” by<br /><a href=\"http://yosinski.com/\">Jason Yosinski</a> (now going to <a href=\"http://www.geometricintelligence.com/\">Geometric Intelligence, Inc.</a>) and “Visualizing and Understanding Recurrent Networks” presented by Andrej Karpathy (now going to <a href=\"https://openai.com/\">OpenAI</a>). Yosinski presented his work on studying what happens when you learn two different networks using different initializations. Do the nets learn the same thing? I remember a great conversation with Jason about figuring out if the neurons in network A can be represented as linear combinations of network B, and his visualizations helped make the case. Andrej’s visualizations of recurrent networks are best consumed in presentation/blog form[2]. For those of you that haven’t yet seen Andrej’s analysis of Recurrent Nets on Hacker News, check it out <a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\">here</a>.<br /><a href=\"http://arxiv.org/abs/1511.07543\"><br /></a><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://3.bp.blogspot.com/-xUV5xuzWXss/VzuJ8xC4n9I/AAAAAAAAOnk/Fvqx11nQDPc45cCFJ-aIjfghE2hhubaIgCLcB/s1600/jason.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"187\" src=\"https://3.bp.blogspot.com/-xUV5xuzWXss/VzuJ8xC4n9I/AAAAAAAAOnk/Fvqx11nQDPc45cCFJ-aIjfghE2hhubaIgCLcB/s400/jason.png\" width=\"400\" /></a></div><a href=\"http://arxiv.org/abs/1511.07543\">Convergent Learning: Do different neural networks learn the same representations?</a> Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, John Hopcroft. In ICLR 2016. See <a href=\"http://videolectures.net/iclr2016_yosinski_convergent_learning/\">Yosinski's video here.</a><br /><br /><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://2.bp.blogspot.com/-6uqAR3BM4ew/VzuKdEhrhFI/AAAAAAAAOns/Z-MeozwrTDwAVBtw_T-YKquhNezChFOBACLcB/s1600/karpathy.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"202\" src=\"https://2.bp.blogspot.com/-6uqAR3BM4ew/VzuKdEhrhFI/AAAAAAAAOns/Z-MeozwrTDwAVBtw_T-YKquhNezChFOBACLcB/s400/karpathy.png\" width=\"400\" /></a></div><a href=\"https://arxiv.org/abs/1506.02078\">Visualizing and Understanding Recurrent Networks</a> Andrej Karpathy, Justin Johnson, Li Fei-Fei. In ICLR 2016.<br /><br /><b>Do Deep Convolutional Nets Really Need to be Deep (Or Even Convolutional)?&nbsp;</b><br /><table cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"float: right; margin-left: 1em; text-align: right;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-zyh8fC4l0Dg/VzuLLC5doJI/AAAAAAAAOn4/EYCgIR07ZWYfDTBKGTEXjKsqmjUWNc4ywCLcB/s1600/caruana.png\" style=\"clear: right; margin-bottom: 1em; margin-left: auto; margin-right: auto;\"><img border=\"0\" height=\"200\" src=\"https://1.bp.blogspot.com/-zyh8fC4l0Dg/VzuLLC5doJI/AAAAAAAAOn4/EYCgIR07ZWYfDTBKGTEXjKsqmjUWNc4ywCLcB/s200/caruana.png\" width=\"113\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Figure from Do Nets have to be Deep?</td></tr></tbody></table>This was the key question asked in the paper presented by Rich Caruana. (Dr. Caruana is now at Microsoft, but I remember meeting him at Cornell eleven years ago) Their papers' two key results which are quite meaningful if you sit back and think about them. First, there is something truly special about convolutional layers that when applied to images, they are significantly better than using solely fully connected layers -- there’s something about the 2D structure of images and the 2D structures of filters that makes convolutional layers get a lot of value out of their parameters. Secondly, we now have teacher-student training algorithms which you can use to have a shallower network “mimic” the teacher’s responses on a large dataset. These shallower networks are able to learn much better using a teacher and in fact, such shallow networks produce inferior results when the are trained on the teacher’s training set. &nbsp;So it seems you get go [Data to MegaDeep], and [MegaDeep to MiniDeep], but you cannot directly go from [Data to MiniDeep].<br /><br /><br /><a href=\"https://arxiv.org/abs/1603.05691\">Do Deep Convolutional Nets Really Need to be Deep (Or Even Convolutional)?</a> Gregor Urban, Krzysztof J. Geras, Samira Ebrahimi Kahou, Ozlem Aslan, Shengjie Wang, Rich Caruana, Abdelrahman Mohamed, Matthai Philipose, Matt Richardson. In ICLR 2016.<br /><br /><br />Another interesting idea on the [MegaDeep to MiniDeep] and [MiniDeep to MegaDeep] front,<br /><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://4.bp.blogspot.com/-bshX-UYGcBw/VzuLyYRek6I/AAAAAAAAOoA/MGatuBrhEMw5yFVX89O1AdHxCTTpkHldgCLcB/s1600/net2deepernet.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"97\" src=\"https://4.bp.blogspot.com/-bshX-UYGcBw/VzuLyYRek6I/AAAAAAAAOoA/MGatuBrhEMw5yFVX89O1AdHxCTTpkHldgCLcB/s400/net2deepernet.png\" width=\"400\" /></a></div><br /><br /><a href=\"http://arxiv.org/abs/1511.05641\">Net2Net: Accelerating Learning via Knowledge Transfer</a> Tianqi Chen, Ian Goodfellow, Jonathon Shlens. In ICLR 2016.<br /><br /><br /><b>Language Modeling with LSTMs</b><br />There was also considerable focus on methods that deal with large bodies of text. Chris Dyer (who is supposedly also going to DeepMind), gave a keynote asking the question “Should Model Architecture Reflect Linguistic Structure?” See <a href=\"http://videolectures.net/iclr2016_dyer_model_architecture/\">Chris Dyer's Keynote video here.</a> Some of his key take-aways from comparing word-level embedding vs character-level embeddings&nbsp;is that for different languages, different methods work better. &nbsp;For languages which have a rich syntax, character-level encodings outperform word-level encodings.<br /><br /><a href=\"http://arxiv.org/abs/1508.00657\">Improved Transition-Based Parsing by Modeling Characters instead of Words with LSTMs</a> Miguel Ballesteros, Chris Dyer, Noah A. Smith.&nbsp;In Proceedings of EMNLP 2015.<br /><br />An interesting approach, with a great presentation by Ivan Vendrov, was “Order-Embeddings of Images and Language\" by Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun which showed a great intuitive coordinate-system-y way for thinking about concepts. I really love these coordinate system analogies and I’m all for new ways of thinking about classical problems.<br /><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://1.bp.blogspot.com/-MZtgHEPVtgc/VzuRW9bBTMI/AAAAAAAAOok/cUasXjkQArU8RcdN5Sbx1XpZ0XVba6XuwCLcB/s1600/order.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"320\" src=\"https://1.bp.blogspot.com/-MZtgHEPVtgc/VzuRW9bBTMI/AAAAAAAAOok/cUasXjkQArU8RcdN5Sbx1XpZ0XVba6XuwCLcB/s320/order.png\" width=\"314\" /></a></div><br /><a href=\"http://arxiv.org/abs/1511.06361\"><br /></a> <a href=\"http://arxiv.org/abs/1511.06361\">Order-Embeddings of Images and Language</a> Ivan Vendrov, Ryan Kiros, Sanja Fidler, Raquel Urtasun. In ICLR 2016. <a href=\"http://videolectures.net/iclr2016_vendrov_order_embeddings/\">See Video here.</a><br /><br /><br /><br /><b>Training-Free Methods: Brain-dead applications of CNNs to Image Matching</b><br /><br />These techniques use the activation maps of deep neural networks trained on an ImageNet classification task for other important computer vision tasks. These techniques employ clever ways of matching image regions and from the following ICLR paper, are applied to smart image retrieval.<br /><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://4.bp.blogspot.com/-klhAxCYUJtU/VzuGlS1XnrI/AAAAAAAAOm4/q4XI90vBGno8jGZdG87IJEzbIkkJRQumgCLcB/s1600/mac.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"130\" src=\"https://4.bp.blogspot.com/-klhAxCYUJtU/VzuGlS1XnrI/AAAAAAAAOm4/q4XI90vBGno8jGZdG87IJEzbIkkJRQumgCLcB/s400/mac.png\" width=\"400\" /></a></div><a href=\"http://arxiv.org/abs/1511.05879\">Particular object retrieval with integral max-pooling of CNN activations</a>. Giorgos Tolias, Ronan Sicre, Hervé Jégou. In ICLR 2016.<br /><br />This reminds me of the RSS 2015 paper which uses ConvNets to match landmarks for a relocalization-like SLAM task.<br /><br /><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://2.bp.blogspot.com/-_gd5f9tSbv8/VzuGqXBphKI/AAAAAAAAOm8/w0umiPYN8yA6Mhke88kHXhoDZZhF4KUxgCLcB/s1600/rss.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"71\" src=\"https://2.bp.blogspot.com/-_gd5f9tSbv8/VzuGqXBphKI/AAAAAAAAOm8/w0umiPYN8yA6Mhke88kHXhoDZZhF4KUxgCLcB/s400/rss.png\" width=\"400\" /></a></div><a href=\"http://eprints.qut.edu.au/84931/1/rss15_placeRec.pdf\">Place Recognition with ConvNet Landmarks: Viewpoint-Robust, Condition-Robust, Training-Free</a>.&nbsp;Niko Sunderhauf, Sareh Shirazi, Adam Jacobson, Feras Dayoub, Edward Pepperell, Ben Upcroft, and Michael Milford. In RSS 2015.<br /><br /><b><br class=\"Apple-interchange-newline\" />Gaussian Processes and Auto Encoders</b><br />Gaussian Processes used to be quite popular at NIPS, sometimes used for vision problems, but mostly “forgotten” in the era of Deep Learning. VAEs or Variational Auto Encoders used to be much more popular when pertaining was the only way to train deep neural nets. However, with new techniques like adversarial networks, people keep revisiting Auto Encoders, because we still “hope” that something as simple as an encoder / decoder network should give us the unsupervised learning power we all seek, deep down inside. VAEs got quite a lot of action but didn't make the cut for today's blog post.<br /><br /><b>Geometric Methods</b><br />Overall, very little content pertaining to the SfM / SLAM side of the vision problem was present at ICLR 2016. This kind of work is very common at CVPR, and it's a bit of a surprise that there wasn't a lot of Robotics work at ICLR. It should be noted that the techniques used in SfM/SLAM are more based on multiple-view geometry and linear algebra than the data-driven deep learning of today.<br /><br />Perhaps a better venue for Robotics and Deep Learning will be the June 2016 workshop titled&nbsp;<a href=\"http://juxi.net/workshop/deep-learning-rss-2016/\">Are the Sceptics Right? Limits and Potentials of Deep Learning in Robotics.</a>&nbsp;This workshop is being held at RSS 2016, one of the world's leading Robotics conferences.<br /><br /><h3>Part III: Quo Vadis Deep Learning?</h3>Neural Net Compression is going to be big -- real-world applications demand it. The algos guys aren't going to wait for TPU and VPUs to become mainstream. Deep Nets which can look at a picture and tell you what’s going on are going to be inside every single device which has a camera. In fact, I don’t see any reason why all cameras by 2020 won’t be able to produce a high-quality RGB image as well as a neural network response vector. New image formats will even have such “deep interpretation vectors” directly saved alongside the image. And it's all going to be a neural net, in one shape or another.<br /><br />OpenAI had a strong presence at ICLR 2016, and I feel like every week a new PhD joins OpenAI. Google DeepMind and Facebook FAIR had a large number of papers. Google demoed a real-time version of deep-learning based style transfer using TensorFlow. Microsoft is no longer King of research. Startups were giving out little toys -- Clarifai even gave out free sandals. Graduates with well-tuned Deep Learning skills will continue being in high-demand, but once the next generation of AI-driven startups emerge, it is only those willing to transfer their academic skills into a product world-facing focus, aka the upcoming wave of deep entrepreneurs, that will make serious $$$.<br /><br />Research-wise, arXiv is a big productivity booster. Hopefully, now you know where to place your future deep learning research bets, have enough new insights to breath some inspiration into your favorite research problem, and you've gotten a taste of where the top researchers are heading. I encourage you to turn off your computer and have a white-board conversation with your colleagues about deep learning. Grab a friend, teach him some tricks.<br /><br />I'll see you all at CVPR 2016. Until then, keep learning.<br /><br /><h3>Related computervisionblog.com Blog Posts</h3><a href=\"http://www.computervisionblog.com/2012/05/why-your-vision-lab-needs-reading-group.html\">Why your lab needs a reading group</a>.&nbsp;May 2012<br /><a href=\"http://www.computervisionblog.com/2015/12/iccv-2015-twenty-one-hottest-research.html\">ICCV 2015: 21 Hottest Research Papers</a>&nbsp;December 2015<br /><a href=\"http://www.computervisionblog.com/2015/06/deep-down-rabbit-hole-cvpr-2015-and.html\">Deep Down the Rabbit Hole: CVPR 2015 and Beyond</a>&nbsp;June 2015<br /><a href=\"http://www.computervisionblog.com/2015/11/the-deep-learning-gold-rush-of-2015.html\">The Deep Learning Gold Rush of 2015</a>&nbsp;November 2015<br /><a href=\"http://www.computervisionblog.com/2015/03/deep-learning-vs-machine-learning-vs.html\">Deep Learning vs Machine Learning vs Pattern Recognition</a>&nbsp;March 2015<br /><a href=\"http://www.computervisionblog.com/2015/04/deep-learning-vs-probabilistic.html\">Deep Learning vs Probabilistic Graphical Models</a>&nbsp;April 2015<br /><a href=\"http://www.computervisionblog.com/2016/01/why-slam-matters-future-of-real-time.html\">Future of Real-time SLAM and \"Deep Learning vs SLAM\"</a>&nbsp;January 2016<br /><br /><h3>Relevant Outside Links</h3>[1]&nbsp;<a href=\"http://www.recode.net/2015/7/15/11614684/ai-conspiracy-the-scientists-behind-deep-learning\">Welcome to the AI Conspiracy: The 'Canadian Mafia' Behind Tech's Latest Craze</a>&nbsp;@ &lt;re/code&gt;<br />[2]&nbsp;<a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\">The Unreasonable Effectiveness of Recurrent Neural Networks</a>&nbsp;@ Andrej Karpathy's Blog<br />[3]&nbsp;<a href=\"http://googleresearch.blogspot.com/2016/03/deep-learning-for-robots-learning-from.html\">Deep Learning for Robots: Learning from Large-Scale Interaction</a>.&nbsp;@ Google Research Blog<br /><br />",
  "link": [
    "",
    "",
    "",
    "",
    ""
  ],
  "author": {
    "name": "Tomasz Malisiewicz",
    "uri": "http://www.blogger.com/profile/17507234774392358321",
    "email": "noreply@blogger.com",
    "gd:image": ""
  },
  "media:thumbnail": "",
  "thr:total": 14
}