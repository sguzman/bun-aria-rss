{
  "title": "Adaptive data analysis",
  "description": "<p>I just returned from <a href=\"https://nips.cc/Conferences/2015/\">NIPS 2015</a>, a joyful week \nof corporate parties featuring deep learning themed cocktails, <a href=\"http://www.nytimes.com/2015/12/12/science/artificial-intelligence-research-center-is-founded-by-silicon-valley-investors.html?_r=0\">money\ntalk</a>,\nrecruiting events, and some scientific activities on the side. In the latter\ncategory, I co-organized a <a href=\"http://wadapt.org\">workshop on adaptive data\nanalysis</a> with Vitaly Feldman, Aaron Roth and Adam Smith.</p>\n\n<p>Our workshop responds to an increasingly pressing issue in machine learning and\nstatistics. The classic view of these fields has it that we choose our method\nindependently of the data to which we intend to apply the method. For example,\na hypothesis test must be fixed in advance before we see the data. I sometimes\ncall this <em>static</em> or <em>confirmatory</em> data analysis. You need to know exactly\nwhat you want to do before you collect the data and run your experiment. In\ncontrast, in practice we typically choose our methods as a function of the data\nto which we apply them. In other words, we <em>adapt</em> our method to the data.</p>\n\n<p><img src=\"/assets/static-adaptive.jpg\" alt=\"static vs adaptive\" /></p>\n\n<p>Adaptivity is both powerful and dangerous. Working adaptively gives us greater\nflexibility to make unanticipated discoveries as it allows us to execute more\ncomplex analysis work flows. But it can also lead to false discovery and\nmisleading conclusions far more easily than static data analysis. While there\nare many other issues to keep in mind,  is not unreasonable to blame our lack\nof understanding adaptivity in part for exacerbating a range of problems from\n<a href=\"http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf\">false discovery in the empirical\nsciences</a>\nto <a href=\"http://blog.mrtz.org/2015/03/09/competition.html\">overfitting in machine learning\ncompetitions</a>.</p>\n\n<p>I was hugely impressed with the incredibly exciting group of participants and\naudiences from both computer science and statistics that our workshop brought\ntogether. I felt that we made actual progress on beginning to understand the\ndiverse perspectives on this complex issue. The goal of this post is to convey\nmy excitement for this emerging research area as I attempt to summarize the\ndifferent perspectives we saw.</p>\n\n<h2 id=\"the-frequentist-statistics-perspective\">The frequentist statistics perspective</h2>\n\n<p>Null hypothesis tests are still widely used across the empirical sciences to\ngauge the validity of findings. Scientists routinely calculate\n<a href=\"https://en.wikipedia.org/wiki/P-value\">p-values</a> with the hope of being able\nto reject a null hypothesis and thus claim a “statistically significant”\nfinding. If we carry out multiple hypothesis tests, we need to adjust our\np-values for the fact that we made multiple tests. A safe way of correcting for\nmultiple tests is the <a href=\"https://en.wikipedia.org/wiki/Bonferroni_correction\">Bonferroni\ncorrection</a> which amounts\nto multiplying all p-values by the number of tests. Computer scientists call this\nthe union bound. While Bonferroni is safe, it makes discoveries difficult in\nthe common situation where we have lots of tests and no individual signal is\nparticularly strong. A more\n<a href=\"https://en.wikipedia.org/wiki/Statistical_power\">powerful</a> alternative to\nBonferroni is to control the <a href=\"https://en.wikipedia.org/wiki/False_discovery_rate\">False Discovery\nRate</a> (FDR) proposed in a\ncelebrated paper by Benjamini and Hochberg. Intuitively, controlling FDR\namounts to putting a bound on the expected ratio of all false discoveries\n(number of rejected true null hypotheses) to all discoveries (number of\nrejected nulls). The famous Benjamini-Hochberg procedure gives one beautiful\nway of doing this.</p>\n\n<h3 id=\"online-false-discovery-rate\">Online False Discovery Rate</h3>\n\n<p>Andrea Montanari and Dean Foster discussed more recent works on <a href=\"http://arxiv.org/abs/1502.06197\">online\nvariants of FDR</a>. Here, the goal is to control\nFDR not at the end of the testing process, but rather at all points along the\nway. In particular, the scientist must choose whether or not to reject a\nhypothesis at any time point without knowing the outcome of future tests. The\nword <em>online</em> should not be confused with <em>adaptive</em>. Although we could in\nprinciple choose hypotheses adaptively in this framework, we still need the\ntraditional assumptions that all p-values are independent of each other and\ndistributed the way they should be (i.e., uniform if the null hypothesis is\ntrue). If the selection of hypothesis tests was truly adaptive, these\nassumptions are unlikely to be satisfied and hard to be verified at any rate.</p>\n\n<h3 id=\"inference-after-selection\">Inference after selection</h3>\n\n<p>But what if our hypothesis tests are chosen as a function of the data? For\nexample, what if we first choose a set of promising data attributes and test\nonly these attributes for significance? This natural two-step procedure is\ncalled <em>inference after selection</em> in statistics. Rob Tibshirani gave an entire\nkeynote about this topic. Will Fithian went into further detail in his talk at\nour workshop. Several recent works in this area show how we can first perform\nvariable selection using an algorithm such as\n<a href=\"http://statweb.stanford.edu/~tibs/lasso.html\">Lasso</a>, followed by hypothesis\ntesting on the selected variables. What makes these results possible is a\ncareful analysis of the distribution of the p-values conditional on the\nselection by Lasso. For example, if the test statistic followed a normal\ndistribution before selection, it will follow a certain truncated normal\ndistribution after selection. This approach leads to very accurate\ncharacterizations and often tight confidence intervals. However, it has the\nshortcoming that the we need to commit to a particular selection and inference\nprocedure as the analysis crucially exploits these.</p>\n\n<p>Rina Foygel Barber expanded on the theme of adaptivity and false discovery rate\nby showing how to control FDR when we first <a href=\"http://arxiv.org/abs/1505.07352\">order our hypothesis\ntests</a> in a data-dependent manner to allow for\nmaking important discoveries sooner.</p>\n\n<h2 id=\"the-bayesian-view\">The Bayesian view</h2>\n\n<p>Andrew Gelman’s talk (see <a href=\"http://wadapt.org/slides/gelman.pdf\">his slides</a>)\ncontributed many illustrative examples of flawed empirical studies. His\nsuggested cure for the woes of adaptivity was to do <em>more</em> of it. However, he\nadvocated <a href=\"https://en.wikipedia.org/wiki/Bayesian_hierarchical_modeling\">Bayesian hierarchical\nmodeling</a> to\nexplicitly account for all the possible outcomes of an adaptive study. When\nasked if this wouldn’t put too much burden on the side of modeling, he replied\nthat modeling should be seen as a feature as it forces people to explicitly\ndiscuss the experimental setup.</p>\n\n<h2 id=\"the-stability-approach\">The stability approach</h2>\n\n<p>An intriguing approach to generalization in machine learning is the idea of\n<em>stability</em>. An algorithm is <em>stable</em> if its output changes only slightly (in\nsome formal sense) under an arbitrary substitution of a single input data\npoint. A seminal work of Bousquet and Elisseeff showed that <a href=\"http://www.jmlr.org/papers/volume2/bousquet02a/bousquet02a.pdf\">stability implies\ngeneralization</a>.\nThat is, patterns observed by a <em>stable</em> algorithm on a sample must also exist\nin the underlying population from which the sample was drawn. Nati Srebro\nexplained in his talk how stability is a universal property in the sense that\nit is both <a href=\"http://jmlr.csail.mit.edu/papers/volume11/shalev-shwartz10a/shalev-shwartz10a.pdf\">sufficient and necessary for\nlearnability</a>.</p>\n\n<p><img src=\"/assets/stability.png\" alt=\"stability\" /></p>\n\n<p>The issue is that stability by itself doesn’t address adaptivity. Indeed, the\nclassic works on stability apply to the typical <em>non-adaptive</em> setting of\nlearning where the sample is independent of the learning algorithm.</p>\n\n<p>Cynthia Dwork talked about recent works that address this shortcoming.\nSpecifically, differential privacy is a stability notion that applies even in\nthe setting of adaptive data analysis. Hence, <a href=\"http://arxiv.org/abs/1411.2664\">differential privacy implies\nvalidity in adaptive data analysis</a>. Drawing on\nmany powerful adaptive data analysis algorithms from differential privacy, this\ngives a range of statistically valid tools for adaptive data analysis. See, for\nexample, my blog post on the <a href=\"http://googleresearch.blogspot.com/2015/08/the-reusable-holdout-preserving.html\">reusable holdout\nmethod</a>\nthat came out of this line of work.</p>\n\n<h2 id=\"information-theoretic-measures\">Information-theoretic measures</h2>\n\n<p>The intuition behind differential privacy is that if an analysis does not\nreveal too much about the specifics of a data set, then it is impossible to\noverfit to the data set based on the available information. This suggests an\ninformation-theoretic approach that quantifies the <em>mutual information</em> between\nthe sample and the output of the algorithm. A <a href=\"http://arxiv.org/abs/1506.02629\">recent\nwork</a> shows that indeed certain strengthenings\nof mutual information prevent overfitting. Being completely general, this\nviewpoint allows us, for example, to discuss deterministic algorithms, whereas\nall differentially private algorithms are randomized. James Zou discussed the\ninformation-theoretic approach further and mentioned his <a href=\"http://arxiv.org/abs/1511.05219\">recent\nwork</a> on this topic.</p>\n\n<h2 id=\"complexity-theoretic-obstructions\">Complexity-theoretic obstructions</h2>\n\n<p>In a dramatic turn of events, Jon Ullman told us why <a href=\"http://arxiv.org/abs/1408.1655\">preventing false\ndiscovery in adaptive data analysis can be computationally\nintractable</a>.</p>\n\n<p>To understand the result, think of adaptive data analysis for a moment as an\ninteractive protocol between the data analyst and the algorithm. The algorithm\nhas access to a sample of size \\(n\\) and the analyst wants to learn about the\nunderlying population by asking the algorithm a sequence of adaptively chosen\nquestions.</p>\n\n<p>What Jon talked about is that there is a computationally efficient adaptive\ndata analyst that can force any computationally efficient algorithm on a sample\nof size \\(n\\) to return a completely invalid estimate after no more than\n\\(n^2\\) adaptively chosen questions. Non-adaptively, this would require\nexponentially many questions. While this is a worst-case hardness result, it\npoints to a computational barrier for what might have looked like a purely\nstatistical problem. The \\(n^2\\) bound turns out to be tight in light of the\ndifferential privacy based positive results I mentioned earlier.</p>\n\n<h2 id=\"panel-discussion\">Panel discussion</h2>\n\n<p>Toward the end of our workshop, we had a fantastic panel discussion on open\nproblems and conceptual questions. I don’t remember all of it, unfortunately.\nBelow are some topics that got stuck in my mind. If you remember more, please\nleave a comment.</p>\n\n<h3 id=\"exact-modeling-versus-worst-case-assumptions\">Exact modeling versus worst-case assumptions</h3>\n\n<p>The approaches we saw cluster around two extremes. One is the case of exact\nanalysis or modeling, for example, in the work on inference after selection. In\nthese cases, we are able to exactly analyze the conditional distributions\narising as a result of adaptivity and adjust our methods accordingly. The work\non differential privacy in contrast makes no assumptions on the analyst. Both\napproaches have advantages and disadvantages. One is more exact and\nquantitatively less wasteful, but applies only to specific procedures. The\nother is more general and less restrictive to the analyst, but this generality\nalso leads to hardness results. A goal for future work is to find middle\nground.</p>\n\n<h3 id=\"human-adaptivity-versus-algorithmic-adaptivity\">Human adaptivity versus algorithmic adaptivity</h3>\n\n<p>There is a curious distinction we need to draw. Algorithms can be adaptive.\n<a href=\"http://blog.mrtz.org/2013/09/07/the-zen-of-gradient-descent.html\">Gradient\ndescent</a>, for\nexample, is adaptive because it probes the data at a sequence of adaptively\nchosen points. Lasso followed by inference is another example of a single\nalgorithm that exhibits some adaptive behavior. However, adaptivity also arises\nthrough the way humans work adaptively with the data. It even arises when a\ngroup of researchers study the same data set through a sequence of\npublications. Each publication builds on previous insights and is hence\nadaptive. It is much harder to analyze human adaptivity than algorithmic\nadaptivity. How can we even quantify the extent to which this is a problem?\nTake <a href=\"http://yann.lecun.com/exdb/mnist/\">MNIST</a>, for example. For almost two\ndecades this data set has been used as a benchmark with the <em>same</em> test set.\nAlthough no individual algorithm is directly trained against the test set, it\nis quite likely that the sequence of proposed algorithms over the years\nstrongly overfits to the test data. It seems that this is an issue we should\ntake more seriously.</p>\n\n<h3 id=\"building-better-intuition-in-practice\">Building better intuition in practice</h3>\n\n<p>Any practical application of the algorithms we discussed will likely violate\neither the modeling assumptions we make or the quantitative regime to which our\ntheory applies. This shouldn’t be surprising. Even in non-adaptive data\nanalysis we make assumptions (such as normal approximations) that are not\ncompletely true in practice. What is different is that we have far less\nintuition for what works in practice in the adaptive setting.</p>\n\n<h3 id=\"whats-next\">What’s next?</h3>\n\n<p>After this workshop, I’m even more convinced that adaptive data analysis\nis here to stay. I expect lots of work in this area with many new theoretical\ninsights. I also hope that these developments will lead to new tools that make\nthe practice of machine learning and statistics more reliable.</p>",
  "pubDate": "Mon, 14 Dec 2015 02:00:00 +0000",
  "link": "http://blog.mrtz.org/2015/12/14/adaptive-data-analysis.html",
  "guid": "http://blog.mrtz.org/2015/12/14/adaptive-data-analysis.html",
  "category": [
    "tcs",
    "statistics"
  ]
}