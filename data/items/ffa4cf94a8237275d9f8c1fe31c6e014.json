{
  "title": 2020,
  "link": "https://blogs.princeton.edu/imabandit/2020/10/13/2020/",
  "comments": "https://blogs.princeton.edu/imabandit/2020/10/13/2020/#comments",
  "dc:creator": "Sebastien Bubeck",
  "pubDate": "Wed, 14 Oct 2020 03:14:07 +0000",
  "category": "Uncategorized",
  "guid": "https://blogs.princeton.edu/imabandit/?p=1437",
  "description": "<p>My latest post on this blog was on December 30th 2019. It seems like a lifetime away. The rate at which paradigm shifting events have been happening in 2020 is staggering. And it might very well be that the worst &#8230; <a href=\"https://blogs.princeton.edu/imabandit/2020/10/13/2020/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a></p>\n",
  "content:encoded": "<p>My latest post on this blog was on December 30th 2019. It seems like a lifetime away. The rate at which paradigm shifting events have been happening in 2020 is staggering. And it might very well be that the worst of 2020 is ahead of us, especially for those of us currently in the USA. </p>\n<p>When I started communicating online broadly (blog, twitter) I promised myself to keep it strictly about science (or very closely neighboring topics), so the few lines above is all I will say about the current worldwide situation.</p>\n<p>In other news, as is evident from the 10 months hiatus in blogging, I have taken elsewhere (at least temporarily) my need for rapid communication about theorems that currently excite me. Namely to <a href=\"https://www.youtube.com/sebastienbubeck\" class=\"liinternal\">youtube</a>. Since the beginning of the pandemic I have been recording home videos of what would have been typically blog posts, with currently 5 such videos:</p>\n<ol>\n<li><a href=\"https://youtu.be/uRarIjJGmhs\" class=\"liinternal\">A law of robustness for neural networks</a> : I explain the conjecture we recently made that, for random data, any interpolating two-layers neural network must have its Lipschitz constant larger than the squareroot of the ratio between the size of the data set and the number of neurons in the network. This would prove that overparametrization is *necessary* for robustness.</li>\n<li><a href=\"https://youtu.be/U-XsUB69mvc\" class=\"liinternal\">Provable limitations of kernel methods</a> : I give the proof by Zeyuan Allen-Zhu and Yuanzhi Li that there are simple noisy learning tasks where *no kernel* can perform well while simple two-steps procedures can learn.</li>\n<li><a href=\"https://youtu.be/6-GBDpe2kuI\" class=\"liinternal\">Memorization with small neural networks</a> : I explain old (classical combinatorial) and new (NTK style) construction of optimally-sized interpolating two-layers neural networks.</li>\n<li><a href=\"https://youtu.be/HIwZH2C--nA\" class=\"liinternal\">Coordination without communication</a> : This video is the only one in the current series where I don&#8217;t talk at all about neural networks. Specifically it is about the cooperative multiplayer multiarmed bandit problem. I explain the strategy we devised with Thomas Budzinski to solve this problem (for the stochastic version) without *any* collision at all between the players.</li>\n<li><a href=\"https://youtu.be/O84mcq7P_es\" class=\"liinternal\">Randomized smoothing for certified robustness</a> : Finally, in the first video chronologically, I explain the only known technique for provable robustness guarantees in neural networks that can scale up to large models.</li>\n</ol>\n<p>The next video will be about basic properties of tensors, and how it can be used for smooth interpolation (in particular in the context of our law of robustness conjecture). After that, we will see, maybe more neural networks, maybe more bandits, maybe some non-convex optimization &#8230;.</p>\n<p>Stay safe out there!</p></p>\n",
  "wfw:commentRss": "https://blogs.princeton.edu/imabandit/2020/10/13/2020/feed/",
  "slash:comments": 19
}