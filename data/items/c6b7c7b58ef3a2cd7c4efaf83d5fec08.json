{
  "title": "Recognizing Human Activities with Kinect - Choosing a temporal model",
  "description": "<p><em>Update: I have posted the sequel to this post <a href=\"/machinelearning/classifying-human-activities-kinect-2/\">here</a></em></p>\n\n<p>In this blog post, I will very briefly talk about some popular models used for <strong>temporal/sequence classification</strong>, \ntheir advantages/disadvantages, which one I used for my human activity recognition project, and why. \nThis post is intended for people who would like to delve into sequence classification, but don’t know where to start. \nI plan to follow up on this post with another post that explains in detail our implementation of recognizing human \nactivities from RGBD data.  However, if you want to have a look at it now, \n<a href=\"/assets/activity-classification.pdf\">here</a> are the slides.</p>\n\n<p>In one my graduate-level course <strong>Machine Learning for Computer Vision</strong>, we were asked to select\na research paper to review and present. We selected the paper \n<a href=\"http://www.cs.cornell.edu/~jysung/paper/unstructured_human_activity_learning.pdf\">Unstructured Human Activity Detection from RGBD Images</a>.\nOur reasons for this selection were several: it was fairly recent (2012), had a large number of citations (according to google scholar, at least), and it dealt with sequential data (RGBD videos). Temporal models, or sequence classification, was\nsomething that was not covered in our course, and so we were eager to explore this area of Machine Learning. \nWe read the paper, made a <a href=\"/assets/activity-poster.pdf\">poster</a> out of it, and presented it to our peers, TAs and the professor.</p>\n\n<p>The next part of the course was more interesting, and it involved us picking up a Machine Learning problem, and\nwe then had the option of either implementing an existing approach to the problem, or we could come with our\nown approach to solve it. We could have implemented the paper that we reviewed, but it seemed to more interesting\nto have a look at the models available for sequence classification, and then use one \nsuch model for our problem.</p>\n\n<p>So we started looking around, and found that that following three models (and their variations)\nseem to be the most popular:</p>\n\n<ol>\n  <li>Hidden Markov Models (HMMs)</li>\n  <li>Maximum Entropy Markov Models (MEMMs)</li>\n  <li>Conditional Random Fields (CRFs)</li>\n</ol>\n\n<p>Here’s the very basic intuition about temporal models: Suppose you are reading some text character by character. The first \ncharacter that you observe is an “i”. Now, what do you think are the chances of you observing another “i”. Pretty slim, right?\nThis is because consecutive “i” are pretty rare while reading english text. Modeling such probabilistic relationships\nin a mathematical form is precisely why we use temporal models, instead of just using some regular classifier (such as\nlogistic regression). There’s two more popular models for sequential classification (or structured prediction, as some people\nlike to call it), and they are: 1) <strong>Structural SVM</strong>, 2) <strong>Recurrent Neural Nets</strong>. I won’t talk about for either of them,\nas I have not used them, but you are welcome to check them out.</p>\n\n<p>Hidden Markov Models are the oldest, and have been used in things like speech-to-text since the 1960s. MEMMs came\naround in 2000, only to be followed (and overshadowed) by Conditional Random Fields an year later. Both MEMMs and CRF\ncame from the <a href=\"http://people.cs.umass.edu/~mccallum/\">Andrew McCallum’s research group</a>, and were focused on <a href=\"http://en.wikipedia.org/wiki/Natural_language_processing\">Natural Language Processing</a> tasks.\nHowever, once you have extracted features from sequential data, you can use these models as long as your features\nsatisfy the assumptions made by these models. Note that all of these models are special cases of \n<a href=\"http://en.wikipedia.org/wiki/Graphical_model\">probabilistic graphical models</a>, so all the inference and learning algorithms from \nthere can directly be applied here.</p>\n\n<h3 id=\"hidden-markov-models\">Hidden Markov Models</h3>\n\n<figure>\n\t<img img=\"\" height=\"155\" width=\"410\" src=\"/images/kinect_activity/hmm.png\" />\n\t<figcaption>Graphical Model Representation of a stack of HMMs</figcaption>\n</figure>\n\n<p>As I mentioned earlier, Hidden Markov Models have been around for a long time, and were heavily used by the speech processing community.\nI won’t much into the details/code of HMMs, as there are a large number of resources that describe the topic, targeted both at \n<a href=\"http://www.comp.leeds.ac.uk/roger/HiddenMarkovModels/html_dev/main.html\">beginners</a> and those who want to go into all the \n<a href=\"http://www.ece.ucsb.edu/Faculty/Rabiner/ece259/Reprints/tutorial%20on%20hmm%20and%20applications.pdf\">details</a>. \nHMMs are <a href=\"http://en.wikipedia.org/wiki/Generative_model\"><strong>generative models</strong></a>, and efficient dynamic programming algorithms \nare available for both training and inference. The models uses <strong>hidden states</strong>, and assumes that the <strong>observed states</strong> are independent of each other, given their hidden states. A common way to go about doing classification with HMMS is the following: Train an HMM\nfor every class, and then for every new example, find the probability of that example being generated by each HMM, the HMM that gives the\nmaximum probability is your final class.</p>\n\n<p>However, with HMMs come a number of disadvantages, with the major ones being:</p>\n\n<ol>\n  <li>Requires enumeration of all possible observation sequences.</li>\n  <li>Requires the observations to be independent of each other (given the hidden state).</li>\n  <li>Generative approach for solving a conditional problem leading to unnecessary computations.</li>\n</ol>\n\n<h3 id=\"maximum-entropy-markov-models\">Maximum Entropy Markov Models</h3>\n\n<p>So, let’s move onto a new model, which, in theory, solves all of the above problems: MEMMs.\nMEMMs were introduced in 2000, and were at that time used in NLP tasks, and showed\nimprovements in tasks where assumption [2] mentioned above was not true. MEMMs are discriminative models, so\nthey also do away with problems [1] and [3]. There’s also a hierarchical version of the same model, \nand a Hierarchical MEMM is what was used in the <a href=\"http://www.cs.cornell.edu/~jysung/paper/unstructured_human_activity_learning.pdf\">paper</a> \nthat we reviewed. The paper contains an interesting way of selecting graph structure, and I recommend checking it out.</p>\n\n<figure>\n\t<img src=\"/images/kinect_activity/memm.png\" />\n\t<figcaption>Graphical Representation of an MEMM. Note how the direction of arrow from observation to hidden state has been reversed.\n\t</figcaption>\n</figure>\n\n<p>But along with MEMMs comes it’s own problem, commonly called as the label-bias problem.</p>\n\n<h4 id=\"label-bias-problem\">Label bias problem</h4>\n\n<ol>\n  <li>States with low-entropy transition distributions ”effectively ignore” their observations. States with lower transitions have ”unfair advantage”.</li>\n  <li>Since training is always done with respect to known previous tags, so the model struggles at test time when there is uncertainty in the previous tag.</li>\n</ol>\n\n<p>It is impossible to understand the above without some background on what MEMMs are, so it is advisable\nto first look at <a href=\"http://courses.ischool.berkeley.edu/i290-dm/s11/SECURE/gidofalvi.pdf\">how MEMMs work</a>\n, and then at the original <a href=\"http://www.cs.columbia.edu/~jebara/6772/papers/crf.pdf\">CRF paper</a>\nwhich talks about the label bias problem.</p>\n\n<h3 id=\"conditional-random-fields---star-of-the-show\">Conditional Random Fields -&gt; Star of the show</h3>\n\n<figure>\n\t<img img=\"\" height=\"155\" width=\"410\" src=\"/images/kinect_activity/crf.png\" />\n\t<figcaption>Graphical Representation of a CRF. Note that this an undirected graphical model, as opposed to HMM/MEMM</figcaption>\n</figure>\n\n<p>To overcome the label-bias problem of MEMMs, CRFs were introduced an year later, and demonstrated superior or\nequivalent performance in almost every NLP task that the authors tested it on. CRFs (and its variants) are considered as \nstate-of-the-art in a  number of machine learning problems, specially in Computer Vision. They are used not only \nfor temporal modeling, but can also model more complicated relationships in high-dimensional data, and some applications include\nimage segmentation and depth estimation from monocular images. Understanding CRFs is a little more challenging than\nHMMs or MEMMs, so I will list a few resources for you to get started with. For beginners, the best resource is this \n<a href=\"http://videolectures.net/cikm08_elkan_llmacrf/\">short course</a> by <a href=\"http://cseweb.ucsd.edu/~elkan/\">Charles Elkan</a>.\nIt also has accompanying course notes,\nand if you go to this guy’s academic website, you can also find some programming assignments to implement CRFs. \n<a href=\"https://onionesquereality.wordpress.com/2011/08/20/conditional-random-fields-a-beginners-survey/\">Here</a> is a \nmore comprehensive list of resources related to CRFs, and it’s pretty thorough.</p>\n\n<p>Now, in 2006, there was an extension to CRF by the MIT CSAIL lab, called hidden CRFs. Here is the original paper<a href=\"http://people.csail.mit.edu/sybor/cvpr06_wang.pdf\">original paper</a>. What this does, in essence, is to introduce another layer of hidden states, and is designed to\nassign a single label to every sequence. This is different from MEMMs and CRFs, which assigned a label to every observation in\na sequence, and different from HMMs too (wherein a stack of HMMs was trained for classification).</p>\n\n<figure>\n\t<img img=\"\" height=\"155\" width=\"410\" src=\"/images/kinect_activity/hcrf.png\" />\n\t<figcaption>Graphical Representation of an hCRF. Note the extra hidden layer.</figcaption>\n</figure>\n\n<p>The original hCRF paper applied it to gesture recognition from RGB videos, and demonstrated superior\nperformance to CRF in classifying gestures, so we zeroed down on this model, to be used for our\nHuman Activity Classification task (note that activities are not exactly the same as gestures).</p>\n\n<p>The real icing on the cake was this-&gt; MIT CSAIL had released a well documented <a href=\"http://sourceforge.net/projects/hcrf/\">toolbox</a>,\nmaking it ridiculously easy for us to use this model on whichever dataset that we wanted, and \nthe only major programming part that was left to us now was was the feature extraction stage.</p>\n\n<p>In a future blog post, I will describe in detail the implementation of our project: the dataset, the features we used,\nand what results we got.</p>",
  "pubDate": "Wed, 27 May 2015 00:00:00 +0000",
  "link": "https://avisingh599.github.io/machinelearning/classifying-human-activities-kinect/",
  "guid": "https://avisingh599.github.io/machinelearning/classifying-human-activities-kinect/"
}