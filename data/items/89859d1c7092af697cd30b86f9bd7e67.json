{
  "title": "Discussion of \"Fast Approximate Inference for Arbitrarily Large Semiparametric Regression Models via Message Passing\"",
  "link": "http://dustintran.com/blog/discussion-of-fast-approximate-inference",
  "guid": "http://dustintran.com/blog/discussion-of-fast-approximate-inference",
  "description": "<p><em>This article is written with much help by David Blei. It is extracted from a discussion paper on “Fast Approximate Inference for Arbitrarily Large Semiparametric Regression Models via Message Passing”.</em> <a href=\"http://arxiv.org/abs/1609.05615\">[link]</a></p>\n\n<p>We commend <a href=\"#wand2016fast\">Wand (2016)</a> for an excellent description of\nmessage passing (<span style=\"font-variant:small-caps;\">mp</span>) and for developing it to infer large semiparametric\nregression models.  We agree with the author in fully embracing the\nmodular nature of message passing, where one can define “fragments”\nthat enable us to compose localized algorithms.  We believe this\nperspective can aid in the development of new algorithms for automated\ninference.</p>\n\n<p><strong>Automated inference.</strong>  The promise of automated algorithms is\nthat modeling and inference can be separated. A user can construct\nlarge, complicated models in accordance with the assumptions he or she\nis willing to make about their data. Then the user can use generic\ninference algorithms as a computational backend in a “probabilistic\nprogramming language,” i.e., a language for specifying generative\nprobability models.</p>\n\n<p>With probabilistic programming, the user no longer has to write their\nown algorithms, which may require tedious model-specific derivations\nand implementations. In the same spirit, the user no longer has to\nbottleneck their modeling choices in order to fit the requirements of\nan existing model-specific algorithm.  Automated inference enables\nprobabilistic programming systems, such as\nStan <a href=\"#carpenter2016stan\">(Carpenter et al., 2016)</a>, through methods like\nautomatic differentiation variational inference (<span style=\"font-variant:small-caps;\">advi</span>) <a href=\"#kucukelbir2016automatic\">(Kucukelbir, Tran, Ranganath, Gelman, &amp; Blei, 2016)</a> and\nno U-turn sampler (<span style=\"font-variant:small-caps;\">nuts</span>) <a href=\"#hoffman2014nuts\">(Hoffman &amp; Gelman, 2014)</a>.</p>\n\n<p>Though they aim to apply to a large class of models, automated\ninference algorithms typically need to incorporate modeling structure\nin order to remain practical. For example, Stan assumes that one can\nat least take gradients of a model’s joint density. (Contrast this\nwith other languages which assume one can only sample from the model.)\nHowever, more structure is often necessary: <span style=\"font-variant:small-caps;\">advi</span> and <span style=\"font-variant:small-caps;\">nuts</span>\nare not fast enough by themselves to infer very large models, such as\nhierarchical models with many groups.</p>\n\n<p>We believe <span style=\"font-variant:small-caps;\">mp</span> and Wand’s work could offer fruitful avenues for\nexpanding the frontiers of automated inference. From our perspective,\na core principle underlying <span style=\"font-variant:small-caps;\">mp</span> is to leverage structure when it\nis available—in particular, statistical properties in the model—which provides useful computational properties.  In <span style=\"font-variant:small-caps;\">mp</span>, two\nexamples are conditional independence and conditional conjugacy.</p>\n\n<p><strong>From conditional independence to distributed computation.</strong>\nAs <a href=\"#wand2016fast\">Wand (2016)</a> indicates, a crucial advantage of message\npassing is that it modularizes inference; the computation can be\nperformed separately over conditionally independent posterior\nfactors. By definition, conditional independence separates a posterior\nfactor from the rest of the model, which enables <span style=\"font-variant:small-caps;\">mp</span> to define a\nseries of iterative updates. These updates can be run asynchronously\nand in a distributed environment.</p>\n\n<center>\n<img src=\"/blog/assets/2016-09-19-figure.png\" style=\"width:200px;\" />\n</center>\n<p><em>Figure 1.\nA hierarchical model, with latent variables <script type=\"math/tex\">\\alpha_k</script> defined locally\nper group and latent variables <script type=\"math/tex\">\\phi</script> defined globally to be shared across groups.</em></p>\n\n<p>We are motivated by hierarchical models, which substantially benefit\nfrom this property. Formally, let <script type=\"math/tex\">y_{nk}</script> be the <script type=\"math/tex\">n^{th}</script> data\npoint in group <script type=\"math/tex\">k</script>, with a total of <script type=\"math/tex\">N_k</script> data points in group <script type=\"math/tex\">k</script> and\n<script type=\"math/tex\">K</script> many groups. We model the data using local latent variables\n<script type=\"math/tex\">\\alpha_k</script> associated to a group <script type=\"math/tex\">k</script>, and using global latent\nvariables <script type=\"math/tex\">\\phi</script> which are shared across groups. The model is depicted\nin Figure 1.</p>\n\n<p>The posterior distribution of local variables <script type=\"math/tex\">\\alpha_k</script> and global\nvariables <script type=\"math/tex\">\\phi</script> is</p>\n\n<script type=\"math/tex; mode=display\">p(\\alpha,\\phi\\mid\\mathbf{y}) \\propto\np(\\phi\\mid\\mathbf{y}) \\prod_{k=1}^K\n\\Big[ p(\\alpha_k\\mid \\beta) \\prod_{n=1}^{N_K} p(y_{nk}\\mid\\alpha_k,\\phi) \\Big].</script>\n\n<p>The benefit of distributed updates over the independent factors is\nimmediate. For example, suppose the data consists of 1,000 data points\nper group (with 5,000 groups); we model it with 2 latent variables per\ngroup and 20 global latent variables.  Passing messages, or\ninferential updates, in parallel provides an attractive approach to\nhandling all 10,020 latent dimensions. (In contrast, consider a\nsequential algorithm that requires taking 10,019 steps for all other\nvariables before repeating an update of the first.)</p>\n\n<p>While this approach to leveraging conditional independence is\nstraightforward from the message passing perspective, it is not\nnecessarily immediate from other perspectives.  For example, the\nstatistics literature has only recently come to similar ideas,\nmotivated by scaling up Markov chain Monte Carlo using divide and\nconquer strategies <a href=\"#huang2005sampling\">(Huang &amp; Gelman, 2005; Wang &amp; Dunson, 2013)</a>.\nThese first analyze data locally over a partition of the joint\ndensity, and second aggregate the local inferences.  In our work in\n<a href=\"#gelman2014expectation\">Gelman et al. (2014)</a>, we arrive at the continuation of this\nidea. Like message passing, the process is iterated, so that local\ninformation propagates to global information and global information\npropagates to local information. In doing so, we obtain a scalable\napproach to Monte Carlo inference, both from a top-down view which\ndeals with fitting statistical models to large data sets and from a\nbottom-up view which deals with combining information across local\nsources of data and models.</p>\n\n<p><strong>From conditional conjugacy to exact iterative updates.</strong>\nAnother important element of message passing algorithms is conditional\nconjugacy, which lets us easily calculate the exact distribution for a\nposterior factor conditional on other latent variables. This enables\nanalytically tractable messages (c.f., Equations (7)-(8) of\n<a href=\"#wand2016fast\">Wand (2016)</a>).</p>\n\n<p>Consider the same hierarchical model discussed above, and set</p>\n\n<script type=\"math/tex; mode=display\">p(y_k,\\alpha_k\\mid \\phi)\n= h(y_k, \\alpha_k) \\exp\\{\\phi^\\top t(y_k, \\alpha_k) - a(\\phi)\\},</script>\n\n<script type=\"math/tex; mode=display\">p(\\phi)\n= h(\\phi) \\exp\\{\\eta^{(0) \\top} t(\\phi) - a(\\eta_0)\\}\n.</script>\n\n<p>The local factor <script type=\"math/tex\">p(y_k,\\alpha_k\\mid\\phi)</script> has sufficient statistics\n<script type=\"math/tex\">t(y_k,\\alpha_k)</script> and natural parameters given by the global latent\nvariable <script type=\"math/tex\">\\phi</script>.  The global factor <script type=\"math/tex\">p(\\phi)</script> has sufficient\nstatistics <script type=\"math/tex\">t(\\phi) = (\\phi, -a(\\phi))</script>, and with fixed\nhyperparameters <script type=\"math/tex\">\\eta^{(0)}</script>, which has two components: <script type=\"math/tex\">\\eta^{(0)} =\n(\\eta^{(0)}_1,\\eta^{(0)}_2)</script>.</p>\n\n<p>This exponential family structure implies that, conditionally, the\nposterior factors are also in the same exponential families\nas the prior factors <a href=\"#diaconis1979conjugate\">(Diaconis &amp; Ylvisaker, 1979)</a>,</p>\n\n<script type=\"math/tex; mode=display\">p(\\phi\\mid\\mathbf{y},\\alpha)\n= h(\\phi) \\exp\\{\\eta(\\mathbf{y},\\alpha)^\\top t(\\phi) - a(\\mathbf{y},\\alpha)\\},</script>\n\n<script type=\"math/tex; mode=display\">p(\\alpha_k\\mid y_k, \\phi)\n= h(\\alpha_k) \\exp\\{\\eta(y_k, \\phi)^\\top t(\\alpha_k) - a(y_k, \\phi)\\}\n.</script>\n\n<p>The global factor’s natural parameter is <script type=\"math/tex\">\\eta(\\mathbf{y},\\alpha) =\n(\\eta^{(0)}_1 + \\sum_{k=1}^K t(y_k, \\alpha_k), \\eta^{(0)}_2 + \\sum_{k=1}^K N_k)</script>.</p>\n\n<p>With this statistical property at play—namely that conjugacy gives\nrise to tractable conditional posterior factors—we can derive\nalgorithms at a conditional level with exact iterative updates.  This\nis assumed for most of the message passing of semiparametric models in\n<a href=\"#wand2016fast\">Wand (2016)</a>.  Importantly, this is not necessarily a\nlimitation of the algorithm. It is a testament to leveraging model\nstructure: without access to tractable conditional posteriors,\nadditional approximations must be made.  <a href=\"#wand2016fast\">Wand (2016)</a> provides\nan elegant way to separate out these nonconjugate pieces from the\nconjugate pieces.</p>\n\n<p>In statistics, the most well-known example which leverages\nconditionally conjugate factors is the Gibbs sampling algorithm.  From\nour own work, we apply the idea in order to access fast natural\ngradients in variational inference, which accounts for the information\ngeometry of the parameter space <a href=\"#hoffman2013stochastic\">(Hoffman, Blei, Wang, &amp; Paisley, 2013)</a>.  In\nother work, we demonstrate a collection of methods for gradient-based\nmarginal optimization <a href=\"#tran2016gradient\">(Tran, Gelman, &amp; Vehtari, 2016)</a>.  Assuming forms of\nconjugacy in the model class arrives at the classic idea of\niteratively reweighted least squares as well as the EM algorithm. Such\nstructure in the model provides efficient algorithms—both\nstatistically and computationally—for their automated inference.</p>\n\n<p><strong>Open Challenges and Future Directions.</strong> Message passing is a\nclassic algorithm in the computer science literature, which is ripe\nwith interesting ideas for statistical inference. In particular,\n<span style=\"font-variant:small-caps;\">mp</span> enables new advancements in the realm of automated inference,\nwhere one can take advantage of statistical structure in the model.\n<a href=\"#wand2016fast\">Wand (2016)</a> makes great steps following this direction.</p>\n\n<p>With that said, important open challenges still exist in order to\nrealize this fusion.</p>\n\n<p>First is about the design and implementation of probabilistic\nprogramming languages. In order to implement <a href=\"#wand2016fast\">Wand (2016)</a>’s\nmessage passing, the language must provide ways of identifying local\nstructure in a probabilistic program.  While that is enough to let\npractitioners use <span style=\"font-variant:small-caps;\">mp</span>, a much larger challenge is to\nthen automate the process of detecting local structure.</p>\n\n<p>Second is about the design and implementation of inference engines.\nThe inference must be extensible, so that users can not only employ\nthe algorithm in <a href=\"#wand2016fast\">Wand (2016)</a> but easily build on top of it.\nFurther, its infrastructure must be able to encompass a variety of\nalgorithms, so that users can incorporate <span style=\"font-variant:small-caps;\">mp</span> as one of many\ntools in their toolbox.</p>\n\n<p>Third, we think there are innovations to be made on taking the stance\nof modularity to a further extreme. In principle, one can compose not\nonly localized message passing updates but compose localized inference\nalgorithms of any choice—whether it be exact inference, Monte Carlo,\nor variational methods.  This modularity will enable new\nexperimentation with inference hybrids and can bridge the gap among\ninference methods.</p>\n\n<p>Finally, while we discuss <span style=\"font-variant:small-caps;\">mp</span> in the context of automation,\nfully automatic algorithms are not possible. Associated to all\ninference are statistical and computational\ntradeoffs <a href=\"#jordan2013statistics\">(Jordan, 2013)</a>.  Thus we need algorithms along\nthe frontier, where a user can explicitly define a computational\nbudget and employ an algorithm achieving the best statistical\nproperties within that budget; or conversely, define desired\nstatistical properties and employ the fastest algorithm to achieve\nthem.  We think ideas in <span style=\"font-variant:small-caps;\">mp</span> will also help in developing some of\nthese algorithms.</p>\n\n<h2 id=\"references\">References</h2>\n\n<ol class=\"bibliography\"><li><span id=\"carpenter2016stan\">Carpenter, B., Gelman, A., Hoffman, M. D., Lee, D., Goodrich, B., Betancourt, M., … Riddell, A. (2016). Stan: A probabilistic programming language. <i>Journal of Statistical Software</i>.</span></li>\n<li><span id=\"diaconis1979conjugate\">Diaconis, P., &amp; Ylvisaker, D. (1979). Conjugate Priors for Exponential Families. <i>The Annals of Statistics</i>, <i>7</i>(2), 269–281.</span></li>\n<li><span id=\"gelman2014expectation\">Gelman, A., Vehtari, A., Jylänki, P., Robert, C., Chopin, N., &amp; Cunningham, J. P. (2014). Expectation propagation as a way of life. <i>ArXiv Preprint ArXiv:1412.4869</i>.</span></li>\n<li><span id=\"hoffman2013stochastic\">Hoffman, M. D., Blei, D. M., Wang, C., &amp; Paisley, J. (2013). Stochastic Variational Inference. <i>Journal of Machine Learning Research</i>, <i>14</i>, 1303–1347.</span></li>\n<li><span id=\"hoffman2014nuts\">Hoffman, M. D., &amp; Gelman, A. (2014). The no-U-turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo. <i>Journal of Machine Learning Research</i>, <i>15</i>, 1593–1623.</span></li>\n<li><span id=\"huang2005sampling\">Huang, Z., &amp; Gelman, A. (2005). Sampling for Bayesian Computation with Large Datasets. <i>SSRN Electronic Journal</i>.</span></li>\n<li><span id=\"jordan2013statistics\">Jordan, M. I. (2013). On statistics, computation and scalability. <i>Bernoulli</i>, <i>19</i>(4), 1378–1390.</span></li>\n<li><span id=\"kucukelbir2016automatic\">Kucukelbir, A., Tran, D., Ranganath, R., Gelman, A., &amp; Blei, D. M. (2016). Automatic Differentiation Variational Inference. <i>ArXiv Preprint ArXiv:1603.00788</i>.</span></li>\n<li><span id=\"tran2016gradient\">Tran, D., Gelman, A., &amp; Vehtari, A. (2016). Gradient-based marginal optimization. <i>Technical Report</i>.</span></li>\n<li><span id=\"wand2016fast\">Wand, M. P. (2016). Fast Approximate Inference for Arbitrarily Large Semiparametric Regression Models via Message Passing. <i>ArXiv Preprint ArXiv:1602.07412</i>.</span></li>\n<li><span id=\"wang2013parallelizing\">Wang, X., &amp; Dunson, D. B. (2013). Parallelizing MCMC via Weierstrass sampler. <i>ArXiv Preprint ArXiv:1312.4605</i>.</span></li></ol>",
  "pubDate": "Mon, 19 Sep 2016 00:00:00 -0700"
}