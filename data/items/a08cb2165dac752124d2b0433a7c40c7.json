{
  "title": "Model AUC depends on test set difficulty",
  "link": "",
  "published": "2017-03-18T22:36:00-07:00",
  "updated": "2017-03-18T22:36:00-07:00",
  "author": {
    "name": "Jonathan Landy"
  },
  "id": "tag:efavdb.com,2017-03-18:/model-auc-depends-on-test-set-difficulty",
  "summary": "<p>The <span class=\"caps\">AUC</span> score is a popular summary statistic that is often used to communicate the performance of a classifier. However, we illustrate here that this score depends not only on the quality of the model in question, but also on the difficulty of the test set considered: If samples are …</p>",
  "content": "<p>The <span class=\"caps\">AUC</span> score is a popular summary statistic that is often used to communicate the performance of a classifier. However, we illustrate here that this score depends not only on the quality of the model in question, but also on the difficulty of the test set considered: If samples are added to a test set that are easily classified, the <span class=\"caps\">AUC</span> will go up &#8212; even if the model studied has not improved. In general, this behavior implies that isolated, single <span class=\"caps\">AUC</span> scores cannot be used to meaningfully qualify a model&#8217;s performance. Instead, the <span class=\"caps\">AUC</span> should be considered a score that is primarily useful for comparing and ranking multiple models &#8212; each at a common test set&nbsp;difficulty.</p>\n<h3>Introduction</h3>\n<p>An important challenge associated with building good classification algorithms centers around their optimization: If an adjustment is made to an algorithm, we need a score that will enable us to decide whether or not the change made was an improvement. Many scores are available for this purpose. A sort-of all-purpose score that is quite popular for characterizing binary classifiers is the model <span class=\"caps\">AUC</span> score (defined&nbsp;below).</p>\n<p>The purpose of this post is to illustrate a subtlety associated with the <span class=\"caps\">AUC</span> that is not always appreciated: The score depends strongly on the difficulty of the test set used to measure model performance. In particular, if any soft-balls are added to a test set that are easily classified (i.e., are far from any decision boundary), the <span class=\"caps\">AUC</span> will increase. This increase does not imply a model improvement. Two key take-aways&nbsp;follow:</p>\n<ul>\n<li>The <span class=\"caps\">AUC</span> is an inappropriate score for comparing models validated on test sets having differing sampling distributions. Therefore, comparing the AUCs of models trained on samples having differing distributions requires care: The training sets can have different distributions, but the test sets must&nbsp;not.</li>\n<li>A single <span class=\"caps\">AUC</span> measure cannot typically be used to meaningfully communicate the quality of a single model (though single model <span class=\"caps\">AUC</span> scores are often&nbsp;reported!)</li>\n</ul>\n<p>The primary utility of the <span class=\"caps\">AUC</span> is that it allows one to compare multiple models at fixed test set difficulty: If a model change results in an increase in the <span class=\"caps\">AUC</span> at fixed test set distribution, it can often be considered an&nbsp;improvement.</p>\n<p>We review the definition of the <span class=\"caps\">AUC</span> below and then demonstrate the issues alluded to&nbsp;above.</p>\n<h3>The <span class=\"caps\">AUC</span> score,&nbsp;reviewed</h3>\n<p>Here, we quickly review the definition of the <span class=\"caps\">AUC</span>. This is a score that can be used to quantify the accuracy of a binary classification algorithm on a given test set <span class=\"math\">\\(\\mathcal{S}\\)</span>. The test set consists of a set of feature vector-label pairs of the&nbsp;form\n</p>\n<div class=\"math\">\\begin{eqnarray}\\tag{1}\n\\mathcal{S} = \\{(\\textbf{x}_i, y_i) \\}.\n\\end{eqnarray}</div>\n<p>Here, <span class=\"math\">\\(\\textbf{x}_i\\)</span> is the set of features, or predictor variables, for example <span class=\"math\">\\(i\\)</span> and <span class=\"math\">\\(y_i \\in \\{0,1 \\}\\)</span> is the label for example <span class=\"math\">\\(i\\)</span>. A classifier function <span class=\"math\">\\(\\hat{p}_1(\\textbf{x})\\)</span> is one that attempts to guess the value of <span class=\"math\">\\(y_i\\)</span> given only the feature vector <span class=\"math\">\\(\\textbf{x}_i\\)</span>. In particular, the output of the function <span class=\"math\">\\(\\hat{p}_1(\\textbf{x}_i)\\)</span> is an estimate for the probability that the label <span class=\"math\">\\(y_i\\)</span> is equal to <span class=\"math\">\\(1\\)</span>. If the algorithm is confident that the class is <span class=\"math\">\\(1\\)</span> (<span class=\"math\">\\(0\\)</span>), the probability returned will be large&nbsp;(small).</p>\n<p>To characterize model performance, we can set a threshold value of <span class=\"math\">\\(p^*\\)</span> and mark all examples in the test set with <span class=\"math\">\\(\\hat{p}(\\textbf{x}_i) &gt; p^*\\)</span> as being candidates for class one. The fraction of the truly positive examples in <span class=\"math\">\\(\\mathcal{S}\\)</span> marked in this way is referred to as the true-positive rate (<span class=\"caps\">TPR</span>) at threshold <span class=\"math\">\\(p^*\\)</span>. Similarly, the fraction of negative examples in <span class=\"math\">\\(\\mathcal{S}\\)</span> marked is referred to as the false-positive rate (<span class=\"caps\">FPR</span>) at threshold <span class=\"math\">\\(p^*\\)</span>. Plotting the <span class=\"caps\">TPR</span> against the <span class=\"caps\">FPR</span> across all thresholds gives the model&#8217;s so-called receiver operating characteristic (<span class=\"caps\">ROC</span>) curve. A hypothetical example is shown at right in blue. The dashed line is just the <span class=\"math\">\\(y=x\\)</span> line, which corresponds to the <span class=\"caps\">ROC</span> curve of a random classifier (one returning a uniform random <span class=\"math\">\\(p\\)</span> value each&nbsp;time).</p>\n<p><a href=\"https://efavdb.com/wp-content/uploads/2017/03/example.png\"><img alt=\"example\" src=\"https://efavdb.com/wp-content/uploads/2017/03/example.png\"></a></p>\n<p>Notice that if the threshold is set to <span class=\"math\">\\(p^* = 1\\)</span>, no positive or negative examples will typically be marked as candidates, as this would require one-hundred percent confidence of class <span class=\"math\">\\(1\\)</span>. This means that we can expect an <span class=\"caps\">ROC</span> curve to always go through the point <span class=\"math\">\\((0,0)\\)</span>. Similarly, with <span class=\"math\">\\(p^*\\)</span> set to <span class=\"math\">\\(0\\)</span>, all examples should be marked as candidates for class <span class=\"math\">\\(1\\)</span> &#8212; and so an <span class=\"caps\">ROC</span> curve should also always go through the point <span class=\"math\">\\((1,1)\\)</span>. In between, we hope to see a curve that increases in the <span class=\"caps\">TPR</span> direction more quickly than in the <span class=\"caps\">FPR</span> direction &#8212; since this would imply that the examples the model is most confident about tend to actually be class <span class=\"math\">\\(1\\)</span> examples. In general, the larger the Area Under the (<span class=\"caps\">ROC</span>) Curve &#8212; again, blue at right &#8212; the better. We call this area the &#8220;<span class=\"caps\">AUC</span> score for the model&#8221; &#8212; the topic of this&nbsp;post.</p>\n<h3><span class=\"caps\">AUC</span> sensitivity to test set&nbsp;difficulty</h3>\n<p>To illustrate the sensitivity of the <span class=\"caps\">AUC</span> score to test set difficulty, we now consider a toy classification problem: In particular, we consider a set of unit-variance normal distributions, each having a different mean <span class=\"math\">\\(\\mu_i\\)</span>. From each distribution, we will take a single sample <span class=\"math\">\\(x_i\\)</span>. From this, we will attempt to estimate whether or not the corresponding mean satisfies <span class=\"math\">\\(\\mu_i &gt; 0\\)</span>. That is, our training set will take the form <span class=\"math\">\\(\\mathcal{S} = \\{(x_i, \\mu_i)\\}\\)</span>, where <span class=\"math\">\\(x_i \\sim N(\\mu_i, 1)\\)</span>. For different <span class=\"math\">\\(\\mathcal{S}\\)</span>, we will study the <span class=\"caps\">AUC</span> of the classifier&nbsp;function,</p>\n<div class=\"math\">\\begin{eqnarray} \\label{classifier} \\tag{2}\n\\hat{p}(x) = \\frac{1}{2} (1 + \\text{tanh}(x))\n\\end{eqnarray}</div>\n<p>\nA plot of this function is shown below. You can see that if any test sample <span class=\"math\">\\(x_i\\)</span> is far to the right (left) of <span class=\"math\">\\(x=0\\)</span>, the model will classify the sample as positive (negative) with high certainty. At intermediate values near the boundary, the estimated probability of being in the positive class lifts in a reasonable&nbsp;way.</p>\n<p><a href=\"https://efavdb.com/wp-content/uploads/2017/03/classifier-2.png\"><img alt=\"classifier\" src=\"https://efavdb.com/wp-content/uploads/2017/03/classifier-2.png\"></a></p>\n<p>Notice that if a test example has a mean very close to zero, it will be difficult to classify that example as positive or negative. This is because both positive and negative <span class=\"math\">\\(x\\)</span> samples are equally likely in this case. This means that the model cannot do much better than a random guess for such <span class=\"math\">\\(\\mu\\)</span>. On the other hand, if an example <span class=\"math\">\\(\\mu\\)</span> is selected that is very far from the origin, a single sample <span class=\"math\">\\(x\\)</span> from <span class=\"math\">\\(N(\\mu, 1)\\)</span> will be sufficient to make a very good guess as to whether <span class=\"math\">\\(\\mu &gt; 0\\)</span>. Such examples are hard to get wrong,&nbsp;soft-balls.</p>\n<p>The impact of adding soft-balls to the test set on the <span class=\"caps\">AUC</span> for model (\\ref{classifier}) can be studied by changing the sampling distribution of <span class=\"math\">\\(\\mathcal{S}\\)</span>. The following python snippet takes samples <span class=\"math\">\\(\\mu_i\\)</span> from three distributions &#8212; one tight about <span class=\"math\">\\(0\\)</span> (resulting in a very difficult test set), one that is very wide containing many soft-balls that are easily classified, and one that is intermediate. The <span class=\"caps\">ROC</span> curves that result from these three cases are shown following the code. The three curves are very different, with the <span class=\"caps\">AUC</span> of the soft-ball set very large and that of the tight set close to that of the random classifier. Yet, in each case the model considered was the same &#8212; (\\ref{classifier}). How could the <span class=\"caps\">AUC</span> have&nbsp;improved?!</p>\n<div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn</span> <span class=\"kn\">import</span> <span class=\"n\">metrics</span>\n\n<span class=\"n\">fig</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">ax1</span><span class=\"p\">,</span> <span class=\"n\">ax2</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplots</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">figsize</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">12</span><span class=\"p\">,</span><span class=\"mf\">3.5</span><span class=\"p\">))</span>\n\n<span class=\"n\">SAMPLES</span> <span class=\"o\">=</span> <span class=\"mi\">1000</span>\n<span class=\"n\">means_std</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span>\n<span class=\"k\">for</span> <span class=\"n\">means_std</span> <span class=\"ow\">in</span> <span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"o\">.</span><span class=\"mi\">001</span><span class=\"p\">]:</span>\n    <span class=\"n\">means</span> <span class=\"o\">=</span> <span class=\"n\">means_std</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">SAMPLES</span><span class=\"p\">)</span>\n    <span class=\"n\">x_set</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">samples</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">means</span>\n    <span class=\"n\">predictions</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">classifier</span><span class=\"p\">(</span><span class=\"n\">item</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">item</span> <span class=\"ow\">in</span> <span class=\"n\">x_set</span><span class=\"p\">]</span>\n    <span class=\"n\">fpr</span><span class=\"p\">,</span> <span class=\"n\">tpr</span><span class=\"p\">,</span> <span class=\"n\">thresholds</span> <span class=\"o\">=</span> <span class=\"n\">metrics</span><span class=\"o\">.</span><span class=\"n\">roc_curve</span><span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">means</span><span class=\"o\">&gt;</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">predictions</span><span class=\"p\">)</span>\n    <span class=\"n\">ax1</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">fpr</span><span class=\"p\">,</span> <span class=\"n\">tpr</span><span class=\"p\">,</span> <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"n\">means_std</span><span class=\"p\">)</span>\n    <span class=\"n\">ax1</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">fpr</span><span class=\"p\">,</span> <span class=\"n\">fpr</span><span class=\"p\">,</span> <span class=\"s1\">&#39;k--&#39;</span><span class=\"p\">)</span>\n    <span class=\"n\">ax2</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">means</span><span class=\"p\">,</span> <span class=\"mi\">0</span> <span class=\"o\">*</span> <span class=\"n\">means</span><span class=\"p\">,</span> <span class=\"s1\">&#39;*&#39;</span><span class=\"p\">,</span> <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"n\">means_std</span><span class=\"p\">)</span>\n<span class=\"n\">ax1</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">(</span><span class=\"n\">loc</span><span class=\"o\">=</span><span class=\"s1\">&#39;lower right&#39;</span><span class=\"p\">,</span> <span class=\"n\">shadow</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"n\">ax2</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">(</span><span class=\"n\">loc</span><span class=\"o\">=</span><span class=\"s1\">&#39;lower right&#39;</span><span class=\"p\">,</span> <span class=\"n\">shadow</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"n\">ax1</span><span class=\"o\">.</span><span class=\"n\">set_title</span><span class=\"p\">(</span><span class=\"s1\">&#39;TPR versus FPR -- The ROC curve&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax2</span><span class=\"o\">.</span><span class=\"n\">set_title</span><span class=\"p\">(</span><span class=\"s1\">&#39;Means sampled for each case&#39;</span><span class=\"p\">)</span>\n</pre></div>\n\n\n<p><a href=\"https://efavdb.com/wp-content/uploads/2017/03/Examples.png\"><img alt=\"Examples\" src=\"https://efavdb.com/wp-content/uploads/2017/03/Examples.png\"></a></p>\n<p>The explanation for the differing <span class=\"caps\">AUC</span> values above is clear: Consider, for example, the effect of adding soft-ball negatives to <span class=\"math\">\\(\\mathcal{S}\\)</span>. In this case, the model (\\ref{classifier}) will be able to correctly identify almost all true positive examples at a much higher threshold than that where it begins to mis-classify the introduced negative softballs. This means that the <span class=\"caps\">ROC</span> curve will now hit a <span class=\"caps\">TPR</span> value of <span class=\"math\">\\(1\\)</span> well-before the <span class=\"caps\">FPR</span> does (which requires all negatives &#8212; including the soft-balls to be mis-classified). Similarly, if many soft-ball positives are added in, these will be easily identified as such well-before any negative examples are mis-classified. This again results in a raising of the <span class=\"caps\">ROC</span> curve, and an increase in <span class=\"caps\">AUC</span> &#8212; all without any improvement in the actual model quality, which we have held&nbsp;fixed.</p>\n<h3>Discussion</h3>\n<p>The toy example considered above illustrates the general point the <span class=\"caps\">AUC</span> of a model is really a function of both the model and the test set it is being applied to. Keeping this in mind will help to prevent incorrect interpretations of the <span class=\"caps\">AUC</span>. A special case to watch out for in practice is the situation where the <span class=\"caps\">AUC</span> changes upon adjustment of the training and testing protocol applied (which can result, for example, from changes to how training examples are collected for the model). If you see such a change occur in your work, be careful to consider whether or not it is possible that the difficulty of the test set has changed in the process. If so, the change in the <span class=\"caps\">AUC</span> may not indicate a change in model&nbsp;quality.</p>\n<p>Because the <span class=\"caps\">AUC</span> score of a model can depend highly on the difficulty of the test set, reporting this score alone will generally not provide much insight into the accuracy of the model &#8212; which really depends only on performance near the true decision boundary and not on soft-ball performance. Because of this, it may be a good practice to always report <span class=\"caps\">AUC</span> scores for optimized models next to those of some fixed baseline model. Comparing the differences of the two <span class=\"caps\">AUC</span> scores provides an approximate method for removing the effect of test set difficulty. If you come across an isolated, high <span class=\"caps\">AUC</span> score in the wild, remember that this does not imply a good&nbsp;model!</p>\n<p>A special situation exists where reporting an isolated <span class=\"caps\">AUC</span> score for a single model can provide value: The case where the test set employed shares the same distribution as that of the application set (the space where the model will be employed). In this case, performance within the test set directly relates to expected performance during application. However, applying the <span class=\"caps\">AUC</span> to situations such as this is not always useful. For example, if the positive class sits within only a small subset of feature space, samples taken from much of the rest of the space will be &#8220;soft-balls&#8221; &#8212; examples easily classified as not being in the positive class. Measuring the <span class=\"caps\">AUC</span> on test sets over the full feature space in this context will always result in <span class=\"caps\">AUC</span> values near one &#8212; leaving it difficult to register improvements in the model near the decision boundary through measurement of the <span class=\"caps\">AUC</span>.</p>\n<script type=\"text/javascript\">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}\n</script>",
  "category": ""
}