{
  "title": "Second Annual Data Science Bowl &#8211; Part 3 &#8211; Automatically Finding the Heart Location in an MRI Image",
  "link": "https://colinpriest.com/2016/03/08/second-annual-data-science-bowl-part-3-automatically-finding-the-heart-location-in-an-mri-image/",
  "comments": "https://colinpriest.com/2016/03/08/second-annual-data-science-bowl-part-3-automatically-finding-the-heart-location-in-an-mri-image/#comments",
  "dc:creator": "Colin Priest",
  "pubDate": "Tue, 08 Mar 2016 06:24:24 +0000",
  "category": [
    "Automation",
    "Convolutional Neural Networks",
    "Deep Learning",
    "Image Processing",
    "Kaggle",
    "Machine Learning",
    "Medical Imaging",
    "Python"
  ],
  "guid": "http://colinpriest.com/?p=799",
  "description": "My last blog wasn't so sexy, what with all the data cleansing, and no predictive modelling. But in this blog I do something really cool - I train a machine learning model to find the left ventricle of the heart in an MRI image. And I couldn't have done it without all of that boring data cleansing. #kaggle @kaggle<p><a href=\"https://colinpriest.com/2016/03/08/second-annual-data-science-bowl-part-3-automatically-finding-the-heart-location-in-an-mri-image/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a></p>",
  "content:encoded": "<p>MyÂ <a href=\"https://colinpriest.com/2016/03/07/second-annual-data-science-bowl-part-2/\" target=\"_blank\">last blog</a> wasn&#8217;t so sexy, what with all the data cleansing, and no predictive modelling. But in this blog I do something really cool &#8211; I train a machine learning model to find the left ventricle of the heart in an MRI image. And I couldn&#8217;t have done it without all of that boring data cleansing. #kaggle @kaggle</p>\n<p>Aside from being a really cool thing to do, there is a purpose to this modelling.Â I want to find the boundaries of the heart chamber, and that is much easier and faster to do when I remove distractions. Once I have found the location of the heart chamber, I can crop the image to a much smaller square.</p>\n<p>The input to the model will be a set of images. In order to simply what the model learns, I only gave it training images fromÂ sax locations near the centreÂ of the heart.</p>\n<p>\n<a href='https://colinpriest.com/2016/03/08/second-annual-data-science-bowl-part-3-automatically-finding-the-heart-location-in-an-mri-image/20160308-image01/'><img width=\"113\" height=\"150\" src=\"https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image01.png?w=113\" class=\"attachment-thumbnail size-thumbnail\" alt=\"\" loading=\"lazy\" srcset=\"https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image01.png?w=113 113w, https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image01.png 192w\" sizes=\"(max-width: 113px) 100vw, 113px\" data-attachment-id=\"823\" data-permalink=\"https://colinpriest.com/2016/03/08/second-annual-data-science-bowl-part-3-automatically-finding-the-heart-location-in-an-mri-image/20160308-image01/\" data-orig-file=\"https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image01.png\" data-orig-size=\"192,256\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"20160308-image01\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image01.png?w=192\" data-large-file=\"https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image01.png?w=192\" /></a>\n<a href='https://colinpriest.com/2016/03/08/second-annual-data-science-bowl-part-3-automatically-finding-the-heart-location-in-an-mri-image/20160308-image02/'><img width=\"113\" height=\"150\" src=\"https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image02.png?w=113\" class=\"attachment-thumbnail size-thumbnail\" alt=\"\" loading=\"lazy\" srcset=\"https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image02.png?w=113 113w, https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image02.png 192w\" sizes=\"(max-width: 113px) 100vw, 113px\" data-attachment-id=\"825\" data-permalink=\"https://colinpriest.com/2016/03/08/second-annual-data-science-bowl-part-3-automatically-finding-the-heart-location-in-an-mri-image/20160308-image02/\" data-orig-file=\"https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image02.png\" data-orig-size=\"192,256\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"20160308-image02\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image02.png?w=192\" data-large-file=\"https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image02.png?w=192\" /></a>\n<a href='https://colinpriest.com/2016/03/08/second-annual-data-science-bowl-part-3-automatically-finding-the-heart-location-in-an-mri-image/20160308-image03/'><img width=\"113\" height=\"150\" src=\"https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image03.png?w=113\" class=\"attachment-thumbnail size-thumbnail\" alt=\"\" loading=\"lazy\" srcset=\"https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image03.png?w=113 113w, https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image03.png 192w\" sizes=\"(max-width: 113px) 100vw, 113px\" data-attachment-id=\"822\" data-permalink=\"https://colinpriest.com/2016/03/08/second-annual-data-science-bowl-part-3-automatically-finding-the-heart-location-in-an-mri-image/20160308-image03/\" data-orig-file=\"https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image03.png\" data-orig-size=\"192,256\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"20160308-image03\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image03.png?w=192\" data-large-file=\"https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image03.png?w=192\" /></a>\n<a href='https://colinpriest.com/2016/03/08/second-annual-data-science-bowl-part-3-automatically-finding-the-heart-location-in-an-mri-image/20160308-image04/'><img width=\"113\" height=\"150\" src=\"https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image04.png?w=113\" class=\"attachment-thumbnail size-thumbnail\" alt=\"\" loading=\"lazy\" srcset=\"https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image04.png?w=113 113w, https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image04.png 192w\" sizes=\"(max-width: 113px) 100vw, 113px\" data-attachment-id=\"824\" data-permalink=\"https://colinpriest.com/2016/03/08/second-annual-data-science-bowl-part-3-automatically-finding-the-heart-location-in-an-mri-image/20160308-image04/\" data-orig-file=\"https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image04.png\" data-orig-size=\"192,256\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"20160308-image04\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image04.png?w=192\" data-large-file=\"https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image04.png?w=192\" /></a>\n</p>\n<p>The output from the model will be the row number and column number of the centroid of the left ventricle heart chamber (the red dot in the images above).</p>\n<p>I had to manually define those centroid locations for a trainingÂ set of a few hundred of the images. This was laborious and time consuming, even after I automated some of the process. But it needed to be done, because otherwise the machine learning algorithm has no way of knowing what the true answers should be.</p>\n<p>Even though I am much more comfortable coding in R than in Python, I used Python for this step because I wanted to use <a href=\"http://danielnouri.org/\" target=\"_blank\">Daniel Nouri</a>&#8216;s <a href=\"https://pypi.python.org/pypi/nolearn\" target=\"_blank\">nolearn</a> library, which sits above <a href=\"http://lasagne.readthedocs.org/en/latest/\" target=\"_blank\">lasagne</a> and <a href=\"http://deeplearning.net/software/theano/\" target=\"_blank\">theano</a>, and these libraries are not available in R. The convolution neural network architecture was based upon the architecture in <a href=\"http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/\" target=\"_blank\">Daniel Nouri&#8217;s tutorial</a> for the <a href=\"https://www.kaggle.com/c/facial-keypoints-detection\" target=\"_blank\">Facial Keypoints Detection competition in Kaggle</a>.</p>\n<h1>Step 1: Importing of all the Required Libraries</h1>\n<p>OK, so this part isn&#8217;t all that sexy either. But it&#8217;s the engine for all the cool modelling that is about to be done.</p>\n<pre class=\"brush: python; title: ; notranslate\">\n\nimport numpy as np\nimport csv\nimport random\nimport math\nimport os\nimport cv2\nimport itertools\nimport math\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport itertools\n\nfrom lasagne import layers\nfrom lasagne.updates import nesterov_momentum\nfrom lasagne.nonlinearities import softmax\nfrom lasagne.nonlinearities import sigmoid\nfrom nolearn.lasagne import BatchIterator\nfrom nolearn.lasagne import NeuralNet\nfrom nolearn.lasagne import TrainSplit\nfrom nolearn.lasagne import PrintLayerInfo\nfrom nolearn.lasagne.visualize import plot_loss\nfrom nolearn.lasagne.visualize import plot_conv_weights\nfrom nolearn.lasagne.visualize import plot_conv_activity\nfrom nolearn.lasagne.visualize import plot_occlusion\n\n%pylab inline\nfrom lasagne.layers import DenseLayer\nfrom lasagne.layers import InputLayer\nfrom lasagne.layers import DropoutLayer\nfrom lasagne.layers import Conv2DLayer\nfrom lasagne.layers import MaxPool2DLayer\nfrom lasagne.nonlinearities import softmax\nfrom lasagne.updates import adam\nfrom lasagne.layers import get_all_params\nfrom nolearn.lasagne import NeuralNet\nfrom nolearn.lasagne import TrainSplit\nfrom nolearn.lasagne import objective\n\nimport theano\nimport theano.tensor as T\n\n</pre>\n<p>&nbsp;</p>\n<h1>Step 2: Defining the Helper Functions</h1>\n<p>I used <a href=\"http://jupyter.org/\" target=\"_blank\">jupyter notebook</a>Â as the development environment to set up and run my Python scripts. While there&#8217;s a lot to like about jupyter, one thing that annoys me is that print commands run in jupyter don&#8217;t immediately show text on the screen. But here&#8217;s a trick to work around that:</p>\n<pre class=\"brush: python; title: ; notranslate\">\n\ndef printQ(s):\n print(s)\n sys.stdout.flush()\n\n</pre>\n<p>Using this helper function instead of the print function results in text immediately appearing in the output. This is particular helpful for progress messages on long training runs.</p>\n<p>I like to use R&#8217;s <a href=\"https://stat.ethz.ch/R-manual/R-devel/library/base/html/expand.grid.html\" target=\"_blank\">expand.grid</a> function, but it isn&#8217;t built in to Python. So I wrote my own helper function in Python that mimics the functionality:</p>\n<pre class=\"brush: python; title: ; notranslate\">\n\ndef product2(*args, repeat=1):\n # product('ABCD', 'xy') --> Ax Ay Bx By Cx Cy Dx Dy\n # product(range(2), repeat=3) -> 000 001 010 011 100 101 110 111\n pools = [tuple(pool) for pool in args] * repeat\n result = [[]]\n for pool in pools:\n result = [x+[y] for x in result for y in pool]\n for prod in result:\n yield tuple(prod)\n\ndef expand_grid(dictionary):\n return pd.DataFrame([row for row in product2(*dictionary.values())],\n columns=dictionary.keys())\n\n</pre>\n<p>This next function reads a cleaned up image, checking that it has the correct aspect ratio, then resizing it to 96 x 96 pixels. The resizing is done to reduce memory usage in my GPU, and to speed up the training and scoring.</p>\n<pre class=\"brush: python; title: ; notranslate\">\n\ndef load_image(path):\n # check that the file exists\n if not os.path.isfile(path):\n   printQ('BAD PATH: ' + path)\n image = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n # ensure portrait aspect ratio\n if (image.shape[0] < image.shape[1]):\n   image = cv2.transpose(image)\n # resize to 96x96\n resized_image = cv2.resize(image, (96, 96))\n # check that the image isn't empty\n s = sum(sum(resized_image))\n if np.isnan(s):\n   print(path)\n return resized_image\n\n</pre>\n<p>My initial models were not performing as well as I would like. So to force theÂ the modelÂ to generalise, I added some image transformations (rotation and reflection) to the training data. This required helper functions:</p>\n<pre class=\"brush: python; title: ; notranslate\">\n\ndef rotate_image(img, degrees):\n rows,cols = img.shape\n M = cv2.getRotationMatrix2D((cols/2,rows/2),degrees,1)\n dst = cv2.warpAffine(img,M,(cols,rows))\n return dst\n\ndef transform_image(img, equalise, gammaAdjust, reflection, rotation):\n if equalise == 1:\n   img = cv2.equalizeHist(img)\n if gammaAdjust != 1:\n   img = pow(img / 255.0, gammaAdjust) * 255.0\n if reflection == 0 or reflection == 1:\n   img = cv2.flip(img, int(reflection))\n if rotation != 0:\n   img = rotate_image(img, rotation)\n return img\n\ndef transform_xy(x, y, equalise, gammaAdjust, reflection, rotation):\n # if equalise, then no change to x and y\n # if gamma adjustment, then no change to x and y\n # reflection\n if reflection == 0:\n   y = 1.0 - y\n if reflection == 1:\n   x = 1.0 - x\n if rotation == 180:\n   x = 1.0 - x\n   y = 1.0 - y\n if rotation != 0 and rotation != 180:\n   x1 = x - 0.5\n   y1 = y - 0.5\n   theta = rotation / 180 * pi\n   x2 = x1 * cos(theta) + y1 * sin(theta)\n   y2 = -x1 * sin(theta) + y1 * cos(theta)\n   x = x2 + 0.5\n   y = y2 + 0.5\n return numpy.array([x, y])\n\n# set up the image adjustments\n#equalise, gammaAdjust, reflection\ndAdjustShort = { 'equalise': [0],\n 'gammaAdjust': [1],\n 'reflection': [-1],\n 'rotation': [0]}\ndAdjust = { 'equalise': [0, 1],\n 'gammaAdjust': [1, 0.75, 1.5],\n 'reflection': [-1, 0, 1],\n 'rotation': [0, 180, 3, -3]}\n# 'rotation': [0, 180, 3, -3, 10, -10]}\nimgAdj = expand_grid(dAdjust)\n#imgAdj = expand_grid(dAdjustShort)\nprint (imgAdj.shape)\n# can't have both reflection AND rotation by 180 degrees\nimgAdj = imgAdj.query('not (reflection > -1 and rotation == 180)')\n# can't have equalise AND gamma adjust\nimgAdj = imgAdj.query('not (equalise == 1 and gammaAdjust != 1)')\n</pre>\n<p>There are two feature sets that enable the machine learning model to find the heart:</p>\n<ol>\n<li>shape in the image &#8211; by looking at shape in the image it can find the heart</li>\n<li>movement between images at different points of time &#8211; the heart is moving but most of the chest is stationary</li>\n</ol>\n<p>So I set up the network architecture to use two channels. The first channel is the image, and the second channel is the difference between the image at this point of time and the image 8 time periods in the future.</p>\n<pre class=\"brush: python; title: ; notranslate\">\n\ndef load_train_set():\n #numTimes = 8 # how many time periods to use\n plusTime = 5 # gaps between time periods for comparison\n xs = []\n ys = []\n ids = []\n all_y = '/home/colin/data/Second-Annual-Data-Science-Bowl/working/centroids-20160218-R.csv'\n with open(all_y) as f:\n   rows = csv.reader(f, delimiter=',', quotechar='\"')\n   iRow = 1\n   for line in rows:\n     # first line is column headers, so ignore it\n     if iRow > 1:\n       # parse the line\n       patient = line[0]\n       x = float(line[1])\n       y = float(line[2])\n       sax = int(line[4])\n       firstTime = int(line[5])\n       if int(patient) % 25 == 0 and firstTime == 1:\n         printQ(patient)\n       # enhance the training data with rotations, reflections, NOT histogram equalisation\n       for index, row in imgAdj.iterrows():\n         # append the target values\n         xy = transform_xy(x, y, row['equalise'], row['gammaAdjust'], row['reflection'], row['rotation'])\n         ys.append(xy.astype('float32').reshape((1, 2)))\n         #\n         # read the images\n         folder = '/home/colin/data/Second-Annual-Data-Science-Bowl/train-cleaned/' + patient + '/study/sax_0' + str(sax) + '/'\n         xm = np.zeros([1, 2, 96, 96])\n         #\n         #\n         # current frame\n         path = folder + 'image-' + ('%0*d' % (2, firstTime)) + '.png'\n         img = load_image(path)\n         # transform the image - rotation, reflection etc\n         img = transform_image(img, row['equalise'], row['gammaAdjust'], row['reflection'], row['rotation'])\n         # get the pixels into the range [-1, 1]\n         img = (img / 128.0 - 1.0).astype('float32').reshape((96, 96))\n         xm[0, 0, :, :] = img\n         #\n         #\n         # find movement of current frame to future frame\n         path = folder + 'image-' + ('%0*d' % (2, firstTime + plusTime)) + '.png'\n         img = load_image(path)\n         # transform the image - rotation, reflection etc\n         img = transform_image(img, row['equalise'], row['gammaAdjust'], row['reflection'], row['rotation'])\n         # get the pixels into the range [-1, 1]\n         img = (img / 128.0 - 1.0).astype('float32').reshape((96, 96))\n         # first time is the complete image at time 1\n         # subsequent frames are the differences between frames\n         xm[0, 1, :, :] = img - xm[0, 0, :, :]\n         xs.append(xm.astype('float32').reshape((1, 2, 96, 96)))\n         ids.append(patient)\n\n     iRow = iRow + 1\n return np.vstack(xs), np.vstack(ys), np.vstack(ids)\n\n</pre>\n<p>I used early stopping to help reduce overfitting:</p>\n<pre class=\"brush: python; title: ; notranslate\">\n\nclass EarlyStopping(object):\n def __init__(self, patience=100):\n   self.patience = patience\n   self.best_valid = np.inf\n   self.best_valid_epoch = 0\n   self.best_weights = None\n\ndef __call__(self, nn, train_history):\n current_valid = train_history[-1]['valid_loss']\n current_epoch = train_history[-1]['epoch']\n if current_valid < self.best_valid:\n   self.best_valid = current_valid\n   self.best_valid_epoch = current_epoch\n   self.best_weights = nn.get_all_params_values()\n elif self.best_valid_epoch + self.patience < current_epoch:\n   print('Early stopping')\n   print('Best valid loss was {:.6f} at epoch {}.'.format(\n   self.best_valid, self.best_valid_epoch))\n   nn.load_params_from(self.best_weights)\n   raise StopIteration()\n\n</pre>\n<h1>Step 3: Read the Training Data</h1>\n<p>As well as reading the training data, I shuffled the order of the data. This allowed me to use batch training.</p>\n<pre class=\"brush: python; title: ; notranslate\">\n\n# read the training data\n\nprintQ('reading the training data')\ntrain_x, train_y, train_id = load_train_set()\n\nprintQ ('shuffling training rows')\nrandom.seed(1234)\nrows = random.choice(arange(0, train_x.shape[0]), train_x.shape[0])\nt_x = train_x[rows,:,:,:]\nt_y = train_y[rows,:]\nt_id = train_id[rows]\n\nprintQ('finished')\n\n</pre>\n<h1>Step 4: Train the Model</h1>\n<p>The network architecture used deep convolutional layers to find features in the image, then fully connected layers to convert these features into the centroid location:</p>\n<p><img loading=\"lazy\" data-attachment-id=\"925\" data-permalink=\"https://colinpriest.com/2016/03/08/second-annual-data-science-bowl-part-3-automatically-finding-the-heart-location-in-an-mri-image/20160308-image05/\" data-orig-file=\"https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image05.png\" data-orig-size=\"220,346\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"20160308-image05\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image05.png?w=191\" data-large-file=\"https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image05.png?w=220\" class=\"alignnone size-full wp-image-925\" src=\"https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image05.png\" alt=\"20160308-image05\" width=\"220\" height=\"346\" srcset=\"https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image05.png 220w, https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image05.png?w=95&h=150 95w\" sizes=\"(max-width: 220px) 100vw, 220px\" /></p>\n<pre class=\"brush: python; title: ; notranslate\">\n\n# fit the models\n\n# set up the model\nprintQ ('setting up the model structure')\nlayers0 = [\n # layer dealing with the input data\n (InputLayer, {'shape': (None, 2, 96, 96)}),\n\n# first stage of our convolutional layers\n (Conv2DLayer, {'num_filters': 32, 'filter_size': 5}),\n #(DropoutLayer, {'p': 0.2}),\n (Conv2DLayer, {'num_filters': 32, 'filter_size': 3}),\n #(DropoutLayer, {'p': 0.2}),\n (Conv2DLayer, {'num_filters': 32, 'filter_size': 3}),\n #(DropoutLayer, {'p': 0.2}),\n (Conv2DLayer, {'num_filters': 32, 'filter_size': 3}),\n #(DropoutLayer, {'p': 0.2}),\n (Conv2DLayer, {'num_filters': 32, 'filter_size': 3}),\n (MaxPool2DLayer, {'pool_size': 2}),\n (DropoutLayer, {'p': 0.2}),\n\n# second stage of our convolutional layers\n (Conv2DLayer, {'num_filters': 64, 'filter_size': 3}),\n #(DropoutLayer, {'p': 0.3}),\n (Conv2DLayer, {'num_filters': 64, 'filter_size': 3}),\n #(DropoutLayer, {'p': 0.3}),\n (Conv2DLayer, {'num_filters': 64, 'filter_size': 3}),\n (MaxPool2DLayer, {'pool_size': 2}),\n (DropoutLayer, {'p': 0.3}),\n\n# two dense layers with dropout\n (DenseLayer, {'num_units': 128}),\n (DropoutLayer, {'p': 0.5}),\n (DenseLayer, {'num_units': 128}),\n\n# the output layer\n (DenseLayer, {'num_units': 2, 'nonlinearity': sigmoid}),\n]\n\nprintQ ('creating and training the networks architectures')\nnumNets = 1\nNNs = list()\nfor iNet in arange(numNets):\n nn = NeuralNet(\n layers = layers0,\n max_epochs = 2000,\n update=adam,\n update_learning_rate=0.0002,\n regression=True, # flag to indicate we're dealing with regression problem\n batch_iterator_train=BatchIterator(batch_size=100),\n on_epoch_finished=[EarlyStopping(patience=10),],\n train_split=TrainSplit(eval_size=0.25),\n verbose=1,\n )\n result = nn.fit(t_x, t_y)\n NNs.append(nn)\n\nprintQ('finished')\n\n</pre>\n<p>Based upon how quickly the training converged, the network could possiblyÂ have been simplified, reducing the number of layers, or using fewer neurons in the fully connected layers. But I didn&#8217;t have time to experiment with different architectures.</p>\n<p><img loading=\"lazy\" data-attachment-id=\"931\" data-permalink=\"https://colinpriest.com/2016/03/08/second-annual-data-science-bowl-part-3-automatically-finding-the-heart-location-in-an-mri-image/20160308-image06/\" data-orig-file=\"https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image06.png\" data-orig-size=\"469,276\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"20160308-image06\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image06.png?w=300\" data-large-file=\"https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image06.png?w=469\" class=\"alignnone size-full wp-image-931\" src=\"https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image06.png\" alt=\"20160308-image06\" width=\"469\" height=\"276\" srcset=\"https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image06.png 469w, https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image06.png?w=150&h=88 150w, https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image06.png?w=300&h=177 300w\" sizes=\"(max-width: 469px) 100vw, 469px\" /></p>\n<p>The GPU quickly ran out of RAM unless I used the batch iterator. I found the batch size via trial and error. Large batch sizes caused the GPU to run out of RAM. Small batch sizes ran much slower.</p>\n<h1>Step 5: Review the Training Errors</h1>\n<p>Just like humans, all models make mistakes. The heart chamber segmentation algorithms I used later in this project were sensitive to how well the heart chamber was centred in the image. But as long as the model output wasÂ a centroid that was inside the heart chamber, things usually went OK. Early versions of my model made mistakes that placed the centroid outside the heart chamber, sometimes even far away from the heart.Tweaks to the training data (especially enhancing the data with rotation and reflection) and the architecture (especially dropout layers) improved the performance.</p>\n<pre class=\"brush: python; title: ; notranslate\">\n\ndef getHeartLocation(trainX):\n  # get the heart locations from each network\n  heartLocs = zeros(numNets * trainX.shape[0] * 2).reshape((numNets, trainX.shape[0], 2))\n  for j in arange(numNets):\n    nn = NNs[j]\n    heartLocs[j, :, :] = nn.predict(trainX)\n\n    # use median as an ensembler\n    heartLocsMedian = zeros(trainX.shape[0] * 2).reshape((trainX.shape[0], 2))\n    heartLocsMedian[:,0] = median(heartLocs[:,:,0], axis = 0)\n    heartLocsMedian[:,1] = median(heartLocs[:,:,1], axis = 0)\n\n    # use a 'max distance from centre' ensembler\n    heartLocsDist = zeros(trainX.shape[0] * 2).reshape((trainX.shape[0], 2))\n    distance = abs(heartLocs - 0.5)\n    am0 = distance[:,:,0].argmax(0)\n    am1 = distance[:,:,1].argmax(0)\n    heartLocsDist[:,0] = heartLocs[am0, arange(trainX.shape[0]), 0]\n    heartLocsDist[:,1] = heartLocs[am1, arange(trainX.shape[0]), 1]\n\n    # combine the two using an arithmetic average\n    heartLocations = 0.5 * heartLocsMedian + 0.5 * heartLocsDist\n\n    return heartLocations\n\nheartLocations = getHeartLocation(train_x)\n\n# review the training errors to check for model improvements\ndef plot_sample(x, y, predicted, axis):\n  img = x[0, :, :].reshape(96, 96)\n  axis.imshow(img, cmap='gray')\n  axis.scatter(y[0::2] * 96, y[1::2] * 96, marker='x', s=10)\n  axis.scatter(predicted[0::2] * 96, predicted[1::2] * 96, marker='x', s=10, color='red')\n\nnTrain = train_x.shape[0]\nerrors = np.zeros(nTrain)\n\nfor i in arange(0, nTrain):\n  errors[i] = sqrt( square(heartLocations[i, 0] - train_y[i, 0]) + square(heartLocations[i, 1] - train_y[i, 1]) )\n\nprint('Prob(error > 0.05)' + str(mean(errors > 0.05)))\nprint('Mean: ' + str(mean(errors)))\nprint('Percentiles: ' + str(percentile(errors, [50, 75, 90, 95, 99, 100])))\n\nfor i in arange(0, nTrain):\n  error = sqrt( square(heartLocations[i, 0] - train_y[i, 0]) + square(heartLocations[i, 1] - train_y[i, 1]) )\n  if (error > 0.04):\n    if train_id[i] != train_id[i-1]:\n      #print(i)\n      print(train_id[i]) # only errors on the original images - not the altered images\n      fig = pyplot.figure(figsize=(6, 3))\n      ax = fig.add_subplot(1, 2, 1, xticks=[], yticks=[])\n      plot_sample(train_x[i,:,:,:], train_y[i, :], heartLocations[i, :], ax)\n      pyplot.show()\n\nprint('error review completed')\n\n</pre>\n<p><img loading=\"lazy\" data-attachment-id=\"951\" data-permalink=\"https://colinpriest.com/2016/03/08/second-annual-data-science-bowl-part-3-automatically-finding-the-heart-location-in-an-mri-image/20160308-image07/\" data-orig-file=\"https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image07.png\" data-orig-size=\"701,432\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"20160308-image07\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image07.png?w=300\" data-large-file=\"https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image07.png?w=529\" class=\"alignnone size-full wp-image-951\" src=\"https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image07.png\" alt=\"20160308-image07\" width=\"701\" height=\"432\" srcset=\"https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image07.png 701w, https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image07.png?w=150&h=92 150w, https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image07.png?w=300&h=185 300w\" sizes=\"(max-width: 701px) 100vw, 701px\" /></p>\n<p>After many failed models, I was excited when the two worst training errors were still close to the centre of the heart chamber <img src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png\" alt=\"ðŸ™‚\" class=\"wp-smiley\" style=\"height: 1em; max-height: 1em;\" /></p>\n<h1>Step 6: Find the Left Ventricle Locations for the Submission Data</h1>\n<p>The main point of building a heart finder machine learning model is to automate the process of finding the left ventricle in the test images that will be used as part of the competition submission. These are images that the model has never seen before.</p>\n<pre class=\"brush: python; title: ; notranslate\">\n\ndef load_submission_set():\n  numTimes = 2 # how many time periods to use\n  plusTime = 5\n  xs = []\n  ys = []\n  ids = []\n  paths = []\n  times = []\n  saxes = []\n  all_y = '/home/colin/data/Second-Annual-Data-Science-Bowl/working/centroids-submission-R.csv'\n  with open(all_y) as f:\n    rows = csv.reader(f, delimiter=',', quotechar='\"')\n    iRow = 1\n    for line in rows:\n      # first line is column headers\n      if iRow > 1:\n        # parse the line\n        patient = line[0]\n        x = 0\n        y = 0\n        sax = int(line[1])\n        firstTime = int(line[2])\n        # save the targets\n        xy = np.asarray([x, y])\n        ys.append(xy.astype('float32').reshape((1, 2)))\n        # read the images\n        folder = '/home/colin/data/Second-Annual-Data-Science-Bowl/validate-cleaned/' + patient + '/study/sax_0' + str(sax) + '/'\n        xm = np.zeros([1, 2, 96, 96])\n        #\n        #\n        # current frame\n        path0 = folder + 'image-' + ('%0*d' % (2, firstTime)) + '.png'\n        img = load_image(path0)\n        # transform the image - rotation, reflection etc\n        #img = transform_image(img, row['equalise'], row['gammaAdjust'], row['reflection'], row['rotation'])\n        # get the pixels into the range [-1, 1]\n        img = (img / 128.0 - 1.0).astype('float32').reshape((96, 96))\n        xm[0, 0, :, :] = img\n        #\n        #\n        # find movement of current frame to future frame\n        path5 = folder + 'image-' + ('%0*d' % (2, firstTime + plusTime)) + '.png'\n        img = load_image(path5)\n        # transform the image - rotation, reflection etc\n        #img = transform_image(img, row['equalise'], row['gammaAdjust'], row['reflection'], row['rotation'])\n        # get the pixels into the range [-1, 1]\n        img = (img / 128.0 - 1.0).astype('float32').reshape((96, 96))\n        # first time is the complete image at time 1\n        # subsequent frames are the differences between frames\n        xm[0, 1, :, :] = img - xm[0, 0, :, :]\n        xs.append(xm.astype('float32').reshape((1, numTimes, 96, 96)))\n        ids.append(patient)\n        paths.append(path0)\n        times.append(firstTime)\n        saxes.append(sax)\n      iRow = iRow + 1\n  return np.vstack(xs), np.vstack(ids), np.vstack(paths), np.vstack(times), np.vstack(saxes)\n\nprintQ('reading the submission data')\ntest_x, test_ids, test_paths, test_times, test_sax = load_submission_set()\n\nprintQ('quot;getting the predictions')\npredicted_y = getHeartLocation(test_x)\n\nprintQ('creating the output table')\nfullIDs = []\nfullPaths = []\nfullX = []\nfullY = []\nfullSax = []\nfullTime = []\nnTest = test_x.shape[0]\niTime = 1\nfor i in arange(0, nTest):\n  patient = (test_ids[i])[0]\n  sax = int((test_sax[i])[0])\n  path = (test_paths[i])[0]\n  iTime = int((test_times[i])[0])\n  fullIDs.append(patient)\n  fullPaths.append(path)\n  fullX.append(predicted_y[i, 0])\n  fullY.append(predicted_y[i, 1])\n  fullSax.append(sax)\n  fullTime.append(iTime)\n  outPath = '/home/colin/data/Second-Annual-Data-Science-Bowl/predicted-heart-location-submission/'\n  outPath = outPath + patient\n  outPath = outPath + '-' + str(sax)\n  outPath = outPath + '-' + str(iTime) + '.png'\n  img = load_image256x192(path)\n  x = int(round(predicted_y[i, 0] * 192))\n  y = int(round(predicted_y[i, 1] * 256))\n  img[y, x] = 255\n  img[y-1, x-1] = 255\n  img[y-1, x+1] = 255\n  img[y+1, x-1] = 255\n  img[y+1, x+1] = 255\n  write_image(img, outPath)\n\nfullIDs = array(fullIDs)\nfullPaths = array(fullPaths)\nfullX = array(fullX)\nfullY = array(fullY)\nfullSax = array(fullSax)\nfullTime = array(fullTime)\n\nprintQ('saving results table')\nd = { 'patient': fullIDs, 'path' : fullPaths, 'x' : fullX, 'y' : fullY, 'iTime' : fullTime, 'sax' : fullSax}\nimport pandas as pd\nd = pd.DataFrame(d)\nd.to_csv('/home/colin/data/Second-Annual-Data-Science-Bowl/working/heartfinderV4b-centroids-submission.csv', index = False)\n\n</pre>\n<p>In the animated gif below, you can see the left ventricle centroid location that has been automatically fitted, displayed as a dark rectangle moving around near the centre of the heart chamber. The machine learning algorithm was not trained on this patient&#8217;s images &#8211; so what you see here is artificial intelligence in action!</p>\n<p><img loading=\"lazy\" data-attachment-id=\"965\" data-permalink=\"https://colinpriest.com/2016/03/08/second-annual-data-science-bowl-part-3-automatically-finding-the-heart-location-in-an-mri-image/20160308-submission-images/\" data-orig-file=\"https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-submission-images.gif\" data-orig-size=\"384,512\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"20160308-submission-images\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-submission-images.gif?w=225\" data-large-file=\"https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-submission-images.gif?w=384\" class=\"alignnone size-full wp-image-965\" src=\"https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-submission-images.gif\" alt=\"20160308-submission-images\" width=\"384\" height=\"512\" /></p>\n",
  "wfw:commentRss": "https://colinpriest.com/2016/03/08/second-annual-data-science-bowl-part-3-automatically-finding-the-heart-location-in-an-mri-image/feed/",
  "slash:comments": 10,
  "media:thumbnail": "",
  "media:content": [
    {
      "media:title": "20160308-submission-images"
    },
    {
      "media:title": "colinpriest1966"
    },
    "",
    "",
    "",
    "",
    {
      "media:title": "20160308-image05"
    },
    {
      "media:title": "20160308-image06"
    },
    {
      "media:title": "20160308-image07"
    }
  ]
}