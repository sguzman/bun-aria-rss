{
  "title": "Dask Release 0.17.2",
  "link": "",
  "updated": "2018-03-21T00:00:00+00:00",
  "id": "https://mrocklin.github.io/blog/work/2018/03/21/dask-0.17.2",
  "content": "<p><em>This work is supported by <a href=\"http://anaconda.com\">Anaconda Inc.</a>\nand the Data Driven Discovery Initiative from the <a href=\"https://www.moore.org/\">Moore\nFoundation</a>.</em></p>\n\n<p>I’m pleased to announce the release of Dask version 0.17.2.  This is a minor\nrelease with new features and stability improvements.\nThis blogpost outlines notable changes since the 0.17.0 release on February\n12th.</p>\n\n<p>You can conda install Dask:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>conda install dask\n</code></pre></div></div>\n\n<p>or pip install from PyPI:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>pip install dask[complete] --upgrade\n</code></pre></div></div>\n\n<p>Full changelogs are available here:</p>\n\n<ul>\n  <li><a href=\"https://github.com/dask/dask/blob/master/docs/source/changelog.rst\">dask/dask</a></li>\n  <li><a href=\"https://github.com/dask/distributed/blob/master/docs/source/changelog.rst\">dask/distributed</a></li>\n</ul>\n\n<p>Some notable changes follow:</p>\n\n<h3 id=\"tornado-50\">Tornado 5.0</h3>\n\n<p>Tornado is a popular framework for concurrent network programming that Dask\nrelies on heavily.  Tornado recently released a major version update that\nincluded both some major features for Dask as well as a couple of bugs.</p>\n\n<p>The new <code class=\"language-plaintext highlighter-rouge\">IOStream.read_into</code> method allows Dask communications (or anyone using\nthis API) to move large datasets more efficiently over the network with\nfewer copies.  This enables Dask to take advantage of high performance\nnetworking available on modern super-computers.  On the Cheyenne system, where\nwe tested this, we were able to get the full 3GB/s bandwidth available through\nthe Infiniband network with this change (when using a few worker processes).</p>\n\n<p>Many thanks to <a href=\"https://github.com/pitrou\">Antoine Pitrou</a> and <a href=\"https://github.com/bdarnell\">Ben\nDarnell</a> for their efforts on this.</p>\n\n<p>At the same time there were some unforeseen issues in the update to Tornado 5.0.\nMore pervasive use of bytearrays over bytes caused issues with compression\nlibraries like Snappy and Python 2 that were not expecting these types.  There\nis a brief window in <code class=\"language-plaintext highlighter-rouge\">distributed.__version__ == 1.21.3</code> that enables this\nfunctionality if Tornado 5.0 is present but will misbehave if Snappy is also\npresent.</p>\n\n<h3 id=\"http-file-system\">HTTP File System</h3>\n\n<p>Dask leverages a <a href=\"https://github.com/dask/dask/issues/2880\">file-system-like protocol</a>\nfor access to remote data.\nThis is what makes commands like the following work:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">import</span> <span class=\"nn\">dask.dataframe</span> <span class=\"k\">as</span> <span class=\"n\">dd</span>\n\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">dd</span><span class=\"p\">.</span><span class=\"n\">read_parquet</span><span class=\"p\">(</span><span class=\"s\">'s3://...'</span><span class=\"p\">)</span>\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">dd</span><span class=\"p\">.</span><span class=\"n\">read_parquet</span><span class=\"p\">(</span><span class=\"s\">'hdfs://...'</span><span class=\"p\">)</span>\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">dd</span><span class=\"p\">.</span><span class=\"n\">read_parquet</span><span class=\"p\">(</span><span class=\"s\">'gcs://...'</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>We have now added http and https file systems for reading data directly from\nweb servers.  These also support random access if the web server supports range\nqueries.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">dd</span><span class=\"p\">.</span><span class=\"n\">read_parquet</span><span class=\"p\">(</span><span class=\"s\">'https://...'</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>As with S3, HDFS, GCS, … you can also use these tools outside of Dask\ndevelopment.  Here we read the first twenty bytes of the Pandas license:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">dask.bytes.http</span> <span class=\"kn\">import</span> <span class=\"n\">HTTPFileSystem</span>\n<span class=\"n\">http</span> <span class=\"o\">=</span> <span class=\"n\">HTTPFileSystem</span><span class=\"p\">()</span>\n<span class=\"k\">with</span> <span class=\"n\">http</span><span class=\"p\">.</span><span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s\">'https://raw.githubusercontent.com/pandas-dev/pandas/master/LICENSE'</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">.</span><span class=\"n\">read</span><span class=\"p\">(</span><span class=\"mi\">20</span><span class=\"p\">))</span>\n</code></pre></div></div>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>b'BSD 3-Clause License'\n</code></pre></div></div>\n\n<p>Thanks to <a href=\"https://github.com/martindurant\">Martin Durant</a> who did this work\nand manages Dask’s byte handling generally.  See <a href=\"http://dask.pydata.org/en/latest/remote-data-services.html\">remote data documentation</a> for more information.</p>\n\n<h3 id=\"fixed-a-correctness-bug-in-dask-dataframes-shuffle\">Fixed a correctness bug in Dask dataframe’s shuffle</h3>\n\n<p>We identified and resolved a correctness bug in dask.dataframe’s shuffle that\nresulted in some rows being dropped during complex operations like joins and\ngroupby-applies with many partitions.</p>\n\n<p>See <a href=\"https://github.com/dask/dask/pull/3201\">dask/dask #3201</a> for more information.</p>\n\n<h3 id=\"cluster-super-class-and-intelligent-adaptive-deployments\">Cluster super-class and intelligent adaptive deployments</h3>\n\n<p>There are many Python subprojects that help you deploy Dask on different\ncluster resource managers like Yarn, SGE, Kubernetes, PBS, and more.  These\nhave all converged to have more-or-less the same API that we have now combined\ninto a consistent interface that downstream projects can inherit from in\n<code class=\"language-plaintext highlighter-rouge\">distributed.deploy.Cluster</code>.</p>\n\n<p>Now that we have a consistent interface we have started to invest more in\nimproving the interface and intelligence of these systems as a group.  This\nincludes both pleasant IPython widgets like the following:</p>\n\n<p><img src=\"https://mrocklin.github.io/blog/images/dask-kubernetes-widget.png\" width=\"70%\" /></p>\n\n<p>as well as improved logic around adaptive deployments.  Adaptive deployments\nallow clusters to scale themselves automatically based on current workload.  If\nyou have recently submitted a lot of work the scheduler will estimate its\nduration and ask for an appropriate number of workers to finish the computation\nquickly.  When the computation has finished the scheduler will release the\nworkers back to the system to free up resources.</p>\n\n<p>The logic here has improved substantially including the following:</p>\n\n<ul>\n  <li>You can specify minimum and maximum limits on your adaptivity</li>\n  <li>The scheduler estimates computation duration and asks for workers\nappropriately</li>\n  <li>There is some additional delay in giving back workers to avoid hysteresis,\nor cases where we repeatedly ask for and return workers</li>\n</ul>\n\n<h2 id=\"related-projects\">Related projects</h2>\n\n<p>Some news from related projects:</p>\n\n<ul>\n  <li>The young daskernetes project was renamed to <a href=\"http://dask-kubernetes.readthedocs.io/en/latest/\">dask-kubernetes</a>.  This displaces a previous project (that had not been released) for launching Dask on Google Cloud Platform.  That project has been renamed to <a href=\"https://github.com/dask/dask-gke\">dask-gke</a>.</li>\n  <li>A new project, <a href=\"https://github.com/dask/dask-jobqueue/\">dask-jobqueue</a> was\nstarted to handle launching Dask clusters on traditional batch queuing\nsystems like PBS, SLURM, SGE, TORQUE, etc..  This projet grew out of the <a href=\"https://pangeo-data.github.io/\">Pangeo</a> collaboration</li>\n  <li>A Dask Helm chart has been added to Helm’s stable channel</li>\n</ul>\n\n<h2 id=\"acknowledgements\">Acknowledgements</h2>\n\n<p>The following people contributed to the dask/dask repository since the 0.17.0\nrelease on February 12h:</p>\n\n<ul>\n  <li>Anderson Banihirwe</li>\n  <li>Dan Collins</li>\n  <li>Dieter Weber</li>\n  <li>Gabriele Lanaro</li>\n  <li>John Kirkham</li>\n  <li>James Bourbeau</li>\n  <li>Julien Lhermitte</li>\n  <li>Matthew Rocklin</li>\n  <li>Martin Durant</li>\n  <li>Max Epstein</li>\n  <li>nkhadka</li>\n  <li>okkez</li>\n  <li>Pangeran Bottor</li>\n  <li>Rich Postelnik</li>\n  <li>Scott M. Edenbaum</li>\n  <li>Simon Perkins</li>\n  <li>Thrasibule</li>\n  <li>Tom Augspurger</li>\n  <li>Tor E Hagemann</li>\n  <li>Uwe L. Korn</li>\n  <li>Wes Roach</li>\n</ul>\n\n<p>The following people contributed to the dask/distributed repository since the\n1.21.0 release on February 12th:</p>\n\n<ul>\n  <li>Alexander Ford</li>\n  <li>Andy Jones</li>\n  <li>Antoine Pitrou</li>\n  <li>Brett Naul</li>\n  <li>Joe Hamman</li>\n  <li>John Kirkham</li>\n  <li>Loïc Estève</li>\n  <li>Matthew Rocklin</li>\n  <li>Matti Lyra</li>\n  <li>Sven Kreiss</li>\n  <li>Thrasibule</li>\n  <li>Tom Augspurger</li>\n</ul>"
}