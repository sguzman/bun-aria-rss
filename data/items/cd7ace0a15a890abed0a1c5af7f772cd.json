{
  "title": "The Authenticity Gap in Human Evaluation. (arXiv:2205.11930v2 [cs.CL] UPDATED)",
  "link": "http://arxiv.org/abs/2205.11930",
  "description": "<p>Human ratings are the gold standard in NLG evaluation. The standard protocol\nis to collect ratings of generated text, average across annotators, and rank\nNLG systems by their average scores. However, little consideration has been\ngiven as to whether this approach faithfully captures human preferences.\nAnalyzing this standard protocol through the lens of utility theory in\neconomics, we identify the implicit assumptions it makes about annotators.\nThese assumptions are often violated in practice, in which case annotator\nratings cease to reflect their preferences. The most egregious violations come\nfrom using Likert scales, which provably reverse the direction of the true\npreference in certain cases. We suggest improvements to the standard protocol\nto make it more theoretically sound, but even in its improved form, it cannot\nbe used to evaluate open-ended tasks like story generation. For the latter, we\npropose a new human evaluation protocol called $\\textit{system-level\nprobabilistic assessment}$ (SPA). When human evaluation of stories is done with\nSPA, we can recover the ordering of GPT-3 models by size, with statistically\nsignificant results. However, when human evaluation is done with the standard\nprotocol, less than half of the expected preferences can be recovered (e.g.,\nthere is no significant difference between $\\texttt{curie}$ and\n$\\texttt{davinci}$, despite using a highly powered test).\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Ethayarajh_K/0/1/0/all/0/1\">Kawin Ethayarajh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1\">Dan Jurafsky</a>"
}