{
  "title": "Solving MNIST with a Neural Network from the ground up",
  "link": "https://aimatters.wordpress.com/2021/01/18/solving-mnist-with-a-neural-network-from-the-ground-up/",
  "comments": "https://aimatters.wordpress.com/2021/01/18/solving-mnist-with-a-neural-network-from-the-ground-up/#respond",
  "dc:creator": "Stephen Oman",
  "pubDate": "Mon, 18 Jan 2021 19:37:34 +0000",
  "category": [
    "Algorithms",
    "Artificial Intelligence",
    "Example",
    "Examples",
    "Machine Intelligence",
    "Machine Learning",
    "MNIST"
  ],
  "guid": "http://aimatters.wordpress.com/?p=1335",
  "description": "Note: Here&#8217;s the Python source code for this project in a Jupyter notebook on GitHub I&#8217;ve written before about the benefits of reinventing the wheel and this is one of those occasions where it was definitely worth the effort. Sometimes, there is just no substitute for trying to implement an algorithm to really understand what&#8217;s [&#8230;]",
  "content:encoded": "\n<p><a rel=\"noreferrer noopener\" href=\"Here's the final source code for this project on GitHub\" target=\"_blank\"><i>Note: Here&#8217;s the Python source code for this project in a Jupyter</i></a><em><a rel=\"noreferrer noopener\" href=\"Here's the final source code for this project on GitHub\" target=\"_blank\"> </a></em><i><a rel=\"noreferrer noopener\" href=\"Here's the final source code for this project on GitHub\" target=\"_blank\">noteboo</a></i><a rel=\"noreferrer noopener\" href=\"Here's the final source code for this project on GitHub\" target=\"_blank\"><i>k on GitHub</i></a></p>\n\n\n\n<p>I&#8217;ve <a rel=\"noreferrer noopener\" href=\"https://aimatters.wordpress.com/2016/08/23/in-praise-of-reinventing-the-wheel/\" target=\"_blank\">written before about the benefits of reinventing the wheel</a> and this is one of those occasions where it was definitely worth the effort. Sometimes, there is just no substitute for trying to implement an algorithm to really understand what&#8217;s going on under the hood. This is especially true when learning about artificial neural networks. Sure, there are plenty of frameworks available that you can use which implement any flavour of neural network, complete with a dazzling arrays of optimisations,  activations and loss functions. That may solve your problem, but it abstracts away a lot of the details about why it solves it. </p>\n\n\n\n<p>MNIST is a great dataset to start with. It&#8217;s a collection of images containing 60,000 handwritten digits. It also contains a further 10,000 images that can be used as the test set. It&#8217;s been well studied and most frameworks have sample implementations. Here&#8217;s an example image:</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><a href=\"https://aimatters.files.wordpress.com/2021/01/mnist-4.png\"><img data-attachment-id=\"1341\" data-permalink=\"https://aimatters.wordpress.com/mnist-4/\" data-orig-file=\"https://aimatters.files.wordpress.com/2021/01/mnist-4.png\" data-orig-size=\"251,309\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"mnist-4\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://aimatters.files.wordpress.com/2021/01/mnist-4.png?w=244\" data-large-file=\"https://aimatters.files.wordpress.com/2021/01/mnist-4.png?w=251\" src=\"https://aimatters.files.wordpress.com/2021/01/mnist-4.png?w=251\" alt=\"\" class=\"wp-image-1341\" srcset=\"https://aimatters.files.wordpress.com/2021/01/mnist-4.png 251w, https://aimatters.files.wordpress.com/2021/01/mnist-4.png?w=122 122w\" sizes=\"(max-width: 251px) 100vw, 251px\" /></a></figure>\n\n\n\n<p>You can find the <a rel=\"noreferrer noopener\" href=\"http://yann.lecun.com/exdb/mnist/\" target=\"_blank\">full dataset of images</a><a href=\"http://yann.lecun.com/exdb/mnist/\" target=\"_blank\" rel=\"noreferrer noopener\"> </a><a rel=\"noreferrer noopener\" href=\"http://yann.lecun.com/exdb/mnist/\" target=\"_blank\">on Yann Le Cun&#8217;s website</a>.</p>\n\n\n\n<p>While it&#8217;s useful to reinvent the wheel, we should at least learn from those that have already built wheels before. The first thing I borrowed was the network architecture from TensorFlow. Their example has:</p>\n\n\n\n<ul><li>28&#215;28 input</li><li>a hidden layer with 512 neurons with ReLU activation</li><li>an output layer with 10 neutrons (representing the 10 possible digits) with Softmax activation</li><li>Cross-Entropy loss function</li></ul>\n\n\n\n<p>The next thing to work on was the feedforward part of the network. This is relatively straightforward as these functions are well documented online and the network itself isn&#8217;t complicated.</p>\n\n\n\n<p>The tough part was working through the back-propagation algorithm. In a previous post, I detailed how to work out the derivatives of the Softmax function and the Cross Entropy loss. The most obvious way is to use the <a rel=\"noreferrer noopener\" href=\"https://en.wikipedia.org/wiki/Chain_rule\" target=\"_blank\">Chain Rule in Differential Calculus</a> to work out the gradients and propagate them back through the network. The steps are pleasing to my eye and appeal to my sense of order in code. (Tip: Use a spreadsheet on a small example network to see the actual matrices in action.)</p>\n\n\n\n<p>But (and it&#8217;s a big but), the basic approach uses Jacobian matrices. Each cell in these kind of matrices is a partial derivative; each matrix represents a change in every variable with respect to every output. As a result, they can grow rather large very quickly. We run into several issues multiplying very large matrices together. In the notebook, I&#8217;ve left the functions representing this approach in for comparison and if you do run it, you&#8217;ll notice immediately the problems with speed and memory.</p>\n\n\n\n<p>Luckily there are shortcuts, which mean that we can directly calculate the gradients without resorting to Jacobian matrix multiplication. You can see these in the Short Form section of the notebook. In a sense though, these are abstractions too and it&#8217;s difficult to see the back-propagation from the shortcut methods.</p>\n\n\n\n<p>Lastly, I&#8217;ve implemented some code to gather the standard metrics for evaluating how good a machine learning model is. I&#8217;ve run it several times and it usually gets an overall accuracy score of between 92% and 95% on the MNIST test dataset.</p>\n\n\n\n<p>One of the main things I learned from this exercise is that the actual coding of a network is relatively simple. The really hard part that took a while was figuring out the calculus and especially the shortcuts. I really appreciate now why those frameworks are popular and make coding neural networks so much easier.</p>\n\n\n\n<p>If you fancy a challenge, I can recommend working on a neural network from first principles. You never know what you might learn!</p>\n",
  "wfw:commentRss": "https://aimatters.wordpress.com/2021/01/18/solving-mnist-with-a-neural-network-from-the-ground-up/feed/",
  "slash:comments": 0,
  "media:content": [
    {
      "media:title": "stephenoman"
    },
    ""
  ]
}