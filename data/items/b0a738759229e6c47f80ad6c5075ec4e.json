{
  "title": "Tips for Interactive HPC",
  "link": "",
  "updated": "2019-10-01T00:00:00+00:00",
  "id": "https://mrocklin.github.io/blog/work/2019/10/01/interactive-hpc",
  "content": "<meta property=\"og:title\" content=\"Tips for Interactive HPC\" />\n\n<meta property=\"og:description\" content=\"Using Conda, Jupyter, and Conda to transorm HPC centers into interactive machines\" />\n\n<meta property=\"og:image\" content=\"https://github.com/jacobtomlinson/jupyterlab-nvdashboard/raw/master/demo.gif\" />\n\n<meta property=\"og:url\" content=\"http://matthewrocklin.com/blog/work/2019/10/01/interactive-hpc\" />\n\n<p><em>This is co-released with a companion post <a href=\"stay-on-hpc\">Five reeasons to keep your HPC\ncenter, and avoid the cloud</a>.</em></p>\n\n<p>Scientific institutions today are considering how to balance their current HPC\ninfrastructure with a possible transition to commercial cloud.</p>\n\n<p>This transition is largely motivated by data science users,\nwho find a mismatch between interactive workflows and HPC policies.\nThese data science workloads differ in a few ways from traditional HPC.\nIn particular they are data centric, interactive, and ad-hoc, using tools like\nPython and Jupyter notebooks rather than highly tuned C++/MPI code that runs\novernight.</p>\n\n<p>This mismatch causes frustration both for users and for system administrators.\nThis frustration is reasonable.\nHPC hardware and policies weren’t designed for these use cases,\nand the rise of data science happened more quickly than our typical HPC\nprocurement cycle.\nFortunately, there are steps we can take to adapt existing HPC centers for data science use today.\nThese steps can either be taken at the user level (this is prevalent today)\nor at the institutional level (this is generally rare, but growing quickly).</p>\n\n<p>This post briefly outlines three of the main causes of frustration:</p>\n\n<ol>\n  <li>Rapidly changing software environments</li>\n  <li>Accessibility and rich user interfaces</li>\n  <li>Elastic and on-demand computing</li>\n</ol>\n\n<p>This post also describes technology choices that users and institutions make\ntoday to solve or at least reduce this frustration.  In particular, we focus on\ntools common in the Python ecosystem, including Conda, Jupyter{Hub/Lab}, and a\nmarriage of Dask and job schedulers.  This post can be seen either as a how-to\nfor users, or as a sales pitch to IT.</p>\n\n<h3 id=\"software-environments\">Software environments</h3>\n\n<p>Historically, when a user needed some piece of software, they would raise a\nticket with the HPC system administrators who would then build the software\nand include it as a module.  This might take a few days.</p>\n\n<p>Today, modern data science users can change their software stack several times\na day, and rely on a variety of both stable and bleeding edge software.\nAdditionally, every data science user uses a <em>slightly</em> different set\nof libraries, compounding this problem by 100x\n(assuming an HPC center supports only 100 users).</p>\n\n<p>Asking an IT system administrator to manage hundreds of bespoke software\nenvironments is infeasible.  Neither side would be happy with the arrangement.</p>\n\n<p>To solve this …</p>\n\n<ul>\n  <li>\n    <p><strong>Users</strong> today often install their software stack in user space,\ncommonly with tools like Anaconda or Miniconda, which were designed to be\nself contained and require only user-level permissions.  For many fields,\nAnaconda today includes most libraries that a data scientist needs,\nincluding both high-level Python and R libraries,\nas well as native compiled libraries like HDF5, MPI, and GDAL.\nThey might not be optimally compiled for the machine at hand,\nbut they’re often good enough.</p>\n  </li>\n  <li>\n    <p><strong>System Administrators</strong> understandably have mixed feelings about this.\nIt is both liberating and scary to give up control over the software that runs on your machine.\nIn the end though, at least now it’s the user’s responsibility\nto take charge of their own problems,\nfreeing up IT for other more institutionally focused work.</p>\n  </li>\n</ul>\n\n<h3 id=\"easy-access-and-rich-user-environments\">Easy Access and Rich User Environments</h3>\n\n<p><img src=\"https://miro.medium.com/max/2714/1*BjjkqOv7jwJIZdaBTIsbhA.png\" width=\"50%\" align=\"right\" alt=\"Amazon SageMaker splash screen\" /></p>\n\n<p>Most cloud offerings today provide slick visual user interfaces with Jupyter\nnotebooks, dashboards, point-and-click UI’s and simple authentication that\nappeal to less technical data science or science audiences.</p>\n\n<p>In contrast, users on HPC machines typically open up a terminal window, SSH in\na few times (checking their security token each time), and operate in bash.\nThis experience feels clunky and uninviting to newer users who may not have CS\nbackgrounds.</p>\n\n<p>To solve this …</p>\n\n<ul>\n  <li>\n    <p><strong>Users</strong> today often use SSH-tunneling to get a rich environment like\nJupyterLab up and running, and from there they use Jupyter notebooks,\ninstall various dashboards, open up a variety of Terminals (who needs <code class=\"language-plaintext highlighter-rouge\">tmux</code>\nanymore?).</p>\n\n    <p>Here are some images of such a session.</p>\n\n    <p><a href=\"https://data.bloomberglp.com/company/sites/40/2016/07/JupyterLab.png\">\n  <img src=\"https://data.bloomberglp.com/company/sites/40/2016/07/JupyterLab.png\" width=\"47%\" alt=\"Jupyter Lab session\" /></a></p>\n\n    <p><a href=\"https://github.com/jacobtomlinson/jupyterlab-nvdashboard/raw/master/demo.gif\">\n<img src=\"https://github.com/jacobtomlinson/jupyterlab-nvdashboard/raw/master/demo.gif\" width=\"47%\" alt=\"Jupyter Lab session\" /></a></p>\n  </li>\n  <li>\n    <p><strong>System Administrators</strong> might support this kind of activity by deploying\n<a href=\"https://jupyter.org/hub\">JupyterHub</a> within their institution.\nJupyterHub integrates nicely into existing job queuing systems like\nSLURM/PBS/LSF/SGE/… and can use the existing security policies that you\nalready have in place, including two-factor security tokens.</p>\n\n    <p>This gives users the visual rich UI/UX that they’ve come to expect,\nwhile still giving system administrators the security and control that they\nneed in order to maintain stability over the system.\nIt also, conveniently, gets users off of the login nodes and onto compute\nnodes where they belong.</p>\n\n    <p>Several large and well respected supercomputing centers have deployed\nJupyterHub internally, including NERSC, SDSC, NCAR, and many others.</p>\n  </li>\n</ul>\n\n<p>For more information, interested readers may want to watch this talk from\nAnderson Banirhiwe at SciPy 2019 on using JupyterHub from within their\nsupercomputing center.</p>\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/vhawO8fgD64\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen=\"\"></iframe>\n\n<h3 id=\"elastic-scaling-and-on-demand-computing\">Elastic scaling and On-demand computing</h3>\n\n<p>Data scientist’s workloads are both bursty, and intolerant to delays.</p>\n\n<p>Often they want to quickly spin up 50 machines to churn through 100 TB of data\nfor five minutes, generate a plot, and then stare at that plot for an hour.\nThen, when they get an idea, they want to do that again, immediately.\nThey’re comfortable waiting for a minute or two, but if there is an\nhour-long wait in the queue then they’re going to switch off to something\nelse, and they’re probably not going to try this workflow again in the future.</p>\n\n<p>These bursty workloads don’t fit into most HPC job scheduling policies.\nThese policies are designed to optimize not for on-demand computing, but for\nhigh utilization and batch jobs.  This policy causes some users a great deal of\npain because it means long wait times even for very short jobs.</p>\n\n<p>Today there are a few solutions to this, but they’re not perfect without some\nhelp from IT.</p>\n\n<ul>\n  <li>\n    <p><strong>Users</strong> end up doing one of the following:</p>\n\n    <ol>\n      <li>Download a sample of the dataset locally and abandon their HPC resources</li>\n      <li>Keep their 50 machines running the entire time, wasting valuable resources</li>\n      <li>Get frustrated, and buy some AWS credits</li>\n    </ol>\n\n    <p>That is, unless of course there are some gaps in the schedule, and if IT\n have taken some steps to promote small jobs.</p>\n\n    <p>Most HPC machines don’t hit 100% utilization.  There are always little gaps\n in the schedule which, if your jobs are fast and loosely coupled, you can\n sneak into.  A good analogy I’ve heard before is that if you fill a bucket\n full of rocks, you can still pour in a fair amount of sand.  Our goal is to\n be like sand.</p>\n\n    <p>Data science oriented distributed systems like <a href=\"https://dask.org\">Dask</a>,\n are designed to work with job-queue schedulers (PBS/SLURM/SGE/LSF/…)\n to submit many small fast jobs and assemble those jobs into a broader\n distributed computing network.  Users frequently use systems like Dask to\n take over 10-20 nodes and process 100 TB datasets today.  It’s commonly run\n on a large number of supercomputer centers.</p>\n\n    <p><em>For more on this, see <a href=\"https://jobqueue.dask.org\">Dask Jobqueue</a></em></p>\n  </li>\n  <li>\n    <p><strong>System administrators</strong> can improve the situation here by making a few\nsmall policy decisions:</p>\n\n    <ol>\n      <li>Promote very short jobs to the front of the queue.  This is a common option in many HPC job queuing systems.</li>\n      <li>\n        <p>Consider creating and advertising high priority queues that are designed\nfor bursty workloads.  Users are comfortable paying a premium for\non-demand access, especially if it allows them to be efficient with\ntheir use of jobs (remember that their alternative is to keep their\njobs running all the time, even when they’re staring at plots).</p>\n\n        <p>Many HPC systems are judged on <em>utilization</em>, but it’s important to\nremember that scientific productivity does not necessarily follow\nutilization perfectly.  Policies that optimize slightly away from\nutilization may be better at producing actual science.</p>\n      </li>\n    </ol>\n  </li>\n</ul>\n\n<h2 id=\"summary\">Summary</h2>\n\n<p>Our ability to generate large scientific datasets on\nsupercomputers has vastly outstripped our ability to analyze them.\nIt’s correct for us to drastically rethink how we support scalable data science\nworkloads on these datasets.</p>\n\n<p>However, in the meantime, we can use our existing infrastructure to great effect.\nAdvanced users do this today and generate excellent science as a result.\nWe should work to make these workflows more accessible to less advanced users,\nboth by documenting best practices\nand by codifying some of these approaches into our software.</p>\n\n<p>At the same time, we should also approach this from the systems administration side.\nSysadmins want to provide systems that guide data science users towards safe and\ncontrolled behavior, while also giving them the tools they need to do great science.</p>\n\n<p>Fortunately, new tooling is out there that was intentionally designed to bridge\nthis IT-User gap, including Conda, JupyterHub, and Dask.</p>\n\n<p><em>Thanks to <a href=\"https://andersonbanihirwe.dev/\">Anderson Banihirwe</a>,\n<a href=\"https://www.jacobtomlinson.co.uk/\">Jacob Tomlinson</a>,\nand <a href=\"http://joehamman.com/\">Joe Hamman</a>\nfor their review and help in writing this post</em></p>"
}