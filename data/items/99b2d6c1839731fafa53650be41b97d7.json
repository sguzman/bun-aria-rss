{
  "title": "Some thoughts about deep learning criticism",
  "link": "https://mirror2image.wordpress.com/2014/12/14/some-thoughts-about-deep-learning-criticism/",
  "comments": "https://mirror2image.wordpress.com/2014/12/14/some-thoughts-about-deep-learning-criticism/#comments",
  "dc:creator": "mirror2image",
  "pubDate": "Sun, 14 Dec 2014 09:31:43 +0000",
  "category": [
    "computer vision",
    "Deep Learnig",
    "Machine Learning",
    "convnet",
    "convolutional network",
    "Deep Learning",
    "machine learning"
  ],
  "guid": "http://mirror2image.wordpress.com/?p=1632",
  "description": "There is some deep learning (specifically #convnet) criticism based on the artificially constructed misclassification examples. There is a new paper &#8220;Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images&#8221; by Nguen et al http://arxiv.org/abs/1412.1897 and other direction of critics is based on the older, widely cited paper &#8220;Intriguing properties of neural networks&#8221; [&#8230;]",
  "content:encoded": "<p>There is some deep learning (specifically #convnet) criticism based on the artificially constructed misclassification examples.<br />\nThere is a new paper<br />\n&#8220;Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images&#8221; by Nguen et al<br />\n<a href=\"http://arxiv.org/abs/1412.1897\" rel=\"nofollow\">http://arxiv.org/abs/1412.1897</a><br />\nand other direction of critics is based on the older, widely cited paper<br />\n&#8220;Intriguing properties of neural networks&#8221; by Szegedy et al<br />\n<a href=\"http://arxiv.org/abs/1312.6199\" rel=\"nofollow\">http://arxiv.org/abs/1312.6199</a><br />\nIn the first paper authors construct examples which classified by convnet with confidence, but look nothing like label to human eye.<br />\nIn the second paper authors show that correctly classified image could be converted to misclassified with small perturbation, which perturbation could be found by iterative procedure<br />\nWhat I think is that those phenomenons have no impact on practical performance of convolutional neural networks.</p>\n<p>First paper is really simple to address. The real world images produced by camera are not dense in the image space(space of all pixel vectors of image size dimension).<br />\nIn fact camera images belong to low-dimensional manifold in the image space, and there are some interesting works on dimensionality and property of that manifold. For example dimensionality of the space images of the fixed 3D scene it is around 7, which is not surprising, and the geodesics of that manifold could be defined through the optical flow.<br />\nOf cause if sample is outside of image manifold it will be misclassified, method of training notwithstanding. The images in the paper are clearly not real-world camera images, no wonder convnet assign nonsensical labels to them.</p>\n<p>Second paper is more interesting. First I want to point that perturbation which cause misclassification is produced by iterative procedure. That hint that in the neighbourhood of the image perturbed misclassified images are belong to measure near-zero set.<br />\nPractically that mean that probability of this type of misclassification is near zero, and orders of magnitude less than &#8220;normal&#8221; misclassification rate of most deep networks.<br />\nBut what is causing that misclassification? I&#8217;d suggest that just high dimensionality of the image and parameters spaces and try to illustrate it. In fact it&#8217;s the same reason why epsilon-sparse vector are ubiquitous in real-world application: If we take <em>n</em>-dimensional vector, probability that all it&#8217;s components more than <img src=\"https://s0.wp.com/latex.php?latex=%5Cepsilon&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;epsilon\" class=\"latex\" /> is <img src=\"https://s0.wp.com/latex.php?latex=%281-%5Cepsilon%29%5En&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"(1-&#92;epsilon)^n\" class=\"latex\" />, which is near zero. This and like effects explored in depth in <a href=\"http://en.wikipedia.org/wiki/Compressed_sensing\" title=\"compressed sensing\">compressed sensing</a> ( also very good <a href=\"https://sites.google.com/site/igorcarron2/cs\" title=\"Igor Carron's page\">Igor Carron&#8217;s page</a>)<br />\nNow to convnet &#8211; convnet classify images by signs of piecewise-linear functions.<br />\nTake any effective pixel which is affecting activations. Convolutional network separate image space into piecewise-linear areas, which are not aligned with coordinate axes. That mean if we change intensity of pixel far enough we are out of correct classification area.<br />\n We don&#8217;t know how incorrect areas are distributed in the image space, but for common convolutional network dimensionality of subspace of the hyperplanes which make piecewise-linear separation boundary is several times more than dimensionality of the image vector. This suggest that correlation between incorrect areas of different pixels is quite weak.<br />\n Now assume that image is stable to perturbation, that mean that exist \\epsilon such that for any effective pixel it&#8217;s epsilon-neighbourhood is in the correct area. If incorrect areas are weakly correlated that mean probability of image being stable is about <img src=\"https://s0.wp.com/latex.php?latex=%281-%5Cepsilon%29%5En&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"(1-&#92;epsilon)^n\" class=\"latex\" />, where <em>n </em>is number of effective pixels. That is probability of stable image is near zero. That illustrate suggestion that this &#8220;adversarial&#8221; effect is only caused by dimensionality of the problem and parameter space, not by some intrinsic deficiency of the deep network.</p>\n",
  "wfw:commentRss": "https://mirror2image.wordpress.com/2014/12/14/some-thoughts-about-deep-learning-criticism/feed/",
  "slash:comments": 4,
  "media:content": {
    "media:title": "mirror2image"
  }
}