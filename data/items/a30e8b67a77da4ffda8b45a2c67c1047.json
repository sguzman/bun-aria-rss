{
  "title": "Relating graph auto-encoders to linear models. (arXiv:2211.01858v1 [cs.LG])",
  "link": "http://arxiv.org/abs/2211.01858",
  "description": "<p>Graph auto-encoders are widely used to construct graph representations in\nEuclidean vector spaces. However, it has already been pointed out empirically\nthat linear models on many tasks can outperform graph auto-encoders. In our\nwork, we prove that the solution space induced by graph auto-encoders is a\nsubset of the solution space of a linear map. This demonstrates that linear\nembedding models have at least the representational power of graph\nauto-encoders based on graph convolutional networks. So why are we still using\nnonlinear graph auto-encoders? One reason could be that actively restricting\nthe linear solution space might introduce an inductive bias that helps improve\nlearning and generalization. While many researchers believe that the\nnonlinearity of the encoder is the critical ingredient towards this end, we\ninstead identify the node features of the graph as a more powerful inductive\nbias. We give theoretical insights by introducing a corresponding bias in a\nlinear model and analyzing the change in the solution space. Our experiments\nshow that the linear encoder can outperform the nonlinear encoder when using\nfeature information.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Klepper_S/0/1/0/all/0/1\">Solveig Klepper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luxburg_U/0/1/0/all/0/1\">Ulrike von Luxburg</a>"
}