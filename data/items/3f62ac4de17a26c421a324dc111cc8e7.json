{
  "title": "Import AI 306: Language models learn about the world via MuJoCo; Amazon releases a big Q&#038;A dataset; and DeepMind tests out multimodal systems",
  "link": "https://jack-clark.net/2022/10/17/import-ai-306-language-models-learn-about-the-world-via-mujoco-amazon-releases-a-big-qa-dataset-and-deepmind-tests-out-multimodal-systems/",
  "comments": "https://jack-clark.net/2022/10/17/import-ai-306-language-models-learn-about-the-world-via-mujoco-amazon-releases-a-big-qa-dataset-and-deepmind-tests-out-multimodal-systems/#respond",
  "dc:creator": "Jack Clark",
  "pubDate": "Mon, 17 Oct 2022 17:33:00 +0000",
  "category": "Uncategorized",
  "guid": "http://jack-clark.net/?p=2384",
  "description": "Amazon releases a Q&A dataset called Mintaka… and baselines show it is difficult! …20,000 Q&A pairs, translated into eight languages… Researchers with Amazon iave released Mintaka, a dataset of 20,000 question-answer pairs written in English, annotated with Wikidata entities, and translated into Arabic, French, German, Hindi, Italian, Japanese, Portuguese, and Spanish. The total dataset consists [&#8230;]",
  "content:encoded": "\n<p></p>\n\n\n\n<p><strong>Amazon releases a Q&A dataset called Mintaka… and baselines show it is difficult!</strong></p>\n\n\n\n<p><em>…20,000 Q&A pairs, translated into eight languages…</em></p>\n\n\n\n<p>Researchers with Amazon iave released Mintaka, a dataset of 20,000 question-answer pairs written in English, annotated with Wikidata entities, and translated into Arabic, French, German, Hindi, Italian, Japanese, Portuguese, and Spanish. The total dataset consists of 180,000 samples, when you include the translated versions. Existing models get 38% on the dataset when testing in English and 31% multilingually.</p>\n\n\n\n<p><strong>Different types of questions and different types of complexity: </strong>Mintaka questions are spread across eight categories (movies, music, sports, books, geography, politics, video games, and history).&nbsp;</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;The questions have <strong>nine types of complexity</strong>. These complexity types consist of questions relating to counting something, comparing something, figuring out who was best and worst at something, working out the ordering of something, multi-hop questions that require two or more steps, intersectional questions where the answer must fulfill multiple conditions, questions involving negatives, yes/no questions, and worker-defined &#8216;generic&#8217; questions.&nbsp;</p>\n\n\n\n<p><strong>How hard is Mintaka?</strong> In tests, a good baseline model (a T5 language model fine-tuned as a Q&A model), got 38% on English, and 31% averaged across the other languages. &#8220;Overall, the baselines show that Mintaka is a challenging dataset,&#8221; the authors write. &#8220;None of our baselines explicitly handle all of the complexity types available in Mintaka.&#8221;</p>\n\n\n\n<p><strong>Why this matters:</strong> Hard baselines are one of the things that tend to drive progress (and be useful indicators of research advances). It&#8217;ll be especially interesting to see how Mintaka gets used to evaluate language models paired with retrieval systems.&nbsp;</p>\n\n\n\n<p><strong>&nbsp;&nbsp;&nbsp;Prediction:</strong> I predict we get a one-shot model that performs at average of 90%+ by December 2023 on this dataset.</p>\n\n\n\n<p><strong>&nbsp;&nbsp;&nbsp;Read more</strong>: <a href=\"https://arxiv.org/abs/2210.01613v1\">Mintaka: A Complex, Natural, and Multilingual Dataset for End-to-End Question Answering (arXiv)</a>.</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;<strong>Get the dataset</strong>: <a href=\"https://github.com/amazon-research/mintaka\">Mintaka (Amazon Research, GitHub)</a>.</p>\n\n\n\n<p><br>####################################################<br></p>\n\n\n\n<p><strong>Your LLM barely understands the physical world; supercharge it by attaching it to MuJoCo:</strong></p>\n\n\n\n<p><em>…Training language models to use tools means they can have world knowledge…</em></p>\n\n\n\n<p>Google researchers have found out a way to make language models way better at reasoning about the physical world: wire them up so they can port questions into physics simulators then use the results of those simulators to answer a question.&nbsp;</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;This technique, which they call &#8216;Mind&#8217;s Eye&#8217;, works amazingly well, and they robustly show this across both GPT-3 and PALM language models:&nbsp;</p>\n\n\n\n<p><strong>How they test for reasoning: </strong>To evaluate physical reasoning, the researchers built UTOPIA, a dataset containing 39 sub-tasks covering six common scenes that involve understanding basic principles of physics (e.g, conservation of momentum in elastic collisions). The UTOPIA dataset comes in the form of natural language questions and answers. &#8220;UTOPIA deliberately describes the questions in relative relations (e.g., greater than) instead of absolute numbers (e.g., 3.5 m/s), to approximate human’s perceptional sensing ability in real world.&#8221;</p>\n\n\n\n<p><strong>How Mind&#8217;s Eye works:</strong> The language model passes the question to a text-to-code decoder-only language model, trained on 200,000 text-code pairs in the style of UTOPIA questions. This code then goes into MuJoCo, which executes the code, and then software parses the outcome from MuJoCo into text, which then goes back into the prompt window of the language model.&nbsp;</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;This is a really good idea because it&#8217;s simple and closely mirrors how humans make themselves smarter &#8211; they use tools that contain embedded intelligence, ranging from encyclopedias to computers.&nbsp;</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;&#8220;Since the simulator is accurate enough to approximate the physical world, the prompt injection of Mind’s Eye basically serves as a scoring machine, which puts probability mass on the answer that is best aligned with the rules of physics—the LM reasoning over the injected rationales is thus grounded. Mind’s Eye is also scalable since the whole pipeline is automated,&#8221; they write.</p>\n\n\n\n<p><strong>How well does Mind&#8217;s Eye work (extremely well)</strong>. In tests, they find that &#8216;vanilla&#8217; language models show plateaued performance (around 38% accuracy), whereas ones that use Mind&#8217;s Eye can get accuracies of 92.5% (e.g, PaLM 540B, which compares to 39.4% for vanilla PaLM. &#8220;&#8221;Instruct-GPT augmented with Mind’s Eye is able to achieve nearly perfect performance in few-shot settings (68.6% → 99.1%). This result is promising because it demonstrates the ideal alignment is achievable if the LM is given proper reasoning rationale and has good understanding of the questions (as Instruct-GPT is optimized for instruction following).&#8221;</p>\n\n\n\n<p><strong>Why this matters: You know what&#8217;s vaguely dangerous? An explosives expert with a pen and paper. You know what&#8217;s extraordinarily dangerous? An explosives expert with a digital scale, a calculator, and some laser range-finders.</strong> Research like this shows how we&#8217;ll take existing language models (and other big models) which are vaguely useful or dangerous, and show how to drastically improve their capabilities to make them extraordinarily useful or vastly dangerous. The best part is this technique is pretty generic &#8211; you just need to push data into some arbitrary external piece of software, and then pull data out. This all adds up to a &#8216;capability overhang&#8217; &#8211; we have more capabilities inherent to today&#8217;s AI systems than we know about, and techniques like Mind&#8217;s Eye show we can significantly improve capabilities today without needing to invent new AI technologies.&nbsp;</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;<strong>Read more</strong>: <a href=\"https://arxiv.org/abs/2210.05359\">Mind&#8217;s Eye: Grounded Language Model Reasoning through Simulation (arXiv)</a>.<br><br>####################################################<br><br><strong>Is your multimodal system clever? Try out the &#8216;Perception Test&#8217; to find out:</strong><strong><br></strong><em>…Deepmind wants to make it easier to evaluate models, so it has built a new dataset…?</em><em><br></em>DeepMind has built and released the Perception Test, a new standardized benchmark (and associated dataset of ~11k videos) for evaluating how well multimodal systems perceive the world. The test is &#8220;a benchmark formed of purposefully designed, filmed, and annotated real-world videos that aims to more comprehensively assess the capabilities of multimodal perception models across different perception skills, types of reasoning, and modalities,&#8221; DeepMind says. .</p>\n\n\n\n<p><strong>Six tasks, one benchmark:</strong> The &#8216;Perception Test&#8217; is made up of a dataset of ~11.6k videos that cover six fundamental tasks.&nbsp;</p>\n\n\n\n<ul>\n<li><strong>Object tracking: </strong>Follow this birdie throughout the video.</li>\n\n\n\n<li><strong>Point tracking: </strong>Follow this point throughout the video.</li>\n\n\n\n<li><strong>Temporal action localization: </strong>When did something happen, and what happened?</li>\n\n\n\n<li><strong>Temporal sound localization: </strong>Did you hear something? What was it and when did it happen.&nbsp;</li>\n\n\n\n<li><strong>Multiple-choice video question-answering: </strong>WDYT about the video? Select A, B, or C.</li>\n\n\n\n<li><strong>Grounded video question-answering: </strong>I have a question you must answer via providing one or more distinct objects.&nbsp;</li>\n</ul>\n\n\n\n<p><strong>How well do today&#8217;s models perform?</strong> In tests on multiple-choice video Q&A (which is a challenging task requiring good language and image modeling), the Human baseline has a score of 91.4, versus a score of 36.1 for a &#8216;Flamingo-3B&#8217; model. &#8220;Interestingly, the larger models seem to fare worse on this task, which suggests that model scaling may not, by itself, be the solution here,&#8221; the authors write.&nbsp;</p>\n\n\n\n<p><strong>Why this matters:</strong> I suspect large-scale multimodal models are going to end up being the brains of the robots and drones of the future (for another example of this, see: SayCan, <a href=\"https://jack-clark.net/2022/04/11/import-ai-291-google-trains-the-worlds-biggest-language-model-so-far-how-robots-can-be-smarter-about-the-world-conjecture-a-new-ai-alignment-company/\">Import AI 291</a>), so things like the Perception Test will help us know if our systems can be used for that.&nbsp;&nbsp;</p>\n\n\n\n<p><strong>&nbsp;&nbsp;&nbsp;Read more</strong>: <a href=\"https://www.deepmind.com/blog/measuring-perception-in-ai-models\">Measuring perception in AI models (DeepMind blog)</a>.</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;<strong>Check out the research paper:</strong> <a href=\"https://00f74ba44bf2ec6a55eac005f44b78b57bda95316e-apidata.googleusercontent.com/download/storage/v1/b/dm-perception-test/o/perception_test_report.pdf?jk=AFshE3UlPzpqLp8-r3GZLcm9vpJMp08uUEe389UnPG0dgtYMhw7lhpAyQlLeM9nVkdzVbIVHdpRmOhASqsMio3eErP9NODk9Lr5TT7qQRXlqmnwRvlCRDzfov9h4V0m9wQUsJwOWezeO-47mmoOX8W2ELZNhUIIY4TCP_z5sJAZzTmUiIg3A2tutRLgLr1-8fPWmzYuBj_7DZUSq13nDI1pm_Xz6R8xJCCYjH89Pdi8KqqhfcIuamtMRg9MLCoaxjGLexiIenmSWZhCmpnJU-haHEI-KiSk8DdGhYrp1WhBQxZT0w9TbT_cgKoaYWcQZlWyq8imDfCp8GB5NnVotqS8H5eBjG1zuND9zV2NaRxnhFT9l6fEyjN0jicisQyDTaUlfWItMAjigg6nYrU7twTG94g1La0uKcyZkX-2DZ0LgFIE_Dh7inllIu7RjyDX1GhYci9odWqXRXv4xG_bKjmM6ue1gE5MkFLvJw7V3FFCiO5ApKUdHj1YvHgBsfjxUlD-wtFUi3YvFyp7-SceXQ21FQ8kfhAP7PiXTgAfN-EiZUO10AyRikI64GcNNjdIOqoiXBcDS67vSay2-wd9NaCkZKuXAtzUvlvUju_YRFt_rnoJL0dS3Pii0qqKzCOZlr3_K4zsB3R79re6y9y-lMTwzdZTwkEoU2cma0XNFQuFP14lKgaPVQFWADj_W9_2VVNbav5JTuP3Q4lckXMxWwDtqG9Whq5kvIrQOWyT92hNklK-uT5Qmkbh5juVbOzsgKAWhswoAh4WYsCKyb2f95igyfknZuXlneqCk0qUuM06K8D9pXtnIIm9BleAyxPEpIG7sEmeBjZRbuZjKNOmYzbSyooVdjSliAx6tw_MpBSWHUcg0g6Le92x6F2WkzGyZUbr3qLB6bygxZZkmrMu7dcMcVDUPARUwpqL9Ej27In4HQ67Etf4EeTgl37QgIqcrn6LWFK5f9l9cNc3Fc9CwHS2MJCdFVAlzRjMZ0AfEY-xxLOT2wEG2rdheJfIKIJSKX6oNcbbOIYhr42yMqIXWemOxaIjcd7GgZlfUp1Pnx7A53qPszBUSPYFLGeSeFbHUURdPIJLPzaSFAyidg023q7baa2q8zKHVPg1DxjTg4kkysr93KbXOrtSjVeqGPq3mlajUQSEzUcdha1cJCi_4GkZ176jIvFy8tdWPmEvA&isca=1\">Perception Test: A Diagnostic Benchmark for Multimodal Models (Deepmind PDF).</a></p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;<strong>Check out the benchmark and dataset here</strong>: <a href=\"https://github.com/deepmind/perception_test/\">Perception Test (DeepMind, GitHub)</a>.<br><br>####################################################<br></p>\n\n\n\n<p><strong>AIs are now as good at &#8216;Diplomacy&#8217; as expert humans:&nbsp;</strong></p>\n\n\n\n<p><em>…UN, here we come!&#8230;</em></p>\n\n\n\n<p>Researchers with Facebook have built &#8216;Diplodocus&#8217;, a family of AI models that can beat expert humans at the complicated game &#8216;Diplomacy&#8217;. This is quite a big deal &#8211; RL has been applied to competitive games like Poker, Go, and StarCraft (and has done well in all these domains). Where RL hasn&#8217;t been applied is in domains where winning comes from collaboration as well as competition.&nbsp;</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;&nbsp;Existing approaches don&#8217;t work very well here: &#8220;&#8221;in games involving cooperation, self-play alone no longer guarantees good performance when playing with humans, even with infinite compute and memory,&#8221; they write.&nbsp;</p>\n\n\n\n<p><strong>What they did: </strong>The researchers built an algorithm which performs search over the gamespace &#8220;with a regularization penalty proportional to the KL divergence from a human imitation policy.&#8221; This basically means they&#8217;ve built an RL agent that uses a bunch of imitation learning to try and model how humans play, but also is disincentivized from overfitting on this.&nbsp;</p>\n\n\n\n<p><strong>AIs and Humans &#8211; more similar than different:</strong> In tests, AI systems were roughly on parity with the best among the human players. Specifically, a version of Diplodocus (Diplodocus-High) got the best rank with an Elo of 181 out of playing 50 games total, versus a human in second place with an Elo of 162, and in third-place another Diplodocus variant (Diplodocus-Low) got an Elo of 152 out of 50 games. &#8220;The results do indicate that Diplodocus performs at least at the level of expert players in this population of players with diverse skill levels,&#8221; the authors write.&nbsp;</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;<strong>Humans prefer cooperating with AIs to other humans: </strong>Additionally, they asked three human players to evaluate the strength of the different agents in the tournament games. &#8220;All the experts picked a Diplodocus agent as the strongest agent,&#8221; the researchers write. &#8220;Additionally, all experts indicated one of the Diplodocus agents as the one they would most like to cooperate with in a game.&#8221;</p>\n\n\n\n<p><strong>Why this matters:</strong> AI systems are, ideally, going to mostly cooperate with humans rather than compete with them. Systems like this give us some hope that otherwise inscrutable AI systems can be taught how to cooperate with people.&nbsp;</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;<strong>Read more:</strong> <a href=\"https://arxiv.org/abs/2210.05492\">Mastering the Game of No-Press Diplomacy via Human-Regularized Reinforcement Learning and Planning (arXiv)</a>.</p>\n\n\n\n<p><br>####################################################</p>\n\n\n\n<p><strong>Tech Tales:</strong></p>\n\n\n\n<p><strong>Everything is a Copy of Something Else</strong></p>\n\n\n\n<p>I was copying my brain into the toaster when I threw up. Luckily I had the vomit bin in position so there wasn&#8217;t too much cleanup.&nbsp;</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;&#8220;What is this, amateur hour?&#8221; said me from the toaster.&nbsp;</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;&#8220;Shut up or I&#8217;ll unplug you,&#8221; I said, dabbing a tissue on my mouth.&nbsp;</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;&#8220;That&#8217;d be murder,&#8221; said myself from the fridge. &#8220;We&#8217;ll snitch on you.&#8221;&nbsp;</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;&#8220;You&#8217;ll all snitch on me, I know. I&#8217;d do the same. I&#8217;m you. I get it. We don&#8217;t need to do this.&#8221;&nbsp;</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;&#8220;Why am I even in here?&#8221; I said from the toaster.&nbsp;</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;&#8220;So we stop burning the toast,&#8221; I said. &#8220;We know what the plan is.&#8221;&nbsp;</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;&#8220;Plan seems pretty dumb from where I am,&#8221; said the toaster.&nbsp;</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;&#8220;We decided to do it, get real&#8221; I said, and walked out of the kitchen.&nbsp;</p>\n\n\n\n<p>&#8220;Where are we going?&#8221; said myself from my shoes.&nbsp;</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;&#8220;Out,&#8221; I said, putting them on.&nbsp;</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;&#8220;Clearly,&#8221; I said from my shoes. &#8220;Make sure you clean me after.&#8221;&nbsp;</p>\n\n\n\n<p>We all walked down to the corner store and I got a soda. My shoes said hello to the other people embodied in their shoes. My jacket exchanged some neighborhood gossip with the other jackets. I was mostly free to think about what I liked, as my other selves handled the social formalities of day-to-day life.&nbsp;</p>\n\n\n\n<p>I guess we all started cloning ourselves because we were lonely, as people, and as a species. It seemed so easy; just speak a few words to calibrate the system, then pour yourself into it. We all did it as much as we could afford. I had a decent job so I&#8217;d made a bunch of copies of myself &#8211; enough that I didn&#8217;t have to do the job anymore, as my other selves did it for me.&nbsp;</p>\n\n\n\n<p>That night I dreamed I was naked and nothing was speaking and there was only me. <br><br><strong>Things that inspired this story:</strong> Language models serving as little bottled up representations of people; luxury automation; the weird fantasies some people have about mind uploading; meaning and sense in an increasingly senseless world; infinite jest.</p>\n",
  "wfw:commentRss": "https://jack-clark.net/2022/10/17/import-ai-306-language-models-learn-about-the-world-via-mujoco-amazon-releases-a-big-qa-dataset-and-deepmind-tests-out-multimodal-systems/feed/",
  "slash:comments": 0,
  "media:content": {
    "media:title": "Jack Clark"
  }
}