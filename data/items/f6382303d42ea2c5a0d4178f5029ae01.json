{
  "title": "Anatomy of a Probabilistic Programming Framework",
  "link": "",
  "id": "https://www.georgeho.org/prob-prog-frameworks/",
  "updated": "2019-09-30T00:00:00Z",
  "published": "2019-09-30T00:00:00Z",
  "content": "<p>Recently, the PyMC4 developers <a href=\"https://openreview.net/forum?id=rkgzj5Za8H\">submitted an\nabstract</a> to the <a href=\"https://program-transformations.github.io/\"><em>Program Transformations\nfor Machine Learning</em> NeurIPS workshop</a>. I\nrealized that despite knowing a thing or two about Bayesian modelling, I don&rsquo;t\nunderstand how probabilistic programming frameworks are structured, and therefore\ncouldn&rsquo;t appreciate the sophisticated design work going into PyMC4. So I trawled through\npapers, documentation and source code<sup id=\"fnref:1\"><a href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\">1</a></sup> of various open-source probabilistic\nprogramming frameworks, and this is what I&rsquo;ve managed to take away from it.</p>\n<p>I assume you know a fair bit about probabilistic programming and Bayesian modelling, and\nare familiar with the big players in the probabilistic programming world. If you&rsquo;re\nunsure, you can <a href=\"https://www.georgeho.org/bayesian-inference-reading/\">read up here</a>.</p>\n<div>\n<h2>Contents</h2>\n<nav id=\"TableOfContents\">\n<ul>\n<li><a href=\"#dissecting-probabilistic-programming-frameworks\">Dissecting Probabilistic Programming Frameworks</a>\n<ul>\n<li><a href=\"#specifying-the-model-languageapi\">Specifying the model: language/API</a></li>\n<li><a href=\"#building-the-model-density-distributions-and-transformations\">Building the model density: distributions and transformations</a></li>\n<li><a href=\"#computing-the-posterior-inference-algorithm\">Computing the posterior: inference algorithm</a></li>\n<li><a href=\"#computing-the-mode-optimizer\">Computing the mode: optimizer</a></li>\n<li><a href=\"#computing-gradients-autodifferentiation\">Computing gradients: autodifferentiation</a></li>\n<li><a href=\"#monitoring-inference-diagnostics\">Monitoring inference: diagnostics</a></li>\n</ul>\n</li>\n<li><a href=\"#a-zoo-of-probabilistic-programming-frameworks\">A Zoo of Probabilistic Programming Frameworks</a>\n<ul>\n<li><a href=\"#stan\">Stan</a></li>\n<li><a href=\"#tensorflow-probability-aka-tfp\">TensorFlow Probability (a.k.a. TFP)</a></li>\n<li><a href=\"#pymc3\">PyMC3</a></li>\n<li><a href=\"#pymc4\">PyMC4</a></li>\n<li><a href=\"#pyro\">Pyro</a></li>\n</ul>\n</li>\n</ul>\n</nav>\n</div>\n<h2 id=\"dissecting-probabilistic-programming-frameworks\">Dissecting Probabilistic Programming Frameworks</h2>\n<p>A probabilistic programming framework needs to provide six things:</p>\n<ol>\n<li>A language or API for users to specify a model</li>\n<li>A library of probability distributions and transformations to build the posterior\ndensity</li>\n<li>At least one inference algorithm, which either draws samples from the posterior (in\nthe case of Markov Chain Monte Carlo, MCMC) or computes some approximation of it (in\nthe case of variational inference, VI)</li>\n<li>At least one optimizer, which can compute the mode of the posterior density</li>\n<li>An autodifferentiation library to compute gradients required by the inference\nalgorithm and optimizer</li>\n<li>A suite of diagnostics to monitor and analyze the quality of inference</li>\n</ol>\n<p>These six pieces come together like so:</p>\n<p><img src=\"https://www.georgeho.org/assets/images/prob-prog-flowchart.png\" alt=\"Flowchart illustrating the structure of a probabilistic programmingframeworks\"></p>\n<p>Let&rsquo;s break this down one by one.</p>\n<h3 id=\"specifying-the-model-languageapi\">Specifying the model: language/API</h3>\n<p>This is what users will use to specify their models. Most frameworks will let users\nwrite in some existing programming language and call the framework&rsquo;s functions and\nclasses, but <del>some others</del> — why don&rsquo;t I just say it — Stan rolls their own\ndomain-specific language.</p>\n<p>The main question here is what language you think is best for users to specify models\nin: any sufficiently popular host language (such as Python) will reduce the learning\ncurve for users and make the framework easier to develop and maintain, but a creating\nyour own language allows you to introduce helpful abstractions for your framework&rsquo;s\nparticular use case (as <a href=\"https://mc-stan.org/docs/2_20/reference-manual/blocks-chapter.html\">Stan\ndoes</a>, for example).</p>\n<p>At this point I should point out the non-universal, Python bias in this post: there are\nplenty of interesting non-Python probabilistic programming frameworks out there (e.g.\n<a href=\"https://greta-stats.org/\">Greta</a> in R, <a href=\"https://turing.ml/dev/\">Turing</a> and\n<a href=\"https://www.gen.dev/\">Gen</a> in Julia, <a href=\"https://github.com/p2t2/figaro\">Figaro</a> and\n<a href=\"https://github.com/stripe/rainier\">Rainier</a> in Scala), as well as universal\nprobabilistic programming systems<sup id=\"fnref:2\"><a href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\">2</a></sup> (e.g.\n<a href=\"http://probcomp.csail.mit.edu/software/venture/\">Venture</a> from MIT,\n<a href=\"https://probprog.github.io/anglican/index.html\">Angelican</a> from Oxford)<sup id=\"fnref:3\"><a href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\">3</a></sup>. I just\ndon&rsquo;t know anything about any of them.</p>\n<h3 id=\"building-the-model-density-distributions-and-transformations\">Building the model density: distributions and transformations</h3>\n<p>These are what the user&rsquo;s model calls, in order to compile/build the model itself\n(whether that means a posterior log probability, in the case of MCMC, or some loss\nfunction to minimize, in the case of VI). By <em>distributions</em>, I mean the probability\ndistributions that the random variables in your model can assume (e.g. Normal or\nPoisson), and by <em>transformations</em> I mean deterministic mathematical operations you can\nperform on these random variables, while still keeping track of the derivative of these\ntransformations<sup id=\"fnref:4\"><a href=\"#fn:4\" class=\"footnote-ref\" role=\"doc-noteref\">4</a></sup> (e.g. exponentials, logarithms, sines or cosines).</p>\n<p>This is a good time to point out that the interactions between the language/API and the\ndistributions and transformations libraries is a major design problem. Here&rsquo;s a (by no\nmeans exhaustive) list of necessary considerations:</p>\n<ol>\n<li>In order to build the model density, the framework must keep track of every\ndistribution and transformation, while also computing the derivatives of any such\ntransformations. This results in a Jekyll-and-Hyde problem where every transformation\nrequires a forward and backwards definition. Should this tracking happen eagerly, or\nshould it be deferred until the user specifies what the model will be used for?</li>\n<li>Theoretically, a model&rsquo;s specification should be the same whether it is to be used\nfor evaluation, inference or debugging. However, in practice, the program execution\n(and computational graph) are different for these three purposes. How should the\nframework manage this?</li>\n<li>The framework must also keep track of the shapes of random variables, which is\nfrighteningly non-trivial! Check out <a href=\"https://ericmjl.github.io/blog/2019/5/29/reasoning-about-shapes-and-probability-distributions/\">this blog\npost</a>\nor <a href=\"https://arxiv.org/abs/1711.10604\">the original Tensorflow Distributions paper</a>\n(specifically section 3.3 on shape semantics) for more details.</li>\n</ol>\n<p>For a more comprehensive treatment, I can&rsquo;t recommend <a href=\"https://docs.google.com/presentation/d/1xgNRJDwkWjTHOYMj5aGefwWiV8x-Tz55GfkBksZsN3g/edit?usp=sharing\">Junpeng Lao&rsquo;s PyData Córdoba 2019\ntalk</a>\nhighly enough — he explains in depth the main challenges in implementing a probabilistic\nprogramming API and highlights how various frameworks manage these difficulties.</p>\n<h3 id=\"computing-the-posterior-inference-algorithm\">Computing the posterior: inference algorithm</h3>\n<p>Having specified and built the model, the framework must now actually perform inference:\ngiven a model and some data, obtain the posterior (either by sampling from it, in the\ncase of MCMC, or by approximating it, in the case of VI).</p>\n<p>Most probabilistic programming frameworks out there implement both MCMC and VI\nalgorithms, although strength of support and quality of documentation can lean heavily\none way or another. For example, Stan invests heavily into its MCMC, whereas Pyro has\nthe most extensive support for its stochastic VI.</p>\n<h3 id=\"computing-the-mode-optimizer\">Computing the mode: optimizer</h3>\n<p>Sometimes, instead of performing full-blown inference, it&rsquo;s useful to find the mode of\nthe model density. These modes can be used as point estimates of parameters, or as the\nbasis of approximations to a Bayesian posterior. Or perhaps you&rsquo;re doing VI, and you\nneed some way to perform SGD on a loss function. In either case, a probabilistic\nprogramming framework calls for an optimizer.</p>\n<p>If you don&rsquo;t need to do VI, then a simple and sensible thing to do is to use some\n<a href=\"https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm\">BFGS-based optimization\nalgorithm</a>\n(e.g. some quasi-Newton method like\n<a href=\"https://en.wikipedia.org/wiki/Limited-memory_BFGS\">L-BFGS</a>) and call it a day.\nHowever, frameworks that focus on VI need to implement <a href=\"http://docs.pyro.ai/en/stable/optimization.html#module-pyro.optim.optim\">optimizers commonly seen in deep\nlearning</a>, such\nas Adam or RMSProp.</p>\n<h3 id=\"computing-gradients-autodifferentiation\">Computing gradients: autodifferentiation</h3>\n<p>Both the inference algorithm and the optimizer require gradients (at least, if you&rsquo;re\nnot using ancient inference algorithms and optimizers!), and so you&rsquo;ll need some way to\ncompute these gradients.</p>\n<p>The easiest thing to do would be to rely on a deep learning framework like TensorFlow or\nPyTorch. I&rsquo;ve learned not to get too excited about this though: while deep learning\nframeworks&rsquo; heavy optimization of parallelized routines lets you e.g. obtain <a href=\"https://colindcarroll.com/2019/08/18/very-parallel-mcmc-sampling/\">thousands\nof MCMC chains in a reasonable amount of\ntime</a>, it&rsquo;s not\nobvious that this is useful at all (although there&rsquo;s definitely some work going on in\nthis area).</p>\n<h3 id=\"monitoring-inference-diagnostics\">Monitoring inference: diagnostics</h3>\n<p>Finally, once the inference algorithm has worked its magic, you&rsquo;ll want a way to verify\nthe validity and efficiency of that inference. This involves some <a href=\"https://arviz-devs.github.io/arviz/api.html#stats\">off-the-shelf\nstatistical diagnostics</a> (e.g. BFMI,\ninformation criteria, effective sample size, etc.), but mainly <a href=\"https://arviz-devs.github.io/arviz/api.html#plots\">lots and lots of\nvisualization</a>.</p>\n<h2 id=\"a-zoo-of-probabilistic-programming-frameworks\">A Zoo of Probabilistic Programming Frameworks</h2>\n<p>Having outlined the basic internals of probabilistic programming frameworks, I think\nit&rsquo;s helpful to go through several of the popular frameworks as examples. I&rsquo;ve tried to\nlink to the relevant source code in the frameworks where possible.</p>\n<h3 id=\"stan\">Stan</h3>\n<p>It&rsquo;s very easy to describe how Stan is structured: literally everything is\nimplemented from scratch in C++.</p>\n<ol>\n<li>Stan has a compiler for <a href=\"https://github.com/stan-dev/stan/tree/develop/src/stan/lang\">a small domain-specific language for specifying Bayesian\nmodels</a></li>\n<li>Stan has libraries of <a href=\"https://github.com/stan-dev/math/tree/develop/stan/math/prim\">probability\ndistributions</a> and\n<a href=\"https://github.com/stan-dev/math/tree/develop/stan/math/prim/fun\">transforms</a></li>\n<li>Stan implements <a href=\"https://github.com/stan-dev/stan/tree/develop/src/stan/mcmc/hmc\">dynamic\nHMC</a> and\n<a href=\"https://github.com/stan-dev/stan/tree/develop/src/stan/variational\">variational\ninference</a></li>\n<li>Stan also rolls their own <a href=\"https://github.com/stan-dev/math/tree/develop/stan/math\">autodifferentiation\nlibrary</a><sup id=\"fnref:5\"><a href=\"#fn:5\" class=\"footnote-ref\" role=\"doc-noteref\">5</a></sup></li>\n<li>Stan implements an <a href=\"https://github.com/stan-dev/stan/tree/develop/src/stan/optimization\">L-BFGS based\noptimizer</a> (but\nalso implements <a href=\"https://mc-stan.org/docs/2_20/reference-manual/optimization-algorithms-chapter.html\">a less efficient Newton\noptimizer</a>)</li>\n<li>Finally, Stan has a <a href=\"https://github.com/stan-dev/stan/tree/develop/src/stan/analyze/mcmc\">suite of\ndiagnostics</a></li>\n</ol>\n<p>Note that contrary to popular belief, Stan <em>does not</em> implement NUTS:</p>\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Stan implements a dynamic Hamiltonian Monte Carlo method with multinomial sampling of dynamic length trajectories, generalized termination criterion, and improved adaptation of the Euclidean metric.</p>&mdash; Dan Simpson (<a href=\"https://twitter.com/dan_p_simpson\">@dan_p_simpson</a>) <a href=\"https://twitter.com/dan_p_simpson/status/1037332473175265280\">September 5, 2018</a></blockquote>\n<p>And in case you&rsquo;re looking for a snazzy buzzword to drop:</p>\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Adaptive HMC. <a href=\"https://twitter.com/betanalpha\">@betanalpha</a> is reluctant to give it a more specific name because, to paraphrase, that’s just marketing bullshit that leads to us celebrating tiny implementation details rather than actual meaningful contributions to comp stats. This is a wide-ranging subtweet.</p>&mdash; Dan Simpson (<a href=\"https://twitter.com/dan_p_simpson\">@dan_p_simpson</a>) <a href=\"https://twitter.com/dan_p_simpson/status/1034098649406554113\">August 27, 2018</a></blockquote>\n<h3 id=\"tensorflow-probability-aka-tfp\">TensorFlow Probability (a.k.a. TFP)</h3>\n<ol>\n<li>TFP users write Python (albeit through an <a href=\"https://colcarroll.github.io/ppl-api/\">extremely verbose\nAPI</a>)</li>\n<li>TFP implements their own\n<a href=\"https://github.com/tensorflow/probability/tree/master/tensorflow_probability/python/distributions\">distributions</a>\nand\n<a href=\"https://github.com/tensorflow/probability/tree/master/tensorflow_probability/python/bijectors\">transforms</a>\n(which TensorFlow, for some reason, calls &ldquo;bijectors&rdquo;). You can find more details in\n<a href=\"https://arxiv.org/abs/1711.10604\">their arXiv paper</a></li>\n<li>TFP implements <a href=\"https://github.com/tensorflow/probability/tree/master/tensorflow_probability/python/mcmc\">a ton of\nMCMC</a>\nalgorithms and a handful of <a href=\"https://github.com/tensorflow/probability/tree/master/tensorflow_probability/python/vi\">VI\nalgorithms</a>\nin TensorFlow</li>\n<li>TFP implements <a href=\"https://github.com/tensorflow/probability/tree/master/tensorflow_probability/python/optimizer\">several\noptimizers</a>,\nincluding Nelder-Mead, BFGS and L-BFGS (again, in TensorFlow)</li>\n<li>TFP relies on TensorFlow to compute gradients (er, duh)</li>\n<li>TFP implements <a href=\"https://github.com/tensorflow/probability/blob/master/tensorflow_probability/python/mcmc/diagnostic.py\">a handful of\nmetrics</a>\n(e.g. effective sample size and potential scale reduction), but seems to lack a\ncomprehensive suite of diagnostics and visualizations: even\n<a href=\"https://github.com/tensorflow/probability/tree/master/tensorflow_probability/python/experimental/edward2\">Edward2</a>\n(an experimental interface to TFP for flexible modelling, inference and criticism)\nsuggests that you <a href=\"https://github.com/tensorflow/probability/blob/master/tensorflow_probability/python/experimental/edward2/Upgrading_From_Edward_To_Edward2.md#model--inference-criticism\">build your metrics manually or use boilerplate in\n<code>tf.metrics</code></a></li>\n</ol>\n<h3 id=\"pymc3\">PyMC3</h3>\n<ol>\n<li>PyMC3 users write Python code, using a context manager pattern (i.e. <code>with pm.Model as model</code>)</li>\n<li>PyMC3 implements its own\n<a href=\"https://github.com/pymc-devs/pymc3/tree/master/pymc3/distributions\">distributions</a>\nand\n<a href=\"https://github.com/pymc-devs/pymc3/blob/master/pymc3/distributions/transforms.py\">transforms</a></li>\n<li>PyMC3 implements\n<a href=\"https://github.com/pymc-devs/pymc3/blob/master/pymc3/step_methods/hmc/nuts.py\">NUTS</a>,\n(as well as <a href=\"https://github.com/pymc-devs/pymc3/tree/master/pymc3/step_methods\">a range of other MCMC step\nmethods</a>) and\n<a href=\"https://github.com/pymc-devs/pymc3/tree/master/pymc3/variational\">several variational inference\nalgorithms</a>,\nalthough NUTS is the default and recommended inference algorithm</li>\n<li>PyMC3 (specifically, the <code>find_MAP</code> function) <a href=\"https://github.com/pymc-devs/pymc3/blob/master/pymc3/tuning/starting.py\">relies on\n<code>scipy.optimize</code></a>,\nwhich in turn implements a BFGS-based optimizer</li>\n<li>PyMC3 <a href=\"https://github.com/pymc-devs/pymc3/blob/master/pymc3/theanof.py\">relies on\nTheano</a> to compute\ngradients</li>\n<li>PyMC3 <a href=\"https://github.com/pymc-devs/pymc3/blob/master/pymc3/plots/__init__.py\">delegates posterior visualization and\ndiagnostics</a>\nto its cousin project <a href=\"https://arviz-devs.github.io/arviz/\">ArviZ</a></li>\n</ol>\n<p>Some remarks:</p>\n<ul>\n<li>PyMC3&rsquo;s context manager pattern is an interceptor for sampling statements: essentially\n<a href=\"https://arxiv.org/abs/1811.06150\">an accidental implementation of effect handlers</a>.</li>\n<li>PyMC3&rsquo;s distributions are simpler than those of TFP or PyTorch: they simply need to\nhave a <code>random</code> and a <code>logp</code> method, whereas TFP/PyTorch implement a whole bunch of\nother methods to handle shapes, parameterizations, etc. In retrospect, we realize\nthat this is <a href=\"https://docs.pymc.io/developer_guide.html#what-we-got-wrong\">one of PyMC3&rsquo;s design\nflaws</a>.</li>\n</ul>\n<h3 id=\"pymc4\">PyMC4</h3>\n<p>PyMC4 is still under active development (at least, at the time of writing), but it&rsquo;s\nsafe to call out the overall architecture.</p>\n<ol>\n<li>PyMC4 users will write Python, although now with a generator pattern (e.g. <code>x = yield Normal(0, 1, &quot;x&quot;)</code>), instead of a context manager</li>\n<li>PyMC4 will <a href=\"https://github.com/pymc-devs/pymc4/tree/master/pymc4/distributions/\">rely on TensorFlow distributions (a.k.a.\n<code>tfd</code>)</a> for both\ndistributions and transforms</li>\n<li>PyMC4 will also <a href=\"https://github.com/pymc-devs/pymc4/tree/master/pymc4/inference/\">rely on TensorFlow for\nMCMC</a> (although the\nspecifics of the exact MCMC algorithm are still fairly fluid at the time of writing)</li>\n<li>As far as I can tell, the optimizer is still TBD</li>\n<li>Because PyMC4 relies on TFP, which relies on TensorFlow, TensorFlow manages all\ngradient computations automatically</li>\n<li>Like its predecessor, PyMC4 will delegate diagnostics and visualization to ArviZ</li>\n</ol>\n<p>Some remarks:</p>\n<ul>\n<li>With the generator pattern for model specification, PyMC4 embraces the notion of a\nprobabilistic program as one that defers its computation. For more color on this, see\n<a href=\"https://twitter.com/avibryant/status/1150827954319982592\">this Twitter thread</a> I had\nwith <a href=\"https://about.me/avibryant\">Avi Bryant</a>.</li>\n</ul>\n<h3 id=\"pyro\">Pyro</h3>\n<ol>\n<li>Pyro users write Python</li>\n<li>Pyro <a href=\"https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/__init__.py\">relies on PyTorch\ndistributions</a>\n(<a href=\"https://github.com/pyro-ppl/pyro/tree/dev/pyro/distributions\">implementing its own where\nnecessary</a>), and also\nrelies on PyTorch distributions <a href=\"https://github.com/pyro-ppl/pyro/tree/dev/pyro/distributions/transforms\">for its\ntransforms</a></li>\n<li>Pyro implements <a href=\"http://docs.pyro.ai/en/stable/inference.html\">many inference\nalgorithms</a> in PyTorch (including <a href=\"https://github.com/pyro-ppl/pyro/tree/dev/pyro/infer/mcmc\">HMC\nand NUTS</a>), but support\nfor <a href=\"https://github.com/pyro-ppl/pyro/blob/dev/pyro/infer/svi.py\">stochastic VI</a> is\nthe most extensive</li>\n<li>Pyro implements <a href=\"https://github.com/pyro-ppl/pyro/blob/master/pyro/optim/optim.py\">its own\noptimizer</a> in\nPyTorch</li>\n<li>Pyro relies on PyTorch to compute gradients (again, duh)</li>\n<li>As far as I can tell, Pyro doesn&rsquo;t provide any diagnostic or visualization\nfunctionality</li>\n</ol>\n<p>Some remarks:</p>\n<ul>\n<li>Pyro includes the Poutine submodule, which is a library of composable <a href=\"https://arxiv.org/abs/1811.06150\">effect\nhandlers</a>. While this might sound like recondite\nabstractions, they allow you to implement your own custom inference algorithms and\notherwise manipulate Pyro probabilistic programs. In fact, all of Pyro&rsquo;s inference\nalgorithms use these effect handlers.</li>\n</ul>\n<div class=\"footnotes\" role=\"doc-endnotes\">\n<hr>\n<ol>\n<li id=\"fn:1\">\n<p>In case you&rsquo;re testifying under oath and need more reliable sources than\na blog post, I&rsquo;ve kept a <a href=\"https://www.zotero.org/eigenfoo/items/collectionKey/AE8882GQ\">Zotero\ncollection</a> for\nthis project.&#160;<a href=\"#fnref:1\" class=\"footnote-backref\" role=\"doc-backlink\">&#x21a9;&#xfe0e;</a></p>\n</li>\n<li id=\"fn:2\">\n<p>Universal probabilistic programming is an interesting field of inquiry,\nbut has mainly remained in the realm of academic research. For a (much) more\ncomprehensive treatment, check out <a href=\"http://www.robots.ox.ac.uk/~twgr/assets/pdf/rainforth2017thesis.pdf\">Tom Rainforth&rsquo;s PhD\nthesis</a>.&#160;<a href=\"#fnref:2\" class=\"footnote-backref\" role=\"doc-backlink\">&#x21a9;&#xfe0e;</a></p>\n</li>\n<li id=\"fn:3\">\n<p>Since publishing this blog post, I have been informed that I am more\nignorant than I know: I have forgotten\n<a href=\"https://github.com/cscherrer/Soss.jl\">Soss.jl</a> in Julia and\n<a href=\"https://github.com/thu-ml/zhusuan\">ZhuSuan</a> in Python.&#160;<a href=\"#fnref:3\" class=\"footnote-backref\" role=\"doc-backlink\">&#x21a9;&#xfe0e;</a></p>\n</li>\n<li id=\"fn:4\">\n<p>It turns out that such transformations must be <a href=\"https://en.wikipedia.org/wiki/Local_diffeomorphism\">local\ndiffeomorphisms</a>, and the\nderivative information requires computing the log determinant of the Jacobian\nof the transformation, commonly abbreviated to <code>log_det_jac</code> or something\nsimilar.&#160;<a href=\"#fnref:4\" class=\"footnote-backref\" role=\"doc-backlink\">&#x21a9;&#xfe0e;</a></p>\n</li>\n<li id=\"fn:5\">\n<p>As an aside, I&rsquo;ll say that it&rsquo;s mind boggling how Stan does this. To\nquote a (nameless) PyMC core developer:</p>\n<blockquote>\n<p>I think that maintaining your own autodifferentiation library is the\npath of a crazy person.</p>\n</blockquote>\n&#160;<a href=\"#fnref:5\" class=\"footnote-backref\" role=\"doc-backlink\">&#x21a9;&#xfe0e;</a></li>\n</ol>\n</div>"
}