{
  "id": "tag:blogger.com,1999:blog-15418143.post-3255634544070567159",
  "published": "2015-12-08T23:57:00.000-05:00",
  "updated": "2016-06-13T07:40:03.573-05:00",
  "category": [
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    ""
  ],
  "title": "ICCV 2015: Twenty one hottest research papers",
  "content": "<h3>\"Geometry vs Recognition\" becomes ConvNet-for-X</h3>Computer Vision used to be cleanly separated into two schools: <b>geometry</b> and <b>recognition</b>.&nbsp;Geometric methods like structure from motion and optical flow usually focus on measuring objective real-world quantities like 3D \"real-world\" distances directly from images and recognition techniques like support vector machines and probabilistic graphical models traditionally focus on perceiving high-level semantic information (i.e., is this a dog or a table) directly from images.<br /><br />The world of computer vision <strike>is changing fast</strike> has changed. We now have powerful convolutional neural networks that are able to extract <i>just about anything</i> directly from images. So if your input is an image (or set of images), then there's probably a ConvNet for your problem. &nbsp;While you do need a large labeled dataset, believe me when I say that collecting a large dataset is much easier than manually tweaking knobs inside your 100K-line codebase. As we're about to see, the separation between geometric methods and learning-based methods is no longer easily discernible.<br /><br />By 2016 just about everybody in the computer vision community will have tasted the power of ConvNets, so let's take a look at some of the hottest new research directions in computer vision.<br /><br /><h3>ICCV 2015's Twenty One Hottest Research Papers</h3><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"http://2.bp.blogspot.com/-OItMW6lzZ9A/VmepmMJ2COI/AAAAAAAAOa8/MB1unZ7hr8Y/s1600/onfire-01.png\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"233\" src=\"https://2.bp.blogspot.com/-OItMW6lzZ9A/VmepmMJ2COI/AAAAAAAAOa8/MB1unZ7hr8Y/s320/onfire-01.png\" width=\"320\" /></a></div><div><br /></div><br />This December in Santiago, Chile, the <a href=\"http://pamitc.org/iccv15/\">International Conference of Computer Vision 2015</a> is going to bring together the world's leading researchers in Computer Vision, Machine Learning, and Computer Graphics.<br /><br />To no surprise, this year's ICCV is filled with lots of ConvNets, but this time the applications of these Deep Learning tools are being applied to much much more creative tasks. Let's take a look at the following <b>twenty one ICCV 2015 research papers</b>, which will hopefully give you a taste of where the field is going.<br /><br /><br />1.&nbsp;<a href=\"https://www.d2.mpi-inf.mpg.de/sites/default/files/iccv15-neural_qa.pdf\">Ask Your Neurons: A Neural-Based Approach to Answering Questions About Images</a> Mateusz Malinowski, Marcus Rohrbach, Mario Fritz<br /><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"http://2.bp.blogspot.com/-YZ8eabNTeI0/VmOay_v1zlI/AAAAAAAAOW4/05yONZ7wws4/s1600/malinowski.png\" imageanchor=\"1\" style=\"clear: right; float: right; margin-bottom: 1em; margin-left: 1em;\"><img border=\"0\" height=\"155\" src=\"https://2.bp.blogspot.com/-YZ8eabNTeI0/VmOay_v1zlI/AAAAAAAAOW4/05yONZ7wws4/s320/malinowski.png\" width=\"320\" /></a></div><br />\"We propose a novel approach based on recurrent neural networks for the challenging task of answering of questions about images. It combines a CNN with a LSTM into an end-to-end architecture that predict answers conditioning on a question and an image.\"<br /><br /><br /><br /><br />2.&nbsp;<a href=\"http://arxiv.org/pdf/1506.06724v1.pdf\">Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books</a>&nbsp;Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, Sanja Fidler<br /><br /><br /><a href=\"http://2.bp.blogspot.com/-mamWdhY2KL4/VmOcUrBCZfI/AAAAAAAAOXE/H1tJf7cIYE0/s1600/torralba.png\" imageanchor=\"1\" style=\"clear: left; float: left; margin-bottom: 1em; margin-right: 1em;\"><img border=\"0\" height=\"196\" src=\"https://2.bp.blogspot.com/-mamWdhY2KL4/VmOcUrBCZfI/AAAAAAAAOXE/H1tJf7cIYE0/s320/torralba.png\" width=\"320\" /></a><br />\"To align movies and books we exploit a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book.\"<br /><br /><br /><br /><br /><br /><br /><br />3.&nbsp;<a href=\"http://arxiv.org/pdf/1505.01596.pdf\">Learning to See by Moving</a> Pulkit Agrawal, Joao Carreira, Jitendra Malik<br /><a href=\"http://2.bp.blogspot.com/-b_tUpJAOD2c/VmOdcXy74DI/AAAAAAAAOXQ/YlH43tryOYY/s1600/pulkit.png\" imageanchor=\"1\" style=\"clear: right; float: right; margin-bottom: 1em; margin-left: 1em;\"><img border=\"0\" height=\"196\" src=\"https://2.bp.blogspot.com/-b_tUpJAOD2c/VmOdcXy74DI/AAAAAAAAOXQ/YlH43tryOYY/s320/pulkit.png\" width=\"320\" /></a><br /><br />\"We show that using the same number of training images, features learnt using egomotion as supervision compare favourably to features learnt using class-label as supervision on the tasks of scene recognition, object recognition, visual odometry and keypoint matching.\"<br /><br /><br /><br /><br /><br /><br /><br />4.&nbsp;<a href=\"https://hal.inria.fr/hal-01207966/document\">Local Convolutional Features With Unsupervised Training for Image Retrieval </a>Mattis Paulin, Matthijs Douze, Zaid Harchaoui, Julien Mairal, Florent Perronin, Cordelia Schmid<br /><br /><a href=\"http://3.bp.blogspot.com/-EhW5uNu3grc/VmOeApfDUoI/AAAAAAAAOXc/rREG2irhh3w/s1600/paulin.png\" imageanchor=\"1\" style=\"clear: left; float: left; margin-bottom: 1em; margin-right: 1em;\"><img border=\"0\" height=\"152\" src=\"https://3.bp.blogspot.com/-EhW5uNu3grc/VmOeApfDUoI/AAAAAAAAOXc/rREG2irhh3w/s320/paulin.png\" width=\"320\" /></a><br /><br />\"We introduce a deep convolutional architecture that yields patch-level descriptors, as an alternative to the popular SIFT descriptor for image retrieval.\"<br /><br /><br /><br /><br /><br /><br />5.&nbsp;<a href=\"http://arxiv.org/pdf/1507.08905v4.pdf\">Deep Networks for Image Super-Resolution With Sparse Prior</a> Zhaowen Wang, Ding Liu, Jianchao Yang, Wei Han, Thomas Huang<br /><br /><a href=\"http://3.bp.blogspot.com/-zi-b1m717LU/VmOfwOVlYjI/AAAAAAAAOXo/hojgfeeMRQ4/s1600/huang.png\" imageanchor=\"1\" style=\"clear: right; float: right; margin-bottom: 1em; margin-left: 1em;\"><img border=\"0\" height=\"100\" src=\"https://3.bp.blogspot.com/-zi-b1m717LU/VmOfwOVlYjI/AAAAAAAAOXo/hojgfeeMRQ4/s320/huang.png\" width=\"320\" /></a><br /><br />\"We show that a sparse coding model particularly designed for super-resolution can be incarnated as a neural network, and trained in a cascaded structure from end to end.\"<br /><br /><br /><br />6.&nbsp;<a href=\"http://arxiv.org/pdf/1504.06201v3.pdf\">High-for-Low and Low-for-High: Efficient Boundary Detection From Deep Object Features and its Applications to High-Level Vision</a> Gedas Bertasius, Jianbo Shi, Lorenzo Torresani<br /><br /><a href=\"http://1.bp.blogspot.com/-nfEiHlEE2r4/VmOghOhDpBI/AAAAAAAAOX0/xu6q_xwUalE/s1600/lorenzo.png\" imageanchor=\"1\" style=\"clear: left; float: left; margin-bottom: 1em; margin-right: 1em;\"><img border=\"0\" height=\"306\" src=\"https://1.bp.blogspot.com/-nfEiHlEE2r4/VmOghOhDpBI/AAAAAAAAOX0/xu6q_xwUalE/s320/lorenzo.png\" width=\"320\" /></a><br /><br />\"In this work we show how to predict boundaries by exploiting object level features from a pretrained object-classification network.\"<br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />7. A<a href=\"http://research.baidu.com/wp-content/uploads/2015/11/A-Deep-Visual-Correspondence-Embedding-Model-for-Stereo-Matching-Costs.pdf\"> Deep Visual Correspondence Embedding Model for Stereo Matching Costs</a> Zhuoyuan Chen, Xun Sun, Liang Wang, Yinan Yu, Chang Huang<br /><br /><a href=\"http://1.bp.blogspot.com/-ZymsUgepamg/VmOhORou1TI/AAAAAAAAOYA/4p_AK2hh35Q/s1600/baidu.png\" imageanchor=\"1\" style=\"clear: right; float: right; margin-bottom: 1em; margin-left: 1em;\"><img border=\"0\" height=\"190\" src=\"https://1.bp.blogspot.com/-ZymsUgepamg/VmOhORou1TI/AAAAAAAAOYA/4p_AK2hh35Q/s320/baidu.png\" width=\"320\" /></a><br /><br />\"A novel deep visual correspondence embedding model is trained via Convolutional Neural Network on a large set of stereo images with ground truth disparities. This deep embedding model leverages appearance data to learn visual similarity relationships between corresponding image patches, and explicitly maps intensity values into an embedding feature space to measure pixel dissimilarities.\"<br /><br /><br /><br /><br /><br />8.&nbsp;<a href=\"http://www.cs.ubc.ca/~murphyk/Papers/im2calories_iccv15.pdf\">Im2Calories: Towards an Automated Mobile Vision Food Diary</a> Austin Meyers, Nick Johnston, Vivek Rathod, Anoop Korattikara, Alex Gorban, Nathan Silberman, Sergio Guadarrama, George Papandreou, Jonathan Huang, Kevin P. Murphy<br /><br /><a href=\"http://1.bp.blogspot.com/-EE90tdIWdSk/VmOiWNe32DI/AAAAAAAAOYM/Ulu9qXokp8Y/s1600/im2calories.png\" imageanchor=\"1\" style=\"clear: left; float: left; margin-bottom: 1em; margin-right: 1em;\"><img border=\"0\" height=\"194\" src=\"https://1.bp.blogspot.com/-EE90tdIWdSk/VmOiWNe32DI/AAAAAAAAOYM/Ulu9qXokp8Y/s320/im2calories.png\" width=\"320\" /></a><br /><br />\"We present a system which can recognize the contents of your meal from a single image, and then predict its nutritional contents, such as calories.\"<br /><br /><br /><br /><br /><br /><br /><br /><br /><br />9.&nbsp;<a href=\"http://arxiv.org/pdf/1505.05192v2.pdf\">Unsupervised Visual Representation Learning by Context Prediction</a> Carl Doersch, Abhinav Gupta, Alexei A. Efros<br /><br /><a href=\"http://2.bp.blogspot.com/-9ew7pwXd6x4/VmOkS9xZaCI/AAAAAAAAOYY/tuL9fS-pP-I/s1600/carl.png\" imageanchor=\"1\" style=\"clear: right; float: right; margin-bottom: 1em; margin-left: 1em;\"><img border=\"0\" height=\"300\" src=\"https://2.bp.blogspot.com/-9ew7pwXd6x4/VmOkS9xZaCI/AAAAAAAAOYY/tuL9fS-pP-I/s320/carl.png\" width=\"320\" /></a><br /><br />\"How can one write an objective function to encourage a representation to capture, for example, objects, if none of the objects are labeled?\"<br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />10.&nbsp;<a href=\"http://research.microsoft.com/pubs/255952/ICCV15_DeepNDF_main.pdf\">Deep Neural Decision Forests</a> Peter Kontschieder, Madalina Fiterau, Antonio Criminisi, Samuel Rota Bulò<br /><br /><a href=\"http://1.bp.blogspot.com/-tnHOhE9j0gM/VmOmpqBdWUI/AAAAAAAAOYk/QY3rsHYeQRs/s1600/neural_forests.png\" imageanchor=\"1\" style=\"clear: left; float: left; margin-bottom: 1em; margin-right: 1em;\"><img border=\"0\" height=\"137\" src=\"https://1.bp.blogspot.com/-tnHOhE9j0gM/VmOmpqBdWUI/AAAAAAAAOYk/QY3rsHYeQRs/s320/neural_forests.png\" width=\"320\" /></a><br /><br />\"We introduce a stochastic and differentiable decision tree model, which steers the representation learning usually conducted in the initial layers of a (deep) convolutional network.\"<br /><br /><br /><br /><br /><br /><br />11.&nbsp;<a href=\"http://www.robots.ox.ac.uk/~szheng/papers/CRFasRNN.pdf\">Conditional Random Fields as Recurrent Neural Networks</a> Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang Huang, Philip H. S. Torr<br /><br /><a href=\"http://4.bp.blogspot.com/-akVEQ-3jL5I/VmOncOQeG4I/AAAAAAAAOYw/VLQ9QQShjL4/s1600/crfrnn.png\" imageanchor=\"1\" style=\"clear: right; float: right; margin-bottom: 1em; margin-left: 1em;\"><img border=\"0\" height=\"134\" src=\"https://4.bp.blogspot.com/-akVEQ-3jL5I/VmOncOQeG4I/AAAAAAAAOYw/VLQ9QQShjL4/s320/crfrnn.png\" width=\"320\" /></a><br /><br />\"We formulate mean-field approximate inference for the Conditional Random Fields with Gaussian pairwise potentials as Recurrent Neural Networks.\"<br /><br /><br /><br /><br /><br /><br />12.&nbsp;<a href=\"https://www.robots.ox.ac.uk/~vgg/publications/2015/Pfister15a/pfister15a.pdf\">Flowing ConvNets for Human Pose Estimation in Videos</a> Tomas Pfister, James Charles, Andrew Zisserman<br /><br /><a href=\"http://3.bp.blogspot.com/-sx8eJ1gyB_w/VmOo2N9548I/AAAAAAAAOY8/YT2zl9pSQgw/s1600/pose.png\" imageanchor=\"1\" style=\"clear: left; float: left; margin-bottom: 1em; margin-right: 1em;\"><img border=\"0\" height=\"128\" src=\"https://3.bp.blogspot.com/-sx8eJ1gyB_w/VmOo2N9548I/AAAAAAAAOY8/YT2zl9pSQgw/s320/pose.png\" width=\"320\" /></a><br /><br />\"We investigate a ConvNet architecture that is able to benefit from temporal context by combining information across the multiple frames using optical flow.\"<br /><br /><br /><br /><br /><br />13.&nbsp;<a href=\"http://arxiv.org/pdf/1505.00295v1.pdf\">Dense Optical Flow Prediction From a Static Image</a> Jacob Walker, Abhinav Gupta, Martial Hebert<br /><br /><br /><a href=\"http://2.bp.blogspot.com/-Dpb7ON1dnqU/VmOqG3qZ_GI/AAAAAAAAOZI/n8uOPdCUCjY/s1600/walker.png\" imageanchor=\"1\" style=\"clear: right; float: right; margin-bottom: 1em; margin-left: 1em;\"><img border=\"0\" height=\"120\" src=\"https://2.bp.blogspot.com/-Dpb7ON1dnqU/VmOqG3qZ_GI/AAAAAAAAOZI/n8uOPdCUCjY/s320/walker.png\" width=\"320\" /></a><br />\"Given a static image, P-CNN predicts the future motion of each and every pixel in the image in terms of optical flow. Our P-CNN model leverages the data in tens of thousands of realistic videos to train our model. Our method relies on absolutely no human labeling and is able to predict motion based on the context of the scene.\"<br /><br /><br />14.&nbsp;<a href=\"http://www.cs.berkeley.edu/~wckuo/KuoICCV2015.pdf\">DeepBox: Learning Objectness With Convolutional Networks</a> Weicheng Kuo, Bharath Hariharan, Jitendra Malik<br /><br /><a href=\"http://2.bp.blogspot.com/-9jYNKfnL9cY/VmOrcQz_P6I/AAAAAAAAOZU/bXAirMjpnwc/s1600/deepbox.png\" imageanchor=\"1\" style=\"clear: left; float: left; margin-bottom: 1em; margin-right: 1em;\"><img border=\"0\" height=\"179\" src=\"https://2.bp.blogspot.com/-9jYNKfnL9cY/VmOrcQz_P6I/AAAAAAAAOZU/bXAirMjpnwc/s320/deepbox.png\" width=\"320\" /></a><br /><br />\"Our framework, which we call DeepBox, uses convolutional neural networks (CNNs) to rerank proposals from a bottom-up method.\"<br /><br /><br /><br /><br /><br /><br /><br /><br />15.&nbsp;<a href=\"http://web.engr.illinois.edu/~slazebni/publications/iccv15_active.pdf\">Active Object Localization With Deep Reinforcement Learning</a> Juan C. Caicedo, Svetlana Lazebnik<br /><br /><a href=\"http://2.bp.blogspot.com/--aE0740Mvxs/VmOsREiQ0GI/AAAAAAAAOZg/F1AL0yUMQ68/s1600/reinf.png\" imageanchor=\"1\" style=\"clear: right; float: right; margin-bottom: 1em; margin-left: 1em;\"><img border=\"0\" height=\"122\" src=\"https://2.bp.blogspot.com/--aE0740Mvxs/VmOsREiQ0GI/AAAAAAAAOZg/F1AL0yUMQ68/s320/reinf.png\" width=\"320\" /></a><br /><br />\"This agent learns to deform a bounding box using simple transformation actions, with the goal of determining the most specific location of target objects following top-down reasoning.\"<br /><br /><br /><br /><br /><br />16.&nbsp;<a href=\"http://arxiv.org/pdf/1411.4734v3.pdf\">Predicting Depth, Surface Normals and Semantic Labels With a Common Multi-Scale Convolutional Architecture</a> David Eigen, Rob Fergus<br /><br /><a href=\"http://2.bp.blogspot.com/-O7D9kuYg1Po/VmebD1TBFdI/AAAAAAAAOZ0/r_GAL23Hx9U/s1600/eigen.png\" imageanchor=\"1\" style=\"clear: left; float: left; margin-bottom: 1em; margin-right: 1em;\"><img border=\"0\" height=\"320\" src=\"https://2.bp.blogspot.com/-O7D9kuYg1Po/VmebD1TBFdI/AAAAAAAAOZ0/r_GAL23Hx9U/s320/eigen.png\" width=\"288\" /></a><br /><br />\"We address three different computer vision tasks using a single multiscale convolutional network architecture: depth prediction, surface normal estimation, and semantic labeling.\"<br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />17.&nbsp;<a href=\"https://dl.dropboxusercontent.com/u/44884434/2015-HDCNN/hdcnn-iccv15-CD.pdf\">HD-CNN: Hierarchical Deep Convolutional Neural Networks for Large Scale Visual Recognition</a> Zhicheng Yan, Hao Zhang, Robinson Piramuthu, Vignesh Jagadeesh, Dennis DeCoste, Wei Di, Yizhou Yu<br /><br /><a href=\"http://1.bp.blogspot.com/-71RqjVid3w4/VmebjSYyNPI/AAAAAAAAOaA/Cp5K9QKZ0So/s1600/hdcnn_imagenet_vgg16layer.png\" imageanchor=\"1\" style=\"clear: right; float: right; margin-bottom: 1em; margin-left: 1em;\"><img border=\"0\" height=\"164\" src=\"https://1.bp.blogspot.com/-71RqjVid3w4/VmebjSYyNPI/AAAAAAAAOaA/Cp5K9QKZ0So/s320/hdcnn_imagenet_vgg16layer.png\" width=\"320\" /></a><br /><br />\"We introduce hierarchical deep CNNs (HD-CNNs) by embedding deep CNNs into a category hierarchy. An HD-CNN separates easy classes using a coarse category classifier while distinguishing difficult classes using fine category classifiers.\"<br /><br /><br /><br /><br /><br />18.&nbsp;<a href=\"http://lmb.informatik.uni-freiburg.de/Publications/2015/DFIB15/flownet.pdf\">FlowNet: Learning Optical Flow With Convolutional Networks</a> Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Häusser, Caner Hazırbaş, Vladimir Golkov, Patrick van der Smagt, Daniel Cremers, Thomas Brox<br /><br /><a href=\"http://3.bp.blogspot.com/-bSwoFVVrptI/VmecdGKlBvI/AAAAAAAAOaM/RH8llRLM9YA/s1600/flownet.png\" imageanchor=\"1\" style=\"clear: left; float: left; margin-bottom: 1em; margin-right: 1em;\"><img border=\"0\" height=\"168\" src=\"https://3.bp.blogspot.com/-bSwoFVVrptI/VmecdGKlBvI/AAAAAAAAOaM/RH8llRLM9YA/s320/flownet.png\" width=\"320\" /></a><br /><br />\"We construct appropriate CNNs which are capable of solving the optical flow estimation problem as a supervised learning task.\"<br /><br /><br /><br /><br /><br /><br /><br />19.&nbsp;<a href=\"http://imagine.enpc.fr/~aubrym/projects/features_analysis/texts/understanding_deep_features_with_CG.pdf\">Understanding Deep Features With Computer-Generated Imagery</a> Mathieu Aubry, Bryan C. Russell<br /><br /><a href=\"http://4.bp.blogspot.com/-B-WqC6T614c/VmedW-WnQ2I/AAAAAAAAOaY/JjXaFysD4Cw/s1600/russell.png\" imageanchor=\"1\" style=\"clear: right; float: right; margin-bottom: 1em; margin-left: 1em;\"><img border=\"0\" height=\"116\" src=\"https://4.bp.blogspot.com/-B-WqC6T614c/VmedW-WnQ2I/AAAAAAAAOaY/JjXaFysD4Cw/s320/russell.png\" width=\"320\" /></a><br />\"Rendered images are presented to a trained CNN and responses for different layers are studied with respect to the input scene factors.\"<br /><br /><br /><br /><br /><br /><br /><br />20.&nbsp;<a href=\"http://arxiv.org/pdf/1505.07427v3.pdf\">PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization</a> Alex Kendall, Matthew Grimes, Roberto Cipolla<br /><br /><a href=\"http://4.bp.blogspot.com/-NiHeDknbdOA/VmedusLsKAI/AAAAAAAAOak/6X_qO6P196g/s1600/posenet.png\" imageanchor=\"1\" style=\"clear: left; float: left; margin-bottom: 1em; margin-right: 1em;\"><img border=\"0\" height=\"134\" src=\"https://4.bp.blogspot.com/-NiHeDknbdOA/VmedusLsKAI/AAAAAAAAOak/6X_qO6P196g/s320/posenet.png\" width=\"320\" /></a><br /><br />\"Our system trains a convolutional neural network to regress the 6-DOF camera pose from a single RGB image in an end-to-end manner with no need of additional engineering or graph optimisation.\"<br /><br /><br /><br /><br /><br />21.&nbsp;<a href=\"http://scott89.github.io/FCNT/\">Visual Tracking With Fully Convolutional Networks</a> Lijun Wang, Wanli Ouyang, Xiaogang Wang, Huchuan Lu<br /><br /><a href=\"http://2.bp.blogspot.com/-fQD2V_1EUqM/VmeeG1BNZkI/AAAAAAAAOaw/NcDLLrViaE8/s1600/fcnt.png\" imageanchor=\"1\" style=\"clear: right; float: right; margin-bottom: 1em; margin-left: 1em;\"><img border=\"0\" height=\"90\" src=\"https://2.bp.blogspot.com/-fQD2V_1EUqM/VmeeG1BNZkI/AAAAAAAAOaw/NcDLLrViaE8/s320/fcnt.png\" width=\"320\" /></a><br /><br /><br />\"A new approach for general object tracking with fully convolutional neural network.\"<br /><br /><br /><br /><h3>Conclusion</h3>While some can argue that the great convergence upon ConvNets is making the field less diverse, it is actually making the techniques easier to comprehend. It is easier to \"borrow breakthrough thinking\" from one research direction when the core computations are cast in the language of ConvNets. Using ConvNets, properly trained (and motivated!) 21 year old graduate student are actually able to compete on benchmarks, where previously it would take an entire 6-year PhD cycle to compete on a non-trivial benchmark.<br /><br />See you next week in Chile!<br /><h3><b><br /></b><b>Update (January 13th, 2016)</b></h3><div style=\"background-color: white; color: #333333; font-size: 18px; line-height: 23px; margin-bottom: 11.5px;\"><span style=\"font-family: &quot;times&quot; , &quot;times new roman&quot; , serif;\">The following awards were given at ICCV 2015.</span></div><h3 style=\"background-color: white; color: #333333; line-height: 34.5px; margin: 0px; text-rendering: optimizeLegibility;\"><span style=\"font-family: &quot;times&quot; , &quot;times new roman&quot; , serif; font-size: small;\">Achievement awards</span></h3><ul style=\"background-color: white; color: #333333; line-height: 23px; margin: 0px 0px 11.5px 25px; padding: 0px;\"><li><span style=\"font-family: &quot;times&quot; , &quot;times new roman&quot; , serif;\">PAMI Distinguished Researcher Award (1):&nbsp;<strong>Yann LeCun</strong></span></li><li><span style=\"font-family: &quot;times&quot; , &quot;times new roman&quot; , serif;\">PAMI Distinguished Researcher Award (2):&nbsp;<strong>David Lowe</strong></span></li><li><span style=\"font-family: &quot;times&quot; , &quot;times new roman&quot; , serif;\">PAMI Everingham Prize Winner (1):&nbsp;<strong>Andrea Vedaldi</strong>&nbsp;for&nbsp;<a href=\"http://www.vlfeat.org/\" style=\"color: #0088cc; text-decoration: none;\">VLFeat</a></span></li><li><span style=\"font-family: &quot;times&quot; , &quot;times new roman&quot; , serif;\">PAMI Everingham Prize Winner (2):&nbsp;<strong>Daniel Scharstein</strong>&nbsp;and&nbsp;<strong>Rick Szeliski&nbsp;</strong>for the&nbsp;<a href=\"http://vision/middlebury.edu/stereo/data/\" style=\"color: #0088cc; text-decoration: none;\">Middlebury Datasets</a></span></li></ul><h2 style=\"background-color: white; color: #333333; line-height: 46px; margin: 0px; text-rendering: optimizeLegibility;\"><span style=\"font-family: &quot;times&quot; , &quot;times new roman&quot; , serif; font-size: small;\">Paper awards</span></h2><ul style=\"background-color: white; color: #333333; line-height: 23px; margin: 0px 0px 11.5px 25px; padding: 0px;\"><li><span style=\"font-family: &quot;times&quot; , &quot;times new roman&quot; , serif;\">PAMI Helmholtz Prize (1):&nbsp;<strong>David Martin</strong>,&nbsp;<strong>Charles Fowlkes</strong>,&nbsp;<strong>Doron Tal</strong>, and&nbsp;<strong>Jitendra Malik</strong>&nbsp;for their ICCV 2001 paper \"A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics\".</span></li><li><span style=\"font-family: &quot;times&quot; , &quot;times new roman&quot; , serif;\">PAMI Helmholtz Prize (2):&nbsp;<strong>Serge Belongie</strong>,&nbsp;<strong>Jitendra Malik</strong>, and&nbsp;<strong>Jan Puzicha</strong>, for their ICCV 2001 paper \"Matching Shapes\".</span></li><li><span style=\"font-family: &quot;times&quot; , &quot;times new roman&quot; , serif;\">Marr Prize:&nbsp;<strong>Peter Kontschieder</strong>,&nbsp;<strong>Madalina Fiterau</strong>,&nbsp;<strong>Antonio Criminisi</strong>, and&nbsp;<strong>Samual Rota Bulo</strong>, for&nbsp;<a href=\"http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Kontschieder_Deep_Neural_Decision_ICCV_2015_paper.pdf\" style=\"color: #0088cc; text-decoration: none;\">\"Deep Neural Decision Forests\"</a>.</span></li><li><span style=\"font-family: &quot;times&quot; , &quot;times new roman&quot; , serif;\">Marr Prize honorable mention:&nbsp;<strong>Saining Xie</strong>&nbsp;and&nbsp;<strong>Zhuowen Tu</strong>&nbsp;for<a href=\"http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Xie_Holistically-Nested_Edge_Detection_ICCV_2015_paper.pdf\" style=\"color: #0088cc; text-decoration: none;\">\"Holistically-Nested Edge Detection\"</a>.</span></li></ul><div><span style=\"color: #333333; font-family: &quot;times&quot; , &quot;times new roman&quot; , serif;\"><span style=\"line-height: 23px;\">For more information about awards, see <a href=\"http://www.nowozin.net/sebastian/blog/iccv-2015-day-2.html\">Sebastian Nowozin's ICCV-day-2 blog post</a>.</span></span></div><div><span style=\"color: #333333; font-family: &quot;times&quot; , &quot;times new roman&quot; , serif;\"><span style=\"line-height: 23px;\"><br /></span></span></div>I also wrote another ICCV-related blog post (January 13, 2016) about the <a href=\"http://www.computervisionblog.com/2016/01/why-slam-matters-future-of-real-time.html\">Future of Real-Time SLAM</a>.",
  "link": [
    "",
    "",
    "",
    "",
    ""
  ],
  "author": {
    "name": "Tomasz Malisiewicz",
    "uri": "http://www.blogger.com/profile/17507234774392358321",
    "email": "noreply@blogger.com",
    "gd:image": ""
  },
  "media:thumbnail": "",
  "thr:total": 7
}