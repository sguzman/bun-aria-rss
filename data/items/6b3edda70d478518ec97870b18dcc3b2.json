{
  "title": "What is Scalable Machine Learning?",
  "link": "",
  "updated": "2014-07-02T17:38:00+02:00",
  "published": "2014-07-02T17:38:00+02:00",
  "author": {
    "name": "Mikio L. Braun",
    "uri": "http://mikiobraun.de",
    "email": "mikiobraun@gmail.com"
  },
  "id": "http://blog.mikiobraun.de/2014/07/what-is-scalable-machine-learning",
  "content": "<p><p>Scalability has become one of those core concept slash buzzwords of Big Data.\nIt’s all about scaling out, web scale, and so on. In principle, the idea is to\nbe able to take one piece of code and then throw any number of computers at\nit to make it fast.</p>\n\n<p>The terms “scalable” and “large scale” have been used in machine\nlearning circles long before there was Big Data. There had always been\ncertain problems which lead to a large amount of data, for example in\nbioinformatics, or when dealing with large number of text documents. So\nfinding learning algorithms, or more generally data analysis algorithms which\ncan deal with a very large set of data was always a relevant question.</p>\n\n<p>Interestingly, this issue of scalability were seldom solved using actual\nscaling in  in machine learning, at least not in the Big Data kind of sense.\nPart of the reason is certainly that multicore processors didn’t yet exist at\nthe scale they do today and that the idea of “just scaling out” wasn’t as\npervasive as it is today.</p>\n\n<p>Instead, “scalable” machine learning is almost always based on finding more\nefficient algorithms, and most often, approximations to the original algorithm\nwhich can be computed much more efficiently.</p>\n\n<p>To illustrate this, let’s search for NIPS papers (the annual Advances in\nNeural Information Processing Systems, short NIPS, conference is one of the\nbig ML community meetings) for <a href=\"http://papers.nips.cc/search/?q=scalable\">papers which have the term\n“scalable”</a> in the title.</p>\n\n<p>Here are some examples:</p>\n\n<ul>\n  <li>\n    <p><strong>Scalable Inference for Logistic-Normal Topic Models</strong></p>\n\n    <p><em>… This paper presents a partially collapsed Gibbs sampling algorithm\nthat approaches the provably correct distribution by exploring the\nideas of data augmentation …</em></p>\n\n    <p>Partially collapsed Gibbs sampling is a kind of estimation algorithm for\ncertain graphical models.</p>\n  </li>\n  <li>\n    <p><strong>A Scalable Approach to Probabilistic Latent Space Inference of\n Large-Scale Networks</strong></p>\n\n    <p><em>… With […] an efficient stochastic variational inference algorithm, we\n are able to analyze real networks with over a million vertices […] on a\n single machine in a matter of hours …</em></p>\n\n    <p>Stochastic variational inference algorithm is both an approximation and an\n estimation algorithm.</p>\n  </li>\n  <li>\n    <p><strong>Scalable kernels for graphs with continuous attributes</strong></p>\n\n    <p><em>… In this paper, we present a class of path kernels with computational\ncomplexity $O(n^2(m + \\delta^2 ))$ …</em></p>\n\n    <p>And this algorithm has squared runtime in the number of data points, so wouldn’t even scale out well even if you could.</p>\n  </li>\n</ul>\n\n<p>Usually, even if there is potential for scalability, it usually something\nthat is “embarassingly parallel” (yep, that’s a technical term), meaning that\nit’s something like a summation which can be parallelized very easily. Still, the actual “scalability” comes from the algorithmic side.</p>\n\n<p>So how do scalable ML algorithms look like? A typical example are the\n<a href=\"http://leon.bottou.org/research/stochastic\">stochastic gradient descent</a> (SGD) class of algorithms. These algorithms can\nbe used, for example, to train classifiers like linear SVMs or logistic\nregression. One data point is considered at each iteration. The prediction\nerror on that point is computed and then the gradient is taken with respect to\nthe model parameters, giving information about how to adapt these parameters\nslightly to make the error smaller.</p>\n\n<p><a href=\"http://github.com/JohnLangford/vowpal_wabbit/wiki\">Vowpal Wabbit</a> is one program based on this approach and it has a nice\ndefinition of what it considers to mean scalable in machine learning:</p>\n\n<blockquote>\n  <p>There are two ways to have a fast learning algorithm: (a) start\nwith a slow algorithm and speed it up, or (b) build an\nintrinsically fast learning algorithm. This project is about\napproach (b), and it’s reached a state where it may be useful to\nothers as a platform for research and experimentation.</p>\n</blockquote>\n\n<p>So “scalable” means having a learning algorithm which can deal with any amount\nof data, without consuming ever growing amounts of resources like memory. For\nSGD type algorithms this is the case, because all you need to store are the\nmodel parameters, usually a few ten to hundred thousand double precision\nfloating point value, so maybe a few megabytes in total. The main problem to\nspeed this kind of computation up is how to stream the data by fast enough.</p>\n\n<div class=\"figure\">\n<img src=\"/images/sml-sgd.png\" />\n</div>\n\n<p>To put it differently, not only does this kind of scalability not rely on\nscaling out, it’s actually not even necessary or possible to scale the\ncomputation out because the main state of the computation easily fits into\nmain memory and computations on it cannot be distributed easily.</p>\n\n<p>I know that gradient descent is often taken as an example for map reduce and\nother approaches like in <a href=\"http://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf\">this paper on the architecture of Spark</a>, but\nthat paper discusses a version of gradient descent where you are not taking\none point at a time, but aggregate the gradient information for the whole data\nset before making the update to the model parameters. While this can be easily\nparallelized, it does not perform well in practice because the gradient\ninformation tends to average out when computed over the whole data set.</p>\n\n<p>If you want to know more, this <a href=\"http://largescale.ml.tu-berlin.de/about/\">large scale learning challenge</a>\nSören Sonnneburg organized in 2008 still has valuable information on how to\ndeal with massive data sets.</p>\n\n<p>Of course, there are things which can be easily scaled well using Hadoop or\nSpark, in particular any kind of data preprocessing or feature extraction\nwhere you need to apply the same operation to each data point in your data\nset. Another area where parallelization is easy and useful is when you are\nusing cross validation to do model selection where you usually have to train a\nlarge number of models for different parameter sets to find the combination\nwhich performs best. Again, even here there is more potential for even\nspeeding up such computations using better algorithms like in <a href=\"http://arxiv.org/abs/1206.2248\">this paper of\nmine</a>.</p>\n\n<p>I’ve just scratched the surface of this, but I hope you got the idea that\nscalability can mean quite different things. In Big Data (meaning the\ninfrastructure side of it) what you want to compute is pretty well defined,\nfor example some kind of aggregate over your data set, so you’re left with the\nquestion of how to parallelize that computation well. In machine learning, you\nhave much more freedom because data is noisy and there’s always some freedom\nin how you model your data, so you can often get away with computing some\nvariation of what you originally wanted to do and still perform well. Often,\nthis allows you to speed up your computations significantly by decoupling\ncomputations. Parallelization is important, too, but alone it won’t get you\nvery far.</p>\n\n<p>Luckily, there are projects like Spark and Stratosphere/Flink which work on\nproviding more useful abstractions beyond map and reduce to make the last part\neasier for data scientists, but you won’t get rid of the algorithmic design\npart any time soon.</p>\n\n</p>\n   <p><a href=\"http://blog.mikiobraun.de/2014/07/what-is-scalable-machine-learning.html\">Click here for the full article</a>"
}