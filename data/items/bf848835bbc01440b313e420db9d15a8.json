{
  "title": "Neural Networks and the generalisation problem",
  "link": "https://aimatters.wordpress.com/2018/01/28/neural-networks-and-the-generalisation-problem/",
  "comments": "https://aimatters.wordpress.com/2018/01/28/neural-networks-and-the-generalisation-problem/#respond",
  "dc:creator": "Stephen Oman",
  "pubDate": "Sun, 28 Jan 2018 22:02:50 +0000",
  "category": [
    "Artificial Intelligence",
    "Examples",
    "Machine Learning"
  ],
  "guid": "http://aimatters.wordpress.com/?p=1185",
  "description": "Over the last few weeks, a robust debate has been taking place online about the prospects that Deep Learning neural networks would lead to advances in the quest for Artificial General Intelligence. All current AI is what is known as Artificial Narrow Intelligence. This means that the models work well (sometimes extremely well) on specific [&#8230;]",
  "content:encoded": "<p>Over the last few weeks, a robust debate has been taking place online about the prospects that Deep Learning neural networks would lead to advances in the quest for Artificial General Intelligence.</p>\n<p>All current AI is what is known as Artificial Narrow Intelligence. This means that the models work well (sometimes extremely well) on specific problems that are well defined. Unfortunately, they are also quite brittle and do not generalise to other problems, or even variants of the problem they are trained on. By contrast, a long-term goal of the field is to get to AIs that can generalise and extrapolate, amongst other things. This is called Artificial General Intelligence.</p>\n<p>The debate started back in <a href=\"https://aimatters.wordpress.com/2017/09/17/deep-learning-dead-end/\" target=\"_blank\" rel=\"noopener\">September when Geoffrey Hinton</a> proposed that researchers should start looking at alternatives to the default back propagation algorithms that are currently quite successful. This was followed up by a more detailed critical review published by <a href=\"https://arxiv.org/abs/1801.00631\" target=\"_blank\" rel=\"noopener\">Gary Marcus earlier this month</a> outlining many of the problems with neural networks and deep learning. There has been quite a bit of debate about the merits of Marcus&#8217; points on social media, so much so that he published a <a href=\"https://medium.com/@GaryMarcus/in-defense-of-skepticism-about-deep-learning-6e8bfd5ae0f1\" target=\"_blank\" rel=\"noopener\">defence on Medium</a>, responding to the various criticisms raised.</p>\n<p>One of the most serious points is that artificial neural networks don&#8217;t generalise and cannot extrapolate from what they have been trained on to new instances with different characteristics. This is quite simple to <a href=\"https://github.com/StephenOman/TensorFlowExamples/blob/master/Reverse%20Binary%20Number%20Simple%20NN.ipynb\" target=\"_blank\" rel=\"noopener\">demonstrate</a> using Keras and TensorFlow. In this example, we have a neural network that tries to learn to reverse the digits in a binary number:</p>\n<pre>import numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.optimizers import SGD\n\nx = np.array([[0,0,0,0], [0,0,0,1], [0,0,1,0],\n      [0,1,0,1], [0,1,1,0], [0,1,1,1]])\ny = np.array([[0,0,0,0], [1,0,0,0], [0,1,0,0],\n      [1,0,1,0], [0,1,1,0], [1,1,1,0]])\n\nmodel = Sequential()\nmodel.add(Dense(4, input_shape=(4,)))\nmodel.add(Activation('sigmoid'))\nmodel.add(Dense(4))\nmodel.add(Activation('sigmoid'))\nmodel.compile(loss='mean_squared_error', optimizer=SGD(lr=0.1))\n\nmodel.fit(x,y, epochs=10000, batch_size=8, verbose=0)</pre>\n<p>The above model  trains the network on examples where the first digit is always 0. If we test the trained network, it will perform as expected, reversing the order of the digits:</p>\n<pre>np.around(model.predict(x))</pre>\n<p>generates the following output:</p>\n<pre>array([[0., 0., 0., 0.],\n   [1., 0., 0., 0.],\n   [0., 1., 0., 0.],\n   [1., 0., 1., 0.],\n   [0., 1., 1., 0.],\n   [1., 1., 1., 0.]], dtype=float32)</pre>\n<p>Which is exactly what we want. However, when we change the inputs to begin with &#8216;1&#8217; then the network gets confused:</p>\n<pre>offdim = np.array([[1,0,0,0], [1,0,0,1], [1,0,1,0],\n  [1,1,0,1], [1,1,1,0], [1,1,1,1]])\nnp.around(model.predict(offdim))</pre>\n<p>as we can see in the following output:</p>\n<pre>array([[0., 0., 0., 0.],\n   [1., 0., 0., 0.],\n   [0., 1., 0., 0.],\n   [1., 0., 1., 0.],\n   [0., 1., 1., 0.],\n   [1., 1., 1., 0.]], dtype=float32)</pre>\n<p>Clearly the network isn&#8217;t learning anything about the concept of &#8220;reversing&#8221; even though it looks like it initially. It has completely ignored the new number.</p>\n<p>This lack of generalising demonstrates clearly that neural networks, as currently architected, don&#8217;t operate on any level of abstraction. This is one of the fundamental problems with neural networks that must be solved if we have any hope of them advancing our efforts towards Artificial General Intelligence.</p>\n<p>As Marcus points out in his defence article, there may be a need for a blend of techniques, including old-school symbolic AI to help deep learning networks move forward towards solving the generalisation problem.</p>\n<p>Sources:</p>\n<p>Deep Learning, A Critical Appraisal &#8211; Gary Marcus <a href=\"https://arxiv.org/abs/1801.00631\">https://arxiv.org/abs/1801.00631</a></p>\n<p>In defense of skepticism about deep learning &#8211; Gary Marcus <a href=\"https://medium.com/@GaryMarcus/in-defense-of-skepticism-about-deep-learning-6e8bfd5ae0f1\">https://medium.com/@GaryMarcus/in-defense-of-skepticism-about-deep-learning-6e8bfd5ae0f1</a></p>\n<p>Sample code on Github: <a href=\"https://github.com/StephenOman/TensorFlowExamples/blob/master/Reverse%20Binary%20Number%20Simple%20NN.ipynb\">https://github.com/StephenOman/TensorFlowExamples/blob/master/Reverse%20Binary%20Number%20Simple%20NN.ipynb</a></p>\n",
  "wfw:commentRss": "https://aimatters.wordpress.com/2018/01/28/neural-networks-and-the-generalisation-problem/feed/",
  "slash:comments": 0,
  "media:content": {
    "media:title": "stephenoman"
  }
}