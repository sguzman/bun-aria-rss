{
  "title": "Subgoal-based Exploration via Bayesian Optimization. (arXiv:1910.09143v3 [math.OC] UPDATED)",
  "link": "http://arxiv.org/abs/1910.09143",
  "description": "<p>Policy optimization in unknown, sparse-reward environments with expensive and\nlimited interactions is challenging, and poses a need for effective\nexploration. Motivated by complex navigation tasks that require real-world\ntraining (when cheap simulators are not available), we consider an agent that\nfaces an unknown distribution of environments and must decide on an exploration\nstrategy, through a series of training environments, that can benefit policy\nlearning in a test environment drawn from the environment distribution. Most\nexisting approaches focus on fixed exploration strategies, while the few that\nview exploration as a meta-optimization problem tend to ignore the need for\ncost-efficient exploration. We propose a cost-aware Bayesian optimization\napproach that efficiently searches over a class of dynamic subgoal-based\nexploration strategies. The algorithm adjusts a variety of levers -- the\nlocations of the subgoals, the length of each episode, and the number of\nreplications per trial -- in order to overcome the challenges of sparse\nrewards, expensive interactions, and noise. Our experimental evaluation\ndemonstrates that, when averaged across problem domains, the proposed algorithm\noutperforms the meta-learning algorithm MAML by 19%, the hyperparameter tuning\nmethod Hyperband by 23%, BO techniques EI and LCB by 24% and 22%, respectively.\nWe also provide a theoretical foundation and prove that the method\nasymptotically identifies a near-optimal subgoal design from the search space.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/math/1/au:+Wang_Y/0/1/0/all/0/1\">Yijia Wang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Poloczek_M/0/1/0/all/0/1\">Matthias Poloczek</a>, <a href=\"http://arxiv.org/find/math/1/au:+Jiang_D/0/1/0/all/0/1\">Daniel R. Jiang</a>"
}