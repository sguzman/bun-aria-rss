{
  "title": "GPU-accelerated Theano & Keras with Windows 10",
  "link": "",
  "published": "2016-09-22T21:48:00-07:00",
  "updated": "2016-09-22T21:48:00-07:00",
  "author": {
    "name": "Damien RJ"
  },
  "id": "tag:efavdb.com,2016-09-22:/gpu-accelerated-theano-keras-with-windows-10",
  "summary": "<p>There are many tutorials with directions for how to use your Nvidia graphics card for <span class=\"caps\">GPU</span>-accelerated Theano and Keras for Linux, but there is only limited information out there for you if you want to set everything up with Windows and the current <span class=\"caps\">CUDA</span> toolkit. This is a shame …</p>",
  "content": "<p>There are many tutorials with directions for how to use your Nvidia graphics card for <span class=\"caps\">GPU</span>-accelerated Theano and Keras for Linux, but there is only limited information out there for you if you want to set everything up with Windows and the current <span class=\"caps\">CUDA</span> toolkit. This is a shame however because there are a large number of computers out there with very nice video cards that are only running windows, and it is not always practical to use a Virtual Machine, or Dual-Boot.  So for today&#8217;s post we will go over how to get everything running in Windows 10 by saving you all the trial and error I went through. (All of these steps should also work in earlier versions of&nbsp;Windows).</p>\n<h2>Dependencies</h2>\n<p>Before getting started, make sure you have the&nbsp;following:</p>\n<ul>\n<li><span class=\"caps\">NVIDIA</span> card that supports <span class=\"caps\">CUDA</span> (<a href=\"https://developer.nvidia.com/cuda-gpus\">link</a>)</li>\n<li>Python 2.7 (<a href=\"http://conda.pydata.org/miniconda.html\">Anaconda</a>&nbsp;preferably)</li>\n<li>Compilers for&nbsp;C/C++</li>\n<li><span class=\"caps\">CUDA</span>&nbsp;7.5</li>\n<li><span class=\"caps\">GCC</span> for code generated by&nbsp;Theano</li>\n</ul>\n<h2>Setup</h2>\n<h3>Visual Studio 2013 Community Edition Update&nbsp;4</h3>\n<p>First, go and download the installer for <a href=\"https://www.visualstudio.com/en-us/news/vs2013-community-vs.aspx\">Visual Studio 2013 Community Edition Update 4</a>.  You can not use the 2015 version because it is still not supported by <span class=\"caps\">CUDA</span>.  When installing, there is no need to install any of the optional packages.  When you are done add the compiler, <strong>C:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\<span class=\"caps\">VC</span>\\bin</strong>,  to your windows&nbsp;path.</p>\n<p>To  add something to your windows path go to System, and then Advanced system&nbsp;settings.</p>\n<p>System → Advanced system settings → Environment Variables →&nbsp;Path.</p>\n<h3><span class=\"caps\">CUDA</span></h3>\n<p>Next, go the <span class=\"caps\">NVIDIA</span>&#8217;s website and <a href=\"https://developer.nvidia.com/cuda-downloads\">download</a> the <span class=\"caps\">CUDA</span> 7.5 toolkit. Select the right version for you computer. When you are installing it, make sure to pick custom install if you don&#8217;t want your video card drivers to be overwritten with the version that comes with the toolkit, which are often out of date.  If it turns out that your version of the drivers are older than what comes with the toolkit,then there is no harm in updating your drivers, otherwise only pick the three boxes starting with <span class=\"caps\">CUDA</span>.</p>\n<h3><span class=\"caps\">GCC</span></h3>\n<p>The last thing we need to do  <span class=\"caps\">GCC</span> compiler, I recommend <a href=\"http://tdm-gcc.tdragon.net/download\"><span class=\"caps\">TDM</span>-gcc</a>.  Install the 64 bit version, and then add the compiler to your windows path, the install has an option to do that for you automatically if you&nbsp;wish.</p>\n<p>To make sure that everything is working at this point, run the the following command on the command line (cmd.exe) . If if finds the path for everything you are good to&nbsp;go.</p>\n<p><code>where gcc where cl where nvcc where cudafe where cudafe++</code></p>\n<h3>Theano and&nbsp;Keras</h3>\n<p>At this point it is easy to install Theano and Keras, just you pip (or conda and&nbsp;pip)!</p>\n<div class=\"highlight\"><pre><span></span><span class=\"err\">conda install mingw libpython</span>\n<span class=\"err\">pip install theano</span>\n<span class=\"err\">pip install keras</span>\n</pre></div>\n\n\n<p>After installing the python libraries you need to tell Theano to use the <span class=\"caps\">GPU</span> instead of the <span class=\"caps\">CPU</span>.  A lot of older posts would have you set this in the system environment, but it is possible to make a config file in your home directory named &#8220;<em>.theanorc.txt</em>&#8221; instead.  This also makes it easy to switch out config files.  Inside the file put the&nbsp;following:</p>\n<div class=\"highlight\"><pre><span></span><span class=\"k\">[global]</span>\n<span class=\"na\">device</span> <span class=\"o\">=</span> <span class=\"s\">gpu</span>\n<span class=\"na\">floatX</span> <span class=\"o\">=</span> <span class=\"s\">float32</span>\n\n<span class=\"k\">[nvcc]</span>\n<span class=\"na\">compiler_bindir</span><span class=\"o\">=</span><span class=\"s\">C:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\VC\\bin</span>\n</pre></div>\n\n\n<p>Lastly, set up the Keras config file <code>~/.keras/keras.json</code>.  If you haven&#8217;t started Keras yet, the folder and file won&#8217;t be there but you can create it. Inside the config put the&nbsp;following.</p>\n<div class=\"highlight\"><pre><span></span><span class=\"err\">{</span>\n<span class=\"err\"> &quot;image_dim_ordering&quot;: &quot;tf&quot;,</span>\n<span class=\"err\"> &quot;epsilon&quot;: 1e-07,</span>\n<span class=\"err\"> &quot;floatx&quot;: &quot;float32&quot;,</span>\n<span class=\"err\"> &quot;backend&quot;: &quot;theano&quot;</span>\n<span class=\"err\">}</span>\n</pre></div>\n\n\n<h2>Testing Theano with <span class=\"caps\">GPU</span></h2>\n<p>Using the following python code,  check if your installation of Theano is using your <span class=\"caps\">GPU</span>.</p>\n<div class=\"highlight\"><pre><span></span><span class=\"kn\">from</span> <span class=\"nn\">theano</span> <span class=\"kn\">import</span> <span class=\"n\">function</span><span class=\"p\">,</span> <span class=\"n\">config</span><span class=\"p\">,</span> <span class=\"n\">shared</span><span class=\"p\">,</span> <span class=\"n\">sandbox</span>\n<span class=\"kn\">import</span> <span class=\"nn\">theano.tensor</span> <span class=\"k\">as</span> <span class=\"nn\">T</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span>\n<span class=\"kn\">import</span> <span class=\"nn\">time</span>\n\n<span class=\"n\">vlen</span> <span class=\"o\">=</span> <span class=\"mi\">10</span> <span class=\"o\">*</span> <span class=\"mi\">30</span> <span class=\"o\">*</span> <span class=\"mi\">768</span> <span class=\"c1\"># 10 x #cores x # threads per core</span>\n<span class=\"n\">iters</span> <span class=\"o\">=</span> <span class=\"mi\">1000</span>\n\n<span class=\"n\">rng</span> <span class=\"o\">=</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">RandomState</span><span class=\"p\">(</span><span class=\"mi\">22</span><span class=\"p\">)</span>\n<span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">shared</span><span class=\"p\">(</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">asarray</span><span class=\"p\">(</span><span class=\"n\">rng</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">vlen</span><span class=\"p\">),</span> <span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">floatX</span><span class=\"p\">))</span>\n<span class=\"n\">f</span> <span class=\"o\">=</span> <span class=\"n\">function</span><span class=\"p\">([],</span> <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">))</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"o\">.</span><span class=\"n\">maker</span><span class=\"o\">.</span><span class=\"n\">fgraph</span><span class=\"o\">.</span><span class=\"n\">toposort</span><span class=\"p\">())</span>\n<span class=\"n\">t0</span> <span class=\"o\">=</span> <span class=\"n\">time</span><span class=\"o\">.</span><span class=\"n\">time</span><span class=\"p\">()</span>\n<span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">iters</span><span class=\"p\">):</span>\n    <span class=\"n\">r</span> <span class=\"o\">=</span> <span class=\"n\">f</span><span class=\"p\">()</span>\n    <span class=\"n\">t1</span> <span class=\"o\">=</span> <span class=\"n\">time</span><span class=\"o\">.</span><span class=\"n\">time</span><span class=\"p\">()</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;Looping </span><span class=\"si\">%d</span><span class=\"s2\"> times took </span><span class=\"si\">%f</span><span class=\"s2\"> seconds&quot;</span> <span class=\"o\">%</span> <span class=\"p\">(</span><span class=\"n\">iters</span><span class=\"p\">,</span> <span class=\"n\">t1</span> <span class=\"o\">-</span> <span class=\"n\">t0</span><span class=\"p\">))</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;Result is </span><span class=\"si\">%s</span><span class=\"s2\">&quot;</span> <span class=\"o\">%</span> <span class=\"p\">(</span><span class=\"n\">r</span><span class=\"p\">,))</span>\n    <span class=\"k\">if</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">any</span><span class=\"p\">([</span><span class=\"nb\">isinstance</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">op</span><span class=\"p\">,</span> <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">Elemwise</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">x</span> <span class=\"ow\">in</span> <span class=\"n\">f</span><span class=\"o\">.</span><span class=\"n\">maker</span><span class=\"o\">.</span><span class=\"n\">fgraph</span><span class=\"o\">.</span><span class=\"n\">toposort</span><span class=\"p\">()]):</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;Used the cpu&#39;</span><span class=\"p\">)</span>\n    <span class=\"k\">else</span><span class=\"p\">:</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;Used the gpu&#39;</span><span class=\"p\">)</span>\n</pre></div>\n\n\n<h2>Testing Keras with <span class=\"caps\">GPU</span></h2>\n<p>This code will make sure that everything is working and train a model on some random data. The first time might take a little longer because it the software needs to do some&nbsp;compiling.</p>\n<div class=\"highlight\"><pre><span></span><span class=\"kn\">from</span> <span class=\"nn\">keras.models</span> <span class=\"kn\">import</span> <span class=\"n\">Sequential</span>\n<span class=\"kn\">from</span> <span class=\"nn\">keras.layers</span> <span class=\"kn\">import</span> <span class=\"n\">Dense</span><span class=\"p\">,</span> <span class=\"n\">Activation</span>\n\n<span class=\"c1\"># for a single-input model with 2 classes (binary):</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">Sequential</span><span class=\"p\">()</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">add</span><span class=\"p\">(</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">input_dim</span><span class=\"o\">=</span><span class=\"mi\">784</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s1\">&#39;sigmoid&#39;</span><span class=\"p\">))</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">compile</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"o\">=</span><span class=\"s1\">&#39;rmsprop&#39;</span><span class=\"p\">,</span>\n<span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"s1\">&#39;binary_crossentropy&#39;</span><span class=\"p\">,</span>\n<span class=\"n\">metrics</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;accuracy&#39;</span><span class=\"p\">])</span>\n\n<span class=\"c1\"># generate dummy data</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"p\">((</span><span class=\"mi\">1000</span><span class=\"p\">,</span> <span class=\"mi\">784</span><span class=\"p\">))</span>\n<span class=\"n\">labels</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randint</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1000</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># train the model, iterating on the data in batches</span>\n<span class=\"c1\"># of 32 samples</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"p\">,</span> <span class=\"n\">nb_epoch</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">32</span><span class=\"p\">)</span>\n</pre></div>\n\n\n<p>If everything works you will see something like&nbsp;this!</p>\n<p><a href=\"https://efavdb.com/wp-content/uploads/2016/09/output.png\"><img alt=\"output\" src=\"https://efavdb.com/wp-content/uploads/2016/09/output.png\"></a></p>\n<p>Now you can start playing with neural networks using your <span class=\"caps\">GPU</span>!</p>",
  "category": ""
}