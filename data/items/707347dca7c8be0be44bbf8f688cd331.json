{
  "title": "Adaptive Stochastic Variance Reduction for Non-convex Finite-Sum Minimization. (arXiv:2211.01851v1 [math.OC])",
  "link": "http://arxiv.org/abs/2211.01851",
  "description": "<p>We propose an adaptive variance-reduction method, called AdaSpider, for\nminimization of $L$-smooth, non-convex functions with a finite-sum structure.\nIn essence, AdaSpider combines an AdaGrad-inspired [Duchi et al., 2011, McMahan\n&amp; Streeter, 2010], but a fairly distinct, adaptive step-size schedule with the\nrecursive stochastic path integrated estimator proposed in [Fang et al., 2018].\nTo our knowledge, Adaspider is the first parameter-free non-convex\nvariance-reduction method in the sense that it does not require the knowledge\nof problem-dependent parameters, such as smoothness constant $L$, target\naccuracy $\\epsilon$ or any bound on gradient norms. In doing so, we are able to\ncompute an $\\epsilon$-stationary point with $\\tilde{O}\\left(n +\n\\sqrt{n}/\\epsilon^2\\right)$ oracle-calls, which matches the respective lower\nbound up to logarithmic factors.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/math/1/au:+Kavis_A/0/1/0/all/0/1\">Ali Kavis</a>, <a href=\"http://arxiv.org/find/math/1/au:+Skoulakis_S/0/1/0/all/0/1\">Stratis Skoulakis</a>, <a href=\"http://arxiv.org/find/math/1/au:+Antonakopoulos_K/0/1/0/all/0/1\">Kimon Antonakopoulos</a>, <a href=\"http://arxiv.org/find/math/1/au:+Dadi_L/0/1/0/all/0/1\">Leello Tadesse Dadi</a>, <a href=\"http://arxiv.org/find/math/1/au:+Cevher_V/0/1/0/all/0/1\">Volkan Cevher</a>"
}