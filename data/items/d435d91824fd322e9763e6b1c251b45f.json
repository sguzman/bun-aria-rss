{
  "title": "Long term credit assignment with temporal reward transport",
  "link": "",
  "published": "2020-06-29T12:00:00-07:00",
  "updated": "2020-06-29T12:00:00-07:00",
  "author": {
    "name": "Cathy Yeh"
  },
  "id": "tag:efavdb.com,2020-06-29:/ltca",
  "summary": "<p>[<span class=\"caps\">TOC</span>]</p>\n<h1>Summary</h1>\n<p>Standard reinforcement learning algorithms struggle with poor sample efficiency in the presence of sparse rewards with long temporal delays between action and effect. To address the long term credit assignment problem, we build on the work of [1] to use “temporal reward transport” (<span class=\"caps\">TRT</span>) to augment the immediate …</p>",
  "content": "<p>[<span class=\"caps\">TOC</span>]</p>\n<h1>Summary</h1>\n<p>Standard reinforcement learning algorithms struggle with poor sample efficiency in the presence of sparse rewards with long temporal delays between action and effect. To address the long term credit assignment problem, we build on the work of [1] to use “temporal reward transport” (<span class=\"caps\">TRT</span>) to augment the immediate rewards of significant state-action pairs with rewards from the distant future using an attention mechanism to identify candidates for <span class=\"caps\">TRT</span>. A series of gridworld experiments show clear improvements in learning when <span class=\"caps\">TRT</span> is used in conjunction with a standard advantage actor critic&nbsp;algorithm.</p>\n<h1>Introduction</h1>\n<p>Episodic reinforcement learning (<span class=\"caps\">RL</span>) models the interaction of an agent with an environment as a Markov Decision Process with a finite number of time steps <span class=\"math\">\\(T\\)</span>.  The environment dynamics <span class=\"math\">\\(p(s’,r|s, a)\\)</span> are modeled as a joint probability distribution over the next state <span class=\"math\">\\(s'\\)</span> and reward <span class=\"math\">\\(r\\)</span> picked up along the way given the previous state <span class=\"math\">\\(s\\)</span> and action <span class=\"math\">\\(a\\)</span>.  In general, the agent does not have access to an exact model of the&nbsp;environment.</p>\n<p>The agent&#8217;s goal is to maximize its cumulative rewards, the discounted returns <span class=\"math\">\\(G_t\\)</span>,</p>\n<div class=\"math\">\\begin{eqnarray}\\label{return} \\tag{1}\nG_t := R_{t+1} + \\gamma R_{t+2} + … = \\sum_{k=0}^T \\gamma^k R_{t+k+1}\n\\end{eqnarray}</div>\n<p>where <span class=\"math\">\\(0 \\leq \\gamma \\leq 1\\)</span>, and <span class=\"math\">\\(R_{t}\\)</span> is the reward at time <span class=\"math\">\\(t\\)</span>.  In episodic <span class=\"caps\">RL</span>, the discount factor <span class=\"math\">\\(\\gamma\\)</span> is often used to account for uncertainty in the future, to favor rewards now vs. later, and as a variance reduction technique, e.g. in policy gradient methods [2,&nbsp;3].</p>\n<p>Using a discount factor <span class=\"math\">\\(\\gamma &lt; 1\\)</span> introduces a timescale by exponentially suppressing rewards in the future by <span class=\"math\">\\(\\exp(-n/\\tau_{\\gamma})\\)</span>.  The number of timesteps it takes for a reward to decay by <span class=\"math\">\\(1/e\\)</span> is <span class=\"math\">\\(\\tau_{\\gamma} = 1/(1-\\gamma)\\)</span>, in units of timesteps, which follows from solving for <span class=\"math\">\\(n\\)</span> after setting the left and right sides of (\\ref{discount-timescale}) to be&nbsp;equal</p>\n<div class=\"math\">\\begin{align}\\label{discount-timescale}\\tag{2}\n\\gamma ^ n \\approx \\frac{1}{e} = \\lim_{n \\rightarrow \\infty} \\left(1 - \\frac{1}{n} \\right)^n\n\\end{align}</div>\n<hr>\n<p>The state value function <span class=\"math\">\\(v_{\\pi}(s)\\)</span> is the expected return when starting in state <span class=\"math\">\\(s\\)</span>, following policy <span class=\"math\">\\(\\pi(a|s) := p(a|s)\\)</span>, a function of the current&nbsp;state.</p>\n<div class=\"math\">\\begin{eqnarray}\\label{state-value} \\tag{3}\nv_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t | S_t = s]\n\\end{eqnarray}</div>\n<p>Policy gradient algorithms improve the policy by using gradient ascent along the gradient of the value&nbsp;function.</p>\n<div class=\"math\">\\begin{eqnarray}\\label{policy-gradient} \\tag{4} \n\\nabla_{\\theta} v_\\pi(s_0) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{T-1} \\nabla_{\\theta} \\log \\pi_{\\theta}(A_t | S_t) \\mathcal{R}(\\tau)\\right],\n\\end{eqnarray}</div>\n<p>where <span class=\"math\">\\(\\tau \\sim \\pi\\)</span> describes the agent&#8217;s trajectory following policy <span class=\"math\">\\(\\pi\\)</span> beginning from state <span class=\"math\">\\(s_0\\)</span>, and <span class=\"math\">\\(\\mathcal{R}(\\tau)\\)</span> is a function of the rewards obtained along the trajectory.  In practice, policy gradients approximate the expected value in (\\ref{policy-gradient}) by sampling, which results in very high variance estimates of the&nbsp;gradient.</p>\n<p>Common techniques to reduce the variance of the estimated policy gradient include&nbsp;[2]</p>\n<ol>\n<li>only assigning credit for rewards (the &#8220;rewards-to-go&#8221;) accumulated after a particular action was taken instead of crediting the action for all rewards from the&nbsp;trajectory.</li>\n<li>subtracting a baseline from the rewards weight that is independent of action.  Oftentimes, this baseline is the value function in&nbsp;(\\ref{state-value}).</li>\n<li>using a large batch&nbsp;size.</li>\n<li>using the value function (\\ref{state-value}) to bootstrap the returns some number of steps into the future instead of using the full raw discounted return, giving rise to a class of algorithms called actor critics that learn a policy and value function in parallel.  For example, one-step bootstrapping would approximate the discounted returns in (\\ref{return})&nbsp;as</li>\n</ol>\n<div class=\"math\">\\begin{eqnarray}\\label{bootstrap} \\tag{5}\nG_t = R_{t+1} + \\gamma R_{t+2} +  \\gamma^2 R_{t+3} + \\ldots \\approx R_{t+1} + \\gamma V(S_{t+1}),\n\\end{eqnarray}</div>\n<p>where <span class=\"math\">\\(V(S_{t+1})\\)</span> is the estimate of the value of state <span class=\"math\">\\(S_{t+1}\\)</span>&nbsp;(\\ref{state-value}).</p>\n<p>All of these techniques typically make use of discounting, so an action receives little credit for rewards that happen more than <span class=\"math\">\\(\\tau_{\\gamma}\\)</span> timesteps in the future, making it challenging for standard reinforcement learning algorithms to learn effective policies in situations where action and effect are separated by long temporal&nbsp;delays.</p>\n<h1>Results</h1>\n<h2>Temporal reward&nbsp;transport</h2>\n<p>We use temporal reward transport (or <span class=\"caps\">TRT</span>), inspired directly by the Temporal Value Transport algorithm from [1], to mitigate the loss of signal from discounting by splicing temporally delayed future rewards to the immediate rewards following an action that the <span class=\"caps\">TRT</span> algorithm determines should receive&nbsp;credit.</p>\n<p>To assign credit to a specific observation-action pair, we use an attention layer in a neural network binary classifier.  The classifier predicts whether the undiscounted returns for an episode are below or above a certain threshold.  If a particular observation and its associated action are highly attended to for the classification problem, then that triggers the splicing of future rewards in the episode to that particular observation-action&nbsp;pair.</p>\n<p>Model training is divided into two&nbsp;parts:</p>\n<ol>\n<li>Experience collection using the current policy in an advantage actor critic (<span class=\"caps\">A2C</span>)&nbsp;model.</li>\n<li>Parameter updates for the <span class=\"caps\">A2C</span> model and binary&nbsp;classifier.</li>\n</ol>\n<p><span class=\"caps\">TRT</span> happens between step 1 and 2; it plays no role in experience collection, but modifies the collected rewards through the splicing mechanism, thereby affecting the advantage and, consequently, the policy gradient in&nbsp;(\\ref{policy-gradient}).</p>\n<div class=\"math\">\\begin{eqnarray} \\notag\nR_t \\rightarrow R_t + [\\text{distal rewards} (t' &gt; t + \\tau_\\gamma)]\n\\end{eqnarray}</div>\n<h2>Environment for&nbsp;experiments</h2>\n<p>We created a <a href=\"https://github.com/openai/gym\">gym</a> <a href=\"https://github.com/maximecb/gym-minigrid\">gridworld</a> environment to specifically study long term credit assignment.  The environment is a simplified version of the 3-d DeepMind Lab experiments laid out in [1].  As in [1], we structure the environment to comprise three phases.  In the first phase, the agent must take an action that yields no immediate reward.  In the second phase, the agent engages with distractions that yield immediate rewards.  In the final phase, the agent can acquire a distal reward, depending on the action it took in phase&nbsp;1.</p>\n<p>Concretely, the gridworld environment consists&nbsp;of:</p>\n<p>(1) Empty grid with key: agent can pick up the key but receives no immediate reward for picking it&nbsp;up.</p>\n<p align=\"center\">\n<img src=\"images/ltca_gridworld_p1.png\" alt=\"Phase 1\" style=\"width:190px;\">\n</p>\n\n<p>(2) Distractor phase: Agent engages with distractors, gifts that yield immediate&nbsp;rewards.</p>\n<p align=\"center\">\n<img src=\"images/ltca_gridworld_p2.png\" alt=\"Phase 1\" style=\"width:190px;\">\n</p>\n\n<p>(3) Delayed reward phase: Agent should move to a green goal grid cell.  If the agent is carrying the key when it reaches the goal, it is rewarded extra&nbsp;points.</p>\n<p align=\"center\">\n<img src=\"images/ltca_gridworld_p3.png\" alt=\"Phase 1\" style=\"width:190px;\">\n</p>\n\n<p>The agent remains in each phase for a fixed period of time, regardless of how quickly it finishes the intermediate task, and then teleports to the next phase.  At the end of each episode, the environment resets with a different random seed that randomizes the placement of the key in phase 1 and distractor objects in phase&nbsp;2.</p>\n<h2>Experiments</h2>\n<p>In all experiments, we fix the time spent in phase 1 and phase 3, the number of distractor gifts in phase 2, as well as the distal reward in phase 3.  In phase 3, the agent receives 5 points for reaching the goal without a key and 20 points for reaching the goal carrying a key (with a small penalty proportional to step count to encourage moving quickly to the&nbsp;goal).</p>\n<p>Our evaluation metric for each experiment is the the distal reward obtained in phase 3, which focuses on whether the agent learns to pick up the key in phase 1 in order to acquire the distal reward, although we verify that the agent is also learning to open the gifts in phase 2 by plotting the overall returns (see &#8220;Data and code availability&#8221;&nbsp;section).</p>\n<p>Each experiment varies a particular parameter in the second phase, namely, the time delay, distractor reward size, and distractor reward variance, and compares the performance of the baseline <span class=\"caps\">A2C</span> algorithm with <span class=\"caps\">A2C</span> supplemented with <span class=\"caps\">TRT</span> (<span class=\"caps\">A2C</span>+<span class=\"caps\">TRT</span>).</p>\n<h3>Time delay in distractor&nbsp;phase</h3>\n<p>We vary the time spent in the distractor phase, <span class=\"math\">\\(T_2\\)</span>, as a multiple of the discount factor timescale.  We used a discount factor of <span class=\"math\">\\(\\gamma=0.99\\)</span>, which corresponds to a timescale of ~100 steps according to (\\ref{discount-timescale}).  We ran experiments for <span class=\"math\">\\(T_2 = (0, 0.5, 1, 2) * \\tau_{\\gamma}\\)</span>.  The distractor reward is 3 points per&nbsp;gift.</p>\n<p align=\"center\">\n<img src=\"images/ltca_time_delay_expt_plots.png\" alt=\"P2 time delay expt\" style=\"width:800px;\">\n</p>\n\n<p><small>Fig 1. Returns in phase 3 for time delays in phase 2 of 0.5<span class=\"math\">\\(\\tau_{\\gamma}\\)</span>, <span class=\"math\">\\(\\tau_{\\gamma}\\)</span>, and 2<span class=\"math\">\\(\\tau_{\\gamma}\\)</span>.\n</small></p>\n<p>As the environment becomes more challenging from left to right with increasing time delay, we see that <span class=\"caps\">A2C</span> plateaus around 5 points in phase 3, corresponding to reaching the goal without the key, whereas <span class=\"caps\">A2C</span>+<span class=\"caps\">TRT</span> increasingly learns to pick up the key over the training&nbsp;period.</p>\n<h3>Distractor reward&nbsp;size</h3>\n<p>We vary the size of the distractor rewards, 4 gifts for the agent to toggle open, in phase 2.  We run experiments for a reward of 0, 1, 5, and 8, resulting in maximum possible rewards in phase 2 of 0, 4, 20, and&nbsp;32.</p>\n<p>In comparison, the maximum possible reward in phase 3 is&nbsp;20.</p>\n<p align=\"center\">\n<img src=\"images/ltca_reward_expt_plots.png\" alt=\"P2 reward size expt\" style=\"width:800px;\">\n</p>\n\n<p><small>Fig 2. Returns in phase 3 for distractor rewards of size 0, 5, and 8.\n</small></p>\n<p>Like the time delay experiments, we see that <span class=\"caps\">A2C</span>+<span class=\"caps\">TRT</span> shows progress learning to pick up the key, whereas <span class=\"caps\">A2C</span> does not over the training period with increasing distractor&nbsp;sizes.</p>\n<h3>Distractor reward&nbsp;variance</h3>\n<p>We fix the mean reward size of the gifts in phase 2 at 5, but change the variance of the rewards by drawing each reward from a uniform distribution centered at 5, with minimum and maximum ranges of [5, 5], [3, 7], and [0, 10], corresponding to variances of 0, 1.33, and 8.33,&nbsp;respectively.</p>\n<p align=\"center\">\n<img src=\"images/ltca_reward_var_expt_plots.png\" alt=\"P2 reward variance expt\" style=\"width:800px;\">\n</p>\n\n<p><small>Fig 3. Returns in phase 3 for distractor reward variance of size 0, 1.33, and 8.33.\n</small></p>\n<p>The signal-to-noise ratio of the policy gradient, defined as the ratio of the squared magnitude of the expected gradient to the variance of the gradient estimate, was shown to be approximately inversely proportional to the variance of the distractor rewards in phase 2 in [1] for <span class=\"math\">\\(\\gamma = 1\\)</span>.  The poor performance of <span class=\"caps\">A2C</span> in the highest variance (low signal-to-noise ratio) case is consistent with this observation, with a small standard deviation in performance around the plateau value of 5 compared to the experiments on time delay and distractor reward&nbsp;size.</p>\n<h1>Discussion</h1>\n<p>Like temporal value transport introduced in [1], <span class=\"caps\">TRT</span> is a heuristic.  Nevertheless, coupling this heuristic with <span class=\"caps\">A2C</span> has been shown to improve performance on several tasks characterized by delayed rewards that are a challenge for standard deep <span class=\"caps\">RL</span>.</p>\n<p>Our contribution is a simplified, modular implementation of core ideas in [1], namely, splicing additional rewards from the distant future to state-action pairs identified as significant through a self-attention mechanism.  Unlike [1], we implement the self-attention mechanism in a completely separate model and splice the rewards-to-go instead of an estimated value.  In addition to the modularity that comes splitting out the attention mechanism for <span class=\"caps\">TRT</span> into a separate model, another advantage of decoupling the models is that we can increase the learning rate of the classifier without destabilizing the learning of the main actor critic model if the classification problem is comparatively&nbsp;easy.</p>\n<p><strong>Related&nbsp;work</strong></p>\n<p>Other works also draw on the idea of using hindsight to reduce the variance estimates of the policy gradient, and hence increase sample efficiency.  &#8220;Hindsight credit assignment&#8221; proposed in [7] similarly learns discriminative models in hindsight that give rise to a modified form of the value function, evaluated using tabular models in a few toy environments (not focused specifically on the long term credit assignment problem).  <span class=\"caps\">RUDDER</span> [8] is more similar in spirit to [1] and <span class=\"caps\">TRT</span> in the focus on redistributing rewards to significant state-action pairs, but identified using saliency analysis on an <span class=\"caps\">LSTM</span> instead of an attention&nbsp;mechanism.</p>\n<p><strong>Future&nbsp;work</strong></p>\n<p>The robustness of the <span class=\"caps\">TRT</span> algorithm should be further assessed on a wider variety of environments, including e.g. Atari Bowling, which is another environment with a delayed reward task used for evaluations by [1] and [8].  It remains to be seen whether the attention mechanism and <span class=\"caps\">TRT</span> can handle more complex scenarios, in particular scenarios where a sequence of actions must be taken.  Just as it is difficult to extract interpretable features from a linear model in the presence of multicollinearity, it is possible that the attention-based classifier may encounter similar problems identifying important state-action pairs when a sequence of actions is required, as our model has no mechanism for causal&nbsp;reasoning.</p>\n<p>Although our experiments only evaluated <span class=\"caps\">TRT</span> on <span class=\"caps\">A2C</span>, coupling it with any policy gradient method based on sampling action space should yield similar benefits, which could be straightforwardly tested with our modular&nbsp;implementation.</p>\n<p>A benefit of using self-attention is the temporal granularity over an <span class=\"caps\">LSTM</span>.  However, a downside is that our approach relies on having the full context of the episode for the attention mechanism ([1] similarly relies on full episodes), in contrast to other methods that can handle commonly used truncation windows with a bootstrapped final value for non-terminal states.  Holding full episodes in memory can become untenable for very long episodes, but we have not yet worked out a way to handle this situation in the current&nbsp;setup.</p>\n<p>Our first pass implementation transported the raw rewards-to-go instead of the value estimate used in [1], but it is unclear whether transporting the rewards-to-go (essentially re-introducing a portion of the undiscounted Monte Carlo returns) for a subset of important state-action pairs provides a strong enough signal to outweigh the advantages of using a boostrapped estimate intended for variance reduction; the answer may depend on the particular task/environment and is of course contingent on the quality of the value&nbsp;estimate.</p>\n<p>The classifier model itself has a lot of room for experimentation.  The idea of using a classifier was motivated by a wish to easily extract state-action pairs with high returns from the attention layer, although we have yet to explore whether this provides a clear benefit over a regression model like&nbsp;[1].</p>\n<p>The binary classifier is trained to predict whether the rewards-to-go of each subsequence of an episode exceeds a moving average of maximum returns.  On the one hand, this is less intuitive than only making the prediction for undiscounted returns of the full episode and introduces highly non-iid inputs for the classifier, which can make make training less stable.  On the other hand, one can interpret the current format as a form of data augmentation that results in more instances of the positive class (high return episodes) that benefits the&nbsp;classifier.</p>\n<p>If the classifier were modified to only make a single prediction per episode, it may be necessary to create a buffer of recent experiences to shift the distribution of data towards more positive samples for the classifier to draw from in addition to episodes generated from the most recent policy (with the untested assumption that the classifier would be less sensitive to training on such off-policy data than the main actor critic model while benefiting from the higher incidence of the positive&nbsp;class).</p>\n<p>Finally, the <span class=\"caps\">TRT</span> algorithm introduces additional hyperparameters that could benefit from additional tuning, including the constant factor multiplying the transported rewards and the attention score threshold to trigger <span class=\"caps\">TRT</span>.</p>\n<h1>Methods</h1>\n<h2>Environment</h2>\n<p>The agent receives a partial observation of the environment, the 7x7 grid in front of it, with each grid cell encoding 3 input values, resulting in 7x7x3 values total (not&nbsp;pixels).</p>\n<p>The gridworld environment supports 7 actions: left, right, forward, pickup, drop, toggle,&nbsp;done.</p>\n<p>The environment consists of three&nbsp;phases:</p>\n<ul>\n<li>Phase 1 &#8220;key&#8221;: 6x6 grid cells, time spent = 30&nbsp;steps</li>\n<li>Phase 2 &#8220;gifts&#8221;: 10x10 grid cells, time spent = 50 steps (except for the time delay experiment, which varies the time&nbsp;spent)</li>\n<li>Phase 3 &#8220;goal&#8221;: 7x7 grid cells, time spent =&nbsp;70.</li>\n</ul>\n<p>If the agent picks up the key in phase 1, it is initialized carrying the key in phase 3, but not phase 2.  The carrying state is visible to the agent in phase 1 and 3.  Except for the time delay experiment, each episode is 150&nbsp;timesteps.</p>\n<p><strong>Distractor rewards in phase&nbsp;2:</strong></p>\n<ul>\n<li>4 distractor objects, gifts that the agent can toggle open, that yield immediate&nbsp;rewards</li>\n<li>Each opened gift yields a mean reward of 3 points (except in the reward size&nbsp;experiment)</li>\n<li>Gift rewards have a variance of 0 (except in the reward variance&nbsp;experiment)</li>\n</ul>\n<p><strong>Distal rewards in phase&nbsp;3:</strong></p>\n<p>5 points for reaching the goal without a key and 20 points for reaching the goal carrying a key.  There is a small penalty of <code>-0.9 * step_count / max_steps=70</code> to encourage moving quickly to the goal.  For convenience of parallelizing experience collection of complete episodes, the time in the final phase is fixed, even if the agent finishes the task of navigating to the green goal earlier.  Furthermore, for convenience of tracking rewards acquired in the final phase, the agent only receives the reward for task completion in the last step of the final phase, even though this last reward reflects the time and state in which the agent initially reached the&nbsp;goal.</p>\n<p>Note, unlike the Reconstructive Memory Agent in [1], our agent does not have the ability to encode and reconstruct memories, and our environment is not set up to test for that&nbsp;ability.</p>\n<h2>Agent&nbsp;model</h2>\n<p>The agent&#8217;s model is an actor critic consisting of 3 components: an image encoder convolutional net (<span class=\"caps\">CNN</span>), an recurrent neural net layer providing memory, and dual heads outputting the policy and value.  We used an open-sourced model that has been extensively tested for gym-minigrid environments from&nbsp;[4].</p>\n<p align=\"center\">\n<img src=\"images/ltca_a2c_model.png\" alt=\"A2C model\" style=\"width:300px;\">\n</p>\n\n<p><small>Fig 4. <span class=\"caps\">A2C</span> model with three convolutional layers, <span class=\"caps\">LSTM</span>, and dual policy and value function heads.\n</small></p>\n<p>The image encoder consists of three convolutional layers interleaved with rectified linear (ReLU) activation functions.  A max pooling layer also immediately precedes the second convolutional&nbsp;layer.</p>\n<p>The encoded image is followed by a single Long Short Term Memory (<span class=\"caps\">LSTM</span>)&nbsp;layer.</p>\n<p>The <span class=\"caps\">LSTM</span> outputs a hidden state which feeds into the dual heads of the actor critic.  Both heads consist of two fully connected linear layers sandwiching a tanh activation layer.  The output of the actor, the policy, is the same size as the action space in the environment.  The output of the critic is a scalar corresponding to the estimated&nbsp;value.</p>\n<h2>Binary classifier with&nbsp;self-attention</h2>\n<p>The inputs to the binary classifier are the sequence of image embeddings output by the actor critic model&#8217;s <span class=\"caps\">CNN</span> (not the hidden state of the <span class=\"caps\">LSTM</span>) and one-hot encoded actions taken in that&nbsp;state.</p>\n<p>The action passes through a linear layer with 32 hidden units before concatenation with the image&nbsp;embedding.</p>\n<p>Next, the concatenated vector <span class=\"math\">\\(\\mathbf{x}_i\\)</span> undergoes three separate linear transformations, playing the role of &#8220;query&#8221;, &#8220;key&#8221; and &#8220;value&#8221; (see [6] for an excellent explanation upon which we based our implementation of attention).  Each transformation projects the vector to a space of size equal to the length of the&nbsp;episode.</p>\n<div class=\"math\">\\begin{eqnarray}\\label{key-query} \\tag{6}\n\\mathbf{q}_i &amp;=&amp; \\mathbf{W}_q \\mathbf{x}_i \\\\\n\\mathbf{k}_i &amp;=&amp; \\mathbf{W}_k \\mathbf{x}_i \\\\\n\\mathbf{v}_i &amp;=&amp; \\mathbf{W}_v \\mathbf{x}_i \\\\\n\\end{eqnarray}</div>\n<p>The self-attention layer outputs a weighted average over the value vectors, where the weight is not a parameter of the neural net, but the dot product of the query and key&nbsp;vectors.</p>\n<div class=\"math\">\\begin{eqnarray}\\label{self-attention} \\tag{7}\nw'_{ij} &amp;=&amp; \\mathbf{q}_i^\\top \\mathbf{k}_j \\\\\nw_{ij} &amp;=&amp; \\text{softmax}(w'_{ij}) \\\\\n\\mathbf{y_i} &amp;=&amp; \\sum_j w_{ij} \\mathbf{v_j}\n\\end{eqnarray}</div>\n<p>The dot product in (\\ref{self-attention}) is between embeddings of different frames in an episode.  We apply masking to the weight matrix before the softmax in (\\ref{self-attention}) to ensure that observations from different episodes do not pay attention to each other, in addition to future masking (observations can only attend past observations in the same&nbsp;episode).</p>\n<p>The output of the attention layer then passes through a fully connected layer with 64 hidden units, followed by a ReLU activation, and the final output is a scalar, the logit predicting whether the rewards-to-go from a given observation are below or above a&nbsp;threshold.</p>\n<p>The threshold itself is a moving average over the maximum undiscounted returns seen across network updates, where the averaging window is a hyperparameter that should balance updating the threshold in response to higher returns due to an improving policy (in general, increasing, although monotonicity is not enforced) with not increasing so quickly such that there are too few episodes in the positive (high returns) class in a given batch of collected&nbsp;experiences.</p>\n<p align=\"center\">\n<img src=\"images/ltca_classifier_model.png\" alt=\"Classifier model\" style=\"width:300px;\">\n</p>\n\n<p><small>Fig 5. Binary classifier model with attention, accepting sequences as input.\n</small></p>\n<h2>Temporal reward&nbsp;transport</h2>\n<p>After collecting a batch of experiences by following the <span class=\"caps\">A2C</span> model&#8217;s policy, we calculate the attention scores <span class=\"math\">\\(w_{ij}\\)</span> from (\\ref{self-attention}) using observations from the full episode as&nbsp;context.</p>\n<p align=\"center\">\n<img src=\"images/ltca_attention_single_episode.png\" alt=\"Attention scores single episode\" style=\"width:400px;\">\n</p>\n\n<p><small>Fig 6. Attention scores for a single episode with future masking (of the upper right triangle).  The bright vertical stripes correspond to two highly attended state-action pairs.\n</small></p>\n<p>We calculate the importance, defined as the average weight of observation <span class=\"math\">\\(i\\)</span>, ignoring masked regions in the attention matrix&nbsp;as</p>\n<div class=\"math\">\\begin{eqnarray}\\label{importance} \\tag{8}\n\\text{importance}_i = \\frac{1}{T - i} \\sum_{j \\geq i}^{T} w_{ij}\n\\end{eqnarray}</div>\n<p align=\"center\">\n<img src=\"images/ltca_importances.png\" alt=\"Importances for a batch of frames\" style=\"width:600px;\">\n</p>\n\n<p><small>Fig 7. Importances for a batch of collected experiences (16 processes x 600 frames = 9600 frames), with frame or step number on the horizontal axis and process number on the vertical axis.\n</small></p>\n<p>Observations with an importance score above a threshold (between 0 and 1) hyperparameter are eligible for <span class=\"caps\">TRT</span>.  After identifying the candidates for <span class=\"caps\">TRT</span>, we add the distal rewards-to-go, weighted by the importance and another hyperparameter for tuning the impact of the <span class=\"caps\">TRT</span> rewards <span class=\"math\">\\(\\alpha_{TRT}\\)</span> to the original reward <span class=\"math\">\\(r_i\\)</span> obtained during experience&nbsp;collection:</p>\n<div class=\"math\">\\begin{eqnarray}\\label{trt} \\tag{9}\nr_i &amp;\\rightarrow&amp; r_i + \\text{TRT-reward}_i \\\\\n\\text{TRT-reward}_i &amp;\\equiv&amp; \\alpha_{TRT} * \\text{importance}_i * \\text{rewards-to-go}_i\n\\end{eqnarray}</div>\n<p>We define the distal rewards-to-go in (\\ref{trt}) as the total undiscounted returns from observation <span class=\"math\">\\(i\\)</span>, excluding rewards accumulated in an immediate time window of size equal to the discount factor timescale <span class=\"math\">\\(\\tau_\\gamma\\)</span> defined in (\\ref{discount-timescale}).  This temporal exclusion zone helps prevent overcounting&nbsp;rewards.</p>\n<p>We calculate the advantage after <span class=\"caps\">TRT</span> using the generalized advantage estimation algorithm <span class=\"caps\">GAE</span>-<span class=\"math\">\\(\\lambda\\)</span> [3] with <span class=\"math\">\\(\\lambda=0.95\\)</span>, which, analogous to <span class=\"caps\">TD</span>-<span class=\"math\">\\(\\lambda\\)</span> [2], calculates the advantage from an exponentially weighted average over 1- to n-step bootstrapped estimates of the <span class=\"math\">\\(Q\\)</span> value.  One of the benefits of using <span class=\"caps\">GAE</span>-<span class=\"math\">\\(\\lambda\\)</span> is the spillover effect that enables the <span class=\"caps\">TRT</span>-reinforced rewards to directly affect neighboring state-action pairs in addition to the triggering state-action&nbsp;pair.</p>\n<h2>Training</h2>\n<p>For experience collection, we used 16 parallel processes, with 600 frames collected per process for a batch size of 9600 frames between parameter&nbsp;updates.</p>\n<p>The <span class=\"caps\">A2C</span> model loss per time step&nbsp;is</p>\n<div class=\"math\">\\begin{eqnarray} \\label{a2c-loss} \\tag{10}\n\\mathcal{L}_{A2C} \\equiv \\mathcal{L}_{policy} + \\alpha_{value} \\mathcal{L}_{value} - \\alpha_{entropy} \\text{entropy},\n\\end{eqnarray}</div>\n<p>where</p>\n<div class=\"math\">\\begin{eqnarray} \\notag\n\\mathcal{L}_{policy} &amp;=&amp; - \\log p(a_t | o_t, h_t), \\\\\n\\mathcal{L}_{value} &amp;=&amp; \\left\\Vert \\hat{V}(o_t, h_t) - R_t \\right\\Vert^2,\n\\end{eqnarray}</div>\n<p>and <span class=\"math\">\\(o_t\\)</span> and <span class=\"math\">\\(h_t\\)</span> are the observation and hidden state from the <span class=\"caps\">LSTM</span> at time <span class=\"math\">\\(t\\)</span>, respectively.  We accumulate the losses defined in (\\ref{a2c-loss}) by iterating over batches of consecutive time steps equal to the size of the <span class=\"caps\">LSTM</span> memory of 10, i.e. truncated backpropagation in time for 10&nbsp;timesteps.</p>\n<p>The classifier with attention has a <a href=\"https://pytorch.org/docs/stable/nn.html#bcewithlogitsloss\">binary cross entropy loss</a>, where the contribution to the loss from positive examples is weighted by a factor of&nbsp;2.</p>\n<p>We clip both gradient norms according to a hyperparameter <span class=\"math\">\\(\\text{max_grad_norm}=0.5\\)</span>, and we optimize both models using RMSprop with learning rates of 0.01, RMSprop <span class=\"math\">\\(\\alpha=0.99\\)</span>, and RMSprop <span class=\"math\">\\(\\epsilon=1e^{-8}\\)</span>.</p>\n<h1>Data and code&nbsp;availability</h1>\n<h2>Environment</h2>\n<p>Code for the components of the 3 phase environment is in our <a href=\"https://github.com/frangipane/gym-minigrid\">fork</a> of <a href=\"https://github.com/maximecb/gym-minigrid\">gym-minigrid</a>.</p>\n<p>The base environment for running the experiments is defined in <a href=\"https://github.com/frangipane/rl-credit/blob/master/rl_credit/examples/environment.py\">https://github.com/frangipane/rl-credit/</a>.  Each experiment script subclasses that base environment, varying some parameter in the distractor&nbsp;phase.</p>\n<h2>Experiments</h2>\n<p>The parameters and results of the experiments are documented in the following publicly available reports on Weights and&nbsp;Biases:</p>\n<ul>\n<li><a href=\"https://app.wandb.ai/frangipane/distractor_time_delays/reports/Distractor-Gift-time-delay--VmlldzoxMjYyNzY\">Distractor phase time&nbsp;delays</a></li>\n<li><a href=\"https://app.wandb.ai/frangipane/distractor_reward_size/reports/Distractor-gift-reward-size--VmlldzoxMjcxMTI\">Distractor phase reward&nbsp;size</a></li>\n<li><a href=\"https://app.wandb.ai/frangipane/distractor_reward_variance/reports/Distractor-gift-variance--VmlldzoxMjgzNTc\">Distractor phase variance of&nbsp;rewards</a></li>\n</ul>\n<p>Code for running the experiments is at <a href=\"https://github.com/frangipane/rl-credit\">https://github.com/frangipane/rl-credit</a> in the examples/&nbsp;submodule.</p>\n<h1>Acknowledgements</h1>\n<p>Thank you to OpenAI, my OpenAI mentor J. Tworek, Microsoft for the cloud computing credits, Square for supporting my participation in the program, and my 2020 cohort of Scholars: A. Carrera, P. Mishkin, K. Ndousse, J. Orbay, A. Power (especially for the tip about future masking in transformers), and K.&nbsp;Slama.</p>\n<h1>References</h1>\n<p>[1] Hung C, Lillicrap T, Abramson J, et al. 2019. <a href=\"https://www.nature.com/articles/s41467-019-13073-w\">Optimizing agent behavior over long time scales by transporting value</a>. Nat Commun 10,&nbsp;5223.</p>\n<p>[2] Sutton R, Barto A.  2018.  <a href=\"http://incompleteideas.net/book/RLbook2018.pdf\">Reinforcement Learning: An Introduction (2nd Edition)</a>.  Cambridge (<span class=\"caps\">MA</span>): <span class=\"caps\">MIT</span>&nbsp;Press.</p>\n<p>[3] Schulman J, Moritz P, Levine S, et al. 2016. <a href=\"https://arxiv.org/abs/1506.02438\">High-Dimensional Continuous Control Using Generalized Advantage Estimation</a>. <span class=\"caps\">ICLR</span>.</p>\n<p>[4] Willems L.  <a href=\"https://github.com/lcswillems/rl-starter-files\"><span class=\"caps\">RL</span> Starter Files</a> and <a href=\"https://github.com/lcswillems/torch-ac\">Torch <span class=\"caps\">AC</span></a>.&nbsp;GitHub.</p>\n<p>[5] Chevalier-Boisvert M, Willems L, Pal S.  2018.  <a href=\"https://github.com/maximecb/gym-minigrid\">Minimalistic Gridworld Environment for OpenAI Gym</a>.&nbsp;GitHub.</p>\n<p>[6] Bloem P.  2019.  <a href=\"http://www.peterbloem.nl/blog/transformers\">Transformers from Scratch</a> [blog].  [accessed 2020 May 1].&nbsp;http://www.peterbloem.nl/blog/transformers.</p>\n<p>[7] Harutyunyan A, Dabney W, Mesnard T. 2019.  <a href=\"http://papers.nips.cc/paper/9413-hindsight-credit-assignment.pdf\">Hindsight Credit Assignment</a>. Advances in Neural Information Processing Systems 32:&nbsp;12488&#8212;12497.</p>\n<p>[8] Arjona-Medina J, Gillhofer M, Widrich M, et al. 2019.  <a href=\"https://papers.nips.cc/paper/9509-rudder-return-decomposition-for-delayed-rewards.pdf\"><span class=\"caps\">RUDDER</span>: Return Decomposition for Delayed Rewards</a>.  Advances in Neural Information Processing Systems 32:&nbsp;13566&#8212;13577.</p>\n<script type=\"text/javascript\">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}\n</script>",
  "category": [
    "",
    "",
    "",
    ""
  ]
}