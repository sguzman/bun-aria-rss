{
  "title": "Extracting rich embedding features from COCO pictures using PyTorch and ResNeXt-WSL",
  "link": "https://datasciencevademecum.com/2020/12/02/extracting-rich-embedding-features-from-pictures-using-pytorch-and-resnext-wsl/",
  "comments": "https://datasciencevademecum.com/2020/12/02/extracting-rich-embedding-features-from-pictures-using-pytorch-and-resnext-wsl/#comments",
  "dc:creator": "Gianmario",
  "pubDate": "Wed, 02 Dec 2020 12:00:00 +0000",
  "category": [
    "Classification",
    "Embedding",
    "Machine Learning",
    "Python",
    "computer vision",
    "embeddings",
    "nearest neighbors",
    "pytorch",
    "resnext",
    "tsne"
  ],
  "guid": "https://datasciencevademecum.com/?p=2511",
  "description": "<p>In this tutorial, I will show you how to leverage a powerful pre-trained convolution neural network to extract embedding vectors that can accurately describe any kind of picture in an abstract latent feature space.I will show some examples of using ResNext-WSL on the COCO dataset using the library PyTorch and other conventional tools from the &#8230; <a href=\"https://datasciencevademecum.com/2020/12/02/extracting-rich-embedding-features-from-pictures-using-pytorch-and-resnext-wsl/\" class=\"more-link\">Continue reading <span class=\"screen-reader-text\">Extracting rich embedding features from COCO pictures using PyTorch and ResNeXt-WSL</span></a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://datasciencevademecum.com/2020/12/02/extracting-rich-embedding-features-from-pictures-using-pytorch-and-resnext-wsl/\">Extracting rich embedding features from COCO pictures using PyTorch and ResNeXt-WSL</a> appeared first on <a rel=\"nofollow\" href=\"https://datasciencevademecum.com\">Vademecum of Practical Data Science</a>.</p>\n",
  "content:encoded": "\n<p>In this tutorial, I will show you how to leverage a powerful pre-trained convolution neural network to extract embedding vectors that can accurately describe any kind of picture in an abstract latent feature space.<br>I will show some examples of using ResNext-WSL on the COCO dataset using the library PyTorch and other conventional tools from the PyData stack. </p>\n\n\n\n<h2>Why ResNext-WSL?</h2>\n\n\n\n<p><a href=\"https://arxiv.org/abs/1805.00932https://arxiv.org/pdf/1611.05431.pdf\">ResNeXt</a> is the evolution of the well famous <a href=\"https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a> model that adds an additional dimension on top of it called the &#8220;cardinality&#8221; dimension. Through this improvement, the authors managed to <a href=\"https://towardsdatascience.com/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac\">beat the benchmark of ILSVRC classification task with a 15% improvement</a>. Although better models were developed in the last couple of years, it still stands on the <a href=\"https://paperswithcode.com/sota/image-classification-on-imagenet\">top leaderboard of the ImageNet task</a>, with 85.4% top-1 accuracy.</p>\n\n\n\n<figure class=\"wp-block-image is-resized\"><img decoding=\"async\" loading=\"lazy\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/Screen_Shot_2020-06-06_at_4.32.52_PM_iXtkYE5.png?resize=491%2C295&#038;ssl=1\" alt=\"ResNeXt Block Explained | Papers With Code\" width=\"491\" height=\"295\" data-recalc-dims=\"1\" /><figcaption>ResNext block, from <a href=\"https://paperswithcode.com/method/resnext-block#\">paperswithcode</a>.</figcaption></figure>\n\n\n\n<p>The weakly supervised learning (WSL) version was developed by the Facebook AI Research (FAIR) group, but it was trained on 3.5 billion public Instagram pictures in order to predict around 8000 hashtags sourced by the users. <br>This novel approach made ResNeXt-WSL my favorite choice among many other publicly available. I believe that a model trained on a broader variety of pictures can encapsulate many different topics of real-world scenarios while other pre-trained models were mostly optimized solely for their benchmark task.</p>\n\n\n\n<p>Below are some facts about the training of this massive model (<a href=\"https://medium.com/syncedreview/facebook-model-pretrained-on-billions-of-instagram-hashtags-achieves-sota-results-on-top-1-imagenet-ae8113bb3145\">source</a>):</p>\n\n\n\n<ul><li>synchronous stochastic gradient descend on 336 GPUs across 42 machines with mini-batches of 8064 images</li><li>153 billion multiply-add FLOPs and 829 million model parameters for the largest capacity model</li><li>22 days long training for the smaller capacity model (32x16d)</li></ul>\n\n\n\n<p>The model was trained with different capacities in order to reduce its size and inference time.</p>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter is-resized\"><img decoding=\"async\" loading=\"lazy\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/img_5fc62988d5c79.png?resize=466%2C218&#038;ssl=1\" alt=\"Image for post\" width=\"466\" height=\"218\" data-recalc-dims=\"1\" /><figcaption>Different ResNext models and capacities from <a href=\"https://pytorch.org/hub/facebookresearch_WSL-Images_resnext/\">PyTorch Hub</a></figcaption></figure></div>\n\n\n\n<p>After training the model on the Instagram hashtag classification task, the last layer was replaced with the ImageNet classes and fine-tuned on that task. Clearly training those models is not something every organization can afford.</p>\n\n\n\n<p>Luckily for us, a simplified version was released in the <a href=\"https://pytorch.org/hub/facebookresearch_WSL-Images_resnext/\">PyTorch hub</a>, trained on &#8220;only&#8221; 940 million public images and 1.5k hashtags. For this tutorial, we choose the smallest version of ResNext-WSL (32x8d) since it is still accurate enough on the public benchmarks but it is less memory and computationally expensive.</p>\n\n\n\n<h2>Why COCO dataset?</h2>\n\n\n\n<p>Common Objects in COntext (<a href=\"https://cocodataset.org/\">COCO</a>) is a large-scale object detection, segmentation, and captioning dataset, widely used as a benchmark for many machine learning tasks.</p>\n\n\n\n<figure class=\"wp-block-image is-resized\"><img decoding=\"async\" loading=\"lazy\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/coco-logo.png?resize=280%2C86&#038;ssl=1\" alt=\"COCO - Common Objects in Context\" width=\"280\" height=\"86\" data-recalc-dims=\"1\" /></figure>\n\n\n\n<p>For this tutorial, we would focus on the <a href=\"https://cocodataset.org/#detection-2017\">Detection 2017</a> dataset (validation fold) consisting of 5000 annotated pictures. The dataset satisfies a few desired properties:</p>\n\n\n\n<ul><li>Contains pictures from disparate context and kind of objects</li><li>The pictures are realistic with different shapes and resolutions</li><li>Each picture may contain multiple annotations</li><li>The annotations are grouped into two levels hierarchical taxonomy</li><li>There are data loaders and libraries for COCO already implemented in Python and PyTorch </li></ul>\n\n\n\n<p>The 5000 pictures are annotated with 80 categories grouped in 12 supercategories:</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"660\" height=\"284\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-2.png?resize=660%2C284&#038;ssl=1\" alt=\"\" class=\"wp-image-2514\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-2.png?resize=1024%2C440&ssl=1 1024w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-2.png?resize=300%2C129&ssl=1 300w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-2.png?resize=768%2C330&ssl=1 768w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-2.png?w=1044&ssl=1 1044w\" sizes=\"(max-width: 660px) 100vw, 660px\" data-recalc-dims=\"1\" /><figcaption>Distribution of categories in COCO 2017 validation set. Photo by the author.</figcaption></figure>\n\n\n\n<p>And below are a sample of random pictures with their annotations:</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"660\" height=\"626\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image.png?resize=660%2C626&#038;ssl=1\" alt=\"\" class=\"wp-image-2512\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image.png?w=856&ssl=1 856w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image.png?resize=300%2C285&ssl=1 300w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image.png?resize=768%2C729&ssl=1 768w\" sizes=\"(max-width: 660px) 100vw, 660px\" data-recalc-dims=\"1\" /><figcaption>Examples of annotated pictures from COCO detection 2017</figcaption></figure>\n\n\n\n<p>As we can see the pictures in the COCO dataset do represent different real-world scenarios, even though they are skewed with a lof of pictures containing people.</p>\n\n\n\n<h2>Feature Extraction</h2>\n\n\n\n<p>The ResNeXt traditional 32x4d architecture is composed by stacking multiple convolutional blocks each composed by multiple layers with 32 groups and a bottleneck width equal to 4. That is the first convolution layer with 64 filters is parallelized in 32 independent convolutions with only 4 filters each.</p>\n\n\n\n<figure class=\"wp-block-image is-resized\"><img decoding=\"async\" loading=\"lazy\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/resnext.png?resize=476%2C616&#038;ssl=1\" alt=\"ResNext | PyTorch\" width=\"476\" height=\"616\" data-recalc-dims=\"1\" /><figcaption>ResNet versus ResNext architecture comparison. Source: <a href=\"https://pytorch.org/hub/pytorch_vision_resnext/\">PyTorch Hub</a>. </figcaption></figure>\n\n\n\n<p>The smallest WSL model in the PyTorch hub uses a 32x8d architecture (bottleneck width = 8) instead. Regardless of those architecture details, what matters for us is the last average pool layer that would concatenate all of those filters into a single dimensional array of size 2048. </p>\n\n\n\n<p>The concatenated features are then supposed to be fed to the output softmax layer predicting the 1000 classes of ImageNet. Since we are not interested in the class predictions, we will drop the softmax layer and use the array of the average pool as the embedding features for our pictures.</p>\n\n\n\n<p>The embedding-only model will have the following size:</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"550\" height=\"165\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/Screen-Shot-2020-12-01-at-10.24.29-AM.png?resize=550%2C165&#038;ssl=1\" alt=\"\" class=\"wp-image-2519\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/Screen-Shot-2020-12-01-at-10.24.29-AM.png?w=550&ssl=1 550w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/Screen-Shot-2020-12-01-at-10.24.29-AM.png?resize=300%2C90&ssl=1 300w\" sizes=\"(max-width: 550px) 100vw, 550px\" data-recalc-dims=\"1\" /><figcaption>Embedding model size<br></figcaption></figure>\n\n\n\n<p>The first thing to do in order to be usable is to pre-process the input pictures in the format the model would expect. The preprocessing consists of:</p>\n\n\n\n<ul><li>Scaling to 256&#215;256</li><li>Centering crop to 224&#215;224</li><li>Normaliing with mean = [0.485, 0.456, 0.406] and stdev = [0.229, 0.224, 0.225]</li></ul>\n\n\n\n<p>I have used a Google Colab environment with GPU runtime to perform the loading, preprocessing, and inference on the 5000 pictures in around 10 minutes.</p>\n\n\n\n<p> We now have obtained a matrix of (5000, 2048) float numbers.</p>\n\n\n\n<h2>Feature exploration</h2>\n\n\n\n<p>Let&#8217;s dive into the feature space and extract some insights.<br>For example, if we consider a sample picture with a dog: </p>\n\n\n\n<figure class=\"wp-block-image size-large is-resized\"><img decoding=\"async\" loading=\"lazy\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-3.png?resize=375%2C294&#038;ssl=1\" alt=\"\" class=\"wp-image-2525\" width=\"375\" height=\"294\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-3.png?resize=1024%2C803&ssl=1 1024w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-3.png?resize=300%2C235&ssl=1 300w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-3.png?resize=768%2C603&ssl=1 768w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-3.png?resize=1536%2C1205&ssl=1 1536w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-3.png?w=1546&ssl=1 1546w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-3.png?w=1320&ssl=1 1320w\" sizes=\"(max-width: 375px) 100vw, 375px\" data-recalc-dims=\"1\" /><figcaption>Sample picture from pytorch github repository</figcaption></figure>\n\n\n\n<p> It would be embedded with the following values:</p>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"352\" height=\"368\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-4.png?resize=352%2C368&#038;ssl=1\" alt=\"\" class=\"wp-image-2526\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-4.png?w=352&ssl=1 352w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-4.png?resize=287%2C300&ssl=1 287w\" sizes=\"(max-width: 352px) 100vw, 352px\" data-recalc-dims=\"1\" /><figcaption>Distribution of embedding values for the sample picture</figcaption></figure></div>\n\n\n\n<p>We can notice that by the design of the neural network we only get non-negative values without any upper bound.</p>\n\n\n\n<p>Since not all of the 2048 features will contribute the same way, we can train a random forest classifier on the COCO multi-label annotation task in order to estimate the feature importance.<br>Let&#8217;s zoom into the most important features extracted from the random forest classifier, whose first 25 are: [ 12, 3, 2011, 1151, 28, 1715, 1004, 583, 479, 1377, 1505, 1690, 1028, 1831, 1704, 466, 634, 1329, 2003, 176, 485, 1066, 1669, 445, 1227].</p>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"660\" height=\"590\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-7.png?resize=660%2C590&#038;ssl=1\" alt=\"\" class=\"wp-image-2529\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-7.png?w=812&ssl=1 812w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-7.png?resize=300%2C268&ssl=1 300w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-7.png?resize=768%2C687&ssl=1 768w\" sizes=\"(max-width: 660px) 100vw, 660px\" data-recalc-dims=\"1\" /><figcaption>Correlation matrix between pairs of features</figcaption></figure></div>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"660\" height=\"660\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-8.png?resize=660%2C660&#038;ssl=1\" alt=\"\" class=\"wp-image-2530\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-8.png?resize=1024%2C1024&ssl=1 1024w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-8.png?resize=300%2C300&ssl=1 300w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-8.png?resize=150%2C150&ssl=1 150w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-8.png?resize=768%2C768&ssl=1 768w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-8.png?resize=1536%2C1536&ssl=1 1536w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-8.png?w=1768&ssl=1 1768w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-8.png?w=1320&ssl=1 1320w\" sizes=\"(max-width: 660px) 100vw, 660px\" data-recalc-dims=\"1\" /><figcaption>Correlation scatter plots between pairs of features</figcaption></figure></div>\n\n\n\n<p>We can derive a couple of properties:</p>\n\n\n\n<ol><li>The feature space is slightly sparse (many feature values are close to 0)</li><li>The feature values for each dimension follow an exponential distribution on our dataset</li></ol>\n\n\n\n<p>The measure the first point we can count what percentage of the dimensions are used by each picture (value > 0.01): </p>\n\n\n\n<figure class=\"wp-block-image size-large is-resized\"><img decoding=\"async\" loading=\"lazy\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-9.png?resize=354%2C370&#038;ssl=1\" alt=\"\" class=\"wp-image-2531\" width=\"354\" height=\"370\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-9.png?w=352&ssl=1 352w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-9.png?resize=287%2C300&ssl=1 287w\" sizes=\"(max-width: 354px) 100vw, 354px\" data-recalc-dims=\"1\" /><figcaption>Distribution of the percentage of dimensions activated in each picture</figcaption></figure>\n\n\n\n<p>Only 40% of dimensions are used in average by each picture, where we can assume the higher the value the more the topics embedded in the picture.</p>\n\n\n\n<p>In order to confirm the exponential distribution hypothesis we can plot the mean of each dimensions against its standard deviation across the dataset:</p>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"424\" height=\"424\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-10.png?resize=424%2C424&#038;ssl=1\" alt=\"\" class=\"wp-image-2532\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-10.png?w=424&ssl=1 424w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-10.png?resize=300%2C300&ssl=1 300w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-10.png?resize=150%2C150&ssl=1 150w\" sizes=\"(max-width: 424px) 100vw, 424px\" data-recalc-dims=\"1\" /><figcaption>Mean vs standard deviation of the 2048 dimensions on COCO pictures</figcaption></figure></div>\n\n\n\n<p>The proportionality between mean and standard deviation gives us a hint that the dimensions tend to follow an exponential-like distribution on our dataset.</p>\n\n\n\n<h3>3D visualization</h3>\n\n\n\n<p>We can apply a few dimensionality reduction techniques to reduce the embedding space into 3 dimensions and use the supercategory to highlight the pictures that belong to the same taxonomy.</p>\n\n\n\n<p>We will focus on 3 unsupervised techniques: PCA, t-SNE, and UMAP. For a more exhaustive comparison of their differences, I recommend the following reading: <a href=\"https://towardsdatascience.com/dimensionality-reduction-for-data-visualization-pca-vs-tsne-vs-umap-be4aa7b1cb29\">Dimensionality Reduction for Data Visualization: PCA vs TSNE vs UMAP vs LDA</a>.</p>\n\n\n\n<figure class=\"wp-block-image size-large is-resized\"><img decoding=\"async\" loading=\"lazy\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-11.png?resize=660%2C405&#038;ssl=1\" alt=\"\" class=\"wp-image-2538\" width=\"660\" height=\"405\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-11.png?w=907&ssl=1 907w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-11.png?resize=300%2C185&ssl=1 300w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-11.png?resize=768%2C472&ssl=1 768w\" sizes=\"(max-width: 660px) 100vw, 660px\" data-recalc-dims=\"1\" /><figcaption>3D visualization using PCA</figcaption></figure>\n\n\n\n<figure class=\"wp-block-image size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"660\" height=\"406\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-13.png?resize=660%2C406&#038;ssl=1\" alt=\"\" class=\"wp-image-2540\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-13.png?w=907&ssl=1 907w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-13.png?resize=300%2C185&ssl=1 300w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-13.png?resize=768%2C472&ssl=1 768w\" sizes=\"(max-width: 660px) 100vw, 660px\" data-recalc-dims=\"1\" /><figcaption>3D visualization using TSNE</figcaption></figure>\n\n\n\n<figure class=\"wp-block-image size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"660\" height=\"406\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-12.png?resize=660%2C406&#038;ssl=1\" alt=\"\" class=\"wp-image-2539\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-12.png?w=907&ssl=1 907w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-12.png?resize=300%2C185&ssl=1 300w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-12.png?resize=768%2C472&ssl=1 768w\" sizes=\"(max-width: 660px) 100vw, 660px\" data-recalc-dims=\"1\" /><figcaption>3D visualization using UMAP</figcaption></figure>\n\n\n\n<p>To make the exploration more fun I have deployed a TensorBoard using a sample of pictures in a sprite image:</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"660\" height=\"650\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-14.png?resize=660%2C650&#038;ssl=1\" alt=\"\" class=\"wp-image-2541\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-14.png?w=698&ssl=1 698w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-14.png?resize=300%2C295&ssl=1 300w\" sizes=\"(max-width: 660px) 100vw, 660px\" data-recalc-dims=\"1\" /><figcaption>Sprite image for interactive visualization in TensorBoard</figcaption></figure>\n\n\n\n<p>And the TSNE projection looked like this:</p>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"600\" height=\"270\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/ezgif.com-gif-maker-2.gif?resize=600%2C270&#038;ssl=1\" alt=\"\" class=\"wp-image-2542\" data-recalc-dims=\"1\"/><figcaption>TSNE projection animation</figcaption></figure></div>\n\n\n\n<p>The interactive projector of the TensorBoard is a powerful tool for exploring the embedding space and spot correlations and interesting manifolds of points.</p>\n\n\n\n<h2>Nearest Neighbors</h2>\n\n\n\n<p>What can we actually do now with this embedding space? A relevant application would be to find similar pictures. Thus, I have selected a few random pictures and plotted the 15 closest ones based on the cosine similarity in the 2048-dimensional embedding space.</p>\n\n\n\n<p>The results are astonishing:</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"660\" height=\"652\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/bike-neighbours.png?resize=660%2C652&#038;ssl=1\" alt=\"\" class=\"wp-image-2553\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/bike-neighbours.png?resize=1024%2C1011&ssl=1 1024w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/bike-neighbours.png?resize=300%2C296&ssl=1 300w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/bike-neighbours.png?resize=768%2C759&ssl=1 768w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/bike-neighbours.png?resize=1536%2C1517&ssl=1 1536w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/bike-neighbours.png?w=1718&ssl=1 1718w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/bike-neighbours.png?w=1320&ssl=1 1320w\" sizes=\"(max-width: 660px) 100vw, 660px\" data-recalc-dims=\"1\" /><figcaption>&#8220;Bikes&#8221; neighborhood</figcaption></figure>\n\n\n\n<figure class=\"wp-block-image size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"660\" height=\"635\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/tennis-neighbours.png?resize=660%2C635&#038;ssl=1\" alt=\"\" class=\"wp-image-2554\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/tennis-neighbours.png?resize=1024%2C985&ssl=1 1024w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/tennis-neighbours.png?resize=300%2C289&ssl=1 300w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/tennis-neighbours.png?resize=768%2C739&ssl=1 768w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/tennis-neighbours.png?w=1353&ssl=1 1353w\" sizes=\"(max-width: 660px) 100vw, 660px\" data-recalc-dims=\"1\" /><figcaption>&#8220;Tennis players&#8221; neighborhood</figcaption></figure>\n\n\n\n<figure class=\"wp-block-image size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"660\" height=\"607\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/livingroom-neighbours.png?resize=660%2C607&#038;ssl=1\" alt=\"\" class=\"wp-image-2555\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/livingroom-neighbours.png?resize=1024%2C942&ssl=1 1024w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/livingroom-neighbours.png?resize=300%2C276&ssl=1 300w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/livingroom-neighbours.png?resize=768%2C707&ssl=1 768w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/livingroom-neighbours.png?w=1379&ssl=1 1379w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/livingroom-neighbours.png?w=1320&ssl=1 1320w\" sizes=\"(max-width: 660px) 100vw, 660px\" data-recalc-dims=\"1\" /><figcaption>&#8220;Living room with sofas&#8221; neighborhood</figcaption></figure>\n\n\n\n<figure class=\"wp-block-image size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"660\" height=\"630\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/skiing-neighbours.png?resize=660%2C630&#038;ssl=1\" alt=\"\" class=\"wp-image-2556\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/skiing-neighbours.png?resize=1024%2C977&ssl=1 1024w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/skiing-neighbours.png?resize=300%2C286&ssl=1 300w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/skiing-neighbours.png?resize=768%2C733&ssl=1 768w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/skiing-neighbours.png?w=1366&ssl=1 1366w\" sizes=\"(max-width: 660px) 100vw, 660px\" data-recalc-dims=\"1\" /><figcaption>&#8220;People skiing&#8221; neighborhood</figcaption></figure>\n\n\n\n<figure class=\"wp-block-image size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"660\" height=\"604\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/cats-neighbours.png?resize=660%2C604&#038;ssl=1\" alt=\"\" class=\"wp-image-2557\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/cats-neighbours.png?resize=1024%2C937&ssl=1 1024w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/cats-neighbours.png?resize=300%2C274&ssl=1 300w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/cats-neighbours.png?resize=768%2C703&ssl=1 768w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/cats-neighbours.png?w=1387&ssl=1 1387w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/cats-neighbours.png?w=1320&ssl=1 1320w\" sizes=\"(max-width: 660px) 100vw, 660px\" data-recalc-dims=\"1\" /><figcaption>&#8220;Cats on a laptop&#8221; neighborhood</figcaption></figure>\n\n\n\n<figure class=\"wp-block-image size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"660\" height=\"642\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/food-neighbours.png?resize=660%2C642&#038;ssl=1\" alt=\"\" class=\"wp-image-2558\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/food-neighbours.png?resize=1024%2C996&ssl=1 1024w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/food-neighbours.png?resize=300%2C292&ssl=1 300w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/food-neighbours.png?resize=768%2C747&ssl=1 768w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/food-neighbours.png?w=1353&ssl=1 1353w\" sizes=\"(max-width: 660px) 100vw, 660px\" data-recalc-dims=\"1\" /><figcaption>&#8220;Food on a plate&#8221; neighborhood</figcaption></figure>\n\n\n\n<figure class=\"wp-block-image size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"660\" height=\"628\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/animals-neighbours.png?resize=660%2C628&#038;ssl=1\" alt=\"\" class=\"wp-image-2559\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/animals-neighbours.png?resize=1024%2C974&ssl=1 1024w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/animals-neighbours.png?resize=300%2C285&ssl=1 300w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/animals-neighbours.png?resize=768%2C730&ssl=1 768w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/animals-neighbours.png?w=1404&ssl=1 1404w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/animals-neighbours.png?w=1320&ssl=1 1320w\" sizes=\"(max-width: 660px) 100vw, 660px\" data-recalc-dims=\"1\" /><figcaption>&#8220;Large animals exhibition&#8221; neighborhood</figcaption></figure>\n\n\n\n<p>Not just the embedding space is able to accurately group pictures that do are very similar in terms of objects and context, but it can also correctly discriminate the entropy on each kind of picture based on the cosine distance. For example the neighborhoods of &#8220;tennis players&#8221; and &#8220;people skiing&#8221; have a much lower distance from the query picture compared to the other more entropic neighborhoods.</p>\n\n\n\n<h2>Conclusions</h2>\n\n\n\n<p>In this tutorial, we have seen how easily, and without having to train any model, we can leverage the state-of-the-art to extract rich embedding features from pictures of different natures. We have proven the ability of the embedding space to capture a lot of fine details including both the objects and the surrounding context. The most salient part was showing that the embedding features can accurately find similar pictures. The measure of similarity and the latent manifold structures can enable a lot of downstream applications.</p>\n\n\n\n<p>In the next articles, we will see how to discover and define those latent topics represented by the manifolds in the embedding space, to learn how to cluster those pictures, and to learn advanced averaging techniques for document embedding. <br>Stay tuned!</p>\n\n\n\n<p>You can find the code and the notebooks at <a href=\"https://github.com/gm-spacagna/docem\" rel=\"noreferrer noopener\" target=\"_blank\">https://github.com/gm-spacagna/docem</a>.</p>\n\n\n\n<p>If you are interested in extracting embedding features from text data, you can check out this other article: <a href=\"https://datasciencevademecum.com/2020/05/21/embedding-billions-of-text-documents-using-tensorflow-universal-sentence-encoder-on-top-of-spark-emr/\">Embedding billions of text documents using Tensorflow Universal Sentence Encoder and Spark EMR</a>.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://datasciencevademecum.com/2020/12/02/extracting-rich-embedding-features-from-pictures-using-pytorch-and-resnext-wsl/\">Extracting rich embedding features from COCO pictures using PyTorch and ResNeXt-WSL</a> appeared first on <a rel=\"nofollow\" href=\"https://datasciencevademecum.com\">Vademecum of Practical Data Science</a>.</p>\n",
  "wfw:commentRss": "https://datasciencevademecum.com/2020/12/02/extracting-rich-embedding-features-from-pictures-using-pytorch-and-resnext-wsl/feed/",
  "slash:comments": 1,
  "post-id": 2511
}