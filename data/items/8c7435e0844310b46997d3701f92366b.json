{
  "title": "Stochastic Computation Graphs",
  "link": "",
  "published": "2015-07-24T22:00:00+01:00",
  "updated": "2015-07-24T22:00:00+01:00",
  "author": {
    "name": "Sebastian Nowozin"
  },
  "id": "tag:www.nowozin.net,2015-07-24:/sebastian/blog/stochastic-computation-graphs.html",
  "summary": "<p>This post is about a recent arXiv submission entitled\n<a href=\"http://arxiv.org/abs/1506.05254\">Gradient Estimation Using Stochastic Computation\nGraphs</a>, and authored by\n<a href=\"http://www.eecs.berkeley.edu/~joschu/\">John Schulman</a>,\nNicolas Heess,\n<a href=\"http://thphn.com/\">Theophane Weber</a>, and\n<a href=\"http://www.cs.berkeley.edu/~pabbeel/\">Pieter Abbeel</a>.</p>\n<p>In a nutshell this paper generalizes the <a href=\"https://en.wikipedia.org/wiki/Backpropagation\">backpropagation\nalgorithm</a> to allow\n<em>differentiation through â€¦</em></p>",
  "content": "<p>This post is about a recent arXiv submission entitled\n<a href=\"http://arxiv.org/abs/1506.05254\">Gradient Estimation Using Stochastic Computation\nGraphs</a>, and authored by\n<a href=\"http://www.eecs.berkeley.edu/~joschu/\">John Schulman</a>,\nNicolas Heess,\n<a href=\"http://thphn.com/\">Theophane Weber</a>, and\n<a href=\"http://www.cs.berkeley.edu/~pabbeel/\">Pieter Abbeel</a>.</p>\n<p>In a nutshell this paper generalizes the <a href=\"https://en.wikipedia.org/wiki/Backpropagation\">backpropagation\nalgorithm</a> to allow\n<em>differentiation through expectations</em>, that is, to compute unbiased estimates\nof</p>\n<div class=\"math\">$$\\frac{\\partial}{\\partial \\theta} \\mathbb{E}_{x \\sim q(x|\\theta)}[f(x,\\theta)].$$</div>\n<p>The paper also provides a nice calculus on directed graphs that allows quick\nderivation of unbiased gradient estimates.\nThe basic technical results in the paper have been known and used in various\ncommunities before and the arXiv submission properly discusses these.</p>\n<p>But dismissing the paper as non novel would miss the point in a similar way as\nmissing the point when stating that backpropagation is ``just an application\nof the chain rule of differentiation''.\nInstead, the contribution of the current paper is in the practical utility of\nthe graphical calculus and a rich catalogue of machine learning problems where\nthe computation of unbiased gradients of expectations is useful.</p>\n<p>In typical statistical point estimation tasks\n<a href=\"https://en.wikipedia.org/wiki/Bias_of_an_estimator\"><em>unbiasedness</em></a> is often\nnot quite as important compared to expected risk.\nHowever, here it is crucial.\nThis is because the applications where stochastic computation graphs are\nuseful involve <em>optimization</em> over <span class=\"math\">\\(\\theta\\)</span> and <a href=\"https://en.wikipedia.org/wiki/Stochastic_approximation\">stochastic\napproximation</a>\nmethods such as <a href=\"https://en.wikipedia.org/wiki/Stochastic_gradient_descent\">stochastic gradient\nmethods</a> can only\nbe justified theoretically in the case of unbiased gradient estimates.</p>\n<h3>A Neat Derivative Trick</h3>\n<p>To get an idea of the flavour of derivatives involving expectations, let us\nlook at a simpler case explained in Section 2.1 of the paper.\nThe proof of that case also contains a neat trick worth knowing.\nThe case is as above but inside the expectation we have only <span class=\"math\">\\(f(x)\\)</span> instead of <span class=\"math\">\\(f(x,\\theta)\\)</span>.\nThe ``trick'' is in the identity (obvious in retrospect),\n</p>\n<div class=\"math\">$$\\frac{\\partial}{\\partial \\theta} p(x|\\theta) =\np(x|\\theta) \\frac{\\partial}{\\partial \\theta} \\log p(x|\\theta).$$</div>\n<p>This allows to establish\n</p>\n<div class=\"math\">\\begin{eqnarray}\n\\frac{\\partial}{\\partial \\theta} \\mathbb{E}_{x \\sim p(x|\\theta)}[f(x)]\n&amp; = &amp; \\frac{\\partial}{\\partial \\theta} \\int p(x|\\theta) f(x) \\,\\textrm{d}x\\nonumber\\\\\n&amp; = &amp; \\int \\frac{\\partial}{\\partial \\theta} p(x|\\theta) f(x) \\,\\textrm{d}x\\nonumber\\\\\n&amp; = &amp; \\int p(x|\\theta) f(x) \\frac{\\partial}{\\partial \\theta} \\log p(x|\\theta) \\,\\textrm{d}x\\nonumber\\\\\n&amp; = &amp; \\mathbb{E}_{x \\sim p(x|\\theta)}[f(x) \\frac{\\partial}{\\partial \\theta} \\log p(x|\\theta)].\\nonumber\n\\end{eqnarray}</div>\n<p>In this case the derivation was straightforward but for multiple expectations\na derivation based on this elementary definition of the expectation is\ncumbersome and error-prone.  Stochastic computation graphs allow a much\nquicker derivation of the derivative.</p>\n<h2>Stochastic Computation Graphs</h2>\n<p>Stochastic computation graphs are directed acyclic graphs that encode the\ndependency structure of computation to be performed.  The graphical notation\ngeneralizes directed graphical models.\nHere is an example graph.</p>\n<p><img alt=\"Stochastic computation graph of problem (1) in Schulman et al.\" src=\"http://www.nowozin.net/sebastian/blog/images/stochastic-computation-graphs-example1.svg\"></p>\n<p>There are three (or four) types of nodes in a stochastic computation graph:</p>\n<ol>\n<li><em>Input nodes</em>.  These are the fixed parameters we would like to compute the\nderivative of.  In the example graph, this is the <span class=\"math\">\\(\\theta\\)</span> node and they are\ndrawn without any container.  While technically it is possible to have graphs\nwithout input nodes, in order to compute gradients the graph should include at\nleast one input node.</li>\n<li><em>Deterministic nodes</em>.  These compute a deterministic function of their\nparents.  In the above graph this is the case for the <span class=\"math\">\\(x\\)</span> and <span class=\"math\">\\(f\\)</span> nodes.</li>\n<li><em>Stochastic nodes</em>.  These nodes specify a random variable through a\ndistribution conditional on their parents.  In the above graph this is true\nfor the <span class=\"math\">\\(y\\)</span> node, and the circle mirrors the notation used in directed\ngraphical models.</li>\n<li><em>Cost nodes</em>.  These are a subset of the deterministic nodes in the graph\nwhose range are the real numbers.  In the above graph the node <span class=\"math\">\\(f\\)</span> is a cost\nnode.  I draw them shaded, this is not the case in the original paper.</li>\n</ol>\n<p>The entire stochastic computation graph specifies a single objective function\nwhose domain are the input nodes and whose scalar objective is the sum of all\ncost nodes.\nThe sum of all cost nodes is taken as an expectation over all stochastic nodes\nin the graph.</p>\n<p>Therefore the above graph has the objective function\n</p>\n<div class=\"math\">$$F(\\theta) = \\mathbb{E}_{y \\sim p(y|x(\\theta))}[f(y)].$$</div>\n<h3>Derivative Calculus</h3>\n<p>The notation used in the paper is a bit heavy and (for my taste at least) a\nbit too custom, but here it is.\nLet <span class=\"math\">\\(\\Theta\\)</span> be the set of input nodes, <span class=\"math\">\\(\\mathcal{C}\\)</span> the set of cost nodes,\nand <span class=\"math\">\\(\\mathcal{S}\\)</span> be the set of stochastic nodes.\nThe notation <span class=\"math\">\\(u \\prec v\\)</span> denotes that there exist a directed path from <span class=\"math\">\\(u\\)</span> to\n<span class=\"math\">\\(v\\)</span> in the graph.\nThe notation <span class=\"math\">\\(u \\prec^D v\\)</span> denotes that there exist a path whose nodes are all\ndeterministic with the exception of the last node <span class=\"math\">\\(v\\)</span> which may be of any\ntype.\nWe write <span class=\"math\">\\(\\hat{c}\\)</span> for a sample realization of a cost node <span class=\"math\">\\(c\\)</span>.\nThe final notation needed for the result is\n</p>\n<div class=\"math\">$$\\textrm{DEPS}_v = \\{ w \\in \\Theta \\cup \\mathcal{S} | w \\prec^D v\\}.$$</div>\n<p>The key result of the paper, Theorem 1, is now stated as follows:\n</p>\n<div class=\"math\">$$\\frac{\\partial}{\\partial \\theta} \\mathbb{E}\\left[\\sum_{c \\in \\mathcal{C}} c\\right]\n= \\mathbb{E}\\Bigg[\\underbrace{\\sum_{w \\in \\mathcal{S}, \\theta \\prec^D w} \\left(\n\\frac{\\partial}{\\partial \\theta} \\log p(w|\\textrm{DEPS}_w)\n\\right) \\sum_{c \\in \\mathcal{C}, w \\prec c} \\hat{c}}_{\\textrm{(A)}}\n+ \\underbrace{\\sum_{c \\in \\mathcal{C}, \\theta \\prec^D c} \\frac{\\partial}{\\partial \\theta}\nc(\\textrm{DEPS}_c)}_{\\textrm{(B)}}\\Bigg].$$</div>\n<p>The two parts, (A) and (B) can be interpreted as follows.  If we only have\ndeterministic computation so that <span class=\"math\">\\(\\mathcal{S} = \\emptyset\\)</span>, as in an ordinary\nfeedforward neural network for example, the part (B) is just the ordinary\nderivative and we have to apply the chain rule to that expression.\nThe part (A) originates from each stochastic node and the consequences that\noriginate from the stochastic nodes is absorbed in the sample realizations\n<span class=\"math\">\\(\\hat{c}\\)</span>.</p>\n<p>It takes a bit of practice to apply Theorem 1 quickly to a given graph, and I\nfound it easier to instead manually, on a piece of paper, executing\nAlgorithm 1 of the paper, which generalizes backpropagation and builds the\nderivative node by node by traversing the graph backwards.</p>\n<h2>Example</h2>\n<p>To understand the basic technique I illustrate the stochastic computation\ngraph technique on the concrete graph above, which is problem (1) in the paper\n(Section 2.3), but I make the example concrete.</p>\n<p><img alt=\"Stochastic computation graph of problem (1) in Schulman et al.\" src=\"http://www.nowozin.net/sebastian/blog/images/stochastic-computation-graphs-example1.svg\"></p>\n<div class=\"math\">$$x(\\theta) = (\\theta-1)^2,$$</div>\n<div class=\"math\">$$y(x) \\sim \\mathcal{N}(x,1),$$</div>\n<div class=\"math\">$$f(y) = \\left(y-\\frac{5}{2}\\right)^2.$$</div>\n<p>Before we apply Theorem 1 to the graph, here is how the problem actually looks\nlike.  First, the objective <span class=\"math\">\\(F(\\theta) = \\mathbb{E}_{y \\sim p(y|x(\\theta))}[f(y)]\\)</span>.\nThis objective is just an ordinary one-dimensional deterministic function.</p>\n<p><img alt=\"True objective to be minimized\" src=\"http://www.nowozin.net/sebastian/blog/images/stochastic-computation-graphs-Ef.svg\"></p>\n<p>The true gradient of the objective is also just an ordinary function.\nYou can see three zero-crossings at approximately -0.6, 1, and 2.6,\ncorresponding to two local minima and a saddle-point of the objective function.</p>\n<p><img alt=\"True gradient of objective\" src=\"http://www.nowozin.net/sebastian/blog/images/stochastic-computation-graphs-grad.svg\"></p>\n<p>For this simple example we can find a closed form expression for <span class=\"math\">\\(F(\\theta)\\)</span>,\nbut in general stochastic computation graphs we are not able to evaluate\n<span class=\"math\">\\(F(\\theta)\\)</span> and instead only sample values <span class=\"math\">\\(\\hat{F}_1, \\hat{F}_2, \\dots\\)</span> which\nare unbiased estimates of the true <span class=\"math\">\\(F(\\theta)\\)</span>.\nBy taking averages of a few samples, say of a 100 samples, we can improve the\naccuracy of our estimates.\nIn order to minimize <span class=\"math\">\\(F(\\theta)\\)</span> over <span class=\"math\">\\(\\theta\\)</span> our goal is to sample unbiased\ngradients as well.\nThe unbiased sample gradients look as follows, for <span class=\"math\">\\(1\\)</span> sample (shown in green)\nand for averages of a <span class=\"math\">\\(100\\)</span> samples (shown in red), evaluated at a 100 points\nequispaced along the <span class=\"math\">\\(\\theta\\)</span> axis shown.</p>\n<p><img alt=\"Sample gradient of objective\" src=\"http://www.nowozin.net/sebastian/blog/images/stochastic-computation-graphs-gradsample.svg\"></p>\n<p>To derive the unbiased gradient estimate we apply Theorem 1.\nFrom the summation (A) we will only have one term because our graph contains\nonly one stochastic node, namely <span class=\"math\">\\(y\\)</span>.\nWe will not have any term from (B) as there is no deterministic path from\n<span class=\"math\">\\(\\theta\\)</span> to <span class=\"math\">\\(f\\)</span>.\nTherefore we have</p>\n<div class=\"math\">$$\\frac{\\partial}{\\partial \\theta} \\mathbb{E}_{y \\sim p(y|x(\\theta))}[f(y)]\n= \\mathbb{E}_{y \\sim p(y|x(\\theta))}\\left[\\frac{\\partial}{\\partial \\theta}\n\\log p(y|x(\\theta)) \\hat{f}\\right].$$</div>\n<p>For the logarithm we need to differentiate the log-likelihood of the Normal\ndistribution and compute \n</p>\n<div class=\"math\">\\begin{eqnarray}\n\\frac{\\partial x}{\\partial \\theta} \\frac{\\partial}{\\partial x} \\log p(y|x(\\theta))\n&amp; = &amp;\n\\frac{\\partial x}{\\partial \\theta} \\frac{\\partial}{\\partial x} \\left[\n- \\frac{(y-x(\\theta))^2}{2} - \\frac{1}{2} \\log 2\\pi \\right]\\nonumber\\\\\n&amp; = &amp;\n\\frac{\\partial x}{\\partial \\theta} (y-x(\\theta))\\nonumber\\\\\n&amp; = &amp;\n2(\\theta - 1)(y - x(\\theta)).\\nonumber\n\\end{eqnarray}</div>\n<p>So the overall unbiased gradient estimator is\n</p>\n<div class=\"math\">$$\\mathbb{E}\\left[\\frac{\\partial}{\\partial \\theta} \\log p(y|x(\\theta)) \\hat{f}\\right]\n= \\mathbb{E}[2(\\theta-1)(\\hat{y}-\\hat{x}) \\hat{f}].$$</div>\n<p>\nAnd the last expression in the expectation is the estimate for a single sample\nrealization.</p>\n<h2>Variational Bayesian Neural Networks</h2>\n<p>One important application of being able to compute gradients of expectation\nobjectives is the approximate variational Bayesian posterior inference of\nneural network parameters.</p>\n<p>The original pioneering work of applying variational Bayes (aka mean field\ninference) to neural network learning is <a href=\"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.119.5194\">this 1993 paper of Hinton and van\nKamp</a>.\nRecently this has made a revival in particular through the appearance of\n<a href=\"http://www.cs.princeton.edu/courses/archive/fall11/cos597C/reading/Blei2011.pdf\">stochastic variational inference\nmethods</a>\naround 2011, including a <a href=\"http://papers.nips.cc/paper/4329-practical-variational-inference-for-neural-networks-spotlight.pdf\">paper of Alex\nGraves</a>.\nMany works followed up on this lead, for example <a href=\"http://dpkingma.com/wordpress/wp-content/uploads/2014/10/iclr14_vae.pdf\">Kingma and\nWelling</a>,\n<a href=\"http://arxiv.org/abs/1401.4082\">Rezende et al., ICML 2014</a>, <a href=\"http://arxiv.org/abs/1505.05424\">Blundell et al.,\nICML 2015</a>, and <a href=\"http://arxiv.org/abs/1402.0030\">Mnih and\nGregor</a>.  They use different estimators of the\ngradient with varying quality and the SCG paper provides a nice overview of\nthe bigger picture.</p>\n<p>In any case, here is a visualization of prototypical variational Bayes\nlearning for feedforward neural networks.  A normal feedforward neural network\ntraining objective yields the following computation graph, without any\nstochastic nodes.</p>\n<p><img alt=\"Feedforward neural network training objective computation graph\" src=\"http://www.nowozin.net/sebastian/blog/images/scg-nn1.svg\"></p>\n<p>Here we have a fixed weight vector <span class=\"math\">\\(w\\)</span> with a regularizer <span class=\"math\">\\(R(w)\\)</span>.\nWe have <span class=\"math\">\\(n\\)</span> training instances and each input <span class=\"math\">\\(x_i\\)</span> produces a network output,\n<span class=\"math\">\\(P_i(x_i,w)\\)</span>, for example a distribution over class labels.  Together with a\nknown ground truth label <span class=\"math\">\\(y_i\\)</span> this yields a loss <span class=\"math\">\\(\\ell_i(P_i,y_i)\\)</span>, for\nexample the cross-entropy loss.\nIf we use a likelihood based loss and a regularizer derived from a prior, i.e.\n<span class=\"math\">\\(R(w)=-\\log P(w)\\)</span> the training objective becomes just regularized maximum\nlikelihood estimation.</p>\n<div class=\"math\">$$F(w) = -\\log P(w) - \\sum_{i=1}^n \\log P(y_i|x_i;w).$$</div>\n<p>The <a href=\"https://en.wikipedia.org/wiki/Variational_Bayesian_methods\">variational\nBayes</a> training\nobjective yields the following slightly extended <em>stochastic</em> computation\ngraph.</p>\n<p><img alt=\"Variational Bayes neural network training objective stochastic computation graph\" src=\"http://www.nowozin.net/sebastian/blog/images/scg-nn2.svg\"></p>\n<p>Here <span class=\"math\">\\(w\\)</span> is still a network parameter, but it is now a stochastic vector, <span class=\"math\">\\(w\n\\sim Q(w|\\theta)\\)</span> and <span class=\"math\">\\(\\theta\\)</span> becomes the parameter we would like to learn.\nThe additional cost node <span class=\"math\">\\(H\\)</span> arises from the entropy of the approximating\nposterior distribution <span class=\"math\">\\(Q\\)</span>.  (An interesting detail: in principle we would not\nneed an arrow <span class=\"math\">\\(w \\to H\\)</span> because we can compute <span class=\"math\">\\(H(Q)\\)</span>.  However, if we allow\nthis arrow, then we can use a Monte Carlo approximation of the entropy for\napproximating families which do not have an analytic entropy expression.)\nThe training objective becomes:</p>\n<div class=\"math\">$$F(\\theta) = \\mathbb{E}_{w \\sim Q(w|\\theta)}\\left[-\\log P(w) + \\log Q(w|\\theta) - \\sum_{i=1}^n \\log P(y_i|x_i;w)\\right].$$</div>\n<p>The stochastic computation graph rules can now be used to derive the unbiased\ngradient estimate.</p>\n<div class=\"math\">$$\\frac{\\partial}{\\partial \\theta} F(\\theta) =\n\\mathbb{E}_{w \\sim Q(w|\\theta)}\\left[\n\\frac{\\partial}{\\partial \\theta} \\log Q(w|\\theta) \\left(\n-\\log P(w) + \\log Q(w|\\theta) - \\sum_{i=1}^n \\log P(y_i|x_i;w)\n\\right)\\right].$$</div>\n<p>This is now quite practical: the expectation can be approximated using simple\nMonte Carlo samples of <span class=\"math\">\\(w\\)</span> values using the current approximating posterior\n<span class=\"math\">\\(Q(w|\\theta)\\)</span>.  Because the gradient is unbiased we can improve the\napproximation by running standard stochastic gradient methods.</p>\n<h2>Additional Applications</h2>\n<p>The paper contains a large number of machine learning applications, but there\nare many others.\nHere is one I find useful.</p>\n<p><img alt=\"Experimental design stochastic computation graph\" src=\"http://www.nowozin.net/sebastian/blog/images/scg-experimental-design.svg\"></p>\n<p><em>Experimental design.</em>  In <a href=\"http://eu.wiley.com/WileyCDA/WileyTitle/productCd-047149657X.html\">Bayesian experimental\ndesign</a> we\nmake a choice that influences our future measurements and we would like to\nmake these choices in such a way that we will maximize the future expected\nutility or minimize expected loss.\nFor this we use a model of how our choices relate to the information we will\ncapture and to how valuable these information will be.  Because this is just\ndecision theory and the idea is general, let me be more concrete.  Let us\nassume the objective function\n</p>\n<div class=\"math\">$$\\mathbb{E}_{z \\sim p(z)}[\\mathbb{E}_{x \\sim p(x|z,\\theta)}[\\ell(\\tilde{z}(x,\\theta), z)]].$$</div>\n<p>\nHere <span class=\"math\">\\(\\theta\\)</span> is our design parameter, <span class=\"math\">\\(z\\)</span> is the true state we are interested\nin with a prior <span class=\"math\">\\(p(z)\\)</span>.\nThe measurement process produces <span class=\"math\">\\(x \\sim p(x|z,\\theta)\\)</span>.\nWe have an estimator <span class=\"math\">\\(\\tilde{z}(x,\\theta)\\)</span> and a loss function which compares\nthe estimated value against the true state.\nThe full objective function is then the expected loss of our estimator\n<span class=\"math\">\\(\\tilde{z}\\)</span> as a function of the design parameters <span class=\"math\">\\(\\theta\\)</span>.\nThe above expression looks a bit convoluted but this structure appears\nfrequently when the type of information that is collected can be controlled.\nOne example application of this: <span class=\"math\">\\(z\\)</span> could represent user behaviour and\n<span class=\"math\">\\(\\theta\\)</span> some subset of questions we could ask that user to learn more about\nhis behaviour.  We then assume a model <span class=\"math\">\\(p(x|z,\\theta)\\)</span> of how the user would\nprovide answers <span class=\"math\">\\(x\\)</span> given questions <span class=\"math\">\\(\\theta\\)</span> and behaviour <span class=\"math\">\\(z\\)</span>.  This allows\nus to build an estimator <span class=\"math\">\\(\\tilde{z}(x,\\theta)\\)</span>.  The design objective then tries\nto find the most informative set of questions to ask.</p>\n<p><em>Acknowledgements</em>.  I thank <a href=\"http://ei.is.tuebingen.mpg.de/person/mschober\">Michael\nSchober</a> for discussions about\nthe paper and Nicolas Heess for feedback on this article.</p>\n<script type=\"text/javascript\">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}\n</script>",
  "category": ""
}