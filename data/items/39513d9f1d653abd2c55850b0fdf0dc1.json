{
  "title": "CycleGAN: Unpaired Image-to-Image Translation (Part 1)",
  "link": "https://pyimagesearch.com/2022/09/12/cyclegan-unpaired-image-to-image-translation-part-1/",
  "dc:creator": "Shivam Chandhok",
  "pubDate": "Mon, 12 Sep 2022 13:00:00 +0000",
  "category": [
    "Computer Vision",
    "CycleGAN",
    "Deep Learning",
    "Keras",
    "Keras and TensorFlow",
    "TensorFlow",
    "Unpaired Image Translation",
    "cyclegan",
    "deep learning",
    "tensorflow"
  ],
  "guid": "https://pyimagesearch.com/?p=29439",
  "description": "<p>Table of Contents CycleGAN: Unpaired Image-to-Image Translation (Part 1) Introduction Unpaired Image Translation CycleGAN Pipeline and Training Loss Formulation Adversarial Loss Cycle Consistency Summary Citation Information CycleGAN: Unpaired Image-to-Image Translation (Part 1) In this tutorial, you will learn about image-to-image&#8230;</p>\n<p>The post <a rel=\"nofollow\" href=\"https://pyimagesearch.com/2022/09/12/cyclegan-unpaired-image-to-image-translation-part-1/\">CycleGAN: Unpaired Image-to-Image Translation (Part 1)</a> appeared first on <a rel=\"nofollow\" href=\"https://pyimagesearch.com\">PyImageSearch</a>.</p>\n",
  "content:encoded": "\n<script src=\"https://fast.wistia.com/embed/medias/9t0rez7iel.jsonp\" async=\"\"></script><script src=\"https://fast.wistia.com/assets/external/E-v1.js\" async=\"\"></script><div class=\"wistia_responsive_padding\" style=\"padding:56.25% 0 0 0;position:relative;\"><div class=\"wistia_responsive_wrapper\" style=\"height:100%;left:0;position:absolute;top:0;width:100%;\"><div class=\"wistia_embed wistia_async_9t0rez7iel videoFoam=true\" style=\"height:100%;position:relative;width:100%\"><div class=\"wistia_swatch\" style=\"height:100%;left:0;opacity:0;overflow:hidden;position:absolute;top:0;transition:opacity 200ms;width:100%;\"><img src=\"https://fast.wistia.com/embed/medias/9t0rez7iel/swatch\" style=\"filter:blur(5px);height:100%;object-fit:contain;width:100%;\" alt=\"\" aria-hidden=\"true\" onload=\"this.parentNode.style.opacity=1;\"></div></div></div></div>\n\n\n\n<hr class=\"wp-block-separator\" id=\"TOC\">\n\n\n\n<h2><strong>Table of Contents</strong></h2>\n\n\n\n<div class=\"toc\">\n<ul>\n    <li><a rel=\"noopener\" target=\"_blank\" href=\"#h2BPTitle\">CycleGAN: Unpaired Image-to-Image Translation (Part 1)</a></li>\n        <ul>\n            <li><a rel=\"noopener\" target=\"_blank\" href=\"#h3Introduction\">Introduction</a></li>\n            <li><a rel=\"noopener\" target=\"_blank\" href=\"#h3Unpaired\">Unpaired Image Translation</a></li>\n            <li><a rel=\"noopener\" target=\"_blank\" href=\"#h3Pipeline\">CycleGAN Pipeline and Training</a></li>\n                <ul>\n                    <li><a rel=\"noopener\" target=\"_blank\" href=\"#h4Formulation\">Loss Formulation</a></li>\n                    <li><a rel=\"noopener\" target=\"_blank\" href=\"#h4Adversarial\">Adversarial Loss</a></li>\n                    <li><a rel=\"noopener\" target=\"_blank\" href=\"#h4Consistency\">Cycle Consistency</a></li>\n                </ul>\n        </ul>\n    <li><a rel=\"noopener\" target=\"_blank\" href=\"#h2Summary\">Summary</a></li>\n        <ul>\n            <li><a rel=\"noopener\" target=\"_blank\" href=\"#h3Citation\">Citation Information</a></li>\n        </ul>\n</ul>\n</div>\n\n\n\n<hr class=\"wp-block-separator\" id=\"h2BPTitle\">\n\n\n\n<h2><a href=\"#TOC\"><strong>CycleGAN: Unpaired Image-to-Image Translation (Part 1)</strong></a></h2>\n\n\n\n<p>In this tutorial, you will learn about image-to-image translation and how we can achieve it in case we have unpaired image data. Further, we will see how CycleGAN, one of the most famous efforts toward unpaired image translation, works and take an in-depth dive into the mechanism it uses to seamlessly translate images from one domain to another without needing paired image data.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/Cycle-GAN-Featured-1.png\" target=\"_blank\" rel=\"noreferrer noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/Cycle-GAN-Featured-1-1024x575.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34655\" width=\"700\" height=\"393\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/Cycle-GAN-Featured-1.png?size=126x71&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/Cycle-GAN-Featured-1-300x169.png?lossy=1&strip=1&webp=1 300w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/Cycle-GAN-Featured-1.png?size=378x212&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/Cycle-GAN-Featured-1.png?size=504x283&lossy=1&strip=1&webp=1 504w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/Cycle-GAN-Featured-1.png?size=630x354&lossy=1&strip=1&webp=1 630w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/Cycle-GAN-Featured-1-768x431.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/Cycle-GAN-Featured-1-1024x575.png?lossy=1&strip=1&webp=1 1024w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/Cycle-GAN-Featured-1-1536x863.png?lossy=1&strip=1&webp=1 1536w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/Cycle-GAN-Featured-1-2048x1151.png?lossy=1&strip=1&webp=1 2048w\" sizes=\"(max-width: 630px) 100vw, 630px\" /></a></figure></div>\n\n\n<p>Specifically, in detail, we will discuss the following in this tutorial.</p>\n\n\n\n<ul><li>The paradigm of Unpaired Image Translation </li><li>The idea and intuition behind CycleGAN (one of the most successful methods for unpaired image translation)</li><li>The details and intuition behind the loss functions used by CycleGAN, especially cyclic consistency loss</li><li>Details about the pipeline and training process of CycleGAN for unpaired image translation</li></ul>\n\n\n\n<p>This lesson is the first in a 3-part series on <strong>GANs 301</strong>:</p>\n\n\n\n<ol><li><a href=\"https://pyimg.co/7vh0s\" target=\"_blank\" rel=\"noreferrer noopener\"><strong><em>CycleGAN: Unpaired Image-to-Image Translation (Part 1)</em></strong></a><strong> (this tutorial)</strong></li><li><em>CycleGAN: Unpaired Image-to-Image Translation (Part 2)</em></li><li><em>CycleGAN: Unpaired Image-to-Image Translation (Part 3)</em></li></ol>\n\n\n\n<p>In the first part of this series (this tutorial), we will learn about the idea behind CycleGAN and understand the mechanism it uses to perform image translation. Further, we will understand the concept of cycle consistency that allows CycleGAN to perform image translation without needing paired data.</p>\n\n\n\n<p>Next, in Part 2 of this series, we will start implementing the CycleGAN model using TensorFlow and Keras and dive into the details of the model architecture and the <a href=\"https://www.kaggle.com/datasets/balraj98/apple2orange-dataset\" target=\"_blank\" rel=\"noreferrer noopener\">Apples-to-Oranges dataset</a>, which we will use for our unpaired image translation task. </p>\n\n\n\n<p>Finally, in the last part, we will look into the training details and generate images to see our real-time CycleGAN model in action.</p>\n\n\n\n<p><strong>To learn how CycleGAN performs image-to-image translation from unpaired images, </strong><strong><em>just keep reading.</em></strong></p>\n\n\n\n<hr class=\"wp-block-separator\" id=\"h2BPTitle\"/>\n\n\n\n<h2><a href=\"#TOC\"><strong>CycleGAN: Unpaired Image-to-Image Translation (Part 1)</strong></a></h2>\n\n\n\n<hr class=\"wp-block-separator\" id=\"h3Introduction\"/>\n\n\n\n<h3><a href=\"#TOC\"><strong>Introduction</strong></a></h3>\n\n\n\n<p>A pivotal point in the journey toward high-quality image generation was the inception of Generative Adversarial Networks in the year 2014. We have seen in a <a href=\"https://pyimagesearch.com/2021/10/25/training-a-dcgan-in-pytorch/\" target=\"_blank\" rel=\"noreferrer noopener\">previous tutorial</a> how GAN can be used to generate images from arbitrary distributions. Further, in <a href=\"https://pyimagesearch.com/2022/01/10/torch-hub-series-4-pgan-model-on-gan/\" target=\"_blank\" rel=\"noreferrer noopener\">another tutorial</a>, we have seen how we can transform the training paradigm of GANs and generate high-resolution images suited for practical applications.</p>\n\n\n\n<p>This is mainly possible due to the elegant adversarial training paradigm that forms the basis for GAN training. It is worth noting that the underlying 2-player-based adversarial paradigm can be used to match arbitrary distributions by formulating it as a competition between the generator and discriminator.</p>\n\n\n\n<p>This raises the question of whether such a powerful GAN framework can be used for more than just image generation. In one of our <a href=\"https://pyimagesearch.com/2022/07/27/image-translation-with-pix2pix/\" target=\"_blank\" rel=\"noreferrer noopener\">previous tutorials</a>, we took a detailed dive into how these networks can be repurposed for translating images from one domain to another: a task called Image Translation.</p>\n\n\n\n<p>However, the pix-to-pix model relies on the presence of paired data in the two domains across which image translation has to be performed. <strong>Figure 1</strong> shows examples of paired and unpaired data. </p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/Figure_1.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/Figure_1-1024x620.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34414\" width=\"700\" height=\"424\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/Figure_1.png?size=126x76&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/Figure_1-300x182.png?lossy=1&strip=1&webp=1 300w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/Figure_1.png?size=378x229&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/Figure_1.png?size=504x305&lossy=1&strip=1&webp=1 504w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/Figure_1.png?size=630x382&lossy=1&strip=1&webp=1 630w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/Figure_1-768x465.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/Figure_1-1024x620.png?lossy=1&strip=1&webp=1 1024w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/Figure_1.png?lossy=1&strip=1&webp=1 1515w\" sizes=\"(max-width: 630px) 100vw, 630px\" /></a><figcaption><strong>Figure 1:</strong> Example depicting the case where data in the two domains is paired (<em>left</em>) and data is unpaired (<em>right</em>) (source: <a href=\"https://arxiv.org/pdf/1703.10593.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">Zhu et al., 2020</a>).</figcaption></figure></div>\n\n\n<p>This creates a bottleneck and hinders utilization in practical scenarios where such tailored paired image datasets might not be present due to availability, practical feasibility constraints, or high annotation costs. Thus, it is natural to ask if we can still harness the capabilities of GANs for image translation without the need for paired images.</p>\n\n\n\n<hr class=\"wp-block-separator\" id=\"h3Unpaired\"/>\n\n\n\n<h3><a href=\"#TOC\"><strong>Unpaired Image Translation</strong></a></h3>\n\n\n\n<p>The CycleGAN model marks one of the first and most elegant solutions to tackle the problem of unpaired image-to-image translations. Instead of using the conventional supervised paradigm, which requires one-to-one mapping in dataset images, it proposes to use cycle consistency loss to achieve image translation from unpaired datasets.</p>\n\n\n\n<p>Formally, given a source domain <em>X</em> and a target domain <em>Y</em>, CycleGAN aims to learn a mapping <em>G</em>: <em>X</em> → <em>Y</em> such that the <em>G</em>(<em>X</em>) is the translation of the image from domain <em>X</em> to domain <em>Y</em>. Additionally, it also aims to learn a reverse mapping <em>F</em>: <em>Y</em> → <em>X</em> such that the <em>F</em>(<em>Y</em>) is the translation of the image from domain <em>Y</em> to domain <em>X</em>. <strong>Figure 2</strong> shows the overview of the components of CycleGAN.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/Figure_2.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/Figure_2.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34416\" width=\"643\" height=\"500\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/Figure_2.png?size=126x98&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/Figure_2-300x233.png?lossy=1&strip=1&webp=1 300w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/Figure_2.png?size=378x294&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/Figure_2.png?size=504x392&lossy=1&strip=1&webp=1 504w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/Figure_2.png?size=630x490&lossy=1&strip=1&webp=1 630w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/Figure_2-768x597.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/Figure_2.png?lossy=1&strip=1&webp=1 842w\" sizes=\"(max-width: 630px) 100vw, 630px\" /></a><figcaption><strong>Figure 2:</strong> Overview of CycleGAN Objective (source: <a href=\"https://arxiv.org/pdf/1703.10593.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">Zhu et al., 2020</a>).</figcaption></figure></div>\n\n\n<p>For example, this could mean translating zebra images into corresponding horse images (changing class semantics), transferring the stylistic difference from a summer view to a winter view, or predicting segmentation masks of an image scene, as shown in <strong>Figure 3</strong>.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter is-resized\"><a href=\"https://lh4.googleusercontent.com/dS9-NpOYmHZ_h8GISe52Qrzo3x3aT9sCz4sesxK-FiVqj-gidqhmDorl-KB-uMfNwV_pKksqlRQIUXrB3sl8KSjahvuA14ABOkoauAhvcVDHZ7rLrkSK8jRcLMW2dNRTd5mjcchMCI-MlYfXwZhLcRU\" target=\"_blank\" rel=\"noopener\"><img src=\"https://lh4.googleusercontent.com/dS9-NpOYmHZ_h8GISe52Qrzo3x3aT9sCz4sesxK-FiVqj-gidqhmDorl-KB-uMfNwV_pKksqlRQIUXrB3sl8KSjahvuA14ABOkoauAhvcVDHZ7rLrkSK8jRcLMW2dNRTd5mjcchMCI-MlYfXwZhLcRU\" alt=\"\" width=\"395\" height=\"500\"/></a><figcaption><strong>Figure 3:</strong> Example images depicting image translation from winter to summer (<em>top</em>), zebra to a horse (<em>middle</em>), apple to orange (<em>bottom</em>) (source: <a href=\"https://arxiv.org/pdf/1703.10593.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">Zhu et al., 2020</a>).</figcaption></figure></div>\n\n\n<p>However, it is worth noting that adversarial loss only ensures correspondence at the distribution level. Thus it can ensure that an image that belongs to the distribution of images from Domain A is translated into an image from the distribution of images from Domain B. It does not guarantee correspondence of images at the sample level. </p>\n\n\n\n<p>Let us understand this with an example where we want to translate a zebra (Domain A)  to a horse (Domain B). As shown in <strong>Figure 4</strong>, we see that the first is an image that belongs to the distribution of zebra images, and the second and third are images that belong to the distribution of horse images. </p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter is-resized\"><a href=\"https://lh4.googleusercontent.com/0WBkByPjo9gexxNnREeJeTdsQ_JMY5Qg9uOdJJF3hLa6m2kDFpXHh6ZjNgaGvBPropydaVIsDXS0lJUlTXMDzJK1jKFNjnaRczMZ-KxqjOfqiOAnujK4U8wJ_NmB7_MqFD2iAPdBYPpY6F05V-5z-Cg\" target=\"_blank\" rel=\"noopener\"><img src=\"https://lh4.googleusercontent.com/0WBkByPjo9gexxNnREeJeTdsQ_JMY5Qg9uOdJJF3hLa6m2kDFpXHh6ZjNgaGvBPropydaVIsDXS0lJUlTXMDzJK1jKFNjnaRczMZ-KxqjOfqiOAnujK4U8wJ_NmB7_MqFD2iAPdBYPpY6F05V-5z-Cg\" alt=\"\" width=\"700\" height=\"192\"/></a><figcaption><strong>Figure 4:</strong> Example images depicting image translation from Domain A (zebra) to Domain B (horse) (source: <a href=\"https://arxiv.org/pdf/1703.10593.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">Zhu et al., 2020</a>).</figcaption></figure></div>\n\n\n<p>However, note that only the second image is the corresponding sample to the first image, and the third image (even though an image from Domain B) is not a corresponding sample for the first image. Applying adversarial loss can ensure that the translated image belongs to the distribution of images from Domain B but cannot guarantee that we get the exact corresponding sample in Domain B.</p>\n\n\n\n<p>In the case of pix-to-pix, a translation of an image from Domain A to a corresponding image from Domain B was easily possible since we had paired samples from both domains. Thus, a simple supervised loss like the mean absolute error on the output of the generator and the ground-truth image in Domain B was enough to ensure that the translation of an image from Domain A results in the corresponding image in Domain B. </p>\n\n\n\n<p>However, since we do not have access to corresponding paired samples in the task of unpaired image translation, we cannot use supervised loss, which makes it difficult to ensure correspondence at the sample level in the two domains.</p>\n\n\n\n<p>In order to tackle this issue, the CycleGAN framework proposes to use cyclic consistency loss, which we discuss in detail later. The cyclic consistency loss ensures that the two functions learned are inverse functions of each other. Since the requirement for functions to have an inverse is that they have to be bijective (i.e., one-one and onto), it implicitly ensures a one-to-one correspondence at the sample level in the case of unpaired image translation.</p>\n\n\n\n<hr class=\"wp-block-separator\" id=\"h3Pipeline\">\n\n\n\n<h3><a href=\"#TOC\"><strong>CycleGAN Pipeline and Training</strong></a></h3>\n\n\n\n<p>Now that we have understood the intuition behind CycleGAN, let us take a deeper dive into its mechanism for performing unpaired image translation.</p>\n\n\n\n<p>Since we will be using the Apples-to-Oranges dataset for our implementation in later tutorials, let us consider a case where we have to translate an image <em>x</em> in Domain A, that is, apples, to an image <em>y</em> in Domain B, that is, Oranges.</p>\n\n\n\n<p><strong>Figure 5</strong> shows the overall pipeline of a CycleGAN.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter is-resized\"><a href=\"https://lh3.googleusercontent.com/0_NSF60-Gj3ff7EQp_ThbI7Ug-4W0N3Z36M5Y6G0mgFANV2woDpsGrKkpdycYV4nP9oikkdSQa3whBt46-F3yAH2ai0TAn9lOcNdv29vCRFyBC-oBZdQoZ88sb5ntk8LLSzN74B3sYMkfcJ_5eEdCC8\" target=\"_blank\" rel=\"noopener\"><img src=\"https://lh3.googleusercontent.com/0_NSF60-Gj3ff7EQp_ThbI7Ug-4W0N3Z36M5Y6G0mgFANV2woDpsGrKkpdycYV4nP9oikkdSQa3whBt46-F3yAH2ai0TAn9lOcNdv29vCRFyBC-oBZdQoZ88sb5ntk8LLSzN74B3sYMkfcJ_5eEdCC8\" alt=\"\" width=\"700\" height=\"298\"/></a><figcaption><strong>Figure 5:</strong> The end-to-end training pipeline of CycleGAN to achieve unpaired image translation (image by the author).</figcaption></figure></div>\n\n\n<p>First, we take image <em>x</em> from Domain A, which belongs to the distribution of images that depict apples (<em>top</em>). This image is passed through Generator <em>G</em> (as shown), which tries to output an image that belongs to the distribution of images in Domain B.</p>\n\n\n\n<p>Discriminator <em>D</em> is an adversary against which differentiates between the samples generated by Generator <em>G</em> (i.e., <em>y</em>′) and actual samples from Domain B (i.e.,  <em>y</em>). The generator is trained against this adversary using an adversarial training paradigm. This allows the generator to generate images at the output that belong to Domain B&#8217;s distribution (i.e., oranges).</p>\n\n\n\n<p>Similarly, we take an image <em>y</em> from Domain B, which belongs to the distribution of images that depict oranges (<em>bottom</em>). This image is passed through a Generator <em>F</em> (as shown), which tries to output an image that belongs to the distribution of images in Domain A.</p>\n\n\n\n<p>Discriminator <em>D</em> is an adversary against which differentiates between the samples generated by Generator <em>F</em> (i.e., <em>x</em>′) and actual samples from Domain A (i.e., <em>x</em>). The generator is trained against this adversary using an adversarial training paradigm. This allows the generator to generate images at the output that belong to the distribution of Domain A (i.e., apples).</p>\n\n\n\n<p>Finally, we notice that in addition to the two adversarial losses, we also have the forward and backward cyclic consistency losses. This ensures that:</p>\n\n\n\n<ul><li>for each image <em>x</em> from Domain A, the image translation cycle should be able to bring <em>x</em> back to the original image</li><li>for each image <em>y</em> from Domain B, the image translation cycle should be able to bring <em>y</em> back to the original image</li></ul>\n\n\n\n<hr class=\"wp-block-separator\" id=\"h4Formulation\">\n\n\n\n<h4><a href=\"#TOC\"><strong>Loss Formulation</strong></a></h4>\n\n\n\n<p>Finally, to complete our understanding of the training process, let us look at the mathematical formulation of the loss functions employed by CysleGAN during training.</p>\n\n\n\n<p>As discussed earlier, CycleGAN basically uses two loss functions (i.e., adversarial loss and cyclic consistency loss). Let us now take a look at the formulation of these losses.</p>\n\n\n\n<hr class=\"wp-block-separator\" id=\"h4Adversarial\"/>\n\n\n\n<h4><a href=\"#TOC\"><strong>Adversarial Loss</strong></a></h4>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter is-resized\"><a href=\"https://lh4.googleusercontent.com/WLI1b-odvUJRUXXw0dQZ77zJb2zv9TQUrfxBLfR7z9n5TNDjNB1azEkxRuHFpQMd75YePY7MCCxI2gC_kjRmYfEg_ud4mkhzC3bhXRtI-2IouKj5IRta2LuCd9j01AXRZ0DXClS1-YkHqvbenO1SsCY\" target=\"_blank\" rel=\"noopener\"><img src=\"https://lh4.googleusercontent.com/WLI1b-odvUJRUXXw0dQZ77zJb2zv9TQUrfxBLfR7z9n5TNDjNB1azEkxRuHFpQMd75YePY7MCCxI2gC_kjRmYfEg_ud4mkhzC3bhXRtI-2IouKj5IRta2LuCd9j01AXRZ0DXClS1-YkHqvbenO1SsCY\" alt=\"\" width=\"700\" height=\"103\"/></a><figcaption><strong>Equation 1</strong></figcaption></figure></div>\n\n\n<p>The mathematical formulation of adversarial loss is defined by <strong>Equation 1</strong>. </p>\n\n\n\n<p>Here, the generator tries to generate images that look like they belong to the distribution of images from Domain B. On the other hand, the adversary or the discriminator tries to distinguish between the output of the generator (i.e., <em>G</em>(<em>x</em>)) and real samples from Domain B (i.e., <em>Y</em>).</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter is-resized\"><a href=\"https://lh6.googleusercontent.com/dDWajx_ZPDH3spyQFuXUMuoZaPcwRAemXU4nMoLDYF2Rd8rhTwXbdy8wkx3GoDNUS4BnvLHZTMZx_VWDJxX-xVJZV7mV6P0LTBjCjoWOSR66tr3hEBfTLX3A3UlWHqJJhXNjZHQBZkVsw0cASyGkRLk\" target=\"_blank\" rel=\"noopener\"><img src=\"https://lh6.googleusercontent.com/dDWajx_ZPDH3spyQFuXUMuoZaPcwRAemXU4nMoLDYF2Rd8rhTwXbdy8wkx3GoDNUS4BnvLHZTMZx_VWDJxX-xVJZV7mV6P0LTBjCjoWOSR66tr3hEBfTLX3A3UlWHqJJhXNjZHQBZkVsw0cASyGkRLk\" alt=\"\" width=\"700\" height=\"61\"/></a><figcaption><strong>Equation 2</strong></figcaption></figure></div>\n\n\n<p>Thus, at each training iteration:</p>\n\n\n\n<ul><li>the discriminator is updated to maximize the loss (with the generator frozen)</li><li>the generator is updated to minimize the loss (with the discriminator frozen)</li></ul>\n\n\n\n<p>Following the adversarial training paradigm, eventually, the generator learns and can generate images from the distribution of Domain B (which are indistinguishable from original samples <em>y</em>).</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter is-resized\"><a href=\"https://lh4.googleusercontent.com/lcoZsP9Kqe8AUPfggafXzSvKYp1ij-tGWr_hw3kzWbX7CHqln0O-N5t703ty8N_Yf6SxJ2zA1mqmWzhAneWStbSJN3kzCwof6S9ELREbtpJHN3NlpbavkjE1pTuts5BLc9XJeu_13UpglBf5HbWIwOo\" target=\"_blank\" rel=\"noopener\"><img src=\"https://lh4.googleusercontent.com/lcoZsP9Kqe8AUPfggafXzSvKYp1ij-tGWr_hw3kzWbX7CHqln0O-N5t703ty8N_Yf6SxJ2zA1mqmWzhAneWStbSJN3kzCwof6S9ELREbtpJHN3NlpbavkjE1pTuts5BLc9XJeu_13UpglBf5HbWIwOo\" alt=\"\" width=\"700\" height=\"64\"/></a><figcaption><strong>Equation 3</strong></figcaption></figure></div>\n\n\n<p>Similarly, as discussed earlier, another adversarial loss is imposed for the reverse mapping from domain <em>Y</em> to domain <em>X</em>. Here, Generator <em>F</em> tries to generate images that resemble the distribution of Domain A. Also, Discriminator <em>D</em><em><sub>X</sub></em> tries to distinguish between the output of the generator (i.e., <em>F</em>(<em>x</em>)) and the real samples from Domain A (i.e., <em>X</em>).</p>\n\n\n\n<hr class=\"wp-block-separator\" id=\"h4Consistency\"/>\n\n\n\n<h4><a href=\"#TOC\"><strong>Cycle Consistency</strong></a></h4>\n\n\n\n<p>As we have seen earlier, the adversarial loss only ensures consistency at the distribution level. However, to ensure that corresponding samples are generated when image translation is performed, CycleGAN utilizes cyclic consistency loss at the sample level.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter is-resized\"><a href=\"https://lh5.googleusercontent.com/uDd_Iad3eNise26sExJJM5qY8mUfKPgHdtlWvBC9ySiAbUGSbg-8XeWIoQY8VZeyDkgf_MfDA8N1YA82mFT7At7MtLxHPg2MTZSj-m0Dk6rpTzqm6_RqV9P7nh0o8ooseUESkSPiPtuKs4Z1SZ4RZgc\" target=\"_blank\" rel=\"noopener\"><img src=\"https://lh5.googleusercontent.com/uDd_Iad3eNise26sExJJM5qY8mUfKPgHdtlWvBC9ySiAbUGSbg-8XeWIoQY8VZeyDkgf_MfDA8N1YA82mFT7At7MtLxHPg2MTZSj-m0Dk6rpTzqm6_RqV9P7nh0o8ooseUESkSPiPtuKs4Z1SZ4RZgc\" alt=\"\" width=\"700\" height=\"144\"/></a><figcaption><strong>Equation 4</strong></figcaption></figure></div>\n\n\n<p>As shown in <strong>Equation 4</strong>, the cycle consistency loss consists of two terms: </p>\n\n\n\n<ul><li>forward cyclic consistency </li><li>backward cycle consistency</li></ul>\n\n\n\n<p>Specifically, the first term ensures that when a sample in Domain A (say <em>x</em>) is passed through Generator <em>G</em> (that transforms from Domain A to B) and then through Generator <em>F</em> (when it transforms it back to Domain A), the output that is <em>F</em>(<em>G</em>(<em>x</em>)) is the same as the original input sample <em>x.</em> (<em>x</em> → <em>G</em>(<em>x</em>) → <em>F</em>(<em>G</em>(<em>x</em>)) <img src='https://929687.smushcdn.com/2633864/wp-content/latex/fb4/fb4f353ef9a72c24566678c957a5ae9f-ffffff-000000-0.png?lossy=1&strip=1&webp=1' alt='\\approx' title='\\approx' class='latex' /> <em>x</em>).</p>\n\n\n\n<p>Similarly, the second term ensures that when a sample in Domain B (say <em>y</em>) is passed through Generator <em>F</em> (that transforms from Domain B to A) and then through Generator <em>G</em> (when it transforms it back to Domain B), the output that is <em>G</em>(<em>F</em>(<em>x</em>)) is the same as the original input sample <em>y</em>. (<em>y</em> → <em>F</em>(<em>y</em>) → <em>G</em>(<em>F</em>(<em>y</em>)) <img src='https://929687.smushcdn.com/2633864/wp-content/latex/fb4/fb4f353ef9a72c24566678c957a5ae9f-ffffff-000000-0.png?lossy=1&strip=1&webp=1' alt='\\approx' title='\\approx' class='latex' /> <em>y</em>).</p>\n\n\n\n<p>In other words, both terms together aim to learn functions <em>F</em>() and <em>G</em>() such that they are inverses of each other. Thus, the cyclic consistency loss ensures correspondence at the sample level between sample <em>x</em> from Domain A and sample <em>y</em> from Domain B.</p>\n\n\n\n<hr class=\"wp-block-separator has-css-opacity\"/>\n\n\n\n<div id=\"pitch\" style=\"padding: 40px; width: 100%; background-color: #F4F6FA;\">\n\t<h3>What's next? I recommend <a target=\"_blank\" href=\"https://pyimagesearch.com/pyimagesearch-university/?utm_source=blogPost&utm_medium=bottomBanner&utm_campaign=What%27s%20next%3F%20I%20recommend\">PyImageSearch University</a>.</h3>\n\n\t<script src=\"https://fast.wistia.com/embed/medias/kno0cmko2z.jsonp\" async></script><script src=\"https://fast.wistia.com/assets/external/E-v1.js\" async></script><div class=\"wistia_responsive_padding\" style=\"padding:56.25% 0 0 0;position:relative;\"><div class=\"wistia_responsive_wrapper\" style=\"height:100%;left:0;position:absolute;top:0;width:100%;\"><div class=\"wistia_embed wistia_async_kno0cmko2z videoFoam=true\" style=\"height:100%;position:relative;width:100%\"><div class=\"wistia_swatch\" style=\"height:100%;left:0;opacity:0;overflow:hidden;position:absolute;top:0;transition:opacity 200ms;width:100%;\"><img src=\"https://fast.wistia.com/embed/medias/kno0cmko2z/swatch\" style=\"filter:blur(5px);height:100%;object-fit:contain;width:100%;\" alt=\"\" aria-hidden=\"true\" onload=\"this.parentNode.style.opacity=1;\" /></div></div></div></div>\n\n\t<div style=\"margin-top: 32px; margin-bottom: 32px; \">\n\t\t<strong>Course information:</strong><br/>\n\t\t53+ total classes • 57+ hours of on-demand code walkthrough videos • Last updated: October 2022<br/>\n\t\t<span style=\"color: #169FE6;\">★★★★★</span> 4.84 (128 Ratings) • 15,800+ Students Enrolled\n\t</div>\n\n\t<p><strong>I strongly believe that if you had the right teacher you could <em>master</em> computer vision and deep learning.</strong></p>\n\n\t<p>Do you think learning computer vision and deep learning has to be time-consuming, overwhelming, and complicated? Or has to involve complex mathematics and equations? Or requires a degree in computer science?</p>\n\n\t<p>That’s <em>not</em> the case.</p>\n\n\t<p>All you need to master computer vision and deep learning is for someone to explain things to you in <em>simple, intuitive</em> terms. <em>And that’s exactly what I do</em>. My mission is to change education and how complex Artificial Intelligence topics are taught.</p>\n\n\t<p>If you're serious about learning computer vision, your next stop should be PyImageSearch University, the most comprehensive computer vision, deep learning, and OpenCV course online today. Here you’ll learn how to <em>successfully</em> and <em>confidently</em> apply computer vision to your work, research, and projects. Join me in computer vision mastery.</p>\n\n\t<p><strong>Inside PyImageSearch University you'll find:</strong></p>\n\n\t<ul style=\"margin-left: 0px;\">\n\t\t<li style=\"list-style: none;\">&check; <strong>53+ courses</strong> on essential computer vision, deep learning, and OpenCV topics</li>\n\t\t<li style=\"list-style: none;\">&check; <strong>53+ Certificates</strong> of Completion</li>\n\t\t<li style=\"list-style: none;\">&check; <strong>57+ hours</strong> of on-demand video</li>\n\t\t<li style=\"list-style: none;\">&check; <strong>Brand new courses released <em>regularly</em></strong>, ensuring you can keep up with state-of-the-art techniques</li>\n\t\t<li style=\"list-style: none;\">&check; <strong>Pre-configured Jupyter Notebooks in Google Colab</strong></li>\n\t\t<li style=\"list-style: none;\">&check; Run all code examples in your web browser — works on Windows, macOS, and Linux (no dev environment configuration required!)</li>\n\t\t<li style=\"list-style: none;\">&check; Access to <strong>centralized code repos for <em>all</em> 450+ tutorials</strong> on PyImageSearch</li>\n\t\t<li style=\"list-style: none;\">&check; <strong> Easy one-click downloads</strong> for code, datasets, pre-trained models, etc.</li>\n\t\t<li style=\"list-style: none;\">&check; <strong>Access</strong> on mobile, laptop, desktop, etc.</li>\n\t</ul>\n\n\t<p style=\"text-align: center;\">\n\t\t<a target=\"_blank\" class=\"button link\" href=\"https://pyimagesearch.com/pyimagesearch-university/?utm_source=blogPost&utm_medium=bottomBanner&utm_campaign=What%27s%20next%3F%20I%20recommend\" style=\"background-color: #6DC713; border-bottom: none;\">Click here to join PyImageSearch University</a>\n\t</p>\n</div>\n\n\n\n<hr class=\"wp-block-separator\" id=\"h2Summary\">\n\n\n\n<h2><a href=\"#TOC\"><strong>Summary</strong></a></h2>\n\n\n\n<p>In this tutorial, we learned about the task of image translation and how it can be achieved when paired input images in the two domains are unavailable.</p>\n\n\n\n<p>Specifically, we discussed in detail the idea behind CycleGAN and understood the paradigm of cyclic consistency, which allows CycleGAN to perform image translation from unpaired data seamlessly.</p>\n\n\n\n<p>In addition, we discussed the various losses that CycleGAN used during training and understood their mathematical formulation and the role they play in the task of unpaired image translation.</p>\n\n\n\n<p>Finally, we discussed the end-to-end pipeline of CycleGAN to decode the process of image translation from unpaired data.</p>\n\n\n\n<hr class=\"wp-block-separator\" id=\"h3Citation\"/>\n\n\n\n<h3><a href=\"#TOC\"><strong>Citation Information</strong></a></h3>\n\n\n\n<p><strong>Chandhok, S. </strong>“CycleGAN: Unpaired Image-to-Image Translation (Part 1),” <em>PyImageSearch</em>, P. Chugh, A. R. Gosthipaty, S. Huot, K. Kidriavsteva, R. Raha, and A. Thanki, eds., 2022, <a href=\"https://pyimg.co/7vh0s\" target=\"_blank\" rel=\"noreferrer noopener\">https://pyimg.co/7vh0s</a> </p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"raw\" data-enlighter-theme=\"classic\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"false\" data-enlighter-lineoffset=\"\" data-enlighter-title=\"\" data-enlighter-group=\"\">@incollection{Chandhok_2022_CycleGAN,\n  author = {Shivam Chandhok},\n  title = {{CycleGAN}: Unpaired Image-to-Image Translation (Part 1)},\n  booktitle = {PyImageSearch},\n  editor = {Puneet Chugh and Aritra Roy Gosthipaty and Susan Huot and Kseniia Kidriavsteva and Ritwik Raha and Abhishek Thanki},\n  year = {2022},\n  note = {https://pyimg.co/7vh0s},\n}</pre>\n\n\n\n<hr class=\"wp-block-separator has-css-opacity\"/>\n\n\n\n<div style=\"padding: 40px; width: 100%; background-color: #F4F6FA;\">\n\t<h3>Want free GPU credits to train models?</h3>\n\n\t<ul style=\"margin-left: 0px;\">\n\t\t<li style=\"list-style: none;\">We used <a target=\"_blank\" href=\"https://cloud.jarvislabs.ai/\">Jarvislabs.ai</a>, a GPU cloud, for all the experiments.</li>\n\t\t<li style=\"list-style: none;\">We are proud to offer PyImageSearch University students $20 worth of Jarvislabs.ai GPU cloud credits. Join PyImageSearch University and claim your $20 credit <a target=\"_blank\" href=\"https://pyimagesearch.com/pyimagesearch-university/\">here</a>.</li>\n\t</ul>\n\n\n\t<p>In Deep Learning, we need to train Neural Networks. These Neural Networks can be trained on a CPU but take a lot of time. Moreover, sometimes these networks do not even fit (run) on a CPU.</p>\n\n\t<p>To overcome this problem, we use <strong>GPUs</strong>.  The problem is these GPUs are <strong>expensive</strong> and become outdated quickly. </p>\n\n\t<p>GPUs are great because they take your Neural Network and train it quickly.  The problem is that GPUs are expensive, so you don’t want to buy one and use it only occasionally.  Cloud GPUs let you use a GPU and <strong>only pay for the time you are running the GPU</strong>.  It’s a brilliant idea that saves you money.</p>\n\n\t<p><strong>JarvisLabs</strong> provides the best-in-class GPUs, and <strong>PyImageSearch University students</strong> get between 10-50 hours on a world-class GPU (time depends on the specific GPU you select).</p>\n\n\n\t<p>This gives you a chance to <strong>test-drive a monstrously powerful GPU</strong> on any of our tutorials in a jiffy. So join <a target=\"_blank\" href=\"https://pyimagesearch.com/pyimagesearch-university/\">PyImageSearch University</a> today and try it for yourself.</p>\n\n\n\t<p style=\"text-align: center;\">\n\t\t<a target=\"_blank\" class=\"button link\" href=\"https://pyimagesearch.com/pyimagesearch-university/\" style=\"background-color: #6DC713; border-bottom: none;\">Click here to get Jarvislabs credits now</a>\n\t</p>\n</div>\n\n\n\n<div id=\"download-the-code\" class=\"post-cta-wrap\">\n<div class=\"gpd-post-cta\">\n\t<div class=\"gpd-post-cta-content\">\n\t\t\n\n\t\t\t<div class=\"gpd-post-cta-top\">\n\t\t\t\t<div class=\"gpd-post-cta-top-image\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2020/01/cta-source-guide-1.png?lossy=1&strip=1&webp=1\" alt=\"\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2020/01/cta-source-guide-1.png?lossy=1&strip=1&webp=1 410w,https://929687.smushcdn.com/2633864/wp-content/uploads/2020/01/cta-source-guide-1.png?size=126x174&lossy=1&strip=1&webp=1 126w,https://929687.smushcdn.com/2633864/wp-content/uploads/2020/01/cta-source-guide-1.png?size=252x348&lossy=1&strip=1&webp=1 252w\" sizes=\"(max-width: 410px) 100vw, 410px\" /></div>\n\t\t\t\t\n\t\t\t\t<div class=\"gpd-post-cta-top-title\"><h4>Join the PyImageSearch Newsletter and Grab My FREE 17-page Resource Guide PDF</h4></div>\n\t\t\t\t<div class=\"gpd-post-cta-top-desc\"><p>Enter your email address below to <strong>join the PyImageSearch Newsletter</strong> and <strong>download my FREE 17-page Resource Guide PDF</strong> on Computer Vision, OpenCV, and Deep Learning.</p></div>\n\n\n\t\t\t</div>\n\n\t\t\t<div class=\"gpd-post-cta-bottom\">\n\t\t\t\t<form class=\"footer-cta\" action=\"https://www.getdrip.com/forms/657075648/submissions\" method=\"post\" target=\"_blank\" data-drip-embedded-form=\"657075648\">\n\t\t\t\t\t<input name=\"fields[email]\" type=\"email\" value=\"\" placeholder=\"Your email address\" class=\"form-control\" />\n\n\t\t\t\t\t<button type=\"submit\">Join the Newsletter!</button>\n\n\t\t\t\t\t<div style=\"display: none;\" aria-hidden=\"true\"><label for=\"website\">Website</label><br /><input type=\"text\" id=\"website\" name=\"website\" tabindex=\"-1\" autocomplete=\"false\" value=\"\" /></div>\n\t\t\t\t</form>\n\t\t\t</div>\n\n\n\t\t\n\t</div>\n\n</div>\n</div>\n<p>The post <a rel=\"nofollow\" href=\"https://pyimagesearch.com/2022/09/12/cyclegan-unpaired-image-to-image-translation-part-1/\">CycleGAN: Unpaired Image-to-Image Translation (Part 1)</a> appeared first on <a rel=\"nofollow\" href=\"https://pyimagesearch.com\">PyImageSearch</a>.</p>\n"
}