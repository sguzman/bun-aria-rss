{
  "title": "Effective Sample Size in Importance Sampling",
  "link": "",
  "published": "2015-08-21T21:30:00+01:00",
  "updated": "2015-08-21T21:30:00+01:00",
  "author": {
    "name": "Sebastian Nowozin"
  },
  "id": "tag:www.nowozin.net,2015-08-21:/sebastian/blog/effective-sample-size-in-importance-sampling.html",
  "summary": "<p>In this article we will look at a practically important measure of efficiency\nin importance sampling, the so called <em>effective sample size</em> (ESS) estimate.\nThis measure was proposed by <a href=\"http://www.decode.com/management/\">Augustine\nKong</a> in 1992 in a technical report which\nuntil recently â€¦</p>",
  "content": "<p>In this article we will look at a practically important measure of efficiency\nin importance sampling, the so called <em>effective sample size</em> (ESS) estimate.\nThis measure was proposed by <a href=\"http://www.decode.com/management/\">Augustine\nKong</a> in 1992 in a technical report which\nuntil recently has been difficult to locate online, but after getting in\ncontact with the University of Chicago I am pleased that the report is now\navailable (again):</p>\n<ul>\n<li>Augustine Kong, \"A Note on Importance Sampling using Standardized Weights\",\nTechnical Report 348,\n<a href=\"https://galton.uchicago.edu/techreports/tr348.pdf\">PDF</a>,\nDepartment of Statistics, University of Chicago, July 1992.</li>\n</ul>\n<p>Before we discuss the usefulness of the effective sample size, let us first\ndefine the notation and context for importance sampling.</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Importance_sampling\">Importance sampling</a> is one\nof the most generally applicable method to sample from otherwise intractable\ndistributions.\nIn machine learning and statistics importance sampling is regularly used for\nsampling from distributions in low dimensions (say, up to maybe 20\ndimensions).\nThe general idea of importance sampling has been extended since the 1950s\nto the sequential setting and the resulting class of modern\n<a href=\"http://www.stats.ox.ac.uk/~doucet/doucet_defreitas_gordon_smcbookintro.pdf\"><em>Sequential Monte Carlo</em> (SMC) methods</a>\nconstitute the state of the art Monte Carlo methods in many important time\nseries modeling applications.</p>\n<p>The general idea of importance sampling is as follows.\nWe are interested in computing an expectation,\n</p>\n<div class=\"math\">$$\\mu = \\mathbb{E}_{X \\sim p}[h(X)] = \\int h(x) p(x) \\,\\textrm{d}x.$$</div>\n<p>\nIf we can sample from <span class=\"math\">\\(p\\)</span> directly, the standard Monte Carlo estimate is\npossible, and we draw <span class=\"math\">\\(X_i \\sim p\\)</span>, <span class=\"math\">\\(i=1,\\dots,n\\)</span>, then use\n</p>\n<div class=\"math\">$$\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^n h(X_i).$$</div>\n<p>In many applications we cannot directly sample from <span class=\"math\">\\(p\\)</span>.\nIn this case importance sampling can still be applied by sampling from a\ntractable proposal distribution <span class=\"math\">\\(q\\)</span>, with <span class=\"math\">\\(X_i \\sim q\\)</span>, <span class=\"math\">\\(i=1,\\dots,n\\)</span>, then\nreweighting the sample using the ratio <span class=\"math\">\\(p(X_i)/q(X_i)\\)</span>, leading to the\nstandard importance sampling estimate\n</p>\n<div class=\"math\">$$\\tilde{\\mu} = \\frac{1}{n} \\sum_{i=1}^n \\frac{p(X_i)}{q(X_i)} h(X_i).$$</div>\n<p>In case <span class=\"math\">\\(p\\)</span> is known only up to an unknown normalizing constant, the so called\n<em>self-normalized importance sampling estimate</em> can be used.\nDenoting the weights by <span class=\"math\">\\(w(X_i) = \\frac{p(X_i)}{q(X_i)}\\)</span> it is defined as\n</p>\n<div class=\"math\">$$\\bar{\\mu} = \\frac{\\frac{1}{n} \\sum_{i=1}^n w(X_i) h(X_i)}{\n    \\frac{1}{n} \\sum_{i=1}^n w(X_i)}.$$</div>\n<p>The quality of this estimate chiefly depends on how good the proposal\ndistribution <span class=\"math\">\\(q\\)</span> matches the form of <span class=\"math\">\\(p\\)</span>.  Because <span class=\"math\">\\(p\\)</span> is difficult to sample\nfrom, it typically is also difficult to make a precise statement about the\nquality of approximation of <span class=\"math\">\\(q\\)</span>.</p>\n<p>The effective sample size solves this issue: it can be used after or during\nimportance sampling to provide a quantitative measure of the quality of the\nestimated mean.\nEven better, the estimate is provided on a natural scale of worth in samples\nfrom <span class=\"math\">\\(p\\)</span>, that is, if we use <span class=\"math\">\\(n=1000\\)</span> samples <span class=\"math\">\\(X_i \\sim q\\)</span> and obtain an ESS\nof say 350 then this indicates that the quality of our estimate is about the\nsame as if we would have used 350 direct samples <span class=\"math\">\\(X_i \\sim p\\)</span>.  This justifies\nthe name <em>effective sample size</em>.</p>\n<p>Since the late 1990s the effective sample size is popularly used as a reliable\ndiagnostic in importance sampling and sequential Monte Carlo applications.\nSometimes it even informs the algorithm during sampling; for example, one can\ncontinue an importance sampling method until a certain ESS has been reached.\nAnother example is during SMC where the ESS is often used to decide whether\noperations such as resampling or rejuvenation are performed.</p>\n<h2>Definition</h2>\n<p>Two alternative but equivalent definitions exist.  Assume normalized weights\n<span class=\"math\">\\(w_i \\geq 0\\)</span> with <span class=\"math\">\\(\\sum_{i=1}^n w_i = 1\\)</span>.  Then, the original definition of\nthe effective sample size estimate is by Kong, popularized by <a href=\"http://www.people.fas.harvard.edu/~junliu/\">Jun\nLiu</a> in\n<a href=\"https://stat.duke.edu/~scs/Courses/Stat376/Papers/ConvergeRates/LiuMetropolized1996.pdf\">this\npaper</a>, as\n</p>\n<div class=\"math\">$$\\textrm{ESS} = \\frac{n}{1 + \\textrm{Var}_q(W)},$$</div>\n<p>\nwhere <span class=\"math\">\\(\\textrm{Var}_q(W) = \\frac{1}{n-1} \\sum_{i=1}^n (w_i - \\frac{1}{n})^2\\)</span>.\nThe alternative form emerged later (I did not manage to find its first use\nprecisely), and has the form\n</p>\n<div class=\"math\">$$\\textrm{ESS} = \\frac{1}{\\sum_{i=1}^n w_i^2}.$$</div>\n<p>When the weights are unnormalized, we define <span class=\"math\">\\(\\tilde{w}_i = w_i /\n(\\sum_{i=1}^n w_i)\\)</span> and see that\n</p>\n<div class=\"math\">$$\\textrm{ESS} = \\frac{1}{\\sum_{i=1}^n \\tilde{w}_i^2}\n= \\frac{(\\sum_{i=1}^n w_i)^2}{\\sum_{i=1}^n w_i^2}.$$</div>\n<p>As is often the case in numerical computation in probabilistic models the\nquantities are often stored in log-domain, i.e. we would store <span class=\"math\">\\(\\log w_i\\)</span>\ninstead of <span class=\"math\">\\(w_i\\)</span>, and compute the above equations in log-space.</p>\n<h2>Example</h2>\n<p>As a simple example we set the target distribution to be a\n<span class=\"math\">\\(\\textrm{StudentT}(0,\\nu)\\)</span> with <span class=\"math\">\\(\\nu=8\\)</span> degrees of freedom, and the proposal\nto be a Normal <span class=\"math\">\\(\\mathcal{N}(\\mu,16)\\)</span>.\nWe then visualize the ESS as a function of the shift <span class=\"math\">\\(\\mu\\)</span> of the Normal\nproposal.  The sample size should decrease away from the true mean (zero) and\nbe highest at zero.</p>\n<p><img alt=\"ESS example, StudentT target, Normal proposal\" src=\"http://www.nowozin.net/sebastian/blog/images/ess-demo-1.svg\"></p>\n<p>This is indeed what happens in the above plot and, not shown, the estimated\nvariance from the ESS agrees with the variance over many replicates.</p>\n<h2>Derivation</h2>\n<p>The following derivation is from Kong's technical report, however, to make it\nself-contained and accessible I fleshed out some details and give explanations\ninline.</p>\n<p>We start with an expression for <span class=\"math\">\\(\\textrm{Var}(\\bar{\\mu})\\)</span>.\nThis is a variance of a ratio expression with positive denominator; hence we\ncan apply the multivariate delta method for ratio expressions (see appendix\nbelow) to obtain an asymptotic approximation.\nFollowing Kong's original notation we define <span class=\"math\">\\(W_i = w(X_i)\\)</span> and <span class=\"math\">\\(W=W_1\\)</span>, as\nwell as <span class=\"math\">\\(Z_i = h(X_i) w(X_i)\\)</span> and <span class=\"math\">\\(Z = Z_1\\)</span>.\nThen we have the asymptotic delta method approximation\n</p>\n<div class=\"math\">\\begin{eqnarray}\n\\textrm{Var}_q(\\bar{\\mu}) &amp; \\approx &amp;\n    \\frac{1}{n}\\left[\\frac{\\textrm{Var}_q(Z)}{(\\mathbb{E}_q W)^2}\n        - 2 \\frac{\\mathbb{E}_q Z}{(\\mathbb{E}_q W)^3} \\textrm{Cov}_q(Z,W)\n        + \\frac{(\\mathbb{E}_q Z)^2}{(\\mathbb{E}_q W)^4}\n            \\textrm{Var}_q(W)\\right].\\label{eqn:delta1}\n\\end{eqnarray}</div>\n<p>\nWe can simplify this somewhat intimidating expression by realizing that\n</p>\n<div class=\"math\">$$\\mathbb{E}_q W = \\int \\frac{p(x)}{q(x)} q(x) \\,\\textrm{d}x\n    = \\int p(x) \\,\\textrm{d}x = 1.$$</div>\n<p>\n(For the unnormalized case the derivation result is the same because the ratio\n<span class=\"math\">\\(\\bar{\\mu}\\)</span> does not depend on the normalization constant.)\nThen we can simplify <span class=\"math\">\\((\\ref{eqn:delta1})\\)</span> to\n</p>\n<div class=\"math\">\\begin{eqnarray}\n    &amp; = &amp;\n    \\frac{1}{n}\\left[\\textrm{Var}_q(Z)\n        - 2 (\\mathbb{E}_q Z) \\textrm{Cov}_q(Z,W)\n        + (\\mathbb{E}_q Z)^2 \\textrm{Var}_q(W)\\right].\\label{eqn:delta2}\n\\end{eqnarray}</div>\n<p>\nThe next step is to realize that\n<span class=\"math\">\\(\\mathbb{E}_q Z = \\int w(x) h(x) q(x) \\,\\textrm{d}x\n    = \\int \\frac{p(x)}{q(x)} q(x) h(x) \\,\\textrm{d}x\n    = \\int h(x) p(x) \\,\\textrm{d}x = \\mu.\\)</span>\nThus <span class=\"math\">\\((\\ref{eqn:delta2})\\)</span> further simplifies to\n</p>\n<div class=\"math\">\\begin{eqnarray}\n    &amp; = &amp;\n    \\frac{1}{n}\\big[\\underbrace{\\textrm{Var}_q(Z)}_{\\textrm{(B)}}\n        - 2 \\mu \\underbrace{\\textrm{Cov}_q(Z,W)}_{\\textrm{(A)}}\n        + \\mu^2 \\textrm{Var}_q(W)\\big].\n    \\label{eqn:delta3}\n\\end{eqnarray}</div>\n<p>This is great progress, but we need to nibble on this expression some more.\nLet us consider the parts (A) and (B), in this order.</p>\n<p><strong>(A)</strong>.  To simplify this expression we can leverage the <a href=\"http://en.wikipedia.org/wiki/Covariance#Definition\">definition of the\ncovariance</a> and then apply\nthe known relations of our special expectations.\nThis yields.\n</p>\n<div class=\"math\">\\begin{eqnarray}\n\\textrm{(A)} = \\textrm{Cov}_q(Z,W) &amp; = &amp;\n    \\mathbb{E}_q[\\underbrace{Z}_{= W H} W]\n        - \\underbrace{(\\mathbb{E}_q Z)}_{= \\mu}\n            \\underbrace{(\\mathbb{E}_q W)}_{= 1}\\nonumber\\\\\n    &amp; = &amp; \\mathbb{E}_q[H W^2] - \\mu\\nonumber\\\\\n    &amp; = &amp; \\mathbb{E}_p[H W] - \\mu.\\label{eqn:A1}\n\\end{eqnarray}</div>\n<p>\nNote the change of measure from <span class=\"math\">\\(q\\)</span> to <span class=\"math\">\\(p\\)</span> in the last step.\nTo break down the expectation of the product further we use the known rules\nabout expectations, namely <span class=\"math\">\\(\\textrm{Cov}(X,Y) = \\mathbb{E}[XY] -\n(\\mathbb{E}X)(\\mathbb{E}Y)\\)</span>, which leds us to\n</p>\n<div class=\"math\">\\begin{eqnarray}\n\\textrm{(A)} = \\textrm{Cov}_q(Z,W) &amp; = &amp;\n    \\textrm{Cov}_p(H,W) + \\mu \\mathbb{E}_p W - \\mu.\\label{eqn:A2}\n\\end{eqnarray}</div>\n<p><strong>(B)</strong>.  First we expand the variance by its definition, then simplify.\n</p>\n<div class=\"math\">$$\\textrm{Var}_q(Z) = \\textrm{Var}_q(W H)\n    = \\mathbb{E}_q[W^2 H^2] - (\\underbrace{\\mathbb{E}_q[WH]}_{= \\mu})^2\n    = \\mathbb{E}_p[W H^2] - \\mu^2.$$</div>\n<p>For approaching <span class=\"math\">\\(\\mathbb{E}_p[W H^2]\\)</span> we need to leverage the second-order\ndelta method (see appendix) which gives the following approximation,\n</p>\n<div class=\"math\">\\begin{eqnarray}\n\\mathbb{E}_p[W H^2] &amp; \\approx &amp;\n    (\\mathbb{E}_p W)\\underbrace{(\\mathbb{E}_p H)^2}_{= \\mu^2}\n    + 2 \\underbrace{\\mathbb{E}_p[H]}_{\\mu} \\textrm{Cov}_p(W,H)\n    + (\\mathbb{E}_p W) \\textrm{Var}_p(H)\\nonumber\\\\\n    &amp; = &amp; (\\mathbb{E}_p W) \\mu^2 + 2 \\mu \\textrm{Cov}_p(W,H)\n        + (\\mathbb{E}_p W) \\textrm{Var}_p(H).\n    \\label{eqn:B1}\n\\end{eqnarray}</div>\n<p>Ok, almost done.  We now leverage our work to harvest:\n</p>\n<div class=\"math\">\\begin{eqnarray}\n\\textrm{Var}_q(\\bar{\\mu}) &amp; \\approx &amp;\n    \\frac{1}{n}\\big[\\underbrace{\\textrm{Var}_q(Z)}_{\\textrm{(B)}}\n        - 2 \\mu \\underbrace{\\textrm{Cov}_q(Z,W)}_{\\textrm{(A)}}\n        + \\mu^2 \\textrm{Var}_q(W)\\big]\\nonumber\\\\\n    &amp; \\approx &amp; \\frac{1}{n}\\big[\n        \\left(\n            (\\mathbb{E}_p W) \\mu^2 + 2 \\mu \\textrm{Cov}_p(W,H)\n            + (\\mathbb{E}_p W) \\textrm{Var}_p(H) - \\mu^2\n        \\right)\\nonumber\\\\\n    &amp; &amp; \\qquad\n        - 2 \\mu \\left(\\textrm{Cov}_p(H,W) + \\mu\\mathbb{E}_p W - \\mu\\right)\n        \\nonumber\\\\\n    &amp; &amp; \\qquad + \\mu^2 \\textrm{Var}_q(W)\n        \\big]\\nonumber\\\\\n    &amp; = &amp; \\frac{1}{n}\\left[\\mu^2 \\left(\n            1 + \\textrm{Var}_q(W) - \\mathbb{E}_p W\\right)\n        + (\\mathbb{E}_p W) \\textrm{Var}_p(H)\\right].\\label{eqn:H1}\n\\end{eqnarray}</div>\n<p>Finally, we can reduce <span class=\"math\">\\((\\ref{eqn:H1})\\)</span> further by\n</p>\n<div class=\"math\">$$\\mathbb{E}_p W = \\mathbb{E}_q[W^2]\n    = \\textrm{Var}_q(W) + (\\mathbb{E}_q W)^2\n    = \\textrm{Var}_q(W) + 1.$$</div>\n<p>\nFor the other term we have\n</p>\n<div class=\"math\">$$\\frac{1}{n} \\textrm{Var}_p(H) = \\textrm{Var}_p(\\hat{\\mu}).$$</div>\n<p>\nThis simplifies <span class=\"math\">\\((\\ref{eqn:H1})\\)</span> to the following satisfying expression.\n</p>\n<div class=\"math\">$$\\textrm{Var}_q(\\bar{\\mu}) \\approx \\textrm{Var}_p(\\hat{\\mu})\n    (1 + \\textrm{Var}_q(W)).$$</div>\n<p>\nThis reads as \"the variance of the self-normalized importance sampling\nestimate is approximately equal to the variance of the simple Monte Carlo\nestimate times <span class=\"math\">\\(1 + \\textrm{Var}_q(W)\\)</span>.\"</p>\n<p>Therefore, when taking <span class=\"math\">\\(n\\)</span> samples to compute <span class=\"math\">\\(\\bar{\\mu}\\)</span> the <em>effective\nsample size</em> is estimated as\n</p>\n<div class=\"math\">$$\\textrm{ESS} = \\frac{n}{1 + \\textrm{Var}_q(W)}.$$</div>\n<p>Two comments:</p>\n<ol>\n<li>We can estimate <span class=\"math\">\\(\\textrm{Var}_q(W)\\)</span> by the sample variance of the\nnormalized importance weights.</li>\n<li>This estimate does not depend on the integrand <span class=\"math\">\\(h\\)</span>.</li>\n</ol>\n<p>The simpler form of the ESS estimate can be obtained by estimating\n</p>\n<div class=\"math\">\\begin{eqnarray}\n\\textrm{Var}_q(W) &amp; \\approx &amp; \\frac{1}{n} \\sum_{i=1}^n (w_i - \\frac{1}{n})^2\n    \\nonumber\\\\\n    &amp; = &amp; \\frac{1}{n} \\sum_{i=1}^n w_i^2 - \\frac{1}{n^2}.\\nonumber\n\\end{eqnarray}</div>\n<p>\nwhich yields\n</p>\n<div class=\"math\">$$\\textrm{ESS} = \\frac{n}{1 + \\frac{1}{n} \\sum_i w_i^2 - \\frac{1}{n^2}}\n    = \\frac{1}{\\sum_{i=1}^n w_i^2}.$$</div>\n<h2>Conclusion</h2>\n<p>Monte Carlo methods such as importance sampling and Markov chain Monte Carlo\ncan fail in case the proposal distribution is not suitable chosen.  Therefore,\nwe should always employ diagnostics, and for importance sampling the effective\nsampling size diagnostic has become the standard due to its simplicity,\nintuitive interpretation, and its robustness in practical applications.</p>\n<p>However, the effective sample size can fail, for example when all proposal\nsamples are in a region where the target distribution has few probability mass.\nIn that case, the weights would be approximately equal and the ESS close to\noptimal, failing to diagnose the mismatch between proposal and target\ndistribution.  This is, in a way, unavoidable: if we never get to see a high\nprobability region of the target distribution, the low value of our samples is\nhard to recognize.</p>\n<p>For another discussion on importance sampling diagnostics and an alternative\nderivation, see <a href=\"http://statweb.stanford.edu/~owen/mc/Ch-var-is.pdf\">Section 9.3 in Art Owen's upcoming Monte Carlo\nbook</a>.\nAmong many interesting things in that chapter, he proposes an effective sample\nsize statistic specific to the particular integrand <span class=\"math\">\\(h\\)</span>.  For this, redefine\nthe weights as\n</p>\n<div class=\"math\">$$w_h(X_i) = \\frac{\\frac{p(X_i)}{q(X_i)} |h(X_i)|}{\n    \\sum_{i=1}^n \\frac{p(X_i)}{q(X_i)} |h(X_i)|},$$</div>\n<p>\nthen use the normal <span class=\"math\">\\(1/\\sum_i w_h(X_i)^2\\)</span> estimate.  This variant is more\naccurate because it takes the integrand into account.</p>\n<p><em>Addendum:</em> <a href=\"http://arxiv.org/abs/1602.03572\">This paper</a> by Martino, Elvira,\nand Louzada, takes a detailed look at variations of the effective sample size\nstatistic.</p>\n<h2>Appendix: The Multivariate Delta Method</h2>\n<p>The <a href=\"http://en.wikipedia.org/wiki/Delta_method\"><em>delta method</em></a> is a classic\nmethod using in asymptotic statistics to obtain limiting expressions for the\nmean and variance of functions of random variables.  It can be seen as the\nstatistical analog of the Taylor approximation to a function.</p>\n<p>The multivariate extension is also classic, and the following theorem can be\nfound in many works, I picked the one given as Theorem 3.7 in\n<a href=\"http://www.stat.purdue.edu/~dasgupta/\">DasGupta's</a>\n<a href=\"http://www.springer.com/mathematics/probability/book/978-0-387-75970-8\">book on asymptotic\nstatistics</a>\n(by the way, this book is a favorite of mine for its accessible presentation\nof many practical result in classical statistics).\nA more advanced and specialized book on expansions beyond the delta method is\n<a href=\"https://math.uwaterloo.ca/statistics-and-actuarial-science/people-profiles/christopher-small\">Christopher\nSmall</a>'s\n<a href=\"https://books.google.co.uk/books?id=uXexXLoZnZAC\">book on the topic</a>.</p>\n<h3>Delta Method for Distributions</h3>\n<p><em><strong>Theorem</strong> (Multivariate Delta Method for Distributions).</em>  Suppose\n<span class=\"math\">\\(\\{T_n\\}\\)</span> is a sequence of <span class=\"math\">\\(k\\)</span>-dimensional random vectors such that\n</p>\n<div class=\"math\">$$\\sqrt{n}(T_n - \\theta) \\stackrel{\\mathcal{L}}{\\rightarrow}\n    \\mathcal{N}_k(0,\\Sigma(\\theta)).$$</div>\n<p>\nLet <span class=\"math\">\\(g:\\mathbb{R}^k \\to \\mathbb{R}^m\\)</span> be once differentiable at <span class=\"math\">\\(\\theta\\)</span> with\nthe gradient vector <span class=\"math\">\\(\\nabla g(\\theta)\\)</span>.  Then\n</p>\n<div class=\"math\">$$\\sqrt{n}(g(T_n) - g(\\theta)) \\stackrel{\\mathcal{L}}{\\rightarrow}\n    \\mathcal{N}_m(0, \\nabla g(\\theta)^T \\Sigma(\\theta) \\nabla g(\\theta))$$</div>\n<p>\nprovided <span class=\"math\">\\(\\nabla g(\\theta)^T \\Sigma(\\theta) \\nabla g(\\theta)\\)</span> is positive\ndefinite.</p>\n<p>This simply says that if we have a vector <span class=\"math\">\\(T\\)</span> of random variables and we know\nthat <span class=\"math\">\\(T\\)</span> converges asymptotically to a Normal, then we can make a similar\nstatement about the convergence of <span class=\"math\">\\(g(T)\\)</span>.</p>\n<p>For the effective sample size derivation we will need to instantiate this\ntheorem for a special case of <span class=\"math\">\\(g\\)</span>, namely where <span class=\"math\">\\(g: \\mathbb{R}^2 \\to\n\\mathbb{R}\\)</span> and <span class=\"math\">\\(g(x,y) = \\frac{x}{y}\\)</span>.  Let's quickly do that.\nWe have\n</p>\n<div class=\"math\">$$\\nabla g(x,y) = \\left(\\begin{array}{c}\n    \\frac{1}{y} \\\\\n    -\\frac{x}{y^2}\\end{array}\\right).$$</div>\n<p>\nWe further define <span class=\"math\">\\(X_i \\sim P_X\\)</span>, <span class=\"math\">\\(Y_i \\sim P_Y\\)</span> iid, <span class=\"math\">\\(X=X_1\\)</span>, <span class=\"math\">\\(Y=Y_1\\)</span>,\n</p>\n<div class=\"math\">$$T_n=\\left(\\begin{array}{c} \\frac{1}{n}\\sum_{i=1}^n X_i\\\\\n    \\frac{1}{n} \\sum_{i=1}^n Y_i\\end{array}\\right),\\qquad\n    \\theta=\\left(\\begin{array}{c} \\mathbb{E}X\\\\\n        \\mathbb{E}Y\\end{array}\\right),$$</div>\n<p>\nassuming our sequence <span class=\"math\">\\(\\frac{1}{n} \\sum_{i=1}^n X_i \\to \\mathbb{E}X\\)</span> and\n<span class=\"math\">\\(\\frac{1}{n} \\sum_{i=1}^n Y_i \\to \\mathbb{E}Y\\)</span>.\nFor the covariance matrix we know that the empirical average of <span class=\"math\">\\(n\\)</span> iid\nsamples has a variance as <span class=\"math\">\\(1/n\\)</span>, that is\n</p>\n<div class=\"math\">$$\\textrm{Var}(\\frac{1}{n}\\sum_{i=1}^n X_i)\n    = \\frac{1}{n^2} \\textrm{Var}(\\sum_{i=1}^n X_i)\n    = \\frac{1}{n^2} \\sum_{i=1}^n \\textrm{Var}(X_i)\n    = \\frac{1}{n} \\textrm{Var}(X),$$</div>\n<p>\nand <a href=\"http://en.wikipedia.org/wiki/Covariance#Properties\">similar for the\ncovariance</a>, so we have\n</p>\n<div class=\"math\">$$\\Sigma(\\theta) = \\frac{1}{n} \\left(\\begin{array}{cc}\n    \\textrm{Var}(X) &amp; \\textrm{Cov}(X,Y)\\\\\n    \\textrm{Cov}(X,Y) &amp; \\textrm{Var}(Y)\\end{array}\\right).$$</div>\n<p>\nApplying the above theorem we have for the resulting one-dimensional\ntransformed variance\n</p>\n<div class=\"math\">\\begin{eqnarray}\nB(\\theta) &amp; := &amp; \\nabla g(\\theta)^T \\Sigma(\\theta) \\nabla g(\\theta)\\nonumber\\\\\n    &amp; = &amp; \\frac{1}{n} \\left(\\begin{array}{c}\n        \\frac{1}{\\mathbb{E}Y} \\\\\n        -\\frac{\\mathbb{E}X}{(\\mathbb{E}Y)^2}\\end{array}\\right)^T\n        \\left(\\begin{array}{cc}\n            \\textrm{Var}(X) &amp; \\textrm{Cov}(X,Y)\\\\\n            \\textrm{Cov}(X,Y) &amp; \\textrm{Var}(Y)\\end{array}\\right)\n        \\left(\\begin{array}{c}\n        \\frac{1}{\\mathbb{E}Y} \\\\\n        -\\frac{\\mathbb{E}X}{(\\mathbb{E}Y)^2}\\end{array}\\right)\\nonumber\\\\\n    &amp; = &amp; \\frac{1}{n} \\left[\n        \\frac{1}{(\\mathbb{E}Y)^2} \\textrm{Var}(X)\n            - 2 \\frac{\\mathbb{E}X}{(\\mathbb{E}Y)^3} \\textrm{Cov}(X,Y)\n            + \\frac{(\\mathbb{E}X)^2}{(\\mathbb{E}Y)^4} \\textrm{Var}(Y)\n        \\right].\\nonumber\n\\end{eqnarray}</div>\n<p>One way to interpret the quantity <span class=\"math\">\\(B(\\theta)\\)</span> is that the limiting variance of\nthe ratio <span class=\"math\">\\(X/Y\\)</span> depends both on the variances of <span class=\"math\">\\(X\\)</span> and of <span class=\"math\">\\(Y\\)</span>, but crucially\nit depends most sensitively on <span class=\"math\">\\(\\mathbb{E}Y\\)</span> because this quantity appears in\nthe denominator: small values of <span class=\"math\">\\(Y\\)</span> have large effects on <span class=\"math\">\\(X/Y\\)</span>.</p>\n<p>This is an asymptotic expression which is based on the assumption that both\n<span class=\"math\">\\(X\\)</span> and <span class=\"math\">\\(Y\\)</span> are concentrated around the mean so that the linearization of <span class=\"math\">\\(g\\)</span>\naround the mean will incur a small error.  As such, this approximation may\ndeteriorate if the variance of <span class=\"math\">\\(X\\)</span> or <span class=\"math\">\\(Y\\)</span> is large so that the linear\napproximation of <span class=\"math\">\\(g\\)</span> deviates from the actual values of <span class=\"math\">\\(g\\)</span>.</p>\n<p>(For an exact expansion of the expectation of a ratio, see <a href=\"http://www.faculty.biol.ttu.edu/Rice/ratio-derive.pdf\">this 2009\nnote</a> by <a href=\"http://www.faculty.biol.ttu.edu/Rice/\">Sean\nRice</a>.)</p>\n<h3>Second-order Delta Method</h3>\n<p>The above delta method can be extended to higher-order by a <a href=\"http://en.wikipedia.org/wiki/Taylor%27s_theorem#Taylor.27s_theorem_for_multivariate_functions\">multivariate\nTaylor\nexpansion</a>.\nI give the following result without proof.</p>\n<p><em><strong>Theorem</strong> (Second-order Multivariate Delta Method).</em>  Let <span class=\"math\">\\(T\\)</span> be a\n<span class=\"math\">\\(k\\)</span>-dimensional random vectors such that <span class=\"math\">\\(\\mathbb{E} T = \\theta\\)</span>.  Let\n<span class=\"math\">\\(g:\\mathbb{R}^k \\to \\mathbb{R}\\)</span> be twice differentiable at <span class=\"math\">\\(\\theta\\)</span> with\n<a href=\"http://en.wikipedia.org/wiki/Hessian_matrix\">Hessian</a> <span class=\"math\">\\(H(\\theta)\\)</span>.  Then\n</p>\n<div class=\"math\">$$\\mathbb{E} g(T) \\approx\n    g(\\theta) + \\frac{1}{2} \\textrm{tr}(\\textrm{Cov}(T) \\: H(\\theta)).$$</div>\n<p>For the proof of the effective sample size we need to apply this theorem to\nthe function <span class=\"math\">\\(g(X,Y)=XY^2\\)</span> so that\n</p>\n<div class=\"math\">$$H(X,Y)=\\left[\\begin{array}{cc} 0 &amp; 2Y\\\\ 2Y &amp; 2X\\end{array}\\right].$$</div>\n<p>\nThen the above result gives\n</p>\n<div class=\"math\">$$\\mathbb{E} g(X,Y) \\approx\n    (\\mathbb{E}X)(\\mathbb{E}Y)^2 + 2 (\\mathbb{E}Y) \\textrm{Cov}(X,Y)\n    + (\\mathbb{E}X) \\textrm{Var}(Y).$$</div>\n<script type=\"text/javascript\">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}\n</script>",
  "category": ""
}