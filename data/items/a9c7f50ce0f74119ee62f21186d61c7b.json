{
  "title": "Cookbook — Bayesian Modelling with PyMC3",
  "link": "",
  "id": "https://www.georgeho.org/bayesian-modelling-cookbook/",
  "updated": "2018-06-24T00:00:00Z",
  "published": "2018-06-24T00:00:00Z",
  "content": "<p>Recently I&rsquo;ve started using <a href=\"https://github.com/pymc-devs/pymc3\">PyMC3</a> for\nBayesian modelling, and it&rsquo;s an amazing piece of software! The API only exposes\nas much of heavy machinery of MCMC as you need — by which I mean, just the\n<code>pm.sample()</code> method (a.k.a., as <a href=\"http://twiecki.github.io/blog/2013/08/12/bayesian-glms-1/\">Thomas\nWiecki</a> puts it, the\n<em>Magic Inference Button™</em>). This really frees up your mind to think about your\ndata and model, which is really the heart and soul of data science!</p>\n<p>That being said however, I quickly realized that the water gets very deep very\nfast: I explored my data set, specified a hierarchical model that made sense to\nme, hit the <em>Magic Inference Button™</em>, and… uh, what now? I blinked at the\nangry red warnings the sampler spat out.</p>\n<p>So began by long, rewarding and ongoing exploration of Bayesian modelling. This\nis a compilation of notes, tips, tricks and recipes that I&rsquo;ve collected from\neverywhere: papers, documentation, peppering my <a href=\"https://twitter.com/twiecki\">more\nexperienced</a>\n<a href=\"https://twitter.com/aseyboldt\">colleagues</a> with questions. It&rsquo;s still very much\na work in progress, but hopefully somebody else finds it useful!</p>\n<p><img src=\"https://www.georgeho.org/assets/images/pymc-logo.png\" alt=\"PyMC logo\"></p>\n<div>\n<h2>Contents</h2>\n<nav id=\"TableOfContents\">\n<ul>\n<li><a href=\"#for-the-uninitiated\">For the Uninitiated</a>\n<ul>\n<li><a href=\"#bayesian-modelling\">Bayesian modelling</a></li>\n<li><a href=\"#markov-chain-monte-carlo\">Markov-chain Monte Carlo</a></li>\n<li><a href=\"#variational-inference\">Variational inference</a></li>\n</ul>\n</li>\n<li><a href=\"#model-formulation\">Model Formulation</a>\n<ul>\n<li><a href=\"#hierarchical-models\">Hierarchical models</a></li>\n</ul>\n</li>\n<li><a href=\"#model-implementation\">Model Implementation</a></li>\n<li><a href=\"#mcmc-initialization-and-sampling\">MCMC Initialization and Sampling</a></li>\n<li><a href=\"#mcmc-trace-diagnostics\">MCMC Trace Diagnostics</a>\n<ul>\n<li><a href=\"#fixing-divergences\">Fixing divergences</a></li>\n<li><a href=\"#other-common-warnings\">Other common warnings</a></li>\n<li><a href=\"#model-reparameterization\">Model reparameterization</a></li>\n</ul>\n</li>\n<li><a href=\"#model-diagnostics\">Model Diagnostics</a></li>\n</ul>\n</nav>\n</div>\n<h2 id=\"for-the-uninitiated\">For the Uninitiated</h2>\n<ul>\n<li>First of all, <em>welcome!</em> It&rsquo;s a brave new world out there — where statistics\nis cool, Bayesian and (if you&rsquo;re lucky) even easy. Dive in!</li>\n</ul>\n<blockquote>\n<p><strong>EDIT (1/24/2020):</strong> I published a <a href=\"https://www.georgeho.org/bayesian-inference-reading/\">subsequent blog\npost</a> with a reading list\nfor Bayesian inference and modelling. Check it out for reading material in\naddition to the ones I list below!</p>\n</blockquote>\n<h3 id=\"bayesian-modelling\">Bayesian modelling</h3>\n<ul>\n<li>\n<p>If you don&rsquo;t know any probability, I&rsquo;d recommend <a href=\"https://betanalpha.github.io/assets/case_studies/probability_theory.html\">Michael\nBetancourt&rsquo;s</a>\ncrash-course in practical probability theory.</p>\n</li>\n<li>\n<p>For an introduction to general Bayesian methods and modelling, I really liked\n<a href=\"http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/\">Cam Davidson Pilon&rsquo;s <em>Bayesian Methods for\nHackers</em></a>:\nit really made the whole “thinking like a Bayesian” thing click for me.</p>\n</li>\n<li>\n<p>If you&rsquo;re willing to spend some money, I&rsquo;ve heard that <a href=\"https://sites.google.com/site/doingbayesiandataanalysis/\"><em>Doing Bayesian Data\nAnalysis</em> by\nKruschke</a> (a.k.a.\n<em>“the puppy book”</em>) is for the bucket list.</p>\n</li>\n<li>\n<p>Here we come to a fork in the road. The central problem in Bayesian modelling\nis this: given data and a probabilistic model that we think models this data,\nhow do we find the posterior distribution of the model&rsquo;s parameters? There are\ncurrently two good solutions to this problem. One is Markov-chain Monte Carlo\nsampling (a.k.a. MCMC sampling), and the other is variational inference\n(a.k.a. VI). Both methods are mathematical Death Stars: extremely powerful but\nincredibly complicated. Nevertheless, I think it&rsquo;s important to get at least a\nhand-wavy understanding of what these methods are. If you&rsquo;re new to all this,\nmy personal recommendation is to invest your time in learning MCMC: it&rsquo;s been\naround longer, we know that there are sufficiently robust tools to help you,\nand there&rsquo;s a lot more support/documentation out there.</p>\n</li>\n</ul>\n<h3 id=\"markov-chain-monte-carlo\">Markov-chain Monte Carlo</h3>\n<ul>\n<li>\n<p>For a good high-level introduction to MCMC, I liked <a href=\"https://www.youtube.com/watch?v=DJ0c7Bm5Djk&amp;feature=youtu.be&amp;t=4h40m9s\">Michael Betancourt&rsquo;s\nStanCon 2017\ntalk</a>:\nespecially the first few minutes where he provides a motivation for MCMC, that\nreally put all this math into context for me.</p>\n</li>\n<li>\n<p>For a more in-depth (and mathematical) treatment of MCMC, I&rsquo;d check out his\n<a href=\"https://arxiv.org/abs/1701.02434\">paper on Hamiltonian Monte Carlo</a>.</p>\n</li>\n</ul>\n<h3 id=\"variational-inference\">Variational inference</h3>\n<ul>\n<li>\n<p>VI has been around for a while, but it was only in 2017 (2 years ago, at the\ntime of writing) that <em>automatic differentiation variational inference</em> was\ninvented. As such, variational inference is undergoing a renaissance and is\ncurrently an active area of statistical research. Since it&rsquo;s such a nascent\nfield, most resources on it are very theoretical and academic in nature.</p>\n</li>\n<li>\n<p>Chapter 10 (on approximate inference) in Bishop&rsquo;s <em>Pattern Recognition and\nMachine Learning</em> and <a href=\"https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf\">this\ntutorial</a>\nby David Blei are excellent, if a bit mathematically-intensive, resources.</p>\n</li>\n<li>\n<p>The most hands-on explanation of variational inference I&rsquo;ve seen is the docs\nfor <a href=\"http://pyro.ai/examples/svi_part_i.html\">Pyro</a>, a probabilistic\nprogramming language developed by Uber that specializes in variational\ninference.</p>\n</li>\n</ul>\n<h2 id=\"model-formulation\">Model Formulation</h2>\n<ul>\n<li>\n<p>Try thinking about <em>how</em> your data would be generated: what kind of machine\nhas your data as outputs? This will help you both explore your data, as well\nas help you arrive at a reasonable model formulation.</p>\n</li>\n<li>\n<p>Try to avoid correlated variables. Some of the more robust samplers can cope\nwith <em>a posteriori</em> correlated random variables, but sampling is much easier\nfor everyone involved if the variables are uncorrelated. By the way, the bar\nis pretty low here: if the jointplot/scattergram of the two variables looks\nlike an ellipse, thats usually okay. It&rsquo;s when the ellipse starts looking like\na line that you should be alarmed.</p>\n</li>\n<li>\n<p>Try to avoid discrete latent variables, and discrete parameters in general.\nThere is no good method to sample them in a smart way (since discrete\nparameters have no gradients); and with “naïve” samplers (i.e. those that do\nnot take advantage of the gradient), the number of samples one needs to make\ngood inferences generally scales exponentially in the number of parameters.\nFor an instance of this, see <a href=\"https://docs.pymc.io/notebooks/marginalized_gaussian_mixture_model.html\">this example on marginal Gaussian\nmixtures</a>.</p>\n</li>\n<li>\n<p>The <a href=\"https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations\">Stan GitHub\nwiki</a> has\nsome excellent recommendations on how to choose good priors. Once you get a\ngood handle on the basics of using PyMC3, I <em>100% recommend</em> reading this wiki\nfrom start to end: the Stan community has fantastic resources on Bayesian\nstatistics, and even though their APIs are quite different, the mathematical\ntheory all translates over.</p>\n</li>\n</ul>\n<h3 id=\"hierarchical-models\">Hierarchical models</h3>\n<ul>\n<li>\n<p>First of all, hierarchical models can be amazing! <a href=\"https://docs.pymc.io/notebooks/GLM-hierarchical.html\">The PyMC3\ndocs</a> opine on this at\nlength, so let&rsquo;s not waste any digital ink.</p>\n</li>\n<li>\n<p>The poster child of a Bayesian hierarchical model looks something like this\n(equations taken from\n<a href=\"https://en.wikipedia.org/wiki/Bayesian_hierarchical_modeling\">Wikipedia</a>):</p>\n<p><img style=\"float: center\"\nsrc=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/765f37f86fa26bef873048952dccc6e8067b78f4\"\nalt=\"Example Bayesian hierarchical model equation #1\"></p>\n<p><img style=\"float: center\"\nsrc=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/ca8c0e1233fd69fa4325c6eacf8462252ed6b00a\"\nalt=\"Example Bayesian hierarchical model equation #2\"></p>\n<p><img style=\"float: center\"\nsrc=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/1e56b3077b1b3ec867d6a0f2539ba9a3e79b45c1\"\nalt=\"Example Bayesian hierarchical model equation #3\"></p>\n<p>This hierarchy has 3 levels (some would say it has 2 levels, since there are\nonly 2 levels of parameters to infer, but honestly whatever: by my count there\nare 3). 3 levels is fine, but add any more levels, and it becomes harder for\nto sample. Try out a taller hierarchy to see if it works, but err on the side\nof 3-level hierarchies.</p>\n</li>\n<li>\n<p>If your hierarchy is too tall, you can truncate it by introducing a\ndeterministic function of your parameters somewhere (this usually turns out to\njust be a sum). For example, instead of modelling your observations are drawn\nfrom a 4-level hierarchy, maybe your observations can be modeled as the sum of\nthree parameters, where these parameters are drawn from a 3-level hierarchy.</p>\n</li>\n<li>\n<p>More in-depth treatment here in <a href=\"https://arxiv.org/abs/1312.0906\">(Betancourt and Girolami,\n2013)</a>. <strong>tl;dr:</strong> hierarchical models all\nbut <em>require</em> you use to use Hamiltonian Monte Carlo; also included are some\npractical tips and goodies on how to do that stuff in the real world.</p>\n</li>\n</ul>\n<h2 id=\"model-implementation\">Model Implementation</h2>\n<ul>\n<li>\n<p>At the risk of overgeneralizing, there are only two things that can go wrong\nin Bayesian modelling: either your data is wrong, or your model is wrong. And\nit is a hell of a lot easier to debug your data than it is to debug your\nmodel. So before you even try implementing your model, plot histograms of your\ndata, count the number of data points, drop any NaNs, etc. etc.</p>\n</li>\n<li>\n<p>PyMC3 has one quirky piece of syntax, which I tripped up on for a while. It&rsquo;s\ndescribed quite well in <a href=\"http://twiecki.github.io/blog/2014/03/17/bayesian-glms-3/#comment-2213376737\">this comment on Thomas Wiecki&rsquo;s\nblog</a>.\nBasically, suppose you have several groups, and want to initialize several\nvariables per group, but you want to initialize different numbers of variables\nfor each group. Then you need to use the quirky <code>variables[index]</code>\nnotation. I suggest using <code>scikit-learn</code>&rsquo;s <code>LabelEncoder</code> to easily create the\nindex. For example, to make normally distributed heights for the iris dataset:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-python\" data-lang=\"python\"><span style=\"display:flex;\"><span><span style=\"color:#75715e\"># Different numbers of examples for each species</span>\n</span></span><span style=\"display:flex;\"><span>species <span style=\"color:#f92672\">=</span> (<span style=\"color:#ae81ff\">48</span> <span style=\"color:#f92672\">*</span> [<span style=\"color:#e6db74\">&#39;setosa&#39;</span>] <span style=\"color:#f92672\">+</span> <span style=\"color:#ae81ff\">52</span> <span style=\"color:#f92672\">*</span> [<span style=\"color:#e6db74\">&#39;virginica&#39;</span>] <span style=\"color:#f92672\">+</span> <span style=\"color:#ae81ff\">63</span> <span style=\"color:#f92672\">*</span> [<span style=\"color:#e6db74\">&#39;versicolor&#39;</span>])\n</span></span><span style=\"display:flex;\"><span>num_species <span style=\"color:#f92672\">=</span> len(list(set(species))) <span style=\"color:#75715e\"># 3</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#75715e\"># One variable per group</span>\n</span></span><span style=\"display:flex;\"><span>heights_per_species <span style=\"color:#f92672\">=</span> pm<span style=\"color:#f92672\">.</span>Normal(<span style=\"color:#e6db74\">&#39;heights_per_species&#39;</span>,\n</span></span><span style=\"display:flex;\"><span> mu<span style=\"color:#f92672\">=</span><span style=\"color:#ae81ff\">0</span>, sd<span style=\"color:#f92672\">=</span><span style=\"color:#ae81ff\">1</span>, shape<span style=\"color:#f92672\">=</span>num_species)\n</span></span><span style=\"display:flex;\"><span>idx <span style=\"color:#f92672\">=</span> sklearn<span style=\"color:#f92672\">.</span>preprocessing<span style=\"color:#f92672\">.</span>LabelEncoder()<span style=\"color:#f92672\">.</span>fit_transform(species)\n</span></span><span style=\"display:flex;\"><span>heights <span style=\"color:#f92672\">=</span> heights_per_species[idx]\n</span></span></code></pre></div></li>\n<li>\n<p>You might find yourself in a situation in which you want to use a centered\nparameterization for a portion of your data set, but a noncentered\nparameterization for the rest of your data set (see below for what these\nparameterizations are). There&rsquo;s a useful idiom for you here:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-python\" data-lang=\"python\"><span style=\"display:flex;\"><span>num_xs <span style=\"color:#f92672\">=</span> <span style=\"color:#ae81ff\">5</span>\n</span></span><span style=\"display:flex;\"><span>use_centered <span style=\"color:#f92672\">=</span> np<span style=\"color:#f92672\">.</span>array([<span style=\"color:#ae81ff\">0</span>, <span style=\"color:#ae81ff\">1</span>, <span style=\"color:#ae81ff\">1</span>, <span style=\"color:#ae81ff\">0</span>, <span style=\"color:#ae81ff\">1</span>]) <span style=\"color:#75715e\"># len(use_centered) = num_xs</span>\n</span></span><span style=\"display:flex;\"><span>x_sd <span style=\"color:#f92672\">=</span> pm<span style=\"color:#f92672\">.</span>HalfCauchy(<span style=\"color:#e6db74\">&#39;x_sd&#39;</span>, sd<span style=\"color:#f92672\">=</span><span style=\"color:#ae81ff\">1</span>)\n</span></span><span style=\"display:flex;\"><span>x_raw <span style=\"color:#f92672\">=</span> pm<span style=\"color:#f92672\">.</span>Normal(<span style=\"color:#e6db74\">&#39;x_raw&#39;</span>, mu<span style=\"color:#f92672\">=</span><span style=\"color:#ae81ff\">0</span>, sd<span style=\"color:#f92672\">=</span>x_sd<span style=\"color:#f92672\">**</span>use_centered, shape<span style=\"color:#f92672\">=</span>num_xs)\n</span></span><span style=\"display:flex;\"><span>x <span style=\"color:#f92672\">=</span> pm<span style=\"color:#f92672\">.</span>Deterministic(<span style=\"color:#e6db74\">&#39;x&#39;</span>, x_sd<span style=\"color:#f92672\">**</span>(<span style=\"color:#ae81ff\">1</span> <span style=\"color:#f92672\">-</span> use_centered) <span style=\"color:#f92672\">*</span> x_raw)\n</span></span></code></pre></div><p>You could even experiment with allowing <code>use_centered</code> to be <em>between</em> 0 and\n1, instead of being <em>either</em> 0 or 1!</p>\n</li>\n<li>\n<p>I prefer to use the <code>pm.Deterministic</code> function instead of simply using normal\narithmetic operations (e.g. I&rsquo;d prefer to write <code>x = pm.Deterministic('x', y + z)</code> instead of <code>x = y + z</code>). This means that you can index the <code>trace</code> object\nlater on with just <code>trace['x']</code>, instead of having to compute it yourself with\n<code>trace['y'] + trace['z']</code>.</p>\n</li>\n</ul>\n<h2 id=\"mcmc-initialization-and-sampling\">MCMC Initialization and Sampling</h2>\n<ul>\n<li>\n<p>Have faith in PyMC3&rsquo;s default initialization and sampling settings: someone\nmuch more experienced than us took the time to choose them! NUTS is the most\nefficient MCMC sampler known to man, and <code>jitter+adapt_diag</code>… well, you get\nthe point.</p>\n</li>\n<li>\n<p>However, if you&rsquo;re truly grasping at straws, a more powerful initialization\nsetting would be <code>advi</code> or <code>advi+adapt_diag</code>, which uses variational inference\nto initialize the sampler. An even better option would be to use\n<code>advi+adapt_diag_grad</code> <del>which is (at the time of writing) an experimental\nfeature in beta</del>.</p>\n</li>\n<li>\n<p>Never initialize the sampler with the MAP estimate! In low dimensional\nproblems the MAP estimate (a.k.a. the mode of the posterior) is often quite a\nreasonable point. But in high dimensions, the MAP becomes very strange. Check\nout <a href=\"http://www.inference.vc/high-dimensional-gaussian-distributions-are-soap-bubble/\">Ferenc Huszár&rsquo;s blog\npost</a>\non high-dimensional Gaussians to see why. Besides, at the MAP all the derivatives\nof the posterior are zero, and that isn&rsquo;t great for derivative-based samplers.</p>\n</li>\n</ul>\n<h2 id=\"mcmc-trace-diagnostics\">MCMC Trace Diagnostics</h2>\n<ul>\n<li>You&rsquo;ve hit the <em>Magic Inference Button™</em>, and you have a <code>trace</code> object. Now\nwhat? First of all, make sure that your sampler didn&rsquo;t barf itself, and that\nyour chains are safe for consumption (i.e., analysis).</li>\n</ul>\n<ol>\n<li>\n<p>Theoretically, run the chain for as long as you have the patience or\nresources for. In practice, just use the PyMC3 defaults: 500 tuning\niterations, 1000 sampling iterations.</p>\n</li>\n<li>\n<p>Check for divergences. PyMC3&rsquo;s sampler will spit out a warning if there are\ndiverging chains, but the following code snippet may make things easier:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-python\" data-lang=\"python\"><span style=\"display:flex;\"><span><span style=\"color:#75715e\"># Display the total number and percentage of divergent chains</span>\n</span></span><span style=\"display:flex;\"><span>diverging <span style=\"color:#f92672\">=</span> trace[<span style=\"color:#e6db74\">&#39;diverging&#39;</span>]\n</span></span><span style=\"display:flex;\"><span>print(<span style=\"color:#e6db74\">&#39;Number of Divergent Chains: </span><span style=\"color:#e6db74\">{}</span><span style=\"color:#e6db74\">&#39;</span><span style=\"color:#f92672\">.</span>format(diverging<span style=\"color:#f92672\">.</span>nonzero()[<span style=\"color:#ae81ff\">0</span>]<span style=\"color:#f92672\">.</span>size))\n</span></span><span style=\"display:flex;\"><span>diverging_pct <span style=\"color:#f92672\">=</span> diverging<span style=\"color:#f92672\">.</span>nonzero()[<span style=\"color:#ae81ff\">0</span>]<span style=\"color:#f92672\">.</span>size <span style=\"color:#f92672\">/</span> len(trace) <span style=\"color:#f92672\">*</span> <span style=\"color:#ae81ff\">100</span>\n</span></span><span style=\"display:flex;\"><span>print(<span style=\"color:#e6db74\">&#39;Percentage of Divergent Chains: </span><span style=\"color:#e6db74\">{:.1f}</span><span style=\"color:#e6db74\">&#39;</span><span style=\"color:#f92672\">.</span>format(diverging_pct))\n</span></span></code></pre></div></li>\n<li>\n<p>Check the traceplot (<code>pm.traceplot(trace)</code>). You&rsquo;re looking for traceplots\nthat look like “fuzzy caterpillars”. If the trace moves into some region and\nstays there for a long time (a.k.a. there are some “sticky regions”), that&rsquo;s\ncause for concern! That indicates that once the sampler moves into some\nregion of parameter space, it gets stuck there (probably due to high\ncurvature or other bad topological properties).</p>\n</li>\n<li>\n<p>In addition to the traceplot, there are <a href=\"https://docs.pymc.io/api/plots.html\">a ton of other\nplots</a> you can make with your trace:</p>\n<ul>\n<li><code>pm.plot_posterior(trace)</code>: check if your posteriors look reasonable.</li>\n<li><code>pm.forestplot(trace)</code>: check if your variables have reasonable credible\nintervals, and Gelman–Rubin scores close to 1.</li>\n<li><code>pm.autocorrplot(trace)</code>: check if your chains are impaired by high\nautocorrelation. Also remember that thinning your chains is a waste of\ntime at best, and deluding yourself at worst. See Chris Fonnesbeck&rsquo;s\ncomment on <a href=\"https://github.com/pymc-devs/pymc/issues/23\">this GitHub\nissue</a> and <a href=\"https://twitter.com/junpenglao/status/1009748562136256512\">Junpeng Lao&rsquo;s\nreply to Michael Betancourt&rsquo;s\ntweet</a></li>\n<li><code>pm.energyplot(trace)</code>: ideally the energy and marginal energy\ndistributions should look very similar. Long tails in the distribution of\nenergy levels indicates deteriorated sampler efficiency.</li>\n<li><code>pm.densityplot(trace)</code>: a souped-up version of <code>pm.plot_posterior</code>. It\ndoesn&rsquo;t seem to be wildly useful unless you&rsquo;re plotting posteriors from\nmultiple models.</li>\n</ul>\n</li>\n<li>\n<p>PyMC3 has a nice helper function to pretty-print a summary table of the\ntrace: <code>pm.summary(trace)</code> (I usually tack on a <code>.round(2)</code> for my sanity).\nLook out for:</p>\n<ul>\n<li>the $\\hat{R}$ values (a.k.a. the Gelman–Rubin statistic, a.k.a. the\npotential scale reduction factor, a.k.a. the PSRF): are they all close to\n1? If not, something is <em>horribly</em> wrong. Consider respecifying or\nreparameterizing your model. You can also inspect these in the forest plot.</li>\n<li>the sign and magnitude of the inferred values: do they make sense, or are\nthey unexpected and unreasonable? This could indicate a poorly specified\nmodel. (E.g. parameters of the unexpected sign that have low uncertainties\nmight indicate that your model needs interaction terms.)</li>\n</ul>\n</li>\n<li>\n<p>As a drastic debugging measure, try to <code>pm.sample</code> with <code>draws=1</code>,\n<code>tune=500</code>, and <code>discard_tuned_samples=False</code>, and inspect the traceplot.\nDuring the tuning phase, we don&rsquo;t expect to see friendly fuzzy caterpillars,\nbut we <em>do</em> expect to see good (if noisy) exploration of parameter space. So\nif the sampler is getting stuck during the tuning phase, that might explain\nwhy the trace looks horrible.</p>\n</li>\n<li>\n<p>If you get scary errors that describe mathematical problems (e.g. <code>ValueError: Mass matrix contains zeros on the diagonal. Some derivatives might always be zero.</code>), then you&rsquo;re <del>shit out of luck</del> exceptionally unlucky: those kinds of\nerrors are notoriously hard to debug. I can only point to the <a href=\"http://andrewgelman.com/2008/05/13/the_folk_theore/\">Folk Theorem of\nStatistical Computing</a>:</p>\n<blockquote>\n<p>If you&rsquo;re having computational problems, probably your model is wrong.</p>\n</blockquote>\n</li>\n</ol>\n<h3 id=\"fixing-divergences\">Fixing divergences</h3>\n<blockquote>\n<p><code>There were N divergences after tuning. Increase 'target_accept' or reparameterize.</code></p>\n<p>— The <em>Magic Inference Button™</em></p>\n</blockquote>\n<ul>\n<li>\n<p>Divergences in HMC occur when the sampler finds itself in regions of extremely\nhigh curvature (such as the opening of the a hierarchical funnel). Broadly\nspeaking, the sampler is prone to malfunction in such regions, causing the\nsampler to fly off towards to infinity. The ruins the chains by heavily\nbiasing the samples.</p>\n</li>\n<li>\n<p>Remember: if you have even <em>one</em> diverging chain, you should be worried.</p>\n</li>\n<li>\n<p>Increase <code>target_accept</code>: usually 0.9 is a good number (currently the default\nin PyMC3 is 0.8). This will help get rid of false positives from the test for\ndivergences. However, divergences that <em>don&rsquo;t</em> go away are cause for alarm.</p>\n</li>\n<li>\n<p>Increasing <code>tune</code> can sometimes help as well: this gives the sampler more time\nto 1) find the typical set and 2) find good values for the step size, mass\nmatrix elements, etc. If you&rsquo;re running into divergences, it&rsquo;s always possible\nthat the sampler just hasn&rsquo;t started the mixing phase and is still trying to\nfind the typical set.</p>\n</li>\n<li>\n<p>Consider a <em>noncentered</em> parameterization. This is an amazing trick: it all\nboils down to the familiar equation $X = \\sigma Z + \\mu$ from STAT 101, but\nit honestly works wonders. See <a href=\"http://twiecki.github.io/blog/2017/02/08/bayesian-hierchical-non-centered/\">Thomas Wiecki&rsquo;s blog\npost</a>\non it, and <a href=\"https://docs.pymc.io/notebooks/Diagnosing_biased_Inference_with_Divergences.html\">this page from the PyMC3\ndocumentation</a>.</p>\n</li>\n<li>\n<p>If that doesn&rsquo;t work, there may be something wrong with the way you&rsquo;re\nthinking about your data: consider reparameterizing your model, or\nrespecifying it entirely.</p>\n</li>\n</ul>\n<h3 id=\"other-common-warnings\">Other common warnings</h3>\n<ul>\n<li>\n<p>It&rsquo;s worth noting that far and away the worst warning to get is the one about\ndivergences. While a divergent chain indicates that your inference may be\nflat-out <em>invalid</em>, the rest of these warnings indicate that your inference is\nmerely (lol, “merely”) <em>inefficient</em>.</p>\n</li>\n<li>\n<p>It&rsquo;s also worth noting that the <a href=\"https://mc-stan.org/misc/warnings.html\">Brief Guide to Stan&rsquo;s\nWarnings</a> is a tremendous resource for\nexactly what kinds of errors you might get when running HMC or NUTS, and how\nyou should think about them.</p>\n</li>\n<li>\n<p><code>The number of effective samples is smaller than XYZ for some parameters.</code></p>\n<ul>\n<li>Quoting <a href=\"https://discourse.pymc.io/t/the-number-of-effective-samples-is-smaller-than-25-for-some-parameters/1050/3\">Junpeng Lao on\n<code>discourse.pymc3.io</code></a>:\n“A low number of effective samples is usually an indication of strong\nautocorrelation in the chain.”</li>\n<li>Make sure you&rsquo;re using an efficient sampler like NUTS. (And not, for\ninstance, Gibbs or Metropolis–Hastings.)</li>\n<li>Tweak the acceptance probability (<code>target_accept</code>) — it should be large\nenough to ensure good exploration, but small enough to not reject all\nproposals and get stuck.</li>\n</ul>\n</li>\n<li>\n<p><code>The gelman-rubin statistic is larger than XYZ for some parameters. This indicates slight problems during sampling.</code></p>\n<ul>\n<li>When PyMC3 samples, it runs several chains in parallel. Loosely speaking,\nthe Gelman–Rubin statistic measures how similar these chains are. Ideally it\nshould be close to 1.</li>\n<li>Increasing the <code>tune</code> parameter may help, for the same reasons as described\nin the <em>Fixing Divergences</em> section.</li>\n</ul>\n</li>\n<li>\n<p><code>The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.</code></p>\n<ul>\n<li>NUTS puts a cap on the depth of the trees that it evaluates during each\niteration, which is controlled through the <code>max_treedepth</code>. Reaching the\nmaximum allowable tree depth indicates that NUTS is prematurely pulling the\nplug to avoid excessive compute time.</li>\n<li>Yeah, what the <em>Magic Inference Button™</em> says: try increasing\n<code>max_treedepth</code> or <code>target_accept</code>.</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"model-reparameterization\">Model reparameterization</h3>\n<ul>\n<li>\n<p>Countless warnings have told you to engage in this strange activity of\n“reparameterization”. What even is that? Luckily, the <a href=\"https://github.com/stan-dev/stan/releases\">Stan User\nManual</a> (specifically the\n<em>Reparameterization and Change of Variables</em> section) has an excellent\nexplanation of reparameterization, and even some practical tips to help you do\nit (although your mileage may vary on how useful those tips will be to you).</p>\n</li>\n<li>\n<p>Asides from meekly pointing to other resources, there&rsquo;s not much I can do to\nhelp: this stuff really comes from a combination of intuition, statistical\nknowledge and good ol&rsquo; experience. I can, however, cite some examples to give\nyou a better idea.</p>\n<ul>\n<li>The noncentered parameterization is a classic example. If you have a\nparameter whose mean and variance you are also modelling, the noncentered\nparameterization decouples the sampling of mean and variance from the\nsampling of the parameter, so that they are now independent. In this way, we\navoid “funnels”.</li>\n<li>The <a href=\"http://proceedings.mlr.press/v5/carvalho09a.html\"><em>horseshoe\ndistribution</em></a> is known to\nbe a good shrinkage prior, as it is <em>very</em> spikey near zero, and has <em>very</em>\nlong tails. However, modelling it using one parameter can give multimodal\nposteriors — an exceptionally bad result. The trick is to reparameterize and\nmodel it as the product of two parameters: one to create spikiness at zero,\nand one to create long tails (which makes sense: to sample from the\nhorseshoe, take the product of samples from a normal and a half-Cauchy).</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"model-diagnostics\">Model Diagnostics</h2>\n<ul>\n<li>Admittedly the distinction between the previous section and this one is\nsomewhat artificial (since problems with your chains indicate problems with\nyour model), but I still think it&rsquo;s useful to make this distinction because\nthese checks indicate that you&rsquo;re thinking about your data in the wrong way,\n(i.e. you made a poor modelling decision), and <em>not</em> that the sampler is\nhaving a hard time doing its job.</li>\n</ul>\n<ol>\n<li>\n<p>Run the following snippet of code to inspect the pairplot of your variables\none at a time (if you have a plate of variables, it&rsquo;s fine to pick a couple\nat random). It&rsquo;ll tell you if the two random variables are correlated, and\nhelp identify any troublesome neighborhoods in the parameter space (divergent\nsamples will be colored differently, and will cluster near such\nneighborhoods).</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-python\" data-lang=\"python\"><span style=\"display:flex;\"><span>pm<span style=\"color:#f92672\">.</span>pairplot(trace,\n</span></span><span style=\"display:flex;\"><span> sub_varnames<span style=\"color:#f92672\">=</span>[variable_1, variable_2],\n</span></span><span style=\"display:flex;\"><span> divergences<span style=\"color:#f92672\">=</span><span style=\"color:#66d9ef\">True</span>,\n</span></span><span style=\"display:flex;\"><span> color<span style=\"color:#f92672\">=</span><span style=\"color:#e6db74\">&#39;C3&#39;</span>,\n</span></span><span style=\"display:flex;\"><span> kwargs_divergence<span style=\"color:#f92672\">=</span>{<span style=\"color:#e6db74\">&#39;color&#39;</span>: <span style=\"color:#e6db74\">&#39;C2&#39;</span>})\n</span></span></code></pre></div></li>\n<li>\n<p>Look at your posteriors (either from the traceplot, density plots or\nposterior plots). Do they even make sense? E.g. are there outliers or long\ntails that you weren&rsquo;t expecting? Do their uncertainties look reasonable to\nyou? If you had <a href=\"https://en.wikipedia.org/wiki/Plate_notation\">a plate</a> of\nvariables, are their posteriors different? Did you expect them to be that\nway? If not, what about the data made the posteriors different? You&rsquo;re the\nonly one who knows your problem/use case, so the posteriors better look good\nto you!</p>\n</li>\n<li>\n<p>Broadly speaking, there are four kinds of bad geometries that your posterior\ncan suffer from:</p>\n<ul>\n<li>highly correlated posteriors: this will probably cause divergences or\ntraces that don&rsquo;t look like “fuzzy caterpillars”. Either look at the\njointplots of each pair of variables, or look at the correlation matrix of\nall variables. Try using a centered parameterization, or reparameterize in\nsome other way, to remove these correlations.</li>\n<li>posteriors that form “funnels”: this will probably cause divergences. Try\nusing a noncentered parameterization.</li>\n<li>heavy tailed posteriors: this will probably raise warnings about\n<code>max_treedepth</code> being exceeded. If your data has long tails, you should\nmodel that with a long-tailed distribution. If your data doesn&rsquo;t have long\ntails, then your model is ill-specified: perhaps a more informative prior\nwould help.</li>\n<li>multimodal posteriors: right now this is pretty much a death blow. At the\ntime of writing, all samplers have a hard time with multimodality, and\nthere&rsquo;s not much you can do about that. Try reparameterizing to get a\nunimodal posterior. If that&rsquo;s not possible (perhaps you&rsquo;re <em>modelling</em>\nmultimodality using a mixture model), you&rsquo;re out of luck: just let NUTS\nsample for a day or so, and hopefully you&rsquo;ll get a good trace.</li>\n</ul>\n</li>\n<li>\n<p>Pick a small subset of your raw data, and see what exactly your model does\nwith that data (i.e. run the model on a specific subset of your data). I find\nthat a lot of problems with your model can be found this way.</p>\n</li>\n<li>\n<p>Run <a href=\"https://docs.pymc.io/notebooks/posterior_predictive.html\"><em>posterior predictive\nchecks</em></a> (a.k.a.\nPPCs): sample from your posterior, plug it back in to your model, and\n“generate new data sets”. PyMC3 even has a nice function to do all this for\nyou: <code>pm.sample_ppc</code>. But what do you do with these new data sets? That&rsquo;s a\nquestion only you can answer! The point of a PPC is to see if the generated\ndata sets reproduce patterns you care about in the observed real data set,\nand only you know what patterns you care about. E.g. how close are the PPC\nmeans to the observed sample mean? What about the variance?</p>\n<ul>\n<li>For example, suppose you were modelling the levels of radon gas in\ndifferent counties in a country (through a hierarchical model). Then you\ncould sample radon gas levels from the posterior for each county, and take\nthe maximum within each county. You&rsquo;d then have a distribution of maximum\nradon gas levels across counties. You could then check if the <em>actual</em>\nmaximum radon gas level (in your observed data set) is acceptably within\nthat distribution. If it&rsquo;s much larger than the maxima, then you would know\nthat the actual likelihood has longer tails than you assumed (e.g. perhaps\nyou should use a Student&rsquo;s T instead of a normal?)</li>\n<li>Remember that how well the posterior predictive distribution fits the data\nis of little consequence (e.g. the expectation that 90% of the data should\nfall within the 90% credible interval of the posterior). The posterior\npredictive distribution tells you what values for data you would expect if\nwe were to remeasure, given that you&rsquo;ve already observed the data you did.\nAs such, it&rsquo;s informed by your prior as well as your data, and it&rsquo;s not its\njob to adequately fit your data!</li>\n</ul>\n</li>\n</ol>"
}