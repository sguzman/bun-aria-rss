{
  "title": "If you did not already know",
  "link": "https://analytixon.com/2022/11/05/if-you-did-not-already-know-1878/",
  "comments": "https://analytixon.com/2022/11/05/if-you-did-not-already-know-1878/#respond",
  "dc:creator": "Michael Laux",
  "pubDate": "Sat, 05 Nov 2022 22:23:37 +0000",
  "category": "What is ...",
  "guid": "https://analytixon.com/?p=37457",
  "description": "Recombinator-k-Means We present a heuristic algorithm, called recombinator-k-means, that can substantially improve the results of k-means optimization. Instead of using &#8230;<p><a href=\"https://analytixon.com/2022/11/05/if-you-did-not-already-know-1878/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a></p>",
  "content:encoded": "<p><a href=\"http://arxiv.org/abs/1905.00531v1\" target=\"top\" rel=\"noopener\"><strong>Recombinator-k-Means</strong></a>  <a href=\"https://www.google.de/search?q=Recombinator-k-Means\" target=\"_blank\" rel=\"noopener\"><img decoding=\"async\" class=\"alignright\" src=\"https://analytixon.files.wordpress.com/2015/01/google.png?w=529\" alt=\"google\" data-recalc-dims=\"1\"/></a><BR/><span style=\"font-size:12px;font-style:normal;text-align:justify;\">We present a heuristic algorithm, called recombinator-k-means, that can substantially improve the results of k-means optimization. Instead of using simple independent restarts and returning the best result, our scheme performs restarts in batches, using the results of a previous batch as a reservoir of candidates for the new initial starting values (seeds), exploiting the popular k-means++ seeding algorithm to piece them together into new promising initial configurations. Our scheme is general (it only affects the seeding part of the optimization, thus it could be applied even to k-medians or k-medoids, for example), it has no additional costs and it is trivially parallelizable across the restarts of each batch. In some circumstances, it can systematically find better configurations than the best one obtained after 10^4 restarts of a standard scheme. Our implementation is publicly available at <a href=\"https://github.com/carlobaldassi/RecombinatorKMeans.jl.\" target=\"top\" rel=\"noopener\">https://&#8230;/RecombinatorKMeans.jl.</a>  &#8230; </span><BR/><BR/><a href=\"http://arxiv.org/abs/1902.07494v1\" target=\"top\" rel=\"noopener\"><strong>Neural Attentive Interpretable Recommendation System (NAIRS)</strong></a>  <a href=\"https://www.google.de/search?q=Neural Attentive Interpretable Recommendation System\" target=\"_blank\" rel=\"noopener\"><img decoding=\"async\" class=\"alignright\" src=\"https://analytixon.files.wordpress.com/2015/01/google.png?w=529\" alt=\"google\" data-recalc-dims=\"1\"/></a><BR/><span style=\"font-size:12px;font-style:normal;text-align:justify;\">In this paper, we develop a neural attentive interpretable recommendation system, named NAIRS. A self-attention network, as a key component of the system, is designed to assign attention weights to interacted items of a user. This attention mechanism can distinguish the importance of the various interacted items in contributing to a user profile. Based on the user profiles obtained by the self-attention network, NAIRS offers personalized high-quality recommendation. Moreover, it develops visual cues to interpret recommendations. This demo application with the implementation of NAIRS enables users to interact with a recommendation system, and it persistently collects training data to improve the system. The demonstration and experimental results show the effectiveness of NAIRS. &#8230; </span><BR/><BR/><a href=\"http://arxiv.org/abs/1905.06336v1\" target=\"top\" rel=\"noopener\"><strong>Field Attentive Deep Field-aware Factorization Machine (FAT-DeepFFM)</strong></a>  <a href=\"https://www.google.de/search?q=Field Attentive Deep Field-aware Factorization Machine\" target=\"_blank\" rel=\"noopener\"><img decoding=\"async\" class=\"alignright\" src=\"https://analytixon.files.wordpress.com/2015/01/google.png?w=529\" alt=\"google\" data-recalc-dims=\"1\"/></a><BR/><span style=\"font-size:12px;font-style:normal;text-align:justify;\">Click through rate (CTR) estimation is a fundamental task in personalized advertising and recommender systems. Recent years have witnessed the success of both the deep learning based model and attention mechanism in various tasks in computer vision (CV) and natural language processing (NLP). How to combine the attention mechanism with deep CTR model is a promising direction because it may ensemble the advantages of both sides. Although some CTR model such as Attentional Factorization Machine (AFM) has been proposed to model the weight of second order interaction features, we posit the evaluation of feature importance before explicit feature interaction procedure is also important for CTR prediction tasks because the model can learn to selectively highlight the informative features and suppress less useful ones if the task has many input features. In this paper, we propose a new neural CTR model named Field Attentive Deep Field-aware Factorization Machine (FAT-DeepFFM) by combining the Deep Field-aware Factorization Machine (DeepFFM) with Compose-Excitation network (CENet) field attention mechanism which is proposed by us as an enhanced version of Squeeze-Excitation network (SENet) to highlight the feature importance. We conduct extensive experiments on two real-world datasets and the experiment results show that FAT-DeepFFM achieves the best performance and obtains different improvements over the state-of-the-art methods. We also compare two kinds of attention mechanisms (attention before explicit feature interaction vs. attention after explicit feature interaction) and demonstrate that the former one outperforms the latter one significantly. &#8230; </span><BR/><BR/><a href=\"https://www.ncbi.nlm.nih.gov/pubmed/28264647\" target=\"top\" rel=\"noopener\"><strong>Repertoire Dissimilarity Index</strong></a>  <a href=\"https://www.google.de/search?q=Repertoire Dissimilarity Index\" target=\"_blank\" rel=\"noopener\"><img decoding=\"async\" class=\"alignright\" src=\"https://analytixon.files.wordpress.com/2015/01/google.png?w=529\" alt=\"google\" data-recalc-dims=\"1\"/></a><BR/><span style=\"font-size:12px;font-style:normal;text-align:justify;\">In this paper, we present a non-parametric method for directly comparing sequencing repertoires, with the goal of rigorously quantifying differences in V, D, and J gene segment utilization. This method, referred to as the Repertoire Dissimilarity Index (RDI), uses a bootstrapped subsampling approach to account for variance in sequencing depth, and, coupled with a data simulation approach, allows for direct quantification of the average variation between repertoires. We use the RDI method to recapitulate known differences in the formation of the CD4+ and CD8+ T cell repertoires, and further show that antigen-driven activation of na√Øve CD8+ T cells is more selective than in the CD4+ repertoire, resulting in a more specialized CD8+ memory repertoire. &#8230; </span><BR/></p>\n",
  "wfw:commentRss": "https://analytixon.com/2022/11/05/if-you-did-not-already-know-1878/feed/",
  "slash:comments": 0,
  "post-id": 37457
}