{
  "title": "Leave-one-out cross-validation",
  "link": "",
  "published": "2015-08-01T16:08:00-07:00",
  "updated": "2015-08-01T16:08:00-07:00",
  "author": {
    "name": "Jonathan Landy"
  },
  "id": "tag:efavdb.com,2015-08-01:/leave-one-out-cross-validation",
  "summary": "<p>This will be the first of a series of short posts relating to subject matter discussed in the text, <a href=\"http://www-bcf.usc.edu/~gareth/ISL/\">&#8220;An Introduction to Statistical Learning&#8221;</a>. This is an interesting read, but it often skips over statement proofs &#8212; that&#8217;s where this series of posts comes in! Here, I consider the content …</p>",
  "content": "<p>This will be the first of a series of short posts relating to subject matter discussed in the text, <a href=\"http://www-bcf.usc.edu/~gareth/ISL/\">&#8220;An Introduction to Statistical Learning&#8221;</a>. This is an interesting read, but it often skips over statement proofs &#8212; that&#8217;s where this series of posts comes in! Here, I consider the content of Section 5.1.2: This gives a lightning-quick &#8220;short cut&#8221; method for evaluating a regression&#8217;s leave-one-out cross-validation error. The method is applicable to any least-squares linear&nbsp;fit.</p>\n<h3>Introduction: Leave-one-out&nbsp;cross-validation</h3>\n<p>When carrying out a <a href=\"https://en.wikipedia.org/wiki/Regression_analysis\">regression analysis</a>, one is often interested in two types of error measurement. The first is the training set error and the second is the generalization error. The former relates to how close the regression is to the data being fit. In contrast, the generalization error relates to how accurate the model will be when applied to other points. The latter is of particular interest whenever the regression will be used to make predictions on new&nbsp;points.</p>\n<p><a href=\"https://en.wikipedia.org/wiki/Cross-validation_(statistics)\">Cross-validation</a> provides one method for estimating generalization errors. The approach centers around splitting the training data available into two sets, <em>a cross-validation training set</em> and <em>cross-validation test set</em>. The first of these is used for training a regression model. Its accuracy on the test set then provides a generalization error estimate. Here, we focus on a special form of cross-validation, called <em>leave-one-out cross-validation</em> (<span class=\"caps\">LOOCV</span>). In this case, we pick only one point as the test set. We then build a model on all the remaining, complementary points, and evaluate its error on the single-point held out. A generalization error estimate is obtained by repeating this procedure for each of the training points available, averaging the&nbsp;results.</p>\n<p><span class=\"caps\">LOOCV</span> can be computationally expensive because it generally requires one to construct many models &#8212; equal in number to the size of the training set. However, for the special case of least-squares polynomial regression we have the following &#8220;short cut&#8221; identity:<br>\n</p>\n<div class=\"math\">$$ \\label{theorem} \\tag{1}  \n\\sum_i \\left ( \\tilde{y}_i - y_i\\right)^2 = \\sum_i \\left ( \\frac{\\hat{y}_i - y_i}{1 - h_i}\\right)^2.  \n$$</div>\n<p><br>\nHere, <span class=\"math\">\\(y_i\\)</span> is the actual label value of training point <span class=\"math\">\\(i\\)</span>, <span class=\"math\">\\(\\tilde{y}_i\\)</span> is the value predicted by the cross-validation model trained on all points except <span class=\"math\">\\(i\\)</span>, <span class=\"math\">\\(\\hat{y}_i\\)</span> is the value predicted by the regression model trained on all points (including point <span class=\"math\">\\(i\\)</span>), and <span class=\"math\">\\(h_i\\)</span> is a function of the coordinate <span class=\"math\">\\(\\vec{x}_i\\)</span> &#8212; this is defined further below. Notice that the left side of (\\ref{theorem}) is the <span class=\"caps\">LOOCV</span> sum of squares error (the quantity we seek), while the right can be evaluated given only the model trained on the full data set. Fantastically, this allows us to evaluate the <span class=\"caps\">LOOCV</span> error using only a single&nbsp;regression!</p>\n<h3>Statement&nbsp;proof</h3>\n<p>Consider the <span class=\"caps\">LOOCV</span> step where we construct a model trained on all points except training example <span class=\"math\">\\(k\\)</span>. Using a linear model of form <span class=\"math\">\\(\\tilde{y}(\\vec{x}) \\equiv \\vec{x}^T \\cdot \\vec{\\beta}_k\\)</span> &#8212; with <span class=\"math\">\\(\\vec{\\beta}_k\\)</span> a coefficient vector &#8212; the sum of squares that must be minimized is<br>\n</p>\n<div class=\"math\">$$\\tag{2} \\label{error_sum}  \nJ_k \\equiv \\sum_{i \\not = k} \\left ( \\tilde{y}_i - y_i \\right)^2 = \\sum_{i \\not = k} \\left (\\vec{x}^T_i \\cdot \\vec{\\beta}_k - y_i \\right)^2.  \n$$</div>\n<p><br>\nHere, we&#8217;re using a subscript <span class=\"math\">\\(k\\)</span> on <span class=\"math\">\\(\\vec{\\beta}_k\\)</span> to highlight the fact that the above corresponds to the case where example <span class=\"math\">\\(k\\)</span> is held out. We minimize (\\ref{error_sum}) by taking the gradient with respect to <span class=\"math\">\\(\\vec{\\beta}_k\\)</span>. Setting this to zero gives the equation<br>\n</p>\n<div class=\"math\">$$\\tag{3}  \n\\left( \\sum_{i \\not = k} \\vec{x}_i \\vec{x}_i^T \\right) \\cdot \\vec{\\beta}_k = \\sum_{i \\not = k} y_i \\vec{x}_i.  \n$$</div>\n<p><br>\nSimilarly, the full model (trained on all points) coefficient vector <span class=\"math\">\\(\\vec{\\beta}\\)</span> satisfies<br>\n</p>\n<div class=\"math\">$$\\tag{4} \\label{full_con}  \n\\left( \\sum_{i} \\vec{x}_i \\vec{x}_i^T \\right) \\cdot \\vec{\\beta} \\equiv M \\cdot \\vec{\\beta} = \\sum_{i} y_i \\vec{x}_i.  \n$$</div>\n<p><br>\nCombining the prior two equations gives,<br>\n</p>\n<div class=\"math\">$$\\tag{5}  \n\\left (M - \\vec{x}_k \\vec{x}_k^T \\right) \\cdot \\vec{\\beta}_k = \\left (\\sum_{i} y_i \\vec{x}_i\\right) - y_k \\vec{x}_k = M\\cdot \\vec{\\beta} - y_k \\vec{x}_k.  \n$$</div>\n<p><br>\nUsing the definition of <span class=\"math\">\\(\\tilde{y}_k\\)</span>, rearrangement of the above leads to the identity<br>\n</p>\n<div class=\"math\">$$\\tag{6}  \nM \\cdot \\left ( \\vec{\\beta}_k - \\vec{\\beta} \\right) = \\left (\\tilde{y}_k - y_k \\right) \\vec{x}_k.  \n$$</div>\n<p><br>\nLeft multiplication by <span class=\"math\">\\(\\vec{x}_k^T M^{-1}\\)</span> gives,<br>\n</p>\n<div class=\"math\">$$\\tag{7}  \n\\tilde{y}_k - \\hat{y}_k = \\left( \\tilde{y}_k - y_k\\right) - \\left( \\hat{y}_k - y_k \\right) = \\vec{x}_k^T M^{-1} \\vec{x}_k \\left (\\tilde{y}_k - y_k \\right).  \n$$</div>\n<p><br>\nFinally, combining like-terms, squaring, and summing gives<br>\n</p>\n<div class=\"math\">$$\\tag{8}  \n\\sum_k \\left (\\tilde{y}_k - y_k \\right) ^2 = \\sum_k \\left (\\frac{\\hat{y}_k - y_k}{1 -\\vec{x}_k^T M^{-1} \\vec{x}_k } \\right)^2.  \n$$</div>\n<p><br>\nThis is (\\ref{theorem}), where we now see the parameter <span class=\"math\">\\(h_k \\equiv \\vec{x}_k^T M^{-1} \\vec{x}_k\\)</span>. This is referred to as the &#8220;leverage&#8221; of <span class=\"math\">\\(\\vec{x}_k\\)</span> in the text. Notice also that <span class=\"math\">\\(M\\)</span> is proportional to the correlation matrix of the <span class=\"math\">\\(\\{\\vec{x}_i\\}\\)</span>. <span class=\"math\">\\(\\blacksquare\\)</span></p>\n<script type=\"text/javascript\">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}\n</script>",
  "category": ""
}