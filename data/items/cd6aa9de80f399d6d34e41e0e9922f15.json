{
  "title": "Active Learning of Non-semantic Speech Tasks with Pretrained Models. (arXiv:2211.00119v2 [cs.SD] UPDATED)",
  "link": "http://arxiv.org/abs/2211.00119",
  "description": "<p>Pretraining neural networks with massive unlabeled datasets has become\npopular as it equips the deep models with a better prior to solve downstream\ntasks. However, this approach generally assumes that for downstream tasks, we\nhave access to annotated data of sufficient size. In this work, we propose\nALOE, a novel system for improving the data- and label-efficiency of\nnon-semantic speech tasks with active learning (AL). ALOE uses pre-trained\nmodels in conjunction with active learning to label data incrementally and\nlearns classifiers for downstream tasks, thereby mitigating the need to acquire\nlabeled data beforehand. We demonstrate the effectiveness of ALOE on a wide\nrange of tasks, uncertainty-based acquisition functions, and model\narchitectures. Training a linear classifier on top of a frozen encoder with\nALOE is shown to achieve performance similar to several baselines that utilize\nthe entire labeled data.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Harlin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saeed_A/0/1/0/all/0/1\">Aaqib Saeed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertozzi_A/0/1/0/all/0/1\">Andrea L. Bertozzi</a>"
}