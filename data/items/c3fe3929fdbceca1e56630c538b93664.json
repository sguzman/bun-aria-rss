{
  "title": "Guest post by Julien Mairal: A Kernel Point of View on Convolutional Neural Networks, part II",
  "link": "https://blogs.princeton.edu/imabandit/2019/07/17/guest-post-by-julien-mairal-a-kernel-point-of-view-on-convolutional-neural-networks-part-ii/",
  "comments": "https://blogs.princeton.edu/imabandit/2019/07/17/guest-post-by-julien-mairal-a-kernel-point-of-view-on-convolutional-neural-networks-part-ii/#comments",
  "dc:creator": "Sebastien Bubeck",
  "pubDate": "Wed, 17 Jul 2019 16:02:03 +0000",
  "category": "Machine learning",
  "guid": "https://blogs.princeton.edu/imabandit/?p=1397",
  "description": "<p>This is a continuation of Julien Mairal&#8216;s guest post on CNNs, see part I here. Stability to deformations of convolutional neural networks In their ICML paper Zhang et al. introduce a functional space for CNNs with one layer, by noticing &#8230; <a href=\"https://blogs.princeton.edu/imabandit/2019/07/17/guest-post-by-julien-mairal-a-kernel-point-of-view-on-convolutional-neural-networks-part-ii/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a></p>\n",
  "content:encoded": "<p><a href=\"https://blogs.princeton.edu/imabandit/wp-content/uploads/sites/122/2019/07/Mairal2.jpg\" class=\"liimagelink\"><img decoding=\"async\" loading=\"lazy\" class=\"alignnone wp-image-1399\" src=\"https://blogs.princeton.edu/imabandit/wp-content/uploads/sites/122/2019/07/Mairal2-300x152.jpg\" alt=\"\" width=\"639\" height=\"324\" srcset=\"https://blogs.princeton.edu/imabandit/wp-content/uploads/sites/122/2019/07/Mairal2-300x152.jpg 300w, https://blogs.princeton.edu/imabandit/wp-content/uploads/sites/122/2019/07/Mairal2-768x389.jpg 768w, https://blogs.princeton.edu/imabandit/wp-content/uploads/sites/122/2019/07/Mairal2-1024x518.jpg 1024w, https://blogs.princeton.edu/imabandit/wp-content/uploads/sites/122/2019/07/Mairal2.jpg 1302w\" sizes=\"(max-width: 639px) 100vw, 639px\" /></a></p>\n<p>This is a continuation of <a href=\"https://lear.inrialpes.fr/people/mairal/\" class=\"liinternal\">Julien Mairal</a>&#8216;s guest post on CNNs, see <a href=\"https://blogs.princeton.edu/imabandit/2019/07/10/guest-post-by-julien-mairal-a-kernel-point-of-view-on-convolutional-neural-networks-part-i/\" class=\"liinternal\">part I here.</a></p>\n<p><strong>Stability to deformations of convolutional neural networks</strong></p>\n<p>In their <a href=\"http://proceedings.mlr.press/v70/zhang17f/zhang17f.pdf\" class=\"lipdf\">ICML paper</a> Zhang et al. introduce a functional space for CNNs with one layer, by noticing that for some dot-product kernels, smoothed variants of rectified linear unit activation functions (ReLU) live in the corresponding RKHS, see also <a href=\"http://proceedings.mlr.press/v48/zhangd16.pdf\" class=\"lipdf\">this paper</a> and <a href=\"https://www.cs.cornell.edu/~sridharan/sicomp.pdf\" class=\"lipdf\">that one</a>. By following a similar reasoning with multiple layers, it is then possible to show that the functional space described in <a href=\"https://blogs.princeton.edu/imabandit/2019/07/10/guest-post-by-julien-mairal-a-kernel-point-of-view-on-convolutional-neural-networks-part-i/\" class=\"liinternal\">part I</a> <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-3e321e5f0406c9879f25b6b1d69a5fc3_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#123;&#32;&#102;&#95;&#119;&#58;&#32;&#120;&#32;&#92;&#109;&#97;&#112;&#115;&#116;&#111;&#32;&#92;&#108;&#97;&#110;&#103;&#108;&#101;&#32;&#119;&#32;&#44;&#32;&#92;&#80;&#104;&#105;&#95;&#110;&#40;&#120;&#95;&#48;&#41;&#32;&#92;&#114;&#97;&#110;&#103;&#108;&#101;&#59;&#32;&#119;&#32;&#92;&#105;&#110;&#32;&#76;&#94;&#50;&#40;&#92;&#79;&#109;&#101;&#103;&#97;&#44;&#92;&#109;&#97;&#116;&#104;&#99;&#97;&#108;&#123;&#72;&#125;&#95;&#110;&#41;&#32;&#92;&#125;\" title=\"Rendered by QuickLaTeX.com\" height=\"20\" width=\"298\" style=\"vertical-align: -5px;\"/> contains CNNs with such smoothed ReLU, and that the norm <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-62e1a48032624994ba16c4e26421676e_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#124;&#102;&#95;&#119;&#92;&#124;\" title=\"Rendered by QuickLaTeX.com\" height=\"19\" width=\"34\" style=\"vertical-align: -5px;\"/> of such networks can be controlled by the spectral norms of filter matrices. This is consistent with previous measures of complexity for CNNs, see <a href=\"https://papers.nips.cc/paper/7204-spectrally-normalized-margin-bounds-for-neural-networks.pdf\" class=\"lipdf\">this paper</a> by Bartlett et al.</p>\n<p>A perhaps more interesting finding is that the abstract representation <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-3ca9a5351b772e88bebe85e7e4a13632_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#80;&#104;&#105;&#95;&#110;&#40;&#120;&#41;\" title=\"Rendered by QuickLaTeX.com\" height=\"18\" width=\"45\" style=\"vertical-align: -4px;\"/>, which only depends on the network architecture, may provide near-translation invariance and stability to small image deformations while preserving information&#8212;that is, <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-1b9fbfb207b6d17d74b33c6d8342a1a4_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#120;\" title=\"Rendered by QuickLaTeX.com\" height=\"8\" width=\"10\" style=\"vertical-align: 0px;\"/> can be recovered from <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-3ca9a5351b772e88bebe85e7e4a13632_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#80;&#104;&#105;&#95;&#110;&#40;&#120;&#41;\" title=\"Rendered by QuickLaTeX.com\" height=\"18\" width=\"45\" style=\"vertical-align: -4px;\"/>. The original characterization we use was introduced by Mallat in <a href=\"https://www.di.ens.fr/~mallat/papiers/ScatCPAM.pdf\" class=\"lipdf\">his paper</a> on the scattering transform&#8212;a multilayer architecture akin to CNNs based on wavelets, and was extended to <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-273242b8e92b3a9f4dc13c62b2785bd3_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#80;&#104;&#105;&#95;&#110;\" title=\"Rendered by QuickLaTeX.com\" height=\"16\" width=\"21\" style=\"vertical-align: -3px;\"/> by Alberto Bietti, who should be credited for all the hard work here.</p>\n<p>Our goal is to understand under which conditions it is possible to obtain a representation that (i) is near-translation invariant, (ii) is stable to deformations, (iii) preserves signal information. Given a <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-2e9ea203bbd77c5cd8bee967e2729d8b_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#67;&#94;&#49;\" title=\"Rendered by QuickLaTeX.com\" height=\"15\" width=\"20\" style=\"vertical-align: 0px;\"/>-diffeomorphism <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c6f8f1dde2ee4682653c2a6b37d8a42d_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#116;&#97;&#117;&#58;&#32;&#92;&#109;&#97;&#116;&#104;&#98;&#98;&#123;&#82;&#125;&#94;&#50;&#32;&#92;&#116;&#111;&#32;&#92;&#109;&#97;&#116;&#104;&#98;&#98;&#123;&#82;&#125;&#94;&#50;\" title=\"Rendered by QuickLaTeX.com\" height=\"16\" width=\"93\" style=\"vertical-align: -1px;\"/> and denoting by <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-835d9f864f712213ee317332b3f3675a_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#76;&#95;&#92;&#116;&#97;&#117;&#32;&#120;&#40;&#117;&#41;&#32;&#61;&#32;&#120;&#40;&#117;&#45;&#92;&#116;&#97;&#117;&#40;&#117;&#41;&#41;\" title=\"Rendered by QuickLaTeX.com\" height=\"18\" width=\"167\" style=\"vertical-align: -4px;\"/> its action operator (for an image defined on the continuous domain <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-d5abe0f29e8cc710ae26f4f0af5a0859_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#109;&#97;&#116;&#104;&#98;&#98;&#123;&#82;&#125;&#94;&#50;\" title=\"Rendered by QuickLaTeX.com\" height=\"15\" width=\"20\" style=\"vertical-align: 0px;\"/>), the main stability bound we obtain is the following one, see Theorem 7 in <a href=\"https://www.di.ens.fr/~mallat/papiers/ScatCPAM.pdf\" class=\"lipdf\">Mallat&#8217;s paper</a> if <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-758b3cac273166048ed1879acf427860_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#124;&#92;&#110;&#97;&#98;&#108;&#97;&#32;&#92;&#116;&#97;&#117;&#92;&#124;&#95;&#92;&#105;&#110;&#102;&#116;&#121;&#32;&#92;&#108;&#101;&#113;&#32;&#49;&#47;&#50;\" title=\"Rendered by QuickLaTeX.com\" height=\"19\" width=\"104\" style=\"vertical-align: -5px;\"/>, for all <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-1b9fbfb207b6d17d74b33c6d8342a1a4_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#120;\" title=\"Rendered by QuickLaTeX.com\" height=\"8\" width=\"10\" style=\"vertical-align: 0px;\"/>,</p>\n<p class=\"ql-center-displayed-equation\" style=\"line-height: 43px;\"><span class=\"ql-right-eqno\"> &nbsp; </span><span class=\"ql-left-eqno\"> &nbsp; </span><img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-51041d0067a72066938e31b1f00529fa_l3.png\" height=\"43\" width=\"455\" class=\"ql-img-displayed-equation \" alt=\"&#92;&#91; &#92;&#124;&#32;&#92;&#80;&#104;&#105;&#95;&#110;&#40;&#76;&#95;&#92;&#116;&#97;&#117;&#32;&#120;&#41;&#32;&#45;&#32;&#92;&#80;&#104;&#105;&#95;&#110;&#40;&#120;&#41;&#92;&#124;&#32;&#92;&#108;&#101;&#113;&#32;&#92;&#108;&#101;&#102;&#116;&#32;&#40;&#32;&#67;&#95;&#49;&#32;&#40;&#49;&#43;&#110;&#41;&#32;&#92;&#124;&#92;&#110;&#97;&#98;&#108;&#97;&#32;&#92;&#116;&#97;&#117;&#92;&#124;&#95;&#92;&#105;&#110;&#102;&#116;&#121;&#32;&#43;&#32;&#92;&#102;&#114;&#97;&#99;&#123;&#67;&#95;&#50;&#125;&#123;&#92;&#115;&#105;&#103;&#109;&#97;&#95;&#110;&#125;&#32;&#92;&#124;&#92;&#116;&#97;&#117;&#92;&#124;&#95;&#92;&#105;&#110;&#102;&#116;&#121;&#32;&#92;&#114;&#105;&#103;&#104;&#116;&#41;&#32;&#92;&#124;&#120;&#92;&#124;&#44; &#92;&#93;\" title=\"Rendered by QuickLaTeX.com\"/></p>\n<p>where <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-08d1f29fa9c0981e916619b6c6bc7eee_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#67;&#95;&#49;&#44;&#32;&#67;&#95;&#50;\" title=\"Rendered by QuickLaTeX.com\" height=\"16\" width=\"48\" style=\"vertical-align: -4px;\"/> are universal constants, <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c0142846a2999e170f7beec7be1523f2_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#115;&#105;&#103;&#109;&#97;&#95;&#110;\" title=\"Rendered by QuickLaTeX.com\" height=\"11\" width=\"18\" style=\"vertical-align: -3px;\"/> is the scale parameter of the pooling operator <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-e1872c7d7a65e0dc92f8a4a04608b88a_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#65;&#95;&#110;\" title=\"Rendered by QuickLaTeX.com\" height=\"15\" width=\"21\" style=\"vertical-align: -3px;\"/> corresponding to the &#8220;amount of pooling&#8221; performed up to the last layer, <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c1a5effb150d36de3c7074eaa980c357_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#124;&#92;&#116;&#97;&#117;&#92;&#124;&#95;&#92;&#105;&#110;&#102;&#116;&#121;\" title=\"Rendered by QuickLaTeX.com\" height=\"19\" width=\"39\" style=\"vertical-align: -5px;\"/> is the maximum pixel displacement and <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-ab4c5d3fe8fd25af25beb4f58a55c938_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#124;&#92;&#110;&#97;&#98;&#108;&#97;&#32;&#92;&#116;&#97;&#117;&#92;&#124;&#95;&#92;&#105;&#110;&#102;&#116;&#121;\" title=\"Rendered by QuickLaTeX.com\" height=\"19\" width=\"53\" style=\"vertical-align: -5px;\"/> represents the maximum amount of deformation, see <a href=\"https://www.di.ens.fr/~mallat/papiers/ScatCPAM.pdf\" class=\"lipdf\">the paper</a> for the precise definitions of all these quantities. Note that when <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b732bf857c5f04c7d10dda247f1a5022_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#67;&#95;&#50;&#47;&#92;&#115;&#105;&#103;&#109;&#97;&#95;&#110;&#32;&#92;&#116;&#111;&#32;&#48;\" title=\"Rendered by QuickLaTeX.com\" height=\"18\" width=\"85\" style=\"vertical-align: -5px;\"/>, the representation <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-273242b8e92b3a9f4dc13c62b2785bd3_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#80;&#104;&#105;&#95;&#110;\" title=\"Rendered by QuickLaTeX.com\" height=\"16\" width=\"21\" style=\"vertical-align: -3px;\"/> becomes translation invariant: indeed, consider the particular case of <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-3af6c51247895b176bb502f0ee0857ee_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#116;&#97;&#117;\" title=\"Rendered by QuickLaTeX.com\" height=\"8\" width=\"10\" style=\"vertical-align: 0px;\"/> being a translation, then <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-aa1278a7149925a4f299de0dbb85cec0_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#110;&#97;&#98;&#108;&#97;&#32;&#92;&#116;&#97;&#117;&#61;&#48;\" title=\"Rendered by QuickLaTeX.com\" height=\"14\" width=\"57\" style=\"vertical-align: -1px;\"/> and <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-cd1d650abd9970e357384c0653960577_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#124;&#92;&#80;&#104;&#105;&#95;&#110;&#40;&#76;&#95;&#92;&#116;&#97;&#117;&#32;&#120;&#41;&#32;&#45;&#32;&#92;&#80;&#104;&#105;&#95;&#110;&#40;&#120;&#41;&#92;&#124;&#32;&#92;&#116;&#111;&#32;&#48;\" title=\"Rendered by QuickLaTeX.com\" height=\"19\" width=\"186\" style=\"vertical-align: -5px;\"/>.</p>\n<p>The stability bound and a few additional results tell us a few things about the network architecture: (a) small patches lead to more stable representations (the dependency is hidden in <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-782c65cbd411fb8862688afc92bc1eea_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#67;&#95;&#49;\" title=\"Rendered by QuickLaTeX.com\" height=\"16\" width=\"19\" style=\"vertical-align: -4px;\"/>); (b) signal preservation for discrete signals requires small subsampling factors (and thus small pooling) between layers. In such a setting, the scale parameter <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c0142846a2999e170f7beec7be1523f2_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#115;&#105;&#103;&#109;&#97;&#95;&#110;\" title=\"Rendered by QuickLaTeX.com\" height=\"11\" width=\"18\" style=\"vertical-align: -3px;\"/> still grows exponentially with <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-a63eb5ff0272d3119fa684be6e7acce8_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#110;\" title=\"Rendered by QuickLaTeX.com\" height=\"8\" width=\"11\" style=\"vertical-align: 0px;\"/> and near translation invariance may be achieved with several layers.</p>\n<p>Interestingly, we may now come back to the Cauchy-Schwarz inequality from part 1, and note that if <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-273242b8e92b3a9f4dc13c62b2785bd3_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#80;&#104;&#105;&#95;&#110;\" title=\"Rendered by QuickLaTeX.com\" height=\"16\" width=\"21\" style=\"vertical-align: -3px;\"/> is stable, the RKHS norm <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-afe70184469e7e3a14405a7193eedf29_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#124;&#102;&#92;&#124;\" title=\"Rendered by QuickLaTeX.com\" height=\"19\" width=\"24\" style=\"vertical-align: -5px;\"/> is then a natural quantity that provides stability to deformations to the prediction function <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c7d97b919a3b73617cf2fbb375fff3b1_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#102;\" title=\"Rendered by QuickLaTeX.com\" height=\"16\" width=\"10\" style=\"vertical-align: -4px;\"/>, in addition to measuring model complexity in a traditional sense.</p>\n<p><strong>Feature learning in RKHSs and convolutional kernel networks</strong></p>\n<p>The previous paragraph is devoted to the characterization of convolutional architectures such as CNNs but the previous kernel construction can in fact be used to derive more traditional kernel methods. After all, why should one spend efforts defining a kernel between images if not to use it?</p>\n<p>This can be achieved by considering finite-dimensional approximations of the previous feature maps. In order to shorten the presentation, we simply describe the main idea based on the Nystrom approximation and refer to <a href=\"http://papers.nips.cc/paper/6184-end-to-end-kernel-learning-with-supervised-convolutional-kernel-networks.pdf\" class=\"lipdf\">the paper</a> for more details. Approximating the infinite-dimensional feature maps <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-3ad23c5c360c3f33031a5d000d37416f_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#120;&#95;&#107;\" title=\"Rendered by QuickLaTeX.com\" height=\"11\" width=\"17\" style=\"vertical-align: -3px;\"/> (see the figure at the top of <a href=\"https://blogs.princeton.edu/imabandit/2019/07/10/guest-post-by-julien-mairal-a-kernel-point-of-view-on-convolutional-neural-networks-part-i/\" class=\"liinternal\">part I</a>) can be done by projecting each point in <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-bc26f0de4084a72b9e625a080bd5d674_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#109;&#97;&#116;&#104;&#99;&#97;&#108;&#123;&#72;&#125;&#95;&#107;\" title=\"Rendered by QuickLaTeX.com\" height=\"15\" width=\"22\" style=\"vertical-align: -3px;\"/> onto a <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-5c72bc331dc0008f57d454e7071dc39e_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#112;&#95;&#107;\" title=\"Rendered by QuickLaTeX.com\" height=\"12\" width=\"17\" style=\"vertical-align: -4px;\"/>-dimensional subspace <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b08b176c0bf0adbd9cbe41b31147e1f7_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#109;&#97;&#116;&#104;&#99;&#97;&#108;&#123;&#70;&#125;&#95;&#107;\" title=\"Rendered by QuickLaTeX.com\" height=\"16\" width=\"20\" style=\"vertical-align: -3px;\"/> leading to a finite-dimensional feature map <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-5f55b75318f3b8da67917ee0b0e190ce_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#116;&#105;&#108;&#100;&#101;&#123;&#120;&#125;&#95;&#107;\" title=\"Rendered by QuickLaTeX.com\" height=\"15\" width=\"17\" style=\"vertical-align: -3px;\"/> akin to CNNs, see the figure at the top of the post.</p>\n<p>By parametrizing <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-5dd8802df8efddb9acc5056af47339d7_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#109;&#97;&#116;&#104;&#99;&#97;&#108;&#123;&#70;&#125;&#95;&#107;&#61;&#92;&#116;&#101;&#120;&#116;&#123;&#115;&#112;&#97;&#110;&#125;&#40;&#92;&#118;&#97;&#114;&#112;&#104;&#105;&#95;&#107;&#40;&#122;&#95;&#49;&#41;&#44;&#92;&#118;&#97;&#114;&#112;&#104;&#105;&#95;&#107;&#40;&#122;&#95;&#50;&#41;&#44;&#92;&#108;&#100;&#111;&#116;&#115;&#44;&#92;&#118;&#97;&#114;&#112;&#104;&#105;&#95;&#107;&#40;&#122;&#95;&#123;&#112;&#95;&#107;&#125;&#41;&#41;\" title=\"Rendered by QuickLaTeX.com\" height=\"20\" width=\"297\" style=\"vertical-align: -6px;\"/> with <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-5c72bc331dc0008f57d454e7071dc39e_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#112;&#95;&#107;\" title=\"Rendered by QuickLaTeX.com\" height=\"12\" width=\"17\" style=\"vertical-align: -4px;\"/> anchor points <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-4614e1cdba47dc6a6db7957fb1d82632_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#90;&#61;&#91;&#122;&#95;&#49;&#44;&#92;&#108;&#100;&#111;&#116;&#115;&#44;&#122;&#95;&#123;&#112;&#95;&#107;&#125;&#93;\" title=\"Rendered by QuickLaTeX.com\" height=\"19\" width=\"123\" style=\"vertical-align: -6px;\"/>, and using a dot-product kernel, a patch <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-d9d772a59543419785ce66946592259a_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#122;\" title=\"Rendered by QuickLaTeX.com\" height=\"8\" width=\"9\" style=\"vertical-align: 0px;\"/> from <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-6419748397a324cd2a2ebc3f119b7f80_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#116;&#105;&#108;&#100;&#101;&#123;&#120;&#125;&#95;&#123;&#107;&#45;&#49;&#125;\" title=\"Rendered by QuickLaTeX.com\" height=\"16\" width=\"35\" style=\"vertical-align: -4px;\"/> is encoded through the mapping function</p>\n<p class=\"ql-center-displayed-equation\" style=\"line-height: 43px;\"><span class=\"ql-right-eqno\"> &nbsp; </span><span class=\"ql-left-eqno\"> &nbsp; </span><img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-cc43da382f024d96cb50e3dc3f051d6f_l3.png\" height=\"43\" width=\"306\" class=\"ql-img-displayed-equation \" alt=\"&#92;&#91; &#92;&#112;&#115;&#105;&#95;&#107;&#40;&#122;&#41;&#32;&#61;&#32;&#92;&#124;&#122;&#92;&#124;&#32;&#92;&#107;&#97;&#112;&#112;&#97;&#95;&#107;&#40;&#32;&#90;&#94;&#92;&#116;&#111;&#112;&#32;&#90;&#41;&#94;&#123;&#45;&#49;&#47;&#50;&#125;&#32;&#92;&#107;&#97;&#112;&#112;&#97;&#95;&#107;&#92;&#108;&#101;&#102;&#116;&#40;&#32;&#90;&#94;&#92;&#116;&#111;&#112;&#32;&#92;&#102;&#114;&#97;&#99;&#123;&#122;&#125;&#123;&#92;&#124;&#122;&#92;&#124;&#125;&#32;&#92;&#114;&#105;&#103;&#104;&#116;&#41;&#44; &#92;&#93;\" title=\"Rendered by QuickLaTeX.com\"/></p>\n<p>where <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-684fcf23472c51919624049fb4e0129a_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#107;&#97;&#112;&#112;&#97;&#95;&#107;\" title=\"Rendered by QuickLaTeX.com\" height=\"11\" width=\"17\" style=\"vertical-align: -3px;\"/> is applied pointwise. Then, computing <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-5f55b75318f3b8da67917ee0b0e190ce_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#116;&#105;&#108;&#100;&#101;&#123;&#120;&#125;&#95;&#107;\" title=\"Rendered by QuickLaTeX.com\" height=\"15\" width=\"17\" style=\"vertical-align: -3px;\"/> from <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-6419748397a324cd2a2ebc3f119b7f80_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#116;&#105;&#108;&#100;&#101;&#123;&#120;&#125;&#95;&#123;&#107;&#45;&#49;&#125;\" title=\"Rendered by QuickLaTeX.com\" height=\"16\" width=\"35\" style=\"vertical-align: -4px;\"/> admits a CNN interpretation, where only the normalization and the matrix multiplication by <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-a74cffbbd51922298a13f864fbedaa98_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#107;&#97;&#112;&#112;&#97;&#95;&#107;&#40;&#32;&#90;&#94;&#92;&#116;&#111;&#112;&#32;&#90;&#41;&#94;&#123;&#45;&#49;&#47;&#50;&#125;\" title=\"Rendered by QuickLaTeX.com\" height=\"21\" width=\"103\" style=\"vertical-align: -4px;\"/> are not standard operations. It remains now to choose the anchor points:</p>\n<ul>\n<li><strong>kernel approximation:</strong> a first approach consists of using a variant of the Nystrom method, see <a href=\"https://papers.nips.cc/paper/1866-using-the-nystrom-method-to-speed-up-kernel-machines.pdf\" class=\"lipdf\">this paper</a> and <a href=\"http://home.cse.ust.hk/~twinsen/nystrom.pdf\" class=\"lipdf\">that one</a>. When plugging the corresponding image representation in a linear classifier, the resulting approach behaves as a classical kernel machine. Empirically, we observe that the higher the number of anchor points, the better the kernel approximation, and the higher the accuracy. For instance, a two-layer network with a <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-81a4466abb5fecba81f8a3aa055a1a14_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#51;&#48;&#48;&#107;\" title=\"Rendered by QuickLaTeX.com\" height=\"13\" width=\"36\" style=\"vertical-align: 0px;\"/>-dimensional representations achieves about <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-0eea28372ada596bc618b4b94fee69ec_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#56;&#54;&#92;&#37;\" title=\"Rendered by QuickLaTeX.com\" height=\"15\" width=\"32\" style=\"vertical-align: -1px;\"/> accuracy on CIFAR-10 without data augmentation (see <a href=\"https://gitlab.inria.fr/mairal/ckn-cudnn-matlab\" class=\"liinternal\">here</a>).</li>\n<li><strong>back-propagation, feature selection</strong>: learning the anchor points <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-cc9f8fff9fd24060bc054e78f01d5bfb_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#90;\" title=\"Rendered by QuickLaTeX.com\" height=\"12\" width=\"12\" style=\"vertical-align: 0px;\"/> can also be done as in a traditional CNN, by optimizing them end-to-end. This allows using deeper lower-dimensional architectures and empirically seems to perform better when enough data is available, e.g., <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-659ab3cccda2422f955af880d20646cf_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#57;&#50;&#92;&#37;\" title=\"Rendered by QuickLaTeX.com\" height=\"15\" width=\"32\" style=\"vertical-align: -1px;\"/> accuracy on CIFAR-10 with simple data augmentation. There, the subspaces <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b08b176c0bf0adbd9cbe41b31147e1f7_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#109;&#97;&#116;&#104;&#99;&#97;&#108;&#123;&#70;&#125;&#95;&#107;\" title=\"Rendered by QuickLaTeX.com\" height=\"16\" width=\"20\" style=\"vertical-align: -3px;\"/> are not learned anymore to provide the best kernel approximation, but the model seems to perform a sort of feature selection in each layer&#8217;s RKHS <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-bc26f0de4084a72b9e625a080bd5d674_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#109;&#97;&#116;&#104;&#99;&#97;&#108;&#123;&#72;&#125;&#95;&#107;\" title=\"Rendered by QuickLaTeX.com\" height=\"15\" width=\"22\" style=\"vertical-align: -3px;\"/>, which is not well understood yet (This feature selection interpretation is due to my collaborator Laurent Jacob).</li>\n</ul>\n<p>Note that the first CKN model published <a href=\"https://papers.nips.cc/paper/5348-convolutional-kernel-networks.pdf\" class=\"lipdf\">here</a> was based on a different approximation principle, which was not compatible with end-to-end training. We found this to be less scalable and effective.</p>\n<p><strong>Other links between neural networks and kernel methods</strong></p>\n<p>Finally, other links between kernels and infinitely-wide neural networks with random weights are classical, but they were not the topic of this blog post (they should be the topic of another one!). In a nutshell, for a large collection of weights distributions and nonlinear functions <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b4680e3f9e8274687d2d04f0a262ed00_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#115;&#58;&#32;&#92;&#109;&#97;&#116;&#104;&#98;&#98;&#123;&#82;&#125;&#32;&#92;&#116;&#111;&#32;&#92;&#109;&#97;&#116;&#104;&#98;&#98;&#123;&#82;&#125;\" title=\"Rendered by QuickLaTeX.com\" height=\"13\" width=\"76\" style=\"vertical-align: -1px;\"/>, the following quantity admits an analytical form</p>\n<p class=\"ql-center-displayed-equation\" style=\"line-height: 22px;\"><span class=\"ql-right-eqno\"> &nbsp; </span><span class=\"ql-left-eqno\"> &nbsp; </span><img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-9144f2d0b847adb69db90629ed805148_l3.png\" height=\"22\" width=\"229\" class=\"ql-img-displayed-equation \" alt=\"&#92;&#91; &#75;&#40;&#120;&#44;&#120;'&#41;&#32;&#61;&#32;&#92;&#69;&#95;&#123;&#119;&#125;&#91;&#32;&#115;&#40;&#119;&#94;&#92;&#116;&#111;&#112;&#32;&#120;&#41;&#32;&#115;&#40;&#119;&#94;&#92;&#116;&#111;&#112;&#32;&#120;'&#41;&#93;&#44; &#92;&#93;\" title=\"Rendered by QuickLaTeX.com\"/></p>\n<p>where the terms <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-da82e444bbc3a5e594b7edbf0b1ba3a0_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#115;&#40;&#119;&#94;&#92;&#116;&#111;&#112;&#32;&#120;&#41;\" title=\"Rendered by QuickLaTeX.com\" height=\"19\" width=\"56\" style=\"vertical-align: -4px;\"/> may be seen as an infinitely-wide single-layer neural network. The first time such a relation appears is likely to be in <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.446.9306&rep=rep1&type=pdf\" class=\"liexternal\">the PhD thesis</a> of Radford Neal with a Gaussian process interpretation, and it was revisited later by <a href=\"http://proceedings.mlr.press/v2/leroux07a/leroux07a.pdf\" class=\"lipdf\">Le Roux and Bengio</a> and by <a href=\"http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf\" class=\"lipdf\">Cho and Saul</a> with multilayer models.</p>\n<p>In particular, when <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-3bcfb3f0b6b04be3b598743cd774dd78_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#115;\" title=\"Rendered by QuickLaTeX.com\" height=\"8\" width=\"8\" style=\"vertical-align: 0px;\"/> is the rectified linear unit and <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-78d46af3f19bae0d88ac0cabd450a296_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#119;\" title=\"Rendered by QuickLaTeX.com\" height=\"8\" width=\"13\" style=\"vertical-align: 0px;\"/> follows a Gaussian distribution, it is known that we recover the arc-cosine kernel. We may also note that <a href=\"http://papers.nips.cc/paper/3182-random-features-for-large-scale-kernel-machines.pdf\" class=\"lipdf\">random Fourier features</a> also yield a similar interpretation.</p>\n<p>Other important links have also been drawn recently between kernel regression and strongly over-parametrized neural networks, see <a href=\"http://papers.nips.cc/paper/8076-neural-tangent-kernel-convergence-and-generalization-in-neural-networks.pdf\" class=\"lipdf\">this paper</a> and <a href=\"https://arxiv.org/pdf/1812.07956.pdf\" class=\"lipdf\">that one</a>, which is another exciting story.</p>\n",
  "wfw:commentRss": "https://blogs.princeton.edu/imabandit/2019/07/17/guest-post-by-julien-mairal-a-kernel-point-of-view-on-convolutional-neural-networks-part-ii/feed/",
  "slash:comments": 31
}