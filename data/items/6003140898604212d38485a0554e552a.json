{
  "title": "Combining Contrastive and Non-Contrastive Losses for Fine-Tuning Pretrained Models in Speech Analysis. (arXiv:2211.01964v1 [cs.CL])",
  "link": "http://arxiv.org/abs/2211.01964",
  "description": "<p>Embedding paralinguistic properties is a challenging task as there are only a\nfew hours of training data available for domains such as emotional speech. One\nsolution to this problem is to pretrain a general self-supervised speech\nrepresentation model on large amounts of unlabeled speech. This pretrained\nmodel is then finetuned to a specific task. Paralinguistic properties however\nhave notoriously high class variance, making the finetuning ineffective. In\nthis work, we propose a two step approach to this. First we improve the\nembedding space, then we train an adapter to bridge the gap from the embedding\nspace to a classification task. In order to improve the class invariance we use\na combination of contrastive and non-contrastive losses to explicitly optimize\nfor class invariant, yet discriminative features. Our approach consistently\noutperforms baselines that are finetuned end-to-end on multiple tasks and\nsurpasses a benchmark on state-of-the-art emotion classification.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Lux_F/0/1/0/all/0/1\">Florian Lux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Ching-Yi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>"
}