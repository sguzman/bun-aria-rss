{
  "title": "A Research to Engineering Workflow",
  "link": "http://dustintran.com/blog/a-research-to-engineering-workflow",
  "guid": "http://dustintran.com/blog/a-research-to-engineering-workflow",
  "description": "<p>Going from a research idea to experiments is fundamental. But this\nstep is typically glossed over with little explicit advice. In\nacademia, the graduate student is often left toiling away—fragmented\ncode, various notes and LaTeX write-ups scattered around.\nNew projects often result in entirely new code bases, and if they do\nrely on past code, are difficult to properly extend to these new projects.</p>\n\n<p>Motivated by this, I thought it’d be useful to outline the steps I\npersonally take in going from research idea to experimentation, and\nhow that then improves my research understanding so I can revise the\nidea. This process is crucial: given an initial idea, all my time is\nspent on this process;\n<!--(with the majority of my time specifically spent on the experiments)-->\nand for me at least, the experiments are key to\nlearning about and solving problems that I couldn’t predict otherwise.<a href=\"#references\"><sup>1</sup></a>\n<!--More generally, I think without having a good                         -->\n<!--handle on this process, you can sometimes lose touch with             -->\n<!--reality and have a hard time                                          -->\n<!--[>figuring out what open problems and/or solutions are important.<]   -->\n<!--recalling why the problems you're working on are important.           --></p>\n\n<!--Much of what I'll describe is what other researchers, collaborators,-->\n<!--and friends I know already do. I'm hoping to make these steps       -->\n<!--transparent, we can review the workflow, compare it to alternatives,-->\n<!--and see how it might be optimized.                                  -->\n\n<!--## Coming up with an Idea-->\n<h2 id=\"finding-the-right-problem\">Finding the Right Problem</h2>\n<!--## A Master List of Research Ideas-->\n\n<!--+ reading papers                                                      -->\n<!--+ talking to people at conferences, workshops, etc. to see what find  -->\n<!--  important                                                           -->\n<!--+ personal experiences (in both research/understanding and in your own-->\n<!--  experiments)                                                        -->\n<!--+ frequent communication with people near you, if you have the benefit-->\n<!--  of like minded(or even not like minded) people as neighbors to      -->\n<!--  bounce ideas off of                                                 -->\n\n<!--This is the modt open ended and often in my opinion one of the most-->\n<!--challenging. It must be interesting to you, ideally ambitious with -->\n<!--clear and amazing end goals, important to many people in the       -->\n<!--community, and with short and long term visions.                   -->\n\n<!--Research is an organic process. Repositories file that research in-->\n<!--discrete units. Before making a repository, it's necessary to     -->\n<!--decide how initial ideas might jumpstart into more official and   -->\n<!--formalized.                                                       -->\n\n<!--This particular step varies widely. -->\n<p>Before working on a project, it’s necessary to decide how\nideas might jumpstart into something more official. Sometimes it’s as\nsimple as having a mentor suggest a project to work on; or tackling a\nspecific data set or applied problem; or having a conversation with a\nfrequent collaborator and then striking up a useful problem\nto work on together. More often, I find that research is\na result of a long chain of ideas which were continually\niterated upon—through frequent conversations, recent\nwork, longer term readings of subjects I’m unfamiliar with\n(e.g., <a href=\"#pearl2000causality\">Pearl (2000)</a>),\nand\nfavorite papers I like to revisit (e.g.,\n<a href=\"#wainwright2008graphical\">Wainwright &amp; Jordan (2008)</a>,\n<a href=\"#neal1994bayesian\">Neal (1994)</a>).</p>\n\n<p><img src=\"/blog/assets/2017-06-03-fig0.png\" alt=\"\" />\n<em><center>A master document of all my unexplored research ideas.</center></em></p>\n\n<p>One technique I’ve found immensely helpful is to maintain a single\nmaster document.<a href=\"#references\"><sup>2</sup></a>\nIt does a few things.</p>\n\n<p>First, it has a bulleted list of all ideas, problems, and topics that\nI’d like to think more carefully about (Section 1.3 in the figure).\nSometimes they’re as high-level as “Bayesian/generative approaches to\nreinforcement learning” or “addressing fairness in machine learning”;\nor they’re as specific as “Inference networks to handle memory\ncomplexity in EP” or “analysis of size-biased vs symmetric Dirichlet\npriors.”.  I try to keep the list succinct: subsequent sections go in\ndepth on a particular entry (Section 2+ in the figure).</p>\n\n<p>Second, the list of ideas is sorted according to what I’d like to work on\nnext. This guides me to understand the general direction of my\nresearch beyond present work. I can continually revise my\npriorities according to whether I think the direction aligns\nwith my broader research vision, and if I think the direction is\nnecessarily impactful for the community at large.\n<!-- -->\nImportantly, the list isn’t just about the next publishable idea to\nwork on, but generally what things I’d like to learn about next. This\ncontributes long-term in finding important problems and arriving at\nsimple or novel solutions.</p>\n\n<p>Every so often, I revisit the list, resorting things, adding things,\ndeleting things. Eventually I might elaborate upon an idea enough that\nit becomes a formal paper. In general, I’ve found that this process\nof iterating upon ideas within one location (and one format) makes\nthe transition to formal paper-writing and experiments to be a fluid experience.</p>\n\n<h2 id=\"managing-papers\">Managing Papers</h2>\n\n<p><img src=\"/blog/assets/2017-06-03-fig5.png\" alt=\"\" /></p>\n\n<p>Good research requires reading <em>a lot</em> of papers. Without a good way\nof organizing your readings, you can easily get overwhelmed by the\nfield’s hurried pace. (These past\nweeks have been especially notorious in trying to catch up on the slew\nof NIPS submissions posted to arXiv.)</p>\n\n<p>I’ve experimented with a lot of approaches to this, and ultimately\nI’ve arrived at the <a href=\"http://papersapp.com\">Papers app</a> which I highly\nrecommend.<sup>3</sup></p>\n\n<p>The most fundamental utility in a good management system is a\ncentralized repository which can be referenced back to. The advantage\nof having one location for this cannot be underestimated, whether it\nbe 8 page conference papers, journal papers, surveys, or even textbooks.\nMoreover, Papers is a nice tool for actually reading PDFs, and it\nconveniently syncs across devices as I read and star things on my\ntablet or laptop.  As I cite papers when I write, I can go back to\nPapers and get the corresponding BibTeX file and citekey.</p>\n\n<p>I personally enjoy taking painstaking effort in organizing papers. In\nthe screenshot above, I have a sprawling list of topics as paper tags.\nThese range from <code class=\"highlighter-rouge\">applications</code>, <code class=\"highlighter-rouge\">models</code>, <code class=\"highlighter-rouge\">inference</code> (each with\nsubtags), and there are also miscellaneous topics such as\n<code class=\"highlighter-rouge\">information-theory</code> and <code class=\"highlighter-rouge\">experimental-design</code>. An important\ncollection not seen in the screenshot is a tag called <code class=\"highlighter-rouge\">research</code>,\nwhich I bin all papers relevant to a particular research topic into.\nFor example, <a href=\"https://arxiv.org/abs/1706.00531\">the PixelGAN paper</a>\npresently highlighted is tagged into two topics I’ve currently been\nthinking a lot about—these are sorted into <code class=\"highlighter-rouge\">research→alignment-semi</code>\nand <code class=\"highlighter-rouge\">research→generative-images</code>.</p>\n\n<h2 id=\"managing-a-project\">Managing a Project</h2>\n\n<p><img src=\"/blog/assets/2017-06-03-fig1.png\" alt=\"\" />\n<em><center>The repository we used for a recent\n<a href=\"https://arxiv.org/abs/1610.09037\">arXiv preprint</a>.</center></em></p>\n\n<p>I like to maintain one research project in one Github repository.\n<!--Whatever one \"unit\" of research is varies. I define it as something   -->\n<!--relatively self-contained; for example, it might be tied to a specific-->\n<!--paper, an applied data analysis, or a particular topic at hand.       -->\n<!-- -->\nThey’re useful not only for tracking code but also\nin tracking general research progress, paper writing, and tying others\nin for collaboration. How Github repositories are organized is a frequent pain point.\nI like the following structure,\nbased originally from <a href=\"http://www.cs.columbia.edu/~blei/seminar/2016_discrete_data/notes/week_01.pdf\">Dave Blei’s preferred one</a>:</p>\n\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>-- doc/\n  -- 2017-nips/\n    -- preamble/\n    -- img/\n    -- main.pdf\n    -- main.tex\n    -- introduction.tex\n-- etc/\n  -- 2017-03-25-whiteboard.jpg\n  -- 2017-04-03-whiteboard.jpg\n  -- 2017-04-06-dustin-comments.md\n  -- 2017-04-08-dave-comments.pdf\n-- src/\n  -- checkpoints/\n  -- codebase/\n  -- log/\n  -- out/\n  -- script1.py\n  -- script2.py\n-- README.md\n</code></pre></div></div>\n\n<p><code class=\"highlighter-rouge\">README.md</code> maintains a list of todo’s, both for myself and\ncollaborators. This makes it transparent how to keep moving forward\nand what’s blocking the work.</p>\n\n<p><code class=\"highlighter-rouge\">doc/</code> contains all write-ups. Each subdirectory corresponds to a\nparticular conference or journal submission, with <code class=\"highlighter-rouge\">main.tex</code> being the\nprimary document and individual sections written in separate files\nsuch as <code class=\"highlighter-rouge\">introduction.tex</code>. Keeping one section per file makes\nit easy for multiple people to work on separate sections\nsimultaneously and avoid merge conflicts. Some people prefer to write\nthe full paper after major experiments are complete. I personally like to\nwrite a paper more as a summary of the current ideas and, as with the\nidea itself, it is continually revised as experiments proceed.</p>\n\n<p><code class=\"highlighter-rouge\">etc/</code> is a dump of everything not relevant to other directories. I\ntypically use it to store pictures of whiteboards during conversations\nabout the project. Or sometimes as I’m just going about my day-to-day,\nI’m struck with a bunch of ideas and so I dump them into a Markdown\ndocument. It’s also a convenient location to handle various\ncommentaries about the work, such as general feedback or paper\nmarkups from collaborators.</p>\n\n<p><code class=\"highlighter-rouge\">src/</code> is where all code is written. Runnable scripts are written\ndirectly in <code class=\"highlighter-rouge\">src/</code>, and classes and utilities are written in\n<code class=\"highlighter-rouge\">codebase/</code>. I’ll elaborate on these next. (The other three are\ndirectories outputted from scripts, which I’ll also elaborate upon.)</p>\n\n<h2 id=\"writing-code\">Writing Code</h2>\n\n<p><img src=\"/blog/assets/2017-06-03-fig2.png\" alt=\"\" />\n<!--_<center>A master document of all my unexplored research ideas.</center>_--></p>\n\n<p>Any code I write now uses <a href=\"http://edwardlib.org\">Edward</a>.\n<!--which uses                                                                    -->\n<!--[Python](https://www.python.org) and [TensorFlow](https://www.tensorflow.org).-->\nI find it to be the best framework for\nquickly experimenting with modern probabilistic models and algorithms.\n<!--[<sup>3</sup>](#references)-->\n<!--developing modern probabilistic models and inference algorithms.-->\n<!--, and with plug-and-play with built-in methods and pre-existing examples.-->\n<!-- -->\n<!--Previously I had to resort to working in fragmented code bases, where -->\n<!--one language had one idea, a pre-existing code base was hacked upon to-->\n<!--support certain other features, additional interface layers were      -->\n<!--written to get them all to communicate together... it was not good.   -->\n<!--Maintaining these dependencies and duplicate implemented ideas is a  -->\n<!--nightmare. And more importantly the code just constrains the sorts of-->\n<!--ideas/experiments you'd like to do.                                  -->\n<!--With Edward, everything is just there™. --></p>\n\n<p>On a conceptual level, Edward’s\nappealing because the language explicitly follows the math: the\nmodel’s generative process translates to specific lines of Edward\ncode; then the proposed algorithm translates to the next lines; etc. This\nclean translation\n<!--makes it easy to understand the mapping between math-->\n<!--and code. And it                                    -->\navoids future abstraction headaches when trying to extend the\ncode with natural research questions: for example, what if I used a different\nprior, or tweaked the gradient estimator, or tried a different\nneural net architecture, or applied the method on larger scale data sets?\n<!-- which makes it easy to translate engineering   -->\n<!--ideas about sharing various components to the math, and analogously   -->\n<!--how to easily take tweaked math ideas and replace the corresponding   -->\n<!--code.                                                                 --></p>\n\n<p>On a practical level, I most benefit from Edward by building off\npre-existing model examples\n(in <a href=\"https://github.com/blei-lab/edward/tree/master/examples\"><code class=\"highlighter-rouge\">edward/examples/</code></a> or <a href=\"https://github.com/blei-lab/edward/tree/master/notebooks\"><code class=\"highlighter-rouge\">edward/notebooks/</code></a>),\nand then adapting it to my problem.\nIf I am also implementing a new\nalgorithm, I take a pre-existing algorithm’s source\ncode (in <a href=\"https://github.com/blei-lab/edward/tree/master/edward/inferences\"><code class=\"highlighter-rouge\">edward/inferences/</code></a>),\npaste it as a new file in my research project’s <code class=\"highlighter-rouge\">codebase/</code> directory,\nand then I tweak it. This process makes it really easy to start\nafresh—beginning from templates and avoiding low-level details.</p>\n\n<p>When writing code, I always follow PEP8 (I particularly like the\n<a href=\"https://pypi.python.org/pypi/pep8\"><code class=\"highlighter-rouge\">pep8</code></a> package), and I try\nto separate individual scripts from the class and function definitions shared\nacross scripts; the latter is placed inside <code class=\"highlighter-rouge\">codebase/</code> and then imported.\nMaintaining code quality from the beginning is always a good\ninvestment, and I find this process scales well as the code gets\nincreasingly more complicated and worked on with others.</p>\n\n<p><strong>On Jupyter notebooks.</strong>\nMany people use <a href=\"http://jupyter.org\">Jupyter notebooks</a>\nas a method for interactive code development, and as an easy way to\nembed visualizations and LaTeX. I personally haven’t found\nsuccess in integrating it into my workflow. I like to just write all\nmy code down in a Python script and then run the script. But I can see why\nothers like the interactivity.</p>\n\n<h2 id=\"managing-experiments\">Managing Experiments</h2>\n\n<p><img src=\"/blog/assets/2017-06-03-fig3.png\" alt=\"\" /></p>\n\n<p>Investing in a good workstation or cloud service is a must.\nFeatures such as GPUs should basically\nbe a given with <a href=\"http://timdettmers.com/2017/04/09/which-gpu-for-deep-learning/\">their wide\navailability</a>,\nand one should have access to running many jobs in parallel.</p>\n\n<p>After I finish writing a script on my local computer, my typical workflow is:</p>\n\n<ol>\n  <li>Run <code class=\"highlighter-rouge\">rsync</code> to synchronize my local computer’s Github repository\n  (which includes uncommitted files) with a directory in the server;</li>\n  <li><code class=\"highlighter-rouge\">ssh</code> into the server.</li>\n  <li>Start <code class=\"highlighter-rouge\">tmux</code> and run the script. Among many things, <code class=\"highlighter-rouge\">tmux</code> lets you\ndetach the session so you don’t have to wait for the job to finish\nbefore interacting with the server again.</li>\n</ol>\n\n<p>When the script is sensible, I start diving into experiments with\nmultiple hyperparameter configurations.\nA useful tool for this is\n<a href=\"https://docs.python.org/3/library/argparse.html\"><code class=\"highlighter-rouge\">argparse</code></a>.\nIt augments a Python script with commandline arguments, where you\nadd something like the following to your script:</p>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">parser</span> <span class=\"o\">=</span> <span class=\"n\">argparse</span><span class=\"o\">.</span><span class=\"n\">ArgumentParser</span><span class=\"p\">()</span>\n<span class=\"n\">parser</span><span class=\"o\">.</span><span class=\"n\">add_argument</span><span class=\"p\">(</span><span class=\"s\">'--batch_size'</span><span class=\"p\">,</span> <span class=\"nb\">type</span><span class=\"o\">=</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">default</span><span class=\"o\">=</span><span class=\"mi\">128</span><span class=\"p\">,</span>\n                    <span class=\"n\">help</span><span class=\"o\">=</span><span class=\"s\">'Minibatch during training'</span><span class=\"p\">)</span>\n<span class=\"n\">parser</span><span class=\"o\">.</span><span class=\"n\">add_argument</span><span class=\"p\">(</span><span class=\"s\">'--lr'</span><span class=\"p\">,</span> <span class=\"nb\">type</span><span class=\"o\">=</span><span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"n\">default</span><span class=\"o\">=</span><span class=\"mf\">1e-5</span><span class=\"p\">,</span>\n                    <span class=\"n\">help</span><span class=\"o\">=</span><span class=\"s\">'Learning rate step-size'</span><span class=\"p\">)</span>\n<span class=\"n\">args</span> <span class=\"o\">=</span> <span class=\"n\">parser</span><span class=\"o\">.</span><span class=\"n\">parse_args</span><span class=\"p\">()</span>\n\n<span class=\"n\">batch_size</span> <span class=\"o\">=</span> <span class=\"n\">args</span><span class=\"o\">.</span><span class=\"n\">batch_size</span>\n<span class=\"n\">lr</span> <span class=\"o\">=</span> <span class=\"n\">args</span><span class=\"o\">.</span><span class=\"n\">lr</span>\n</code></pre></div></div>\n<p>Then you can run terminal commands such as</p>\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>python script1.py <span class=\"nt\">--batch_size</span><span class=\"o\">=</span>256 <span class=\"nt\">--lr</span><span class=\"o\">=</span>1e-4\n</code></pre></div></div>\n<p>This makes it easy to submit server jobs which vary these hyperparameters.</p>\n\n<p>Finally, let’s talk about managing the output of experiments.\nRecall the <code class=\"highlighter-rouge\">src/</code> directory structure above:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>-- src/\n  -- checkpoints/\n  -- codebase/\n  -- log/\n  -- out/\n  -- script1.py\n  -- script2.py\n</code></pre></div></div>\n<p>We described the individual scripts and <code class=\"highlighter-rouge\">codebase/</code>.\nThe other three directories are for organizing experiment output:</p>\n\n<ul>\n  <li><code class=\"highlighter-rouge\">checkpoints/</code> records saved model parameters during training.\nUse <code class=\"highlighter-rouge\">tf.train.Saver</code> to save parameters as the algorithm runs every\nfixed number of iterations. This helps with running long experiments, where you\nmight want to cut the experiment short and later restore the\nparameters. Each experiment outputs a subdirectory in <code class=\"highlighter-rouge\">checkpoints/</code>\nwith the convention,\n<code class=\"highlighter-rouge\">20170524_192314_batch_size_25_lr_1e-4/</code>. The first\nnumber is the date (<code class=\"highlighter-rouge\">YYYYMMDD</code>); the second is the timestamp (<code class=\"highlighter-rouge\">%H%M%S</code>);\nand the rest is hyperparameters.</li>\n  <li><code class=\"highlighter-rouge\">log/</code> records logs for visualizing learning.\nEach experiment belongs in a subdirectory with the same\nconvention as <code class=\"highlighter-rouge\">checkpoints/</code>.\nOne benefit of Edward is that for logging, you can simply pass an\nargument as <code class=\"highlighter-rouge\">inference.initialize(logdir='log/' + subdir)</code>.\nDefault TensorFlow summaries are tracked which can then be\nvisualized using TensorBoard (more on this next).</li>\n  <li><code class=\"highlighter-rouge\">out/</code> records exploratory output after training finishes; for example,\ngenerated images or matplotlib plots.\nEach experiment belongs in a subdirectory with the same convention\nas <code class=\"highlighter-rouge\">checkpoints/</code>.</li>\n</ul>\n\n<p><strong>On data sets.</strong>\nData sets are used across many research projects. I prefer storing them\nin the home directory <code class=\"highlighter-rouge\">~/data</code>.</p>\n\n<p><strong>On software containers.</strong>\n<a href=\"http://python-guide-pt-br.readthedocs.io/en/latest/dev/virtualenvs/\">virtualenv</a>\nis a must for managing Python dependencies and avoiding difficulties\nwith system-wide Python installs. It’s particularly nice if you like\nto write Python 2/3-agnostic code.\n<a href=\"https://www.docker.com\">Docker containers</a> are an even more powerful\ntool if you require more from your setup.</p>\n\n<h2 id=\"exploration-debugging--diagnostics\">Exploration, Debugging, &amp; Diagnostics</h2>\n\n<p><img src=\"/blog/assets/2017-06-03-fig4.png\" alt=\"\" />\n<!--_<center>Picture thanks to                                                   -->\n<!--<a href=\"https://github.com/blei-lab/edward/pull/653#issuecomment-304728311\">-->\n<!--Sean Kruzel</a>.</center>_                                                   --></p>\n\n<p><a href=\"https://www.tensorflow.org/get_started/summaries_and_tensorboard\">Tensorboard</a>\nis an excellent tool for visualizing and exploring your model\ntraining. With TensorBoard’s interactivity, I find it\nparticularly convenient in that I don’t have to configure a bunch of\nmatplotlib functions to understand training. One only needs to percolate a\nbunch of <code class=\"highlighter-rouge\">tf.summary</code>s on tensors in the code.</p>\n\n<p>Edward logs a bunch of summaries by default in order to visualize how\nloss function values, gradients, and parameter change across\ntraining iteration.\nTensorBoard also includes wall time comparisons, and\na sufficiently decorated TensorFlow code base provides a nice\ncomputational graph you can stare at.\nFor nuanced issues I can’t diagnose with TensorBoard specifically, I\njust output things in the <code class=\"highlighter-rouge\">out/</code> directory and inspect those results.</p>\n\n<p><strong>Debugging error messages.</strong>\nMy debugging workflow is terrible. I\npercolate print statements across my code and\nfind errors by\nprocess of\nelimination. This is primitive. Although I haven’t tried it, I\nhear good things about\n<a href=\"https://www.tensorflow.org/programmers_guide/debugger\">TensorFlow’s debugger</a>.</p>\n\n<h2 id=\"improving-research-understanding\">Improving Research Understanding</h2>\n\n<p>Interrogating your model, algorithm, and generally the learning\nprocess lets you better understand your work’s success and failure\nmodes. This lets you go back to the drawing board, thinking deeply\nabout the method and how it might be further improved.\nAs the method indicates success, one can go\nfrom tackling simple toy configurations to increasingly large\nscale and high-dimensional problems.</p>\n\n<p>From a higher level, this workflow is really about implementing the\nscientific method in the real world.  No major ideas are necessarily\ndiscarded at each iteration of the experimental process, but rather,\nas in the ideal of science, you start with fundamentals and\niteratively expand upon them as you have a stronger grasp of reality.</p>\n\n<p>Experiments aren’t alone in this process either. Collaboration,\ncommunicating with experts from other fields, reading papers, working\non both short and longer term ideas, and attending talks and\nconferences help broaden your perspective in finding the right\nproblems and solving them.</p>\n\n<h2 id=\"footnotes--references\">Footnotes &amp; References</h2>\n\n<p><sup>1</sup> This workflow is specifically for empirical research.\nTheory is a whole other can of worms, but some of these ideas\nstill generalize.</p>\n\n<p><sup>2</sup>\nThe template for the master document is available\n<a href=\"https://github.com/dustinvtran/latex-templates\"><code class=\"highlighter-rouge\">here</code></a>.</p>\n\n<p><sup>3</sup>\nThere’s one caveat to Papers. I use it for everything: there are at\nleast 2,000 papers stored in my account, and with quite a few dense\ntextbooks.  The application sifts through at least half a dozen\ngigabytes, and so it suffers from a few hiccups when\nreading/referencing back across many papers. I’m not sure if this is a\nbug or just inherent to me exploiting Papers almost <em>too</em> much.</p>\n\n<!--<sup>3</sup>                                                                           -->\n<!--Disclaimer: I wrote most of Edward.                                                    -->\n<!--I personally benefit from the fact that if                                             -->\n<!--something is missing in Edward I can easily add it.                                    -->\n<!--[But of course you can (and should) add things too.](http://edwardlib.org/contributing)-->\n\n<ol class=\"bibliography\"><li><span id=\"neal1994bayesian\">Neal, R. M. (1994). <i>Bayesian Learning for Neural Networks</i> (PhD thesis). University of Toronto.</span></li>\n<li><span id=\"pearl2000causality\">Pearl, J. (2000). <i>Causality</i>. Cambridge University Press.</span></li>\n<li><span id=\"wainwright2008graphical\">Wainwright, M. J., &amp; Jordan, M. I. (2008). Graphical Models, Exponential Families, and Variational Inference. <i>Foundations and Trends in Machine Learning</i>, <i>1</i>(1–2), 1–305.</span></li></ol>\n\n<!--__Failed ideas.__                                                     -->\n<!--They go back into master document. Or if there's a large collection of-->\n<!--perpheral stuff, they remain as Github repos, but I personally store  -->\n<!--them in an `archives/` folder. They're put on hold, and I might-->\n<!--revisit them over the years as I spark up new ideas.           -->\n\n<!--__Deployment to Larger Scales.__                                 -->\n<!--your mileage will vary, given how you specifically deploy things.-->\n<!--different clusters or machines. analysis of how to start placing -->\n<!--device configurations in the code.                               -->\n\n<!--+ device configurations                                          -->\n<!--+ pretrained models                                              -->\n<!--+ xla                                                            -->\n<!--+ file readers                                                   -->\n<!--+ distributed tensorflow stuff and data management systems       -->\n\n<!--+ redditors and new ml researchers would love it                      -->\n<!--+ engineers and non ml experts could read it to understand where we   -->\n<!--  come from and how we work, so maybe                                 -->\n\n<!--other things i might mention                                          -->\n<!--+ note how write-up and formalism of idea can come before or after    -->\n<!--  first iteration of the workflow, depending on whether or kot a forst-->\n<!--  iteration is needed to pass a dummy test of if the idea makes sense -->\n<!--+ the different ways that you might do research                       -->\n<!--+ my personal day to day on how much time i spend reading (relevant   -->\n<!--  papers to current research, other papers for breadth of knowledge,  -->\n<!--  long term understanding of new subjects), research of               -->\n<!--  thinking/writing, coding, meetingd. and also personal management of -->\n<!--  ongoing work, such as maitaining and developing edward, and how many-->\n<!--  independent projects to tackle at once                              -->\n<!--+ code expansion, management of quality, as it grows bigger and bigger-->",
  "pubDate": "Sat, 03 Jun 2017 00:00:00 -0700"
}