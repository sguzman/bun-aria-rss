{
  "guid": "3325eb3c-174a-452c-b5e3-750d7657633d",
  "title": "Is A.I. the Problem? Or Are We?",
  "description": "<p>If you talk to many of the people working on the cutting edge of artificial intelligence research, you’ll hear that we are on the cusp of a technology that will be far more transformative than simply computers and the internet, one that could bring about a new industrial revolution and usher in a utopia — or perhaps pose the greatest threat in our species’s history.</p><p>Others, of course, will tell you those folks are nuts.</p><p>One of my projects this year is to get a better handle on this debate. A.I., after all, isn’t some force only future human beings will face. It’s here now, deciding what advertisements are served to us online, how bail is set after we commit crimes and whether our jobs will exist in a couple of years. It is both shaped by and reshaping politics, economics and society. It’s worth understanding.</p><p>Brian Christian’s recent book <a href=\"https://wwnorton.com/books/9780393635829\">“The Alignment Problem”</a> is the best book on the key technical and moral questions of A.I. that I’ve read. At its center is the term from which the book gets its name. “Alignment problem” originated in economics as a way to describe the fact that the systems and incentives we create often fail to align with our goals. And that’s a central worry with A.I., too: that we will create something to help us that will instead harm us, in part because we didn’t understand how it really worked or what we had actually asked it to do.</p><p>So this conversation is about the various alignment problems associated with A.I. We discuss what machine learning is and how it works, how governments and corporations are using it right now, what it has taught us about human learning, the ethics of how humans should treat sentient robots, the all-important question of how A.I. developers plan to make profits, what kinds of regulatory structures are possible when we’re dealing with algorithms we don’t really understand, the way A.I. reflects and then supercharges the inequities that exist in our society, the saddest Super Mario Bros. game I’ve ever heard of, why the problem of automation isn’t so much job loss as dignity loss and much more.</p><p>Mentioned: </p><p><a href=\"https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf\">“Human-level control through deep reinforcement learning”</a></p><p><a href=\"https://nissenbaum.tech.cornell.edu/papers/Wiener.pdf\">“Some Moral and Technical Consequences of Automation”</a> by Norbert Wiener</p><p>Recommendations: </p><p><a href=\"https://www.amazon.com/What-Expect-Youre-Expecting-Robots/dp/1541699114\">\"What to Expect When You're Expecting Robots\"</a>  by Julie Shah and Laura Major</p><p><a href=\"https://www.simonandschuster.com/books/Finite-and-Infinite-Games/James-Carse/9781476731711\">\"Finite and Infinite Games\"</a> by James P. Carse </p><p><a href=\"https://www.penguinrandomhouse.com/books/600671/how-to-do-nothing-by-jenny-odell/\">\"How to Do Nothing\"</a> by Jenny Odell</p><p>If you enjoyed this episode, check out <a href=\"https://podcasts.apple.com/us/podcast/a-conversation-about-human-minds-for-human-minds/id1548604447?i=1000517373857\">my conversation with Alison Gopnik on what we can all learn from studying the minds of children.</a></p><p>You can find transcripts (posted midday) and more episodes of \"The Ezra Klein Show\" at nytimes.com/ezra-klein-podcast, and you can find Ezra on Twitter @ezraklein.</p><p>Thoughts? Guest suggestions? Email us at ezrakleinshow@nytimes.com.</p><p>“The Ezra Klein Show” is produced by Annie Galvin, Jeff Geld and Rogé Karma; fact-checking by Michelle Harris; original music by Isaac Jones; mixing by Jeff Geld; audience strategy by Shannon Busta. Special thanks to Kristin Lin.</p>\n",
  "pubDate": "Fri, 4 Jun 2021 09:00:00 +0000",
  "author": "ezrakleinshow@nytimes.com (New York Times Opinion)",
  "link": "https://www.nytimes.com/2021/06/04/opinion/ezra-klein-podcast-brian-christian.html",
  "content:encoded": "<p>If you talk to many of the people working on the cutting edge of artificial intelligence research, you’ll hear that we are on the cusp of a technology that will be far more transformative than simply computers and the internet, one that could bring about a new industrial revolution and usher in a utopia — or perhaps pose the greatest threat in our species’s history.</p><p>Others, of course, will tell you those folks are nuts.</p><p>One of my projects this year is to get a better handle on this debate. A.I., after all, isn’t some force only future human beings will face. It’s here now, deciding what advertisements are served to us online, how bail is set after we commit crimes and whether our jobs will exist in a couple of years. It is both shaped by and reshaping politics, economics and society. It’s worth understanding.</p><p>Brian Christian’s recent book <a href=\"https://wwnorton.com/books/9780393635829\">“The Alignment Problem”</a> is the best book on the key technical and moral questions of A.I. that I’ve read. At its center is the term from which the book gets its name. “Alignment problem” originated in economics as a way to describe the fact that the systems and incentives we create often fail to align with our goals. And that’s a central worry with A.I., too: that we will create something to help us that will instead harm us, in part because we didn’t understand how it really worked or what we had actually asked it to do.</p><p>So this conversation is about the various alignment problems associated with A.I. We discuss what machine learning is and how it works, how governments and corporations are using it right now, what it has taught us about human learning, the ethics of how humans should treat sentient robots, the all-important question of how A.I. developers plan to make profits, what kinds of regulatory structures are possible when we’re dealing with algorithms we don’t really understand, the way A.I. reflects and then supercharges the inequities that exist in our society, the saddest Super Mario Bros. game I’ve ever heard of, why the problem of automation isn’t so much job loss as dignity loss and much more.</p><p>Mentioned: </p><p><a href=\"https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf\">“Human-level control through deep reinforcement learning”</a></p><p><a href=\"https://nissenbaum.tech.cornell.edu/papers/Wiener.pdf\">“Some Moral and Technical Consequences of Automation”</a> by Norbert Wiener</p><p>Recommendations: </p><p><a href=\"https://www.amazon.com/What-Expect-Youre-Expecting-Robots/dp/1541699114\">\"What to Expect When You're Expecting Robots\"</a>  by Julie Shah and Laura Major</p><p><a href=\"https://www.simonandschuster.com/books/Finite-and-Infinite-Games/James-Carse/9781476731711\">\"Finite and Infinite Games\"</a> by James P. Carse </p><p><a href=\"https://www.penguinrandomhouse.com/books/600671/how-to-do-nothing-by-jenny-odell/\">\"How to Do Nothing\"</a> by Jenny Odell</p><p>If you enjoyed this episode, check out <a href=\"https://podcasts.apple.com/us/podcast/a-conversation-about-human-minds-for-human-minds/id1548604447?i=1000517373857\">my conversation with Alison Gopnik on what we can all learn from studying the minds of children.</a></p><p>You can find transcripts (posted midday) and more episodes of \"The Ezra Klein Show\" at nytimes.com/ezra-klein-podcast, and you can find Ezra on Twitter @ezraklein.</p><p>Thoughts? Guest suggestions? Email us at ezrakleinshow@nytimes.com.</p><p>“The Ezra Klein Show” is produced by Annie Galvin, Jeff Geld and Rogé Karma; fact-checking by Michelle Harris; original music by Isaac Jones; mixing by Jeff Geld; audience strategy by Shannon Busta. Special thanks to Kristin Lin.</p>\n",
  "enclosure": "",
  "itunes:title": "Is A.I. the Problem? Or Are We?",
  "itunes:author": "New York Times Opinion",
  "itunes:duration": "01:16:39",
  "itunes:summary": "If you talk to many of the people working on the cutting edge of artificial intelligence research, you’ll hear that we are on the cusp of a technology that will be far more transformative than simply computers and the internet, one that could bring about a new industrial revolution and usher in a utopia — or perhaps pose the greatest threat in our species’s history.\n\nOthers, of course, will tell you those folks are nuts.\n\nOne of my projects this year is to get a better handle on this debate. A.I., after all, isn’t some force only future human beings will face. It’s here now, deciding what advertisements are served to us online, how bail is set after we commit crimes and whether our jobs will exist in a couple of years. It is both shaped by and reshaping politics, economics and society. It’s worth understanding.\n\nBrian Christian’s recent book “The Alignment Problem” is the best book on the key technical and moral questions of A.I. that I’ve read. At its center is the term from which the book gets its name. “Alignment problem” originated in economics as a way to describe the fact that the systems and incentives we create often fail to align with our goals. And that’s a central worry with A.I., too: that we will create something to help us that will instead harm us, in part because we didn’t understand how it really worked or what we had actually asked it to do.\n\nSo this conversation is about the various alignment problems associated with A.I. We discuss what machine learning is and how it works, how governments and corporations are using it right now, what it has taught us about human learning, the ethics of how humans should treat sentient robots, the all-important question of how A.I. developers plan to make profits, what kinds of regulatory structures are possible when we’re dealing with algorithms we don’t really understand, the way A.I. reflects and then supercharges the inequities that exist in our society, the saddest Super Mario Bros. game I’ve ever heard of, why the problem of automation isn’t so much job loss as dignity loss and much more.\n\nMentioned: \n\n“Human-level control through deep reinforcement learning” \n\n“Some Moral and Technical Consequences of Automation” by Norbert Wiener\n\n\nRecommendations: \n\nWhat to Expect When You're Expecting Robots  by Julie Shah and Laura Major\n\nFinite and Infinite Games by James P. Carse \n\nHow to Do Nothing by Jenny Odell\n\n\nIf you enjoyed this episode, check out my conversation with Alison Gopnik on what we can all learn from studying the minds of children. \n\n\nYou can find transcripts (posted midday) and more episodes of \"The Ezra Klein Show\" at nytimes.com/ezra-klein-podcast, and you can find Ezra on Twitter @ezraklein.\n\nThoughts? Guest suggestions? Email us at ezrakleinshow@nytimes.com.\n\n“The Ezra Klein Show” is produced by Annie Galvin, Jeff Geld and Rogé Karma; fact-checking by Michelle Harris; original music by Isaac Jones; mixing by Jeff Geld; audience strategy by Shannon Busta. Special thanks to Kristin Lin.",
  "itunes:subtitle": "If you talk to many of the people working on the cutting edge of artificial intelligence research, you’ll hear that we are on the cusp of a technology that will be far more transformative than simply computers and the internet, one that could bring about a new industrial revolution and usher in a utopia — or perhaps pose the greatest threat in our species’s history.\n\nOthers, of course, will tell you those folks are nuts.\n\nOne of my projects this year is to get a better handle on this debate. A.I., after all, isn’t some force only future human beings will face. It’s here now, deciding what advertisements are served to us online, how bail is set after we commit crimes and whether our jobs will exist in a couple of years. It is both shaped by and reshaping politics, economics and society. It’s worth understanding.\n\nBrian Christian’s recent book “The Alignment Problem” is the best book on the key technical and moral questions of A.I. that I’ve read. At its center is the term from which the book gets its name. “Alignment problem” originated in economics as a way to describe the fact that the systems and incentives we create often fail to align with our goals. And that’s a central worry with A.I., too: that we will create something to help us that will instead harm us, in part because we didn’t understand how it really worked or what we had actually asked it to do.\n\nSo this conversation is about the various alignment problems associated with A.I. We discuss what machine learning is and how it works, how governments and corporations are using it right now, what it has taught us about human learning, the ethics of how humans should treat sentient robots, the all-important question of how A.I. developers plan to make profits, what kinds of regulatory structures are possible when we’re dealing with algorithms we don’t really understand, the way A.I. reflects and then supercharges the inequities that exist in our society, the saddest Super Mario Bros. game I’ve ever heard of, why the problem of automation isn’t so much job loss as dignity loss and much more.\n\nMentioned: \n\n“Human-level control through deep reinforcement learning” \n\n“Some Moral and Technical Consequences of Automation” by Norbert Wiener\n\n\nRecommendations: \n\nWhat to Expect When You're Expecting Robots  by Julie Shah and Laura Major\n\nFinite and Infinite Games by James P. Carse \n\nHow to Do Nothing by Jenny Odell\n\n\nIf you enjoyed this episode, check out my conversation with Alison Gopnik on what we can all learn from studying the minds of children. \n\n\nYou can find transcripts (posted midday) and more episodes of \"The Ezra Klein Show\" at nytimes.com/ezra-klein-podcast, and you can find Ezra on Twitter @ezraklein.\n\nThoughts? Guest suggestions? Email us at ezrakleinshow@nytimes.com.\n\n“The Ezra Klein Show” is produced by Annie Galvin, Jeff Geld and Rogé Karma; fact-checking by Michelle Harris; original music by Isaac Jones; mixing by Jeff Geld; audience strategy by Shannon Busta. Special thanks to Kristin Lin.",
  "itunes:explicit": "no",
  "itunes:episodeType": "full"
}