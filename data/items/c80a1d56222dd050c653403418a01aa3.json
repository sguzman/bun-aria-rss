{
  "title": "Exploring Target Representations for Masked Autoencoders. (arXiv:2209.03917v2 [cs.CV] UPDATED)",
  "link": "http://arxiv.org/abs/2209.03917",
  "description": "<p>Masked autoencoders have become popular training paradigms for\nself-supervised visual representation learning. These models randomly mask a\nportion of the input and reconstruct the masked portion according to the target\nrepresentations. In this paper, we first show that a careful choice of the\ntarget representation is unnecessary for learning good representations, since\ndifferent targets tend to derive similarly behaved models. Driven by this\nobservation, we propose a multi-stage masked distillation pipeline and use a\nrandomly initialized model as the teacher, enabling us to effectively train\nhigh-capacity models without any efforts to carefully design target\nrepresentations. Interestingly, we further explore using teachers of larger\ncapacity, obtaining distilled students with remarkable transferring ability. On\ndifferent tasks of classification, transfer learning, object detection, and\nsemantic segmentation, the proposed method to perform masked knowledge\ndistillation with bootstrapped teachers (dBOT) outperforms previous\nself-supervised methods by nontrivial margins. We hope our findings, as well as\nthe proposed method, could motivate people to rethink the roles of target\nrepresentations in pre-training masked autoencoders.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xingbin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jinghao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_T/0/1/0/all/0/1\">Tao Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xianming Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"
}