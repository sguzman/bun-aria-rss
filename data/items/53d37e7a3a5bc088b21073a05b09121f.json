{
  "title": "Tiny-Attention Adapter: Contexts Are More Important Than the Number of Parameters. (arXiv:2211.01979v1 [cs.CL])",
  "link": "http://arxiv.org/abs/2211.01979",
  "description": "<p>Adapter-tuning is a paradigm that transfers a pretrained language model to\ndownstream tasks by adding and tuning a small number of new parameters.\nPreviously proposed adapter architectures are all feed-forward neural networks.\nIn this paper, we investigate the effectiveness of using tiny-attention --\ni.e., attention with extremely small per-head dimensionality -- as adapters.\nOur tiny-attention adapter learns to modify the hidden states at each position\ndirectly conditioned on the hidden states at all the other positions, which is\nmissed by the previously proposed adapters. Moreover, we view its multiple\nattention heads as a mixture of experts and propose to average their weights\nduring deployment, which further reduces its inference computation cost. On the\nGLUE benchmark, our tiny-attention adapter outperforms the other\nparameter-efficient transfer learning methods as well as full fine-tuning while\nonly updating 0.05% of the parameters. On the FewGLUE benchmark, its\nperformance is comparable to that of GPT-3 and PET.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hongyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1\">Hongyuan Mei</a>"
}