{
  "title": "Understanding NUTS and HMC",
  "link": "",
  "id": "https://www.georgeho.org/understanding-nuts-hmc/",
  "updated": "2021-01-07T00:00:00Z",
  "published": "2021-01-07T00:00:00Z",
  "content": "<p><em>&ldquo;Bayesian modeling is harder than deep learning&rdquo;</em> is a sentiment I&rsquo;ve been\nhearing a lot lately. While I&rsquo;m skeptical of sweeping statements like that, I\nagree when it comes to the central inference algorithm &mdash; how MCMC samplers\nwork (especially the <em>de facto</em> standard samplers, NUTS and HMC) is one of the\nmost difficult concepts I&rsquo;ve tried to learn, and is certainly harder than\nautodifferentiation or backpropagation.</p>\n<p>So I thought I&rsquo;d share what worked for me when I tried to teach myself NUTS and\nHMC. In chronological order of publication, these are the three resources that\nI’d recommend reading to grok NUTS/HMC:</p>\n<ol>\n<li><a href=\"http://www.mcmchandbook.net/HandbookChapter5.pdf\">Radford Neal&rsquo;s chapter in the MCMC\nhandbook</a></li>\n<li><a href=\"https://arxiv.org/abs/1111.4246\">Matthew Hoffman’s <em>The No-U-Turn Sampler</em> (a.k.a. the original NUTS\npaper)</a></li>\n<li><a href=\"https://arxiv.org/abs/1701.02434\">Michael Betancourt’s <em>Conceptual Introduction to Hamiltonian Monte\nCarlo</em></a></li>\n</ol>\n<p>Not only did I find it useful to read these papers several times (as one would\nread any sequence of &ldquo;important&rdquo; papers), but also to read them in both\nchronological and reverse-chronological order. Reading both forwards and\nbackwards gave me multiple expositions of important ideas and also let me\nmentally &ldquo;diff&rdquo; the papers to see the progression of ideas over time. For\nexample, Neal&rsquo;s chapter was written before NUTS was discovered, which gives you\na sense of what the MCMC world looked like prior to Hoffman&rsquo;s work: making\nprogress in fits and starts, but in need of a real leap forward.</p>\n<p>In terms of reading code, I&rsquo;d recommend looking through <a href=\"https://github.com/ColCarroll/minimc\">Colin Carroll’s\n<code>minimc</code></a> for a minimal working example\nof NUTS in Python, written for pedagogy rather than actual sampling. For a\n&ldquo;real world&rdquo; implementation of NUTS/HMC, I’d recommend looking through <a href=\"https://github.com/eigenfoo/littlemcmc\">my\n<code>littlemcmc</code></a> for a standalone version\nof PyMC3’s NUTS/HMC samplers.</p>\n<p>Finally, for anyone who wants to read around computational methods for Bayesian\ninference more generally (i.e. not restricted to HMC, for example), I&rsquo;d\n(unashamedly) point to <a href=\"https://www.georgeho.org/bayesian-inference-reading/\">my blog post on\nthis</a>.</p>"
}