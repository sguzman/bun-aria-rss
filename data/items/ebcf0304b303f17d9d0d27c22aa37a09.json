{
  "title": "Custom Parallel Workflows",
  "link": "",
  "updated": "2015-07-23T00:00:00+00:00",
  "id": "https://mrocklin.github.io/blog/work/2015/07/23/Imperative",
  "content": "<p><em>This work is supported by <a href=\"http://continuum.io\">Continuum Analytics</a>\nand the <a href=\"http://www.darpa.mil/program/XDATA\">XDATA Program</a>\nas part of the <a href=\"http://blaze.pydata.org\">Blaze Project</a></em></p>\n\n<p><strong>tl;dr: We motivate the expansion of parallel programming beyond big\ncollections.  We discuss the usability custom of dask graphs.</strong></p>\n\n<h2 id=\"recent-parallel-work-focuses-on-big-collections\">Recent Parallel Work Focuses on Big Collections</h2>\n\n<p>Parallel databases, Spark, and Dask collections all provide large distributed\ncollections that handle parallel computation for you.  You put data into the\ncollection, program with a small set of operations like <code class=\"language-plaintext highlighter-rouge\">map</code> or <code class=\"language-plaintext highlighter-rouge\">groupby</code>, and\nthe collections handle the parallel processing.  This idea has become so\npopular that there are now a dozen projects promising big and friendly Pandas\nclones.</p>\n\n<p>This is good.  These collections provide usable, high-level interfaces for a\nlarge class of common problems.</p>\n\n<h2 id=\"custom-workloads\">Custom Workloads</h2>\n\n<p>However, many workloads are too complex for these collections.  Workloads might\nbe complex either because they come from sophisticated algorithms\n(as we saw in a <a href=\"https://mrocklin.github.io/blog/work/2015/06/26/Complex-Graphs/\">recent post on SVD</a>) or because they come from the real world,\nwhere problems tend to be messy.</p>\n\n<p>In these cases I tend to see people do two things</p>\n\n<ol>\n  <li>Fall back to <code class=\"language-plaintext highlighter-rouge\">multiprocessing</code>, <code class=\"language-plaintext highlighter-rouge\">MPI</code> or some other explicit form of parallelism</li>\n  <li>Perform mental gymnastics to fit their problem into Spark using a\nclever choice of keys.  These cases often fail to acheive much speedup.</li>\n</ol>\n\n<h2 id=\"direct-dask-graphs\">Direct Dask Graphs</h2>\n\n<p>Historically I’ve recommended the manual construction of dask graphs in these\ncases.  Manual construction of dask graphs lets you specify fairly arbitrary\nworkloads that then use the dask schedulers to execute in parallel.\nThe <a href=\"dask.pydata.org/en/latest/custom-graphs.html\">dask docs</a> hold the\nfollowing example of a simple data processing pipeline:</p>\n\n<p><img src=\"https://mrocklin.github.io/blog/images/pipeline.png\" align=\"right\" width=\"15%\" /></p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"k\">def</span> <span class=\"nf\">load</span><span class=\"p\">(</span><span class=\"n\">filename</span><span class=\"p\">):</span>\n    <span class=\"p\">...</span>\n<span class=\"k\">def</span> <span class=\"nf\">clean</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">):</span>\n    <span class=\"p\">...</span>\n<span class=\"k\">def</span> <span class=\"nf\">analyze</span><span class=\"p\">(</span><span class=\"n\">sequence_of_data</span><span class=\"p\">):</span>\n    <span class=\"p\">...</span>\n<span class=\"k\">def</span> <span class=\"nf\">store</span><span class=\"p\">(</span><span class=\"n\">result</span><span class=\"p\">):</span>\n    <span class=\"p\">...</span>\n\n<span class=\"n\">dsk</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s\">'load-1'</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"n\">load</span><span class=\"p\">,</span> <span class=\"s\">'myfile.a.data'</span><span class=\"p\">),</span>\n       <span class=\"s\">'load-2'</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"n\">load</span><span class=\"p\">,</span> <span class=\"s\">'myfile.b.data'</span><span class=\"p\">),</span>\n       <span class=\"s\">'load-3'</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"n\">load</span><span class=\"p\">,</span> <span class=\"s\">'myfile.c.data'</span><span class=\"p\">),</span>\n       <span class=\"s\">'clean-1'</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"n\">clean</span><span class=\"p\">,</span> <span class=\"s\">'load-1'</span><span class=\"p\">),</span>\n       <span class=\"s\">'clean-2'</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"n\">clean</span><span class=\"p\">,</span> <span class=\"s\">'load-2'</span><span class=\"p\">),</span>\n       <span class=\"s\">'clean-3'</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"n\">clean</span><span class=\"p\">,</span> <span class=\"s\">'load-3'</span><span class=\"p\">),</span>\n       <span class=\"s\">'analyze'</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"n\">analyze</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"s\">'clean-%d'</span> <span class=\"o\">%</span> <span class=\"n\">i</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">]]),</span>\n       <span class=\"s\">'store'</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"n\">store</span><span class=\"p\">,</span> <span class=\"s\">'analyze'</span><span class=\"p\">)}</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">dask.multiprocessing</span> <span class=\"kn\">import</span> <span class=\"n\">get</span>\n<span class=\"n\">get</span><span class=\"p\">(</span><span class=\"n\">dsk</span><span class=\"p\">,</span> <span class=\"s\">'store'</span><span class=\"p\">)</span>  <span class=\"c1\"># executes in parallel</span></code></pre>\n</figure>\n\n<p>Feedback from users is that this is interesting and powerful but that\nprogramming directly in dictionaries is not inutitive, doesn’t integrate well\nwith IDEs, and is prone to error.</p>\n\n<h2 id=\"introducing-daskdo\">Introducing dask.do</h2>\n\n<p>To create the same custom parallel workloads using normal-ish Python code we\nuse the <a href=\"http://dask.pydata.org/en/latest/imperative.html\">dask.do</a> function.\nThis <code class=\"language-plaintext highlighter-rouge\">do</code> function turns any normal Python function into a delayed version that\nadds to a dask graph.  The <code class=\"language-plaintext highlighter-rouge\">do</code> function lets us rewrite the computation above\nas follows:</p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"kn\">from</span> <span class=\"nn\">dask</span> <span class=\"kn\">import</span> <span class=\"n\">do</span>\n\n<span class=\"n\">loads</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">do</span><span class=\"p\">(</span><span class=\"n\">load</span><span class=\"p\">)(</span><span class=\"s\">'myfile.a.data'</span><span class=\"p\">),</span>\n         <span class=\"n\">do</span><span class=\"p\">(</span><span class=\"n\">load</span><span class=\"p\">)(</span><span class=\"s\">'myfile.b.data'</span><span class=\"p\">),</span>\n         <span class=\"n\">do</span><span class=\"p\">(</span><span class=\"n\">load</span><span class=\"p\">)(</span><span class=\"s\">'myfile.c.data'</span><span class=\"p\">)]</span>\n\n<span class=\"n\">cleaned</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">do</span><span class=\"p\">(</span><span class=\"n\">clean</span><span class=\"p\">)(</span><span class=\"n\">x</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">x</span> <span class=\"ow\">in</span> <span class=\"n\">loads</span><span class=\"p\">]</span>\n\n<span class=\"n\">analysis</span> <span class=\"o\">=</span> <span class=\"n\">do</span><span class=\"p\">(</span><span class=\"n\">analyze</span><span class=\"p\">)(</span><span class=\"n\">cleaned</span><span class=\"p\">)</span>\n<span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">do</span><span class=\"p\">(</span><span class=\"n\">store</span><span class=\"p\">)(</span><span class=\"n\">analysis</span><span class=\"p\">)</span></code></pre>\n</figure>\n\n<p>The explicit function calls here don’t perform work directly; instead they\nbuild up a dask graph which we can then execute in parallel with our choice of\nscheduler.</p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"kn\">from</span> <span class=\"nn\">dask.multiprocessing</span> <span class=\"kn\">import</span> <span class=\"n\">get</span>\n<span class=\"n\">result</span><span class=\"p\">.</span><span class=\"n\">compute</span><span class=\"p\">(</span><span class=\"n\">get</span><span class=\"o\">=</span><span class=\"n\">get</span><span class=\"p\">)</span></code></pre>\n</figure>\n\n<p>This interface was suggested by <a href=\"http://gael-varoquaux.info/\">Gael Varoquaux</a>\nbased on his experience with <a href=\"https://pythonhosted.org/joblib/\">joblib</a>.  It\nwas implemented by <a href=\"http://jcrist.github.io/\">Jim Crist</a>\nin <a href=\"https://github.com/ContinuumIO/dask/pull/408\">PR (#408)</a>.</p>\n\n<h2 id=\"example-nested-cross-validation\">Example: Nested Cross Validation</h2>\n\n<p>I sat down with a Machine learning student, <a href=\"http://people.inf.ethz.ch/kgabriel/\">Gabriel\nKrummenacher</a> and worked to parallelize a\nsmall code to do nested cross validation.  Below is a comparison of a\nsequential implementation that has been parallelized using <code class=\"language-plaintext highlighter-rouge\">dask.do</code>:</p>\n\n<p>You can safely skip reading this code in depth.  The take-away is that it’s\nsomewhat involved but that the addition of parallelism is light.</p>\n\n<p><a href=\"https://github.com/mrocklin/dask-crossval\">\n<img src=\"https://mrocklin.github.io/blog/images/do.gif\" alt=\"parallized cross validation code\" width=\"80%\" />\n</a></p>\n\n<p>The parallel version runs about four times faster on my notebook.\nDisclaimer: The sequential version presented here is just a downgraded version\nof the parallel code, hence why they look so similar.  This is available\n<a href=\"http://github.com/mrocklin/dask-crossval\">on github</a>.</p>\n\n<p>So the result of our normal imperative-style for-loop code is a fully\nparallelizable dask graph.  We visualize that graph below.</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>test_score.visualize()\n</code></pre></div></div>\n\n<p><a href=\"https://mrocklin.github.io/blog/images/crossval.png\">\n  <img src=\"https://mrocklin.github.io/blog/images/crossval.png\" alt=\"Cross validation dask graph\" />\n</a></p>\n\n<h2 id=\"help\">Help!</h2>\n\n<p>Is this a useful interface?  It would be great if people could try this out\nand <a href=\"http://github.com/ContinuumIO/dask/issues/new\">generate feedback</a> on <code class=\"language-plaintext highlighter-rouge\">dask.do</code>.</p>\n\n<p>For more information on <code class=\"language-plaintext highlighter-rouge\">dask.do</code> see the\n<a href=\"http://dask.pydata.org/en/latest/imperative.html\">dask imperative documentation</a>.</p>"
}