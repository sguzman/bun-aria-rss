{
  "title": "Neural Nets with Caffe Utilizing the GPU",
  "link": "https://www.joyofdata.de/blog/neural-networks-with-caffe-on-the-gpu/",
  "comments": "https://www.joyofdata.de/blog/neural-networks-with-caffe-on-the-gpu/#comments",
  "pubDate": "Sat, 09 May 2015 12:02:22 +0000",
  "dc:creator": "Raffael Vogler",
  "category": [
    "Machine Learning",
    "Kaggle"
  ],
  "guid": "http://www.joyofdata.de/blog/?p=3545",
  "description": "Caffe is an open-source deep learning framework originally created by Yangqing Jia which allows you to leverage your GPU for training neural networks. As opposed to other deep learning frameworks like Theano or Torch you don&#8217;t have to program the &#8230; <a href=\"https://www.joyofdata.de/blog/neural-networks-with-caffe-on-the-gpu/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
  "content:encoded": "<p style=\"text-align: justify;\"><a href=\"http://www.joyofdata.de/blog/wp-content/uploads/2015/05/network-graph.png\"><img class=\" alignright wp-image-3550\" src=\"http://www.joyofdata.de/blog/wp-content/uploads/2015/05/network-graph.png\" alt=\"network-graph\" width=\"115\" height=\"234\" /></a><a href=\"http://caffe.berkeleyvision.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Caffe</a> is an open-source deep learning framework originally created by <a href=\"http://daggerfs.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Yangqing Jia</a> which allows you to leverage your GPU for training neural networks. As opposed to other deep learning frameworks like <a href=\"http://www.deeplearning.net/software/theano/\" target=\"_blank\" rel=\"noopener noreferrer\">Theano</a> or <a href=\"http://torch.ch/\" target=\"_blank\" rel=\"noopener noreferrer\">Torch</a> you don&#8217;t have to program the algorithms yourself; instead you specify your network by means of configuration files. Obviously this approach is less time consuming than programming everything on your own, but it also forces you to stay within the boundaries of the framework, of course. Practically though this won&#8217;t matter most of the time as the framework Caffe provides is quite powerful and continuously advanced.</p>\n<p style=\"text-align: justify;\"><span id=\"more-3545\"></span></p>\n<p><span class=\"alignleft\"></span></p>\n<p style=\"text-align: justify;\">The subject of this article is the composition of a multi-layer feed-forward network. This model will be trained based on data of the &#8220;<a href=\"https://www.kaggle.com/c/otto-group-product-classification-challenge\" target=\"_blank\" rel=\"noopener noreferrer\">Otto Group Product Classification Challenge</a>&#8221; at Kaggle. We&#8217;ll also take a look at applying the model to new data and eventually you&#8217;ll see how to visualize the network graph and the trained weights. I won&#8217;t explain all the details, as this would bloat the text beyond a bearable scale. Also, if you are like me &#8211; straightforward code says more than a thousand words. Instead check out this <span style=\"color: #ff0000;\"><strong><a style=\"color: #ff0000;\" href=\"http://nbviewer.ipython.org/github/joyofdata/joyofdata-articles/blob/master/deeplearning-with-caffe/Neural-Networks-with-Caffe-on-the-GPU.ipynb\" target=\"_blank\" rel=\"noopener noreferrer\">IPython Notebook</a></strong></span> for the programmatical details &#8211; here I will focus on describing the concepts and some of the tripping stones I encountered.</p>\n<p style=\"text-align: center; border: 1px solid #378efd\">\n<img src=\"http://www.joyofdata.de/blog/wp-content/uploads/2015/05/stay-tuned.png\" alt=\"stay-tuned\" style=\"padding-right:30px; height:30px\"/>\n\n<a href=\"https://twitter.com/joyofdata\" target=\"new\" style=\"padding-right:20px\">\n<img src=\"http://www.joyofdata.de/blog/wp-content/uploads/2015/05/twitter.png\" alt=\"twitter\" height=\"28\" /></a>\n<a href=\"http://feedly.com/i/subscription/feed/http://www.joyofdata.de/blog/feed/\" target=\"new\" style=\"padding-right:20px\">\n<img src=\"http://www.joyofdata.de/blog/wp-content/uploads/2015/05/feedly.png\" alt=\"feedly\" width=\"30\" height=\"30\" /></a>\n<a href=\"https://github.com/joyofdata\" target=\"new\" style=\"padding-right:20px\">\n<img src=\"http://www.joyofdata.de/blog/wp-content/uploads/2015/05/github.png\" alt=\"github\" width=\"30\" height=\"30\" /></a>\n</p>\n<h1 style=\"text-align: justify;\">Setting Up</h1>\n<p style=\"text-align: justify;\">Most likely you don&#8217;t have caffe yet installed on your system &#8211; if yes, good for you &#8211; if not, I recommend working on an EC2 instance allowing GPU-processing, f.x. the g2.2xlarge instance. For instructions on how to work with EC2 have a look at <a href=\"http://www.joyofdata.de/blog/guide-to-aws-ec2-on-cli/\" target=\"_blank\" rel=\"noopener noreferrer\">Guide to EC2 from the Command Line</a> and for setting up caffe and its prerequisits work through <a href=\"http://www.joyofdata.de/blog/gpu-powered-deeplearning-with-nvidia-digits/\" target=\"_blank\" rel=\"noopener noreferrer\">GPU Powered DeepLearning with NVIDIA DIGITS on EC2</a>. For playing around with Caffe I also recommend installing IPython Notebook on your instance &#8211; the instructions for this you&#8217;ll find <a href=\"http://badhessian.org/2013/11/cluster-computing-for-027hr-using-amazon-ec2-and-ipython-notebook/\" target=\"_blank\" rel=\"noopener noreferrer\">here</a>.</p>\n<h1 style=\"text-align: justify;\">Defining the Model and Meta-Parameters</h1>\n<p style=\"text-align: justify;\">Training of a model and its application requires at least three configuration files. The format of those configuration files follows an interface description language called <a href=\"https://developers.google.com/protocol-buffers/\" target=\"_blank\" rel=\"noopener noreferrer\">protocol buffers</a>. It supeficially resembles JSON but is significantly different and actually supposed to replace it in use cases where the data document needs to be validateable (by means of a custom schema &#8211; <a href=\"https://github.com/BVLC/caffe/blob/master/src/caffe/proto/caffe.proto\" target=\"_blank\" rel=\"noopener noreferrer\">like this one for Caffe</a>) and serializable.</p>\n<p style=\"text-align: justify;\"><a href=\"http://www.joyofdata.de/blog/wp-content/uploads/2015/05/network-graph.png\"><img class=\"alignleft size-thumbnail wp-image-3550\" src=\"http://www.joyofdata.de/blog/wp-content/uploads/2015/05/network-graph-150x150.png\" alt=\"network-graph\" width=\"150\" height=\"150\" /></a>For training you need one prototxt-file keeping the meta-parameters (<a href=\"https://github.com/joyofdata/joyofdata-articles/blob/master/deeplearning-with-caffe/config.prototxt\" target=\"_blank\" rel=\"noopener noreferrer\">config.prototxt</a>) of the training and the model and another for defining the graph of the network (<a href=\"https://github.com/joyofdata/joyofdata-articles/blob/master/deeplearning-with-caffe/model_train_test.prototxt\" target=\"_blank\" rel=\"noopener noreferrer\">model_train_test.prototxt</a>) &#8211; connecting the layers in an acyclical and directed fashion. Note that the data flows from <pre class=\"crayon-plain-tag\">bottom</pre>  to <pre class=\"crayon-plain-tag\">top</pre>  with regards to how the order of layers is specified. The example network here is composed of five layers:</p>\n<ol style=\"text-align: justify;\">\n<li><a href=\"http://caffe.berkeleyvision.org/tutorial/layers.html#data-layers\" target=\"_blank\" rel=\"noopener noreferrer\">data layer</a> (one for TRAINing and one for TESTing)</li>\n<li><a href=\"http://caffe.berkeleyvision.org/tutorial/layers.html#inner-product\" target=\"_blank\" rel=\"noopener noreferrer\">inner product layer</a> (the weights I)</li>\n<li><a href=\"http://caffe.berkeleyvision.org/tutorial/layers.html#relu--rectified-linear-and-leaky-relu\" target=\"_blank\" rel=\"noopener noreferrer\">rectified linear units</a> (the hidden layer)</li>\n<li>inner product layer (the weights II)</li>\n<li>output layer (Soft Max for classification)\n<ol>\n<li>soft max layer giving the loss</li>\n<li>accuracy layer &#8211; so we can see how the network improves while training.</li>\n</ol>\n</li>\n</ol>\n<p>The following excerpt from model_train_test.prototxt shows layers (4) and (5A):</p><pre class=\"crayon-plain-tag\">[...]\nlayer {\n  name: \"ip2\"\n  type: \"InnerProduct\"\n  bottom: \"ip1\"\n  top: \"ip2\"\n  inner_product_param {\n    num_output: 9\n    weight_filler {\n      type: \"xavier\"\n    }\n    bias_filler {\n      type: \"constant\"\n      value: 0\n    }\n  }\n}\nlayer {\n  name: \"accuracy\"\n  type: \"Accuracy\"\n  bottom: \"ip2\"\n  bottom: \"label\"\n  top: \"accuracy\"\n  include {\n    phase: TEST\n  }\n}\n[...]</pre><p>The third prototxt-file (<a href=\"https://github.com/joyofdata/joyofdata-articles/blob/master/deeplearning-with-caffe/model_prod.prototxt\" target=\"_blank\" rel=\"noopener noreferrer\">model_prod.prototxt</a>) specifies the network to be used for applying it. In this case it is mostly congruent with the specification for training &#8211; but it lacks the data layers (as we don&#8217;t read data from a data source at production) and the Soft Max layer won&#8217;t yield a loss value but classification probabilities. Also the accuracy layer is gone now. Note also that &#8211; at the beginning &#8211; we now specify the input dimensions (as expected: 1,93,1,1) &#8211; it is certainly confusing that all four dimensions are referred to as <pre class=\"crayon-plain-tag\">input_dim</pre> , that only the order defines which is which and no explicit context is specified.</p>\n<h1 style=\"text-align: justify;\">Supported Data Sources</h1>\n<p style=\"text-align: justify;\">This is one of the first mental obstacle to overcome when trying to get started with Caffe. It is not as simple as providing the caffe executable with some CSV and let it have its way with it. Practically, for not-image data, you have three options.</p>\n<ul style=\"text-align: justify;\">\n<li><a href=\"http://symas.com/mdb/\" target=\"_blank\" rel=\"noopener noreferrer\">LMDB</a> (Lightning Memory-Mapped Database)</li>\n<li><a href=\"http://leveldb.org/\" target=\"_blank\" rel=\"noopener noreferrer\">LevelDB</a></li>\n<li><a href=\"https://www.hdfgroup.org/HDF5/\" target=\"_blank\" rel=\"noopener noreferrer\">HDF5</a> format</li>\n</ul>\n<p><span class=\"alignright\"></span></p>\n<p style=\"text-align: justify;\">HDF5 is probably the easiest to use b/c you simply have to store the data sets in files using the HDF5 format. LMDB and LevelDB are databases so you&#8217;ll have to go by their protocol. The size of a data set stored as HDF5 will be limited by your memory, which is why I discarded it. The choice between LMDB and LevelDB was rather arbitrary &#8211; LMDB seemed more powerful, faster and mature judging from the sources I skimmed over. Then again LevelDB seems more actively maintained, judging from its GitHub repo and also has a larger Google and stackoverflow footprint.</p>\n<h1 style=\"text-align: justify;\">Blobs and Datums</h1>\n<p style=\"text-align: justify;\">Caffe internally works with a data structure called <a href=\"http://caffe.berkeleyvision.org/tutorial/net_layer_blob.html\" target=\"_blank\" rel=\"noopener noreferrer\">blobs</a> which is used to pass data forward and gradients backward. It&#8217;s a four dimensional array whose four dimensions are referred to as:</p>\n<ol style=\"text-align: justify;\">\n<li>N or batch_size</li>\n<li>channels</li>\n<li>height</li>\n<li>width</li>\n</ol>\n<p style=\"text-align: justify;\">This is relevant to us b/c we&#8217;ll have to <a href=\"http://nbviewer.ipython.org/github/joyofdata/joyofdata-articles/blob/master/deeplearning-with-caffe/Neural-Networks-with-Caffe-on-the-GPU.ipynb#Load-Data-into-LMDB\" target=\"_blank\" rel=\"noopener noreferrer\">shape our cases into this structure</a> before we can store it in LMDB &#8211; from where it is feeded directly to Caffe. The shape is straight-forward for images where a batch of 64 images each defined by 100&#215;200 RGB-pixels would end up as an array shaped (64, 3, 200, 100). For a batch of 64 feature vectors each of length 93 the blob&#8217;s shape is (64, 93, 1, 1).</p>\n<p style=\"text-align: justify;\">Under <a href=\"http://nbviewer.ipython.org/github/joyofdata/joyofdata-articles/blob/master/deeplearning-with-caffe/Neural-Networks-with-Caffe-on-the-GPU.ipynb#Load-Data-into-LMDB\" target=\"_blank\" rel=\"noopener noreferrer\">Load Data into LMDB</a> you can see that the individual cases or feature vectors are stored in <a href=\"https://github.com/BVLC/caffe/blob/bc6e8386f4fb1bd0ffd5083714a379197071e881/src/caffe/proto/caffe.proto#L28\" target=\"_blank\" rel=\"noopener noreferrer\">Datum objects</a>. Integer valued features are stored (as a byte string) in <pre class=\"crayon-plain-tag\">data</pre>, float valued features in <pre class=\"crayon-plain-tag\">float_data</pre>. In the beginning I made the mistake to assign float valued features to data which caused the model to not learn anything. Before storing the Datum in LMDB you have to serialize the object into a byte string representation.</p>\n<h1 style=\"text-align: justify;\">Bottom Line</h1>\n<p style=\"text-align: justify;\">Getting a grip at Caffe was a surprisingly non-linear experience for me. That means there is no entry point and a continuous learning path which will lead you to a good understanding of the system. The information required to do something useful with Caffe is distributed onto many different <a href=\"http://caffe.berkeleyvision.org/tutorial/\" target=\"_blank\" rel=\"noopener noreferrer\">tutorial sections</a>, <a href=\"https://github.com/BVLC/caffe/\" target=\"_blank\" rel=\"noopener noreferrer\">source code on GitHub</a>, <a href=\"https://github.com/BVLC/caffe/tree/master/examples\" target=\"_blank\" rel=\"noopener noreferrer\">IPython notebooks</a> and <a href=\"https://groups.google.com/forum/#!forum/caffe-users\" target=\"_blank\" rel=\"noopener noreferrer\">forum threads</a>. This is why I took the time to compose this tutorial and its accompanying code, following my maxim to summarize what I learned into a text I would have liked to read myself in the beginning.</p>\n<p style=\"text-align: justify;\">I think Caffe has a bright future ahead &#8211; provided it will not just grow horizontally by adding new features but also vertically by refactoring and improving the over all user experience. It&#8217;s definitely a great tool for high performance deep learning. In case you want to do image processing with convolutional neural networks, I recommend you take a look at <a href=\"http://www.joyofdata.de/blog/gpu-powered-deeplearning-with-nvidia-digits/\" target=\"_blank\" rel=\"noopener noreferrer\">NVIDIA DIGITS</a> which offers you a comfortable GUI for that purpose.</p>\n<hr />\n<p style=\"text-align: justify;\">(original article published on <a href=\"http://www.joyofdata.de/blog/neural-networks-with-caffe-on-the-gpu/\">www.joyofdata.de</a>)</p>\n",
  "wfw:commentRss": "https://www.joyofdata.de/blog/neural-networks-with-caffe-on-the-gpu/feed/",
  "slash:comments": 1
}