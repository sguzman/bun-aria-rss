{
  "title": "Train gigantic models with near-linear scaling using sharded data parallelism on Amazon SageMaker",
  "link": "https://aws.amazon.com/blogs/machine-learning/train-gigantic-models-with-near-linear-scaling-using-sharded-data-parallelism-on-amazon-sagemaker/",
  "dc:creator": "Emily Webber",
  "pubDate": "Mon, 31 Oct 2022 20:23:47 +0000",
  "category": [
    "Amazon SageMaker",
    "Artificial Intelligence",
    "Expert (400)"
  ],
  "guid": "c08a2e8390bb354a82879a764f32faf59f28ccd7",
  "description": "In the pursuit of superior accuracy, deep learning models in areas such as natural language processing and computer vision have significantly grown in size in the past few years, frequently counted in tens to hundreds of billions of parameters. Training these gigantic models is challenging and requires complex distribution strategies. Data scientists and machine learning […]",
  "content:encoded": "<p>In the pursuit of superior accuracy, deep learning models in areas such as natural language processing and computer vision have significantly grown in size in the past few years, frequently counted in tens to hundreds of billions of parameters. Training these gigantic models is challenging and requires complex distribution strategies. Data scientists and machine learning engineers are constantly looking for the best way to optimize their training compute, yet are struggling with the communication overhead that can increase along with the overall cluster size.</p> \n<p>This is why we&nbsp;recently launched&nbsp;<a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-sharded-data-parallelism.html\" target=\"_blank\" rel=\"noopener\">s<em>harded data parallelism</em></a>&nbsp;on <a href=\"https://aws.amazon.com/sagemaker/\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker</a>, a new memory-saving distributed training technique in the&nbsp;<a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel.html\" target=\"_blank\" rel=\"noopener\">SageMaker model parallel (SMP) library</a>. Sharded data parallelism is purpose-built for extreme-scale models and uses Amazon in-house <a href=\"https://arxiv.org/pdf/2205.00119.pdf\" target=\"_blank\" rel=\"noopener\"><em>MiCS</em></a> technology under the hood, a science effort to minimize the communication scale&nbsp;by bringing down expensive communication overhead rooted in parameter gathering and gradient synchronization. With a 30B parameter GPT-2 model with sequence length 2048, this new feature achieved&nbsp;141 TFLOPs, a 39.7% speed up compared to DeepSpeed ZeRO-3. For a 10B GPT-2 model with sequence length 512, this new feature also achieved 564 samples per second, a 13.9% speed up compared to PyTorch’s&nbsp;Fully Sharded Data Parallel (FSDP).&nbsp;Remember that in gigantic model training, every percentage of speedup translates to dollars saved and productivity gained in your team.</p> \n<p>In this blog post, we’ll first take a closer look at the key differentiators of sharded data parallelism and when to use it. Then, you’ll learn how to train a 30B parameter GPT-2 model on SageMaker with ease with this new feature. Finally we’ll compare the performance with other open source options, notably outperforming DeepSpeed ZeRO by up to 39.7% on 256 GPUs.</p> \n<h2>How sharded data parallelism works and when to use it</h2> \n<p>Before we introduce sharded data parallelism, let’s look at its broader technique family. Recent distributed training approaches for large models have moved to a paradigm where model parameters, gradients, and optimizer states are shared across data-parallel nodes. Unlike Pipeline&nbsp;Parallelism which has the innate complexity of choosing layers to partition across devices especially when your framework doesn’t support <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-core-features.html\" target=\"_blank\" rel=\"noopener\">automated model splitting</a>, this paradigm elegantly preserves the simplicity of data parallelism, while removing data parallelism’s constraint where a model must fit into a single GPU.</p> \n<p>In existing frameworks that fall under this paradigm, notably DeepSpeed ZeRO-3 and PyTorch’s FSDP&nbsp;upstreamed from FairScale, model states are sharded across <em>all</em> GPUs, a strategy that lowers the memory consumption on each GPU at the cost of incurring large communication overhead which increases with&nbsp;cluster size and therefore causes the scalability to significantly drop at scale. In contrast, sharded data parallelism in the SMP library partitions model states in a&nbsp;<em>scale-aware </em>manner by partitioning each replica of model states only within <em>a subset </em>of GPUs.</p> \n<p>Let’s look closer at the <em>scale-aware model partitioning</em> in MiCS, the core technology behind sharded data parallel. The intuition behind this design is that partitioning training states across the&nbsp;entire&nbsp;data-parallel group may not be required to train a model with tens of billions of parameters. For example, 8 V100 GPUs (32GB each) are sufficient to hold the model states replica of a 10B-parameter model which needs about 200GB of memory when training with Adam optimizer using mixed-precision. By limiting a complete replica of model states in the <em>smallest</em> subset of GPUs, we can effectively reduce the scale of communication overhead compared to DeepSpeed and PyTorch FSDP. Sharded data parallel also leverages other techniques in MiCS such as Hierarchical Communication and 2-hop Gradient Synchronization. For more information, check out <a href=\"https://www.amazon.science/blog/near-linear-scaling-of-gigantic-model-training-on-aws\" target=\"_blank\" rel=\"noopener\">Near-linear scaling of gigantic-model training on AWS</a> or <a href=\"https://arxiv.org/pdf/2205.00119.pdf\" target=\"_blank\" rel=\"noopener\">MiCS: Near-linear Scaling for Training Gigantic Model on Public Cloud</a>.</p> \n<p>Now, how do you know when to choose&nbsp;sharded data parallel over other distributed training techniques? The general rule&nbsp;is that if your model has less than 1 billion parameters and can fit into&nbsp;GPU memory,&nbsp;<a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel.html\" target=\"_blank\" rel=\"noopener\">SageMaker data parallel library </a>or <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/training-compiler.html\" target=\"_blank\" rel=\"noopener\">SageMaker training compiler</a>&nbsp;can be sufficient for you. If you have larger language or computer vision models, our suggestion is to train it with the sharded data parallelism technique combined with&nbsp;<a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-activation-checkpointing.html\" target=\"_blank\" rel=\"noopener\">activation checkpointing</a>&nbsp;and <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-activation-offloading.html\" target=\"_blank\" rel=\"noopener\">activation offloading</a>&nbsp;in&nbsp;the SageMaker model parallel library first, before other techniques such as <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-tensor-parallelism.html\" target=\"_blank\" rel=\"noopener\">tensor parallelism</a> or pipeline parallelism.</p> \n<h2>Using&nbsp;sharded data parallelism to train GPT-2 on Amazon SageMaker</h2> \n<p>Let’s now learn how to train a GPT-2 model with sharded data parallel, with SMP&nbsp;encapsulating the complexity for you. This&nbsp;<a href=\"https://github.com/aws/amazon-sagemaker-examples/blob/main/training/distributed_training/pytorch/model_parallel/gpt2/smp-train-gpt-simple-sharded-data-parallel.ipynb\" target=\"_blank\" rel=\"noopener\">complete tutorial&nbsp;notebook</a> walks you through the entire process, from data processing, defining and submitting training jobs, to monitoring training logs. What follows is a brief overview&nbsp;highlighting key steps for using this feature.</p> \n<h3>1. Get started</h3> \n<p>Sharded data parallelism is available in PyTorch&nbsp;v1.12.0+ and works with&nbsp;both FP16 and BF16. The easiest way&nbsp;to use the SMP library is through&nbsp;a&nbsp;prebuilt AWS Deep Learning Container for PyTorch. However, if you want to bring your own Docker container, you can refer to <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-sm-sdk.html#model-parallel-bring-your-own-container\" target=\"_blank\" rel=\"noopener\">Create Your Own Docker Container with the SageMaker Distributed Model Parallel Library.</a>&nbsp;To get started, follow&nbsp;<a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-customize-training-script-pt.html\" target=\"_blank\" rel=\"noopener\">Modify a PyTorch Training Script</a>&nbsp;to adapt SMPs’ APIs in your training script. In this section, we only call out a few main steps with code snippets from the&nbsp;ready-to-use training script&nbsp;<code><a href=\"https://github.com/aws/amazon-sagemaker-examples/blob/main/training/distributed_training/pytorch/model_parallel/gpt2/train_gpt_simple.py\" target=\"_blank\" rel=\"noopener\">train_gpt_simple.py</a></code>. You can follow the comments in the script and <a href=\"https://sagemaker.readthedocs.io/en/stable/api/training/smp_versions/latest/smd_model_parallel_pytorch.html\" target=\"_blank\" rel=\"noopener\">API document</a> to learn more about where SMP APIs are used.</p> \n<p>First, import and initialize the library by calling <a href=\"https://sagemaker.readthedocs.io/en/stable/api/training/smp_versions/v1.2.0/smd_model_parallel_common_api.html#smp.init\" target=\"_blank\" rel=\"noopener\"><code>smdistributed.modelparallel.torch.init()</code></a>&nbsp;at the beginning&nbsp;of the training script:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">import smdistributed.modelparallel.torch as smp\n\nsmp.init(smp_config)</code></pre> \n</div> \n<p>Second, wrap the model&nbsp;to be partitioned&nbsp;with&nbsp;<a href=\"https://sagemaker.readthedocs.io/en/stable/api/training/smp_versions/v1.2.0/smd_model_parallel_pytorch.html#smp.DistributedModel\" target=\"_blank\" rel=\"noopener\">smdistributed.modelparallel.torch.DistributedModel</a>&nbsp;and use the returned&nbsp;<code>DistributedModel</code> object going forward:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">from transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_config(model_config)\nmodel = smp.DistributedModel(model, trace_device=\"gpu\", backward_passes_per_step=args.gradient_accumulation)</code></pre> \n</div> \n<p>Wrap the optimizer with <code><a href=\"https://sagemaker.readthedocs.io/en/stable/api/training/smp_versions/v1.2.0/smd_model_parallel_pytorch.html#smp.DistributedOptimizer\" target=\"_blank\" rel=\"noopener\">smdistributed.modelparallel.torch.DistributedOptimizer</a></code>&nbsp;for saving and loading optimizer states.</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">from torch import optim\n\noptimizer = optim.Adam(\n    param_groups, betas=(args.beta1, args.beta2), lr=args.lr, weight_decay=args.weight_decay\n)\n\noptimizer = smp.DistributedOptimizer(\n        optimizer, \n        static_loss_scale=None, \n        dynamic_loss_scale=True,\n        dynamic_loss_args={\"scale_window\": 1000, \"min_scale\": 1, \"delayed_shift\": 2},\n        )</code></pre> \n</div> \n<p>Put the forward and backward logic in a step function and decorate it with<code><a href=\"https://sagemaker.readthedocs.io/en/stable/api/training/smp_versions/v1.2.0/smd_model_parallel_common_api.html#smp.init\" target=\"_blank\" rel=\"noopener\"> smdistributed.modelparallel.torch.step</a></code>.<strong>&nbsp; </strong>Any computation defined inside the <code>smp.step-decorated</code> function is executed in a distributed manner.</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">@smp.step\ndef train_step(model, optimizer, input_ids, attention_mask, args):\n    loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)[\"loss\"]\n    model.backward(loss)\n\n    return loss\n\n@smp.step\ndef test_step(model, input_ids, attention_mask):\n    loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)[\"loss\"]\n    \n    return loss</code></pre> \n</div> \n<h3>2. Prepare the dataset</h3> \n<p>We use the&nbsp;<a href=\"https://huggingface.co/datasets/viewer/?dataset=openwebtext\" target=\"_blank\" rel=\"noopener\">openwebtext</a> is the dataset we use in this example. The notebook uses the script <code>data_prep_512.py</code> to download and preprocess the dataset. You can also train with other datasets by modifying <code>data_pipeline.py</code>. When dealing with large dataset and model, you can speed up the training job by using data stored in&nbsp;<a class=\"c-link\" href=\"https://aws.amazon.com/fsx/lustre/\" target=\"_blank\" rel=\"noopener noreferrer\" data-stringify-link=\"https://aws.amazon.com/fsx/lustre/\" data-sk=\"tooltip_parent\" data-remove-tab-index=\"true\">Amazon FSx for Lustre</a>, which provides a high-performance file system natively integrated with&nbsp;<a class=\"c-link\" href=\"https://aws.amazon.com/s3/\" target=\"_blank\" rel=\"noopener noreferrer\" data-stringify-link=\"https://aws.amazon.com/s3/\" data-sk=\"tooltip_parent\" data-remove-tab-index=\"true\">Amazon Simple Storage Service</a>&nbsp;(S3). Please see the instructions from&nbsp;<a class=\"c-link\" href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html#model-access-training-data-fsx\" target=\"_blank\" rel=\"noopener noreferrer\" data-stringify-link=\"https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html#model-access-training-data-fsx\" data-sk=\"tooltip_parent\" data-remove-tab-index=\"true\">Configure Data Input Channel to Use Amazon FSx for Lustre</a> for guidance on setting an FSx Lustre file system as data input channel.</p> \n<h3>3. Start the training jobs</h3> \n<p>This step assumes you have already <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-customize-training-script-pt.html\" target=\"_blank\" rel=\"noopener\">modified your training script</a>&nbsp;and prepared the dataset as mentioned in the preceding sections.&nbsp;To<strong>&nbsp;</strong>enable sharded data parallelism, simply set the&nbsp;<code>sharded_data_parallel_degree</code>&nbsp;in the<a href=\"https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/sagemaker.pytorch.html\" target=\"_blank\" rel=\"noopener\"> PyTorch Estimator</a>.&nbsp;In this tutorial, we set&nbsp;<code>sharded_data_parallel_degree=128</code>&nbsp;and&nbsp;<code>instace_count=32</code>&nbsp;for p4d.24xlarge nodes, which indicates that the model states will be&nbsp;sharded across 128 GPUs among the total 256 GPUs.&nbsp;Based on this selected value, SMP will then automatically sets the data parallel degree to 2 (because 256/128=2), meaning we’ll have two replicas for data parallelism.&nbsp;A general rule for picking an ideal value for <code>sharded_data_parallel_degree</code>&nbsp;is to add one more node to the sharing group per every 3B of model parameters. In this tutorial, our model size is 30B, so we should use at least 10 nodes for sharding. And because 16 nodes (128 GPUs) is the smallest power-of-2 above the threshold, we set&nbsp;<code>sharded_data_parallel_degree=128</code>.</p> \n<p>For checkpointing, we also provide a set of checkpointing utilities in <a href=\"https://github.com/aws/amazon-sagemaker-examples/blob/main/training/distributed_training/pytorch/model_parallel/gpt2/sharded_data_parallel_checkpoint.py\" target=\"_blank\" rel=\"noopener\"><code>sharded_data_parallel_checkpoint.py</code></a>&nbsp;, including a utility to reconstruct the full <code>state_dict</code> for advanced use cases. Finally, we can launch a distributed training job by calling fit()&nbsp;on the Estimator.</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">smp_estimator = PyTorch(\n    entry_point=\"train_gpt_simple.py\",\n    instance_type=\"ml.p4d.24xlarge\",\n    source_dir=os.getcwd(),\n    volume_size=500,\n    instance_count=32,\n    distribution={\n        \"mpi\": {\n            \"enabled\": True,\n            \"processes_per_host\": processes_per_host,\n            \"custom_mpi_options\": mpioptions,\n        },\n        \"smdistributed\": {\n            \"modelparallel\": {\n                \"enabled\": True,\n                \"parameters\": {\n                    \"ddp\": True,\n                    \"skip_tracing\": True,\n                    \"delayed_parameter_initialization\": True,\n                    \"offload_activations\": True,\n                    \"activation_loading_horizon\": 4,\n                    # To enable sharded data parallelism.\n                    # Here we shard model states across 128 GPUs. \n                    \"sharded_data_parallel_degree\": 128, \n                    \"fp16\": False,\n                    \"bf16\": True,\n                    # This is to disable pipeline parallelism.\n                    \"partitions\": 1,\n                },\n            }\n        },\n    },\n    framework_version=\"1.12\",\n    py_version=\"py38\",\n    hyperparameters=hyperparameters,\n    checkpoint_s3_uri=checkpoint_s3_uri if not use_fsx else None,\n    checkpoint_local_path=hyperparameters[\"checkpoint-dir\"] if use_fsx else None,\n    ...\n)\n\nsmp_estimator.fit(inputs=data_channels)</code></pre> \n</div> \n<h3>4. Monitor the training jobs</h3> \n<p>You can access the training logs and track GPU and memory utilization on&nbsp;<a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/WhatIsCloudWatch.html\" target=\"_blank\" rel=\"noopener\">Amazon CloudWatch</a>. Make sure to look at the logs of “algo-1”<em>&nbsp;</em>because that is the main node whose output stream has&nbsp;the training job logs&nbsp;from all instances.</p> \n<h2>Benchmarking performance</h2> \n<p>We benchmarked sharded data parallelism in the SMP library on both 16 and 32 p4d.24xlarge nodes for sequence length 512 and 2048, respectively. The 30B-parameter GPT2 model is configured to use a hidden width of 7168, 48 layers, and 64 heads. You can adopt the exact same configuration where sequence length is 2048 by setting <code>model_config = \"gpt2-30b\"</code> in the tutorial notebook. With this setting, SMP achieved 73.52 samples per second, a 39.7% speed up compared to DeepSpeed ZeRO-3. If your token size is 500 billion, this speed up means nearly 367 hours of savings on p4d.24xlarge nodes, an equivalent of more than $12,000 budget saved per training!&nbsp;The following table summarizes our benchmark results.</p> \n<table border=\"1px\"> \n <tbody> \n  <tr style=\"background-color: #000000\"> \n   <td style=\"text-align: center;vertical-align: middle\" colspan=\"4\"><span style=\"color: #ffffff\"><strong>Configuration</strong></span></td> \n   <td style=\"text-align: center;vertical-align: middle\" colspan=\"4\"><span style=\"color: #ffffff\"><strong>Performance</strong></span></td> \n   <td style=\"text-align: center;vertical-align: middle\" colspan=\"2\"><span style=\"color: #ffffff\"><strong>Time to train with SMP (days)</strong></span></td> \n  </tr> \n  <tr style=\"background-color: #000000\"> \n   <td style=\"text-align: center;vertical-align: middle\"><span style=\"color: #ffffff;text-align: center;vertical-align: middle\"><strong>Model/Training</strong></span></td> \n   <td style=\"text-align: center;vertical-align: middle\"><span style=\"color: #ffffff;text-align: center;vertical-align: middle\"><strong>Cluster</strong></span></td> \n   <td style=\"text-align: center;vertical-align: middle\"><span style=\"color: #ffffff;text-align: center;vertical-align: middle\"><strong>DeepSpeed</strong></span></td> \n   <td style=\"text-align: center;vertical-align: middle\"><span style=\"color: #ffffff;text-align: center;vertical-align: middle\"><strong>SMP</strong></span></td> \n   <td style=\"text-align: center;vertical-align: middle\"><span style=\"color: #ffffff;text-align: center;vertical-align: middle\"><strong>Speed (samples/sec)</strong></span><br> <span style=\"color: #ffffff;text-align: center;vertical-align: middle\"><strong>DeepSpeed v0.7.2</strong></span></td> \n   <td style=\"text-align: center;vertical-align: middle\"><span style=\"color: #ffffff;text-align: center;vertical-align: middle\"><strong>Speed (samples/sec)</strong></span><br> <span style=\"color: #ffffff;text-align: center;vertical-align: middle\"><strong>SMP v1.11</strong></span></td> \n   <td style=\"text-align: center;vertical-align: middle\"><span style=\"color: #ffffff;text-align: center;vertical-align: middle\"><strong>% Speedup of SMP</strong></span></td> \n   <td style=\"text-align: center;vertical-align: middle\"><span style=\"color: #ffffff;text-align: center;vertical-align: middle\"><strong>TFLOPS achieved by SMP</strong></span></td> \n   <td style=\"text-align: center;vertical-align: middle\"><span style=\"color: #ffffff;text-align: center;vertical-align: middle\"><strong>100 billion tokens</strong></span></td> \n   <td style=\"text-align: center;vertical-align: middle\"><span style=\"color: #ffffff;text-align: center;vertical-align: middle\"><strong>500 billion tokens</strong></span></td> \n  </tr> \n  <tr> \n   <td><strong>30B GPT-2</strong><br> Seq length:512<br> Global batch size:3072<br> FP16</td> \n   <td>16 p4d.24xlarge nodes</td> \n   <td>Activation checkpointing<br> gradient_accumulation_steps:2</td> \n   <td>Activation checkpointing<br> sharded_data_parallel_degree:64<br> gradient_accumulation:1</td> \n   <td style=\"text-align: center;vertical-align: middle\">142</td> \n   <td style=\"text-align: center;vertical-align: middle\">181.05</td> \n   <td style=\"text-align: center;vertical-align: middle\">27.5</td> \n   <td style=\"text-align: center;vertical-align: middle\">173.6</td> \n   <td style=\"text-align: center;vertical-align: middle\">12.49</td> \n   <td style=\"text-align: center;vertical-align: middle\">62.43</td> \n  </tr> \n  <tr> \n   <td><strong>30B GPT-2</strong><br> Seq length:2048<br> Global batch size&nbsp;1536<br> FP16</td> \n   <td>32 p4d.24xlarge nodes</td> \n   <td>Activation checkpointing<br> gradient_accumulation_steps:2</td> \n   <td>Activation checkpointing sharded_data_parallel_degree:128<br> gradient_accumulation:1</td> \n   <td style=\"text-align: center;vertical-align: middle\">52.6</td> \n   <td style=\"text-align: center;vertical-align: middle\">73.52</td> \n   <td style=\"text-align: center;vertical-align: middle\">39.77</td> \n   <td style=\"text-align: center;vertical-align: middle\">141</td> \n   <td style=\"text-align: center;vertical-align: middle\">7.69</td> \n   <td style=\"text-align: center;vertical-align: middle\">38.43</td> \n  </tr> \n </tbody> \n</table> \n<h5><i data-stringify-type=\"italic\">1/ For each model configuration, we tested different features, stages, and configurations in DeepSpeed ZeRO and chose the one that provides the best throughput as the DeepSpeed baseline. The benchmark was run on&nbsp;</i><i data-stringify-type=\"italic\"><a class=\"c-link\" href=\"http://aws.amazon.com/ec2\" target=\"_blank\" rel=\"noopener noreferrer\" data-stringify-link=\"http://aws.amazon.com/ec2\" data-sk=\"tooltip_parent\" data-remove-tab-index=\"true\">Amazon Elastic Compute Cloud</a></i><i data-stringify-type=\"italic\">&nbsp;(Amazon EC2).&nbsp;</i><i data-stringify-type=\"italic\">2/ These results rely on improved communication collectives optimized for AWS which will be made available soon. 3/ Time to train is projected from speed based on number of tokens processed.</i></h5> \n<p>In summary, we observed consistently&nbsp;higher throughput with sharded data parallelism in SMP when compared to DeepSpeed across a range of models and configurations. This new feature also demonstrated a better memory efficiency compared to DeepSpeed, enabling SMP to fit a larger batch size and reduce the level of gradient accumulation required to fit a particular global batch size.</p> \n<h2>Conclusion</h2> \n<p>In this post, we introduced a new distributed training technique — sharded data parallelism — and how it speeds up gigantic model training with near linear-scaling on Amazon SageMaker. We also walked through how to train a GPT-2 model with the new technique following this <a href=\"https://github.com/aws/amazon-sagemaker-examples/tree/main/training/distributed_training/pytorch/model_parallel/gpt2\" target=\"_blank\" rel=\"noopener\">complete example</a>. You can follow the&nbsp;<a href=\"https://github.com/aws/amazon-sagemaker-examples/tree/main/training/distributed_training\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker Examples GitHub repo</a> to track all SageMaker model parallel examples or attend our next&nbsp;<a href=\"https://github.com/aws-samples/sagemaker-distributed-training-workshop\" target=\"_blank\" rel=\"noopener\">distributed training workshops</a>.&nbsp;To learn more about sharded data parallelism, please see the <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-sharded-data-parallelism.html\" target=\"_blank\" rel=\"noopener\">documentation</a>.</p> \n<hr> \n<h3>About the authors</h3> \n<p style=\"clear: both\"><strong><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2021/12/20/Emily-Webber.png\"><img loading=\"lazy\" class=\"size-full wp-image-31931 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2021/12/20/Emily-Webber.png\" alt=\"\" width=\"100\" height=\"133\"></a>Emily Webber</strong> joined AWS just after SageMaker launched, and has been trying to tell the world about it ever since! Outside of building new ML experiences for customers, Emily enjoys meditating and studying Tibetan Buddhism.</p> \n<p style=\"clear: both\"><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/31/Can-Karakus.png\"><img loading=\"lazy\" class=\"size-full wp-image-45165 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/31/Can-Karakus.png\" alt=\"\" width=\"100\" height=\"100\"></a><strong>Can Karakus </strong>is a Senior Applied Scientist at AWS, optimizing large-scale distributed deep learning on AWS. His research interests cover deep learning, distributed optimization, distributed systems, and information theory. Outside of work, he enjoys cycling, traveling, reading and learning.</p> \n<p style=\"clear: both\"><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/27/huilgolr.jpg\"><img loading=\"lazy\" class=\"size-full wp-image-45039 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/27/huilgolr.jpg\" alt=\"\" width=\"100\" height=\"100\"></a><strong>Rahul Huilgol </strong>is a Senior Software Engineer at AWS. He works on distributed deep learning systems, towards making it easy and performant to train large deep learning models in the cloud. In his spare time, he enjoys photography, biking and gardening.</p> \n<p style=\"clear: both\"><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/27/skodgule.jpg\"><img loading=\"lazy\" class=\"size-full wp-image-45038 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/27/skodgule.jpg\" alt=\"\" width=\"100\" height=\"133\"></a><strong>Suhit Kodgule</strong> is a Software Development Engineer with AWS Artificial Intelligence group working on deep learning frameworks. In his spare time, he enjoys hiking, traveling and cooking.<strong><br> </strong></p> \n<p style=\"clear: both\"><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/27/erin-ho.jpg\"><img loading=\"lazy\" class=\"size-full wp-image-45003 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/27/erin-ho.jpg\" alt=\"\" width=\"100\" height=\"133\"></a><strong>Erin Ho</strong> is a Product Manager for AWS Deep Learning. She works on products that make it easier for customers to train deep learning models on AWS. For fun outside work, she enjoys hiking and skiing.</p>"
}