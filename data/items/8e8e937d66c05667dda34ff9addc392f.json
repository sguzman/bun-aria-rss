{
  "title": "Reading and Writing Pandas DataFrames in Chunks",
  "link": "",
  "published": "2021-04-03T00:00:00-05:00",
  "updated": "2021-04-03T00:00:00-05:00",
  "id": "http://janakiev.com/blog/python-pandas-chunks",
  "content": "<p>This is a quick example how to chunk a large data set with <a href=\"https://pandas.pydata.org/\">Pandas</a> that otherwise won’t fit into memory. In this short example you will see how to apply this to CSV files with <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\">pandas.read_csv</a>.</p>\n\n<h1 id=\"create-pandas-iterator\">Create Pandas Iterator</h1>\n\n<p>First, create a <code class=\"language-plaintext highlighter-rouge\">TextFileReader</code> object for iteration. This won’t load the data until you start iterating over it. Here it chunks the data in DataFrames with 10000 rows each:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">df_iterator</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"p\">.</span><span class=\"n\">read_csv</span><span class=\"p\">(</span>\n    <span class=\"s\">'input_data.csv.gz'</span><span class=\"p\">,</span> \n    <span class=\"n\">chunksize</span><span class=\"o\">=</span><span class=\"mi\">10000</span><span class=\"p\">,</span>\n    <span class=\"n\">compression</span><span class=\"o\">=</span><span class=\"s\">'gzip'</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<h1 id=\"iterate-over-the-file-in-batches\">Iterate over the File in Batches</h1>\n\n<p>Now, you can use the iterator to load the chunked DataFrames iteratively. Here you have a function <code class=\"language-plaintext highlighter-rouge\">do_something(df_chunk)</code>, that is some operation that you need to have done on the table:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">df_chunk</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">df_iterator</span><span class=\"p\">)</span>\n\n    <span class=\"n\">do_something</span><span class=\"p\">(</span><span class=\"n\">df_chunk</span><span class=\"p\">)</span>\n    \n    <span class=\"c1\"># Set writing mode to append after first chunk\n</span>    <span class=\"n\">mode</span> <span class=\"o\">=</span> <span class=\"s\">'w'</span> <span class=\"k\">if</span> <span class=\"n\">i</span> <span class=\"o\">==</span> <span class=\"mi\">0</span> <span class=\"k\">else</span> <span class=\"s\">'a'</span>\n    \n    <span class=\"c1\"># Add header if it is the first chunk\n</span>    <span class=\"n\">header</span> <span class=\"o\">=</span> <span class=\"n\">i</span> <span class=\"o\">==</span> <span class=\"mi\">0</span>\n\n    <span class=\"n\">df_chunk</span><span class=\"p\">.</span><span class=\"n\">to_csv</span><span class=\"p\">(</span>\n        <span class=\"s\">\"dst_data.csv.gz\"</span><span class=\"p\">,</span>\n        <span class=\"n\">index</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">,</span>  <span class=\"c1\"># Skip index column\n</span>        <span class=\"n\">header</span><span class=\"o\">=</span><span class=\"n\">header</span><span class=\"p\">,</span> \n        <span class=\"n\">mode</span><span class=\"o\">=</span><span class=\"n\">mode</span><span class=\"p\">,</span>\n        <span class=\"n\">compression</span><span class=\"o\">=</span><span class=\"s\">'gzip'</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>By default, Pandas infers the compression from the filename. Other supported compression formats include <code class=\"language-plaintext highlighter-rouge\">bz2</code>, <code class=\"language-plaintext highlighter-rouge\">zip</code>, and <code class=\"language-plaintext highlighter-rouge\">xz</code>.</p>\n\n<h1 id=\"resources\">Resources</h1>\n\n<p>For more information on chunking, have a look at the documentation on <a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#io-chunking\">chunking</a>. Another useful tool, when working with data that won’t fit your memory, is <a href=\"https://dask.org/\">Dask</a>. Dask can parallelize the workload on multiple cores or even multiple machines, although it is not a drop-in replacement for Pandas and can be rather viewed as a wrapper for Pandas.</p>",
  "author": {
    "name": "Nikolai Janakiev"
  },
  "category": [
    "",
    "",
    ""
  ],
  "summary": "This is a quick example how to chunk a large data set with Pandas that otherwise won’t fit into memory. In this short example you will see how to apply this to CSV files with pandas.read_csv.",
  "media:thumbnail": "",
  "media:content": ""
}