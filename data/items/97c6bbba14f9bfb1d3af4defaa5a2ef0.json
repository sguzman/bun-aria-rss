{
  "title": "Two Easy Ways to Use Scikit Learn and Dask",
  "link": "",
  "updated": "2017-02-07T00:00:00+00:00",
  "id": "https://mrocklin.github.io/blog/work/2017/02/07/dask-sklearn-simple",
  "content": "<p><em>This work is supported by <a href=\"http://continuum.io\">Continuum Analytics</a>\nthe <a href=\"http://www.darpa.mil/program/XDATA\">XDATA Program</a>\nand the Data Driven Discovery Initiative from the <a href=\"https://www.moore.org/\">Moore\nFoundation</a></em></p>\n\n<h2 id=\"summary\">Summary</h2>\n\n<p>This post describes two simple ways to use Dask to parallelize Scikit-Learn\noperations either on a single computer or across a cluster.</p>\n\n<ol>\n  <li>Use the Dask Joblib backend</li>\n  <li>Use the <code class=\"language-plaintext highlighter-rouge\">dklearn</code> projects drop-in replacements for <code class=\"language-plaintext highlighter-rouge\">Pipeline</code>,\n<code class=\"language-plaintext highlighter-rouge\">GridSearchCV</code>, and <code class=\"language-plaintext highlighter-rouge\">RandomSearchCV</code></li>\n</ol>\n\n<p>For the impatient, these look like the following:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\">### Joblib\n</span>\n<span class=\"kn\">from</span> <span class=\"nn\">joblib</span> <span class=\"kn\">import</span> <span class=\"n\">parallel_backend</span>\n<span class=\"k\">with</span> <span class=\"n\">parallel_backend</span><span class=\"p\">(</span><span class=\"s\">'dask.distributed'</span><span class=\"p\">,</span> <span class=\"n\">scheduler_host</span><span class=\"o\">=</span><span class=\"s\">'scheduler-address:8786'</span><span class=\"p\">):</span>\n    <span class=\"c1\"># your now-cluster-ified sklearn code here\n</span>\n\n<span class=\"c1\">### Dask-learn pipeline and GridSearchCV drop-in replacements\n</span>\n<span class=\"c1\"># from sklearn.grid_search import GridSearchCV\n</span>  <span class=\"kn\">from</span> <span class=\"nn\">dklearn.grid_search</span> <span class=\"kn\">import</span> <span class=\"n\">GridSearchCV</span>\n<span class=\"c1\"># from sklearn.pipeline import Pipeline\n</span>  <span class=\"kn\">from</span> <span class=\"nn\">dklearn.pipeline</span> <span class=\"kn\">import</span> <span class=\"n\">Pipeline</span>\n</code></pre></div></div>\n\n<p>However, neither of these techniques are perfect.  These are the easiest things\nto try, but not always the best solutions.  This blogpost focuses on\nlow-hanging fruit.</p>\n\n<h2 id=\"joblib\">Joblib</h2>\n\n<p>Scikit-Learn already parallelizes across a multi-core CPU using\n<a href=\"https://pythonhosted.org/joblib/\">Joblib</a>, a simple but powerful and mature\nlibrary that provides an extensible map operation.  Here is a simple example of\nusing Joblib on its own without sklearn:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\"># Sequential code\n</span><span class=\"kn\">from</span> <span class=\"nn\">time</span> <span class=\"kn\">import</span> <span class=\"n\">sleep</span>\n<span class=\"k\">def</span> <span class=\"nf\">slowinc</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">):</span>\n    <span class=\"n\">sleep</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>  <span class=\"c1\"># take a bit of time to simulate real work\n</span>    <span class=\"k\">return</span> <span class=\"n\">x</span> <span class=\"o\">+</span> <span class=\"mi\">1</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"p\">[</span><span class=\"n\">slowinc</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">)]</span>  <span class=\"c1\"># this takes 10 seconds\n</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"mi\">7</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"mi\">9</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">]</span>\n\n<span class=\"c1\"># Parallel code\n</span><span class=\"kn\">from</span> <span class=\"nn\">joblib</span> <span class=\"kn\">import</span> <span class=\"n\">Parallel</span><span class=\"p\">,</span> <span class=\"n\">delayed</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">Parallel</span><span class=\"p\">(</span><span class=\"n\">n_jobs</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">)(</span><span class=\"n\">delayed</span><span class=\"p\">(</span><span class=\"n\">slowinc</span><span class=\"p\">)(</span><span class=\"n\">i</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">))</span>  <span class=\"c1\"># this takes 3 seconds\n</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"mi\">7</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"mi\">9</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">]</span>\n</code></pre></div></div>\n\n<p>Dask users will recognize the <code class=\"language-plaintext highlighter-rouge\">delayed</code> function modifier.  Dask stole\nthe <code class=\"language-plaintext highlighter-rouge\">delayed</code> decorator from Joblib.</p>\n\n<p>Many of Scikit-learn’s parallel algorithms use Joblib internally.  If we can\nextend Joblib to clusters then we get some added parallelism from\njoblib-enabled Scikit-learn functions immediately.</p>\n\n<h3 id=\"distributed-joblib\">Distributed Joblib</h3>\n\n<p>Fortunately Joblib provides an interface for other parallel systems to step in\nand act as an execution engine.  We can do this with the <code class=\"language-plaintext highlighter-rouge\">parallel_backend</code>\ncontext manager to run with hundreds or thousands of cores in a nearby cluster:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">import</span> <span class=\"nn\">distributed.joblib</span>\n<span class=\"kn\">from</span> <span class=\"nn\">joblib</span> <span class=\"kn\">import</span> <span class=\"n\">parallel_backend</span>\n\n<span class=\"k\">with</span> <span class=\"n\">parallel_backend</span><span class=\"p\">(</span><span class=\"s\">'dask.distributed'</span><span class=\"p\">,</span> <span class=\"n\">scheduler_host</span><span class=\"o\">=</span><span class=\"s\">'scheduler-address:8786'</span><span class=\"p\">):</span>\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">Parallel</span><span class=\"p\">()(</span><span class=\"n\">delayed</span><span class=\"p\">(</span><span class=\"n\">slowinc</span><span class=\"p\">)(</span><span class=\"n\">i</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">))))</span>\n</code></pre></div></div>\n\n<p>The main value for Scikit-learn users here is that Scikit-learn already uses\n<code class=\"language-plaintext highlighter-rouge\">joblib.Parallel</code> within its code, so this trick works with the Scikit-learn\ncode that you already have.</p>\n\n<p>So we can use Joblib to parallelize normally on our multi-core processor:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">estimator</span> <span class=\"o\">=</span> <span class=\"n\">GridSearchCV</span><span class=\"p\">(</span><span class=\"n\">n_jobs</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"p\">...)</span>  <span class=\"c1\"># use joblib on local multi-core processor\n</span></code></pre></div></div>\n\n<p><em>or</em> we can use Joblib together with Dask.distributed to parallelize across a\nmulti-node cluster:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">with</span> <span class=\"n\">parallel_backend</span><span class=\"p\">(</span><span class=\"s\">'dask.distributed'</span><span class=\"p\">,</span> <span class=\"n\">scheduler_host</span><span class=\"o\">=</span><span class=\"s\">'scheduler-address:8786'</span><span class=\"p\">):</span>\n    <span class=\"n\">estimator</span> <span class=\"o\">=</span> <span class=\"n\">GridSearchCV</span><span class=\"p\">(...)</span>  <span class=\"c1\"># use joblib with Dask cluster\n</span></code></pre></div></div>\n\n<p>(There will be a more thorough example towards the end)</p>\n\n<h3 id=\"limitations\">Limitations</h3>\n\n<p>Joblib is used throughout many algorithms in Scikit-learn, but not all.\nGenerally any operation that accepts an <code class=\"language-plaintext highlighter-rouge\">n_jobs=</code> parameter is a possible\nchoice.</p>\n\n<p>From Dask’s perspective Joblib’s interface isn’t ideal.  For example it will\nalways collect intermediate results back to the main process, rather than\nleaving them on the cluster until necessary.  For computationally intense\noperations this is fine but does add some unnecessary communication overhead.\nAlso Joblib doesn’t allow for operations more complex than a parallel map, so\nthe range of algorithms that this can parallelize is somewhat limited.</p>\n\n<p>Still though, given the wide use of Joblib-accelerated workflows (particularly\nwithin Scikit-learn) this is a simple thing to try if you have a cluster nearby\nwith a possible large payoff.</p>\n\n<h2 id=\"dask-learn-pipeline-and-gridsearch\">Dask-learn Pipeline and Gridsearch</h2>\n\n<p>In July 2016, Jim Crist built and <a href=\"http://jcrist.github.io/blog.html\">wrote\nabout</a> a small project,\n<a href=\"https://github.com/dask/dask-learn\">dask-learn</a>.  This project was a\ncollaboration with SKLearn developers and an attempt to see which parts of\nScikit-learn were trivially and usefully parallelizable.  By far the most\nproductive thing to come out of this work were Dask variants of Scikit-learn’s\nPipeline, GridsearchCV, and RandomSearchCV objects that better handle nested\nparallelism.  Jim observed significant speedups over SKLearn code by using\nthese drop-in replacements.</p>\n\n<p>So if you replace the following imports you may get both better single-threaded\nperformance <em>and</em> the ability to scale out to a cluster:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\"># from sklearn.grid_search import GridSearchCV\n</span>  <span class=\"kn\">from</span> <span class=\"nn\">dklearn.grid_search</span> <span class=\"kn\">import</span> <span class=\"n\">GridSearchCV</span>\n<span class=\"c1\"># from sklearn.pipeline import Pipeline\n</span>  <span class=\"kn\">from</span> <span class=\"nn\">dklearn.pipeline</span> <span class=\"kn\">import</span> <span class=\"n\">Pipeline</span>\n</code></pre></div></div>\n\n<p>Here is a simple example from <a href=\"http://jcrist.github.io/dask-sklearn-part-1.html\">Jim’s more in-depth blogpost</a>:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">sklearn.datasets</span> <span class=\"kn\">import</span> <span class=\"n\">make_classification</span>\n\n<span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">make_classification</span><span class=\"p\">(</span><span class=\"n\">n_samples</span><span class=\"o\">=</span><span class=\"mi\">10000</span><span class=\"p\">,</span>\n                           <span class=\"n\">n_features</span><span class=\"o\">=</span><span class=\"mi\">500</span><span class=\"p\">,</span>\n                           <span class=\"n\">n_classes</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span>\n                           <span class=\"n\">n_redundant</span><span class=\"o\">=</span><span class=\"mi\">250</span><span class=\"p\">,</span>\n                           <span class=\"n\">random_state</span><span class=\"o\">=</span><span class=\"mi\">42</span><span class=\"p\">)</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn</span> <span class=\"kn\">import</span> <span class=\"n\">linear_model</span><span class=\"p\">,</span> <span class=\"n\">decomposition</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.pipeline</span> <span class=\"kn\">import</span> <span class=\"n\">Pipeline</span>\n<span class=\"kn\">from</span> <span class=\"nn\">dklearn.pipeline</span> <span class=\"kn\">import</span> <span class=\"n\">Pipeline</span>\n\n<span class=\"n\">logistic</span> <span class=\"o\">=</span> <span class=\"n\">linear_model</span><span class=\"p\">.</span><span class=\"n\">LogisticRegression</span><span class=\"p\">()</span>\n<span class=\"n\">pca</span> <span class=\"o\">=</span> <span class=\"n\">decomposition</span><span class=\"p\">.</span><span class=\"n\">PCA</span><span class=\"p\">()</span>\n<span class=\"n\">pipe</span> <span class=\"o\">=</span> <span class=\"n\">Pipeline</span><span class=\"p\">(</span><span class=\"n\">steps</span><span class=\"o\">=</span><span class=\"p\">[(</span><span class=\"s\">'pca'</span><span class=\"p\">,</span> <span class=\"n\">pca</span><span class=\"p\">),</span>\n                       <span class=\"p\">(</span><span class=\"s\">'logistic'</span><span class=\"p\">,</span> <span class=\"n\">logistic</span><span class=\"p\">)])</span>\n\n\n<span class=\"c1\">#Parameters of pipelines can be set using ‘__’ separated parameter names:\n</span><span class=\"n\">grid</span> <span class=\"o\">=</span> <span class=\"nb\">dict</span><span class=\"p\">(</span><span class=\"n\">pca__n_components</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">50</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"mi\">150</span><span class=\"p\">,</span> <span class=\"mi\">250</span><span class=\"p\">],</span>\n            <span class=\"n\">logistic__C</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mf\">1e-4</span><span class=\"p\">,</span> <span class=\"mf\">1.0</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mf\">1e4</span><span class=\"p\">],</span>\n            <span class=\"n\">logistic__penalty</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">'l1'</span><span class=\"p\">,</span> <span class=\"s\">'l2'</span><span class=\"p\">])</span>\n\n<span class=\"c1\"># from sklearn.grid_search import GridSearchCV\n</span><span class=\"kn\">from</span> <span class=\"nn\">dklearn.grid_search</span> <span class=\"kn\">import</span> <span class=\"n\">GridSearchCV</span>\n\n<span class=\"n\">estimator</span> <span class=\"o\">=</span> <span class=\"n\">GridSearchCV</span><span class=\"p\">(</span><span class=\"n\">pipe</span><span class=\"p\">,</span> <span class=\"n\">grid</span><span class=\"p\">)</span>\n\n<span class=\"n\">estimator</span><span class=\"p\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>SKLearn performs this computation in around 40 seconds while the dask-learn\ndrop-in replacements take around 10 seconds.  Also, if you add the following\nlines to connect to a <a href=\"http://distributed.readthedocs.io/en/latest/quickstart.html\">running\ncluster</a> the whole\nthing scales out:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">dask.distributed</span> <span class=\"kn\">import</span> <span class=\"n\">Client</span>\n<span class=\"n\">c</span> <span class=\"o\">=</span> <span class=\"n\">Client</span><span class=\"p\">(</span><span class=\"s\">'scheduler-address:8786'</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>Here is a live <a href=\"http://bokeh.pydata.org/en/latest/\">Bokeh</a> plot of the\ncomputation on a tiny eight process “cluster” running on my own laptop.  I’m\nusing processes here to highlight the costs of communication between processes\n(red).  It’s actually about 30% faster to run this computation within the same\nsingle process.</p>\n\n<iframe src=\"https://cdn.rawgit.com/mrocklin/a2a42d71d0dd085753277821e24925a4/raw/e29b24bc656ea619eedfaba9ef176d5f3c19a040/dask-learn-task-stream.html\" width=\"800\" height=\"400\"></iframe>\n\n<h2 id=\"conclusion\">Conclusion</h2>\n\n<p>This post showed a couple of simple mechanisms for scikit-learn users to\naccelerate their existing workflows with Dask.  These aren’t particularly\nsophisticated, nor are they performance-optimal, but they are easy to\nunderstand and easy to try out.  In a future blogpost I plan to cover more\ncomplex ways in which Dask can accelerate sophisticated machine learning\nworkflows.</p>\n\n<h2 id=\"what-we-could-have-done-better\">What we could have done better</h2>\n\n<p>As always, I include a brief section on what went wrong or what we could have\ndone better with more time.</p>\n\n<ul>\n  <li>See the bottom of <a href=\"http://jcrist.github.io/dask-sklearn-part-1.html\">Jim’s post</a>\nfor a more thorough explanation of “what we could have done better” for\ndask-learn’s pipeline and gridsearch</li>\n  <li>Joblib + Dask.distributed interaction is convenient, but leaves some\nperformance on the table.  It’s not clear how Dask can help the sklearn\ncodebase without being too invasive.</li>\n  <li>It would have been nice to spin up an actual cluster on parallel hardware\nfor this post.  I wrote this quickly (in a few hours) so decided to skip\nthis.  If anyone wants to write a follow-on experiment I would be happy\nto publish it.</li>\n</ul>"
}