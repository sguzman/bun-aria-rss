{
  "id": "tag:blogger.com,1999:blog-6141980.post-144723434200318029",
  "published": "2020-12-21T16:56:00.003-06:00",
  "updated": "2020-12-21T16:56:40.370-06:00",
  "category": [
    "",
    "",
    ""
  ],
  "title": "Hardware Beyond Backpropagation: a Photonic Co-Processor for Direct Feedback Alignment",
  "content": "**&nbsp;<a href=\"https://nuit-blanche.blogspot.com/\">Nuit Blanche</a> is now on Twitter: <a href=\"https://twitter.com/NuitBlog\">@NuitBlog</a>&nbsp;**&nbsp;<div><br /></div><div style=\"text-align: justify;\">We presented this work at the <a href=\"https://beyondbackprop.github.io/\" target=\"_blank\">Beyond Backpropagation workshop at NeurIPS</a>. A great conjunction between computational hardware and algorithm!&nbsp;</div><div><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://1.bp.blogspot.com/-VNFtkemWiNw/X-EnJl7e2iI/AAAAAAAAXsg/d6cMTAQEPfQglkKwU7k0zq36_ItFc2xpACLcBGAsYHQ/s1014/greatconjunction2020.png\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"451\" data-original-width=\"1014\" height=\"178\" src=\"https://1.bp.blogspot.com/-VNFtkemWiNw/X-EnJl7e2iI/AAAAAAAAXsg/d6cMTAQEPfQglkKwU7k0zq36_ItFc2xpACLcBGAsYHQ/w400-h178/greatconjunction2020.png\" width=\"400\" /></a></div><br /><div><br /></div><div style=\"text-align: justify;\"><a href=\"https://arxiv.org/pdf/2012.06373.pdf\" target=\"_blank\">Hardware Beyond Backpropagation: a Photonic Co-Processor for Direct Feedback Alignment</a>&nbsp;by&nbsp;<a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Launay%2C+J\">Julien Launay</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Poli%2C+I\">Iacopo Poli</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=M%C3%BCller%2C+K\">Kilian Müller</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Pariente%2C+G\">Gustave Pariente</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Carron%2C+I\">Igor Carron</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Daudet%2C+L\">Laurent Daudet</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Krzakala%2C+F\">Florent Krzakala</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Gigan%2C+S\">Sylvain Gigan</a></div><div style=\"text-align: justify;\"><br /></div><div style=\"text-align: justify;\"></div><blockquote><div style=\"text-align: justify;\">The scaling hypothesis motivates the expansion of models past trillions of parameters as a path towards better performance. Recent significant developments, such as GPT-3, have been driven by this conjecture. However, as models scale-up, training them efficiently with backpropagation becomes difficult. Because model, pipeline, and data parallelism distribute parameters and gradients over compute nodes, communication is challenging to orchestrate: this is a bottleneck to further scaling. In this work, we argue that alternative training methods can mitigate these issues, and can inform the design of extreme-scale training hardware. Indeed, using a synaptically asymmetric method with a parallelizable backward pass, such as Direct Feedback Alignement, communication needs are drastically reduced. We present a photonic accelerator for Direct Feedback Alignment, able to compute random projections with trillions of parameters. We demonstrate our system on benchmark tasks, using both fully-connected and graph convolutional networks. Our hardware is the first architecture-agnostic photonic co-processor for training neural networks. This is a significant step towards building scalable hardware, able to go beyond backpropagation, and opening new avenues for deep learning.</div><div style=\"text-align: justify;\"></div></blockquote><div style=\"text-align: justify;\"><br /></div><div style=\"text-align: justify;\"><br /></div><div><div style=\"text-align: justify;\"><br /></div><div style=\"text-align: justify;\">Follow <a href=\"https://twitter.com/NuitBlog\">@NuitBlog</a>&nbsp;or join the <a href=\"http://www.reddit.com/r/CompressiveSensing/\">CompressiveSensing Reddit</a>,&nbsp;the <a href=\"https://www.facebook.com/pages/Nuit-Blanche/166441866740790\">Facebook page</a>, the Compressive Sensing group on&nbsp;<a href=\"http://www.linkedin.com/groups?gid=683737&amp;trk=myg_ugrp_ovr\">LinkedIn</a><a href=\"http://www.linkedin.com/groups?gid=683737&amp;trk=myg_ugrp_ovr\">&nbsp;</a>&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;<a href=\"http://www.linkedin.com/groups?gid=4084620&amp;trk=myg_ugrp_ovr\">LinkedIn</a></div><div style=\"text-align: justify;\"><br /></div><div style=\"text-align: justify;\"><a href=\"http://feeds.feedburner.com/blogspot/wCeDd\" rel=\"alternate\" title=\"Subscribe to my feed\" type=\"application/rss+xml\"><img alt=\"\" src=\"http://www.feedburner.com/fb/images/pub/feed-icon32x32.png\" style=\"border: 0px;\" /></a><a href=\"http://feeds.feedburner.com/blogspot/wCeDd\" rel=\"alternate\" title=\"Subscribe to my feed\" type=\"application/rss+xml\">Liked this entry ? subscribe to Nuit Blanche's feed, there's more where that came from</a>.&nbsp;You can also <a href=\"http://feedburner.google.com/fb/a/mailverify?uri=blogspot/wCeDd&amp;loc=en_US\">subscribe to Nuit Blanche by Email</a>.</div><div style=\"text-align: justify;\"><br /></div><div style=\"text-align: justify;\">Other links:</div><b><div style=\"text-align: justify;\"><b><u><i>Paris Machine Learning</i></u></b>:&nbsp;<a href=\"http://www.meetup.com/Paris-Machine-learning-applications-group/\">Meetup.com</a>||<a href=\"http://nuit-blanche.blogspot.dk/p/paris-based-meetups-on-machine-learning.html\">@Archives</a>||<a href=\"https://www.linkedin.com/groups/6400776/\">LinkedIn</a>||<a href=\"https://www.facebook.com/ParisMachineLearning\">Facebook</a>|| <a href=\"https://twitter.com/ParisMLgroup\">@ParisMLGroup</a> <b><u><i>About&nbsp;<a href=\"http://www.lighton.io/\">LightOn</a></i></u></b>:&nbsp;<a href=\"http://us14.campaign-archive1.com/home/?u=701605c9443ad5e332f87331f&amp;id=85e0ce1094\">Newsletter</a> ||<a href=\"https://twitter.com/LightOnIO\">@LightOnIO</a>|| on <a href=\"https://www.linkedin.com/company/lighton/\">LinkedIn </a>|| on <a href=\"https://www.crunchbase.com/organization/lighton\">CrunchBase</a> || our <a href=\"https://medium.com/@LightOnIO/\">Blog</a></div></b><div style=\"text-align: justify;\"><u><i><b>About myself</b></i></u>:&nbsp;<a href=\"http://www.lighton.io/\">LightOn</a> || <a href=\"https://scholar.google.fr/citations?user=Cjrs0lAAAAAJ&amp;hl=fr&amp;oi=sra\">Google Scholar</a> || <a href=\"http://www.linkedin.com/in/igorcarron\">LinkedIn</a> ||<a href=\"http://www.twitter.com/igorcarron\">@IgorCarron</a> ||<a href=\"https://sites.google.com/site/igorcarron2/home\">Homepage</a>||<a href=\"https://arxiv.org/search/?query=igor+carron&amp;searchtype=all\">ArXiv</a></div></div></div>",
  "link": [
    "",
    "",
    "",
    "",
    ""
  ],
  "author": {
    "name": "Igor",
    "uri": "http://www.blogger.com/profile/17474880327699002140",
    "email": "noreply@blogger.com",
    "gd:image": ""
  },
  "media:thumbnail": "",
  "thr:total": 0
}