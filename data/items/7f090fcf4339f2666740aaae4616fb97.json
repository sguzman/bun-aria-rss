{
  "title": "Self-Adaptive Driving in Nonstationary Environments through Conjectural Online Lookahead Adaptation. (arXiv:2210.03209v2 [cs.RO] UPDATED)",
  "link": "http://arxiv.org/abs/2210.03209",
  "description": "<p>Powered by deep representation learning, reinforcement learning (RL) provides\nan end-to-end learning framework capable of solving self-driving (SD) tasks\nwithout manual designs. However, time-varying nonstationary environments cause\nproficient but specialized RL policies to fail at execution time. For example,\nan RL-based SD policy trained under sunny days does not generalize well to\nrainy weather. Even though meta learning enables the RL agent to adapt to new\ntasks/environments, its offline operation fails to equip the agent with online\nadaptation ability when facing nonstationary environments. This work proposes\nan online meta reinforcement learning algorithm based on the \\emph{conjectural\nonline lookahead adaptation} (COLA). COLA determines the online adaptation at\nevery step by maximizing the agent's conjecture of the future performance in a\nlookahead horizon. Experimental results demonstrate that under dynamically\nchanging weather and lighting conditions, the COLA-based self-adaptive driving\noutperforms the baseline policies in terms of online adaptability. A demo\nvideo, source code, and appendixes are available at {\\tt\nhttps://github.com/Panshark/COLA}\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_H/0/1/0/all/0/1\">Haozhe Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Quanyan Zhu</a>"
}