{
  "title": "World Models Experiments",
  "link": "https://blog.otoro.net/2018/06/09/world-models-experiments/",
  "guid": "https://blog.otoro.net/2018/06/09/world-models-experiments/",
  "description": "<center>\n<video class=\"b-lazy\" autoplay=\"\" muted=\"\" playsinline=\"\" loop=\"\" style=\"display: block; margin: auto; width: 100%;\"><source src=\"/assets/20180609/worldmodels_experiments_small.mp4\" type=\"video/mp4\" /></video>\n<br />\n<code>\n<a href=\"https://github.com/hardmaru/WorldModelsExperiments\">GitHub</a>\n</code>\n</center>\n<p><br /></p>\n\n<p>In this article I will give step-by-step instructions for reproducing the experiments in the <a href=\"https://worldmodels.github.io\">World Models</a> article (<a href=\"https://arxiv.org/abs/1803.10122\">pdf</a>). The reference TensorFlow implementation is on <a href=\"https://github.com/hardmaru/WorldModelsExperiments\">GitHub</a>.</p>\n\n<p>Other people have implemented World Models independently. There is an implementation in <a href=\"https://medium.com/applied-data-science/how-to-build-your-own-world-model-using-python-and-keras-64fb388ba459\">Keras</a> that reproduces part of the CarRacing-v0 experiment. There is also another project in <a href=\"https://dylandjian.github.io/world-models/\">PyTorch</a> that attempts to apply this model on <a href=\"https://blog.openai.com/retro-contest/\">OpenAI Retro Sonic</a> environments.</p>\n\n<p>For general discussion about the World Models article, there are already some good discussion threads here in the GitHub <a href=\"https://github.com/worldmodels/worldmodels.github.io/issues\">issues</a> page of the interactive article. If you have any issues specific to the code, please don’t hessitate to raise an <a href=\"https://github.com/hardmaru/WorldModelsExperiments/issues\">issue</a> to discuss.</p>\n\n<h1 id=\"pre-requisite-reading\">Pre-requisite reading</h1>\n\n<p>I recommend reading the following articles to gain some background knowledge before attempting to reproduce the experiments.</p>\n\n<p><a href=\"https://worldmodels.github.io/\">World Models</a> (<a href=\"https://arxiv.org/abs/1803.10122\">pdf</a>)</p>\n\n<p><a href=\"https://blog.otoro.net/2017/10/29/visual-evolution-strategies/\">A Visual Guide to Evolution Strategies</a></p>\n\n<p><a href=\"https://blog.otoro.net/2017/11/12/evolving-stable-strategies/\">Evolving Stable Strategies</a></p>\n\n<h3 id=\"below-is-optional\"><em>Below is optional</em></h3>\n\n<p><a href=\"https://blog.otoro.net/2015/06/14/mixture-density-networks/\">Mixture Density Networks</a></p>\n\n<p><a href=\"https://blog.otoro.net/2015/11/24/mixture-density-networks-with-tensorflow/\">Mixture Density Networks with TensorFlow</a></p>\n\n<p>Read tutorials on Variational Autoencoders if you are not familiar with them. Some Examples:</p>\n\n<p><a href=\"https://jmetzen.github.io/2015-11-27/vae.html\">Variational Autoencoder in TensorFlow</a></p>\n\n<p><a href=\"https://blog.keras.io/building-autoencoders-in-keras.html\">Building Autoencoders in Keras</a></p>\n\n<p><a href=\"https://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors/\">Generating Large Images from Latent Vectors</a>.</p>\n\n<p>Be familiar with RNNs for continuous sequence generation:</p>\n\n<p><a href=\"https://arxiv.org/abs/1308.0850\">Generating Sequences With Recurrent Neural Networks</a></p>\n\n<p><a href=\"https://arxiv.org/abs/1704.03477\">A Neural Representation of Sketch Drawings</a></p>\n\n<p><a href=\"https://blog.otoro.net/2015/12/12/handwriting-generation-demo-in-tensorflow/\">Handwriting Generation Demo in TensorFlow</a></p>\n\n<p><a href=\"https://blog.otoro.net/2017/01/01/recurrent-neural-network-artist/\">Recurrent Neural Network Tutorial for Artists</a>.</p>\n\n<h1 id=\"software-settings\">Software Settings</h1>\n\n<p>I have tested the code with the following settings:</p>\n\n<ul>\n  <li>Ubuntu 16.04</li>\n  <li>Python 3.5.4</li>\n  <li>TensorFlow 1.8.0</li>\n  <li>NumPy 1.13.3</li>\n  <li><a href=\"https://github.com/ppaquette/gym-doom\">VizDoom Gym Levels</a> <code>(Latest commit 60ff576 on Mar 18, 2017)</code></li>\n  <li>OpenAI Gym 0.9.4 (<strong>Note: Gym 1.0+ breaks this experiment. Only tested for 0.9.x</strong>)</li>\n  <li>cma 2.2.0</li>\n  <li>mpi4py 2, see <a href=\"https://github.com/hardmaru/estool\">estool</a>, which we have forked for this project.</li>\n  <li>Jupyter Notebook for model testing, and tracking progress.</li>\n</ul>\n\n<p>I use a combination of OS X for inference, but trained models using Google Cloud VMs. I trained the V and M models on a P100 GPU instance, but trained the controller C on pure CPU instance with 64 cpu-cores (<a href=\"https://cloud.google.com/compute/pricing\">n1-standard-64</a>) using CMA-ES. I will outline which part of the training requires GPUs and which parts use only CPUs, and try to keep your costs low for running this experiment.</p>\n\n<h1 id=\"instructions-for-running-pre-trained-models\">Instructions for running pre-trained models</h1>\n\n<p>You only need to clone the repo into your desktop computer running in CPU-mode to reproduce the results with pre-trained models provided in the repo. No Clould VM or GPUs necessary.</p>\n\n<h2 id=\"carracing-v0\"><a href=\"https://gym.openai.com/envs/CarRacing-v0/\">CarRacing-v0</a></h2>\n\n<p>If you are using a MacBook Pro, I recommend setting the resolution to “More Space”, since the CarRacing-v0 environment renders at a larger resolution and doesn’t fit in the default screen settings.</p>\n\n<center>\n<img src=\"/assets/20180609/macbook_resolution.jpeg\" width=\"75%\" />\n</center>\n<p><br /></p>\n\n<p>In the command line, go into the <code class=\"highlighter-rouge\">carracing</code> subdirectory. Try to play the game yourself, run <code class=\"highlighter-rouge\">python env.py</code> in a terminal. You can control the car using the four arrow keys on the keyboard. Press (up, down) for accelerate/brake, and (left/right) for steering.</p>\n\n<center>\n<img src=\"/assets/20180609/carracing_human_play.png\" width=\"100%\" />\n</center>\n<p><br /></p>\n\n<p>In this environment, a new random track is generated for each run. While I can consistently get above 800 if I drive very carefully, it is hard for me to consistently get a score above 900 points. Some Stanford <a href=\"https://twitter.com/hardmaru/status/934872621077839872\">students</a> also found it tough to get consistently higher than 900. The requirement to solve this environment is to obtain an average score of 900 over 100 consecutive random trails.</p>\n\n<p>To run the pre-trained model once and see the agent in full-rendered mode, run:</p>\n\n<p><code class=\"highlighter-rouge\">python model.py render log/carracing.cma.16.64.best.json</code></p>\n\n<p>Run the pre-trained model 100 times in <code class=\"highlighter-rouge\">no-render</code> mode (in <code class=\"highlighter-rouge\">no-render</code> mode, it still renders something simpler on the screen due to the need to use OpenGL for this environment to extract the pixel information as observations):</p>\n\n<p><code class=\"highlighter-rouge\">python model.py norender log/carracing.cma.16.64.best.json</code></p>\n\n<p>This command will output the score for each 100 trials, and after running 100 times. It will also output the average score and standard deviation. The average score should be above 900.</p>\n\n<p>To run the pre-trained controller inside of an environment generated using M and visualized using V:</p>\n\n<p><code class=\"highlighter-rouge\">python dream_model.py log/carracing.cma.16.64.best.json</code></p>\n\n<center>\n<img src=\"/assets/20180609/carracing_dream.png\" width=\"50%\" />\n</center>\n\n<h2 id=\"doomtakecover-v0\"><a href=\"https://gym.openai.com/envs/DoomTakeCover-v0/\">DoomTakeCover-v0</a></h2>\n\n<p>In the <code class=\"highlighter-rouge\">doomrnn</code> directory, run <code class=\"highlighter-rouge\">python doomrnn.py</code> to play inside of an environment generated by M.</p>\n\n<center>\n<img src=\"/assets/20180609/doomrnn_dream_env.png\" width=\"50%\" />\n</center>\n\n<p>You can hit left, down, or right to play inside of this envrionment. To visualize the pre-trained model playing inside of the real environment, run:</p>\n\n<p><code class=\"highlighter-rouge\">python model.py doomreal render log/doomrnn.cma.16.64.best.json</code></p>\n\n<center>\n<img src=\"/assets/20180609/doomrnn_actual.png\" width=\"100%\" />\n</center>\n\n<p>Note that this environment is modified to also display the cropped 64x64px frames, in addition to the reconstructed frames and actual frames of the game. To run model inside the actual environment 100 times and compute the mean score, run:</p>\n\n<p><code class=\"highlighter-rouge\">python model.py doomreal norender log/doomrnn.cma.16.64.best.json</code></p>\n\n<p>You should get a mean score of over 900 time-steps over 100 random episodes. The above two lines still work if you substitute <code class=\"highlighter-rouge\">doomreal</code> with <code class=\"highlighter-rouge\">doomrnn</code> if you want to get the statistics of the agent playing inside of the generated environment. If you wish to change the temperature of the generated environment, modify the constant <code class=\"highlighter-rouge\">TEMPERATURE</code> inside <code class=\"highlighter-rouge\">doomrnn.py</code>, which is currently set to 1.25.</p>\n\n<p>To visualie the model playing inside of the generated environment, run:</p>\n\n<p><code class=\"highlighter-rouge\">python model.py doomrnn render log/doomrnn.cma.16.64.best.json</code></p>\n\n<center>\n<img src=\"/assets/20180609/doomrnn_dream_agent.png\" width=\"50%\" />\n</center>\n\n<h1 id=\"instructions-for-training-everything-from-scratch\">Instructions for training everything from scratch</h1>\n\n<p><em>The <code class=\"highlighter-rouge\">DoomTakeCover-0</code> experiment should take less than 24 hours to completely reproduce from scratch using a P100 instance and 64-core CPU instance on <a href=\"https://cloud.google.com/\">Google Cloud Platform</a>.</em></p>\n\n<h2 id=\"doomtakecover-v0-1\"><a href=\"https://gym.openai.com/envs/DoomTakeCover-v0/\">DoomTakeCover-v0</a></h2>\n\n<p>I will discuss the VizDoom experiment first since it requires less compute time to reproduce from scratch. Since you may update the models in the repo, I recommend that you fork the repo and clone/update on your fork. I recommend running any command inside of a <code class=\"highlighter-rouge\">tmux</code> session so that you can close your ssh connections and the jobs will still run on the background.</p>\n\n<p>I first create a 64-core CPU instance with ~ 200GB storage and 220GB RAM, and clone the repo in that instance. In the <code class=\"highlighter-rouge\">doomrnn</code> directory, there is a script called <code class=\"highlighter-rouge\">extract.py</code> that will extract 200 episodes from a random poilcy, and save the episodes as <code class=\"highlighter-rouge\">.npz</code> files in <code class=\"highlighter-rouge\">doomrnn/record</code>. A bash script called <code class=\"highlighter-rouge\">extract.bash</code> will run <code class=\"highlighter-rouge\">extract.py</code> 64 times (~ one job per CPU core), so by running <code class=\"highlighter-rouge\">bash extract.bash</code>, we will generate 12,800 <code class=\"highlighter-rouge\">.npz</code> files in <code class=\"highlighter-rouge\">doomrnn/record</code>. Some instances might randomly fail, so we generate a bit of extra data, although in the end we only use 10,000 episodes for training V and M. This process will take a few hours (probably less than 5 hours).</p>\n\n<p>After the <code class=\"highlighter-rouge\">.npz</code> files have been created in the <code class=\"highlighter-rouge\">record</code> subdirectory, I create a P100 GPU instance with ~ 200GB storage and 220GB RAM, and clone the repo there too. I use the ssh copy command, <code class=\"highlighter-rouge\">scp</code>, to copy all of the <code class=\"highlighter-rouge\">.npz</code> files from the CPU instance to the GPU instance, into the same <code class=\"highlighter-rouge\">record</code> subdirectory. You can use the <code class=\"highlighter-rouge\">gcloud</code> tool if <code class=\"highlighter-rouge\">scp</code> doesn’t work. This should be really fast, like less than a minute, if both instances are in the same region. Shut down the CPU instance after you have copied the <code class=\"highlighter-rouge\">.npz</code> files over to the GPU machine.</p>\n\n<p>On the GPU machine, run the command <code class=\"highlighter-rouge\">bash gpu_jobs.bash</code> to train the VAE, pre-process the recorded dataset, and train the MDN-RNN.</p>\n\n<p>This <code class=\"highlighter-rouge\">gpu_jobs.bash</code> will run 3 things in sequential order:</p>\n\n<p>1) <code class=\"highlighter-rouge\">python vae_train.py</code> - which will train the VAE, and after training, the model will be saved in <code class=\"highlighter-rouge\">tf_vae/vae.json</code></p>\n\n<p>2) Next, it will pre-process collected data using pre-trained VAE by launching: <code class=\"highlighter-rouge\">python series.py</code>. A new dataset will be created in a subdirectory called <code class=\"highlighter-rouge\">series</code>.</p>\n\n<p>3) After this a <code class=\"highlighter-rouge\">series.npz</code> dataset is saved there, the script will launch the MDN-RNN trainer using this command: <code class=\"highlighter-rouge\">python rnn_train.py</code>. This will produce a model in <code class=\"highlighter-rouge\">tf_rnn/rnn.json</code> and also <code class=\"highlighter-rouge\">tf_initial_z/initial_z.json</code>. The file <code class=\"highlighter-rouge\">initial_z.json</code> saves the initial latent variables (z) of an episode which is needed when we need to generate the environment. This entire process might take 6-8 hours.</p>\n\n<center>\n<img src=\"/assets/20180609/doom_vae_test.png\" width=\"50%\" /><br />\n<i>The notebook <code>vae_test.ipynb</code> will visualize input/reconstruction images using your VAE on the training dataset.</i>\n</center>\n<p><br /></p>\n\n<p>After V and M are trained, and you have the 3 new <code class=\"highlighter-rouge\">json</code> files, you must must now copy <code class=\"highlighter-rouge\">vae.json</code>, <code class=\"highlighter-rouge\">initial_z.json</code> and <code class=\"highlighter-rouge\">rnn.json</code> over to <code class=\"highlighter-rouge\">tf_models</code> subdirectory and overwrite previous files that might be there. You should update your git repo with these new models using <code class=\"highlighter-rouge\">git add doomrnn/tf_models/*.json</code> and committing the change to your fork. After you have done this, you can shutdown the GPU machine. You need to start the 64-core CPU instance again, log back into that machine.</p>\n\n<p>Now on a 64-core CPU instance, run the CMA-ES based training by launching the command: <code class=\"highlighter-rouge\">python train.py</code> inside the <code class=\"highlighter-rouge\">doomrnn</code> directory. This will launch the evolution trainer and continue training until you <code class=\"highlighter-rouge\">Ctrl-C</code> this job. The controller C will be trained inside of M’s generated environment with a temperature of 1.25. You can monitor progress using the <code class=\"highlighter-rouge\">plot_training_progress.ipynb</code> notebook which loads the <code class=\"highlighter-rouge\">log</code> files being generated. After 200 generations (or around 4-5 hours), it should be enough to get decent results, and you can stop this job. I left my job running for close to 1800 generations, although it doesn’t really add much value after 200 generations, so I prefer not to waste your money. Add all of the files inside <code class=\"highlighter-rouge\">log/*.json</code> into your forked repo and then shutdown the instance.</p>\n\n<center>\n<img src=\"/assets/20180609/doomrnn.cma.16.64.wall.svg\" width=\"100%\" /><br />\n<img src=\"/assets/20180609/doomrnn.cma.16.64.svg\" width=\"100%\" />\n<i>Training DoomRNN using CMA-ES. Recording C's performance inside of the generated environment.</i>\n</center>\n<p><br /></p>\n\n<p>Using your desktop instance, and pulling your forked repo again, you can now run the following to test your newly trained V, M, and C models.</p>\n\n<p><code class=\"highlighter-rouge\">python model.py doomreal render log/doomrnn.cma.16.64.best.json</code></p>\n\n<p>You can replace <code class=\"highlighter-rouge\">doomreal</code> with <code class=\"highlighter-rouge\">doomrnn</code> or <code class=\"highlighter-rouge\">render</code> to <code class=\"highlighter-rouge\">norender</code> to try on the generated environment, or trying your agent 100 times.</p>\n\n<h2 id=\"carracing-v0-1\"><a href=\"https://gym.openai.com/envs/CarRacing-v0/\">CarRacing-v0</a></h2>\n\n<p>The process for CarRacing-v0 is almost the same as the VizDoom example earlier, so I will discuss the differences in this section.</p>\n\n<p>Since this environment is built using OpenGL, it relies on a graphics output even in <code class=\"highlighter-rouge\">no-render</code> mode of the gym environment, so in a CloudVM box, I had to wrap the command with a headless X server. You can see that inside the <code class=\"highlighter-rouge\">extract.bash</code> file in <code class=\"highlighter-rouge\">carracing</code> directory, I run <code class=\"highlighter-rouge\">xvfb-run -a -s \"-screen 0 1400x900x24 +extension RANDR\"</code> before the real command. Other than this, the procedure to collect data, and training the V and M model are the same as VizDoom.</p>\n\n<p>Please note that after you train your VAE and MDN-RNN models, you must now copy <code class=\"highlighter-rouge\">vae.json</code>, <code class=\"highlighter-rouge\">initial_z.json</code> and <code class=\"highlighter-rouge\">rnn.json</code> over to <code class=\"highlighter-rouge\">vae</code>, <code class=\"highlighter-rouge\">initial_z</code>, and <code class=\"highlighter-rouge\">rnn</code> directories respectively (not <code class=\"highlighter-rouge\">tf_models</code> like in DoomRNN), and overwrite previous files if they were there, and then update the forked repo as usual.</p>\n\n<center>\n<img src=\"/assets/20180609/car_vae_test.png\" width=\"50%\" /><br />\n<i><code>vae_test.ipynb</code> used to examine the VAE trained on <code>CarRacing-v0</code>'s extracted data.</i>\n</center>\n<p><br /></p>\n\n<p>In this environment, we use the V and M model as model predictive control (MPC) and train the controller C on the actual environment, rather than inside of the generated environment. So rather than running <code class=\"highlighter-rouge\">python train.py</code> you need to run <code class=\"highlighter-rouge\">gce_train.bash</code> instead to use the headless X sessions to run the CMA-ES trainer. Because we train in the actual environment, training is slower compared to DoomRNN. By running the training inside a <code class=\"highlighter-rouge\">tmux</code> session, you can monitor progress using the <code class=\"highlighter-rouge\">plot_training_progress.ipynb</code> notebook by running Jupyter in another <code class=\"highlighter-rouge\">tmux</code> session in parallel, which loads the <code class=\"highlighter-rouge\">log</code> files being generated.</p>\n\n<center>\n<img src=\"/assets/20180609/carracing.cma.16.64.wall.svg\" width=\"100%\" /><br />\n<img src=\"/assets/20180609/carracing.cma.16.64.svg\" width=\"100%\" />\n<i>Training CarRacing-v0 using CMA-ES. Recording C's performance inside of the actual environment.</i>\n</center>\n<p><br /></p>\n\n<p>After 150-200 generations (or around 3 days), it should be enough to get around a mean score of ~ 880, which is pretty close to the required score of 900. If you don’t have a lot of money or credits to burn, I recommend you stop if you are satistifed with a score of 850+ (which is around a day of training). Qualitatively, a score of ~ 850-870 is not that much worse compared to our final agent that achieves 900+, and I don’t want to burn your hard-earned money on cloud credits. To get 900+ it might take weeks (who said getting SOTA was easy? :). The final models are saved in <code class=\"highlighter-rouge\">log/*.json</code> and you can test and view them the usual way.</p>\n\n<h1 id=\"contributing\">Contributing</h1>\n\n<p>There are many cool ideas to try out – For instance, iterative training methods, transfer learning, intrinsic motivation, other environments.</p>\n\n<center>\n<video autoplay=\"\" muted=\"\" playsinline=\"\" loop=\"\" style=\"display: block; margin: auto; width: 80%;\"><source src=\"/assets/20180609/generative_pixel_pendulum.mp4\" type=\"video/mp4\" /></video>\n<i>A generative noisy pixel pendulum environment?</i>\n</center>\n<p><br /></p>\n\n<p>If you want to extend the code and try out new things, I recommend modifying the code and trying it out to solve a specific new environment, and not try to improve the code to work for multiple environments at the same time. I find that for research work, and when trying to solve difficult environments, specific custom modifications are usually required. You are welcome to submit a pull request with a self-contained subdirectory that is tailored for a specific challenging environment that you had attempted to solve, with instructions in a <code class=\"highlighter-rouge\">README.md</code> file in your subdirectory.</p>\n\n<h1 id=\"citation\">Citation</h1>\n\n<p>If you found this code useful in an academic setting, please cite:</p>\n\n<p><code>\n@incollection{ha2018worldmodels,<br />\n&nbsp;&nbsp;title = {Recurrent World Models Facilitate Policy Evolution},<br />\n&nbsp;&nbsp;author = {Ha, David and Schmidhuber, J{\\\"u}rgen},<br />\n&nbsp;&nbsp;booktitle = {Advances in Neural Information Processing Systems 31},<br />\n&nbsp;&nbsp;pages = {2451--2463},<br />\n&nbsp;&nbsp;year = {2018},<br />\n&nbsp;&nbsp;publisher = {Curran Associates, Inc.},<br />\n&nbsp;&nbsp;url = {https://papers.nips.cc/paper/7512-recurrent-world-models-facilitate-policy-evolution},<br />\n&nbsp;&nbsp;note = \"\\url{https://worldmodels.github.io}\",<br />\n}<br />\n</code></p>",
  "pubDate": "Sat, 09 Jun 2018 00:00:00 -0500"
}