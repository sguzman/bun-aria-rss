{
  "title": "XAI-Increment: A Novel Approach Leveraging LIME Explanations for Improved Incremental Learning. (arXiv:2211.01413v1 [cs.LG])",
  "link": "http://arxiv.org/abs/2211.01413",
  "description": "<p>Explainability of neural network prediction is essential to understand\nfeature importance and gain interpretable insight into neural network\nperformance. In this work, model explanations are fed back to the feed-forward\ntraining to help the model generalize better. To this extent, a custom weighted\nloss where the weights are generated by considering the Euclidean distances\nbetween true LIME (Local Interpretable Model-Agnostic Explanations)\nexplanations and model-predicted LIME explanations is proposed. Also, in\npractical training scenarios, developing a solution that can help the model\nlearn sequentially without losing information on previous data distribution is\nimperative due to the unavailability of all the training data at once. Thus,\nthe framework known as XAI-Increment incorporates the custom weighted loss\ndeveloped with elastic weight consolidation (EWC), to maintain performance in\nsequential testing sets. Finally, the training procedure involving the custom\nweighted loss shows around 1% accuracy improvement compared to the traditional\nloss based training for the keyword spotting task on the Google Speech Commands\ndataset and also shows low loss of information when coupled with EWC in the\nincremental learning setup.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Mazumder_A/0/1/0/all/0/1\">Arnab Neelim Mazumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyons_N/0/1/0/all/0/1\">Niall Lyons</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubey_A/0/1/0/all/0/1\">Anand Dubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_A/0/1/0/all/0/1\">Ashutosh Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santra_A/0/1/0/all/0/1\">Avik Santra</a>"
}