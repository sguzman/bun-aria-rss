{
  "title": "Data Science Without Leaving the GPU",
  "description": "<p><em>Edit 10/1/2018: When I wrote this blog post, the company and product were named MapD. I’ve changed the title to reflect the new company name, but left the MapD references below to hopefully avoid confusion</em></p>",
  "pubDate": "Mon, 23 Jul 2018 00:00:00 +0000",
  "link": "http://randyzwitch.com/mapd-apache-arrow-xgboost/",
  "guid": "http://randyzwitch.com/mapd-apache-arrow-xgboost/",
  "content": "<p><em>Edit 10/1/2018: When I wrote this blog post, the company and product were named MapD. I’ve changed the title to reflect the new company name, but left the MapD references below to hopefully avoid confusion</em></p>\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/9I207CIvk5Y?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe>\n\n<p>Data has been growing rapidly for some time now, but CPU-based analytics solutions haven’t been able to sustain the same rate of growth in order to keep up. CPUs in desktop and laptop machines have started adding more cores, but even a 4- or 8-core CPU can only do so much work. Eventually the bottleneck will become not having enough bandwidth to keep all the CPU cores ‘fed’ with data to manipulate. <a href=\"/big-data-hadoop-amazon-ec2-cloudera-part-1/\">Hadoop</a> provides a framework for working with larger datasets, but its distributed nature can often feel like setting it up is more hassle than its worth.</p>\n\n<p>GPU-based analytics solutions provide a great middle-ground; high-parallelism via thousands of GPU cores, while not having to automatically use a networked, multi-node architecture such as Hadoop. A single <a href=\"/building-data-science-workstation-2017/\">data science workstation</a> with 2-4  GPUs can reasonably handle hundreds of millions of records, especially when using the <a href=\"https://www.omnisci.com/blog/scaling-pandas-to-the-billions-with-ibis-and-mapd/\">Ibis backend for MapD</a>.</p>\n\n<p>In this webinar, I demonstrate how to do each step of a machine learning workflow, from exploring a dataset to adding features to estimating an xgboost model for predicting the amount of tip a user will give after a taxi ride. Because MapD incorporates Apache Arrow under the hood for its data transfer, this can all be done seamlessly by passing pointers, rather than needing expensive I/O operations, between each tool used. Not having to transfer the data off of the GPU has interesting implications for analytics, which I also discuss towards the end of the talk.</p>\n\n<p>Enjoy!</p>"
}