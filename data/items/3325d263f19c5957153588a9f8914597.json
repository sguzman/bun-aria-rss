{
  "title": "Stochastic Computation Graphs: Fixing REINFORCE",
  "link": "http://artem.sobolev.name/posts/2017-11-12-stochastic-computation-graphs-fixing-reinforce.html",
  "description": "<p>This is the final post of the <a href=\"/tags/stochastic%20computation%20graphs%20series.html\">stochastic computation graphs series</a>. Last time we discussed models with <a href=\"/posts/2017-10-28-stochastic-computation-graphs-discrete-relaxations.html\">discrete relaxations of stochastic nodes</a>, which allowed us to employ the power of reparametrization.</p>\n<p>These methods, however, posses one flaw: they consider different models, thus introducing inherent bias – your test time discrete model will be doing something different from what your training time model did. Therefore in this post we’ll get back to the REINFORCE aka Score Function estimator, and see if we can fix its problems.</p>\n<!--more-->\n<h2 id=\"back-to-reinforce\">Back to REINFORCE</h2>\n<p>REINFORCE<a href=\"#fn1\" class=\"footnoteRef\" id=\"fnref1\"><sup>1</sup></a> estimator arises from the following identity:</p>\n<p><span class=\"math display\">\\[\n\\begin{align*}\n\\nabla_\\theta \\mathcal{F}(\\theta)\n& = \\nabla_\\theta \\mathbb{E}_{p(z|\\theta)} f(z)\n = \\nabla_\\theta \\int f(z) p(z|\\theta) dz\n = \\int f(z) \\nabla_\\theta p(z|\\theta) dz \\\\\n&= \\int f(z) \\nabla_\\theta \\log p(z|\\theta) p(z|\\theta) dz\n = \\mathbb{E}_{p(z|\\theta)} f(z) \\nabla_\\theta \\log p(z|\\theta)\n\\end{align*}\n\\]</span></p>\n<p>This allows us to estimate the gradient of the expected objective using Monte Carlo estimation:</p>\n<p><span class=\"math display\">\\[\n\\hat{\\nabla}_\\theta^{\\text{SF}} \\mathcal{F} = \\frac{1}{L} \\sum_{l=1}^L f(z^{(l)}) \\nabla_\\theta \\log p(z^{(l)}|\\theta)\n, \\quad \\text{ where }\nz^{(l)} \\sim p(z|\\theta)\n\\]</span></p>\n<p>The downside of this method is that it does not use the gradient information of the objective <span class=\"math inline\">\\(f\\)</span>. This is useful in cases where we don’t have access to such information, for example, in Reinforcement Learning. However, when working with Stochastic Computation Graphs, we usually do have gradients <span class=\"math inline\">\\(\\nabla_z f(z)\\)</span> available, and I believe methods that intelligently use this gradient should perform better.</p>\n<p>However, the score function estimator does not use this information, yet it’s an unbiased estimator of the true gradient. What’s the problem then? The problem is in impractically high variance that requires one to obtain some astronomical amount of samples to reduce the variance and make optimization actually feasible <a href=\"#fn2\" class=\"footnoteRef\" id=\"fnref2\"><sup>2</sup></a>. Recall the intuition behind this from <a href=\"/posts/2017-09-10-stochastic-computation-graphs-continuous-case.html\">the first post</a>: a REINFORCE estimator <span class=\"math inline\">\\(\\hat{\\nabla}_\\theta^{\\text{SF}} \\mathcal{F}\\)</span> is just <span class=\"math inline\">\\(L\\)</span> single-sample gradients averaged together and each single single-sample gradient <span class=\"math inline\">\\(f(z) \\nabla_\\theta \\log p(z|\\theta)\\)</span> essentially implements a random search: it wants to increase the probability of a given sample <span class=\"math inline\">\\(z\\)</span> proportionally to <span class=\"math inline\">\\(f(z)\\)</span>, and if the later is negative, then reduce it. Each of the samples then pulls the probability towards itself, and this lack of a consensus is the source of the problem.</p>\n<p>However, despite REINFORCE being essentially a random search in disguise, not all is lost yet. As we shall see, one can extend it with lots of different tricks, greatly reducing the variance.</p>\n<h2 id=\"control-variates\">Control Variates</h2>\n<p>One method for reducing the variance in statistics (and the major one for this post) is the method of <strong>Control Variates</strong>, that’s based on the idea that if you have two negatively correlated random variables, their sum could have lower variance. Indeed, let’s assume we have random variables <span class=\"math inline\">\\(X\\)</span> and <span class=\"math inline\">\\(Y\\)</span> such that <span class=\"math inline\">\\(\\mathbb{D}(X) = \\sigma^2_x\\)</span>, <span class=\"math inline\">\\(\\mathbb{D}(Y) = \\sigma^2_y\\)</span> and <span class=\"math inline\">\\(\\text{Cov}(X, Y) = -\\tau \\sigma_x \\sigma_y\\)</span>. Then</p>\n<p><span class=\"math display\">\\[\n\\mathbb{D}(X + Y) - \\mathbb{D}(X)\n= \\mathbb{D}(Y) + 2 \\text{Cov}(X, Y)\n= \\sigma^2_y - 2 \\tau \\sigma_x \\sigma_y\n= \\sigma_y (\\sigma_y - 2 \\tau \\sigma_x)\n\\]</span></p>\n<p>So if <span class=\"math inline\">\\(\\sigma_y < 2 \\tau \\sigma_x\\)</span>, then the sum <span class=\"math inline\">\\(X + Y\\)</span> will have lower variance than the <span class=\"math inline\">\\(X\\)</span> alone. Of course, <span class=\"math inline\">\\(Y\\)</span> needs to be centered <span class=\"math inline\">\\(\\mathbb{E} Y = 0\\)</span> to not bias the <span class=\"math inline\">\\(X\\)</span>, but centering does not affect the variance.</p>\n<p>We’ll be considering control variates of a special form: <span class=\"math inline\">\\(b(z) \\nabla_\\theta \\log p(z|\\theta)\\)</span> where <span class=\"math inline\">\\(b\\)</span> is a <strong>baseline</strong> and can be either a scalar of a vector (the multiplication is point-wise then)<a href=\"#fn3\" class=\"footnoteRef\" id=\"fnref3\"><sup>3</sup></a>. This leads to the estimator of the following form</p>\n<p><span class=\"math display\">\\[\n\\hat{\\nabla}_\\theta^\\text{SF} \\mathcal{F}(\\theta) = (f(z) - b(z)) \\nabla_\\theta \\log p(z|\\theta)\n\\]</span></p>\n<p>Here I used only one sample to simplify the notation (and will be doing so from now on), in practice you always can average several samples, though that probably won’t help you much <a href=\"#fn4\" class=\"footnoteRef\" id=\"fnref4\"><sup>4</sup></a>. However, by using a baseline we might have introduced unwanted bias in our gradient estimation. Let’s see:</p>\n<p><span class=\"math display\">\\[\n\\begin{align*}\n\\mathbb{E}_{p(z|\\theta)} \\hat{\\nabla}_\\theta^\\text{SF} \\mathcal{F}(\\theta)\n&= \\mathbb{E}_{p(z|\\theta)} (f(z) - b(z)) \\nabla_\\theta \\log p(z|\\theta)\n = \\nabla_\\theta \\mathbb{E}_{p(z|\\theta)} \\left[ f(z) - b(z) \\right] \\\\\n&= \\nabla_\\theta \\mathbb{E}_{p(z|\\theta)} f(z) - \\nabla_\\theta \\mathbb{E}_{p(z|\\theta)} b(z)\n\\end{align*}\n\\]</span></p>\n<p>Looks like we did indeed bias the estimator! In order to be able to reduce the variance and keep the estimator unbiased, we should remove the introduced bias from the <span class=\"math inline\">\\(\\hat{\\nabla}_\\theta^\\text{SF} \\mathcal{F}(\\theta)\\)</span>:</p>\n<p><span class=\"math display\">\\[\n\\hat{\\nabla}_\\theta^\\text{SF} \\mathcal{F}(\\theta) = (f(z) - b(z)) \\nabla_\\theta \\log p(z|\\theta) + \\nabla_\\theta \\mathbb{E}_{p(z|\\theta)} b(z)\n\\]</span></p>\n<p>This, of course, only works if you can compute the last term analytically. Estimating it with REINFORCE won’t help you, as you’d then recover the standard Score Function estimator.</p>\n<p>The easiest baseline one can think of is a constant baseline. It doesn’t introduce any bias: indeed offsetting the target <span class=\"math inline\">\\(f(z)\\)</span> should not (and does not) change the true gradient of the expectation. However, as we’ve seen in the first part of the series, it can mess with the variance. So, let’s use a baseline that would minimize the total variance of the adjusted estimator:</p>\n<p><span class=\"math display\">\\[\n\\hat{\\nabla}_\\theta^\\text{SF-const} \\mathcal{F}(\\theta) = (f(z) - b) \\nabla_\\theta \\log p(z|\\theta)\n\\]</span></p>\n<p>The total variance along all <span class=\"math inline\">\\(D\\)</span> coordinates of this gradient estimator is <span class=\"math display\">\\[\n\\begin{align*}\n\\sum_{d=1}^D &\\mathbb{D}\\left[\\hat{\\nabla}_{\\theta_d}^\\text{SF-const} \\mathcal{F}(\\theta)\\right]\n= \\sum_{d=1}^D \\mathbb{D}\\left[(f(z) - b) \\nabla_{\\theta_d} \\log p(z|\\theta)\\right] \\\\\n&= \\sum_{d=1}^D \\Bigl(\n    {\\scriptsize\n    \\mathbb{D}\\left[f(z) \\nabla_{\\theta_d} \\log p(z|\\theta)\\right]\n    - 2b \\text{Cov}\\left[f(z) \\nabla_{\\theta_d} \\log p(z|\\theta), \\nabla_{\\theta_d} \\log p(z|\\theta)\\right]\n    + b^2 \\mathbb{D}\\left[\\nabla_{\\theta_d} \\log p(z|\\theta)\\right]\n    }\n\\Bigr)\n\\end{align*}\n\\]</span></p>\n<p>The formula does look a bit terrifying, but we only care about <span class=\"math inline\">\\(b\\)</span> at the moment, and the variance is quadratic in b. The optimal value thus is obtained by minimizing the quadratic formula:</p>\n<p><span class=\"math display\">\\[\nb\n= \\frac{\\sum_{d=1}^D \\text{Cov}\\left[f(z) \\nabla_{\\theta_d} \\log p(z|\\theta), \\nabla_{\\theta_d} \\log p(z|\\theta)\\right]}{\\sum_{d=1}^D \\mathbb{D}\\left[\\nabla_{\\theta_d} \\log p(z|\\theta)\\right]}\n= \\frac{\\sum_{d=1}^D \\mathbb{E}\\left[f(z) (\\nabla_{\\theta_d} \\log p(z|\\theta))^2\\right]}{\\sum_{d=1}^D \\mathbb{E}\\left[(\\nabla_{\\theta_d} \\log p(z|\\theta))^2\\right]}\n\\]</span></p>\n<p>Where we used the fact that <span class=\"math inline\">\\(\\mathbb{E} \\nabla_{\\theta_d} \\log p(z|\\theta) = 0\\)</span> for any <span class=\"math inline\">\\(d\\)</span>. The moments in the formula can not be computed analytically, but one can estimate them using running averages.</p>\n<p>In the same fashion one can derive the optimal vector-valued baseline <span class=\"math inline\">\\(b\\)</span> (and even the matrix-valued!), consisting of individual baselines for each dimension of the gradient:</p>\n<p><span class=\"math display\">\\[\nb_d = \\frac{\\mathbb{E}\\left[f(z) (\\nabla_{\\theta_d} \\log p(z|\\theta))^2\\right]}{\\mathbb{E}\\left[(\\nabla_{\\theta_d} \\log p(z|\\theta))^2\\right]}\n\\]</span></p>\n<h2 id=\"self-critical-learning\">Self-critical Learning</h2>\n<p>Ideally, the baseline approximates <span class=\"math inline\">\\(f(z)\\)</span> as good as possible without using the actual sample <span class=\"math inline\">\\(z\\)</span> <a href=\"#fn5\" class=\"footnoteRef\" id=\"fnref5\"><sup>5</sup></a>. However, it can still depend on <span class=\"math inline\">\\(\\theta\\)</span> without introducing any bias:</p>\n<p><span class=\"math display\">\\[\n\\mathbb{E}_{p(z|\\theta)} b(\\theta) \\nabla_\\theta \\log p(z|\\theta) = \nb(\\theta) \\mathbb{E}_{p(z|\\theta)} \\nabla_\\theta \\log p(z|\\theta) = \n0\n\\]</span></p>\n<p>So, how can we use <span class=\"math inline\">\\(\\theta\\)</span> and <span class=\"math inline\">\\(f\\)</span> to approximate <span class=\"math inline\">\\(f(z)\\)</span> without touching the sample <span class=\"math inline\">\\(z\\)</span> itself? Authors of the <a href=\"https://arxiv.org/abs/1612.00563\">Self-critical Sequence Training for Image Captioning</a> paper suggested to replace the stochastic <span class=\"math inline\">\\(z\\)</span> with a deterministic most probable outcome:</p>\n<p><span class=\"math display\">\\[\n\\hat{z} = \\text{argmax}_k \\; p(z = k | \\theta)\n\\]</span></p>\n<p>And then we use <span class=\"math inline\">\\(f(\\hat z)\\)</span> as a baseline:</p>\n<p><span class=\"math display\">\\[\n\\hat{\\nabla}_\\theta^\\text{SF-SC} \\mathcal{F}(\\theta) = (f(z) - f(\\hat{z})) \\nabla_\\theta \\log p(z|\\theta)\n\\]</span></p>\n<p>This is a very interesting baseline. Unlike the standard REINFORCE, where each sample pulls probability towards itself, this estimator pulls probability in only for samples that are better than the most likely one. Conversely, for samples that are worse than the most likely one, this estimator pushes probability away. Unsurprisingly, this baseline is just a constant baseline that automatically adapts to whether a probability should be increased or decreased for a given sample <span class=\"math inline\">\\(z\\)</span>.</p>\n<h2 id=\"special-cases\">Special Cases</h2>\n<p>When <span class=\"math inline\">\\(f\\)</span> is of some special form, one can design ad hoc variance reduction techniques. In particular, we’ll consider two of them:</p>\n<h3 id=\"nvil\">NVIL</h3>\n<p>NVIL stands for <a href=\"https://arxiv.org/abs/1402.0030\">Neural Variational Inference and Learning</a> after a paper it was introduced in. Essentially, it combines tricks to reduce the variance people of Reinforcement Learning came up with to reduce the variance of REINFORCE (which they usually call the Policy Gradients method). The paper introduced three methods: <em>signal centering</em>, <em>variance normalization</em> and <em>local learning signals</em>. The <em>variance normalization</em> normalizes the gradient by a running average estimate of its standard deviation – this is what, say, the Adam optimizer would do for you automatically, so let’s not stop here.</p>\n<p><em>Signal centering</em> can be considering as a baseline amortization for a context-dependent case. Let me decypher that: oftentimes stochastic random variable <span class=\"math inline\">\\(z\\)</span> depends on some context <span class=\"math inline\">\\(x\\)</span> (for example, state of the environment in RL, or the observation <span class=\"math inline\">\\(x\\)</span> in the amortized variational inference), then the expected objective becomes <span class=\"math inline\">\\(\\mathcal{F}(\\theta|x) = \\mathbb{E}_{p(z|x,\\theta)} f(x, z)\\)</span>. Then we can make the baseline <span class=\"math inline\">\\(b\\)</span> depend on <span class=\"math inline\">\\(x\\)</span> as well without any sacrifice:</p>\n<p><span class=\"math display\">\\[\n\\hat{\\nabla}_\\theta^\\text{SF-NVIL} \\mathcal{F}(\\theta) = (f(x, z) - b(x)) \\nabla_\\theta \\log p(z|x, \\theta)\n\\]</span></p>\n<p>We could reuse the formulas from the previous section, but that’d require us to store independent baseline for each <span class=\"math inline\">\\(x\\)</span> in the trainset – doesn’t scale. Therefore instead we’ll amortize the baseline using a neural network <span class=\"math inline\">\\(b(x|\\varphi)\\)</span> with parameters <span class=\"math inline\">\\(\\varphi\\)</span> and learn it by minimizing the expected squared error <a href=\"#fn6\" class=\"footnoteRef\" id=\"fnref6\"><sup>6</sup></a> <span class=\"math display\">\\[\\varphi^* = \\text{argmin}_\\phi \\mathbb{E}_{p(z|x,\\theta)} (b(x|\\varphi) - f(x, z))^2\\]</span></p>\n<p>The <em>local learning signal</em> allows you to exploit some non-trivial structure in <span class=\"math inline\">\\(f(z)\\)</span> (and <span class=\"math inline\">\\(p(z|\\theta)\\)</span>). Namely, suppose we divided our <span class=\"math inline\">\\(z\\)</span> into <span class=\"math inline\">\\(N\\)</span> chunks: <span class=\"math inline\">\\(z = (z_1, \\dots, z_N)\\)</span>, and <span class=\"math inline\">\\(f\\)</span> is a sum of rewards on prefixes: <span class=\"math inline\">\\(f(z) = \\sum_{n=1}^N f_n(z_{<n})\\)</span> <a href=\"#fn7\" class=\"footnoteRef\" id=\"fnref7\"><sup>7</sup></a>. It’s then obvious that choice of later blocks <span class=\"math inline\">\\(z_n\\)</span> layers does not influence the prior rewards <span class=\"math inline\">\\(f_m\\)</span> for <span class=\"math inline\">\\(m < n\\)</span>. Indeed, one can see that the true gradient obeys the following:</p>\n<p><span class=\"math display\">\\[\n\\begin{align*}\n\\nabla_\\theta \\mathcal{F}(\\theta)\n&= \\mathbb{E}_{p(z_{\\le N} | \\theta)} \\sum_{n=1}^N \\left(\\sum_{k=1}^N f_k(z_{\\le k})\\right) \\nabla_\\theta \\log p(z_n|z_{<n}, \\theta) \\\\\n&= \\sum_{n=1}^N \\sum_{k=1}^N \\mathbb{E}_{p(z_{\\le N} | \\theta)} \\left[ f_k(z_{\\le k}) \\nabla_\\theta \\log p(z_n|z_{<n}, \\theta)\\right] \\\\\n&= {\\scriptsize \\sum_{n=1}^N \\left(\\sum_{k=1}^{n-1} \\mathbb{E}_{p(z_{\\le n} | \\theta)} \\left[f_k(z_{\\le k}) \\nabla_\\theta \\log p(z_n|z_{<n}, \\theta) \\right] + \\sum_{k=n}^N \\mathbb{E}_{p(z_{\\le N} | \\theta)} \\left[f_k(z_{\\le k}) \\nabla_\\theta \\log p(z_n|z_{<n}, \\theta) \\right]\\right)} \\\\\n&= {\\scriptsize \\sum_{n=1}^N \\left(\\mathbb{E}_{z_{<n}} \\left[\\left(\\sum_{k=1}^{n-1} f_k(z_{\\le k}) \\right) \\overbrace{\\mathbb{E}_{z_n|z_{<n}} \\nabla_\\theta \\log p(z_n|z_{<n}, \\theta)}^{=0}\\right] + \\sum_{k=n}^N \\mathbb{E}_{z_{\\le N}} \\left[f_k(z_{\\le k}) \\nabla_\\theta \\log p(z_n|z_{<n}, \\theta) \\right]\\right)} \\\\\n&= \\mathbb{E}_{p(z|\\theta)} \\left[{\\scriptsize \\sum_{n=1}^N \\sum_{k=n}^N f_k(z_{\\le k}) \\nabla_\\theta \\log p(z_n|z_{<n}, \\theta)} \\right] \\\\\n\\end{align*}\n\\]</span></p>\n<p>Naturally, the part of the gradient corresponding to <span class=\"math inline\">\\(n\\)</span>th chunk is weighted by the total reward we’d get after deciding upon <span class=\"math inline\">\\(z_n\\)</span>, since the previous rewards do not depend on <span class=\"math inline\">\\(z_n\\)</span>.</p>\n<p>Combined with the context-dependent baseline the estimator would be</p>\n<p><span class=\"math display\">\\[\n\\hat{\\nabla}_\\theta^\\text{SF-NVIL} \\mathcal{F}(\\theta) = \n{\\scriptsize \\sum_{n=1}^N \\sum_{k=n}^N (f_k(x, z_{\\le k}) - b_k(x)) \\nabla_\\theta \\log p(z_n|x, z_{<n}, \\theta)} \\\\\n\\]</span></p>\n<p>Moreover, the baseline can be made dependent on some previous <span class=\"math inline\">\\(z\\)</span> since such baseline does not introduce any bias:</p>\n<p><span class=\"math display\">\\[\n\\begin{align*}\n\\mathbb{E}_{p(z|x, \\theta)} \n& \\sum_{n=1}^N \\sum_{k=n}^N b_{n,k}(x, z_{<n}) \\nabla_\\theta \\log p(z_n|x, z_{<n}, \\theta) \\\\\n& =\n\\sum_{n=1}^N \\sum_{k=n}^N\n\\mathbb{E}_{p(z_{<n}|x, \\theta)} \n\\mathbb{E}_{p(z_n|x, z_{<n}, \\theta)} \nb_{n,k}(x, z_{<n}) \\nabla_\\theta \\log p(z_n|x, z_{<n}, \\theta) \\\\\n& =\n\\sum_{n=1}^N \\sum_{k=n}^N\n\\mathbb{E}_{p(z_{<n}|x, \\theta)} b_{n,k}(x, z_{<n}) \n\\overbrace{\\mathbb{E}_{p(z_n|x, z_{<n}, \\theta)} \\nabla_\\theta \\log p(z_n|x, z_{<n}, \\theta)}^{=0}\n= 0\n\\end{align*}\n\\]</span></p>\n<p>However, learning <span class=\"math inline\">\\({n \\choose 2}\\)</span> different baselines is computationally demanding, so one would probably at least assume some common underlying structure.</p>\n<h3 id=\"vimco\">VIMCO</h3>\n<p>Another case of using the particular structure is the the VIMCO (<a href=\"https://arxiv.org/abs/1602.06725\">Variational inference for Monte Carlo objectives</a>) estimator. Again, consider a case of the latent variable <span class=\"math inline\">\\(z\\)</span> being divided in <span class=\"math inline\">\\(N\\)</span> chunks, but now <span class=\"math inline\">\\(z_n\\)</span> are independent identically distributed samples: <span class=\"math inline\">\\(z_n \\sim p(z|\\theta)\\)</span>. Suppose <span class=\"math inline\">\\(f\\)</span> has the following form: <span class=\"math inline\">\\(f(z) = g\\left(\\tfrac{1}{N} {\\scriptsize\\sum_{n=1}^N} h(z_n)\\right)\\)</span>. Then the REINFORCE gradient estimate would be:</p>\n<p><span class=\"math display\">\\[\n\\begin{align*}\n\\nabla_\\theta \\mathcal{F}(\\theta)\n&= \\mathbb{E}_{p(z | \\theta)} \\sum_{n=1}^N g\\left(\\tfrac{1}{N} {\\scriptsize\\sum_{n=1}^N} h(z_n)\\right) \\nabla_\\theta \\log p(z_n|\\theta) \\\\\n\\end{align*}\n\\]</span></p>\n<p>The problem with this estimator is that <span class=\"math inline\">\\(g(\\dots)\\)</span> is a common multiplier, and defines a magnitude of the gradient for each of <span class=\"math inline\">\\(N\\)</span> samples, without any distinction, despite some samples <span class=\"math inline\">\\(z_n\\)</span> might have turned out better than others. We would like to penalise such samples lesser, performing a kind of <em>credit assignment</em>.</p>\n<p>Just as in the previous section, we can consider baselines <span class=\"math inline\">\\(b_n\\)</span> that depend on samples <span class=\"math inline\">\\(z\\)</span>. To keep them from biasing the gradient estimate we need to make sure each <span class=\"math inline\">\\(b_n\\)</span> does not depend on <span class=\"math inline\">\\(z_n\\)</span>. However, it can depend on all other <span class=\"math inline\">\\(z\\)</span> (denoted <span class=\"math inline\">\\(z_{-n}\\)</span>) since they are independent of <span class=\"math inline\">\\(z_n\\)</span>. Thus the bias of such baseline is:</p>\n<p><span class=\"math display\">\\[\n\\begin{align*}\n\\mathbb{E}_{p(z | \\theta)}\n\\sum_{n=1}^N b_n(z_{-n}) \\nabla_\\theta \\log p(z_n|\\theta)\n=\n\\sum_{n=1}^N\n\\mathbb{E}_{p(z_{-n} | \\theta)}\nb_n(z_{-n})\n\\overbrace{\n\\mathbb{E}_{p(z_{n} | \\theta)}\n\\nabla_\\theta \\log p(z_n|\\theta)\n}^{=0}\n= 0\n\\end{align*}\n\\]</span></p>\n<p>Authors of the VIMCO paper also suggested an interesting trick to avoid learning <span class=\"math inline\">\\(b_n(z_{-n})\\)</span>: we want <span class=\"math inline\">\\(b_{-n}(z_{-n})\\)</span> to approximate <span class=\"math inline\">\\(f(z)\\)</span> as good as possible and we actually have access to everything we need to compute <span class=\"math inline\">\\(f(z)\\)</span> except the term that depends on <span class=\"math inline\">\\(z_n\\)</span>: <span class=\"math inline\">\\(h(z_n)\\)</span>. However, all samples <span class=\"math inline\">\\(z\\)</span> are identically distributed, so we can approximate this missing term as the average of others:</p>\n<p><span class=\"math display\">\\[\n\\hat h_n(z_{-n}) = \\frac{1}{N-1} \\sum_{j \\not= n} h(z_j) \\stackrel{\\text{hopefully}}{\\approx} h(z_n)\n\\]</span></p>\n<p>Then our baseline becomes</p>\n<p><span class=\"math display\">\\[\nb_n(z_{-n})\n=\ng\\left(\\tfrac{{\\scriptsize\\sum_{j \\not= n}} h(z_j) + \\hat h_n(z_{-n})}{N} \\right)\n\\]</span></p>\n<p>One can also consider other averaging schemes for <span class=\"math inline\">\\(\\hat h_n(z_{-n})\\)</span> to approximate <span class=\"math inline\">\\(h(z_n)\\)</span>: geometric, harmonic, Minkowski, etc.</p>\n<h2 id=\"muprop\">MuProp</h2>\n<p>So far we have been considering only baselines <span class=\"math inline\">\\(b\\)</span> that have zero expected value and thus do not bias the gradient estimator. However, there are cases when we actually know the baseline’s expectation and can compensate the introduced bias.</p>\n<p>The <a href=\"https://arxiv.org/abs/1511.05176\">MuProp</a> paper suggests to use a Taylor expansion as a baseline, provided we can compute certain moments of the distribution <span class=\"math inline\">\\(p(z|\\theta)\\)</span> in a closed form. For example, if <span class=\"math inline\">\\(p(z|\\theta) = \\mathcal{N}(z \\mid \\mu(\\theta), \\Sigma(\\theta))\\)</span>, then we already have access to 1st and 2nd moments – the mean and the covariance matrix.</p>\n<p>Consider a Taylor expansion of <span class=\"math inline\">\\(f(z)\\)</span> at <span class=\"math inline\">\\(\\mu(\\theta) = \\mathbb{E}_{p(z|\\theta)} z\\)</span> of the first order:</p>\n<p><span class=\"math display\">\\[\nb_\\theta(z) = f(\\mu(\\theta)) + \\nabla_z f(\\mu(\\theta))^T (z - \\mu(\\theta))\n\\]</span></p>\n<p>Then the bias introduced by such baseline would be</p>\n<p><span class=\"math display\">\\[\n\\begin{align*}\n\\mathbb{E}_{p(z|\\theta)}\n&\nb_\\theta(z) \\nabla_\\theta \\log p(z|\\theta) \\\\\n&=\n\\mathbb{E}_{p(z|\\theta)} \n\\left[\nf(\\mu(\\theta)) + \\nabla_z f(\\mu(\\theta))^T (z - \\mu(\\theta))\n\\right] \\nabla_\\theta \\log p(z|\\theta) \\\\\n&=\n\\mathbb{E}_{p(z|\\theta)} \n\\left[\n\\nabla_z f(\\mu(\\theta))^T z\n+\nf(\\mu(\\theta)) - \\nabla_z f(\\mu(\\theta))^T \\mu(\\theta)\n\\right] \\nabla_\\theta \\log p(z|\\theta) \\\\\n&=\n\\mathbb{E}_{p(z|\\theta)} \n\\left[\n\\nabla_z f(\\mu(\\theta))^T z \\nabla_\\theta \\log p(z|\\theta)\n\\right] \\\\\n& \\quad\\quad\\quad +\n\\left[\nf(\\mu(\\theta)) - \\nabla_z f(\\mu(\\theta))^T \\mu(\\theta)\n\\right] \n\\overbrace{\\mathbb{E}_{p(z|\\theta)} \\nabla_\\theta \\log p(z|\\theta)}^{=0} \\\\\n&=\n\\nabla_z f(\\mu(\\theta))^T\n\\mathbb{E}_{p(z|\\theta)} \n\\left[\nz \\nabla_\\theta \\log p(z|\\theta)\n\\right]\n=\n\\nabla_z f(\\mu(\\theta))^T\n\\nabla_\\theta\n\\mathbb{E}_{p(z|\\theta)} \n\\left[\nz\n\\right] \\\\\n& = \n\\nabla_z f(\\mu(\\theta))^T\n\\nabla_\\theta \\mu(\\theta) = \n\\nabla_\\theta f(\\mu(\\theta))\n\\end{align*}\n\\]</span></p>\n<p>So the (1st order) MuProp estimator has the following form:</p>\n<p><span class=\"math display\">\\[\n\\hat{\\nabla}_\\theta^\\text{SF-MuProp} \\mathcal{F}(\\theta) = (f(z) - f(\\mu(\\theta)) - \\nabla_z f(z)^T (z - \\mu(\\theta))) \\nabla_\\theta \\log p(z|\\theta) + \\nabla_\\theta f(\\mu(\\theta))\n\\]</span></p>\n<p>An appealing property is that not only does this gradient estimator is unbiased, but it also uses the gradients of <span class=\"math inline\">\\(f\\)</span> in the <span class=\"math inline\">\\(\\nabla_\\theta f(\\mu(\\theta))\\)</span>, essentially propagating the learning signal though the mean of the random variable <span class=\"math inline\">\\(z\\)</span>, and then correcting for the introduced bias with REINFORCE.</p>\n<p>One could, of course, envision a second-order baseline, especially considering we have the covariance matrix readily available for many distributions. However, such baseline would be more computationally demanding, requiring us to compute the Hessian matrix of <span class=\"math inline\">\\(f(z)\\)</span> and evaluate it at some point, which would cost at least <span class=\"math inline\">\\(\\text{dim}(z)^2\\)</span> computations. Higher order expansions would require even more computations, thus it’s hard to achive high non-linearity in the baseline using MuProp alone <a href=\"#fn8\" class=\"footnoteRef\" id=\"fnref8\"><sup>8</sup></a>.</p>\n<h2 id=\"rebar\">REBAR</h2>\n<p><a href=\"https://arxiv.org/abs/1703.07370\">REBAR</a><a href=\"#fn9\" class=\"footnoteRef\" id=\"fnref9\"><sup>9</sup></a> is a clever way to use the <a href=\"/posts/2017-10-28-stochastic-computation-graphs-discrete-relaxations.html#gumbel-softmax-relaxation-aka-concrete-distribution\">Gumbel-Softmax (aka Concrete) Relaxation</a> as a baseline.</p>\n<p>A naive approach to the task would be to recall the Gumbel-Max trick: as we have already seen, this trick gives us the reparametrization, albeit not a differentiable one. However, we can move the non-differentiability into the <span class=\"math inline\">\\(f(z)\\)</span>, and then invoke REINFORCE to estimate gradient of average of the non-differentiable function (from now on we will assume <span class=\"math inline\">\\(z\\)</span> is a one-hot vector and argmax is an operator that returns a one-hot vector, indicating position of the maximal element in the input and overall will be abusing notation treating the same <span class=\"math inline\">\\(z\\)</span> a one-hot vector or a number depending on a context):</p>\n<p><span class=\"math display\">\\[\n\\nabla_\\theta \\mathbb{E}_{p(z|\\theta)} f(z)\n= \\nabla_\\theta \\mathbb{E}_{p(\\zeta|\\theta)} f(\\text{argmax} \\zeta)\n= \\mathbb{E}_{p(\\zeta|\\theta)} f(\\text{argmax} \\zeta) \\nabla_\\theta \\log p(\\zeta|\\theta)\n\\]</span></p>\n<p>Where <span class=\"math inline\">\\(\\zeta_k\\)</span> is obtained by shifting an independent standard Gumbel r.v. <span class=\"math inline\">\\(\\gamma_k\\)</span> by a logit of <span class=\"math inline\">\\(k\\)</span>th probability:</p>\n<p><span class=\"math display\">\\[\n \\zeta_k = \\log p(z = k | \\theta) + \\gamma_k, \\quad\\quad \\gamma_k \\sim \\text{Gumbel}(0, 1)\n\\]</span></p>\n<p>Thus <span class=\"math inline\">\\(\\zeta_k\\)</span> also has a Gumbel distribution: <span class=\"math inline\">\\(\\zeta_k \\sim \\text{Gumbel}(\\log p(z = k | \\theta), 1)\\)</span>. Ok, so what have we bought ourselves here? So far it looks like we gained nothing but instead only complicated the whole thing with these extra <span class=\"math inline\">\\(\\zeta\\)</span>s. However, we just obtained a crucial property: we separated non-differentiability from the reparametrization. We now can sample continuous reparametrizeable <span class=\"math inline\">\\(\\zeta\\)</span>s and the troublesome part – the argmax – is now a part of <span class=\"math inline\">\\(f\\)</span>. And this opens up a new way to use <strong>baselines with non-zero expectation</strong>:</p>\n<p><span class=\"math display\">\\[\n\\mathbb{E}_{p(\\zeta|\\theta)} (f(\\text{argmax} \\zeta) - b(\\zeta)) \\nabla_\\theta \\log p(\\zeta | \\theta) + \\nabla_\\theta \\mathbb{E}_{p(\\zeta|\\theta)} b(\\zeta)\n\\]</span></p>\n<p>And the most interesting thing is that the bias correction term, <span class=\"math inline\">\\(\\nabla_\\theta \\mathbb{E}_{p(\\zeta|\\theta)} b(\\zeta)\\)</span>, is differentiable and reparametrizable, and thus its gradient can be estimated with the reparametrization trick. Now, that’s nice, but we can’t just take any <span class=\"math inline\">\\(b(\\zeta)\\)</span> and hope for variance reduction. In order to actually benefit from such baseline, we need <span class=\"math inline\">\\(b(\\zeta) \\approx f(\\text{argmax} \\zeta)\\)</span>. Luckily, we already know a way to organize this: the Gumbel-Softmax obtained nicely by setting <span class=\"math inline\">\\(b(\\zeta) = f(\\text{softmax}_\\tau(\\zeta))\\)</span>:</p>\n<p><span class=\"math display\">\\[\n\\hat{\\nabla}_\\theta^\\text{SF-REBAR-naive} \\mathcal{F}(\\theta) \n=\n(f(\\text{argmax} \\zeta) - b(\\zeta)) \\nabla_\\theta \\log p(\\zeta | \\theta) + \\nabla_\\theta f(\\text{softmax}_\\tau(\\zeta))\n\\]</span></p>\n<p>However, there’s a reason I called this estimator <em>naive</em>. If you actually try implementing this estimator, you would hardly see any improvements. If you look closely, you’d notice that we actually increased the variance of the REINFORCE estimator by switching to <span class=\"math inline\">\\(\\zeta\\)</span>s, and this increase might not be compensated by the Gumbel-Softmax baseline we introduced.</p>\n<p>I guess it all looks a bit confusing at this moment, so lets take a closer look at the original REINFORCE estimator and the naive REBAR without baseline:</p>\n<p><span class=\"math display\">\\[\n\\begin{align*}\n\\hat{\\nabla}_\\theta^\\text{SF} \\mathcal{F}(\\theta) \n&=\nf(z) \\nabla_\\theta \\log p(z | \\theta)\n\\\\\n\\hat{\\nabla}_\\theta^\\text{SF-REBAR-naive-without-baseline} \\mathcal{F}(\\theta) \n&=\nf(\\text{argmax} \\zeta) \\nabla_\\theta \\log p(\\zeta | \\theta)\n\\end{align*}\n\\]</span></p>\n<p>You’d think they’re the same, however actually they’re quite different. But not in the first terms, <span class=\"math inline\">\\(f(z)\\)</span> and <span class=\"math inline\">\\(f(\\text{argmax} \\zeta)\\)</span>, as those are basically the same. It’s the second term that’s important to us: the vanilla REINFORCE has <span class=\"math inline\">\\(\\nabla_\\theta \\log p({\\color{red} z}|\\theta)\\)</span>, whereas our naive REBAR has <span class=\"math inline\">\\(\\nabla_\\theta \\log p({\\color{red} \\zeta}|\\theta)\\)</span>. This seemingly innocent difference is a huge deal! To see why <a href=\"/posts/2017-09-10-stochastic-computation-graphs-continuous-case.html\">recall the REINFORCE intuition</a>: it is not a gradient method, but rather a random search in disguise: it tries a bunch of points, and increases probabilities of those performing good. However, the major problem is that different <span class=\"math inline\">\\(\\zeta\\)</span>s can lead to the same <span class=\"math inline\">\\(z\\)</span>: indeed the argmax takes on only finite number of different values, whereas there’s continuum of different vectors <span class=\"math inline\">\\(\\zeta\\)</span>. This, in result, means that our naive REBAR estimate would be trying some <span class=\"math inline\">\\(\\zeta\\)</span> (corresponding to some <span class=\"math inline\">\\(z\\)</span>) and then trying to pull the probability mass towards (or away from) this point, maybe undoing some useful work it did for a different <span class=\"math inline\">\\(\\zeta\\)</span> (but same <span class=\"math inline\">\\(z\\)</span>).</p>\n<p>To fix this issue we need to stay in “space of <span class=\"math inline\">\\(\\nabla_\\theta \\log p(z|\\theta)\\)</span>” – a control variate of the form <span class=\"math inline\">\\(b(z) \\nabla_\\theta \\log p(z|\\theta)\\)</span>. And one is given with help of the following clever identity:</p>\n<p><span class=\"math display\">\\[\n\\begin{align*}\n\\nabla_\\theta \\mathbb{E}_{p(\\zeta|\\theta)} b(\\text{softmax}_\\tau(\\zeta))\n&=\n\\nabla_\\theta \\mathbb{E}_{p(z, \\zeta|\\theta)} b(\\text{softmax}_\\tau(\\zeta)) \\\\\n&=\n\\nabla_\\theta \\mathbb{E}_{p(z|\\theta)} \\mathbb{E}_{p(\\zeta|z, \\theta)} b(\\text{softmax}_\\tau(\\zeta)) \\\\\n&=\n\\mathbb{E}_{p(z|\\theta)} \\mathbb{E}_{p(\\zeta|z, \\theta)} b(\\text{softmax}_\\tau(\\zeta)) \\nabla_\\theta \\log p(z|\\theta)\n\\\\& \\quad +\n\\mathbb{E}_{p(z|\\theta)} \\nabla_\\theta \\mathbb{E}_{p(\\zeta|z, \\theta)} b(\\text{softmax}_\\tau(\\zeta))\n\\end{align*}\n\\]</span></p>\n<p>On the left hand side we have the usual Gumbel-Softmax relaxed gradient which we can compute using the reparametrization. On the right hand size we have a REINFORCE-like gradient – which is a good candidate for a baseline – and another weirdly looking term. We can rearrange the terms to express the bias of such a baseline through the other two terms:</p>\n<p><span class=\"math display\">\\[\n\\begin{align*}\n\\mathbb{E}_{p(z|\\theta)} \\mathbb{E}_{p(\\zeta|z, \\theta)} & b(\\text{softmax}_\\tau(\\zeta)) \\nabla_\\theta \\log p(z|\\theta)\n\\\\ =\n\\nabla_\\theta \\mathbb{E}_{p(\\zeta|\\theta)} & b(\\text{softmax}_\\tau(\\zeta))\n -\n\\mathbb{E}_{p(z|\\theta)} \\nabla_\\theta \\mathbb{E}_{p(\\zeta|z, \\theta)} b(\\text{softmax}_\\tau(\\zeta))\n\\end{align*}\n\\]</span></p>\n<p>But what about that weirdly looking last term? Can it be estimated efficiently? First, note that we do not need to differentiate through <span class=\"math inline\">\\(z\\)</span>, the dependence through <span class=\"math inline\">\\(z\\)</span> was already accounted for. The expectation we need to differentiate is taken over <span class=\"math inline\">\\(p(\\zeta|z, \\theta)\\)</span> which is a distribution over <span class=\"math inline\">\\(\\zeta\\)</span> such that <span class=\"math inline\">\\(\\text{argmax} \\zeta = z\\)</span>. A reassuring observation is that such random variable is continuous. Moreover, the restriction <span class=\"math inline\">\\(\\text{argmax} \\zeta = z\\)</span> defines a connected region of <span class=\"math inline\">\\(\\mathbb{R}^K\\)</span>, which means there does exist a differentible reparametrization for such random variable! We won’t be deriving this reparametrization here, please refer to <a href=\"https://cmaddis.github.io/gumbel-machinery\">Chris Maddison’s blog</a>. That said, the reparametrization is</p>\n<p><span class=\"math display\">\\[\n\\zeta_k|z = \\begin{cases}\n-\\log (- \\log v_k), \\quad\\quad\\quad & \\text{if $z = k$}, \\\\\n-\\log \\left(-\\frac{\\log v_k}{p(z=k|\\theta)} - \\log v_z \\right), \\quad\\quad\\quad & \\text{otherwise}.\n\\end{cases}\n\\]</span></p>\n<p>Where <span class=\"math inline\">\\(v \\sim U[0,1]^K\\)</span> is a <span class=\"math inline\">\\(K\\)</span>-dimensional standard uniform r.v. Now, having this reparametrization we can estimate both terms in the bias correction via the reparametrization trick, which leads to the following estimate (I use notation <span class=\"math inline\">\\(\\hat{z}|z\\)</span> to mean singular object, the conditional relaxed variable, it’s <strong>not</strong> <span class=\"math inline\">\\(\\hat{z}\\)</span> with some <span class=\"math inline\">\\(|z\\)</span> applied to it, and neither it’s <span class=\"math inline\">\\(b(\\cdot|z)\\)</span>):</p>\n<p><span class=\"math display\">\\[\n\\begin{align*}\n\\hat{\\nabla}_\\theta^\\text{SF-REBAR} \\mathcal{F}(\\theta) \n=\n\\left[f(z) - b(\\hat{z}|z) \\right] \\nabla_\\theta \\log p(z | \\theta)\n+\n\\nabla_\\theta \nb(\\hat{z})\n -\n\\nabla_\\theta b(\\hat{z}|z)\n\\end{align*}\n\\]</span> We use <span class=\"math inline\">\\(u \\sim U[0,1]^K\\)</span> to reparametrize <span class=\"math inline\">\\(\\zeta\\)</span> (which leads to both <span class=\"math inline\">\\(z\\)</span> and <span class=\"math inline\">\\(\\hat{z}\\)</span>), and <span class=\"math inline\">\\(v \\sim U[0,1]^K\\)</span> is used to reparametrize <span class=\"math inline\">\\(\\zeta|z\\)</span>, see the formula above. The quantities of the REBAR gradient estimate are computed as follows: <span class=\"math display\">\\[\n\\begin{align*}\nz = \\text{argmax} \\zeta,\n\\quad\\quad\n\\hat{z} = \\text{softmax}_\\tau(\\zeta),\n\\quad\\quad\n\\hat{z}|z = \\text{softmax}_\\tau(\\zeta|z),\n\\\\\n\\zeta_k = \\log p(z = k | \\theta) - \\log(-\\log u_k),\n\\quad\\quad\n\\zeta_k|z = \\text{given above}\n\\end{align*}\n\\]</span></p>\n<p>What about <span class=\"math inline\">\\(b(\\cdot)\\)</span>? Authors use <span class=\"math inline\">\\(b(\\cdot) = \\eta f(\\cdot)\\)</span> where <span class=\"math inline\">\\(\\eta\\)</span> is some hyperparameter that regulates the strength of a baseline. But turns out, we can avoid hyperparameter search for this variable…</p>\n<h3 id=\"hyperparameter-learing-and-relax\">Hyperparameter learing and RELAX</h3>\n<p>An important observation is that the gradient estimator we’ve obtained is unbiased<a href=\"#fn10\" class=\"footnoteRef\" id=\"fnref10\"><sup>10</sup></a>. That is, for any choice of hyperparameters <span class=\"math inline\">\\(\\tau\\)</span> (the Gumbel-Softmax temperature) and <span class=\"math inline\">\\(\\eta\\)</span> the average value of our estimator is equal to the true gradient. Thus, we can actually learn their values! The only question is, well, which objective should we minimize? We can’t minimize the problem’s loss <span class=\"math inline\">\\(-f(\\cdot)\\)</span>, since we already have its gradient. The next logical step is to minimize the <em>variance</em> of the gradient estimator. <span class=\"math display\">\\[\n\\text{Var}\\left( \\hat{\\nabla}_\\theta^\\text{SF-REBAR} \\mathcal{F}(\\theta)  \\right)\n=\n\\sum_{i}\n\\left(\n\\mathbb{E}\n\\left[\\hat{\\nabla}_{\\theta_i}^\\text{SF-REBAR} \\mathcal{F}(\\theta)\\right]^2\n-\n\\left[\n\\mathbb{E} \\hat{\\nabla}_{\\theta_i}^\\text{SF-REBAR} \\mathcal{F}(\\theta)\\right]^2\n\\right)\n\\]</span> Where the expectation is taken over all randomness. Moreover, since the estimator is unbiased, we can omit the 2nd term in the sum, since it’ll be constant w.r.t. <span class=\"math inline\">\\(\\tau\\)</span> and <span class=\"math inline\">\\(\\eta\\)</span>.</p>\n<p>Thus the objective for <span class=\"math inline\">\\(\\tau\\)</span> and <span class=\"math inline\">\\(\\eta\\)</span> is <span class=\"math display\">\\[\n\\begin{align*}\n\\tau^*, \\eta^*\n&=\n\\text{argmin}_{\\tau, \\eta} \\text{Var}\\left( \\hat{\\nabla}_\\theta^\\text{SF-REBAR} \\mathcal{F}(\\theta)  \\right) \\\\\n&=\n\\text{argmin}_{\\tau, \\eta} \n\\mathbb{E}\n\\sum_{i}\n\\left[\\hat{\\nabla}_{\\theta_i}^\\text{SF-REBAR} \\mathcal{F}(\\theta)\\right]_2^2\n=\n\\text{argmin}_{\\tau, \\eta} \n\\mathbb{E}\n\\left\\|\n\\hat{\\nabla}_{\\theta}^\\text{SF-REBAR} \\mathcal{F}(\\theta)\n\\right\\|^2\n\\end{align*}\n\\]</span></p>\n<p>This optimization problem can be solved using stochastic optimization. We first get a stochastic estimate of the gradient w.r.t. <span class=\"math inline\">\\(\\theta\\)</span>, and then obtain an estimate of the gradient w.r.t. “hyperparameters” <span class=\"math inline\">\\(\\tau\\)</span> and <span class=\"math inline\">\\(\\eta\\)</span>. Practical implementation is somewhat tricky, the <a href=\"https://arxiv.org/abs/1802.05098\">MagicBox operator</a> might be useful.</p>\n<p>Finally, it’s worth noticing that although we can’t apply this estimator some scenarios like Reinforcement Learning (because we don’t have access to <span class=\"math inline\">\\(f(\\cdot)\\)</span>), it’s possible to introduce a minor modification to overcome this issue. Remember the moment we decided to put <span class=\"math inline\">\\(b(\\cdot) = \\eta f(\\cdot)\\)</span>? At this moment we could have made any other choice, for example consider <span class=\"math inline\">\\(b(\\cdot) = h_\\eta(\\cdot)\\)</span> – a neural network with parameters <span class=\"math inline\">\\(\\eta\\)</span> that takes <span class=\"math inline\">\\(\\hat{z}\\)</span> as input and returns the same thig <span class=\"math inline\">\\(f(\\cdot)\\)</span> would return (a scalar in our case). Then we can learn the parameters <span class=\"math inline\">\\(\\eta\\)</span> of this network in the same way as before.</p>\n<p>This gives us the so-called <a href=\"https://arxiv.org/abs/1711.00123\">RELAX</a> gradient estimator: <span class=\"math display\">\\[\n\\hat{\\nabla}_\\theta^\\text{SF-RELAX} \\mathcal{F}(\\theta) \n=\n\\left[f(z) - h_\\eta(\\hat{z}|z) \\right] \\nabla_\\theta \\log p(z | \\theta)\n+\n\\nabla_\\theta \nh_\\eta(\\hat{z})\n -\n\\nabla_\\theta h_\\eta(\\hat{z}|z)\n\\]</span></p>\n<p>This estimator now does not assume access to the optimizeable function <span class=\"math inline\">\\(f(\\cdot)\\)</span>, nor its differentiability, so it can be applied in larger number of scenarios. Of course, having an access to a differentiable <span class=\"math inline\">\\(f(\\cdot)\\)</span> would put this estimator into a disadvantage compared to REBAR, since the later already has a pretty good idea as to how the baseline should look like.</p>\n<p>Overall, I like the REBAR/RELAX gradient estimator for its use of the target function’s gradient <span class=\"math inline\">\\(\\nabla_z f(\\cdot)\\)</span> and non-linear baseline somewhat closely approximating the target <span class=\"math inline\">\\(f(z)\\)</span>. However, it’s effectiveness comes at a cost: you’d need 3 times more computation: one discrete run <span class=\"math inline\">\\(f(z)\\)</span>, one relaxed run <span class=\"math inline\">\\(f(\\hat{z})\\)</span> and one conditionally relaxed run <span class=\"math inline\">\\(f(\\hat{z}|z)\\)</span> – which is much more computation than the plain Gumbel-Softmax does.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>This post closes the series of Stochastic Computation Graphs. There are many other methods, but for some reason I left them uncovered. Maybe I consider them weird mathematical hacks or simply didn’t know about their existence! Overall, I think all these estimators I covered in 3 posts and reasoning behind them establish a solid toolkit for many problems of practical interest.</p>\n<div class=\"footnotes\">\n<hr />\n<ol>\n<li id=\"fn1\"><p>REINFORCE stands for <strong>RE</strong>ward <strong>I</strong>ncrement = <strong>N</strong>onnegative <strong>F</strong>actor × <strong>O</strong>ffset <strong>R</strong>einforcement × <strong>C</strong>haracteristic <strong>E</strong>ligibility<a href=\"#fnref1\">↩</a></p></li>\n<li id=\"fn2\"><p>Monte Carlo averaging isn’t very efficient. The variance decreases as <span class=\"math inline\">\\(O(1/L)\\)</span> for <span class=\"math inline\">\\(L\\)</span> samples, and typical error (by invoking the CLT) drops as <span class=\"math inline\">\\(O(1 / \\sqrt{L})\\)</span>. That is, to reduce the typical error of MC approximation by a factor of 1000, you’d need an order of millions samples! It’s very hard to beat the high variance by sampling alone.<a href=\"#fnref2\">↩</a></p></li>\n<li id=\"fn3\"><p>One could also use matrix baselines and multiply them by the <span class=\"math inline\">\\(\\nabla \\log p(z|\\theta)\\)</span> as usual, but we won’t cover these – this method does not scale well with number of parameters in <span class=\"math inline\">\\(\\theta\\)</span>.<a href=\"#fnref3\">↩</a></p></li>\n<li id=\"fn4\"><p>Monte Carlo averaging isn’t very efficient. The variance decreases as <span class=\"math inline\">\\(O(1/L)\\)</span> for <span class=\"math inline\">\\(L\\)</span> samples, and typical error (by invoking the CLT) drops as <span class=\"math inline\">\\(O(1 / \\sqrt{L})\\)</span>. That is, to reduce the typical error of MC approximation by a factor of 1000, you’d need an order of millions samples! It’s very hard to beat the high variance by sampling alone.<a href=\"#fnref4\">↩</a></p></li>\n<li id=\"fn5\"><p>You might ask, wait, what if we use an independent and identically distributed sample <span class=\"math inline\">\\(z'\\)</span> in the baseline? Consider the following: <span class=\"math display\">\\[ \\left( f(z) - b(z') \\right) \\nabla_\\theta \\log p(z|\\theta), \\quad\\quad\\quad z, z' \\sim p(z|\\theta) \\]</span> This is a valid and unbiased gradient estimate, however since <span class=\"math inline\">\\(z\\)</span> and <span class=\"math inline\">\\(z'\\)</span> are independent, this is essentially a stochastic version of the following estimator: <span class=\"math display\">\\[ \\left( f(z) - \\mathbb{E}_{p(z'|\\theta)} b(z') \\right) \\nabla_\\theta \\log p(z|\\theta), \\quad\\quad\\quad z \\sim p(z|\\theta) \\]</span> So we’re better off with approximating with a constant (w.r.t. <span class=\"math inline\">\\(z\\)</span>) baseline <span class=\"math inline\">\\(b(\\theta)\\)</span> the expectation of that value, and it is done in the NVIL method we’ll talk about later.<a href=\"#fnref5\">↩</a></p></li>\n<li id=\"fn6\"><p>Actually, it’d make much more sense to minimize the variance of the obtained estimator directly, we’ll discuss this later when talking about the REBAR and RELAX methods.<a href=\"#fnref6\">↩</a></p></li>\n<li id=\"fn7\"><p>The Evidence Lower Bound of Variational Inference can be presented in this way. Namely, the ELBO is <span class=\"math display\">\\[\n\\begin{align*}\n\\mathcal{F}(\\theta) &= \\mathbb{E}_{q(z_{1, \\dots, N|\\theta})} \\log \\frac{p(X, z_1, \\dots, z_N | \\theta)}{q(z_{1, \\dots, N|\\theta})} \\\\\n&= \\mathbb{E}_{q(z_{1, \\dots, N|\\theta})} \\left[ \\log p(X|z_{1, \\dots, N}, \\theta) + \\sum_{n=1}^N \\log \\frac{p(z_n | z_{<n})}{q(z_n | z_{<n}, \\theta)} \\right]\n\\end{align*}\n\\]</span> Then each intermediate layer gives you reward corresponding to the KL divergence with the prior, and the last layer also gives you the reconstruction reward.<a href=\"#fnref7\">↩</a></p></li>\n<li id=\"fn8\"><p>This might be due to the Taylor expansion being an unfortunate choice. Probably, considering some other expansion would be advantageous, but I’m unaware of any such works.<a href=\"#fnref8\">↩</a></p></li>\n<li id=\"fn9\"><p>The name is a very clever joke. Rebar is a term from construction works for steel bars that are used to <em>reinforce</em> <em>concrete</em>, and Concrete distribution is the name for the distribution of the Gumbel-Softmax relaxed random variables.<a href=\"#fnref9\">↩</a></p></li>\n<li id=\"fn10\"><p>Unlike the Gumbel-Softmax, which was biased for all <span class=\"math inline\">\\(\\tau > 0\\)</span>. In a sense, REBAR is a debiased version of Gumbel-Softmax.<a href=\"#fnref10\">↩</a></p></li>\n</ol>\n</div>",
  "pubDate": "Sun, 12 Nov 2017 00:00:00 UT",
  "guid": "http://artem.sobolev.name/posts/2017-11-12-stochastic-computation-graphs-fixing-reinforce.html",
  "dc:creator": "Artem"
}