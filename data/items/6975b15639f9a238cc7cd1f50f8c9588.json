{
  "title": "Ad Hoc Distributed Random Forests",
  "link": "",
  "updated": "2016-04-20T00:00:00+00:00",
  "id": "https://mrocklin.github.io/blog/work/2016/04/20/dask-distributed-part-5",
  "content": "<p><em>This work is supported by <a href=\"http://continuum.io\">Continuum Analytics</a>\nand the <a href=\"http://www.darpa.mil/program/XDATA\">XDATA Program</a>\nas part of the <a href=\"http://blaze.pydata.org\">Blaze Project</a></em></p>\n\n<p><em>A screencast version of this post is available here:\n<a href=\"https://www.youtube.com/watch?v=FkPlEqB8AnE\">https://www.youtube.com/watch?v=FkPlEqB8AnE</a></em></p>\n\n<h2 id=\"tldr\">TL;DR.</h2>\n\n<p>Dask.distributed lets you submit individual tasks to the cluster.  We use this\nability combined with Scikit Learn to train and run a distributed random forest\non distributed tabular NYC Taxi data.</p>\n\n<p>Our machine learning model does not perform well, but we do learn how to\nexecute ad-hoc computations easily.</p>\n\n<h2 id=\"motivation\">Motivation</h2>\n\n<p>In the past few posts we analyzed data on a cluster with Dask collections:</p>\n\n<ol>\n  <li><a href=\"http://matthewrocklin.com/blog/work/2016/02/17/dask-distributed-part1\">Dask.bag on JSON records</a></li>\n  <li><a href=\"http://matthewrocklin.com/blog/work/2016/02/22/dask-distributed-part-2\">Dask.dataframe on CSV data</a></li>\n  <li><a href=\"http://matthewrocklin.com/blog/work/2016/02/26/dask-distributed-part-3\">Dask.array on HDF5 data</a></li>\n</ol>\n\n<p>Often our computations don’t fit neatly into the bag, dataframe, or array\nabstractions.  In these cases we want the flexibility of normal code with for\nloops, but still with the computational power of a cluster.  With the\ndask.distributed task interface, we achieve something close to this.</p>\n\n<h2 id=\"application-naive-distributed-random-forest-algorithm\">Application: Naive Distributed Random Forest Algorithm</h2>\n\n<p>As a motivating application we build a random forest algorithm from the ground\nup using the single-machine Scikit Learn library, and dask.distributed’s\nability to quickly submit individual tasks to run on the cluster.  Our\nalgorithm will look like the following:</p>\n\n<ol>\n  <li>Pull data from some external source (S3) into several dataframes on the\ncluster</li>\n  <li>For each dataframe, create and train one <code class=\"language-plaintext highlighter-rouge\">RandomForestClassifier</code></li>\n  <li>Scatter single testing dataframe to all machines</li>\n  <li>For each <code class=\"language-plaintext highlighter-rouge\">RandomForestClassifier</code> predict output on test dataframe</li>\n  <li>Aggregate independent predictions from each classifier together by a\nmajority vote.  To avoid bringing too much data to any one machine, perform\nthis majority vote as a tree reduction.</li>\n</ol>\n\n<h2 id=\"data-nyc-taxi-2015\">Data: NYC Taxi 2015</h2>\n\n<p>As in our <a href=\"http://matthewrocklin.com/blog/work/2016/02/22/dask-distributed-part-2\">blogpost on distributed\ndataframes</a>\nwe use the data on all NYC Taxi rides in 2015.  This is around 20GB on disk and\n60GB in RAM.</p>\n\n<p>We predict the number of passengers in each cab given the other\nnumeric columns like pickup and destination location, fare breakdown, distance,\netc..</p>\n\n<p>We do this first on a small bit of data on a single machine and then on the\nentire dataset on the cluster.  Our cluster is composed of twelve m4.xlarges (4\ncores, 15GB RAM each).</p>\n\n<p><em>Disclaimer and Spoiler Alert</em>: I am not an expert in machine learning.  Our\nalgorithm will perform very poorly.  If you’re excited about machine\nlearning you can stop reading here.  However, if you’re interested in how to\n<em>build</em> distributed algorithms with Dask then you may want to read on,\nespecially if you happen to know enough machine learning to improve upon my\nnaive solution.</p>\n\n<h2 id=\"api-submit-map-gather\">API: submit, map, gather</h2>\n\n<p>We use a small number of <a href=\"http://distributed.readthedocs.org/en/latest/api.html\">dask.distributed\nfunctions</a> to build our\ncomputation:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">futures</span> <span class=\"o\">=</span> <span class=\"n\">executor</span><span class=\"p\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">)</span>                     <span class=\"c1\"># scatter data\n</span><span class=\"n\">future</span> <span class=\"o\">=</span> <span class=\"n\">executor</span><span class=\"p\">.</span><span class=\"n\">submit</span><span class=\"p\">(</span><span class=\"n\">function</span><span class=\"p\">,</span> <span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">)</span>  <span class=\"c1\"># submit single task\n</span><span class=\"n\">futures</span> <span class=\"o\">=</span> <span class=\"n\">executor</span><span class=\"p\">.</span><span class=\"nb\">map</span><span class=\"p\">(</span><span class=\"n\">function</span><span class=\"p\">,</span> <span class=\"n\">sequence</span><span class=\"p\">)</span>           <span class=\"c1\"># submit many tasks\n</span><span class=\"n\">results</span> <span class=\"o\">=</span> <span class=\"n\">executor</span><span class=\"p\">.</span><span class=\"n\">gather</span><span class=\"p\">(</span><span class=\"n\">futures</span><span class=\"p\">)</span>                   <span class=\"c1\"># gather results\n</span><span class=\"n\">executor</span><span class=\"p\">.</span><span class=\"n\">replicate</span><span class=\"p\">(</span><span class=\"n\">futures</span><span class=\"p\">,</span> <span class=\"n\">n</span><span class=\"o\">=</span><span class=\"n\">number_of_replications</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>In particular, functions like <code class=\"language-plaintext highlighter-rouge\">executor.submit(function, *args)</code> let us send\nindividual functions out to our cluster thousands of times a second.  Because\nthese functions consume their own results we can create complex workflows that\nstay entirely on the cluster and trust the distributed scheduler to move data\naround intelligently.</p>\n\n<h2 id=\"load-pandas-from-s3\">Load Pandas from S3</h2>\n\n<p>First we load data from Amazon S3.  We use the <code class=\"language-plaintext highlighter-rouge\">s3.read_csv(..., collection=False)</code>\nfunction to load 178 Pandas DataFrames on our cluster from CSV data on S3.  We\nget back a list of <code class=\"language-plaintext highlighter-rouge\">Future</code> objects that refer to these remote dataframes.  The\nuse of <code class=\"language-plaintext highlighter-rouge\">collection=False</code> gives us this list of futures rather than a single\ncohesive Dask.dataframe object.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">distributed</span> <span class=\"kn\">import</span> <span class=\"n\">Executor</span><span class=\"p\">,</span> <span class=\"n\">s3</span>\n<span class=\"n\">e</span> <span class=\"o\">=</span> <span class=\"n\">Executor</span><span class=\"p\">(</span><span class=\"s\">'52.91.1.177:8786'</span><span class=\"p\">)</span>\n\n<span class=\"n\">dfs</span> <span class=\"o\">=</span> <span class=\"n\">s3</span><span class=\"p\">.</span><span class=\"n\">read_csv</span><span class=\"p\">(</span><span class=\"s\">'dask-data/nyc-taxi/2015'</span><span class=\"p\">,</span>\n                  <span class=\"n\">parse_dates</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">'tpep_pickup_datetime'</span><span class=\"p\">,</span>\n                               <span class=\"s\">'tpep_dropoff_datetime'</span><span class=\"p\">],</span>\n                  <span class=\"n\">collection</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">)</span>\n<span class=\"n\">dfs</span> <span class=\"o\">=</span> <span class=\"n\">e</span><span class=\"p\">.</span><span class=\"n\">compute</span><span class=\"p\">(</span><span class=\"n\">dfs</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>Each of these is a lightweight <code class=\"language-plaintext highlighter-rouge\">Future</code> pointing to a <code class=\"language-plaintext highlighter-rouge\">pandas.DataFrame</code> on the\ncluster.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">dfs</span><span class=\"p\">[:</span><span class=\"mi\">5</span><span class=\"p\">]</span>\n<span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">Future</span><span class=\"p\">:</span> <span class=\"n\">status</span><span class=\"p\">:</span> <span class=\"n\">finished</span><span class=\"p\">,</span> <span class=\"nb\">type</span><span class=\"p\">:</span> <span class=\"n\">DataFrame</span><span class=\"p\">,</span> <span class=\"n\">key</span><span class=\"p\">:</span> <span class=\"n\">finalize</span><span class=\"o\">-</span><span class=\"n\">a06c3dd25769f434978fa27d5a4cf24b</span><span class=\"o\">&gt;</span><span class=\"p\">,</span>\n <span class=\"o\">&lt;</span><span class=\"n\">Future</span><span class=\"p\">:</span> <span class=\"n\">status</span><span class=\"p\">:</span> <span class=\"n\">finished</span><span class=\"p\">,</span> <span class=\"nb\">type</span><span class=\"p\">:</span> <span class=\"n\">DataFrame</span><span class=\"p\">,</span> <span class=\"n\">key</span><span class=\"p\">:</span> <span class=\"n\">finalize</span><span class=\"o\">-</span><span class=\"mi\">7</span><span class=\"n\">dcb27364a8701f45cb02d2fe034728a</span><span class=\"o\">&gt;</span><span class=\"p\">,</span>\n <span class=\"o\">&lt;</span><span class=\"n\">Future</span><span class=\"p\">:</span> <span class=\"n\">status</span><span class=\"p\">:</span> <span class=\"n\">finished</span><span class=\"p\">,</span> <span class=\"nb\">type</span><span class=\"p\">:</span> <span class=\"n\">DataFrame</span><span class=\"p\">,</span> <span class=\"n\">key</span><span class=\"p\">:</span> <span class=\"n\">finalize</span><span class=\"o\">-</span><span class=\"n\">b0dfe075000bd59c3a90bfdf89a990da</span><span class=\"o\">&gt;</span><span class=\"p\">,</span>\n <span class=\"o\">&lt;</span><span class=\"n\">Future</span><span class=\"p\">:</span> <span class=\"n\">status</span><span class=\"p\">:</span> <span class=\"n\">finished</span><span class=\"p\">,</span> <span class=\"nb\">type</span><span class=\"p\">:</span> <span class=\"n\">DataFrame</span><span class=\"p\">,</span> <span class=\"n\">key</span><span class=\"p\">:</span> <span class=\"n\">finalize</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"n\">c9bb25cefa1b892fac9b48c0aef7e04</span><span class=\"o\">&gt;</span><span class=\"p\">,</span>\n <span class=\"o\">&lt;</span><span class=\"n\">Future</span><span class=\"p\">:</span> <span class=\"n\">status</span><span class=\"p\">:</span> <span class=\"n\">finished</span><span class=\"p\">,</span> <span class=\"nb\">type</span><span class=\"p\">:</span> <span class=\"n\">DataFrame</span><span class=\"p\">,</span> <span class=\"n\">key</span><span class=\"p\">:</span> <span class=\"n\">finalize</span><span class=\"o\">-</span><span class=\"n\">c8254256b09ae287badca3cf6d9e3142</span><span class=\"o\">&gt;</span><span class=\"p\">]</span>\n</code></pre></div></div>\n\n<p>If we’re willing to wait a bit then we can pull data from any future back to\nour local process using the <code class=\"language-plaintext highlighter-rouge\">.result()</code> method.  We don’t want to do this too\nmuch though, data transfer can be expensive and we can’t hold the entire\ndataset in the memory of a single machine.  Here we just bring back one of the\ndataframes:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">dfs</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">].</span><span class=\"n\">result</span><span class=\"p\">()</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">df</span><span class=\"p\">.</span><span class=\"n\">head</span><span class=\"p\">()</span>\n</code></pre></div></div>\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>VendorID</th>\n      <th>tpep_pickup_datetime</th>\n      <th>tpep_dropoff_datetime</th>\n      <th>passenger_count</th>\n      <th>trip_distance</th>\n      <th>pickup_longitude</th>\n      <th>pickup_latitude</th>\n      <th>RateCodeID</th>\n      <th>store_and_fwd_flag</th>\n      <th>dropoff_longitude</th>\n      <th>dropoff_latitude</th>\n      <th>payment_type</th>\n      <th>fare_amount</th>\n      <th>extra</th>\n      <th>mta_tax</th>\n      <th>tip_amount</th>\n      <th>tolls_amount</th>\n      <th>improvement_surcharge</th>\n      <th>total_amount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>2015-01-15 19:05:39</td>\n      <td>2015-01-15 19:23:42</td>\n      <td>1</td>\n      <td>1.59</td>\n      <td>-73.993896</td>\n      <td>40.750111</td>\n      <td>1</td>\n      <td>N</td>\n      <td>-73.974785</td>\n      <td>40.750618</td>\n      <td>1</td>\n      <td>12.0</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>3.25</td>\n      <td>0</td>\n      <td>0.3</td>\n      <td>17.05</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2015-01-10 20:33:38</td>\n      <td>2015-01-10 20:53:28</td>\n      <td>1</td>\n      <td>3.30</td>\n      <td>-74.001648</td>\n      <td>40.724243</td>\n      <td>1</td>\n      <td>N</td>\n      <td>-73.994415</td>\n      <td>40.759109</td>\n      <td>1</td>\n      <td>14.5</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>2.00</td>\n      <td>0</td>\n      <td>0.3</td>\n      <td>17.80</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>2015-01-10 20:33:38</td>\n      <td>2015-01-10 20:43:41</td>\n      <td>1</td>\n      <td>1.80</td>\n      <td>-73.963341</td>\n      <td>40.802788</td>\n      <td>1</td>\n      <td>N</td>\n      <td>-73.951820</td>\n      <td>40.824413</td>\n      <td>2</td>\n      <td>9.5</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0.3</td>\n      <td>10.80</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>2015-01-10 20:33:39</td>\n      <td>2015-01-10 20:35:31</td>\n      <td>1</td>\n      <td>0.50</td>\n      <td>-74.009087</td>\n      <td>40.713818</td>\n      <td>1</td>\n      <td>N</td>\n      <td>-74.004326</td>\n      <td>40.719986</td>\n      <td>2</td>\n      <td>3.5</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0.3</td>\n      <td>4.80</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>2015-01-10 20:33:39</td>\n      <td>2015-01-10 20:52:58</td>\n      <td>1</td>\n      <td>3.00</td>\n      <td>-73.971176</td>\n      <td>40.762428</td>\n      <td>1</td>\n      <td>N</td>\n      <td>-74.004181</td>\n      <td>40.742653</td>\n      <td>2</td>\n      <td>15.0</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0.3</td>\n      <td>16.30</td>\n    </tr>\n  </tbody>\n</table>\n\n<h2 id=\"train-on-a-single-machine\">Train on a single machine</h2>\n\n<p>To start lets go through the standard Scikit Learn fit/predict/score cycle with\nthis small bit of data on a single machine.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">sklearn.ensemble</span> <span class=\"kn\">import</span> <span class=\"n\">RandomForestClassifier</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.cross_validation</span> <span class=\"kn\">import</span> <span class=\"n\">train_test_split</span>\n\n<span class=\"n\">df_train</span><span class=\"p\">,</span> <span class=\"n\">df_test</span> <span class=\"o\">=</span> <span class=\"n\">train_test_split</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">)</span>\n\n<span class=\"n\">columns</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s\">'trip_distance'</span><span class=\"p\">,</span> <span class=\"s\">'pickup_longitude'</span><span class=\"p\">,</span> <span class=\"s\">'pickup_latitude'</span><span class=\"p\">,</span>\n           <span class=\"s\">'dropoff_longitude'</span><span class=\"p\">,</span> <span class=\"s\">'dropoff_latitude'</span><span class=\"p\">,</span> <span class=\"s\">'payment_type'</span><span class=\"p\">,</span>\n           <span class=\"s\">'fare_amount'</span><span class=\"p\">,</span> <span class=\"s\">'mta_tax'</span><span class=\"p\">,</span> <span class=\"s\">'tip_amount'</span><span class=\"p\">,</span> <span class=\"s\">'tolls_amount'</span><span class=\"p\">]</span>\n\n<span class=\"n\">est</span> <span class=\"o\">=</span> <span class=\"n\">RandomForestClassifier</span><span class=\"p\">(</span><span class=\"n\">n_estimators</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">)</span>\n<span class=\"n\">est</span><span class=\"p\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">df_train</span><span class=\"p\">[</span><span class=\"n\">columns</span><span class=\"p\">],</span> <span class=\"n\">df_train</span><span class=\"p\">.</span><span class=\"n\">passenger_count</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>This builds a <code class=\"language-plaintext highlighter-rouge\">RandomForestClassifer</code> with four decision trees and then trains\nit against the numeric columns in the data, trying to predict the\n<code class=\"language-plaintext highlighter-rouge\">passenger_count</code> column.  It takes around 10 seconds to train on a single\ncore. We now see how well we do on the holdout testing data:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">est</span><span class=\"p\">.</span><span class=\"n\">score</span><span class=\"p\">(</span><span class=\"n\">df_test</span><span class=\"p\">[</span><span class=\"n\">columns</span><span class=\"p\">],</span> <span class=\"n\">df_test</span><span class=\"p\">.</span><span class=\"n\">passenger_count</span><span class=\"p\">)</span>\n<span class=\"mf\">0.65808188654721012</span>\n</code></pre></div></div>\n\n<p>This 65% accuracy is actually pretty poor.  About 70% of the rides in NYC have\na single passenger, so the model of “always guess one” would out-perform our\nfancy random forest.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">sklearn.metrics</span> <span class=\"kn\">import</span> <span class=\"n\">accuracy_score</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"n\">np</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">accuracy_score</span><span class=\"p\">(</span><span class=\"n\">df_test</span><span class=\"p\">.</span><span class=\"n\">passenger_count</span><span class=\"p\">,</span>\n<span class=\"p\">...</span>                <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">ones_like</span><span class=\"p\">(</span><span class=\"n\">df_test</span><span class=\"p\">.</span><span class=\"n\">passenger_count</span><span class=\"p\">))</span>\n<span class=\"mf\">0.70669390028780987</span>\n</code></pre></div></div>\n\n<p>This is where my ignorance in machine learning really\nkills us.  There is likely a simple way to improve this.  However, because I’m\nmore interested in showing how to build distributed computations with Dask than\nin actually doing machine learning I’m going to go ahead with this naive\napproach.  Spoiler alert: we’re going to do a lot of computation and still not\nbeat the “always guess one” strategy.</p>\n\n<h2 id=\"fit-across-the-cluster-with-executormap\">Fit across the cluster with executor.map</h2>\n\n<p>First we build a function that does just what we did before, builds a random\nforest and then trains it on a dataframe.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">def</span> <span class=\"nf\">fit</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">):</span>\n    <span class=\"n\">est</span> <span class=\"o\">=</span> <span class=\"n\">RandomForestClassifier</span><span class=\"p\">(</span><span class=\"n\">n_estimators</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">)</span>\n    <span class=\"n\">est</span><span class=\"p\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">[</span><span class=\"n\">columns</span><span class=\"p\">],</span> <span class=\"n\">df</span><span class=\"p\">.</span><span class=\"n\">passenger_count</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">est</span>\n</code></pre></div></div>\n\n<p>Second we call this function on all of our training dataframes on the cluster\nusing the standard <code class=\"language-plaintext highlighter-rouge\">e.map(function, sequence)</code> function.  This sends out many\nsmall tasks for the cluster to run.  We use all but the last dataframe for\ntraining data and hold out the last dataframe for testing.  There are more\nprincipled ways to do this, but again we’re going to charge ahead here.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">train</span> <span class=\"o\">=</span> <span class=\"n\">dfs</span><span class=\"p\">[:</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n<span class=\"n\">test</span> <span class=\"o\">=</span> <span class=\"n\">dfs</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n\n<span class=\"n\">estimators</span> <span class=\"o\">=</span> <span class=\"n\">e</span><span class=\"p\">.</span><span class=\"nb\">map</span><span class=\"p\">(</span><span class=\"n\">fit</span><span class=\"p\">,</span> <span class=\"n\">train</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>This takes around two minutes to train on all of the 177 dataframes and now we\nhave 177 independent estimators, each capable of guessing how many passengers a\nparticular ride had.  There is relatively little overhead in this computation.</p>\n\n<h2 id=\"predict-on-testing-data\">Predict on testing data</h2>\n\n<p>Recall that we kept separate a future, <code class=\"language-plaintext highlighter-rouge\">test</code>, that points to a Pandas dataframe on\nthe cluster that was not used to train any of our 177 estimators.  We’re going\nto replicate this dataframe across all workers on the cluster and then ask each\nestimator to predict the number of passengers for each ride in this dataset.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">e</span><span class=\"p\">.</span><span class=\"n\">replicate</span><span class=\"p\">([</span><span class=\"n\">test</span><span class=\"p\">],</span> <span class=\"n\">n</span><span class=\"o\">=</span><span class=\"mi\">48</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">predict</span><span class=\"p\">(</span><span class=\"n\">est</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"n\">est</span><span class=\"p\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">[</span><span class=\"n\">columns</span><span class=\"p\">])</span>\n\n<span class=\"n\">predictions</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">e</span><span class=\"p\">.</span><span class=\"n\">submit</span><span class=\"p\">(</span><span class=\"n\">predict</span><span class=\"p\">,</span> <span class=\"n\">est</span><span class=\"p\">,</span> <span class=\"n\">test</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">est</span> <span class=\"ow\">in</span> <span class=\"n\">estimators</span><span class=\"p\">]</span>\n</code></pre></div></div>\n\n<p>Here we used the <code class=\"language-plaintext highlighter-rouge\">executor.submit(function, *args, **kwrags)</code> function in a\nlist comprehension to individually launch many tasks.  The scheduler determines\nwhen and where to run these tasks for optimal computation time and minimal data\ntransfer.  As with all functions, this returns futures that we can use to\ncollect data if we want in the future.</p>\n\n<p><em>Developers note: we explicitly replicate here in order to take advantage of\nefficient tree-broadcasting algorithms.  This is purely a performance\nconsideration, everything would have worked fine without this, but the explicit\nbroadcast turns a 30s communication+computation into a 2s\ncommunication+computation.</em></p>\n\n<h2 id=\"aggregate-predictions-by-majority-vote\">Aggregate predictions by majority vote</h2>\n\n<p>For each estimator we now have an independent prediction of the passenger\ncounts for all of the rides in our test data.  In other words for each ride we\nhave 177 different opinions on how many passengers were in the cab.  By\naveraging these opinions together we hope to achieve a more accurate consensus\nopinion.</p>\n\n<p>For example, consider the first four prediction arrays:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">a_few_predictions</span> <span class=\"o\">=</span> <span class=\"n\">e</span><span class=\"p\">.</span><span class=\"n\">gather</span><span class=\"p\">(</span><span class=\"n\">predictions</span><span class=\"p\">[:</span><span class=\"mi\">4</span><span class=\"p\">])</span>  <span class=\"c1\"># remote futures -&gt; local arrays\n</span><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">a_few_predictions</span>\n<span class=\"p\">[</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"p\">...,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]),</span>\n <span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"p\">...,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]),</span>\n <span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"p\">...,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]),</span>\n <span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"p\">...,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">])]</span>\n</code></pre></div></div>\n\n<p>For the first ride/column we see that three of the four predictions are for a\nsingle passenger while one prediction disagrees and is for two passengers.  We\ncreate a consensus opinion by taking the mode of the stacked arrays:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">scipy.stats</span> <span class=\"kn\">import</span> <span class=\"n\">mode</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"n\">np</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">mymode</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">arrays</span><span class=\"p\">):</span>\n    <span class=\"n\">array</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">stack</span><span class=\"p\">(</span><span class=\"n\">arrays</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">mode</span><span class=\"p\">(</span><span class=\"n\">array</span><span class=\"p\">)[</span><span class=\"mi\">0</span><span class=\"p\">][</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">mymode</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">a_few_predictions</span><span class=\"p\">)</span>\n<span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"p\">...,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">])</span>\n</code></pre></div></div>\n\n<p>And so when we average these four prediction arrays together we see that the\nmajority opinion of one passenger dominates for all of the six rides visible\nhere.</p>\n\n<h2 id=\"tree-reduction\">Tree Reduction</h2>\n\n<p>We could call our <code class=\"language-plaintext highlighter-rouge\">mymode</code> function on all of our predictions like this:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">mode_prediction</span> <span class=\"o\">=</span> <span class=\"n\">e</span><span class=\"p\">.</span><span class=\"n\">submit</span><span class=\"p\">(</span><span class=\"n\">mymode</span><span class=\"p\">,</span> <span class=\"o\">*</span><span class=\"n\">predictions</span><span class=\"p\">)</span>  <span class=\"c1\"># this doesn't scale well\n</span></code></pre></div></div>\n\n<p>Unfortunately this would move all of our results to a single machine to compute\nthe mode there.  This might swamp that single machine.</p>\n\n<p>Instead we batch our predictions into groups of size 10, average each group,\nand then repeat the process with the smaller set of predictions until we have\nonly one left.  This sort of multi-step reduction is called a tree reduction.\nWe can write it up with a couple nested loops and <code class=\"language-plaintext highlighter-rouge\">executor.submit</code>.  This is\nonly an approximation of the mode, but it’s a much more scalable computation.\nThis finishes in about 1.5 seconds.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">toolz</span> <span class=\"kn\">import</span> <span class=\"n\">partition_all</span>\n\n<span class=\"k\">while</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">predictions</span><span class=\"p\">)</span> <span class=\"o\">&gt;</span> <span class=\"mi\">1</span><span class=\"p\">:</span>\n    <span class=\"n\">predictions</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">e</span><span class=\"p\">.</span><span class=\"n\">submit</span><span class=\"p\">(</span><span class=\"n\">mymode</span><span class=\"p\">,</span> <span class=\"o\">*</span><span class=\"n\">chunk</span><span class=\"p\">)</span>\n                   <span class=\"k\">for</span> <span class=\"n\">chunk</span> <span class=\"ow\">in</span> <span class=\"n\">partition_all</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">predictions</span><span class=\"p\">)]</span>\n\n<span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">e</span><span class=\"p\">.</span><span class=\"n\">gather</span><span class=\"p\">(</span><span class=\"n\">predictions</span><span class=\"p\">)[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">result</span>\n<span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"p\">...,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">])</span>\n</code></pre></div></div>\n\n<h2 id=\"final-score\">Final Score</h2>\n\n<p>Finally, after completing all of our work on our cluster we can see how well\nour distributed random forest algorithm does.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">accuracy_score</span><span class=\"p\">(</span><span class=\"n\">result</span><span class=\"p\">,</span> <span class=\"n\">test</span><span class=\"p\">.</span><span class=\"n\">result</span><span class=\"p\">().</span><span class=\"n\">passenger_count</span><span class=\"p\">)</span>\n<span class=\"mf\">0.67061974451423045</span>\n</code></pre></div></div>\n\n<p>Still worse than the naive “always guess one” strategy.  This just goes to show\nthat, no matter how sophisticated your Big Data solution is, there is no\nsubstitute for common sense and a little bit of domain expertise.</p>\n\n<h2 id=\"what-didnt-work\">What didn’t work</h2>\n\n<p>As always I’ll have a section like this that honestly says what doesn’t work\nwell and what I would have done with more time.</p>\n\n<ul>\n  <li>Clearly this would have benefited from more machine learning knowledge.\nWhat would have been a good approach for this problem?</li>\n  <li>I’ve been thinking a bit about memory management of replicated data on the\ncluster.  In this exercise we specifically replicated out the test data.\nEverything would have worked fine without this step but it would have been\nmuch slower as every worker gathered data from the single worker that\noriginally had the test dataframe.  Replicating data is great until you\nstart filling up distributed RAM.  It will be interesting to think of\npolicies about when to start cleaning up redundant data and when to keep it\naround.</li>\n  <li>Several people from both open source users and Continuum customers have\nasked about a general Dask library for machine learning, something akin to\nSpark’s MLlib.  Ideally a future Dask.learn module would leverage\nScikit-Learn in the same way that Dask.dataframe leverages Pandas.  It’s\nnot clear how to cleanly break up and parallelize Scikit-Learn algorithms.</li>\n</ul>\n\n<h2 id=\"conclusion\">Conclusion</h2>\n\n<p>This blogpost gives a concrete example using basic task submission with\n<code class=\"language-plaintext highlighter-rouge\">executor.map</code> and <code class=\"language-plaintext highlighter-rouge\">executor.submit</code> to build a non-trivial computation.  This\napproach is straightforward and not restrictive.  Personally this interface\nexcites me more than collections like Dask.dataframe; there is a lot of freedom\nin arbitrary task submission.</p>\n\n<h2 id=\"links\">Links</h2>\n\n<ul>\n  <li><a href=\"https://gist.github.com/mrocklin/9f5720d8658e5f2f66666815b1f03f00\">Notebook</a></li>\n  <li><a href=\"https://www.youtube.com/watch?v=FkPlEqB8AnE&amp;list=PLRtz5iA93T4PQvWuoMnIyEIz1fXiJ5Pri&amp;index=11\">Video</a></li>\n  <li><a href=\"https://distributed.readthedocs.org/en/latest/\">distributed</a></li>\n</ul>"
}