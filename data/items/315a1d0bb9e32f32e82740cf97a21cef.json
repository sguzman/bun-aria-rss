{
  "title": "Learning Control by Iterative Inversion. (arXiv:2211.01724v1 [cs.LG])",
  "link": "http://arxiv.org/abs/2211.01724",
  "description": "<p>We formulate learning for control as an $\\textit{inverse problem}$ --\ninverting a dynamical system to give the actions which yield desired behavior.\nThe key challenge in this formulation is a $\\textit{distribution shift}$ -- the\nlearning agent only observes the forward mapping (its actions' consequences) on\ntrajectories that it can execute, yet must learn the inverse mapping for\ninputs-outputs that correspond to a different, desired behavior. We propose a\ngeneral recipe for inverse problems with a distribution shift that we term\n$\\textit{iterative inversion}$ -- learn the inverse mapping under the current\ninput distribution (policy), then use it on the desired output samples to\nobtain new inputs, and repeat. As we show, iterative inversion can converge to\nthe desired inverse mapping, but under rather strict conditions on the mapping\nitself.\n</p>\n<p>We next apply iterative inversion to learn control. Our input is a set of\ndemonstrations of desired behavior, given as video embeddings of trajectories,\nand our method iteratively learns to imitate trajectories generated by the\ncurrent policy, perturbed by random exploration noise. We find that constantly\nadding the demonstrated trajectory embeddings $\\textit{as input}$ to the policy\nwhen generating trajectories to imitate, a-la iterative inversion, steers the\nlearning towards the desired trajectory distribution. To the best of our\nknowledge, this is the first exploration of learning control from the viewpoint\nof inverse problems, and our main advantage is simplicity -- we do not require\nrewards, and only employ supervised learning, which easily scales to\nstate-of-the-art trajectory embedding techniques and policy representations.\nWith a VQ-VAE embedding, and a transformer-based policy, we demonstrate\nnon-trivial continuous control on several tasks. We also report improved\nperformance on imitating diverse behaviors compared to reward based methods.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Leibovich_G/0/1/0/all/0/1\">Gal Leibovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacob_G/0/1/0/all/0/1\">Guy Jacob</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avner_O/0/1/0/all/0/1\">Or Avner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novik_G/0/1/0/all/0/1\">Gal Novik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamar_A/0/1/0/all/0/1\">Aviv Tamar</a>"
}