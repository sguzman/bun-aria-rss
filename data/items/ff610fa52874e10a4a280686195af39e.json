{
  "title": "Clenshaw Graph Neural Networks. (arXiv:2210.16508v2 [cs.LG] UPDATED)",
  "link": "http://arxiv.org/abs/2210.16508",
  "description": "<p>Graph Convolutional Networks (GCNs), which use a message-passing paradigm\nwith stacked convolution layers, are foundational methods for learning graph\nrepresentations. Recent GCN models use various residual connection techniques\nto alleviate the model degradation problem such as over-smoothing and gradient\nvanishing. Existing residual connection techniques, however, fail to make\nextensive use of underlying graph structure as in the graph spectral domain,\nwhich is critical for obtaining satisfactory results on heterophilic graphs. In\nthis paper, we introduce ClenshawGCN, a GNN model that employs the Clenshaw\nSummation Algorithm to enhance the expressiveness of the GCN model. ClenshawGCN\nequips the standard GCN model with two straightforward residual modules: the\nadaptive initial residual connection and the negative second-order residual\nconnection. We show that by adding these two residual modules, ClenshawGCN\nimplicitly simulates a polynomial filter under the Chebyshev basis, giving it\nat least as much expressive power as polynomial spectral GNNs. In addition, we\nconduct comprehensive experiments to demonstrate the superiority of our model\nover spatial and spectral GNN models.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuhe Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhewei Wei</a>"
}