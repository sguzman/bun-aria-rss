{
  "title": "On the Informativeness of Supervision Signals. (arXiv:2211.01407v1 [cs.LG])",
  "link": "http://arxiv.org/abs/2211.01407",
  "description": "<p>Learning transferable representations by training a classifier is a\nwell-established technique in deep learning (e.g., ImageNet pretraining), but\nit remains an open theoretical question why this kind of task-specific\npre-training should result in ''good'' representations that actually capture\nthe underlying structure of the data. We conduct an information-theoretic\nanalysis of several commonly-used supervision signals from contrastive learning\nand classification to determine how they contribute to representation learning\nperformance and how the dynamics of learning are affected by training\nparameters such as the number of labels, classes, and dimensions in the\ntraining dataset. We validate these results empirically in a series of\nsimulations and conduct a cost-benefit analysis to establish a tradeoff curve\nthat enables users to optimize the cost of supervising representation learning\non their own datasets.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Sucholutsky_I/0/1/0/all/0/1\">Ilia Sucholutsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marjieh_R/0/1/0/all/0/1\">Raja Marjieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacoby_N/0/1/0/all/0/1\">Nori Jacoby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>"
}