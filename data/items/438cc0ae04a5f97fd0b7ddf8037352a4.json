{
  "title": "Biased Benchmarks",
  "link": "",
  "updated": "2017-03-09T00:00:00+00:00",
  "id": "https://mrocklin.github.io/blog/work/2017/03/09/biased-benchmarks",
  "content": "<p><em>This work is supported by <a href=\"http://continuum.io\">Continuum Analytics</a>\nthe <a href=\"http://www.darpa.mil/program/XDATA\">XDATA Program</a>\nand the Data Driven Discovery Initiative from the <a href=\"https://www.moore.org/\">Moore\nFoundation</a>.</em></p>\n\n<h2 id=\"summary\">Summary</h2>\n\n<p>Performing benchmarks to compare software is surprisingly difficult to do\nfairly, even assuming the best intentions of the author.  Technical developers\ncan fall victim to a few natural human failings:</p>\n\n<ol>\n  <li>We judge other projects by our own objectives rather than the objectives under which that project was developed</li>\n  <li>We fail to use other projects with the same expertise that we have for our own</li>\n  <li>We naturally gravitate towards cases at which our project excels</li>\n  <li>We improve our software during the benchmarking process</li>\n  <li>We don’t release negative results</li>\n</ol>\n\n<p>We discuss each of these failings in the context of current benchmarks I’m\nworking on comparing Dask and Spark Dataframes.</p>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>Last week I started comparing performance between Dask Dataframes (a project\nthat I maintain) and Spark Dataframes (the current standard).  My initial\nresults showed that Dask.dataframes were overall <em>much</em> faster, somewhere like\n5x.</p>\n\n<p>These results were wrong.  They weren’t wrong in a factual sense, and the\nexperiments that I ran were clear and reproducible, but there was so much bias\nin how I selected, set up, and ran those experiments that the end result was\nmisleading.  After checking results myself and then having other experts come\nin and check my results I now see much more sensible numbers.  At the moment\nboth projects are within a factor of two most of the time, with some\ninteresting exceptions either way.</p>\n\n<p>This blogpost outlines the ways in which library authors can fool themselves\nwhen performing benchmarks, using my recent experience as an anecdote.  I hope\nthat this encourages authors to check themselves, and encourages readers to be\nmore critical of numbers that they see in the future.</p>\n\n<p>This problem exists as well in academic research.  For a pop-science rendition\nI recommend <a href=\"http://www.npr.org/sections/money/2016/01/15/463237871/episode-677-the-experiment-experiment\">“The Experiment Experiment” on the Planet Money\nPodcast</a>.</p>\n\n<h2 id=\"skewed-objectives\">Skewed Objectives</h2>\n\n<p><em>Feature X is so important.  I wonder how the competition fares?</em></p>\n\n<p>Every project is developed with different applications in mind and so has\ndifferent strengths and weaknesses.  If we approach a benchmark caring only\nabout our original objectives and dismissing the objectives of the other\nprojects then we’ll very likely trick ourselves.</p>\n\n<p>For example consider reading CSV files.  Dask’s CSV reader is based off of\nPandas’ CSV reader, which was the target of great effort and love; this is\nbecause CSV was so important to the finance community where Pandas grew up.\nSpark’s CSV solution is less awesome, but that’s less about the quality of\nSpark and more a statement about how Spark users tend not to use CSV.  When\nthey use text-based formats they’re much more likely to use line-delimited\nJSON, which is typical in Spark’s common use cases (web diagnostics, click\nlogs, and so on).  Pandas/Dask came from the scientific and finance worlds where CSV\nis king while Spark came from the web world where JSON reigns.</p>\n\n<p>Conversely, Dask.dataframe hasn’t bothered to hook up the <code class=\"language-plaintext highlighter-rouge\">pandas.read_json</code>\nfunction to Dask.dataframe yet.  Surprisingly it rarely comes up.  Both\nprojects can correctly say that the other project’s solution to what they\nconsider the standard text-based file format is less-than-awesome.  Comparing\nperformance here either way will likely lead to misguided conclusions.</p>\n\n<p>So when benchmarking data ingestion maybe we look around a bit, see that both\nclaim to support Parquet well, and use that as the basis for comparison.</p>\n\n<h2 id=\"skewed-experience\">Skewed Experience</h2>\n\n<p><em>Whoa, this other project has a lot of configuration parameters!  Let’s just use\nthe defaults.</em></p>\n\n<p>Software is often easy to set up, but often requires experience to set up\noptimally.  Authors are naturally more adept at setting up their own software\nthan the software of their competition.</p>\n\n<p>My original (and flawed) solution to this was to “just use the defaults” on\nboth projects.  Given my inability to tune Spark (there are several dozen\nparameters) I decided to also not tune Dask and run under default settings.  I\nfigured that this would be a good benchmark not only of the software, but also\non choices for sane defaults, which is a good design principle in itself.</p>\n\n<p>This failed spectacularly because I was making unconscious decisions like the\nsize of machines that I was using for the experiment, CPU/memory ratios, and so on.\nIt turns out that Spark’s defaults are optimized for <em>very small machines</em> (or\nmore likely, small YARN containers) and use only 1GB of memory per executor by\ndefault while Dask is typically run on larger boxes or has the full use of a\nsingle machine in a single shared-memory process.  My standard cluster\nconfigurations were biased towards Dask before I even considered running a\nbenchmark.</p>\n\n<p>Similarly the APIs of software projects are complex and for any given problem\nthere is often both a fast way and a general-but-slow way.  Authors naturally\nchoose the fast way on their own system but inadvertently choose the general\nway that comes up first when reading documentation for the other project.  It\noften takes months of hands-on experience to understand a project well enough\nto definitely say that you’re not doing things in a dumb way.</p>\n\n<p>In both cases I think the only solution is to collaborate with someone that\nprimarily uses the other system.</p>\n\n<h2 id=\"preference-towards-strengths\">Preference towards strengths</h2>\n\n<p><em>Oh hey, we’re doing <strong>really</strong> well here.  This is great!  Let’s dive into this a\nbit more.</em></p>\n\n<p>It feels great to see your project doing well.  This emotional pleasure\nresponse is powerful.  It’s only natural that we pursue that feeling more,\nexploring different aspects of it.  This can skew our writing as well.  We’ll\nfind that we’ve decided to devote 80% of the text to what originally seemed\nlike a small set of features, but which now seems like <em>the main point</em>.</p>\n\n<p>It’s important that we define a set of things we’re going to study ahead of\ntime and then stick to those things.  When we run into cases where our project\nfails we should take that as an opportunity to raise an issue for future\n(though not current) development.</p>\n\n<h2 id=\"tuning-during-experimentation\">Tuning during experimentation</h2>\n\n<p><em>Oh, I know why this is slow.  One sec, let me change something in the code.</em></p>\n\n<p>I’m doing this right now.  Dask dataframe shuffles are generally slower than\nSpark dataframe shuffles.  On numeric data this used to be around a 2x\ndifference, now it’s more like a 1.2x difference (at least on my current\nproblem and machine).  Overall this is great, seeing that another project was\nbeating Dask motivated me to dive in <a href=\"https://github.com/dask/distributed/issues/932\">(see dask/distributed\n#932)</a> and this will result in\na better experience for users in the future.  As a developer this is also how I\noperate.  I define a benchmark, profile my code, identify bottlenecks, and\noptimize.  Business as usual.</p>\n\n<p>However as an author of a comparative benchmark this is also somewhat dishonest;\nI’m not giving the Spark developers the same opportunity to find and fix\nsimilar performance issues in their software before I publish my results.  I’m\nalso giving a biased picture to my readers.  I’ve made all of the pieces that\nI’m going to show off fast while neglecting the others.  Picking benchmarks,\noptimizing the project to make them fast, and then publishing those results\ngives the incorrect impression that the entire project has been optimized to\nthat level.</p>\n\n<h2 id=\"omission\">Omission</h2>\n\n<p><em>So, this didn’t go as planned.  Let’s wait a few months until the next release.</em></p>\n\n<p>There is no motivation to publish negative results.  Unless of course you’ve\njust written a blogpost announcing that you plan to release benchmarks in the\nnear future.  Then you’re really forced to release numbers, even if they’re\nmixed.</p>\n\n<p>That’s ok. Mixed numbers can be informative.  They build trust and\ncommunity.  And we all talk about open source community driven software, so\nthese should be welcome.</p>\n\n<h2 id=\"straight-up-bias\">Straight up bias</h2>\n\n<p><em>Look, we’re not in grad-school any more.  We’ve got to convince companies to\nactually use this stuff.</em></p>\n\n<p>Everything we’ve discussed so far assumes best intentions, and that the author is\nacting in good faith, but falling victim to basic human failings.</p>\n\n<p>However many developers today (including myself) are paid and work for\nfor-profit companies that need to make money.  To an increasing extent making\nthis money depends on community mindshare, which means publishing\nbenchmarks that sway users to our software.  Authors have bosses that\nthey’re trying to impress or the content and tone of an article may be\ninfluenced by people within the company other than the stated author.</p>\n\n<p>I’ve been pretty lucky working with Continuum Analytics (my employer) in that\nthey’ve been pretty hands-off with technical writing.  For other\nemployers that may be reading, we’ve actually had an easier time getting\nbusiness because of the honest tone in these blogposts in some cases.\nPotential clients generally have the sense that we’re trustworthy.</p>\n\n<p>Technical honesty goes a surprisingly long way towards implying technical\nproficiency.</p>"
}