{
  "title": "Amazon SageMaker Automatic Model Tuning now supports grid search",
  "link": "https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-automatic-model-tuning-now-supports-grid-search/",
  "dc:creator": "Doug Mbaya",
  "pubDate": "Wed, 26 Oct 2022 18:51:17 +0000",
  "category": [
    "Advanced (300)",
    "Amazon SageMaker",
    "Artificial Intelligence"
  ],
  "guid": "09a93671454db4b905711d25b1aeaea1f3ae07ff",
  "description": "Today Amazon SageMaker announced the support of Grid search for automatic model tuning, providing users with an additional strategy to find the best hyperparameter configuration for your model. Amazon SageMaker automatic model tuning finds the best version of a model by running many training jobs on your dataset using a range of hyperparameters that you […]",
  "content:encoded": "<p>Today <a href=\"https://aws.amazon.com/sagemaker/\">Amazon SageMaker</a> announced the support of Grid search for <a href=\"https://aws.amazon.com/sagemaker/automatic-model-tuning/\">automatic model tuning</a>, providing users with an additional strategy to find the best hyperparameter configuration for your model.</p> \n<p>Amazon SageMaker automatic model tuning finds the best version of a model by running many training jobs on your dataset using a <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-define-metrics.html\">range</a> of hyperparameters that you specify. Then it chooses the hyperparameter values that result in a model that performs the best, as measured by a <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-define-metrics.html\">metric</a> of your choice.</p> \n<p>To find the best hyperparameters values for your model, Amazon SageMaker automatic model tuning supports multiple strategies, including&nbsp;<a class=\"c-link\" href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html#automatic-tuning-bayesian-search.title\" target=\"_blank\" rel=\"noopener noreferrer\" data-stringify-link=\"https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html#automatic-tuning-bayesian-search.title\" data-sk=\"tooltip_parent\" data-remove-tab-index=\"true\">Bayesian</a>&nbsp;(default),&nbsp;<a class=\"c-link\" href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html#automatic-tuning-random-search\" target=\"_blank\" rel=\"noopener noreferrer\" data-stringify-link=\"https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html#automatic-tuning-random-search\" data-sk=\"tooltip_parent\" data-remove-tab-index=\"true\">Random</a>&nbsp;search, and&nbsp;<a class=\"c-link\" href=\"https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-automatic-model-tuning-now-provides-up-to-three-times-faster-hyperparameter-tuning-with-hyperband/\" target=\"_blank\" rel=\"noopener noreferrer\" data-stringify-link=\"https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-automatic-model-tuning-now-provides-up-to-three-times-faster-hyperparameter-tuning-with-hyperband/\" data-sk=\"tooltip_parent\" data-remove-tab-index=\"true\">Hyperband</a>.</p> \n<h2>Grid search</h2> \n<p>Grid search exhaustively explores the configurations in the grid of hyperparameters that you define, which allows you to get insights into the most promising hyperparameter configurations in your grid and deterministically reproduce your results across different tuning runs. Grid search gives you more confidence that the entire hyper parameter search space was explored. This benefit comes with a trade-off because it’s computationally more expensive than Bayesian and random search if your main goal is to find the best hyperparameter configuration.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/20/ml-11437-lensimage.png\"><img loading=\"lazy\" class=\"wp-image-44470 size-medium aligncenter\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/20/ml-11437-lensimage-300x283.png\" alt=\"\" width=\"300\" height=\"283\"></a></p> \n<h2>Grid search with Amazon SageMaker</h2> \n<p>In Amazon SageMaker, you use Grid search when your problem requires you to have the optimal hyperparameter combination that maximizes or minimizes your objective metric. A common use case where customer use Grid Search is when model accuracy and reproducibility is more important for your business than the training cost required to obtain it.</p> \n<p>To enable Grid Search in Amazon SageMaker, set the <code>Strategy</code> field to <code>Grid</code> when you create a tuning job, as follows:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-bash\">{\n    \"ParameterRanges\": {...}\n    <strong>\"Strategy\": \"Grid\"</strong>,\n    \"HyperParameterTuningJobObjective\": {...}\n}</code></pre> \n</div> \n<p>Additionally, Grid search requires you to define your search space (Cartesian grid) as a categorical range of discrete values in your job definition using the <code>CategoricalParameterRanges</code> key under the <code>ParameterRanges</code> parameter, as follows:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-bash\">{\n    \"ParameterRanges\": {\n        \"CategoricalParameterRanges\": [\n {\n              <strong>\"Name\": \"eta\", \"Values\": ['0.1', '0.2', '0.3', '0.4', '0.5']</strong>\n            },\n            {\n              <strong>\"Name\": \"alpha\", \"Values\": ['0.1', '0.2']</strong>\n            },\n        ],\n\n    },\n    ...\n}</code></pre> \n</div> \n<p>Note that we don’t specify <code>MaxNumberOfTrainingJobs</code> for Grid search in the job definition because this is determined for you from the number of category combinations. When using Random and Bayesian search, you specify the <code>MaxNumberOfTrainingJobs</code> parameter as a way to control tuning job cost by defining an upper boundary for compute. With Grid search, the value of <code>MaxNumberOfTrainingJobs</code> (now optional) is automatically set as the number of candidates for the grid search in the <a href=\"https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DescribeHyperParameterTuningJob.html\">DescribeHyperParameterTuningJob</a> shape. This allows you to explore your desired grid of hyperparameters exhaustively. Additionally, Grid search job definition only accepts discrete categorical ranges and doesn’t require a continuous or integer ranges definition because each value in the grid is considered discrete.</p> \n<h2>Grid Search experiment</h2> \n<p>In this experiment, given a regression task, we search for the optimal hyperparameters within a search space of 200 hyperparameters, 20 <code>eta</code> and 10 <code>alpha</code> ranging from 0.1 to 1. We use the <a href=\"https://archive.ics.uci.edu/ml/datasets/bank+marketing\">direct marketing dataset</a> to tune a regression model.</p> \n<ul> \n <li><b><i>eta</i></b>: Step size shrinkage used in updates to prevent over-fitting. After each boosting step, you can directly get the weights of new features. The <code>eta</code> parameter actually shrinks the feature weights to make the boosting process more conservative.</li> \n <li><b><i>alpha</i></b>: L1 regularization term on weights. Increasing this value makes models more conservative.</li> \n</ul> \n<table> \n <tbody> \n  <tr> \n   <td><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/20/ml-11437-image002.png\"><img loading=\"lazy\" class=\"alignnone size-full wp-image-44472\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/20/ml-11437-image002.png\" alt=\"\" width=\"528\" height=\"443\"></a></td> \n   <td><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/20/ml-11437-image003.png\"><img loading=\"lazy\" class=\"alignnone size-full wp-image-44471\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/20/ml-11437-image003.png\" alt=\"\" width=\"530\" height=\"456\"></a></td> \n  </tr> \n </tbody> \n</table> \n<p>The chart to the left shows an analysis of the <code>eta</code> hyperparameter in relation to the objective metric and demonstrates how grid search has exhausted the entire search space (grid) in the X axes before returning the best model. Equally, the chart to the right analyzes the two hyperparameters in a single cartesian space to demonstrate that all the points in the grid were picked during tuning.</p> \n<p>The experiment above demonstrates that the exhaustive nature of Grid search guaranties an optimal hyperparameter selection given the defined search space. It also demonstrates that you can reproduce your search result across tuning iterations, all other things being equal.</p> \n<h2>Amazon SageMaker Automatic Model Tuning workflows (AMT)</h2> \n<p>With Amazon SageMaker automatic model tuning, you can find the best version of your model by running training jobs on your dataset with several search strategies, such as Bayesian, Random search, Grid search, and Hyperband. Automatic model tuning allows you to reduce the time to tune a model by automatically searching for the best hyperparameter configuration within the hyperparameter ranges that you specify.</p> \n<p>Now that we have reviewed the advantage of using Grid search in Amazon SageMaker AMT, let’s take a look at AMT’s workflows and understand how it all fits together in SageMaker.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/25/image-85.png\"><img loading=\"lazy\" class=\"alignnone wp-image-44863 size-full\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/25/image-85.png\" alt=\"\" width=\"698\" height=\"1031\"></a></p> \n<h2>Conclusion</h2> \n<p>In this post, we discussed how you can now use the Grid search strategy to find the best model and its ability to deterministically reproduce results across different tuning jobs. We discussed the trade-off when using grid search compared to other strategies, and how it allows you to explore what regions of the hyperparameter spaces are most promising and reproduce your results deterministically.</p> \n<p>To learn more about automatic model tuning, visit the <a href=\"https://aws.amazon.com/sagemaker/automatic-model-tuning/\">product page</a> and <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html\">technical documentation</a>.</p> \n<hr> \n<h3>About the author</h3> \n<p style=\"clear: both\"><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/05/11/dmm3.png\"><img loading=\"lazy\" class=\"wp-image-36333 size-full alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/05/11/dmm3.png\" alt=\"\" width=\"100\" height=\"134\"></a><strong>Doug Mbaya</strong> is a Senior Partner Solution architect with a focus in data and analytics. Doug works closely with AWS partners, helping them integrate data and analytics solutions in the cloud.</p>"
}