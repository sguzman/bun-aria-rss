{
  "title": "A Deep Dive into Transformers with TensorFlow and Keras: Part&#160;2",
  "link": "https://pyimagesearch.com/2022/09/26/a-deep-dive-into-transformers-with-tensorflow-and-keras-part-2/",
  "dc:creator": "Aritra Roy Gosthipaty and Ritwik Raha",
  "pubDate": "Mon, 26 Sep 2022 13:00:00 +0000",
  "category": [
    "Attention",
    "Deep Learning",
    "Transformers",
    "Tutorial",
    "attention",
    "deep learning",
    "transformers",
    "tutorial"
  ],
  "guid": "https://pyimagesearch.com/?p=34098",
  "description": "<p>Table of Contents A Deep Dive into Transformers with TensorFlow and Keras: Part 2 A Brief Recap The Land of Attention Connecting Wires Skip Connections Layer Normalization Feed-Forward Network Positional Encoding Summary Citation Information A Deep Dive into Transformers with&#8230;</p>\n<p>The post <a rel=\"nofollow\" href=\"https://pyimagesearch.com/2022/09/26/a-deep-dive-into-transformers-with-tensorflow-and-keras-part-2/\">A Deep Dive into Transformers with TensorFlow and Keras: Part&nbsp;2</a> appeared first on <a rel=\"nofollow\" href=\"https://pyimagesearch.com\">PyImageSearch</a>.</p>\n",
  "content:encoded": "\n<script src=\"https://fast.wistia.com/embed/medias/zug8vm8eyd.jsonp\" async=\"\"></script><script src=\"https://fast.wistia.com/assets/external/E-v1.js\" async=\"\"></script><div class=\"wistia_responsive_padding\" style=\"padding:56.25% 0 0 0;position:relative;\"><div class=\"wistia_responsive_wrapper\" style=\"height:100%;left:0;position:absolute;top:0;width:100%;\"><div class=\"wistia_embed wistia_async_zug8vm8eyd videoFoam=true\" style=\"height:100%;position:relative;width:100%\"><div class=\"wistia_swatch\" style=\"height:100%;left:0;opacity:0;overflow:hidden;position:absolute;top:0;transition:opacity 200ms;width:100%;\"><img src=\"https://fast.wistia.com/embed/medias/zug8vm8eyd/swatch\" style=\"filter:blur(5px);height:100%;object-fit:contain;width:100%;\" alt=\"\" aria-hidden=\"true\" onload=\"this.parentNode.style.opacity=1;\"></div></div></div></div>\n\n\n\n<hr class=\"wp-block-separator has-alpha-channel-opacity\" id=\"TOC\"/>\n\n\n\n<h2><strong>Table of Contents</strong></h2>\n\n\n\n<div class=\"toc\">\n<ul>\n    <li><a rel=\"noopener\" target=\"_blank\" href=\"#h2BPTitle\">A Deep Dive into Transformers with TensorFlow and Keras: Part 2</a></li>\n        <ul>\n            <li><a rel=\"noopener\" target=\"_blank\" href=\"#h3Recap\">A Brief Recap</a></li>\n            <li><a rel=\"noopener\" target=\"_blank\" href=\"#h3Attention\">The Land of Attention</a></li>\n            <li><a rel=\"noopener\" target=\"_blank\" href=\"#h3Wires\">Connecting Wires</a></li>\n            <li><a rel=\"noopener\" target=\"_blank\" href=\"#h3Connections\">Skip Connections</a></li>\n            <li><a rel=\"noopener\" target=\"_blank\" href=\"#h3Normalization\">Layer Normalization</a></li>\n            <li><a rel=\"noopener\" target=\"_blank\" href=\"#h3FFN\">Feed-Forward Network</a></li>\n            <li><a rel=\"noopener\" target=\"_blank\" href=\"#h3Positional\">Positional Encoding</a></li>\n        </ul>\n    <li><a rel=\"noopener\" target=\"_blank\" href=\"#h2Summary\">Summary</a></li>\n        <ul>\n            <li><a rel=\"noopener\" target=\"_blank\" href=\"#h3Citation\">Citation Information</a></li>\n        </ul>\n</ul>\n</div>\n\n\n\n<hr class=\"wp-block-separator has-alpha-channel-opacity\" id=\"h2BPTitle\"/>\n\n\n\n<h2><a href=\"#TOC\"><strong>A Deep Dive into Transformers with TensorFlow and Keras: Part 2</strong></a></h2>\n\n\n\n<p>In this tutorial, you will learn about the connecting parts of the Transformers architecture that hold together the encoder and decoder.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/Transformers-Part-2-Featured.png\" target=\"_blank\" rel=\"noreferrer noopener\"><img width=\"2501\" height=\"1405\" src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/Transformers-Part-2-Featured.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34770\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/Transformers-Part-2-Featured.png?size=126x71&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/Transformers-Part-2-Featured-300x169.png?lossy=1&strip=1&webp=1 300w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/Transformers-Part-2-Featured.png?size=378x212&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/Transformers-Part-2-Featured.png?size=504x283&lossy=1&strip=1&webp=1 504w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/Transformers-Part-2-Featured.png?size=630x354&lossy=1&strip=1&webp=1 630w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/Transformers-Part-2-Featured-768x431.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/Transformers-Part-2-Featured-1024x575.png?lossy=1&strip=1&webp=1 1024w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/Transformers-Part-2-Featured.png?size=1260x708&lossy=1&strip=1&webp=1 1260w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/Transformers-Part-2-Featured-1536x863.png?lossy=1&strip=1&webp=1 1536w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/Transformers-Part-2-Featured.png?size=1890x1062&lossy=1&strip=1&webp=1 1890w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/Transformers-Part-2-Featured-2048x1151.png?lossy=1&strip=1&webp=1 2048w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/Transformers-Part-2-Featured.png?lossy=1&strip=1&webp=1 2501w\" sizes=\"(max-width: 630px) 100vw, 630px\" /></a></figure></div>\n\n\n<p>This lesson is the 2nd in a 3-part series on <strong>NLP 104</strong>:</p>\n\n\n\n<ol><li><a href=\"https://pyimg.co/8kdj1\" target=\"_blank\" rel=\"noreferrer noopener\"><em>A Deep Dive into Transformers with TensorFlow and Keras: Part 1</em></a></li><li><a href=\"https://pyimg.co/pzu1j\" target=\"_blank\" rel=\"noreferrer noopener\"><strong><em>A Deep Dive into Transformers with TensorFlow and Keras: Part 2</em></strong></a><strong> (today’s tutorial)</strong></li><li><em>A Deep Dive into Transformers with TensorFlow and Keras: Part 3</em></li></ol>\n\n\n\n<p><strong>To learn how the transformers architecture stitches together the multi-head attention layer with other components, </strong><strong><em>just keep reading.</em></strong></p>\n\n\n\n<hr class=\"wp-block-separator has-alpha-channel-opacity\" id=\"h2BPTitle\"/>\n\n\n\n<h2><a href=\"#TOC\"><strong>A Deep Dive into Transformers with TensorFlow and Keras: Part 2</strong></a></h2>\n\n\n\n<hr class=\"wp-block-separator has-alpha-channel-opacity\" id=\"h3Recap\"/>\n\n\n\n<h3><a href=\"#TOC\"></a><a href=\"#TOC\"><strong>A Brief Recap</strong></a></h3>\n\n\n\n<p>In the <a href=\"https://pyimg.co/8kdj1\" target=\"_blank\" rel=\"noreferrer noopener\">previous tutorial</a>, we studied the evolution of attention from its simplest form into Multi-Head Attention as we see today. In <strong>Video 1</strong>, we illustrate how the <strong>Input</strong> matrix is projected into the <strong>Query</strong>, <strong>Key</strong>, and <strong>Value</strong> Matrices.</p>\n\n\n\n<script src=\"https://fast.wistia.com/embed/medias/6xqbszu962.jsonp\" async=\"\"></script><script src=\"https://fast.wistia.com/assets/external/E-v1.js\" async=\"\"></script><div class=\"wistia_responsive_padding\" style=\"padding:56.25% 0 0 0;position:relative;\"><div class=\"wistia_responsive_wrapper\" style=\"height:100%;left:0;position:absolute;top:0;width:100%;\"><div class=\"wistia_embed wistia_async_6xqbszu962 videoFoam=true\" style=\"height:100%;position:relative;width:100%\"><div class=\"wistia_swatch\" style=\"height:100%;left:0;opacity:0;overflow:hidden;position:absolute;top:0;transition:opacity 200ms;width:100%;\"><img src=\"https://fast.wistia.com/embed/medias/6xqbszu962/swatch\" style=\"filter:blur(5px);height:100%;object-fit:contain;width:100%;\" alt=\"\" aria-hidden=\"true\" onload=\"this.parentNode.style.opacity=1;\"></div></div></div></div>\n\n\n\n<p class=\"has-text-align-center has-small-font-size\"><strong>Video 1:</strong> Projecting the Input matrix into the Query, Key, and Value matrices.</p>\n\n\n\n<p>Here the Input matrix is represented through the initial gray matrix <img src='https://929687.smushcdn.com/2633864/wp-content/latex/dd7/dd7536794b63bf90eccfd37f9b147d7f-ffffff-000000-0.png?lossy=1&strip=1&webp=1' alt='I' title='I' class='latex' />. The matrix placed below matrix <img src='https://929687.smushcdn.com/2633864/wp-content/latex/dd7/dd7536794b63bf90eccfd37f9b147d7f-ffffff-000000-0.png?lossy=1&strip=1&webp=1' alt='I' title='I' class='latex' /> is our weight matrix (i.e., <img src='https://929687.smushcdn.com/2633864/wp-content/latex/c1b/c1b0b73f41e2f55f8b5a1244c1d55ef7-ffffff-000000-0.png?lossy=1&strip=1&webp=1' alt='W_Q' title='W_Q' class='latex' /> (red), <img src='https://929687.smushcdn.com/2633864/wp-content/latex/e3f/e3f3072d0e91095bc636881a95c59c6c-ffffff-000000-0.png?lossy=1&strip=1&webp=1' alt='W_K' title='W_K' class='latex' /> (green), and <img src='https://929687.smushcdn.com/2633864/wp-content/latex/459/45946d51ed04e13e572d2cecf735cd1d-ffffff-000000-0.png?lossy=1&strip=1&webp=1' alt='W_V' title='W_V' class='latex' /> (blue), respectively). As the Input is multiplied by these three weight matrices, the Input is projected to produce the Query, Key, and Value matrices, shown with red, green, and blue colors, respectively.</p>\n\n\n\n<hr class=\"wp-block-separator has-alpha-channel-opacity\" id=\"h3Attention\"/>\n\n\n\n<h3><a href=\"#TOC\"><strong>The Land of Attention</strong></a></h3>\n\n\n\n<p>Our three friends, <strong>Query</strong>, <strong>Key</strong>, and <strong>Value</strong>, are the central actors that bring Transformers to life. In <strong>Video 2</strong>, we build an animation that shows how the attention score is <em>computed</em> from the Query, Key, and Value matrices.</p>\n\n\n\n<script src=\"https://fast.wistia.com/embed/medias/ze02weu07i.jsonp\" async=\"\"></script><script src=\"https://fast.wistia.com/assets/external/E-v1.js\" async=\"\"></script><div class=\"wistia_responsive_padding\" style=\"padding:56.25% 0 0 0;position:relative;\"><div class=\"wistia_responsive_wrapper\" style=\"height:100%;left:0;position:absolute;top:0;width:100%;\"><div class=\"wistia_embed wistia_async_ze02weu07i videoFoam=true\" style=\"height:100%;position:relative;width:100%\"><div class=\"wistia_swatch\" style=\"height:100%;left:0;opacity:0;overflow:hidden;position:absolute;top:0;transition:opacity 200ms;width:100%;\"><img src=\"https://fast.wistia.com/embed/medias/ze02weu07i/swatch\" style=\"filter:blur(5px);height:100%;object-fit:contain;width:100%;\" alt=\"\" aria-hidden=\"true\" onload=\"this.parentNode.style.opacity=1;\"></div></div></div></div>\n\n\n\n<p class=\"has-text-align-center has-small-font-size\"><strong><strong>Video 2:</strong></strong> Animation of the Attention module.</p>\n\n\n\n<p>First, the Query and Key matrices are multiplied to arrive at a product term. Let us call this term the <img src='https://929687.smushcdn.com/2633864/wp-content/latex/e3b/e3bab79f69ab6752f83c7ccf04d88a8d-ffffff-000000-0.png?lossy=1&strip=1&webp=1' alt='QK' title='QK' class='latex' /> Product. Next, we scale the product term with <img src='https://929687.smushcdn.com/2633864/wp-content/latex/bca/bcae6ed6c09c3b1bd5c29a1947012f0e-ffffff-000000-0.png?lossy=1&strip=1&webp=1' alt='1/\\sqrt{\\text{dim}}' title='1/\\sqrt{\\text{dim}}' class='latex' />; this was done to prevent the vanishing gradient problem, as explained in our <a href=\"https://pyimagesearch.com/2022/09/05/a-deep-dive-into-transformers-with-tensorflow-and-keras-part-1/#h5Solution\" target=\"_blank\" rel=\"noreferrer noopener\">previous blog post</a>. Finally, the scaled <img src='https://929687.smushcdn.com/2633864/wp-content/latex/e3b/e3bab79f69ab6752f83c7ccf04d88a8d-ffffff-000000-0.png?lossy=1&strip=1&webp=1' alt='QK' title='QK' class='latex' /> Product term is passed through a softmax layer. The resultant output is multiplied by the value matrix to arrive at the final attention layer.</p>\n\n\n\n<p>The entire animation is neatly encapsulated in the attention equation, as shown in <strong>Equation 1</strong>.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/attention-eqn.png\" target=\"_blank\" rel=\"noreferrer noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/attention-eqn-1024x259.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34781\" width=\"600\" height=\"152\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/attention-eqn.png?size=126x32&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/attention-eqn-300x76.png?lossy=1&strip=1&webp=1 300w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/attention-eqn.png?size=378x96&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/attention-eqn.png?size=504x128&lossy=1&strip=1&webp=1 504w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/attention-eqn-768x194.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/attention-eqn-1024x259.png?lossy=1&strip=1&webp=1 1024w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/attention-eqn.png?lossy=1&strip=1&webp=1 1064w\" sizes=\"(max-width: 600px) 100vw, 600px\" /></a><figcaption><strong>Equation 1</strong></figcaption></figure></div>\n\n\n<p>Let’s now talk about a <em>problem</em> with the above module. As we have <a href=\"https://pyimagesearch.com/2022/09/05/a-deep-dive-into-transformers-with-tensorflow-and-keras-part-1/#h4V5\" target=\"_blank\" rel=\"noreferrer noopener\">seen earlier</a>, the attention module can be easily extended to Self-Attention. In a Self-Attention block, the Query, Key, and Value matrices come from the <strong>same source</strong>.</p>\n\n\n\n<p>Intuitively the attention block will attend to <em>each</em> token of the inputs. Take a moment and think about what can possibly go wrong here.</p>\n\n\n\n<p>In NMT (Neural Machine Translation), we predict a target token in the decoder given a set of previously decoded target tokens and the input tokens. </p>\n\n\n\n<p>If the decoder already has access to all the target tokens (both previous and future), it will not <strong>learn to decode</strong>. We would need to mask out the target tokens that are not yet decoded by the decoder. This process is necessary to have an <strong>auto-regressive</strong> decoder.</p>\n\n\n\n<p><a href=\"https://arxiv.org/abs/1706.03762\" target=\"_blank\" rel=\"noreferrer noopener\">Vaswani et al.</a> made a <em>minute</em> modification to this attention pipeline to accomplish masking. Before passing the scaled <img src='https://929687.smushcdn.com/2633864/wp-content/latex/e3b/e3bab79f69ab6752f83c7ccf04d88a8d-ffffff-000000-0.png?lossy=1&strip=1&webp=1' alt='QK' title='QK' class='latex' /> product through a softmax layer, they would mask certain parts of the product with a large number (e.g., negative infinity).</p>\n\n\n\n<p>The change is visualized in <strong>Video 3</strong> and termed Masked Multi-Head Attention or MMHA. </p>\n\n\n\n<script src=\"https://fast.wistia.com/embed/medias/sqzya5h14r.jsonp\" async=\"\"></script><script src=\"https://fast.wistia.com/assets/external/E-v1.js\" async=\"\"></script><div class=\"wistia_responsive_padding\" style=\"padding:56.25% 0 0 0;position:relative;\"><div class=\"wistia_responsive_wrapper\" style=\"height:100%;left:0;position:absolute;top:0;width:100%;\"><div class=\"wistia_embed wistia_async_sqzya5h14r videoFoam=true\" style=\"height:100%;position:relative;width:100%\"><div class=\"wistia_swatch\" style=\"height:100%;left:0;opacity:0;overflow:hidden;position:absolute;top:0;transition:opacity 200ms;width:100%;\"><img src=\"https://fast.wistia.com/embed/medias/sqzya5h14r/swatch\" style=\"filter:blur(5px);height:100%;object-fit:contain;width:100%;\" alt=\"\" aria-hidden=\"true\" onload=\"this.parentNode.style.opacity=1;\"></div></div></div></div>\n\n\n\n<p class=\"has-text-align-center has-small-font-size\"><strong>Video 3:</strong> Animation of the Masked-Multi-Head-Attention module.</p>\n\n\n\n<hr class=\"wp-block-separator has-alpha-channel-opacity\" id=\"h3Wires\"/>\n\n\n\n<h3><a href=\"#TOC\"><strong>Connecting Wires</strong></a></h3>\n\n\n\n<p>At this point, we have covered the most <strong>important</strong> building block of the Transformer architecture, <strong>attention</strong>. But despite what the paper title says (<a href=\"https://arxiv.org/abs/1706.03762\" target=\"_blank\" rel=\"noreferrer noopener\">Attention Is All You Need</a>), only attention cannot create the entire model. We need connecting wires to hold each piece together, as shown in <strong>Figure 1</strong>.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/entire-architecture-1.png\" target=\"_blank\" rel=\"noreferrer noopener\"><img width=\"723\" height=\"1024\" src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/entire-architecture-1-723x1024.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34783\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/entire-architecture-1.png?size=126x178&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/entire-architecture-1-212x300.png?lossy=1&strip=1&webp=1 212w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/entire-architecture-1.png?size=378x535&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/entire-architecture-1.png?size=504x714&lossy=1&strip=1&webp=1 504w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/entire-architecture-1.png?size=630x892&lossy=1&strip=1&webp=1 630w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/entire-architecture-1-723x1024.png?lossy=1&strip=1&webp=1 723w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/entire-architecture-1-768x1088.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/entire-architecture-1-1084x1536.png?lossy=1&strip=1&webp=1 1084w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/entire-architecture-1-1446x2048.png?lossy=1&strip=1&webp=1 1446w\" sizes=\"(max-width: 630px) 100vw, 630px\" /></a><figcaption><strong>Figure 1:</strong> A diagram of the entire Transformer architecture.</figcaption></figure></div>\n\n\n<p>The connecting wires that hold the architecture together are:</p>\n\n\n\n<ul><li>Skip Connection</li><li>Layer Normalization</li><li>Feed-Forward Network</li><li>Positional Encoding</li></ul>\n\n\n\n<hr class=\"wp-block-separator has-alpha-channel-opacity\" id=\"h3Connections\"/>\n\n\n\n<h3><a href=\"#TOC\"><strong>Skip Connections</strong></a></h3>\n\n\n\n<p>Intuitively skip connections introduce the representation of a previous stage into a later stage. This allows the raw representation prior to a sub-layer to be injected along with the output from the sub-layer. The skip connections in the Transformers architecture are highlighted with red arrows in <strong>Figure 2</strong>.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/skip-architecture.png\" target=\"_blank\" rel=\"noreferrer noopener\"><img width=\"723\" height=\"1024\" src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/skip-architecture-723x1024.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34786\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/skip-architecture.png?size=126x178&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/skip-architecture-212x300.png?lossy=1&strip=1&webp=1 212w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/skip-architecture.png?size=378x535&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/skip-architecture.png?size=504x714&lossy=1&strip=1&webp=1 504w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/skip-architecture.png?size=630x892&lossy=1&strip=1&webp=1 630w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/skip-architecture-723x1024.png?lossy=1&strip=1&webp=1 723w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/skip-architecture-768x1088.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/skip-architecture-1084x1536.png?lossy=1&strip=1&webp=1 1084w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/skip-architecture-1446x2048.png?lossy=1&strip=1&webp=1 1446w\" sizes=\"(max-width: 630px) 100vw, 630px\" /></a><figcaption><strong>Figure 2:</strong> A diagram showing only the skip connections in the architecture.</figcaption></figure></div>\n\n\n<p>Now, this raises the question, <strong>why is it important?</strong></p>\n\n\n\n<p>Did you notice that in this blog post series, every time a part of the architecture is mentioned, we bring up the original figure of the entire architecture shown at the very <strong>beginning</strong>? This is because we process the information better when we reference it with information received at an <strong>earlier</strong> stage.</p>\n\n\n\n<p>Well, it turns out the Transformer works the same way. Creating a mechanism to add representations of past stages into later stages of the architecture allows the model to process the information better.</p>\n\n\n\n<hr class=\"wp-block-separator has-alpha-channel-opacity\" id=\"h3Normalization\"/>\n\n\n\n<h3><a href=\"#TOC\"><strong>Layer Normalization</strong></a></h3>\n\n\n\n<p>The official TensorFlow documentation of <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization\" target=\"_blank\" rel=\"noreferrer noopener\">Layer Normalization</a> says:</p>\n\n\n\n<blockquote class=\"wp-block-quote\"><p>Notice that with Layer Normalization, the normalization happens across the axes within each example rather than across different examples in the batch.</p></blockquote>\n\n\n\n<p>Here the input is a matrix <img src='https://929687.smushcdn.com/2633864/wp-content/latex/021/02129bb861061d1a052c592e2dc6b383-ffffff-000000-0.png?lossy=1&strip=1&webp=1' alt='X' title='X' class='latex' /> with <img src='https://929687.smushcdn.com/2633864/wp-content/latex/7b8/7b8b965ad4bca0e41ab51de7b31363a1-ffffff-000000-0.png?lossy=1&strip=1&webp=1' alt='n' title='n' class='latex' /> rows where <img src='https://929687.smushcdn.com/2633864/wp-content/latex/7b8/7b8b965ad4bca0e41ab51de7b31363a1-ffffff-000000-0.png?lossy=1&strip=1&webp=1' alt='n' title='n' class='latex' /> is the number of words in the input sentence. The normalization layer is a row-wise operation that normalizes each row with the same weights.</p>\n\n\n\n<p>In NLP tasks, as well as in the transformer architecture, the requirement is to be able to calculate the statistics across each feature dimension and instance independently. Thus Layer Normalization makes more intuitive sense than Batch Normalization. The highlighted parts in <strong>Figure 3</strong> show the Layer Normalization.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/normalization-architecture.png\" target=\"_blank\" rel=\"noreferrer noopener\"><img width=\"723\" height=\"1024\" src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/normalization-architecture-723x1024.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34788\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/normalization-architecture.png?size=126x178&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/normalization-architecture-212x300.png?lossy=1&strip=1&webp=1 212w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/normalization-architecture.png?size=378x535&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/normalization-architecture.png?size=504x714&lossy=1&strip=1&webp=1 504w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/normalization-architecture.png?size=630x892&lossy=1&strip=1&webp=1 630w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/normalization-architecture-723x1024.png?lossy=1&strip=1&webp=1 723w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/normalization-architecture-768x1088.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/normalization-architecture-1084x1536.png?lossy=1&strip=1&webp=1 1084w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/normalization-architecture-1446x2048.png?lossy=1&strip=1&webp=1 1446w\" sizes=\"(max-width: 630px) 100vw, 630px\" /></a><figcaption><strong>Figure 3:</strong> A diagram showing only the normalization layers in the architecture.</figcaption></figure></div>\n\n\n<p>We would also like to bring forward a <a href=\"https://stats.stackexchange.com/a/505349\" target=\"_blank\" rel=\"noreferrer noopener\">stack exchange discussion</a> thread about why Layer Normalization works in the Transformer architecture.</p>\n\n\n\n<hr class=\"wp-block-separator has-alpha-channel-opacity\" id=\"h3FFN\"/>\n\n\n\n<h3><a href=\"#TOC\"><strong>Feed-Forward Network</strong></a></h3>\n\n\n\n<p>Each encoder and decoder layer consists of a fully connected feed-forward network. The purpose of this layer is straightforward, as shown in <strong>Equation 2</strong>. </p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/ffn.png\" target=\"_blank\" rel=\"noreferrer noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/ffn-1024x119.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34791\" width=\"600\" height=\"69\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/ffn.png?size=120x14&lossy=1&strip=1&webp=1 120w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/ffn.png?size=240x28&lossy=1&strip=1&webp=1 240w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/ffn-300x35.png?lossy=1&strip=1&webp=1 300w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/ffn.png?size=360x41&lossy=1&strip=1&webp=1 360w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/ffn.png?size=480x55&lossy=1&strip=1&webp=1 480w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/ffn.png?size=600x69&lossy=1&strip=1&webp=1 600w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/ffn-768x89.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/ffn-1024x119.png?lossy=1&strip=1&webp=1 1024w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/ffn.png?lossy=1&strip=1&webp=1 1039w\" sizes=\"(max-width: 600px) 100vw, 600px\" /></a><figcaption><strong>Equation 2</strong></figcaption></figure></div>\n\n\n<p>The idea is to project the attention layer output into a higher dimensional space. This essentially means the representations are stretched to a higher dimension, so their details are magnified for the next layer. The application of these layers in the Transformers architecture is shown in <strong>Figure 4</strong>.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/ffn-architecture.png\" target=\"_blank\" rel=\"noreferrer noopener\"><img width=\"723\" height=\"1024\" src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/ffn-architecture-723x1024.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34794\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/ffn-architecture.png?size=126x178&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/ffn-architecture-212x300.png?lossy=1&strip=1&webp=1 212w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/ffn-architecture.png?size=378x535&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/ffn-architecture.png?size=504x714&lossy=1&strip=1&webp=1 504w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/ffn-architecture.png?size=630x892&lossy=1&strip=1&webp=1 630w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/ffn-architecture-723x1024.png?lossy=1&strip=1&webp=1 723w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/ffn-architecture-768x1088.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/ffn-architecture-1084x1536.png?lossy=1&strip=1&webp=1 1084w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/ffn-architecture-1446x2048.png?lossy=1&strip=1&webp=1 1446w\" sizes=\"(max-width: 630px) 100vw, 630px\" /></a><figcaption><strong>Figure 4:</strong> A diagram showing only the feed-forward network (FFN) in the architecture.</figcaption></figure></div>\n\n\n<p>Let us pause here and take a look back at the architecture.</p>\n\n\n\n<ul><li>We studied the encoder and the decoder</li><li>The evolution of attention, as we see in <a href=\"https://arxiv.org/abs/1706.03762\" target=\"_blank\" rel=\"noreferrer noopener\">Vaswani et al.</a></li><li>Skip connections for contextualization</li><li>Layer normalization</li><li>And finally, feed-forward networks</li></ul>\n\n\n\n<p>But there is still a part of the entire architecture that is missing. Can we guess what it is from <strong>Figure 5</strong>?</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/positional-encoding-architecture.png\" target=\"_blank\" rel=\"noreferrer noopener\"><img width=\"723\" height=\"1024\" src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/positional-encoding-architecture-723x1024.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34796\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/positional-encoding-architecture.png?size=126x178&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/positional-encoding-architecture-212x300.png?lossy=1&strip=1&webp=1 212w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/positional-encoding-architecture.png?size=378x535&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/positional-encoding-architecture.png?size=504x714&lossy=1&strip=1&webp=1 504w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/positional-encoding-architecture.png?size=630x892&lossy=1&strip=1&webp=1 630w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/positional-encoding-architecture-723x1024.png?lossy=1&strip=1&webp=1 723w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/positional-encoding-architecture-768x1088.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/positional-encoding-architecture-1084x1536.png?lossy=1&strip=1&webp=1 1084w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/positional-encoding-architecture-1446x2048.png?lossy=1&strip=1&webp=1 1446w\" sizes=\"(max-width: 630px) 100vw, 630px\" /></a><figcaption><strong>Figure 5:</strong> Can you identify the missing piece of the puzzle?</figcaption></figure></div>\n\n\n<hr class=\"wp-block-separator has-alpha-channel-opacity\" id=\"h3Positional\"/>\n\n\n\n<h3><a href=\"#TOC\"><strong>Positional Encoding</strong></a></h3>\n\n\n\n<p>Now, before we understand positional encoding and why it has been introduced in the architecture, let us first look at what our architecture achieves in contrast to sequential models like RNN and LSTM.</p>\n\n\n\n<p>The transformer can attend to parts of the input tokens. The encoder and decoder units are built out of these attention blocks, along with non-linear layers, layer normalization, and skip connections.</p>\n\n\n\n<p>But RNNs and other sequential models had something that the architecture still lacks. It is to understand the <strong>order</strong> of the data. So how do we introduce the order of the data?</p>\n\n\n\n<ol><li>We can add attention to a sequential model, but then it will be similar to <a href=\"https://pyimg.co/kf8ma\" target=\"_blank\" rel=\"noreferrer noopener\">Bahdanau’s</a> or <a href=\"https://pyimg.co/tpf3l\" target=\"_blank\" rel=\"noreferrer noopener\">Luong’s</a> attention which we have already studied. Furthermore, adding a sequential model will beat the purpose of using <em>only attention units</em> for the task at hand.</li><li>We can also inject the data order into the model along with the input and target tokens. </li></ol>\n\n\n\n<p><a href=\"https://arxiv.org/abs/1706.03762\" target=\"_blank\" rel=\"noreferrer noopener\">Vaswani et al.</a> opted for the 2nd option. This meant encoding the position of each token and injecting it with the input. So, now there are a couple of ways we can achieve positional encoding.</p>\n\n\n\n<p>The first is to simply encode everything in a <strong>binary encoding scheme</strong>, as shown in <strong>Figure 6</strong>. Consider the following code snippet where we visualize the binary encoding of a range of numbers.</p>\n\n\n\n<ul><li><code data-enlighter-language=\"python\" class=\"EnlighterJSRAW\">n</code> corresponds to the range of <code data-enlighter-language=\"python\" class=\"EnlighterJSRAW\">0-n</code> positions</li><li><code data-enlighter-language=\"python\" class=\"EnlighterJSRAW\">dims</code> corresponds to the dimension to which each position is encoded</li></ul>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/binary-snippet.png\" target=\"_blank\" rel=\"noreferrer noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/binary-snippet.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34799\" width=\"497\" height=\"500\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/binary-snippet-150x150.png?lossy=1&strip=1&webp=1 150w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/binary-snippet-298x300.png?lossy=1&strip=1&webp=1 298w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/binary-snippet.png?size=378x380&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/binary-snippet-768x773.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/binary-snippet.png?lossy=1&strip=1&webp=1 920w\" sizes=\"(max-width: 497px) 100vw, 497px\" /></a><figcaption><strong>Figure 6:</strong> Code Snippet showing binary encoding.</figcaption></figure></div>\n\n\n<p>The output of the code snippet in <strong>Figure 6</strong> is shown in <strong>Figure 7</strong>.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/binary-pos.png\" target=\"_blank\" rel=\"noreferrer noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/binary-pos.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34800\" width=\"400\" height=\"400\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/binary-pos.png?size=80x80&lossy=1&strip=1&webp=1 80w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/binary-pos-150x150.png?lossy=1&strip=1&webp=1 150w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/binary-pos.png?lossy=1&strip=1&webp=1 265w\" sizes=\"(max-width: 400px) 100vw, 400px\" /></a><figcaption><strong><strong><strong>Figure 7:</strong></strong></strong> Binary encoding output from the code snippet.</figcaption></figure></div>\n\n\n<p><a href=\"https://arxiv.org/abs/1706.03762\" target=\"_blank\" rel=\"noreferrer noopener\">Vaswani et al.</a>, however, introduced a sine and cosine function that encodes the sequence position according to <strong>Equations 3 and 4</strong>:</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/sin.png\" target=\"_blank\" rel=\"noreferrer noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/sin.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34802\" width=\"600\" height=\"253\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/sin.png?size=126x53&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/sin-300x127.png?lossy=1&strip=1&webp=1 300w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/sin.png?size=378x159&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/sin.png?size=504x213&lossy=1&strip=1&webp=1 504w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/sin-768x324.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/sin.png?lossy=1&strip=1&webp=1 799w\" sizes=\"(max-width: 600px) 100vw, 600px\" /></a><figcaption><strong>Equation 3</strong></figcaption></figure></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/cos.png\" target=\"_blank\" rel=\"noreferrer noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/cos.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34804\" width=\"600\" height=\"235\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/cos.png?size=126x49&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/cos-300x117.png?lossy=1&strip=1&webp=1 300w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/cos.png?size=378x148&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/cos.png?size=504x197&lossy=1&strip=1&webp=1 504w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/cos-768x300.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/cos.png?lossy=1&strip=1&webp=1 862w\" sizes=\"(max-width: 600px) 100vw, 600px\" /></a><figcaption><strong>Equation 4</strong></figcaption></figure></div>\n\n\n<p>Here, <code data-enlighter-language=\"python\" class=\"EnlighterJSRAW\">pos</code> refers to the position, and <code data-enlighter-language=\"python\" class=\"EnlighterJSRAW\">i</code> is the dimension. <strong>Figure 8</strong> is the code to implement the equations.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/sinusoid-snippet.png\" target=\"_blank\" rel=\"noreferrer noopener\"><img width=\"796\" height=\"1024\" src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/sinusoid-snippet-796x1024.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34805\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/sinusoid-snippet.png?size=126x162&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/sinusoid-snippet-233x300.png?lossy=1&strip=1&webp=1 233w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/sinusoid-snippet.png?size=378x486&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/sinusoid-snippet.png?size=504x648&lossy=1&strip=1&webp=1 504w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/sinusoid-snippet.png?size=630x810&lossy=1&strip=1&webp=1 630w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/sinusoid-snippet-768x988.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/sinusoid-snippet-796x1024.png?lossy=1&strip=1&webp=1 796w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/sinusoid-snippet-1194x1536.png?lossy=1&strip=1&webp=1 1194w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/sinusoid-snippet-1591x2048.png?lossy=1&strip=1&webp=1 1591w\" sizes=\"(max-width: 630px) 100vw, 630px\" /></a><figcaption><strong>Figure 8:</strong> Code snippet showing sinusoidal encoding.</figcaption></figure></div>\n\n\n<p>The output of the code in <strong>Figure 8</strong> is shown in <strong>Figure 9</strong>.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/sin-pos.png\" target=\"_blank\" rel=\"noreferrer noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/sin-pos.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34807\" width=\"400\" height=\"400\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/sin-pos.png?size=80x80&lossy=1&strip=1&webp=1 80w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/sin-pos-150x150.png?lossy=1&strip=1&webp=1 150w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/sin-pos.png?lossy=1&strip=1&webp=1 265w\" sizes=\"(max-width: 400px) 100vw, 400px\" /></a><figcaption><strong>Figure 9: </strong>Output of sinusoidal encoding from the code snippet.</figcaption></figure></div>\n\n\n<p>The two encodings look similar, don&#8217;t they? Positional encoding is a system in which each position is encoded into a vector. An important thing to note here is that each vector should be unique.</p>\n\n\n\n<p>Binary encoding seems to be a perfect candidate for positional encoding. The problem with the binary system is that of <code data-enlighter-language=\"python\" class=\"EnlighterJSRAW\">0</code>s. This means binary is discrete in nature. Deep Learning models, on the other hand, love to work with continuous data.</p>\n\n\n\n<p>The reason for using sinusoidal encoding is thus three-fold.</p>\n\n\n\n<ul><li>The encoding system is not only dense but also continuous in the range of 0 and 1 (sinusoids are bound in this range).</li><li>It provides a geometric progression, as evident in <strong>Figure 10</strong>. A geometric progression is easily learnable as it repeats itself after specific intervals. Learning the interval rate and the magnitude will allow any model to learn the pattern itself.</li><li><a href=\"https://arxiv.org/abs/1706.03762\" target=\"_blank\" rel=\"noreferrer noopener\">Vaswani et al.</a> hypothesize that doing this would allow the model to learn relative positions better since any position offset by a value <img src='https://929687.smushcdn.com/2633864/wp-content/latex/8ce/8ce4b16b22b58894aa86c421e8759df3-ffffff-000000-0.png?lossy=1&strip=1&webp=1' alt='k' title='k' class='latex' /> <img src='https://929687.smushcdn.com/2633864/wp-content/latex/916/916512d797d344170c5f176f137397b4-ffffff-000000-0.png?lossy=1&strip=1&webp=1' alt='(\\text{PE\\_pos}+k)' title='(\\text{PE\\_pos}+k)' class='latex' /> can be estimated by a linear function of <img src='https://929687.smushcdn.com/2633864/wp-content/latex/bbd/bbd00cc5031f7d51528b524e77db0de8-ffffff-000000-0.png?lossy=1&strip=1&webp=1' alt='\\text{PE\\_pos}' title='\\text{PE\\_pos}' class='latex' />.</li></ul>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/positional-encoder.gif\" target=\"_blank\" rel=\"noreferrer noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/positional-encoder.gif?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34809\" width=\"700\" height=\"393\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/positional-encoder.gif?lossy=1&strip=1&webp=1 427w,https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/positional-encoder.gif?size=126x71&lossy=1&strip=1&webp=1 126w,https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/positional-encoder.gif?size=252x142&lossy=1&strip=1&webp=1 252w\" sizes=\"(max-width: 427px) 100vw, 427px\" /></a><figcaption><strong>Figure 10: </strong>Positional Encoder.</figcaption></figure></div>\n\n\n<hr class=\"wp-block-separator has-alpha-channel-opacity\"/>\n\n\n\n<div id=\"pitch\" style=\"padding: 40px; width: 100%; background-color: #F4F6FA;\">\n\t<h3>What's next? I recommend <a target=\"_blank\" href=\"https://pyimagesearch.com/pyimagesearch-university/?utm_source=blogPost&utm_medium=bottomBanner&utm_campaign=What%27s%20next%3F%20I%20recommend\">PyImageSearch University</a>.</h3>\n\n\t<script src=\"https://fast.wistia.com/embed/medias/kno0cmko2z.jsonp\" async></script><script src=\"https://fast.wistia.com/assets/external/E-v1.js\" async></script><div class=\"wistia_responsive_padding\" style=\"padding:56.25% 0 0 0;position:relative;\"><div class=\"wistia_responsive_wrapper\" style=\"height:100%;left:0;position:absolute;top:0;width:100%;\"><div class=\"wistia_embed wistia_async_kno0cmko2z videoFoam=true\" style=\"height:100%;position:relative;width:100%\"><div class=\"wistia_swatch\" style=\"height:100%;left:0;opacity:0;overflow:hidden;position:absolute;top:0;transition:opacity 200ms;width:100%;\"><img src=\"https://fast.wistia.com/embed/medias/kno0cmko2z/swatch\" style=\"filter:blur(5px);height:100%;object-fit:contain;width:100%;\" alt=\"\" aria-hidden=\"true\" onload=\"this.parentNode.style.opacity=1;\" /></div></div></div></div>\n\n\t<div style=\"margin-top: 32px; margin-bottom: 32px; \">\n\t\t<strong>Course information:</strong><br/>\n\t\t53+ total classes • 57+ hours of on-demand code walkthrough videos • Last updated: October 2022<br/>\n\t\t<span style=\"color: #169FE6;\">★★★★★</span> 4.84 (128 Ratings) • 15,800+ Students Enrolled\n\t</div>\n\n\t<p><strong>I strongly believe that if you had the right teacher you could <em>master</em> computer vision and deep learning.</strong></p>\n\n\t<p>Do you think learning computer vision and deep learning has to be time-consuming, overwhelming, and complicated? Or has to involve complex mathematics and equations? Or requires a degree in computer science?</p>\n\n\t<p>That’s <em>not</em> the case.</p>\n\n\t<p>All you need to master computer vision and deep learning is for someone to explain things to you in <em>simple, intuitive</em> terms. <em>And that’s exactly what I do</em>. My mission is to change education and how complex Artificial Intelligence topics are taught.</p>\n\n\t<p>If you're serious about learning computer vision, your next stop should be PyImageSearch University, the most comprehensive computer vision, deep learning, and OpenCV course online today. Here you’ll learn how to <em>successfully</em> and <em>confidently</em> apply computer vision to your work, research, and projects. Join me in computer vision mastery.</p>\n\n\t<p><strong>Inside PyImageSearch University you'll find:</strong></p>\n\n\t<ul style=\"margin-left: 0px;\">\n\t\t<li style=\"list-style: none;\">&check; <strong>53+ courses</strong> on essential computer vision, deep learning, and OpenCV topics</li>\n\t\t<li style=\"list-style: none;\">&check; <strong>53+ Certificates</strong> of Completion</li>\n\t\t<li style=\"list-style: none;\">&check; <strong>57+ hours</strong> of on-demand video</li>\n\t\t<li style=\"list-style: none;\">&check; <strong>Brand new courses released <em>regularly</em></strong>, ensuring you can keep up with state-of-the-art techniques</li>\n\t\t<li style=\"list-style: none;\">&check; <strong>Pre-configured Jupyter Notebooks in Google Colab</strong></li>\n\t\t<li style=\"list-style: none;\">&check; Run all code examples in your web browser — works on Windows, macOS, and Linux (no dev environment configuration required!)</li>\n\t\t<li style=\"list-style: none;\">&check; Access to <strong>centralized code repos for <em>all</em> 450+ tutorials</strong> on PyImageSearch</li>\n\t\t<li style=\"list-style: none;\">&check; <strong> Easy one-click downloads</strong> for code, datasets, pre-trained models, etc.</li>\n\t\t<li style=\"list-style: none;\">&check; <strong>Access</strong> on mobile, laptop, desktop, etc.</li>\n\t</ul>\n\n\t<p style=\"text-align: center;\">\n\t\t<a target=\"_blank\" class=\"button link\" href=\"https://pyimagesearch.com/pyimagesearch-university/?utm_source=blogPost&utm_medium=bottomBanner&utm_campaign=What%27s%20next%3F%20I%20recommend\" style=\"background-color: #6DC713; border-bottom: none;\">Click here to join PyImageSearch University</a>\n\t</p>\n</div>\n\n\n\n<hr class=\"wp-block-separator has-alpha-channel-opacity\" id=\"h2Summary\"/>\n\n\n\n<h2><a href=\"#TOC\"><strong>Summary</strong></a></h2>\n\n\n\n<p>Now at this point, almost everything about transformers is known to us. But it is also important to pause and ponder the necessity of such an architecture.</p>\n\n\n\n<p>In 2017, the answer lay in their paper. <a href=\"https://arxiv.org/abs/1706.03762\" target=\"_blank\" rel=\"noreferrer noopener\">Vaswani et al.</a> mention their reasons for choosing the simpler version of other transduction models published at the time. But we have come a long way since 2017. </p>\n\n\n\n<p>In the last 5 years, transformers have absolutely taken over Deep Learning. There is no task, nook, or corner safe from attention applications. So what made this model so good? And how was that conceptualized in a simple Machine Translation task?</p>\n\n\n\n<p>Firstly, Deep Learning researchers are not soothsayers, so there was no way of knowing that Transformers would be so good at every task. But the architecture is intuitively simple. It works by attending to parts of the input tokens that are relevant to parts of the target token. This approach worked wonders for Machine Translation. But will it not work on other tasks? Image Classification, Video Classification, Text-to-Image generation, segmentation, or volume rendering?</p>\n\n\n\n<p>Anything that can be expressed as tokenized input being mapped to a tokenized output set falls in the realm of Transformers.</p>\n\n\n\n<p>This brings the theory of Transformers to an end. In the next part of this series, we will focus on creating this architecture in TensorFlow and Keras.</p>\n\n\n\n<hr class=\"wp-block-separator has-alpha-channel-opacity\" id=\"h3Citation\"/>\n\n\n\n<h3><a href=\"#TOC\"><strong>Citation Information</strong></a></h3>\n\n\n\n<p><strong>A. R. Gosthipaty and R. Raha. </strong>“A Deep Dive into Transformers with TensorFlow and Keras: Part 2,” <em>PyImageSearch</em>, P. Chugh, S. Huot, K. Kidriavsteva, and A. Thanki, eds., 2022, <a href=\"https://pyimg.co/pzu1j\" target=\"_blank\" rel=\"noreferrer noopener\">https://pyimg.co/pzu1j</a> </p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"raw\" data-enlighter-theme=\"classic\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"false\" data-enlighter-lineoffset=\"\" data-enlighter-title=\"\" data-enlighter-group=\"\">@incollection{ARG-RR_2022_DDTFK2,\n  author = {Aritra Roy Gosthipaty and Ritwik Raha},\n  title = {A Deep Dive into Transformers with {TensorFlow} and {K}eras: Part 2},\n  booktitle = {PyImageSearch},\n  editor = {Puneet Chugh and Susan Huot and Kseniia Kidriavsteva and Abhishek Thanki},\n  year = {2022},\n  note = {https://pyimg.co/pzu1j},\n}</pre>\n\n\n\n<hr class=\"wp-block-separator has-alpha-channel-opacity\"/>\n\n\n\n<div style=\"padding: 40px; width: 100%; background-color: #F4F6FA;\">\n\t<h3>Want free GPU credits to train models?</h3>\n\n\t<ul style=\"margin-left: 0px;\">\n\t\t<li style=\"list-style: none;\">We used <a target=\"_blank\" href=\"https://cloud.jarvislabs.ai/\">Jarvislabs.ai</a>, a GPU cloud, for all the experiments.</li>\n\t\t<li style=\"list-style: none;\">We are proud to offer PyImageSearch University students $20 worth of Jarvislabs.ai GPU cloud credits. Join PyImageSearch University and claim your $20 credit <a target=\"_blank\" href=\"https://pyimagesearch.com/pyimagesearch-university/\">here</a>.</li>\n\t</ul>\n\n\n\t<p>In Deep Learning, we need to train Neural Networks. These Neural Networks can be trained on a CPU but take a lot of time. Moreover, sometimes these networks do not even fit (run) on a CPU.</p>\n\n\t<p>To overcome this problem, we use <strong>GPUs</strong>.  The problem is these GPUs are <strong>expensive</strong> and become outdated quickly. </p>\n\n\t<p>GPUs are great because they take your Neural Network and train it quickly.  The problem is that GPUs are expensive, so you don’t want to buy one and use it only occasionally.  Cloud GPUs let you use a GPU and <strong>only pay for the time you are running the GPU</strong>.  It’s a brilliant idea that saves you money.</p>\n\n\t<p><strong>JarvisLabs</strong> provides the best-in-class GPUs, and <strong>PyImageSearch University students</strong> get between 10-50 hours on a world-class GPU (time depends on the specific GPU you select).</p>\n\n\n\t<p>This gives you a chance to <strong>test-drive a monstrously powerful GPU</strong> on any of our tutorials in a jiffy. So join <a target=\"_blank\" href=\"https://pyimagesearch.com/pyimagesearch-university/\">PyImageSearch University</a> today and try it for yourself.</p>\n\n\n\t<p style=\"text-align: center;\">\n\t\t<a target=\"_blank\" class=\"button link\" href=\"https://pyimagesearch.com/pyimagesearch-university/\" style=\"background-color: #6DC713; border-bottom: none;\">Click here to get Jarvislabs credits now</a>\n\t</p>\n</div>\n\n\n\n<hr class=\"wp-block-separator has-alpha-channel-opacity\"/>\n\n\n\n<div id=\"download-the-code\" class=\"post-cta-wrap\">\n<div class=\"gpd-post-cta\">\n\t<div class=\"gpd-post-cta-content\">\n\t\t\n\n\t\t\t<div class=\"gpd-post-cta-top\">\n\t\t\t\t<div class=\"gpd-post-cta-top-image\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2020/01/cta-source-guide-1.png?lossy=1&strip=1&webp=1\" alt=\"\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2020/01/cta-source-guide-1.png?lossy=1&strip=1&webp=1 410w,https://929687.smushcdn.com/2633864/wp-content/uploads/2020/01/cta-source-guide-1.png?size=126x174&lossy=1&strip=1&webp=1 126w,https://929687.smushcdn.com/2633864/wp-content/uploads/2020/01/cta-source-guide-1.png?size=252x348&lossy=1&strip=1&webp=1 252w\" sizes=\"(max-width: 410px) 100vw, 410px\" /></div>\n\t\t\t\t\n\t\t\t\t<div class=\"gpd-post-cta-top-title\"><h4>Join the PyImageSearch Newsletter and Grab My FREE 17-page Resource Guide PDF</h4></div>\n\t\t\t\t<div class=\"gpd-post-cta-top-desc\"><p>Enter your email address below to <strong>join the PyImageSearch Newsletter</strong> and <strong>download my FREE 17-page Resource Guide PDF</strong> on Computer Vision, OpenCV, and Deep Learning.</p></div>\n\n\n\t\t\t</div>\n\n\t\t\t<div class=\"gpd-post-cta-bottom\">\n\t\t\t\t<form class=\"footer-cta\" action=\"https://www.getdrip.com/forms/657075648/submissions\" method=\"post\" target=\"_blank\" data-drip-embedded-form=\"657075648\">\n\t\t\t\t\t<input name=\"fields[email]\" type=\"email\" value=\"\" placeholder=\"Your email address\" class=\"form-control\" />\n\n\t\t\t\t\t<button type=\"submit\">Join the Newsletter!</button>\n\n\t\t\t\t\t<div style=\"display: none;\" aria-hidden=\"true\"><label for=\"website\">Website</label><br /><input type=\"text\" id=\"website\" name=\"website\" tabindex=\"-1\" autocomplete=\"false\" value=\"\" /></div>\n\t\t\t\t</form>\n\t\t\t</div>\n\n\n\t\t\n\t</div>\n\n</div>\n</div>\n<p>The post <a rel=\"nofollow\" href=\"https://pyimagesearch.com/2022/09/26/a-deep-dive-into-transformers-with-tensorflow-and-keras-part-2/\">A Deep Dive into Transformers with TensorFlow and Keras: Part&nbsp;2</a> appeared first on <a rel=\"nofollow\" href=\"https://pyimagesearch.com\">PyImageSearch</a>.</p>\n"
}