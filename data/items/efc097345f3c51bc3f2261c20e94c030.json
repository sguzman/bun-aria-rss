{
  "title": "Scaling data ingestion for machine learning training at Meta",
  "link": "https://engineering.fb.com/2022/09/19/ml-applications/data-ingestion-machine-learning-training-meta/",
  "dc:creator": "",
  "pubDate": "Mon, 19 Sep 2022 16:00:23 +0000",
  "category": [
    "Data Center Engineering",
    "DevInfra",
    "ML Applications"
  ],
  "guid": "https://engineering.fb.com/?p=19014",
  "description": "<p>Many of Meta’s products, such as search, ads ranking and Marketplace, utilize AI models to continuously improve user experiences. As the performance of hardware we use to support training infrastructure increases, we need to scale our data ingestion infrastructure accordingly to handle workloads more efficiently. GPUs, which are used for training infrastructure, tend to double [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2022/09/19/ml-applications/data-ingestion-machine-learning-training-meta/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2022/09/19/ml-applications/data-ingestion-machine-learning-training-meta/\">Scaling data ingestion for machine learning training at Meta</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n",
  "content:encoded": "<p><span style=\"font-weight: 400;\">Many of Meta’s products, such as</span> <span style=\"font-weight: 400;\">search, ads ranking and </span><a href=\"https://engineering.fb.com/2018/10/02/ml-applications/under-the-hood-facebook-marketplace-powered-by-artificial-intelligence/\"><span style=\"font-weight: 400;\">Marketplace</span></a><span style=\"font-weight: 400;\">, utilize AI models to continuously improve user experiences. As the performance of hardware we use to support training infrastructure increases, we need to scale our data ingestion infrastructure accordingly to handle workloads more efficiently. </span><a href=\"https://engineering.fb.com/2021/07/15/open-source/fsdp/\"><span style=\"font-weight: 400;\">GPUs</span></a><span style=\"font-weight: 400;\">, which are used for training infrastructure, tend to double in performance every two years, while the performance of CPUs, used for data reading computation, increases at a much slower pace in the same time frame.</span></p>\n<p><span style=\"font-weight: 400;\">To facilitate the level of data ingestion required to support the training models supporting our products, we’ve had to build a new data ingestion infrastructure as well as new last-mile transformation pipelines. By optimizing areas of our data ingestion infrastructure, we improved our power budget requirement by 35-45%, allowing us to support a growing number of AI models in our power constrained data centers.  </span></p>\n<div class=\"fb-video\" data-allowfullscreen=\"true\" data-href=\"https://www.facebook.com/watch/?v=465256971964332\"></div>\n<h2><span style=\"font-weight: 400;\">Meta’s growing AI infrastructure </span></h2>\n<p><span style=\"font-weight: 400;\">As our product groups continue to rely heavily on AI models to improve product experience, the AI infrastructure requirements are growing along the following dimensions:</span></p>\n<ol>\n<li><span style=\"font-weight: 400;\"> Number of models being trained</span></li>\n<li><span style=\"font-weight: 400;\"> Volume of data and features that models train on</span></li>\n<li><span style=\"font-weight: 400;\"> Model size and complexity</span></li>\n<li><span style=\"font-weight: 400;\"> Model training throughput</span></li>\n</ol>\n<p><span style=\"font-weight: 400;\">In the figure below, we observe that over the last two years we have grown: </span></p>\n<ul>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">1.75-2x in the amount of data we train on</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">3-4x in data ingestion throughput</span></li>\n</ul>\n<p><img decoding=\"async\" loading=\"lazy\" class=\"alignnone size-medium wp-image-19140\" src=\"https://engineering.fb.com/wp-content/uploads/2022/08/Fig1.jpeg?w=916\" alt=\"\" width=\"916\" height=\"515\" srcset=\"https://engineering.fb.com/wp-content/uploads/2022/08/Fig1.jpeg 1920w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig1.jpeg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig1.jpeg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig1.jpeg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig1.jpeg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig1.jpeg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig1.jpeg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig1.jpeg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\" /></p>\n<p><em><span style=\"font-weight: 400;\">Fig. 1: Normalized dataset size growth and data ingestion bandwidth growth observed in production.</span></em></p>\n<p><span style=\"font-weight: 400;\">Our data centers must be provisioned to serve infrastructure that trains thousands of models, each consuming petabyte scale datasets. We must enable our engineers to have maximum flexibility when experimenting with new features and training model architectures. In the sections below, we share our experience building data ingestion and last-mile data preprocessing pipelines that are responsible for feeding data into AI training models.</span></p>\n<h2><span style=\"font-weight: 400;\">Data ingestion pipeline overview</span></h2>\n<p><span style=\"font-weight: 400;\">We have exabytes of training data powering our models, and the amount of training data is growing rapidly. We have a wide variety of models that train on terabyte- to petabyte-scale data, but we do not have the storage capacity at that scale to train the data locally on the training hardware. We store and serve training data from </span><a href=\"https://engineering.fb.com/2021/06/21/data-infrastructure/tectonic-file-system/\"><span style=\"font-weight: 400;\">Tectonic,</span></a><span style=\"font-weight: 400;\"> Meta’s exabyte-scale distributed file system that serves as a disaggregated storage infrastructure for our AI training models. Our AI training datasets are modeled as Hive Tables and encoded using a hybrid columnar format called DWRF, based on the Apache ORC format. </span></p>\n<p><span style=\"font-weight: 400;\">The process of selecting raw data and transforming it into features that can be consumed by machine learning (ML) training models is called feature engineering. This is at the core of ML training, and our ML engineers must experiment with new features on a daily basis. We model features as maps in training tables. This gives Meta’s engineers the flexibility to add and remove features easily without continuously maintaining the table schema. </span></p>\n<p><span style=\"font-weight: 400;\">We have built a disaggregated Data PreProcessing tier (DPP) that serves as the reader tier for </span><a href=\"https://arxiv.org/pdf/2108.09373.pdf\"><span style=\"font-weight: 400;\">data ingestion</span></a><span style=\"font-weight: 400;\"> and last-mile data transformations for AI training.</span></p>\n<p><span style=\"font-weight: 400;\">This is responsible for:</span></p>\n<p><span style=\"font-weight: 400;\">&#8211; Fetching data from Tectonic clusters</span></p>\n<p><span style=\"font-weight: 400;\">&#8211; Decrypting and decoding data</span></p>\n<p><span style=\"font-weight: 400;\">&#8211; Extracting the features to be consumed by the model</span></p>\n<p><span style=\"font-weight: 400;\">&#8211; Converting the data to tensor formats</span></p>\n<p><span style=\"font-weight: 400;\">&#8211; Performing last-mile transformations before actual training</span></p>\n<p><span style=\"font-weight: 400;\">For content understanding models, examples of last-mile transformations could mean randomized image clips or crops ‌to detect objectionable images, for example. With recommendation models, last-mile transformations typically trigger operations like feature normalization, bucketization, truncation, sort by score, or even operations that combine multiple features to form new features, like ngram, or categorical feature intersections and unions.</span></p>\n<p><span style=\"font-weight: 400;\">DPP allows us to scale data ingestion and training hardware independently, enabling us to train thousands of very diverse models with different ingestion and training characteristics</span><span style=\"font-weight: 400;\">. DPP provides an easy-to-use, PyTorch-style API to efficiently ingest data into training. It enables classes of new features by leveraging its disaggregated compute tier to support feature transformations (these operations are often computationally intensive). DPP executes in a data parallel fashion, with each compute node (DPP worker) reading, batching, and preprocessing a subset of training data rows. A lightweight DPP client module invoked in the trainer process fetches data from DPP worker nodes and transfers the data to training. DPP can also be invoked as a library on training nodes, in what we call the on-box mode, for models that do not have high throughput demands. However, in practice, many of our recommendation jobs use tens to hundreds of disaggregated nodes to ensure that we can meet the data ingestion demand of trainers . Several of our complex training jobs read massive volumes of data and can take several days to train. To avoid wasted compute due to failures, DPP has built-in support to checkpoint data cursors and resume jobs from checkpoints. </span><span style=\"font-weight: 400;\">Failed reader nodes are replaced transparently, without job interruption.</span><span style=\"font-weight: 400;\"> DPP can also dynamically scale compute resources allocated for reading to ensure </span><span style=\"font-weight: 400;\">we can meet the data throughput demands from the trainers.</span></p>\n<p><span style=\"font-weight: 400;\">Our training infrastructure must serve a wide variety of models trained on distributed CPU and GPU hardware deployments.</span></p>\n<p><span style=\"font-weight: 400;\">The figure below shows our data ingestion architecture:</span></p>\n<p><img decoding=\"async\" loading=\"lazy\" class=\"alignnone size-medium wp-image-19141\" src=\"https://engineering.fb.com/wp-content/uploads/2022/08/Fig2.jpeg?w=916\" alt=\"\" width=\"916\" height=\"515\" srcset=\"https://engineering.fb.com/wp-content/uploads/2022/08/Fig2.jpeg 1920w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig2.jpeg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig2.jpeg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig2.jpeg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig2.jpeg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig2.jpeg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig2.jpeg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig2.jpeg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\" /></p>\n<p><em><span style=\"font-weight: 400;\">Fig. 2: Last-mile data ingestion infrastructure at Meta.</span></em></p>\n<h2><span style=\"font-weight: 400;\">Data ingestion characteristics and optimizations</span></h2>\n<p><b>Trends in hardware evolution and data center power constraints</b></p>\n<p><span style=\"font-weight: 400;\">As mentioned above, we have a mismatch in the rate of growth for our training ​​and ingestion hardware. Our disaggregated architecture enabled us to scale data ingestion for training needs. However, many recommendation models are ingestion-bound (</span><span style=\"font-weight: 400;\">Fig. 3</span><span style=\"font-weight: 400;\">). With a fixed power budget in our data centers, data ingestion requirements limit the training accelerators we can deploy.</span></p>\n<p><img decoding=\"async\" loading=\"lazy\" class=\"alignnone size-medium wp-image-19142\" src=\"https://engineering.fb.com/wp-content/uploads/2022/08/Fig3.jpeg?w=916\" alt=\"\" width=\"916\" height=\"515\" srcset=\"https://engineering.fb.com/wp-content/uploads/2022/08/Fig3.jpeg 1920w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig3.jpeg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig3.jpeg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig3.jpeg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig3.jpeg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig3.jpeg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig3.jpeg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig3.jpeg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\" /></p>\n<p><em><span style=\"font-weight: 400;\">Fig. 3: Storage, reader compute, and training power distribution across three recommendation models. The sum of power allocation for storage and reader tiers is dominant for many ranking models. This limits the training accelerators we can land in our data centers, where we have fixed power budget constraints. </span></em></p>\n<p><b>Data reading tier characterizations and optimizations</b></p>\n<p><span style=\"font-weight: 400;\">We have profiled several production recommendation models, and we&#8217;ve summarized the lessons learned around efficient data reading:</span></p>\n<p><b>Optimizing algorithmic efficiency in readers:</b><span style=\"font-weight: 400;\"> </span></p>\n<p><span style=\"font-weight: 400;\">Training datasets are often shared across multiple jobs, and a single training job often reads only a subset of the available features. This could mean reading as low as 20-37 percent of the stored bytes in many of our prominent ranking models. </span></p>\n<p><span style=\"font-weight: 400;\">The original map column layout did not provide efficient ways to read a subset of features from the available features (see </span><span style=\"font-weight: 400;\">Fig. 4</span><span style=\"font-weight: 400;\">). The data layout of the features in the original map meant we had to fetch, decrypt, and decode the entire map object to extract the features needed by the model.</span></p>\n<p><img decoding=\"async\" loading=\"lazy\" class=\"alignnone size-medium wp-image-19143\" src=\"https://engineering.fb.com/wp-content/uploads/2022/08/Fig4.jpeg?w=916\" alt=\"\" width=\"916\" height=\"515\" srcset=\"https://engineering.fb.com/wp-content/uploads/2022/08/Fig4.jpeg 1920w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig4.jpeg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig4.jpeg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig4.jpeg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig4.jpeg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig4.jpeg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig4.jpeg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig4.jpeg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\" /></p>\n<p><span style=\"font-weight: 400;\"><em>Fig. 4: Original data layout of the feature maps. We need to fetch, decode, and decrypt entire Keys, Values, and Lengths columns to extract desired features of A and E.</em></span></p>\n<p><span style=\"font-weight: 400;\">We implemented a new storage format called feature flattening, which </span><span style=\"font-weight: 400;\">represents each feature as a stream on a disk, as if we had </span><i><span style=\"font-weight: 400;\">n</span></i><span style=\"font-weight: 400;\"> columns instead of a map of </span><i><span style=\"font-weight: 400;\">n</span></i><span style=\"font-weight: 400;\"> features. This columnar feature representation enables reading subsets of features more efficiently. We call this reading functionality as “feature projection.”</span></p>\n<p><img decoding=\"async\" loading=\"lazy\" class=\"alignnone size-medium wp-image-19144\" src=\"https://engineering.fb.com/wp-content/uploads/2022/08/Fig5.jpeg?w=916\" alt=\"\" width=\"916\" height=\"515\" srcset=\"https://engineering.fb.com/wp-content/uploads/2022/08/Fig5.jpeg 1920w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig5.jpeg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig5.jpeg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig5.jpeg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig5.jpeg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig5.jpeg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig5.jpeg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig5.jpeg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\" /></p>\n<p><em><span style=\"font-weight: 400;\">Fig. 5: Feature flattening stores individual features in contiguous streams. This format is more efficient when the goal is to selectively read a subset of features.</span></em></p>\n<p><span style=\"font-weight: 400;\">Since most of our production workloads were selective in terms of features consumed by models compared with features stored in storage, feature projection yielded high data reading efficiency wins, to the tune of 2-2.3x. The normalized throughput gains metric shown in the figure below indicates the improvements in the rows/s metric as executed b by each DPP reader.</span></p>\n<p><span style=\"font-weight: 400;\"><img decoding=\"async\" loading=\"lazy\" class=\"alignnone size-medium wp-image-19146\" src=\"https://engineering.fb.com/wp-content/uploads/2022/08/Fig6.jpeg?w=916\" alt=\"\" width=\"916\" height=\"515\" srcset=\"https://engineering.fb.com/wp-content/uploads/2022/08/Fig6.jpeg 1920w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig6.jpeg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig6.jpeg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig6.jpeg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig6.jpeg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig6.jpeg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig6.jpeg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig6.jpeg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\" /></span></p>\n<p><em><span style=\"font-weight: 400;\">Fig. 6: Normalized throughput gains from feature flattening rollouts in three sample ranking models in our production fleet. Models that selectively read a smaller subset of features in the storage tier (which is typical in our AI training production environment) benefit from feature flattening representation of data.</span></em></p>\n<p><b>Optimizing memory consumption for the data reading tier: </b><span style=\"font-weight: 400;\">The DPP readers provide batches of data for training, or, a number of input rows to be consumed in one training iteration. As training infrastructure onboarded more powerful accelerators, we observed the trend of increasing batch -sizes to increase the training throughput of rows/s on the beefier training nodes. We found several use cases where DPP workers that executed on simpler CPU nodes became memory-bound to support larger batch sizes. We observed that most users mitigated this by launching readers with fewer threads to avoid out-of-memory (OOM) errors. Reducing reader node threads resulted in reduced per-node efficiency, or reduced rows/s as executed by each reader node. To support large batches, we proposed DPP client-side rebatching, where we still read smaller batches with hardware concurrency on our reader tier nodes. However, our client on the beefier training node is responsible for appending batches to support large batch exploration.</span></p>\n<p><img decoding=\"async\" loading=\"lazy\" class=\"alignnone size-medium wp-image-19147\" src=\"https://engineering.fb.com/wp-content/uploads/2022/08/Fig7.jpeg?w=916\" alt=\"\" width=\"916\" height=\"515\" srcset=\"https://engineering.fb.com/wp-content/uploads/2022/08/Fig7.jpeg 1920w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig7.jpeg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig7.jpeg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig7.jpeg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig7.jpeg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig7.jpeg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig7.jpeg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig7.jpeg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\" /></p>\n<p><em><span style=\"font-weight: 400;\">Fig. 7:  Around 20-40 percent improvements in the rows/s throughput as executed by each reader node by enabling DPP Client side rebatching to support large batch explorations.</span></em></p>\n<p><b>Optimizing memory bandwidth for the data reading tier</b></p>\n<p><span style=\"font-weight: 400;\">We expect most of our DPP nodes to be memory bandwidth-bound as we upgrade our data centers with newer CPU versions with more cores (and without a proportional increase of the available memory bandwidth). Many of our data reading workloads in production are memory bandwidth-bound. We also have identified scope to improve our memory bandwidth utilization in preprocessing/transformation operators we executed on the readers. In this section, we will  discuss the project of </span><span style=\"font-weight: 400;\">FlatMaps</span><span style=\"font-weight: 400;\">, which yielded improvements in terms of memory bandwidth utilization on the DPP readers.</span></p>\n<p><span style=\"font-weight: 400;\">As explained in the section above, with feature flattening we changed the physical layout of our features in the storage tier. However, due to legacy reasons of reading unflattened tables, we identified that our in-memory representation of a batch in the DPP reader worker was obsolete, triggering unnecessary format transformations. This is illustrated in Fig. 8, below.</span></p>\n<p><img decoding=\"async\" loading=\"lazy\" class=\"alignnone size-medium wp-image-19148\" src=\"https://engineering.fb.com/wp-content/uploads/2022/08/Fig8.jpeg?w=916\" alt=\"\" width=\"916\" height=\"515\" srcset=\"https://engineering.fb.com/wp-content/uploads/2022/08/Fig8.jpeg 1920w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig8.jpeg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig8.jpeg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig8.jpeg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig8.jpeg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig8.jpeg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig8.jpeg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig8.jpeg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\" /></p>\n<p><em><span style=\"font-weight: 400;\">Fig. 8: Our original in-memory batch data representation manifested the original map layout of features shown in Fig. 4. Reading flattened features from storage, translating this data to the legacy in memory batch representation and then converting the data to tensors triggered unnecessary data format transformations. </span></em></p>\n<p><span style=\"font-weight: 400;\">By identifying a column major in-memory format to read flattened tables, we avoided unnecessary data layout transformations as illustrated in Fig. 9, below.</span></p>\n<p><img decoding=\"async\" loading=\"lazy\" class=\"alignnone size-medium wp-image-19149\" src=\"https://engineering.fb.com/wp-content/uploads/2022/08/Fig9.jpeg?w=916\" alt=\"\" width=\"916\" height=\"515\" srcset=\"https://engineering.fb.com/wp-content/uploads/2022/08/Fig9.jpeg 1920w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig9.jpeg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig9.jpeg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig9.jpeg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig9.jpeg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig9.jpeg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig9.jpeg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig9.jpeg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\" /></p>\n<p><em><span style=\"font-weight: 400;\">Fig. 9: Illustration of data format and Flatmap in-memory representation in readers. This in-memory format eliminates unnecessary data layout transformations from features in our storage tier to tensors that training must consume.</span></em></p>\n<p><img decoding=\"async\" loading=\"lazy\" class=\"alignnone size-medium wp-image-19390\" src=\"https://engineering.fb.com/wp-content/uploads/2025/08/Fig11.jpeg?w=916\" alt=\"\" width=\"916\" height=\"515\" srcset=\"https://engineering.fb.com/wp-content/uploads/2025/08/Fig11.jpeg 1920w, https://engineering.fb.com/wp-content/uploads/2025/08/Fig11.jpeg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2025/08/Fig11.jpeg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2025/08/Fig11.jpeg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2025/08/Fig11.jpeg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2025/08/Fig11.jpeg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2025/08/Fig11.jpeg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2025/08/Fig11.jpeg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\" /></p>\n<p><em><span style=\"font-weight: 400;\">Fig. 10: 9-17 percent the Rows/s throughput as executed by each reader node by applying the FlatMaps in-memory data representations.</span></em></p>\n<p><span style=\"font-weight: 400;\">In general, optimizing data reading tier memory bandwidth utilization remains one of the most compelling areas we continue to invest in to efficiently utilize the newer CPU versions landing in our data centers. </span></p>\n<p><b>Scaling the storage tier to serve AI access patterns</b></p>\n<p><span style=\"font-weight: 400;\">Let us take a look at what drives storage tier power cost. Despite individual models training on terabyte- to petabyte-scale data, we find that many of our models training on accelerators are IO bound due to massive training throughput demand. One reason for this is that models train on a subset of features that are saved in our dataset. Selectively seeking features consumed by models results in smaller IOSize for our disk accesses, thus increasing IOPs demand. On the other hand, if we overread consecutive features in the storage block to minimize seeks, we end up reading bytes that eventually get dropped by training. This is illustrated in</span> <span style=\"font-weight: 400;\">Fig. 11,</span><span style=\"font-weight: 400;\"> below.</span></p>\n<p>&nbsp;</p>\n<p><img decoding=\"async\" loading=\"lazy\" class=\"alignnone size-medium wp-image-19393\" src=\"https://engineering.fb.com/wp-content/uploads/2025/08/Fig11-1.jpeg?w=916\" alt=\"\" width=\"916\" height=\"515\" srcset=\"https://engineering.fb.com/wp-content/uploads/2025/08/Fig11-1.jpeg 1920w, https://engineering.fb.com/wp-content/uploads/2025/08/Fig11-1.jpeg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2025/08/Fig11-1.jpeg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2025/08/Fig11-1.jpeg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2025/08/Fig11-1.jpeg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2025/08/Fig11-1.jpeg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2025/08/Fig11-1.jpeg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2025/08/Fig11-1.jpeg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\" /></p>\n<p><em><span style=\"font-weight: 400;\">Fig. 11: Feature Re-ordering illustration. Feature re-ordering writes features that are popularly consumed together in continuous blocks in our storage tier.</span></em></p>\n<p><span style=\"font-weight: 400;\">In fact, we had some production models that were NIC-bound at the reader ingress due to high overreads from the storage tier. By eliminating over-reads, we were able to further improve data reading algorithmic efficiency for these models as we observed these models moving from being NIC-bound on the readers to memory bandwidth-bound. In the figure below, we present the reduction we observed in storage tier to reader tier data transfer and improvement in storage tier service time once we applied feature reordering.</span></p>\n<p><img decoding=\"async\" loading=\"lazy\" class=\"alignnone size-medium wp-image-19153\" src=\"https://engineering.fb.com/wp-content/uploads/2022/08/Fig12_and_Fig13.jpeg?w=916\" alt=\"\" width=\"916\" height=\"515\" srcset=\"https://engineering.fb.com/wp-content/uploads/2022/08/Fig12_and_Fig13.jpeg 1920w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig12_and_Fig13.jpeg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig12_and_Fig13.jpeg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig12_and_Fig13.jpeg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig12_and_Fig13.jpeg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig12_and_Fig13.jpeg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig12_and_Fig13.jpeg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig12_and_Fig13.jpeg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\" /></p>\n<p><em><span style=\"font-weight: 400;\">Fig. 12: Feature Re-ordering yielded 45-55% reduction in volume of data transferred between storage tier and reader tiers. We also observed 30-70% improvement in service time for several of our models.</span></em></p>\n<p><span style=\"font-weight: 400;\">Applying the optimizations discussed in this post, </span><span style=\"font-weight: 400;\">Fig. 13,</span><span style=\"font-weight: 400;\"> below, illustrates the improvements in data ingestion power budget observed in our recommendation models.</span></p>\n<p><img decoding=\"async\" loading=\"lazy\" class=\"alignnone size-medium wp-image-19154\" src=\"https://engineering.fb.com/wp-content/uploads/2022/08/Fig14.jpeg?w=916\" alt=\"\" width=\"916\" height=\"515\" srcset=\"https://engineering.fb.com/wp-content/uploads/2022/08/Fig14.jpeg 1920w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig14.jpeg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig14.jpeg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig14.jpeg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig14.jpeg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig14.jpeg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig14.jpeg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2022/08/Fig14.jpeg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\" /></p>\n<p><em><span style=\"font-weight: 400;\">Fig. 13: 35-45 % improvements in data ingestion power budget as compared to Fig. 4.</span></em></p>\n<h2><span style=\"font-weight: 400;\"><br />\nAreas of future exploration </span></h2>\n<p><span style=\"font-weight: 400;\">We’re continually working to optimize the pipelines responsible for last- mile data ingestion and computation to meet the demands of AI-driven products at Meta. We are committed to delivering an efficient and scalable infrastructure to support our product teams in achieving this mission.</span></p>\n<p><span style=\"font-weight: 400;\">Here are a few areas of exploration we’re examining going forward:</span></p>\n<p><b>Tiered storage:</b><span style=\"font-weight: 400;\"> Many of our datasets are large enough that our models only need to do a single pass. Hence, we are unable to exploit any data reuse within a job. However, we can exploit reuse patterns across concurrent jobs using the same data. We are working toward building a tiered storage solution, HDD + SSD, with SSD serving as the caching tier for high-reuse features.</span></p>\n<p><b>Preprocessing transformations on GPUs: </b><span style=\"font-weight: 400;\">There have been industry-wide efforts to execute preprocessing transformation operations on accelerators. We contiue our efforts to invest in shifting the computation cycles of preprocessing from our hardware-constrained CPU to the beefier training accelerators. Outlining some challenges in our workloads in this space is that many of our preprocessing operators truncate or clip the volume of data being sent to training. With the possibility of preprocessing moving to training accelerators, we see the risk of increased data transfer to push data to the training accelerators. Another risk is that our models train on a large number of  features and often go through several transformations before the final feature is derived. This results in non negligible CUDA kernel launch overheads, limiting the gains we can derive in this direction. That said, shifting preprocessing transformation to beefier training hardware is a very compelling direction, and our teams are actively working to de-risk this space.</span></p>\n<p><b>Storing derived features: </b><span style=\"font-weight: 400;\">Since our recommendation models often train with only a single pass over the data, this limits our ability to reuse data within a job. However, we still find potential of expensive last-mile feature transformations being reused across multiple independent jobs. Our teams are working on identifying common and expensive transformations across independent jobs. In doing so, we aim to promote the transformations to full-fledged precomputed features in our storage tier instead of evaluating them in the last mile of data ingestion.</span></p>\n<p>&nbsp;</p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2022/09/19/ml-applications/data-ingestion-machine-learning-training-meta/\">Scaling data ingestion for machine learning training at Meta</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n",
  "post-id": 19014
}