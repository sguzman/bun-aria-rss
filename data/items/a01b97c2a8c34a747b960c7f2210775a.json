{
  "title": "Machine Learning Methods: Classification without negative examples",
  "link": "",
  "published": "2014-12-20T09:58:00-08:00",
  "updated": "2014-12-20T09:58:00-08:00",
  "author": {
    "name": "Jonathan Landy"
  },
  "id": "tag:efavdb.com,2014-12-20:/methods-regression-without-negative-examples",
  "summary": "<p>Here, we discuss some methods for carrying out classification when only positive examples are available. The latter half of our discussion borrows heavily from <span class=\"caps\">W.S.</span> Lee and B. Liu, Proc. <span class=\"caps\">ICML</span>-2003 (2003), which we supplement&nbsp;somewhat.  </p>\n<h2>Generic logistic&nbsp;regression</h2>\n<p><a href=\"http://en.wikipedia.org/wiki/Logistic_regression\">Logistic regression</a> is a commonly used tool for estimating …</p>",
  "content": "<p>Here, we discuss some methods for carrying out classification when only positive examples are available. The latter half of our discussion borrows heavily from <span class=\"caps\">W.S.</span> Lee and B. Liu, Proc. <span class=\"caps\">ICML</span>-2003 (2003), which we supplement&nbsp;somewhat.  </p>\n<h2>Generic logistic&nbsp;regression</h2>\n<p><a href=\"http://en.wikipedia.org/wiki/Logistic_regression\">Logistic regression</a> is a commonly used tool for estimating the level sets of a Boolean function <span class=\"math\">\\(y\\)</span> on a set of feature vectors <span class=\"math\">\\(\\textbf{F}\\)</span>: In a sense, you can think of it as a method for playing the game &#8220;Battleship&#8221; on whatever data set you&#8217;re interested in. Its application requires knowledge of the <span class=\"math\">\\(\\{(\\textbf{f}_i,y_i)\\}\\)</span> pairs on a training set <span class=\"math\">\\(\\textbf{E} \\subseteq \\textbf{F}\\)</span>, with label <span class=\"math\">\\(y_i = 0,1\\)</span> for negative and positive examples, respectively. Given these training examples, logistic regression estimates for arbitrary feature vector <span class=\"math\">\\(\\textbf{f}\\)</span>,<br>\n</p>\n<div class=\"math\">\\begin{eqnarray}\nh(\\textbf{f}) = \\frac{1}{1 + \\exp \\left [ - \\textbf{T} \\cdot \\textbf{f} \\right]} \\approx y \\tag{1}\n\\end{eqnarray}</div>\n<p>\nwhere the coefficient vector <span class=\"math\">\\(\\textbf{T}\\)</span> is taken to be that vector that minimizes<br>\n</p>\n<div class=\"math\">\\begin{eqnarray}\\tag{2}\nJ(h) \\equiv -\\frac{1}{\\vert \\textbf{E} \\vert}\\sum_{i=1}^{\\vert \\textbf{E} \\vert} y_i \\log(h_i) + (1-y_i) \\log(1- h_i) + \\frac{\\Lambda}{2}\\sum_j T_j^2,\n\\end{eqnarray}</div>\n<p>\na convex cost function that strongly penalizes poor estimates on the training&nbsp;set.</p>\n<h2>Problem statement: no negative&nbsp;examples</h2>\n<p>Consider now a situation where all training examples given are positive &#8212; i.e., no negative examples are available. One realistic realization of this scenario might involve a simple data set of movies already viewed by some Netflix customer. From this information, one would like to estimate the full subset of the available movies that the customer would watch, given time. We&#8217;ll assign value <span class=\"math\">\\(y = 1\\)</span> to such movies and <span class=\"math\">\\(y=0\\)</span> to movies he wouldn&#8217;t watch. Notice that the generic logistic regression approach outlined above would return a default-positive result if applied to this problem: Assigning <span class=\"math\">\\(h = 1\\)</span> to all of <span class=\"math\">\\(\\textbf{F}\\)</span> minimizes <span class=\"math\">\\(J\\)</span>. This means that no information contained in <span class=\"math\">\\(\\textbf{E}\\)</span> is actually utilized in the logistic learning process &#8212; a counterintuitive choice for structured <span class=\"math\">\\(\\textbf{E}\\)</span> (e.g., the case where all movies watched thus far have been in a single category &#8212; martial arts films,&nbsp;say).</p>\n<h2>Noisy&nbsp;labeling</h2>\n<p>Some reasonable, alternative approaches do not return the default-positive response in the situation above. To see this, we first review here noisy labeling problems. Suppose we are given a training set with noisy labeling <span class=\"math\">\\(y^{\\prime}\\)</span>: Truly-positive examples <span class=\"math\">\\((y = 1)\\)</span> are stochastically mislabeled in this set with frequency <span class=\"math\">\\(\\alpha\\)</span> as negative <span class=\"math\">\\((y^{\\prime} = 0)\\)</span>, and truly-negative examples <span class=\"math\">\\((y=0)\\)</span> are mislabeled with frequency <span class=\"math\">\\(\\beta\\)</span> as positive <span class=\"math\">\\((y^{\\prime} = 1)\\)</span>. For hypothesis <span class=\"math\">\\(h\\)</span>,&nbsp;let\n</p>\n<div class=\"math\">\\begin{eqnarray}\\tag{3}\nC(h) = Pr[h = 0 \\vert y = 1]+ Pr[h = 1 \\vert y= 0],\n\\end{eqnarray}</div>\n<p>\nthe rate at which <span class=\"math\">\\(h\\)</span> mislabels positive examples in the training set added to the rate at which it mislabels negative examples. Similarly, we define <span class=\"math\">\\(C^{\\prime}(h)\\)</span> as above, but with <span class=\"math\">\\(y\\)</span> replaced by <span class=\"math\">\\(y^{\\prime}\\)</span>. Because <span class=\"math\">\\(y^{\\prime}\\)</span> is stochastic, we also average it in this case,&nbsp;giving\n</p>\n<div class=\"math\">\\begin{eqnarray}\\tag{4}\nC^{\\prime}(h) = \\left \\langle Pr[h = 0 \\vert y^{\\prime} = 1]+ Pr[h = 1 \\vert y^{\\prime}= 0] \\right \\rangle_{y^{\\prime}}.\n\\end{eqnarray}</div>\n<p>\nWith these definitions, we have [see Blum and Michael (1998) or derive&nbsp;yourself] \n</p>\n<div class=\"math\">\\begin{eqnarray}\\tag{5}\nC(h) \\propto C^{\\prime}(h),\n\\end{eqnarray}</div>\n<p>\nwith <span class=\"math\">\\(\\text{sign}(C) = \\text{sign}(1 - \\alpha - \\beta) \\times \\text{sign}(C^{\\prime})\\)</span>. This result is very useful whenever we take <span class=\"math\">\\(C(h)\\)</span> as our cost function<span class=\"math\">\\(^1\\)</span>: Provided the total noise rate <span class=\"math\">\\(\\alpha + \\beta &lt;1\\)</span>, it implies that we can find the &#8220;<span class=\"math\">\\(C\\)</span>-optimizing&#8221; <span class=\"math\">\\(h\\)</span> within any class of hypotheses by optimizing instead <span class=\"math\">\\(C^{\\prime}\\)</span> &#8212; a quantity that we can estimate given any particular noisy labeling realization <span class=\"math\">\\(y^{\\prime}_0\\)</span>&nbsp;as\n</p>\n<div class=\"math\">\\begin{eqnarray}\\tag{6}\nC^{\\prime}(h) \\approx \\left (Pr[h = 0 \\vert y^{\\prime} = 1]+ Pr[h = 1 \\vert y^{\\prime}= 0] \\right ) \\vert_{y^{\\prime} =y^{\\prime}_0}.\n\\end{eqnarray}</div>\n<h2>Application to no-negatives&nbsp;problem</h2>\n<p>To make connection between the no-negatives and noisy-labeling problems, one can remodel the former as one where all unlabeled examples are considered to actually be negative examples (<span class=\"math\">\\(y^{\\prime}_0 = 0\\)</span>). This relabeling gives a correct label to all examples in the original training set <span class=\"math\">\\(\\textbf{E}\\)</span> (where <span class=\"math\">\\(y = y^{\\prime}_0 = 1\\)</span>) as well as to all truly-negative examples (where <span class=\"math\">\\(y = y^{\\prime}_0 = 0\\)</span>). However, all positive examples not in <span class=\"math\">\\(\\textbf{E}\\)</span> are now incorrectly labeled (they are assigned <span class=\"math\">\\(y^{\\prime}_0 = 0\\)</span>): This new labeling <span class=\"math\">\\(y^{\\prime}_0\\)</span> is noisy, with <span class=\"math\">\\(\\alpha = Pr(y^{\\prime}_0 =0 \\vert y =1)\\)</span> and <span class=\"math\">\\(\\beta = Pr(y^{\\prime}_0 =1 \\vert y = 0 ) = 0\\)</span>. We can now apply the Blum and Michael approach: We first approximate <span class=\"math\">\\(C^{\\prime}\\)</span> as above, making use of the particular noisy label we have access to. Second, we minimize the approximated <span class=\"math\">\\(C^{\\prime}\\)</span> over some class of hypotheses <span class=\"math\">\\(\\{h\\}\\)</span>. This will in general return a non-uniform hypothesis (i.e., one that now makes use of the information contained in <span class=\"math\">\\(\\textbf{E}\\)</span>).</p>\n<h2>Hybrid noisy-logistic approach of Lee and Liu (plus a&nbsp;tweak)</h2>\n<p>The <span class=\"math\">\\(C \\propto C^{\\prime}\\)</span> result is slick and provides a rigorous method for attacking the no-negatives problem. Unfortunately, <span class=\"math\">\\(C^{\\prime}\\)</span> is not convex, and as a consequence it can be difficult to minimize for large <span class=\"math\">\\(\\vert \\textbf{F} \\vert\\)</span> &#8212; in fact, its minimization is <span class=\"caps\">NP</span>-hard. To mitigate this issue, Lee and Liu combine the noisy relabeling idea &#8212; now well-motivated by the Blum and Michael analysis &#8212; with logistic regression. They also suggest a particular re-weighting of the observed samples. However, we think that their particular choice of weighting is not very well-motivated, and we suggest here that one should instead pick an optimal weighting through consideration of a cross-validation set. With this approach, the method&nbsp;becomes:</p>\n<p>​1) As above, assign examples in <span class=\"math\">\\(\\textbf{E}\\)</span> label <span class=\"math\">\\(y^{\\prime} = 1\\)</span> and examples in <span class=\"math\">\\(\\textbf{F} - \\textbf{E}\\)</span> label <span class=\"math\">\\(y^{\\prime} = 0\\)</span>.<br>\n2) Construct the weighted logistic cost&nbsp;function\n</p>\n<div class=\"math\">\\begin{eqnarray}\\tag{7}\nJ(h; \\rho) \\equiv -\\frac{1}{\\vert \\textbf{E} \\vert}\\sum_{i=1}^{\\vert \\textbf{E} \\vert}  \n\\rho y^{\\prime}_i \\log(h_i) + (1-\\rho) (1-y^{\\prime}_i) \\log(1- h_i) + \\frac{\\Lambda}{2}\\sum_j T_j^2,\n\\end{eqnarray}</div>\n<p>\nwith <span class=\"math\">\\(\\rho \\in [0,1]\\)</span>, a re-weighting factor. (Lee and Liu suggest<span class=\"math\">\\(^2\\)</span> using <span class=\"math\">\\(\\rho = 1-\\frac{\\vert \\textbf{E} \\vert}{\\vert \\textbf{F} \\vert}\\)</span>).<br>\n3) Minimize <span class=\"math\">\\(J\\)</span>. By evaluating performance on a cross-validation set using your favorite criteria, optimize <span class=\"math\">\\(\\rho\\)</span> and <span class=\"math\">\\(\\Lambda\\)</span>.</p>\n<h2>Toy&nbsp;example</h2>\n<p>Here, we provide a toy system that allows for a sense of how the latter method discussed above works in practice. Given is a set of <span class=\"math\">\\(60\\)</span> grid points in the plane, which can be added/subtracted individually to the positive training set (<span class=\"math\">\\(\\textbf{E}\\)</span>, green fill) by mouse click (a few are selected by default). The remaining points are considered to not be in the training set, but are relabeled as negative examples &#8212; this introduces noise, as described above. Clicking compute returns the <span class=\"math\">\\(h\\)</span> values for each grid point, determined by minimizing the weighted cost function <span class=\"math\">\\(J\\)</span> above: Here, we use the features <span class=\"math\">\\(\\{1,x,y,x^2,xy,\\)</span> <span class=\"math\">\\(y^2,x^3, x^2 y,\\)</span> <span class=\"math\">\\(x y^2, y^3\\}\\)</span> to characterize each point. Those points with <span class=\"math\">\\(h\\)</span> values larger than <span class=\"math\">\\(0.5\\)</span> (i.e., those the hypothesis estimates as positive) are outlined in black. We have found that by carefully choosing the <span class=\"math\">\\(\\rho\\)</span> and <span class=\"math\">\\(\\Lambda\\)</span> values (often to be large and small, respectively), one can get a good fit to most training sets. By eye, the optimal weighting seems to often be close &#8212; but not necessarily equal to &#8212; the value suggested by Lee and&nbsp;Liu.</p>\n<p><em>Fig. 1: Interactive weighted noisy-no-negatives solver. Click &#8220;compute&#8221; to run logistic regression.</em>\n[<span class=\"caps\">NOTE</span>:  new site does not yet support processing - I hope to reinsert the interactive object here as soon as&nbsp;possible].</p>\n<h2>Discussion</h2>\n<p>In this note, we have discussed methods for tackling classification sans negative examples &#8212; a problem that we found perplexing at first sight. It is interesting that standard logistic regression returns a default-positive result for such problems, while the two latter methods we discussed here are based on assuming that all points in <span class=\"math\">\\(\\textbf{F} - \\textbf{E}\\)</span> are negatives. In fact, this assumption seems to be the essence of all the other methods referenced in Lee and Liu&#8217;s paper. Ultimately, these methods will only work if the training set provides a good sampling of the truly-positive space. If this is the case, then &#8220;defocusing&#8221; a bit, or blurring one&#8217;s eyes, will give a good sense of where the positive space sits. In the noisy-logistic approach, a good choice of <span class=\"math\">\\(\\rho\\)</span> and <span class=\"math\">\\(\\Lambda\\)</span> should effect a good result. Of course, when the training set does not sample the full positive space well, one can still use this approach to get a good approximation for the outline of the subspace&nbsp;sampled.</p>\n<h2>Footnotes</h2>\n<p><span class=\"math\">\\([1]\\)</span>: The target function <span class=\"math\">\\(y\\)</span> provides the unique minimum of <span class=\"math\">\\(C\\)</span>. Therefore, choosing <span class=\"math\">\\(C\\)</span> as our cost function and minimizing it over some class of hypotheses <span class=\"math\">\\(\\{h\\}\\)</span> should return a reasonable estimate for <span class=\"math\">\\(y\\)</span> (indeed, if <span class=\"math\">\\(y\\)</span> is in the search class, we will find&nbsp;it).</p>\n<p><span class=\"math\">\\([2]\\)</span>: Lee and Liu justify their weighting suggestion on the basis that it means that a randomly selected positive example contributes with expected weight <span class=\"math\">\\(&gt;0.5\\)</span> (see their paper). Yet, other weighting choices give even larger expected weights to the positive examples, so this is a poor justification. Nevertheless, their weighting choice does have the nice feature that the positive and negative spaces are effectively sampled with equal frequency. If optimizing over <span class=\"math\">\\(\\rho\\)</span> is too resource-costly for some application, using their weighting suggestion may be reasonable for this&nbsp;reason.</p>\n<script type=\"text/javascript\">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}\n</script>",
  "category": [
    "",
    ""
  ]
}