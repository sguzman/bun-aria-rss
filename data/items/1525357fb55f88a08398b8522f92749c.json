{
  "title": "Committed to Open Source",
  "link": "",
  "published": "2015-01-25T00:00:00+00:00",
  "updated": "2015-01-25T00:00:00+00:00",
  "id": "https://trevorstephens.com/open-source/committed-to-open-source",
  "content": "<p><code class=\"language-plaintext highlighter-rouge\">scikit-learn</code> is one of my most-used tools, be it at work, or playing in ML competitions. I thought it was high time that I contribute back to this awesome project, and last week one of my pull requests was merged into the master branch!</p>\n\n<p>In <code class=\"language-plaintext highlighter-rouge\">sklearn</code> 0.16 (coming soon) you will now be able to automatically weight your samples based on class. Sure you could do this manually before, but now it is also grid-searchable:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">make_classification</span><span class=\"p\">(</span><span class=\"n\">n_samples</span><span class=\"o\">=</span><span class=\"mi\">1000</span><span class=\"p\">,</span>\n                           <span class=\"n\">n_features</span><span class=\"o\">=</span><span class=\"mi\">20</span><span class=\"p\">,</span>\n                           <span class=\"n\">n_informative</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">,</span>\n                           <span class=\"n\">weights</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mf\">0.8</span><span class=\"p\">,</span> <span class=\"mf\">0.2</span><span class=\"p\">])</span>\n<span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s\">'class_weight'</span><span class=\"p\">:</span> <span class=\"p\">[{</span><span class=\"mi\">0</span><span class=\"p\">:</span> <span class=\"n\">i</span> <span class=\"o\">+</span> <span class=\"mf\">1.</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">:</span> <span class=\"mf\">10.</span> <span class=\"o\">-</span> <span class=\"n\">i</span><span class=\"p\">}</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">)]}</span>\n<span class=\"n\">clf</span> <span class=\"o\">=</span> <span class=\"n\">RandomForestClassifier</span><span class=\"p\">(</span><span class=\"n\">n_estimators</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">)</span>\n<span class=\"n\">grid</span> <span class=\"o\">=</span> <span class=\"n\">GridSearchCV</span><span class=\"p\">(</span><span class=\"n\">clf</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">)</span>\n<span class=\"n\">grid</span><span class=\"p\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>This dataset is quite messy, and unbalanced, so the weighting scheme for best performance may be a bit unclear. This particular grid-search iterates the sample weights by class from 10:1 to 1:10 and declares the winner:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"s\">'class_weight'</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"mi\">0</span><span class=\"p\">:</span> <span class=\"mf\">7.0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">:</span> <span class=\"mf\">4.0</span><span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>Of course, for the lazy or mega-huge-ensemble wielders, you also have a couple of presets to choose from: <code class=\"language-plaintext highlighter-rouge\">'auto'</code> and <code class=\"language-plaintext highlighter-rouge\">'subsample'</code> which will weight samples inversely proportional to the class frequencies. The <code class=\"language-plaintext highlighter-rouge\">'auto'</code> mode performs this (once) over the entire dataset, while the <code class=\"language-plaintext highlighter-rouge\">'subsample'</code> mode calculates the inverse frequencies of the classes in the bootstrap sample fed to the individual tree estimators (n_estimators times of course).</p>\n\n<p>My <a href=\"https://github.com/scikit-learn/scikit-learn/pull/3961\" target=\"_blank\">pull request</a> had the great, albeit unusual, pleasure of three code reviews. No doubt that Random Forests are one of the go-to classifiers out there, so I don’t blame them for a bit of caution with a new feature! Having my code picked apart by some of the very talented core contributors was a great experience and I learnt a lot from those guys, but the idea that <a href=\"http://pypi-ranking.info/module/scikit-learn\" target=\"_blank\">hundreds of thousands of users</a> may one day be running some lines of code that I wrote is a whole other level. It’s an amazing feeling. Now to figure out how to split up time between committing-to and consumption-of the code-base!</p>\n\n<p>If you can’t wait for the 0.16 public release, feel free to grab <a href=\"http://scikit-learn.org/stable/install.html#install-bleeding-edge\" target=\"_blank\">the development branch code</a>. There’s a ton of other excellent goodies from other contributors in there too.</p>\n\n<p>I truly hope that you can get a bit more out of your ensembles now, let me know if you used it in the comments!</p>",
  "author": {
    "name": "Trevor Stephens",
    "uri": "https://trevorstephens.com"
  },
  "category": [
    "",
    ""
  ],
  "summary": "Introducing class_weight to RandomForests in scikit-learn."
}