{
  "title": "The future is different: Large pre-trained language models fail in prediction tasks. (arXiv:2211.00384v2 [cs.CL] UPDATED)",
  "link": "http://arxiv.org/abs/2211.00384",
  "description": "<p>Large pre-trained language models (LPLM) have shown spectacular success when\nfine-tuned on downstream supervised tasks. Yet, it is known that their\nperformance can drastically drop when there is a distribution shift between the\ndata used during training and that used at inference time. In this paper we\nfocus on data distributions that naturally change over time and introduce four\nnew REDDIT datasets, namely the WALLSTREETBETS, ASKSCIENCE, THE DONALD, and\nPOLITICS sub-reddits. First, we empirically demonstrate that LPLM can display\naverage performance drops of about 88% (in the best case!) when predicting the\npopularity of future posts from sub-reddits whose topic distribution changes\nwith time. We then introduce a simple methodology that leverages neural\nvariational dynamic topic models and attention mechanisms to infer temporal\nlanguage model representations for regression tasks. Our models display\nperformance drops of only about 40% in the worst cases (2% in the best ones)\nwhen predicting the popularity of future posts, while using only about 7% of\nthe total number of parameters of LPLM and providing interpretable\nrepresentations that offer insight into real-world events, like the GameStop\nshort squeeze of 2021\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Cvejoski_K/0/1/0/all/0/1\">Kostadin Cvejoski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_R/0/1/0/all/0/1\">Rams&#xe9;s J. S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ojeda_C/0/1/0/all/0/1\">C&#xe9;sar Ojeda</a>"
}