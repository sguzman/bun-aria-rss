{
  "title": "Distributed Pandas on a Cluster with Dask DataFrames",
  "link": "",
  "updated": "2017-01-12T00:00:00+00:00",
  "id": "https://mrocklin.github.io/blog/work/2017/01/12/dask-dataframes",
  "content": "<p><em>This work is supported by <a href=\"http://continuum.io\">Continuum Analytics</a>\nthe <a href=\"http://www.darpa.mil/program/XDATA\">XDATA Program</a>\nand the Data Driven Discovery Initiative from the <a href=\"https://www.moore.org/\">Moore\nFoundation</a></em></p>\n\n<h2 id=\"summary\">Summary</h2>\n\n<p>Dask Dataframe extends the popular Pandas library to operate on big data-sets\non a distributed cluster.  We show its capabilities by running through common\ndataframe operations on a common dataset.  We break up these computations into\nthe following sections:</p>\n\n<ol>\n  <li>Introduction: Pandas is intuitive and fast, but needs Dask to scale</li>\n  <li>Read CSV and Basic operations\n    <ol>\n      <li>Read CSV</li>\n      <li>Basic Aggregations and Groupbys</li>\n      <li>Joins and Correlations</li>\n    </ol>\n  </li>\n  <li>Shuffles and Time Series</li>\n  <li>Parquet I/O</li>\n  <li>Final thoughts</li>\n  <li>What we could have done better</li>\n</ol>\n\n<h2 id=\"accompanying-plots\">Accompanying Plots</h2>\n\n<p>Throughout this post we accompany computational examples with profiles of\nexactly what task ran where on our cluster and when.  These profiles are\ninteractive <a href=\"https://bokeh.pydata.org\">Bokeh plots</a> that include every task\nthat every worker in our cluster runs over time.  For example the following\ncomputation <code class=\"language-plaintext highlighter-rouge\">read_csv</code> computation produces the following profile:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">dd</span><span class=\"p\">.</span><span class=\"n\">read_csv</span><span class=\"p\">(</span><span class=\"s\">'s3://dask-data/nyc-taxi/2015/*.csv'</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p><em>If you are reading this through a syndicated website like planet.python.org or\nthrough an RSS reader then these plots will not show up.  You may want to visit\n<a href=\"http://matthewrocklin.com/blog/work/2017/01/12/dask-dataframes\">http://matthewrocklin.com/blog/work/2017/01/12/dask-dataframes</a>\ndirectly.</em></p>\n\n<iframe src=\"https://cdn.rawgit.com/mrocklin/ade9d1e3b0f44b17a84a551e39946e58/raw/1c3345848d5313cc1c0ea827d66089bf200edaac/task-stream-read-csv.html\" width=\"800\" height=\"400\"></iframe>\n\n<p>Dask.dataframe breaks up reading this data into many small tasks of\ndifferent types.  For example reading bytes and parsing those bytes into\npandas dataframes.  Each rectangle corresponds to one task.  The y-axis\nenumerates each of the worker processes.  We have 64 processes spread over\n8 machines so there are 64 rows.  You can hover over any rectangle to get more\ninformation about that task.  You can also use the tools in the upper right\nto zoom around and focus on different regions in the computation.  In this\ncomputation we can see that workers interleave reading bytes from S3 (light\ngreen) and parsing bytes to dataframes (dark green).  The entire computation\ntook about a minute and most of the workers were busy the entire time (little\nwhite space).  Inter-worker communication is always depicted in red (which is\nabsent in this relatively straightforward computation.)</p>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>Pandas provides an intuitive, powerful, and fast data analysis experience on\ntabular data.  However, because Pandas uses only one thread of execution and\nrequires all data to be in memory at once, it doesn’t scale well to datasets\nmuch beyond the gigabyte scale.  That component is missing.  Generally people\nmove to Spark DataFrames on HDFS or a proper relational database to resolve\nthis scaling issue.  Dask is a Python library for parallel and distributed\ncomputing that aims to fill this need for parallelism among the PyData projects\n(NumPy, Pandas, Scikit-Learn, etc.).  Dask dataframes combine Dask and Pandas\nto deliver a faithful “big data” version of Pandas operating in parallel over a\ncluster.</p>\n\n<p><a href=\"http://matthewrocklin.com/blog/work/2016/02/22/dask-distributed-part-2\">I’ve written about this topic\nbefore</a>.\nThis blogpost is newer and will focus on performance and newer features like\nfast shuffles and the Parquet format.</p>\n\n<h2 id=\"csv-data-and-basic-operations\">CSV Data and Basic Operations</h2>\n\n<p>I have an eight node cluster on EC2 of <code class=\"language-plaintext highlighter-rouge\">m4.2xlarges</code> (eight cores, 30GB RAM each).\nDask is running on each node with one process per core.</p>\n\n<p>We have the <a href=\"http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml\">2015 Yellow Cab NYC Taxi\ndata</a> as 12 CSV\nfiles on S3.  We look at that data briefly with\n<a href=\"http://s3fs.readthedocs.io/en/latest/\">s3fs</a></p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">import</span> <span class=\"nn\">s3fs</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">s3</span> <span class=\"o\">=</span> <span class=\"n\">S3FileSystem</span><span class=\"p\">()</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">s3</span><span class=\"p\">.</span><span class=\"n\">ls</span><span class=\"p\">(</span><span class=\"s\">'dask-data/nyc-taxi/2015/'</span><span class=\"p\">)</span>\n<span class=\"p\">[</span><span class=\"s\">'dask-data/nyc-taxi/2015/yellow_tripdata_2015-01.csv'</span><span class=\"p\">,</span>\n <span class=\"s\">'dask-data/nyc-taxi/2015/yellow_tripdata_2015-02.csv'</span><span class=\"p\">,</span>\n <span class=\"s\">'dask-data/nyc-taxi/2015/yellow_tripdata_2015-03.csv'</span><span class=\"p\">,</span>\n <span class=\"s\">'dask-data/nyc-taxi/2015/yellow_tripdata_2015-04.csv'</span><span class=\"p\">,</span>\n <span class=\"s\">'dask-data/nyc-taxi/2015/yellow_tripdata_2015-05.csv'</span><span class=\"p\">,</span>\n <span class=\"s\">'dask-data/nyc-taxi/2015/yellow_tripdata_2015-06.csv'</span><span class=\"p\">,</span>\n <span class=\"s\">'dask-data/nyc-taxi/2015/yellow_tripdata_2015-07.csv'</span><span class=\"p\">,</span>\n <span class=\"s\">'dask-data/nyc-taxi/2015/yellow_tripdata_2015-08.csv'</span><span class=\"p\">,</span>\n <span class=\"s\">'dask-data/nyc-taxi/2015/yellow_tripdata_2015-09.csv'</span><span class=\"p\">,</span>\n <span class=\"s\">'dask-data/nyc-taxi/2015/yellow_tripdata_2015-10.csv'</span><span class=\"p\">,</span>\n <span class=\"s\">'dask-data/nyc-taxi/2015/yellow_tripdata_2015-11.csv'</span><span class=\"p\">,</span>\n <span class=\"s\">'dask-data/nyc-taxi/2015/yellow_tripdata_2015-12.csv'</span><span class=\"p\">]</span>\n</code></pre></div></div>\n\n<p>This data is too large to fit into Pandas on a single computer.  However, it\ncan fit in memory if we break it up into many small pieces and load these\npieces onto different computers across a cluster.</p>\n\n<p>We connect a client to our Dask cluster, composed of one centralized\n<code class=\"language-plaintext highlighter-rouge\">dask-scheduler</code> process and several <code class=\"language-plaintext highlighter-rouge\">dask-worker</code> processes running on each of the\nmachines in our cluster.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">dask.distributed</span> <span class=\"kn\">import</span> <span class=\"n\">Client</span>\n<span class=\"n\">client</span> <span class=\"o\">=</span> <span class=\"n\">Client</span><span class=\"p\">(</span><span class=\"s\">'scheduler-address:8786'</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>And we load our CSV data using <code class=\"language-plaintext highlighter-rouge\">dask.dataframe</code> which looks and feels just\nlike Pandas, even though it’s actually coordinating hundreds of small Pandas\ndataframes.  This takes about a minute to load and parse.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">import</span> <span class=\"nn\">dask.dataframe</span> <span class=\"k\">as</span> <span class=\"n\">dd</span>\n\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">dd</span><span class=\"p\">.</span><span class=\"n\">read_csv</span><span class=\"p\">(</span><span class=\"s\">'s3://dask-data/nyc-taxi/2015/*.csv'</span><span class=\"p\">,</span>\n                 <span class=\"n\">parse_dates</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">'tpep_pickup_datetime'</span><span class=\"p\">,</span> <span class=\"s\">'tpep_dropoff_datetime'</span><span class=\"p\">],</span>\n                 <span class=\"n\">storage_options</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s\">'anon'</span><span class=\"p\">:</span> <span class=\"bp\">True</span><span class=\"p\">})</span>\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">client</span><span class=\"p\">.</span><span class=\"n\">persist</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<iframe src=\"https://cdn.rawgit.com/mrocklin/ade9d1e3b0f44b17a84a551e39946e58/raw/1c3345848d5313cc1c0ea827d66089bf200edaac/task-stream-read-csv.html\" width=\"800\" height=\"400\"></iframe>\n\n<p>This cuts up our 12 CSV files on S3 into a few hundred blocks of bytes, each\n64MB large.  On each of these 64MB blocks we then call <code class=\"language-plaintext highlighter-rouge\">pandas.read_csv</code> to\ncreate a few hundred Pandas dataframes across our cluster, one for each block\nof bytes.  Our single Dask Dataframe object, <code class=\"language-plaintext highlighter-rouge\">df</code>, coordinates all of those\nPandas dataframes.  Because we’re just using Pandas calls it’s very easy for\nDask dataframes to use all of the tricks from Pandas.  For example we can use\nmost of the keyword arguments from <code class=\"language-plaintext highlighter-rouge\">pd.read_csv</code> in <code class=\"language-plaintext highlighter-rouge\">dd.read_csv</code> without\nhaving to relearn anything.</p>\n\n<p>This data is about 20GB on disk or 60GB in RAM.  It’s not huge, but is also\nlarger than we’d like to manage on a laptop, especially if we value\ninteractivity.  The interactive image above is a trace over time of what each\nof our 64 cores was doing at any given moment.  By hovering your mouse over the\nrectangles you can see that cores switched between downloading byte ranges from\nS3 and parsing those bytes with <code class=\"language-plaintext highlighter-rouge\">pandas.read_csv</code>.</p>\n\n<p>Our dataset includes every cab ride in the city of New York in the year of\n2015, including when and where it started and stopped, a breakdown of the fare,\netc.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">df</span><span class=\"p\">.</span><span class=\"n\">head</span><span class=\"p\">()</span>\n</code></pre></div></div>\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>VendorID</th>\n      <th>tpep_pickup_datetime</th>\n      <th>tpep_dropoff_datetime</th>\n      <th>passenger_count</th>\n      <th>trip_distance</th>\n      <th>pickup_longitude</th>\n      <th>pickup_latitude</th>\n      <th>RateCodeID</th>\n      <th>store_and_fwd_flag</th>\n      <th>dropoff_longitude</th>\n      <th>dropoff_latitude</th>\n      <th>payment_type</th>\n      <th>fare_amount</th>\n      <th>extra</th>\n      <th>mta_tax</th>\n      <th>tip_amount</th>\n      <th>tolls_amount</th>\n      <th>improvement_surcharge</th>\n      <th>total_amount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>2015-01-15 19:05:39</td>\n      <td>2015-01-15 19:23:42</td>\n      <td>1</td>\n      <td>1.59</td>\n      <td>-73.993896</td>\n      <td>40.750111</td>\n      <td>1</td>\n      <td>N</td>\n      <td>-73.974785</td>\n      <td>40.750618</td>\n      <td>1</td>\n      <td>12.0</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>3.25</td>\n      <td>0.0</td>\n      <td>0.3</td>\n      <td>17.05</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2015-01-10 20:33:38</td>\n      <td>2015-01-10 20:53:28</td>\n      <td>1</td>\n      <td>3.30</td>\n      <td>-74.001648</td>\n      <td>40.724243</td>\n      <td>1</td>\n      <td>N</td>\n      <td>-73.994415</td>\n      <td>40.759109</td>\n      <td>1</td>\n      <td>14.5</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>2.00</td>\n      <td>0.0</td>\n      <td>0.3</td>\n      <td>17.80</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>2015-01-10 20:33:38</td>\n      <td>2015-01-10 20:43:41</td>\n      <td>1</td>\n      <td>1.80</td>\n      <td>-73.963341</td>\n      <td>40.802788</td>\n      <td>1</td>\n      <td>N</td>\n      <td>-73.951820</td>\n      <td>40.824413</td>\n      <td>2</td>\n      <td>9.5</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.3</td>\n      <td>10.80</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>2015-01-10 20:33:39</td>\n      <td>2015-01-10 20:35:31</td>\n      <td>1</td>\n      <td>0.50</td>\n      <td>-74.009087</td>\n      <td>40.713818</td>\n      <td>1</td>\n      <td>N</td>\n      <td>-74.004326</td>\n      <td>40.719986</td>\n      <td>2</td>\n      <td>3.5</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.3</td>\n      <td>4.80</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>2015-01-10 20:33:39</td>\n      <td>2015-01-10 20:52:58</td>\n      <td>1</td>\n      <td>3.00</td>\n      <td>-73.971176</td>\n      <td>40.762428</td>\n      <td>1</td>\n      <td>N</td>\n      <td>-74.004181</td>\n      <td>40.742653</td>\n      <td>2</td>\n      <td>15.0</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.3</td>\n      <td>16.30</td>\n    </tr>\n  </tbody>\n</table>\n\n<h3 id=\"basic-aggregations-and-groupbys\">Basic Aggregations and Groupbys</h3>\n\n<p>As a quick exercise, we compute the length of the dataframe.  When we call\n<code class=\"language-plaintext highlighter-rouge\">len(df)</code> Dask.dataframe translates this into many <code class=\"language-plaintext highlighter-rouge\">len</code> calls on each of the\nconstituent Pandas dataframes, followed by communication of the intermediate\nresults to one node, followed by a <code class=\"language-plaintext highlighter-rouge\">sum</code> of all of the intermediate lengths.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">)</span>\n<span class=\"mi\">146112989</span>\n</code></pre></div></div>\n\n<iframe src=\"https://cdn.rawgit.com/mrocklin/ade9d1e3b0f44b17a84a551e39946e58/raw/1c3345848d5313cc1c0ea827d66089bf200edaac/task-stream-len.html\" width=\"640\" height=\"630\"></iframe>\n\n<p>This takes around 400-500ms.  You can see that a few hundred length\ncomputations happened quickly on the left, followed by some delay, then a bit\nof data transfer (the red bar in the plot), and a final summation call.</p>\n\n<p>More complex operations like simple groupbys look similar, although sometimes\nwith more communications.  Throughout this post we’re going to do more and more\ncomplex computations and our profiles will similarly become more and more rich\nwith information.  Here we compute the average trip distance, grouped by number\nof passengers.  We find that single and double person rides go far longer\ndistances on average.  We acheive this one big-data-groupby by performing many\nsmall Pandas groupbys and then cleverly combining their results.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">df</span><span class=\"p\">.</span><span class=\"n\">groupby</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">.</span><span class=\"n\">passenger_count</span><span class=\"p\">).</span><span class=\"n\">trip_distance</span><span class=\"p\">.</span><span class=\"n\">mean</span><span class=\"p\">().</span><span class=\"n\">compute</span><span class=\"p\">()</span>\n<span class=\"n\">passenger_count</span>\n<span class=\"mi\">0</span>     <span class=\"mf\">2.279183</span>\n<span class=\"mi\">1</span>    <span class=\"mf\">15.541413</span>\n<span class=\"mi\">2</span>    <span class=\"mf\">11.815871</span>\n<span class=\"mi\">3</span>     <span class=\"mf\">1.620052</span>\n<span class=\"mi\">4</span>     <span class=\"mf\">7.481066</span>\n<span class=\"mi\">5</span>     <span class=\"mf\">3.066019</span>\n<span class=\"mi\">6</span>     <span class=\"mf\">2.977158</span>\n<span class=\"mi\">9</span>     <span class=\"mf\">5.459763</span>\n<span class=\"mi\">7</span>     <span class=\"mf\">3.303054</span>\n<span class=\"mi\">8</span>     <span class=\"mf\">3.866298</span>\n<span class=\"n\">Name</span><span class=\"p\">:</span> <span class=\"n\">trip_distance</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"p\">:</span> <span class=\"n\">float64</span>\n</code></pre></div></div>\n\n<iframe src=\"https://cdn.rawgit.com/mrocklin/ade9d1e3b0f44b17a84a551e39946e58/raw/1c3345848d5313cc1c0ea827d66089bf200edaac/task-stream-groupby-sum.html\" width=\"640\" height=\"630\"></iframe>\n\n<p>As a more complex operation we see how well New Yorkers tip by hour of day and\nby day of week.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">df2</span> <span class=\"o\">=</span> <span class=\"n\">df</span><span class=\"p\">[(</span><span class=\"n\">df</span><span class=\"p\">.</span><span class=\"n\">tip_amount</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span><span class=\"p\">)</span> <span class=\"o\">&amp;</span> <span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">.</span><span class=\"n\">fare_amount</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span><span class=\"p\">)]</span>    <span class=\"c1\"># filter out bad rows\n</span><span class=\"n\">df2</span><span class=\"p\">[</span><span class=\"s\">'tip_fraction'</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">df2</span><span class=\"p\">.</span><span class=\"n\">tip_amount</span> <span class=\"o\">/</span> <span class=\"n\">df2</span><span class=\"p\">.</span><span class=\"n\">fare_amount</span>  <span class=\"c1\"># make new column\n</span>\n<span class=\"n\">dayofweek</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">df2</span><span class=\"p\">.</span><span class=\"n\">groupby</span><span class=\"p\">(</span><span class=\"n\">df2</span><span class=\"p\">.</span><span class=\"n\">tpep_pickup_datetime</span><span class=\"p\">.</span><span class=\"n\">dt</span><span class=\"p\">.</span><span class=\"n\">dayofweek</span><span class=\"p\">)</span>\n                <span class=\"p\">.</span><span class=\"n\">tip_fraction</span>\n                <span class=\"p\">.</span><span class=\"n\">mean</span><span class=\"p\">())</span>\n<span class=\"n\">hour</span>      <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">df2</span><span class=\"p\">.</span><span class=\"n\">groupby</span><span class=\"p\">(</span><span class=\"n\">df2</span><span class=\"p\">.</span><span class=\"n\">tpep_pickup_datetime</span><span class=\"p\">.</span><span class=\"n\">dt</span><span class=\"p\">.</span><span class=\"n\">hour</span><span class=\"p\">)</span>\n                <span class=\"p\">.</span><span class=\"n\">tip_fraction</span>\n                <span class=\"p\">.</span><span class=\"n\">mean</span><span class=\"p\">())</span>\n</code></pre></div></div>\n\n<iframe src=\"https://cdn.rawgit.com/mrocklin/ade9d1e3b0f44b17a84a551e39946e58/raw/1c3345848d5313cc1c0ea827d66089bf200edaac/task-stream-groupby-datetime.html\" width=\"800\" height=\"400\"></iframe>\n\n<p><a href=\"https://mrocklin.github.io/blog/images/tip-fraction-hourly.png\">\n    <img src=\"https://mrocklin.github.io/blog/images/tip-fraction-hourly.png\" alt=\"tip fraction by hour\" width=\"80%\" /></a></p>\n\n<p>We see that New Yorkers are generally pretty generous, tipping around 20%-25%\non average.  We also notice that they become <em>very generous</em> at 4am, tipping an\naverage of 38%.</p>\n\n<p>This more complex operation uses more of the Dask dataframe API (which mimics\nthe Pandas API).  Pandas users should find the code above fairly familiar.  We\nremove rows with zero fare or zero tip (not every tip gets recorded), make a\nnew column which is the ratio of the tip amount to the fare amount, and then\ngroupby the day of week and hour of day, computing the average tip fraction for\neach hour/day.</p>\n\n<p>Dask evaluates this computation with thousands of small Pandas calls across the\ncluster (try clicking the wheel zoom icon in the upper right of the image\nabove and zooming in).  The answer comes back in about 3 seconds.</p>\n\n<h3 id=\"joins-and-correlations\">Joins and Correlations</h3>\n\n<p>To show off more basic functionality we’ll join this Dask dataframe against a\nsmaller Pandas dataframe that includes names of some of the more cryptic\ncolumns.  Then we’ll correlate two derived columns to determine if there is a\nrelationship between paying Cash and the recorded tip.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">payments</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"p\">.</span><span class=\"n\">Series</span><span class=\"p\">({</span><span class=\"mi\">1</span><span class=\"p\">:</span> <span class=\"s\">'Credit Card'</span><span class=\"p\">,</span>\n                          <span class=\"mi\">2</span><span class=\"p\">:</span> <span class=\"s\">'Cash'</span><span class=\"p\">,</span>\n                          <span class=\"mi\">3</span><span class=\"p\">:</span> <span class=\"s\">'No Charge'</span><span class=\"p\">,</span>\n                          <span class=\"mi\">4</span><span class=\"p\">:</span> <span class=\"s\">'Dispute'</span><span class=\"p\">,</span>\n                          <span class=\"mi\">5</span><span class=\"p\">:</span> <span class=\"s\">'Unknown'</span><span class=\"p\">,</span>\n                          <span class=\"mi\">6</span><span class=\"p\">:</span> <span class=\"s\">'Voided trip'</span><span class=\"p\">})</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">df2</span> <span class=\"o\">=</span> <span class=\"n\">df</span><span class=\"p\">.</span><span class=\"n\">merge</span><span class=\"p\">(</span><span class=\"n\">payments</span><span class=\"p\">,</span> <span class=\"n\">left_on</span><span class=\"o\">=</span><span class=\"s\">'payment_type'</span><span class=\"p\">,</span> <span class=\"n\">right_index</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">df2</span><span class=\"p\">.</span><span class=\"n\">groupby</span><span class=\"p\">(</span><span class=\"n\">df2</span><span class=\"p\">.</span><span class=\"n\">payment_name</span><span class=\"p\">).</span><span class=\"n\">tip_amount</span><span class=\"p\">.</span><span class=\"n\">mean</span><span class=\"p\">().</span><span class=\"n\">compute</span><span class=\"p\">()</span>\n<span class=\"n\">payment_name</span>\n<span class=\"n\">Cash</span>           <span class=\"mf\">0.000217</span>\n<span class=\"n\">Credit</span> <span class=\"n\">Card</span>    <span class=\"mf\">2.757708</span>\n<span class=\"n\">Dispute</span>       <span class=\"o\">-</span><span class=\"mf\">0.011553</span>\n<span class=\"n\">No</span> <span class=\"n\">charge</span>      <span class=\"mf\">0.003902</span>\n<span class=\"n\">Unknown</span>        <span class=\"mf\">0.428571</span>\n<span class=\"n\">Name</span><span class=\"p\">:</span> <span class=\"n\">tip_amount</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"p\">:</span> <span class=\"n\">float64</span>\n</code></pre></div></div>\n\n<p>We see that while the average tip for a credit card transaction is $2.75, the\naverage tip for a cash transaction is very close to zero.  At first glance it\nseems like cash tips aren’t being reported.  To investigate this a bit further\nlets compute the Pearson correlation between paying cash and having zero tip.\nAgain, this code should look very familiar to Pandas users.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">zero_tip</span> <span class=\"o\">=</span> <span class=\"n\">df2</span><span class=\"p\">.</span><span class=\"n\">tip_amount</span> <span class=\"o\">==</span> <span class=\"mi\">0</span>\n<span class=\"n\">cash</span>     <span class=\"o\">=</span> <span class=\"n\">df2</span><span class=\"p\">.</span><span class=\"n\">payment_name</span> <span class=\"o\">==</span> <span class=\"s\">'Cash'</span>\n\n<span class=\"n\">dd</span><span class=\"p\">.</span><span class=\"n\">concat</span><span class=\"p\">([</span><span class=\"n\">zero_tip</span><span class=\"p\">,</span> <span class=\"n\">cash</span><span class=\"p\">],</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">).</span><span class=\"n\">corr</span><span class=\"p\">().</span><span class=\"n\">compute</span><span class=\"p\">()</span>\n</code></pre></div></div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tip_amount</th>\n      <th>payment_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>tip_amount</th>\n      <td>1.000000</td>\n      <td>0.943123</td>\n    </tr>\n    <tr>\n      <th>payment_name</th>\n      <td>0.943123</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n\n<iframe src=\"https://cdn.rawgit.com/mrocklin/ade9d1e3b0f44b17a84a551e39946e58/raw/1c3345848d5313cc1c0ea827d66089bf200edaac/task-stream-join-corr.html\" width=\"800\" height=\"400\"></iframe>\n\n<p>So we see that standard operations like row filtering, column selection,\ngroupby-aggregations, joining with a Pandas dataframe, correlations, etc. all\nlook and feel like the Pandas interface.  Additionally, we’ve seen through\nprofile plots that most of the time is spent just running Pandas functions on\nour workers, so Dask.dataframe is, in most cases, adding relatively little\noverhead.  These little functions represented by the rectangles in these plots\nare <em>just pandas functions</em>.  For example the plot above has many rectangles\nlabeled <code class=\"language-plaintext highlighter-rouge\">merge</code> if you hover over them.  This is just the standard\n<code class=\"language-plaintext highlighter-rouge\">pandas.merge</code> function that we love and know to be very fast in memory.</p>\n\n<h2 id=\"shuffles-and-time-series\">Shuffles and Time Series</h2>\n\n<p>Distributed dataframe experts will know that none of the operations above\nrequire a <em>shuffle</em>.  That is we can do most of our work with relatively little\ninter-node communication.  However not all operations can avoid communication\nlike this and sometimes we need to exchange most of the data between different\nworkers.</p>\n\n<p>For example if our dataset is sorted by customer ID but we want to sort it by\ntime then we need to collect all the rows for January over to one Pandas\ndataframe, all the rows for February over to another, etc..  This operation is\ncalled a shuffle and is the base of computations like groupby-apply,\ndistributed joins on columns that are not the index, etc..</p>\n\n<p>You can do a lot with dask.dataframe without performing shuffles, but sometimes\nit’s necessary.  In the following example we sort our data by pickup datetime.\nThis will allow fast lookups, fast joins, and fast time series operations, all\ncommon cases.  We do one shuffle ahead of time to make all future computations\nfast.</p>\n\n<p>We set the index as the pickup datetime column.  This takes anywhere from\n25-40s and is largely network bound (60GB, some text, eight machines with\neight cores each on AWS non-enhanced network).  This also requires running\nsomething like 16000 tiny tasks on the cluster.  It’s worth zooming in on the\nplot below.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">c</span><span class=\"p\">.</span><span class=\"n\">persist</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">.</span><span class=\"n\">set_index</span><span class=\"p\">(</span><span class=\"s\">'tpep_pickup_datetime'</span><span class=\"p\">))</span>\n</code></pre></div></div>\n\n<iframe src=\"https://cdn.rawgit.com/mrocklin/ade9d1e3b0f44b17a84a551e39946e58/raw/1c3345848d5313cc1c0ea827d66089bf200edaac/task-stream-set-index.html\" width=\"800\" height=\"400\"></iframe>\n\n<p>This operation is expensive, far more expensive than it was with Pandas when\nall of the data was in the same memory space on the same computer.  This is a\ngood time to point out that you should only use distributed tools like\nDask.datframe and Spark after tools like Pandas break down.  We should only\nmove to distributed systems when absolutely necessary.  However, when it does\nbecome necessary, it’s nice knowing that Dask.dataframe can faithfully execute\nPandas operations, even if some of them take a bit longer.</p>\n\n<p>As a result of this shuffle our data is now nicely sorted by time, which will\nkeep future operations close to optimal.  We can see how the dataset is sorted\nby pickup time by quickly looking at the first entries, last entries, and\nentries for a particular day.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">df</span><span class=\"p\">.</span><span class=\"n\">head</span><span class=\"p\">()</span>  <span class=\"c1\"># has the first entries of 2015\n</span></code></pre></div></div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>VendorID</th>\n      <th>tpep_dropoff_datetime</th>\n      <th>passenger_count</th>\n      <th>trip_distance</th>\n      <th>pickup_longitude</th>\n      <th>pickup_latitude</th>\n      <th>RateCodeID</th>\n      <th>store_and_fwd_flag</th>\n      <th>dropoff_longitude</th>\n      <th>dropoff_latitude</th>\n      <th>payment_type</th>\n      <th>fare_amount</th>\n      <th>extra</th>\n      <th>mta_tax</th>\n      <th>tip_amount</th>\n      <th>tolls_amount</th>\n      <th>improvement_surcharge</th>\n      <th>total_amount</th>\n    </tr>\n    <tr>\n      <th>tpep_pickup_datetime</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2015-01-01 00:00:00</th>\n      <td>2</td>\n      <td>2015-01-01 00:00:00</td>\n      <td>3</td>\n      <td>1.56</td>\n      <td>-74.001320</td>\n      <td>40.729057</td>\n      <td>1</td>\n      <td>N</td>\n      <td>-74.010208</td>\n      <td>40.719662</td>\n      <td>1</td>\n      <td>7.5</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.3</td>\n      <td>8.8</td>\n    </tr>\n    <tr>\n      <th>2015-01-01 00:00:00</th>\n      <td>2</td>\n      <td>2015-01-01 00:00:00</td>\n      <td>1</td>\n      <td>1.68</td>\n      <td>-73.991547</td>\n      <td>40.750069</td>\n      <td>1</td>\n      <td>N</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2</td>\n      <td>10.0</td>\n      <td>0.0</td>\n      <td>0.5</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.3</td>\n      <td>10.8</td>\n    </tr>\n    <tr>\n      <th>2015-01-01 00:00:00</th>\n      <td>1</td>\n      <td>2015-01-01 00:11:26</td>\n      <td>5</td>\n      <td>4.00</td>\n      <td>-73.971436</td>\n      <td>40.760201</td>\n      <td>1</td>\n      <td>N</td>\n      <td>-73.921181</td>\n      <td>40.768269</td>\n      <td>2</td>\n      <td>13.5</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>14.5</td>\n    </tr>\n  </tbody>\n</table>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">df</span><span class=\"p\">.</span><span class=\"n\">tail</span><span class=\"p\">()</span>  <span class=\"c1\"># has the last entries of 2015\n</span></code></pre></div></div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>VendorID</th>\n      <th>tpep_dropoff_datetime</th>\n      <th>passenger_count</th>\n      <th>trip_distance</th>\n      <th>pickup_longitude</th>\n      <th>pickup_latitude</th>\n      <th>RateCodeID</th>\n      <th>store_and_fwd_flag</th>\n      <th>dropoff_longitude</th>\n      <th>dropoff_latitude</th>\n      <th>payment_type</th>\n      <th>fare_amount</th>\n      <th>extra</th>\n      <th>mta_tax</th>\n      <th>tip_amount</th>\n      <th>tolls_amount</th>\n      <th>improvement_surcharge</th>\n      <th>total_amount</th>\n    </tr>\n    <tr>\n      <th>tpep_pickup_datetime</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2015-12-31 23:59:56</th>\n      <td>1</td>\n      <td>2016-01-01 00:09:25</td>\n      <td>1</td>\n      <td>1.00</td>\n      <td>-73.973900</td>\n      <td>40.742893</td>\n      <td>1</td>\n      <td>N</td>\n      <td>-73.989571</td>\n      <td>40.750549</td>\n      <td>1</td>\n      <td>8.0</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>1.85</td>\n      <td>0.0</td>\n      <td>0.3</td>\n      <td>11.15</td>\n    </tr>\n    <tr>\n      <th>2015-12-31 23:59:58</th>\n      <td>1</td>\n      <td>2016-01-01 00:05:19</td>\n      <td>2</td>\n      <td>2.00</td>\n      <td>-73.965271</td>\n      <td>40.760281</td>\n      <td>1</td>\n      <td>N</td>\n      <td>-73.939514</td>\n      <td>40.752388</td>\n      <td>2</td>\n      <td>7.5</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.3</td>\n      <td>8.80</td>\n    </tr>\n    <tr>\n      <th>2015-12-31 23:59:59</th>\n      <td>2</td>\n      <td>2016-01-01 00:10:26</td>\n      <td>1</td>\n      <td>1.96</td>\n      <td>-73.997559</td>\n      <td>40.725693</td>\n      <td>1</td>\n      <td>N</td>\n      <td>-74.017120</td>\n      <td>40.705322</td>\n      <td>2</td>\n      <td>8.5</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.3</td>\n      <td>9.80</td>\n    </tr>\n  </tbody>\n</table>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">df</span><span class=\"p\">.</span><span class=\"n\">loc</span><span class=\"p\">[</span><span class=\"s\">'2015-05-05'</span><span class=\"p\">].</span><span class=\"n\">head</span><span class=\"p\">()</span>  <span class=\"c1\"># has the entries for just May 5th\n</span></code></pre></div></div>\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>VendorID</th>\n      <th>tpep_dropoff_datetime</th>\n      <th>passenger_count</th>\n      <th>trip_distance</th>\n      <th>pickup_longitude</th>\n      <th>pickup_latitude</th>\n      <th>RateCodeID</th>\n      <th>store_and_fwd_flag</th>\n      <th>dropoff_longitude</th>\n      <th>dropoff_latitude</th>\n      <th>payment_type</th>\n      <th>fare_amount</th>\n      <th>extra</th>\n      <th>mta_tax</th>\n      <th>tip_amount</th>\n      <th>tolls_amount</th>\n      <th>improvement_surcharge</th>\n      <th>total_amount</th>\n    </tr>\n    <tr>\n      <th>tpep_pickup_datetime</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2015-05-05</th>\n      <td>2</td>\n      <td>2015-05-05 00:00:00</td>\n      <td>1</td>\n      <td>1.20</td>\n      <td>-73.981941</td>\n      <td>40.766460</td>\n      <td>1</td>\n      <td>N</td>\n      <td>-73.972771</td>\n      <td>40.758007</td>\n      <td>2</td>\n      <td>6.5</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.3</td>\n      <td>8.30</td>\n    </tr>\n    <tr>\n      <th>2015-05-05</th>\n      <td>1</td>\n      <td>2015-05-05 00:10:12</td>\n      <td>1</td>\n      <td>1.70</td>\n      <td>-73.994675</td>\n      <td>40.750507</td>\n      <td>1</td>\n      <td>N</td>\n      <td>-73.980247</td>\n      <td>40.738560</td>\n      <td>1</td>\n      <td>9.0</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>2.57</td>\n      <td>0.00</td>\n      <td>0.3</td>\n      <td>12.87</td>\n    </tr>\n    <tr>\n      <th>2015-05-05</th>\n      <td>1</td>\n      <td>2015-05-05 00:07:50</td>\n      <td>1</td>\n      <td>2.50</td>\n      <td>-74.002930</td>\n      <td>40.733681</td>\n      <td>1</td>\n      <td>N</td>\n      <td>-74.013603</td>\n      <td>40.702362</td>\n      <td>2</td>\n      <td>9.5</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.3</td>\n      <td>10.80</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>Because we know exactly which Pandas dataframe holds which data we can\nexecute row-local queries like this very quickly.  The total round trip from\npressing enter in the interpreter or notebook is about 40ms.  For reference,\n40ms is the delay between two frames in a movie running at 25 Hz.  This means\nthat it’s fast enough that human users perceive this query to be entirely\nfluid.</p>\n\n<h3 id=\"time-series\">Time Series</h3>\n\n<p>Additionally, once we have a nice datetime index all of Pandas’ time series\nfunctionality becomes available to us.</p>\n\n<p>For example we can resample by day:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">.</span><span class=\"n\">passenger_count</span>\n       <span class=\"p\">.</span><span class=\"n\">resample</span><span class=\"p\">(</span><span class=\"s\">'1d'</span><span class=\"p\">)</span>\n       <span class=\"p\">.</span><span class=\"n\">mean</span><span class=\"p\">()</span>\n       <span class=\"p\">.</span><span class=\"n\">compute</span><span class=\"p\">()</span>\n       <span class=\"p\">.</span><span class=\"n\">plot</span><span class=\"p\">())</span>\n</code></pre></div></div>\n\n<p><a href=\"https://mrocklin.github.io/blog/images/resample-day.png\">\n    <img src=\"https://mrocklin.github.io/blog/images/resample-day.png\" alt=\"resample by day\" width=\"60%\" /></a></p>\n\n<iframe src=\"https://cdn.rawgit.com/mrocklin/ade9d1e3b0f44b17a84a551e39946e58/raw/1c3345848d5313cc1c0ea827d66089bf200edaac/task-stream-resample.html\" width=\"800\" height=\"400\"></iframe>\n\n<p>We observe a strong periodic signal here.  The number of passengers is reliably\nhigher on the weekends.</p>\n\n<p>We can perform a rolling aggregation in about a second:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">s</span> <span class=\"o\">=</span> <span class=\"n\">client</span><span class=\"p\">.</span><span class=\"n\">persist</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">.</span><span class=\"n\">passenger_count</span><span class=\"p\">.</span><span class=\"n\">rolling</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">).</span><span class=\"n\">mean</span><span class=\"p\">())</span>\n</code></pre></div></div>\n\n<iframe src=\"https://cdn.rawgit.com/mrocklin/ade9d1e3b0f44b17a84a551e39946e58/raw/1c3345848d5313cc1c0ea827d66089bf200edaac/task-stream-rolling-mean.html\" width=\"800\" height=\"400\"></iframe>\n\n<p>Because Dask.dataframe inherits the Pandas index all of these operations become\nvery fast and intuitive.</p>\n\n<h2 id=\"parquet\">Parquet</h2>\n\n<p>Pandas’ standard “fast” recommended storage solution has generally been the\nHDF5 data format.  Unfortunately the HDF5 file format is not ideal for\ndistributed computing, so most Dask dataframe users have had to switch down to\nCSV historically.  This is unfortunate because CSV is slow, doesn’t support\npartial queries (you can’t read in just one column), and also isn’t supported\nwell by the other standard distributed Dataframe solution, Spark.  This makes it\nhard to move data back and forth.</p>\n\n<p>Fortunately there are now two decent Python readers for Parquet, a fast\ncolumnar binary store that shards nicely on distributed data stores like the\nHadoop File System (HDFS, not to be confused with HDF5) and Amazon’s S3.  The\nalready fast <a href=\"https://github.com/apache/parquet-cpp\">Parquet-cpp project</a> has\nbeen growing Python and Pandas support through\n<a href=\"http://pyarrow.readthedocs.io/en/latest/\">Arrow</a>, and the <a href=\"http://fastparquet.readthedocs.io/\">Fastparquet\nproject</a>, which is an offshoot from the\n<a href=\"https://github.com/jcrobak/parquet-python\">pure-python <code class=\"language-plaintext highlighter-rouge\">parquet</code> library</a> has\nbeen growing speed through use of\n<a href=\"https://docs.scipy.org/doc/numpy/reference/\">NumPy</a> and\n<a href=\"http://numba.pydata.org/\">Numba</a>.</p>\n\n<p>Using Fastparquet under the hood, Dask.dataframe users can now happily read and\nwrite to Parquet files.  This increases speed, decreases storage costs, and\nprovides a shared format that both Dask dataframes and Spark dataframes can\nunderstand, improving the ability to use both computational systems in the same\nworkflow.</p>\n\n<p>Writing our Dask dataframe to S3 can be as simple as the following:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">df</span><span class=\"p\">.</span><span class=\"n\">to_parquet</span><span class=\"p\">(</span><span class=\"s\">'s3://dask-data/nyc-taxi/tmp/parquet'</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>However there are also a variety of options we can use to store our data more\ncompactly through compression, encodings, etc..  Expert users will probably\nrecognize some of the terms below.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">df</span><span class=\"p\">.</span><span class=\"n\">astype</span><span class=\"p\">({</span><span class=\"s\">'VendorID'</span><span class=\"p\">:</span> <span class=\"s\">'uint8'</span><span class=\"p\">,</span>\n                <span class=\"s\">'passenger_count'</span><span class=\"p\">:</span> <span class=\"s\">'uint8'</span><span class=\"p\">,</span>\n                <span class=\"s\">'RateCodeID'</span><span class=\"p\">:</span> <span class=\"s\">'uint8'</span><span class=\"p\">,</span>\n                <span class=\"s\">'payment_type'</span><span class=\"p\">:</span> <span class=\"s\">'uint8'</span><span class=\"p\">})</span>\n\n<span class=\"n\">df</span><span class=\"p\">.</span><span class=\"n\">to_parquet</span><span class=\"p\">(</span><span class=\"s\">'s3://dask-data/nyc-taxi/tmp/parquet'</span><span class=\"p\">,</span>\n              <span class=\"n\">compression</span><span class=\"o\">=</span><span class=\"s\">'snappy'</span><span class=\"p\">,</span>\n              <span class=\"n\">has_nulls</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">,</span>\n              <span class=\"n\">object_encoding</span><span class=\"o\">=</span><span class=\"s\">'utf8'</span><span class=\"p\">,</span>\n              <span class=\"n\">fixed_text</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s\">'store_and_fwd_flag'</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">})</span>\n</code></pre></div></div>\n\n<iframe src=\"https://cdn.rawgit.com/mrocklin/ade9d1e3b0f44b17a84a551e39946e58/raw/1c3345848d5313cc1c0ea827d66089bf200edaac/task-stream-to-parquet.html\" width=\"800\" height=\"400\"></iframe>\n\n<p>We can then read our nicely indexed dataframe back with the\n<code class=\"language-plaintext highlighter-rouge\">dd.read_parquet</code> function:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">df2</span> <span class=\"o\">=</span> <span class=\"n\">dd</span><span class=\"p\">.</span><span class=\"n\">read_parquet</span><span class=\"p\">(</span><span class=\"s\">'s3://dask-data/nyc-taxi/tmp/parquet'</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>The main benefit here is that we can quickly compute on single columns.  The\nfollowing computation runs in around 6 seconds, even though we don’t have any\ndata in memory to start (recall that we started this blogpost with a\nminute-long call to <code class=\"language-plaintext highlighter-rouge\">read_csv</code>.and\n<a href=\"http://distributed.readthedocs.io/en/latest/api.html#distributed.client.Client.persist\">Client.persist</a>)</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">df2</span><span class=\"p\">.</span><span class=\"n\">passenger_count</span><span class=\"p\">.</span><span class=\"n\">value_counts</span><span class=\"p\">().</span><span class=\"n\">compute</span><span class=\"p\">()</span>\n<span class=\"mi\">1</span>    <span class=\"mi\">102991045</span>\n<span class=\"mi\">2</span>     <span class=\"mi\">20901372</span>\n<span class=\"mi\">5</span>      <span class=\"mi\">7939001</span>\n<span class=\"mi\">3</span>      <span class=\"mi\">6135107</span>\n<span class=\"mi\">6</span>      <span class=\"mi\">5123951</span>\n<span class=\"mi\">4</span>      <span class=\"mi\">2981071</span>\n<span class=\"mi\">0</span>        <span class=\"mi\">40853</span>\n<span class=\"mi\">7</span>          <span class=\"mi\">239</span>\n<span class=\"mi\">8</span>          <span class=\"mi\">181</span>\n<span class=\"mi\">9</span>          <span class=\"mi\">169</span>\n<span class=\"n\">Name</span><span class=\"p\">:</span> <span class=\"n\">passenger_count</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"p\">:</span> <span class=\"n\">int64</span>\n</code></pre></div></div>\n\n<h2 id=\"final-thoughts\">Final Thoughts</h2>\n\n<p>With the recent addition of faster shuffles and Parquet support, Dask\ndataframes become significantly more attractive.  This blogpost gave a few\ncategories of common computations, along with precise profiles of their\nexecution on a small cluster.  Hopefully people find this combination of Pandas\nsyntax and scalable computing useful.</p>\n\n<p>Now would also be a good time to remind people that Dask dataframe is only one\nmodule among many within the <a href=\"http://dask.pydata.org/en/latest/\">Dask project</a>.\nDataframes are nice, certainly, but Dask’s main strength is its flexibility to\nmove beyond just plain dataframe computations to handle even more complex\nproblems.</p>\n\n<h2 id=\"learn-more\">Learn More</h2>\n\n<p>If you’d like to learn more about Dask dataframe, the Dask distributed system,\nor other components you should look at the following documentation:</p>\n\n<ol>\n  <li><a href=\"http://dask.pydata.org/en/latest/\">http://dask.pydata.org/en/latest/</a></li>\n  <li><a href=\"http://distributed.readthedocs.io/en/latest/\">http://distributed.readthedocs.io/en/latest/</a></li>\n</ol>\n\n<p>The workflows presented here are captured in the following notebooks (among\nother examples):</p>\n\n<ol>\n  <li><a href=\"https://gist.github.com/mrocklin/ada85ef06d625947f7b34886fd2710f8\">NYC Taxi example, shuffling, others</a></li>\n  <li><a href=\"https://gist.github.com/mrocklin/89bccf2f4f37611b40c18967bb182066\">Parquet</a></li>\n</ol>\n\n<h2 id=\"what-we-could-have-done-better\">What we could have done better</h2>\n\n<p>As always with computational posts we include a section on what went wrong, or\nwhat could have gone better.</p>\n\n<ol>\n  <li>The 400ms computation of <code class=\"language-plaintext highlighter-rouge\">len(df)</code> is a regression from previous\nversions where this was closer to 100ms.  We’re getting bogged down\nsomewhere in many small inter-worker communications.</li>\n  <li>It would be nice to repeat this computation at a larger scale.  Dask\ndeployments in the wild are often closer to 1000 cores rather than the 64\ncore cluster we have here and datasets are often in the terrabyte scale\nrather than our 60 GB NYC Taxi dataset.  Unfortunately representative large\nopen datasets are hard to find.</li>\n  <li>The Parquet timings are nice, but there is still room for improvement.  We\nseem to be making many small expensive queries of S3 when reading Thrift\nheaders.</li>\n  <li>It would be nice to support both Python Parquet readers, both the\n<a href=\"http://numba.pydata.org/\">Numba</a> solution\n<a href=\"https://fastparquet.readthedocs.io\">fastparquet</a> and the C++ solution\n<a href=\"https://github.com/apache/parquet-cpp\">parquet-cpp</a></li>\n</ol>"
}