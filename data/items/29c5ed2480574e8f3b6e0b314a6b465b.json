{
  "title": "Introducing vbench, new code performance analysis and monitoring tool",
  "link": "",
  "published": "2011-12-18T00:00:00-08:00",
  "updated": "2011-12-18T00:00:00-08:00",
  "author": {
    "name": "Wes McKinney"
  },
  "id": "tag:wesmckinney.com,2011-12-18:/blog/introducing-vbench-new-code-performance-analysis-and-monitoring-tool/",
  "summary": "<p>Do you know how fast your code is? Is it faster than it was last week? Or a month ago? How do you know if you accidentally made a function slower by changes elsewhere? Unintentional performance regressions are extremely common in my experience: it's hard to unit test the performance â€¦</p>",
  "content": "<p>Do you know how fast your code is? Is it faster than it was last week? Or a month ago? How do you know if you accidentally made a function slower by changes elsewhere? Unintentional performance regressions are extremely common in my experience: it's hard to unit test the performance of your code. Over time I have gotten tired of playing the game of \"performance whack-a-mole\". Thus, I started hacking together a little weekend project that I'm calling <strong><a href=\"http://github.com/pydata/vbench\" title=\"vbench\" target=\"_blank\">vbench</a></strong>. If someone thinks up a cleverer name, I'm all ears.</p>\n<p><a href=\"http://pandas.sourceforge.net/vbench.html \" title=\"vbench page\" target=\"_blank\">Link to pandas benchmarks page produced using vbench</a></p>\n<h2>What is vbench?</h2>\n<p>vbench is a super-lightweight Python library for running a collection of performance benchmarks over the course of your source repository's history. Since I'm a GitHub user, it only does git for now, but it could be generalized to support other VCSs. Basically, you define a benchmark:</p>\n<div class=\"github\"><pre><span></span><code><span class=\"n\">common_setup</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;&quot;&quot;</span>\n<span class=\"s2\">from pandas import *</span>\n<span class=\"s2\">import pandas.util.testing as tm</span>\n<span class=\"s2\">import random</span>\n<span class=\"s2\">import numpy as np</span>\n<span class=\"s2\">&quot;&quot;&quot;</span>\n\n<span class=\"n\">setup</span> <span class=\"o\">=</span> <span class=\"n\">common_setup</span> <span class=\"o\">+</span> <span class=\"s2\">&quot;&quot;&quot;</span>\n\n<span class=\"s2\">N = 100000</span>\n<span class=\"s2\">ngroups = 100</span>\n\n<span class=\"s2\">def get_test_data(ngroups=100, n=N):</span>\n<span class=\"s2\">    unique_groups = range(ngroups)</span>\n<span class=\"s2\">    arr = np.asarray(np.tile(unique_groups, n / ngroups), dtype=object)</span>\n\n<span class=\"s2\">    if len(arr) &lt; n:</span>\n<span class=\"s2\">        arr = np.asarray(list(arr) + unique_groups[:n - len(arr)],</span>\n<span class=\"s2\">                         dtype=object)</span>\n\n<span class=\"s2\">    random.shuffle(arr)</span>\n<span class=\"s2\">    return arr</span>\n\n<span class=\"s2\">df = DataFrame({&#39;key1&#39; : get_test_data(ngroups=ngroups),</span>\n<span class=\"s2\">                &#39;key2&#39; : get_test_data(ngroups=ngroups),</span>\n<span class=\"s2\">                &#39;data&#39; : np.random.randn(N)})</span>\n<span class=\"s2\">def f():</span>\n<span class=\"s2\">    df.groupby([&#39;key1&#39;, &#39;key2&#39;]).agg(lambda x: x.values.sum())</span>\n<span class=\"s2\">&quot;&quot;&quot;</span>\n<span class=\"n\">stmt2</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;df.groupby([&#39;key1&#39;, &#39;key2&#39;]).sum()&quot;</span>\n<span class=\"n\">bm_groupby2</span> <span class=\"o\">=</span> <span class=\"n\">Benchmark</span><span class=\"p\">(</span><span class=\"n\">stmt2</span><span class=\"p\">,</span> <span class=\"n\">setup</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">&quot;GroupBy test 2&quot;</span><span class=\"p\">,</span>\n                        <span class=\"n\">start_date</span><span class=\"o\">=</span><span class=\"n\">datetime</span><span class=\"p\">(</span><span class=\"mi\">2011</span><span class=\"p\">,</span> <span class=\"mi\">7</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n</code></pre></div>\n\n<p>Then you write down the information about your repository and how to build any relevant DLLs, etc., that vary from revision to revision:</p>\n<div class=\"github\"><pre><span></span><code><span class=\"n\">REPO_PATH</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s1\">&#39;/home/wesm/code/pandas&#39;</span><span class=\"w\"></span>\n<span class=\"n\">REPO_URL</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s1\">&#39;git@github.com:wesm/pandas.git&#39;</span><span class=\"w\"></span>\n<span class=\"n\">DB_PATH</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s1\">&#39;/home/wesm/code/pandas/gb_suite/benchmarks.db&#39;</span><span class=\"w\"></span>\n<span class=\"n\">TMP_DIR</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s1\">&#39;/home/wesm/tmp/gb_pandas&#39;</span><span class=\"w\"></span>\n<span class=\"k\">PREPARE</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"ss\">&quot;&quot;&quot;</span>\n<span class=\"ss\">python setup.py clean</span>\n<span class=\"ss\">&quot;&quot;&quot;</span><span class=\"w\"></span>\n<span class=\"n\">BUILD</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"ss\">&quot;&quot;&quot;</span>\n<span class=\"ss\">python setup.py build_ext --inplace</span>\n<span class=\"ss\">&quot;&quot;&quot;</span><span class=\"w\"></span>\n<span class=\"n\">START_DATE</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nc\">datetime</span><span class=\"p\">(</span><span class=\"mi\">2011</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"mi\">3</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"w\"></span>\n</code></pre></div>\n\n<p>Then you pass this info, plus a list of your benchmark objects, to the <code>BenchmarkRunner</code> class:</p>\n<div class=\"github\"><pre><span></span><code>runner = BenchmarkRunner(benchmarks, REPO_PATH, REPO_URL,\n                         BUILD, DB_PATH, TMP_DIR, PREPARE,\n                         run_option=&#39;eod&#39;, start_date=START_DATE)\nrunner.run()\n</code></pre></div>\n\n<p>Now, the <code>BenchmarkRunner</code> makes a clone of your repo, then runs all of the benchmarks once for each revision in the repository (or some other rule, e.g. I've set <code>run_option='eod'</code> to only take the last snapshot on each day). It persists the results in a SQLite database so that you can rerun the process and it will skip benchmarks it's already run (this is key when you add new benchmarks, only the new ones will be updated). Benchmarks are uniquely identified by the MD5 hash of their source code.</p>\n<p>This is the resulting plot over time for the above GroupBy benchmark related to some Cython code that I worked on late last week (where I made a major performance improvement in this case):</p>\n<p><a href=\"https://wesmckinney.com/images/vbench_demo1.png\"><img src=\"https://wesmckinney.com/images/vbench_demo1.png\" alt=\"\" title=\"vbench_demo1\" width=\"997\" height=\"526\" class=\"aligncenter size-full wp-image-383\" /></a></p>\n<p>Here is a <a href=\"https://github.com/wesm/pandas/blob/5d4bf8febdad007d7804c2e91c5bead01ca92637/vb_suite/benchmarks.py\" title=\"suite\" target=\"_blank\">fully-formed vbench suite</a> in the pandas git repository.</p>\n<h2>Kind of like <a href=\"https://github.com/tobami/codespeed\" title=\"codespeed\" target=\"_blank\">codespeed</a> and <a href=\"http://speed.pypy.org\" title=\"speed.pypy.org\" target=\"_blank\">speed.pypy.org</a>?</h2>\n<p>Before starting to write a new project I looked briefly at codespeed and the excellent work that the PyPy guys have done with <strong>speed.pypy.org</strong>. But then I started thinking, you know, Django web application, JSON requests to upload benchmark results? Seemed like far too much work to do something relatively simple. The dealbreaker is that codespeed is just a web application. It doesn't actually (to my knowledge, someone correct me if I'm wrong?) have any kind of a framework for orchestrating the running of benchmarks throughout your code history. That is what this new project is for. I actually see a natural connection between vbench and codespeed, all you need to do is <strong>write a script to upload your vbench results to a codespeed web app!</strong></p>\n<p>At some point I'd like to build a simple web front end or wx/Qt viewer for the generated vbench database. I've never done any JavaScript, but it would be a good opportunity to learn. Knowing me, I might break down and hack out a stupid little wxPython app with an embedded matplotlib widget anyway.</p>\n<p>Anyway, I'm really excited about this project. It's very prototype-y at the moment but I tried to design it in a nice and extensible way. I also plan to put all my git repo analysis tools in there (like code churn graphs etc.) so it should become a nice little collection of tools.</p>\n<h2>Other ideas for extending</h2>\n<ul>\n<li>Dealing with API changes</li>\n<li>Multiple library versions (e.g. NumPy) or Python versions</li>\n</ul>"
}