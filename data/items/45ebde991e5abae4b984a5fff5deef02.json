{
  "title": "Proximal Subgradient Norm Minimization of ISTA and FISTA. (arXiv:2211.01610v1 [math.OC])",
  "link": "http://arxiv.org/abs/2211.01610",
  "description": "<p>For first-order smooth optimization, the research on the acceleration\nphenomenon has a long-time history. Until recently, the mechanism leading to\nacceleration was not successfully uncovered by the gradient correction term and\nits equivalent implicit-velocity form. Furthermore, based on the\nhigh-resolution differential equation framework with the corresponding emerging\ntechniques, phase-space representation and Lyapunov function, the squared\ngradient norm of Nesterov's accelerated gradient descent (\\texttt{NAG}) method\nat an inverse cubic rate is discovered. However, this result cannot be directly\ngeneralized to composite optimization widely used in practice, e.g., the linear\ninverse problem with sparse representation. In this paper, we meticulously\nobserve a pivotal inequality used in composite optimization about the step size\n$s$ and the Lipschitz constant $L$ and find that it can be improved tighter. We\napply the tighter inequality discovered in the well-constructed Lyapunov\nfunction and then obtain the proximal subgradient norm minimization by the\nphase-space representation, regardless of gradient-correction or\nimplicit-velocity. Furthermore, we demonstrate that the squared proximal\nsubgradient norm for the class of iterative shrinkage-thresholding algorithms\n(ISTA) converges at an inverse square rate, and the squared proximal\nsubgradient norm for the class of faster iterative shrinkage-thresholding\nalgorithms (FISTA) is accelerated to convergence at an inverse cubic rate.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/math/1/au:+Li_B/0/1/0/all/0/1\">Bowen Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Shi_B/0/1/0/all/0/1\">Bin Shi</a>, <a href=\"http://arxiv.org/find/math/1/au:+Yuan_Y/0/1/0/all/0/1\">Ya-xiang Yuan</a>"
}