{
  "title": "Hamiltonian Monte Carlo explained",
  "description": "<style type=\"text/css\">\n\t\tdiv.demo-wrapper {\n\t\t\twidth: 800px;\n\t\t\tmargin: auto;\n\t\t}\n\n\t\tdiv.visualization-wrapper {\n\t\t\tfont-size: 14px;\n\t\t\tmargin-top: 0.5em;\n\t\t\tmargin-bottom: 0.5em;\n\t\t}\n\n\t\tdiv.controls {\n\t\t\tmargin: auto;\n\t\t\tpadding: 10px 5px;\n\t\t\tbackground-color: #eee;\n\t\t}\n\n\t\tinput[type=range] {\n\t\t\tdisplay: block;\n\t\t\tmax-width: 130px;\n\t\t}\n\n\t\tdiv.control {\n\t\t\ttext-align: center;\n\t\t\tdisplay: inline-block;\n\t\t\tmargin: 0 12px;\n\t\t\tvertical-align: text-top;\n\t\t}\n\n\t\t.explanation-preview {\n\t\t\tcolor: #237;\n\t\t\tcursor: pointer;\n\t\t\tborder-bottom: 1px dotted #237;\n\t\t}\n\n\t\t.explanation-content {\n\t\t\tdisplay: None;\n\t\t}\n\n\t\t.dataset_control canvas {\n\t\t\tmargin: 4px;\n\t\t\tcursor: pointer;\n\t\t}\n\n\t\t.dataset_control canvas.selected {\n\t\t\toutline: 4px solid #bcc;\n\t\t}\n\n\t\t.dataset_control canvas:hover {\n\t\t\toutline: 5px solid #aca;\n\t\t}\n\n\t\t.alex-remark{\n\t\t\tposition: absolute;\n\t\t\tright: -160px;\n\t\t\twidth: 120px;\n\t\t\ttop: 50%;\n\t\t}\n\t\t.katex {\n\t\t\tfont-size: 1em !important;\n\t\t}\n\n\t\t.rejected_color {color: #844; }\n\t\t.generated_color {color: #484; }\n\t\t.true_sample_color {color: #44a; }\n\t</style>\n\n\n\t<div id=\"demo-wrapper\" class=\"demo-wrapper\">\n\t\t<p>\n\t\t\t<strong>MCMC (Markov chain Monte Carlo)</strong> is a family of methods that are\n\t\t\tapplied in computational physics and chemistry and also widely used in bayesian machine learning.\n\t\t</p>\n\t\t<p>\n\t\t\tIt is used to simulate physical systems with\n\t\t\t<a href=\"https://en.wikipedia.org/wiki/Boltzmann_distribution\">Gibbs canonical distribution</a>:\n\t\t\t<!-- and not only them -->\n\t\t\t$$\n\t\t\t\tp(\\vx) \\propto \\exp\\left( - \\frac{U(\\vx)}{T} \\right)\n\t\t\t$$\n            <!-- In Hamiltonian mechanics, we typically use `$q, p, H(q, p)$`, but I tried to make it as simple as I can... -->\n\t\t\tProbability `$ p(\\vx) $` of a system to be in the state `$ \\vx $` depends on the energy of the state `$U(\\vx)$` and temperature `$ T $`.\n\t\t</p>\n\t\t<p>\n\t\t\tThis distribution describes positions and velocities of particles in the gas, for instance.\n\t\t\t<!-- at equilibrium with heat bath temperature T ... -->\n\t\t\tIn bayesian machine learning, it defines distribution of model parameters (such as weights of a neural network).\n\t\t</p>\n\t\t<p>\n\t\t\tExample: <span  class='explanation-preview' data-explained='normal'>normal distribution is a Gibbs distribution</span>\n\t\t</p>\n\t\t<p class='explanation-content' data-explained='normal'>\n\t\t\tFor example, consider a <a href='https://en.wikipedia.org/wiki/Multivariate_normal_distribution'>multivariate normal distribution</a>:\n\t\t\t$$\n\t\t\t\tp(\\vx) \\propto \\exp\\left( - \\dfrac{1}{2} (\\vx - \\mu)^T \\Sigma^{-1} (\\vx - \\mu) \\right)\n\t\t\t$$\n\t\t\twhich corresponds to the following potential energy:\n\t\t\t$$\n\t\t\t\tU(\\vx) = \\dfrac{1}{2} (\\vx - \\mu)^T \\Sigma^{-1} (\\vx - \\mu), \\qquad T=1.\n\t\t\t$$\n\t\t\tAny distribution can be rewritten as Gibbs canonical distribution, but for many problems such energy-based distributions appear very naturally.\n\t\t</p>\n\t\t<h3>\n\t\t\tWhat is Monte Carlo (and why is it needed)?\n\t\t</h3>\n\t\t<p>\n\t\t\tSuppose that you want to study the properties of some model with thousands of variables\n\t\t\t(for <a href=\"https://en.wikipedia.org/wiki/Lattice_model_(physics)\">lattice models</a> that's very few!).\n\t\t\tFor instance, average energy:\n\t\t\t$$\n\t\t\t\t< U > = \\int U( \\vx ) \\, p( \\vx )  d \\vx\n\t\t\t$$\n\t\t\t <!--(as many other macro-characteristics)-->\n\t\t\tis an integral over distribution.\n\t\t\tComputing this integral isn't possible, even using numerical approximations (lattice over 1000 variables is incredibly large).\n\t\t</p>\n\t\t<p>\n\t\t\tBy sampling from `$ p(\\vx) $` an empirical estimate can be obtained:\n\t\t\t$$\n\t\t\t\t< U > = \\dfrac{1}{N} \\sum_{i=1}^{N} U( \\vx_i ), \\qquad \\vx_i \\sim p ( \\cdot )\n\t\t\t$$\n\n\t\t\tThis is <strong>the idea of Monte Carlo</strong>: to compute average energy / speed of molecules in the gas,\n\t\t\ttake random molecules and average their energy / speed.\n\t\t</p>\n\t\t<div style=\"text-align: center; padding: 8px 0px;\">\n\t\t\t<img src=\"/images/ml_demonstrations/mc_integration.png\" style=\"width: 100%; margin: auto; display: block;\"/>\n\t\t\t<small>\n\t\t\t\tIntegration with distribution pdf (left) is replaced with averaging over samples from distribution (blue points on the right). <br />\n\t\t\t\tRightmost plot demonstrates true samples on the energy surface, thus we can see corresponding energy `$ U(\\vx) $`.\n\t\t\t</small>\n\t\t</div>\n\t\t<p>\n\t\t\tIn modern programming languages there are functions to sample from simple distributions: uniform, normal, Poisson...\n\t\t\tThere is no simple procedure to sample from Gibbs distribution: it is times more complicated, but possible using Markov Chains.\n\t\t</p>\n\t\t<p>\n\t\t\tThat's <strong>our goal</strong>: learn to sample from the canonical distribution.\n\t\t</p>\n\n\n\t\t<h2>\n\t\t\tHow temperature influences canonical distribution:\n\t\t</h2>\n\t\t<div id='temperature_visualization_wrapper'>\n\t\t\t<div class=\"visualization-wrapper \">\n\t\t\t\t<div class=\"visualization\" style=\"display: flex; justify-content: space-between; text-align: center; font-size: 1.3em;\">\n\t\t\t\t\t<div>\n\t\t\t\t\t\t<div>unnormalized pdf `$ p(\\vx) = p(x_1, x_2)$`</div>\n\t\t\t\t\t\t<div class='visualization_right'></div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<div>\n\t\t\t\t\t\t<div>Energy `$ U(\\vx) = U(x_1, x_2)$`</div>\n\t\t\t\t\t\t<div class='visualization_left'></div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t\t<div class=\"controls\">\n\t\t\t\t\t<div class=\"control\" style=\"text-align: left;\">\n\t\t\t\t\t\tDistribution <br />\n\t\t\t\t\t\t<div class='dataset_control'></div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<div class='control' style=\"text-align: left;\">\n\t\t\t\t\t\tShow <br />\n\t\t\t\t\t\t<label>\n\t\t\t\t\t\t\t<input type=\"checkbox\" class=\"show_true_control\" checked=\"checked\" /> <span class='true_sample_color'>true samples<br/>from distribution</span>\n\t\t\t\t\t\t</label>\n\t\t\t\t\t</div>\n\t\t\t\t\t<div class=\"control\">\n\t\t\t\t\t\t<label>\n\t\t\t\t\t\t\ttemperature T: <span class=\"temperature_display\" ></span>\n                        \t<input type=\"range\" min=\"0\" max=\"4\" value=\"2\" step=\"1\" class=\"temperature_control\" />\n\t\t\t\t\t\t</label>\n\t\t\t\t\t</div>\n\t\t\t\t\t<div class='control'>\n\t\t\t\t\t\tPlay with a temperature  <br /> and look at different distributions!\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\n\t\t<p>\n\t\t\tAs intuition says, system has higher probability of staying in the states with lower energies.\n\t\t\tAs temperature goes down, imbalance becomes stronger.\n\t\t\tWhen `$T$` tends to zero, the system stays in the state(s) with the lowest energy.\n\t\t</p>\n\n\t\t<h3>\n\t\t\t— So, we minimize energy?\n\t\t</h3>\n\t\t<p>\n\t\t\tNot really. Minimal-energy state is very important, but it is wrong to think that system can exist only in this state.\n\t\t\tTo get good estimates of the system properties, <strong>a representative sampling </strong> of possible states is required\n\t\t\t(blue points demonstrate the desired distribution).\n\t\t</p>\n\t\t<p>\n\t\t\tPro tip: for bayesian people\n\t\t\tusing <a href='https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation'>maximum a posteriori estimation</a>\n\t\t\tis the same as taking state with the lowest energy, while sampling corresponds to using\n\t\t\t<a href='https://en.wikipedia.org/wiki/Posterior_probability'>bayesian posterior distribution</a>.\n\t\t\tThe latter has its benefits (though it's not simple to obtain!).\n\t\t</p>\n\t\t<h3>\n\t\t\t<span class='explanation-preview' data-explained='complexity'>Why sampling from Gibbs distribution is complex?</span>\n\t\t</h3>\n\t\t<div class='explanation-content' data-explained='complexity'>\n\t\t\t<p>\n\t\t\t\tIn Gibbs distribution probabilities `$ p(\\vx) $` are unknown.\n\t\t\t\tOne can compute `$ U(\\vx )$`, `$ T $` is known, but there is also a normalization coefficient `$Z$`\n\t\t\t\t$$\n\t\t\t\t\tp(\\vx) = \\frac{1}{Z} \\exp \\left( - \\frac{U(\\vx)}{T} \\right) \\qquad Z = \\int \\exp\\left( -\\frac{U(\\vx)}{T} \\right) d \\vx,\n\t\t\t\t$$\n\t\t\t\twhich can't be computed (except simple situations like multivariate normal).\n\t\t\t</p>\n\t\t\t<p>\n\t\t\t\tAll we have is a following ratio of probabilities (which doesn't depend on `$Z$`)\n\t\t\t\t$$\n\t\t\t\t\t\\dfrac{p(\\vx_1)}{p(\\vx_2)} = \\dfrac{ \\exp\\left( - \\frac{U(\\vx_1)}{T} \\right) }{ \\exp\\left(- \\frac{U(\\vx_2)}{T} \\right) }\n\t\t\t\t\t= \\exp\\left( \\frac{U(\\vx_2) - U(\\vx_1)}{T} \\right)\n\t\t\t\t$$\n\t\t\t\tand Markov chains can sample from the distribution using <strong>only this ratio</strong>. Let's see how.\n\t\t\t</p>\n\t\t</div>\n\n\t\t<h2>\n\t\t\tMetropolis-Hastings algorithm for MCMC\n\t\t</h2>\n\t\t<p>\n\t\t\tThe simplest <a href=\"https://en.wikipedia.org/wiki/Markov_chain\">Markov Chain</a> process that can sample from the distribution\n\t\t\tpicks the neighbour of the current state and either <span class='generated_color'>accepts</span>\n\t\t\tit or <span class='rejected_color'>rejects</span> depending on the change in energy:\n\t\t</p>\n\t\t<div id=\"mh_visualization_wrapper\" class=\"visualization-wrapper\"></div>\n\t\t<p>\n\t\t\tAlgorithm produces a chain of states: `$ \\vx_1, \\vx_2, ..., \\vx_n $` (green points). <br>\n\t\t\tEach time a candidate from a neighbourhood of the last state is selected\n\t\t\t`$\\vx_n' = \\vx_n + noise $` (noise is usually taken to be gaussian with some spread `$\\sigma$`).\n\t\t</p>\n\t\t<p>\n\t\t\tWith probability `$ p = \\min \\left[1, \\exp\\left( \\frac{U(\\vx_n) - U(\\vx_n')}{T} \\right) \\right] $` system\n\t\t\t<span class='generated_color'>accepts new state</span> (jumps to the new state):\n\t\t\t`$ \\vx_{n+1} = \\vx_n' $` and with probability `$ 1 - p $`\n\t\t\t<span class='rejected_color'>new state is rejected</span>: `$ \\vx_{n+1} = \\vx_n $`.\n\t\t</p>\n\t\t<p>\n\t\t\tNote, when energy is lower in new state `$ U(\\vx'_n) < U(\\vx_n)$` it is always accepted: `$ p = 1 $`.\n\t\t\tThis way we give preference to the states with lower energies, while not restricting the algorithm to always decrease the energy.\n\t\t\tThe lower temperature, the lower probability to increase energy.\n\t\t</p>\n\t\t<h3>\n\t\t\t<span class='explanation-preview' data-explained='MH'>Why does Metropolis-Hastings work?</span>\n\t\t</h3>\n\t\t<div class='explanation-content' data-explained='MH' >\n\t\t\t<p>\n\t\t\t\tThe most interesting part is why this trivial algorithm is able to correctly sample from Gibbs distribution?\n\t\t\t</p>\n\t\t\t<p>\n\t\t\t\tFirst thing to check is that single step of Metropolis-Hastings preserves canonical distribution.\n\t\t\t\tFormally, if `$\\vx_n \\sim p(\\cdot)$` is Gibbs-distributed, then on the next step\n\t\t\t\t`$\\vx_{n+1} \\sim p(\\cdot)$` is also Gibbs-distributed.\n\t\t\t</p>\n\t\t\t<p>\n\t\t\t\tWhen the space of parameters `$\\vx$` is discrete, this condition is written as:\n\t\t\t\t$$\n\t\t\t\t\t\\forall \\vx \\quad p(\\vx) = \\sum_{\\vx'} p(\\vx') \\; p(\\vx' \\to \\vx), \\qquad  p(\\vx) \\sim e^{-U(\\vx) / T}, \\; p(\\vx') \\sim e^{-U(\\vx') / T}\n\t\t\t\t$$\n\t\t\t\twith `$p(\\vx' \\to \\vx)$` being probability of jumping from the state `$ \\vx_n = \\vx'$` to the state `$ \\vx_{n+1} = \\vx $`.\n\t\t\t</p>\n\t\t\t<p>\n\t\t\t\tHowever, there is a much stronger condition, which guarantees preservation of canonical distribution.\n\t\t\t\tNamely, the <i>detailed balance</i> condition:\n\t\t\t\t$$\n\t\t\t\t\t\\forall \\vx, \\vx' \\quad p(\\vx') \\; p(\\vx' \\to \\vx) = p(\\vx) \\; p(\\vx \\to \\vx')\n\t\t\t\t$$\n\t\t\t\timplies previous condition (check it!). Markov chains with this condition are called <i>reversible</i>.\n\t\t\t</p>\n\t\t\t<p>\n\t\t\t\tReversibility is much easier to check. Below check is done for a Metropolis-Hastings step:\n\t\t\t\t$$\n\t\t\t\t\t\\frac{ p(\\vx' \\to \\vx) }{ p(\\vx \\to \\vx') }\n\t\t\t\t\t= \\frac{ p(noise = \\vx - \\vx') \\, p_\\text{accept} (\\vx' \\to \\vx)  }{ p(noise = \\vx' - \\vx) \\, p_\\text{accept} (\\vx \\to \\vx') } =\n\t\t\t\t$$\n\t\t\t\t$$\n\t\t\t\t\t= \\frac{ p_\\text{accept} (\\vx' \\to \\vx)  }{ p_\\text{accept} (\\vx \\to \\vx') }\n\t\t\t\t\t= \\frac{ \\min(1, e^{ (U(\\vx') - U(\\vx) ) / T }) } { \\min(1, e^{ (U(\\vx) - U(\\vx') ) / T } )}\n\t\t\t\t\t= e^{ (U(\\vx') - U(\\vx) ) / T }\n\t\t\t\t\t= \\frac{ p(\\vx) }{ p(\\vx') }\n\t\t\t\t$$\n\t\t\t\tDue to this equality the MH sampling can converge only to canonical distribution, not something else.\n\t\t\t</p>\n\t\t</div>\n\t\t<!--\n\t\t<h3>\n\t\t\t<span class='explanation-preview' data-explained='burnin'>Burn-in process</span>\n\t\t</h3>\n\t\t<div class='explanation-content' data-explained='burnin' >\n\t\t\t<p>\n\t\t\t\tIn the beginning generated samples look very inappropriate — definitely, it's just some candidate point found by algorithm to be ok,\n\t\t\t\tbut it doesn't seem to 'belong' to the correct distribution.\n\t\t\t</p>\n\t\t\t<p>\n\t\t\t\tMarkov chains CLT says that distribution of generates samples converges to the desired distribution,\n\t\t\t\tbut provides no guarantees about\n\t\t\t</p>\n\t\t\t<ul>\n\t\t\t\t<li> how fast is this convergence </li>\n\t\t\t\t<li> how long would it take to get representative sample from distribution (MH has serious problems here) </li>\n\t\t\t</ul>\n\t\t\t<p>\n\t\t\t\tSo we wait for algorithm to converge to the correct distribution (and discard generated samples) till some moment.\n\t\t\t\tThis process is called <i>burn-in</i>.\n\t\t\t</p>\n\t\t</div>\n\t\t-->\n\t\t<h3>\n\t\t\t<span class='explanation-preview' data-explained='mh_problems'>Problems of Metropolis-Hastings algorithm</span>\n\n\t\t</h3>\n\t\t<div class='explanation-content' data-explained='mh_problems'>\n\t\t\t<p>\n\t\t\t\tMH is very simple and quite general. The only thing required by algorithm is ability to compute energy `$U(\\vx)$`.\n\t\t\t\tAt the same time\n\t\t\t</p>\n\t\t\t<ul>\n\t\t\t\t<li>\n\t\t\t\t\ttoo large step size `$ \\sigma $` leads to a large fraction of rejected samples,\n\t\t\t\t\twhile too small `$ \\sigma $` makes very small steps, thus it takes long time to 'explore the distribution' (check this in demonstration!)\n\t\t\t\t</li>\n\t\t\t\t<li>\n\t\t\t\t\tin high-dimensional spaces (very important use-case),\n\t\t\t\t\tMH explores the space very inefficiently because of it's random-walk behavior.  <br />\n\t\t\t\t\tGuessing good direction in 1000 dimensions is incomparably harder than doing this for 2 dimensions!\n\t\t\t\t</li>\n\t\t\t\t<li>\n\t\t\t\t\tMH can't travel long distances (significantly larger than `$ \\sigma $`) between isolated local minimums\n\t\t\t\t</li>\n\t\t\t</ul>\n\t\t</div>\n\t\t<p>\n\t\t\tSampling high-dimensional distributions with MH becomes very inefficient in practice. A more efficient scheme is called Hamiltonian Monte Carlo (HMC).\n\t\t</p>\n\t\t<h2>Hamiltonian Monte Carlo</h2>\n\t\t<p>\n\t\t\tPhysical analogy to Hamiltonian MC: imagine a hockey pluck sliding over a surface without friction,\n\t\t\tbeing stopped at some point in time and then kicked again in a random direction.\n\t\t</p>\n\t\t<div id=\"hmc_visualization_wrapper\" class=\"visualization-wrapper\"></div>\n\t\t<p>\n\t\t\t<a href='https://en.wikipedia.org/wiki/Hybrid_Monte_Carlo'>Hamiltonian MC</a>\n\t\t\temploys the trick developed by nature (and well-known in statistical physics).\n\t\t\tVelocity `$v$` is added to the parameters describing the system.\n\t\t\tEnergy of the system consists of potential and kinetic parts:\n\t\t\t$$\n\t\t\t\tE(\\vx, \\vv) = U(\\vx) + K(\\vv), \\qquad K(\\vv) = \\sum_i \\dfrac{m \\, v^2_i}{2}\n\t\t\t$$\n\t\t\tThus, velocities `$ \\vv $` and positions `$ \\vx $` have <strong>independent</strong> canonical distributions:\n\t\t\t$$\n\t\t\t\tp(\\vx, \\vv)\n\t\t\t\t\t\\propto \\exp\\left(  \\frac{-E(\\vx, \\vv)}{T}  \\right)\n\t\t\t\t\t= \\exp\\left(  \\frac{-U(\\vx)}{T}  \\right) \\, \\exp \\left( \\frac{-K(\\vv)}{T} \\right)\n\t\t\t\t\t\\propto p(\\vx) \\; p(\\vv).\n\t\t\t$$\n\t\t\tSo once we can sample from joint distribution `$p(\\vx, \\vv)$`, we also can sample `$\\vx$` by ignoring computed velocities `$\\vv$`.\n\t\t</p>\n\t\t<h3>\n\t\t\t<span class='explanation-preview' data-explained='hmcsampling'> — How can we perform joint sampling? </span>\n\t\t</h3>\n\t\t<div class='explanation-content' data-explained='hmcsampling'>\n\t\t\t<p>\n\t\t\t\t– Just run physics simulation and get canonical distribution?\n\t\t\t</p>\n\t\t\t<p>\n\t\t\t\tVery close: after you initialize the system parameters `$\\vx, \\vv$`\n\t\t\t\tand let it evolve ('slide') using <a href='https://en.wikipedia.org/wiki/Hamiltonian_mechanics'>physics equations</a>\n\t\t\t\t$$\n\t\t\t\t\t\\dot{x}_i = v_i, \\qquad m \\dot{v_i} = - \\dfrac{ \\partial U (\\vx)}{ \\partial x_i }\n\t\t\t\t$$\n\t\t\t\tduring a long period of time, you'll <strong>not</strong> get a canonical distribution by collecting system states,\n\t\t\t\tbecause <strong>energy `$ E $` is conserved in the system</strong>.\n\t\t\t\tPhysics allows only producing samples `$ (\\vx, \\vv) $` with the same energy `$ E(\\vx, \\vv) = E_0 $`,\n\t\t\t\tso we need to add something to 'plain physics' to get correct sampling.\n\t\t\t</p>\n\t\t\t<p>\n\t\t\t\tFor instance, gas molecules are colliding, what changes their velocity and energy in an unpredictable way.\n\t\t\t\tSimilar idea can be used here, however there is a much simpler method – at some points in time velocity is resampled from `$p(\\vv)$`,\n\t\t\t\tthus changing the total energy\n\t\t\t\t(this is how 'hit the puck in random direction' step appears).\n\t\t\t</p>\n\t\t\t<p>\n\t\t\t\tSampling from `$p(\\vv)$` is very simple, because `$\\vv$` is normally distributed.\n\t\t\t</p>\n\t\t</div>\n\t\t<h3>\n\t\t\t<span class='explanation-preview' data-explained='thatsimple'> — Is everything that simple? </span>\n\t\t</h3>\n\t\t<div class='explanation-content' data-explained='thatsimple'>\n\t\t\t<p>\n\t\t\t\tHit the puck, wait, stop it and then hit again. <br />\n\t\t\t\tThis recipe sounds too simple to work, but if puck's trajectory could be computed precisely, it would be the solution.\n\t\t\t</p>\n\t\t\t<p>\n\t\t\t\tHowever, the computer simulation is imprecise and this influences the result distribution.\n\t\t\t\tFor instance, the energy isn't conserved in the simulation.\n\t\t\t</p>\n\t\t\t<p>\n\t\t\t\tTo minimize this influence, we can employ two tricks:\n\t\t\t</p>\n\t\t\t<ul>\n\t\t\t\t<li>\n\t\t\t\t\t<a href='https://en.wikipedia.org/wiki/Leapfrog_integration'>leapfrog numerical integration</a>,\n\t\t\t\t\twhich is reversible in time. <br />\n\t\t\t\t\tReversibility makes it drastically easier to ensure detailed balance</li>\n\t\t\t\t<li>Metropolis-Hastings rejections to compensate difference in energy between energy at the start and in the end.\n\t\t\t\t\tThis way (quite few) rejected samples appear in HMC.\n\t\t\t\t</li>\n\t\t\t</ul>\n\t\t</div>\n\t\t<h3>Pros and cons of Hamiltonian Monte Carlo</h3>\n\t\t<p>\n\t\t\tThere is no free lunch, and Hamiltonian MC has its price: HMC uses not only energy `$U(\\vx)$`, but also it's gradient.\n\t\t\tHence possible applications are limited to the case when gradient exists and can be computed in reasonable time.\n\t\t</p>\n\t\t<p>\n\t\t\tThe major differences compared to Metropolis-Hastings are:\n\t\t</p>\n\t\t<ul>\n\t\t\t<li>distances between successive generated points are typically large, so we need less iterations to get representative sampling</li>\n\t\t\t<li>'price' of a single iteration is higher, but HMC is still significantly more efficient </li>\n\t\t\t<li>Hamiltonian MC in most cases accepts new states (take a look at rejected samples in the demonstrations!) </li>\n\t\t\t<li>still, HMC has problems with sampling from distributions with isolated local minimums <br />\n\t\t\t\tInvestigate last distribution at low temperatures — 'puck' doesn't have enough energy to jump\n\t\t\t\tfrom the first minimum to the second over the energy barrier.  </li>\n\t\t</ul>\n\t\t<h2>\n\t\t\tBonus part: <span class='explanation-preview' data-explained='barriers'> Passing energy barriers in Hamiltonian MC </span>\n\t\t</h2>\n\t\t<div class='explanation-content' data-explained='barriers'>\n\t\t\t<p>\n\t\t\t\tAs you probably noticed, that at low temperatures both MH and HMC can't overcome energy barrier\n\t\t\t\t(last distribution has two energy minima, separated by an energy barrier).\n\t\t\t\tOnce system reaches some well-isolated local minimum, it gets stuck inside\n\t\t\t\tand probability to get to the different minimum becomes negligible, thus generated sample is 'incomplete'.\n\t\t\t</p>\n\t\t\t<p>\n\t\t\t\tOne empirical solution is manually restarting the generation from other start point, hoping to get stuck at the other minimum.\n\t\t\t\tAnother way is to develop some algorithm to jump over energy barriers. One of such approaches is called <strong>tempering during a trajectory:</strong>\n\t\t\t</p>\n\t\t\t<div id=\"hmc_tempering_visualization_wrapper\" class=\"visualization-wrapper\"></div>\n\t\t\t<p>\n\t\t\t\tPlay with tempering `$\\alpha$` and trajectory length to get to the other minimum point! <br />\n\t\t\t\tCan you make it jumping between minimums frequently? What if you decrease temperature?\n\t\t\t</p>\n\t\t\t<p>\n\t\t\t\tIdea behind tempering:\n\t\t\t\tthe trajectory is split into two halves, which take equal time.\n\t\t\t\tDuring the first half,\n\t\t\t\tvelocity is increased at each step by factor `$ \\alpha $`: `$ \\vv_\\text{next} = \\alpha \\vv $`,\n\t\t\t\tduring the second half velocity is decreased at each step by the same factor: `$ \\vv_\\text{next} = \\frac{1}{\\alpha} \\vv $`.\n\t\t\t</p>\n\t\t\t<p>\n\t\t\t\tThis trick increases the energy of system during first half of trajectory and then decreases it.\n\t\t\t</p>\n\t\t\t<p>\n\t\t\t\tWhile this process is reversible and can jump over energy barriers, the energies at the beginning and at the end are quite different.\n\t\t\t\tTo correct this, rejection rule from Metropolis-Hastings algorithm is employed.\n\t\t\t\tRejections become very frequent at low temperatures, thus amount of 'useless' computations becomes significant.\n\t\t\t</p>\n\t\t\t<p>\n\t\t\t\tOne needs to (blindly!) guess both 'slide time' and `$\\alpha$`.\n\t\t\t\tAn algorithm is quite sensible to both, in some cases producing too many rejections, in the other exploring the space inefficiently.\n\t\t\t\t<br />\n\t\t\t\tAll in all: exploration is hard.\n\t\t\t</p>\n\t\t</div>\n\n\n\t\t<h2>Links</h2>\n\t\t<ol>\n\t\t\t<li>\n\t\t\t\tRadford M. Neal <a href=\"https://arxiv.org/pdf/1206.1901.pdf\">MCMC using Hamiltonian dynamics</a> <br />\n\t\t\t\t<small>Interactive presentation mostly based on this summary.</small>\n\t\t\t</li>\n\t\t\t<li>\n\t\t\t\tCharles J. Geyer, <a href=\"http://www.mcmchandbook.net/HandbookChapter1.pdf\">Introduction to Markov Chain Monte Carlo</a><br />\n\t\t\t\t<small>Metropolis-Hastings algorithm together with MCMC practicalities </small>\n\t\t\t</li>\n\t\t\t<li>\n\t\t\t\t<a href=\"http://deeplearning.net/tutorial/hmc.html\">HMC implementation in theano</a><br />\n\t\t\t\t<small>When you don't want to compute derivatives in HMC...</small>\n\t\t\t</li>\n\t\t\t<li>\n\t\t\t\t<a href='https://arogozhnikov.github.io/2016/04/28/demonstrations-for-ml-courses.html'>Interactive demonstrations of machine learning</a><br />\n\t\t\t\t<small>A collection of nice visualizations</small>\n\t\t\t</li>\n\t\t\t<!-- <li>\n\t\t\t\t<a href='http://www.stat.columbia.edu/~gelman/research/published/nuts.pdf'>NUTS sampler</a><br />\n\t\t\t\t<small>NUTS sampler can automatically select appropriate parameters of HMC</small>\n\t\t\t</li> -->\n\t\t\t<!--<li>\n\t\t\t\tFastML has <a href='http://fastml.com/bayesian-machine-learning/'>a nice collection of resources</a> about bayesian machine learning.\n\t\t\t</li>-->\n\t\t</ol>\n\t\t<p>\n\t\t\tPlots in this demonstration are powered by webGL shaders and <a href='https://threejs.org/'>three.js</a> library,\n\t\t\tso mobile (and quite old) browsers may have problems with rendering plots correctly.\n\t\t</p>\n\t\t<p>\n\t\t\tThanks to Tatiana Likhomanenko for reading previous versions of the post.\n\t\t</p>\n\t</div>\n\n\t<!-- special place for templates -->\n\t<div id=\"templates\" style=\"display: None;\">\n\t\t<div class=\"visualization-wrapper mc_visualization_wrapper \">\n\t\t\t<div class=\"visualization\"></div>\n\t\t\t<div class=\"controls\">\n\t\t\t\t<div class=\"control\" style=\"text-align: left;\">\n\t\t\t\t\tDistribution <br />\n\t\t\t\t\t<div class='dataset_control'></div>\n\t\t\t\t</div>\n\t\t\t\t<div class=\"control\" style=\"text-align: left;\">\n\t\t\t\t\t<label class='method_controls'>\n\t\t\t\t\tmethod\n\t\t\t\t\t<select class=\"method_select_control\">\n\t\t\t\t\t\t<option value=\"mh\">Metropolis-Hastings</option>\n\t\t\t\t\t\t<option value=\"hmc\">Hamiltonian MC</option>\n\t\t\t\t\t</select>\n\t\t\t\t\t<br />\n\t\t\t\t</label>\n\t\t\t\tShow <br />\n\t\t\t\t<label>\n\t\t\t\t\t<input type=\"checkbox\" value=1 class=\"show_generated_control\"  checked=\"checked\" /> <span class='generated_color'>generated samples</span>\n\t\t\t\t\t<br />\n\t\t\t\t</label>\n\t\t\t\t<label>\n\t\t\t\t\t<input type=\"checkbox\" class=\"show_rejected_control\" /> <span class='rejected_color'>rejected samples</span>\n\t\t\t\t\t<br />\n\t\t\t\t</label>\n\t\t\t\t<label>\n\t\t\t\t\t<input type=\"checkbox\" class=\"show_true_control\" checked=\"checked\" /> <span class='true_sample_color'>true samples</span>\n\t\t\t\t</label>\n\t\t\t\t</div>\n\t\t\t\t<div class=\"control\">\n\t\t\t\t\t<label>\n\t\t\t\t\t\ttemperature T: <span class=\"temperature_display\" ></span>\n\t\t\t\t\t\t<input type=\"range\" min=\"0\" max=\"5\" value=\"4\" step=\"1\" class=\"temperature_control\">\n\t\t\t\t\t</label>\n\t\t\t\t\t<label>\n\t\t\t\t\t\tanimation: <span class=\"speed_display\" style='font-weight: bold;' ></span>\n\t\t\t\t\t\t<input type=\"range\" min=\"0\" max=\"2\" value=\"0\" step=\"1\" class=\"speed_control\">\n\t\t\t\t\t</label>\n\t\t\t\t</div>\n\t\t\t\t<div class='control hmc_only_control'>\n\t\t\t\t\t<div class=\" \">\n\t\t\t\t\t\t<label>\n\t\t\t\t\t\t\tslide time: <span class=\"trajectory_length_display\" ></span>\n\t\t\t\t\t\t\t<input type=\"range\" min=\"0\" max=\"4\" value=\"2\" step=\"1\" class=\"trajectory_length_control\">\n\t\t\t\t\t\t</label>\n\t\t\t\t\t</div>\n\t\t\t\t\t<div class=\"tempering_only_control\">\n\t\t\t\t\t\t<label>\n\t\t\t\t\t\t\ttempering α: <span class=\"tempering_display\" ></span>\n\t\t\t\t\t\t\t<input type=\"range\" min=\"0\" max=\"5\" value=\"0\" step=\"1\" class=\"tempering_control\">\n\t\t\t\t\t\t</label>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\t\t\t\t<div class=\"control mh_only_control\">\n\t\t\t\t\t<label>\n\t\t\t\t\t\tstep size σ: <span class=\"spread_display\" ></span>\n\t\t\t\t\t\t<input type=\"range\" min=\"0\" max=\"4\" value=\"2\" step=\"1\" class=\"spread_control\">\n\t\t\t\t\t</label>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n\n\t<script>\n\t\t// replacing macros before all the js-stuff\n\t\tvar demo_element = document.getElementById('demo-wrapper');\n\t\tdemo_element.innerHTML = demo_element.innerHTML.split('\\\\vx').join('\\\\mathbf{x}').split('\\\\vv').join('\\\\mathbf{v}');\n\t</script>\n\n\t<!-- only for fadeIn -->\n\t<script type=\"text/javascript\" src=\"/scripts/jquery-2.1.4.js\"></script>\n\t<script type=\"text/javascript\" src=\"/scripts/external_scripts/three.min.js\"></script>\n\t<script type=\"text/javascript\" src=\"/scripts/external_scripts/babel-polyfill.min.js\"></script>\n\n\t<script type=\"text/javascript\" src=\"/scripts/demonstration_scripts/utils-compiled.js\"></script>\n\n\t<script type=\"text/javascript\" src=\"/scripts/hmc_demonstration/utils-compiled.js\"></script>\n\t<script type=\"text/javascript\" src=\"/scripts/hmc_demonstration/mcmc-compiled.js\"></script>\n\t<script type=\"text/javascript\" src=\"/scripts/hmc_demonstration/distributions-compiled.js\"></script>\n\t<script type=\"text/javascript\" src=\"/scripts/hmc_demonstration/threejs_OrbitControls-compiled.js\"></script>\n\t<script type=\"text/javascript\" src=\"/scripts/hmc_demonstration/threejs_plotter-compiled.js\"></script>\n\t<script type=\"text/javascript\" src=\"/scripts/hmc_demonstration/mcmc_explained-compiled.js\"></script>",
  "pubDate": "Mon, 19 Dec 2016 00:00:00 +0000",
  "link": "https://arogozhnikov.github.io/2016/12/19/markov_chain_monte_carlo.html",
  "guid": "https://arogozhnikov.github.io/2016/12/19/markov_chain_monte_carlo.html",
  "category": [
    "demonstration",
    "interactive visualization",
    "Markov chain Monte Carlo",
    "Hamiltonian Monte Carlo"
  ]
}