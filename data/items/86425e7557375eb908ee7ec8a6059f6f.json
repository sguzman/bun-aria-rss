{
  "title": "Developing Convex Optimization Algorithms in Dask",
  "link": "",
  "updated": "2017-03-22T00:00:00+00:00",
  "id": "https://mrocklin.github.io/blog/work/2017/03/22/dask-glm-1",
  "content": "<p><em>This work is supported by <a href=\"http://continuum.io\">Continuum Analytics</a>,\nthe <a href=\"http://www.darpa.mil/program/XDATA\">XDATA Program</a>,\nand the Data Driven Discovery Initiative from the <a href=\"https://www.moore.org/\">Moore\nFoundation</a>.</em></p>\n\n<h2 id=\"summary\">Summary</h2>\n\n<p>We build distributed optimization algorithms with Dask.  We show both simple\nexamples and also benchmarks from a nascent\n<a href=\"https://github.com/dask/dask-glm\">dask-glm</a> library for generalized linear\nmodels.  We also talk about the experience of learning Dask to do this kind\nof work.</p>\n\n<p>This blogpost is co-authored by <a href=\"https://github.com/moody-marlin/\">Chris White</a>\n(Capital One) who knows optimization and <a href=\"http://matthewrocklin.com/\">Matthew\nRocklin</a> (Continuum Analytics) who knows\ndistributed computing.</p>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>Many machine learning and statistics models (such as logistic regression) depend\non convex optimization algorithms like Newton’s method, stochastic gradient\ndescent, and others.  These optimization algorithms are both pragmatic (they’re\nused in many applications) and mathematically interesting.  As a result these\nalgorithms have been the subject of study by researchers and graduate students\naround the world for years both in academia and in industry.</p>\n\n<p>Things got interesting about five or ten years ago when datasets grew beyond\nthe size of working memory and “Big Data” became a buzzword.  Parallel and\ndistributed solutions for these algorithms have become the norm, and a\nresearcher’s skillset now has to extend beyond linear algebra and optimization\ntheory to include parallel algorithms and possibly even network programming,\nespecially if you want to explore and create more interesting algorithms.</p>\n\n<p>However, relatively few people understand both mathematical optimization theory\nand the details of distributed systems.  Typically algorithmic researchers\ndepend on the APIs of distributed computing libraries like Spark or Flink to\nimplement their algorithms.  In this blogpost we explore the extent to which\nDask can be helpful in these applications.  We approach this from two\nperspectives:</p>\n\n<ol>\n  <li><strong>Algorithmic researcher</strong> (Chris): someone who knows optimization and\niterative algorithms like Conjugate Gradient, Dual Ascent, or GMRES but\nisn’t so hot on distributed computing topics like sockets, MPI, load\nbalancing, and so on</li>\n  <li><strong>Distributed systems developer</strong> (Matt): someone who knows how to move\nbytes around and keep machines busy but doesn’t know the right way to do a\nline search or handle a poorly conditioned matrix</li>\n</ol>\n\n<h2 id=\"prototyping-algorithms-in-dask\">Prototyping Algorithms in Dask</h2>\n\n<p>Given knowledge of algorithms and of NumPy array computing it is easy to write parallel algorithms with Dask.   For a range of complicated algorithmic structures we have two straightforward choices:</p>\n\n<ol>\n  <li>Use parallel multi-dimensional arrays to construct algorithms from common operations like matrix multiplication, SVD, and so on.  This mirrors mathematical algorithms well but lacks some flexibility.</li>\n  <li>Create algorithms by hand that track operations on individual chunks of in-memory data and dependencies between them.  This is very flexible but requires a bit more care.</li>\n</ol>\n\n<p>Coding up either of these options from scratch can be a daunting task, but with Dask it can be as simple as writing NumPy code.</p>\n\n<p>Let’s build up an example of fitting a large linear regression model using both built-in array parallelism and fancier, more customized parallelization features that Dask offers.  The <a href=\"http://dask.pydata.org/en/latest/array.html\">dask.array</a> module helps us to easily parallelize standard NumPy functionality using the same syntax – we’ll start there.</p>\n\n<h3 id=\"data-creation\">Data Creation</h3>\n\n<p>Dask has <a href=\"http://dask.pydata.org/en/latest/array-creation.html\">many ways to create dask arrays</a>; to get us started quickly prototyping let’s create some random data in a way that should look familiar to NumPy users.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">import</span> <span class=\"nn\">dask</span>\n<span class=\"kn\">import</span> <span class=\"nn\">dask.array</span> <span class=\"k\">as</span> <span class=\"n\">da</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"n\">np</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">dask.distributed</span> <span class=\"kn\">import</span> <span class=\"n\">Client</span>\n\n<span class=\"n\">client</span> <span class=\"o\">=</span> <span class=\"n\">Client</span><span class=\"p\">()</span>\n\n<span class=\"c1\">## create inputs with a bunch of independent normals\n</span><span class=\"n\">beta</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">)</span>  <span class=\"c1\"># random beta coefficients, no intercept\n</span><span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">da</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">normal</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1000000</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">),</span> <span class=\"n\">chunks</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">100000</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">))</span>\n<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">beta</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">da</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">normal</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">size</span><span class=\"o\">=</span><span class=\"mi\">1000000</span><span class=\"p\">,</span> <span class=\"n\">chunks</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">100000</span><span class=\"p\">,))</span>\n\n<span class=\"c1\">## make sure all chunks are ~equally sized\n</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">dask</span><span class=\"p\">.</span><span class=\"n\">persist</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n<span class=\"n\">client</span><span class=\"p\">.</span><span class=\"n\">rebalance</span><span class=\"p\">([</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">])</span>\n</code></pre></div></div>\n\n<p>Observe that <code class=\"language-plaintext highlighter-rouge\">X</code> is a dask array stored in 10 chunks, each of size <code class=\"language-plaintext highlighter-rouge\">(100000, 100)</code>.  Also note that <code class=\"language-plaintext highlighter-rouge\">X.dot(beta)</code> runs smoothly for both <code class=\"language-plaintext highlighter-rouge\">numpy</code> and <code class=\"language-plaintext highlighter-rouge\">dask</code> arrays, so we can write code that basically works in either world.</p>\n\n<p><strong>Caveat:</strong>  If <code class=\"language-plaintext highlighter-rouge\">X</code> is a <code class=\"language-plaintext highlighter-rouge\">numpy</code> array and <code class=\"language-plaintext highlighter-rouge\">beta</code> is a <code class=\"language-plaintext highlighter-rouge\">dask</code> array, <code class=\"language-plaintext highlighter-rouge\">X.dot(beta)</code> will output an <em>in-memory</em> <code class=\"language-plaintext highlighter-rouge\">numpy</code> array.  This is usually not desirable as you want to carefully choose when to load something into memory. One fix is to use <code class=\"language-plaintext highlighter-rouge\">multipledispatch</code> to handle odd edge cases; for a starting example, check out the <code class=\"language-plaintext highlighter-rouge\">dot</code> code <a href=\"https://github.com/dask/dask-glm/blob/master/dask_glm/utils.py#L65-L84\">here</a>.</p>\n\n<p>Dask also has convenient visualization features built in that we will leverage; below we visualize our data in its 10 independent chunks:</p>\n\n<p><img src=\"https://mrocklin.github.io/blog/images/dask-glm-data-creation-black-on-white.svg\" width=\"100%\" alt=\"Create data for dask-glm computations\" /></p>\n\n<h3 id=\"array-programming\">Array Programming</h3>\n\n<p><em>If you can write iterative array-based algorithms in NumPy, then you can write iterative parallel algorithms in Dask</em></p>\n\n<p>As we’ve already seen, Dask inherits much of the NumPy API that we are familiar with, so we can write simple NumPy-style iterative optimization algorithms that will leverage the parallelism <code class=\"language-plaintext highlighter-rouge\">dask.array</code> has built-in already.  For example, if we want to naively fit a linear regression model on the data above, we are trying to solve the following convex optimization problem:\n\\(\\min_{\\beta} \\|y - X\\beta\\|_2^2\\)</p>\n\n<p>Recall that in non-degenerate situations this problem has a closed-form solution that is given by:\n\\(\\beta^* = \\left(X^T X\\right)^{-1} X^T y\\)</p>\n\n<p>We can compute $\\beta^*$ using the above formula with Dask:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\">## naive solution\n</span><span class=\"n\">beta_star</span> <span class=\"o\">=</span> <span class=\"n\">da</span><span class=\"p\">.</span><span class=\"n\">linalg</span><span class=\"p\">.</span><span class=\"n\">solve</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">.</span><span class=\"n\">T</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">),</span> <span class=\"n\">X</span><span class=\"p\">.</span><span class=\"n\">T</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">))</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"nb\">abs</span><span class=\"p\">(</span><span class=\"n\">beta_star</span><span class=\"p\">.</span><span class=\"n\">compute</span><span class=\"p\">()</span> <span class=\"o\">-</span> <span class=\"n\">beta</span><span class=\"p\">).</span><span class=\"nb\">max</span><span class=\"p\">()</span>\n<span class=\"mf\">0.0024817567237768179</span>\n</code></pre></div></div>\n\n<p>Sometimes a direct solve is too costly, and we want to solve the above problem using only simple matrix-vector multiplications.  To this end, let’s take this one step further and actually implement a gradient descent algorithm which exploits parallel matrix operations.  Recall that gradient descent iteratively refines an initial estimate of beta via the update:\n\\(\\beta^+ = \\beta - \\alpha \\nabla f(\\beta)\\)</p>\n\n<p>where $\\alpha$ can be chosen based on a number of different “step-size” rules; for the purposes of exposition, we will stick with a constant step-size:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\">## quick step-size calculation to guarantee convergence\n</span><span class=\"n\">_</span><span class=\"p\">,</span> <span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">linalg</span><span class=\"p\">.</span><span class=\"n\">svd</span><span class=\"p\">(</span><span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">X</span><span class=\"p\">.</span><span class=\"n\">T</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">))</span>\n<span class=\"n\">step_size</span> <span class=\"o\">=</span> <span class=\"mi\">1</span> <span class=\"o\">/</span> <span class=\"n\">s</span> <span class=\"o\">-</span> <span class=\"mf\">1e-8</span>\n\n<span class=\"c1\">## define some parameters\n</span><span class=\"n\">max_steps</span> <span class=\"o\">=</span> <span class=\"mi\">100</span>\n<span class=\"n\">tol</span> <span class=\"o\">=</span> <span class=\"mf\">1e-8</span>\n<span class=\"n\">beta_hat</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">)</span> <span class=\"c1\"># initial guess\n</span>\n<span class=\"k\">for</span> <span class=\"n\">k</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">max_steps</span><span class=\"p\">):</span>\n    <span class=\"n\">Xbeta</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">beta_hat</span><span class=\"p\">)</span>\n    <span class=\"n\">func</span> <span class=\"o\">=</span> <span class=\"p\">((</span><span class=\"n\">y</span> <span class=\"o\">-</span> <span class=\"n\">Xbeta</span><span class=\"p\">)</span><span class=\"o\">**</span><span class=\"mi\">2</span><span class=\"p\">).</span><span class=\"nb\">sum</span><span class=\"p\">()</span>\n    <span class=\"n\">gradient</span> <span class=\"o\">=</span> <span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">X</span><span class=\"p\">.</span><span class=\"n\">T</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">Xbeta</span> <span class=\"o\">-</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n\n    <span class=\"c1\">## Update\n</span>    <span class=\"n\">obeta</span> <span class=\"o\">=</span> <span class=\"n\">beta_hat</span>\n    <span class=\"n\">beta_hat</span> <span class=\"o\">=</span> <span class=\"n\">beta_hat</span> <span class=\"o\">-</span> <span class=\"n\">step_size</span> <span class=\"o\">*</span> <span class=\"n\">gradient</span>\n    <span class=\"n\">new_func</span> <span class=\"o\">=</span> <span class=\"p\">((</span><span class=\"n\">y</span> <span class=\"o\">-</span> <span class=\"n\">X</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">beta_hat</span><span class=\"p\">))</span><span class=\"o\">**</span><span class=\"mi\">2</span><span class=\"p\">).</span><span class=\"nb\">sum</span><span class=\"p\">()</span>\n    <span class=\"n\">beta_hat</span><span class=\"p\">,</span> <span class=\"n\">func</span><span class=\"p\">,</span> <span class=\"n\">new_func</span> <span class=\"o\">=</span> <span class=\"n\">dask</span><span class=\"p\">.</span><span class=\"n\">compute</span><span class=\"p\">(</span><span class=\"n\">beta_hat</span><span class=\"p\">,</span> <span class=\"n\">func</span><span class=\"p\">,</span> <span class=\"n\">new_func</span><span class=\"p\">)</span>  <span class=\"c1\"># &lt;--- Dask code\n</span>\n    <span class=\"c1\">## Check for convergence\n</span>    <span class=\"n\">change</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">absolute</span><span class=\"p\">(</span><span class=\"n\">beta_hat</span> <span class=\"o\">-</span> <span class=\"n\">obeta</span><span class=\"p\">).</span><span class=\"nb\">max</span><span class=\"p\">()</span>\n\n    <span class=\"k\">if</span> <span class=\"n\">change</span> <span class=\"o\">&lt;</span> <span class=\"n\">tol</span><span class=\"p\">:</span>\n        <span class=\"k\">break</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"nb\">abs</span><span class=\"p\">(</span><span class=\"n\">beta_hat</span> <span class=\"o\">-</span> <span class=\"n\">beta</span><span class=\"p\">).</span><span class=\"nb\">max</span><span class=\"p\">()</span>\n<span class=\"mf\">0.0024817567259038942</span>\n</code></pre></div></div>\n\n<p>It’s worth noting that almost all of this code is exactly the same as the equivalent NumPy code.  Because Dask.array and NumPy share the same API it’s pretty easy for people who are already comfortable with NumPy to get started with distributed algorithms right away.  The only thing we had to change was how we produce our original data (<code class=\"language-plaintext highlighter-rouge\">da.random.normal</code> instead of <code class=\"language-plaintext highlighter-rouge\">np.random.normal</code>) and the call to <code class=\"language-plaintext highlighter-rouge\">dask.compute</code> at the end of the update state.  The <code class=\"language-plaintext highlighter-rouge\">dask.compute</code> call tells Dask to go ahead and actually evaluate everything we’ve told it to do so far (Dask is lazy by default).  Otherwise, all of the mathematical operations, matrix multiplies, slicing, and so on are exactly the same as with Numpy, except that Dask.array builds up a chunk-wise parallel computation for us and Dask.distributed can execute that computation in parallel.</p>\n\n<p>To better appreciate all the scheduling that is happening in one update step of the above algorithm, here is a visualization of the computation necessary to compute <code class=\"language-plaintext highlighter-rouge\">beta_hat</code> and the new function value <code class=\"language-plaintext highlighter-rouge\">new_func</code>:</p>\n\n<p><img src=\"https://mrocklin.github.io/blog/images/grad-step-black-on-white.svg\" width=\"100%\" alt=\"Gradient descent step Dask graph\" /></p>\n\n<p>Each rectangle is an in-memory chunk of our distributed array and every circle\nis a numpy function call on those in-memory chunks.  The Dask scheduler\ndetermines where and when to run all of these computations on our cluster of\nmachines (or just on the cores of our laptop).</p>\n\n<h4 id=\"array-programming--daskdelayed\">Array Programming + dask.delayed</h4>\n\n<p>Now that we’ve seen how to use the built-in parallel algorithms offered by Dask.array, let’s go one step further and talk about writing more customized parallel algorithms.  Many distributed “consensus” based algorithms in machine learning are based on the idea that each chunk of data can be processed independently in parallel, and send their guess for the optimal parameter value to some master node.  The master then computes a <em>consensus</em> estimate for the optimal parameters and reports that back to all of the workers.  Each worker then processes their chunk of data given this new information, and the process continues until convergence.</p>\n\n<p>From a parallel computing perspective this is a pretty simple map-reduce procedure.  Any distributed computing framework should be able to handle this easily.  We’ll use this as a very simple example for how to use Dask’s more customizable parallel options.</p>\n\n<p>One such algorithm is the <a href=\"http://stanford.edu/~boyd/admm.html\">Alternating Direction Method of Multipliers</a>, or ADMM for short.  For the sake of this post, we will consider the work done by each worker to be a black box.</p>\n\n<p>We will also be considering a <em>regularized</em> version of the problem above, namely:</p>\n\n\\[\\min_{\\beta} \\|y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_1\\]\n\n<p>At the end of the day, all we will do is:</p>\n\n<ul>\n  <li>create NumPy functions which define how each chunk updates its parameter estimates</li>\n  <li>wrap those functions in <code class=\"language-plaintext highlighter-rouge\">dask.delayed</code></li>\n  <li>call <code class=\"language-plaintext highlighter-rouge\">dask.compute</code> and process the individual estimates, again using NumPy</li>\n</ul>\n\n<p>First we need to define some <em>local</em> functions that the chunks will use to update their individual parameter estimates, and import the black box <code class=\"language-plaintext highlighter-rouge\">local_update</code> step from <code class=\"language-plaintext highlighter-rouge\">dask_glm</code>; also, we will need the so-called <em>shrinkage</em> operator (which is the <a href=\"https://en.wikipedia.org/wiki/Proximal_operator\">proximal operator</a> for the $l1$-norm in our problem):</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">dask_glm.algorithms</span> <span class=\"kn\">import</span> <span class=\"n\">local_update</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">local_f</span><span class=\"p\">(</span><span class=\"n\">beta</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">z</span><span class=\"p\">,</span> <span class=\"n\">u</span><span class=\"p\">,</span> <span class=\"n\">rho</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"p\">((</span><span class=\"n\">y</span> <span class=\"o\">-</span> <span class=\"n\">X</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">beta</span><span class=\"p\">))</span> <span class=\"o\">**</span><span class=\"mi\">2</span><span class=\"p\">).</span><span class=\"nb\">sum</span><span class=\"p\">()</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">rho</span> <span class=\"o\">/</span> <span class=\"mi\">2</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">beta</span> <span class=\"o\">-</span> <span class=\"n\">z</span> <span class=\"o\">+</span> <span class=\"n\">u</span><span class=\"p\">,</span>\n                                                                  <span class=\"n\">beta</span> <span class=\"o\">-</span> <span class=\"n\">z</span> <span class=\"o\">+</span> <span class=\"n\">u</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">local_grad</span><span class=\"p\">(</span><span class=\"n\">beta</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">z</span><span class=\"p\">,</span> <span class=\"n\">u</span><span class=\"p\">,</span> <span class=\"n\">rho</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">X</span><span class=\"p\">.</span><span class=\"n\">T</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">beta</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"n\">y</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">rho</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">beta</span> <span class=\"o\">-</span> <span class=\"n\">z</span> <span class=\"o\">+</span> <span class=\"n\">u</span><span class=\"p\">)</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">shrinkage</span><span class=\"p\">(</span><span class=\"n\">beta</span><span class=\"p\">,</span> <span class=\"n\">t</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">maximum</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">beta</span> <span class=\"o\">-</span> <span class=\"n\">t</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">maximum</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"n\">beta</span> <span class=\"o\">-</span> <span class=\"n\">t</span><span class=\"p\">)</span>\n\n<span class=\"c1\">## set some algorithm parameters\n</span><span class=\"n\">max_steps</span> <span class=\"o\">=</span> <span class=\"mi\">10</span>\n<span class=\"n\">lamduh</span> <span class=\"o\">=</span> <span class=\"mf\">7.2</span>\n<span class=\"n\">rho</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span>\n\n<span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">,</span> <span class=\"n\">p</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"p\">.</span><span class=\"n\">shape</span>\n<span class=\"n\">nchunks</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"p\">.</span><span class=\"n\">npartitions</span>\n\n<span class=\"n\">XD</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"p\">.</span><span class=\"n\">to_delayed</span><span class=\"p\">().</span><span class=\"n\">flatten</span><span class=\"p\">().</span><span class=\"n\">tolist</span><span class=\"p\">()</span>  <span class=\"c1\"># A list of pointers to remote numpy arrays\n</span><span class=\"n\">yD</span> <span class=\"o\">=</span> <span class=\"n\">y</span><span class=\"p\">.</span><span class=\"n\">to_delayed</span><span class=\"p\">().</span><span class=\"n\">flatten</span><span class=\"p\">().</span><span class=\"n\">tolist</span><span class=\"p\">()</span>  <span class=\"c1\"># ... one for each chunk\n</span>\n<span class=\"c1\"># the initial consensus estimate\n</span><span class=\"n\">z</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># an array of the individual \"dual variables\" and parameter estimates,\n# one for each chunk of data\n</span><span class=\"n\">u</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">nchunks</span><span class=\"p\">)])</span>\n<span class=\"n\">betas</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">nchunks</span><span class=\"p\">)])</span>\n\n<span class=\"k\">for</span> <span class=\"n\">k</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">max_steps</span><span class=\"p\">):</span>\n\n    <span class=\"c1\"># process each chunk in parallel, using the black-box 'local_update' magic\n</span>    <span class=\"n\">new_betas</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">dask</span><span class=\"p\">.</span><span class=\"n\">delayed</span><span class=\"p\">(</span><span class=\"n\">local_update</span><span class=\"p\">)(</span><span class=\"n\">xx</span><span class=\"p\">,</span> <span class=\"n\">yy</span><span class=\"p\">,</span> <span class=\"n\">bb</span><span class=\"p\">,</span> <span class=\"n\">z</span><span class=\"p\">,</span> <span class=\"n\">uu</span><span class=\"p\">,</span> <span class=\"n\">rho</span><span class=\"p\">,</span>\n                                            <span class=\"n\">f</span><span class=\"o\">=</span><span class=\"n\">local_f</span><span class=\"p\">,</span>\n                                            <span class=\"n\">fprime</span><span class=\"o\">=</span><span class=\"n\">local_grad</span><span class=\"p\">)</span>\n                 <span class=\"k\">for</span> <span class=\"n\">xx</span><span class=\"p\">,</span> <span class=\"n\">yy</span><span class=\"p\">,</span> <span class=\"n\">bb</span><span class=\"p\">,</span> <span class=\"n\">uu</span> <span class=\"ow\">in</span> <span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">XD</span><span class=\"p\">,</span> <span class=\"n\">yD</span><span class=\"p\">,</span> <span class=\"n\">betas</span><span class=\"p\">,</span> <span class=\"n\">u</span><span class=\"p\">)]</span>\n    <span class=\"n\">new_betas</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">array</span><span class=\"p\">(</span><span class=\"n\">dask</span><span class=\"p\">.</span><span class=\"n\">compute</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">new_betas</span><span class=\"p\">))</span>\n\n    <span class=\"c1\"># everything else is NumPy code occurring at \"master\"\n</span>    <span class=\"n\">beta_hat</span> <span class=\"o\">=</span> <span class=\"mf\">0.9</span> <span class=\"o\">*</span> <span class=\"n\">new_betas</span> <span class=\"o\">+</span> <span class=\"mf\">0.1</span> <span class=\"o\">*</span> <span class=\"n\">z</span>\n\n    <span class=\"c1\"># create consensus estimate\n</span>    <span class=\"n\">zold</span> <span class=\"o\">=</span> <span class=\"n\">z</span><span class=\"p\">.</span><span class=\"n\">copy</span><span class=\"p\">()</span>\n    <span class=\"n\">ztilde</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">beta_hat</span> <span class=\"o\">+</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">array</span><span class=\"p\">(</span><span class=\"n\">u</span><span class=\"p\">),</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n    <span class=\"n\">z</span> <span class=\"o\">=</span> <span class=\"n\">shrinkage</span><span class=\"p\">(</span><span class=\"n\">ztilde</span><span class=\"p\">,</span> <span class=\"n\">lamduh</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"n\">rho</span> <span class=\"o\">*</span> <span class=\"n\">nchunks</span><span class=\"p\">))</span>\n\n    <span class=\"c1\"># update dual variables\n</span>    <span class=\"n\">u</span> <span class=\"o\">+=</span> <span class=\"n\">beta_hat</span> <span class=\"o\">-</span> <span class=\"n\">z</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"c1\"># Number of coefficients zeroed out due to L1 regularization\n</span><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"k\">print</span><span class=\"p\">((</span><span class=\"n\">z</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">).</span><span class=\"nb\">sum</span><span class=\"p\">())</span>\n<span class=\"mi\">12</span>\n</code></pre></div></div>\n\n<p>There is of course a little bit more work occurring in the above algorithm, but it should be clear that the distributed operations are <em>not</em> one of the difficult pieces.  Using dask.delayed we were able to express a simple map-reduce algorithm like ADMM with similarly simple Python for loops and delayed function calls.  Dask.delayed is keeping track of all of the function calls we wanted to make and what other function calls they depend on.  For example all of the <code class=\"language-plaintext highlighter-rouge\">local_update</code> calls can happen independent of each other, but the consensus computation blocks on all of them.</p>\n\n<p>We hope that both parallel algorithms shown above (gradient descent, ADMM) were\nstraightforward to someone reading with an optimization background.  These\nimplementations run well on a laptop, a single multi-core workstation, or a\nthousand-node cluster if necessary.  We’ve been building somewhat more\nsophisticated implementations of these algorithms (and others) in\n<a href=\"https://github.com/dask/dask-glm\">dask-glm</a>.  They are more sophisticated from an\noptimization perspective (stopping criteria, step size, asynchronicity, and so on)\nbut remain as simple from a distributed computing perspective.</p>\n\n<h2 id=\"experiment\">Experiment</h2>\n\n<p><em>We compare dask-glm implementations against Scikit-learn on a laptop, and then\nshow them running on a cluster.</em></p>\n\n<p><a href=\"https://gist.github.com/mrocklin/a0f0826ea7f4463c8c99880f1893a43f\">Reproducible notebook is available here</a></p>\n\n<p>We’re building more sophisticated versions of the algorithms above in\n<a href=\"https://github.com/dask/dask-glm\">dask-glm</a>.  This project has convex\noptimization algorithms for gradient descent, proximal gradient descent,\nNewton’s method, and ADMM.  These implementations extend the implementations\nabove by also thinking about stopping criteria, step sizes, and other niceties\nthat we avoided above for simplicity.</p>\n\n<p>In this section we show off these algorithms by performing a simple numerical\nexperiment that compares the numerical performance of proximal gradient descent\nand ADMM alongside Scikit-Learn’s LogisticRegression and SGD implementations\non a single machine (a personal laptop) and then follows up by scaling the\ndask-glm options to a moderate cluster.  </p>\n\n<p><em>Disclaimer: These experiments are crude.  We’re using artificial data, we’re not tuning\nparameters or even finding parameters at which these algorithms are producing\nresults of the same accuracy.  The goal of this section is just to give a\ngeneral feeling of how things compare.</em></p>\n\n<p>We create data</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\">## size of problem (no. observations)\n</span><span class=\"n\">N</span> <span class=\"o\">=</span> <span class=\"mf\">8e6</span>\n<span class=\"n\">chunks</span> <span class=\"o\">=</span> <span class=\"mf\">1e6</span>\n<span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"mi\">20009</span>\n<span class=\"n\">beta</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">(</span><span class=\"mi\">15</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"mf\">0.5</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"mi\">3</span>\n\n<span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">da</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">((</span><span class=\"n\">N</span><span class=\"p\">,</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">beta</span><span class=\"p\">)),</span> <span class=\"n\">chunks</span><span class=\"o\">=</span><span class=\"n\">chunks</span><span class=\"p\">)</span>\n<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">make_y</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">beta</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">array</span><span class=\"p\">(</span><span class=\"n\">beta</span><span class=\"p\">),</span> <span class=\"n\">chunks</span><span class=\"o\">=</span><span class=\"n\">chunks</span><span class=\"p\">)</span>\n\n<span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">dask</span><span class=\"p\">.</span><span class=\"n\">persist</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n<span class=\"n\">client</span><span class=\"p\">.</span><span class=\"n\">rebalance</span><span class=\"p\">([</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">])</span>\n</code></pre></div></div>\n\n<p>And run each of our algorithms as follows:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\"># Dask-GLM Proximal Gradient\n</span><span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">proximal_grad</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">lamduh</span><span class=\"o\">=</span><span class=\"n\">alpha</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Dask-GLM ADMM\n</span><span class=\"n\">X2</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"p\">.</span><span class=\"n\">rechunk</span><span class=\"p\">((</span><span class=\"mf\">1e5</span><span class=\"p\">,</span> <span class=\"bp\">None</span><span class=\"p\">)).</span><span class=\"n\">persist</span><span class=\"p\">()</span>  <span class=\"c1\"># ADMM prefers smaller chunks\n</span><span class=\"n\">y2</span> <span class=\"o\">=</span> <span class=\"n\">y</span><span class=\"p\">.</span><span class=\"n\">rechunk</span><span class=\"p\">(</span><span class=\"mf\">1e5</span><span class=\"p\">).</span><span class=\"n\">persist</span><span class=\"p\">()</span>\n<span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">admm</span><span class=\"p\">(</span><span class=\"n\">X2</span><span class=\"p\">,</span> <span class=\"n\">y2</span><span class=\"p\">,</span> <span class=\"n\">lamduh</span><span class=\"o\">=</span><span class=\"n\">alpha</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Scikit-Learn LogisticRegression\n</span><span class=\"n\">nX</span><span class=\"p\">,</span> <span class=\"n\">ny</span> <span class=\"o\">=</span> <span class=\"n\">dask</span><span class=\"p\">.</span><span class=\"n\">compute</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>  <span class=\"c1\"># sklearn wants numpy arrays\n</span><span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">LogisticRegression</span><span class=\"p\">(</span><span class=\"n\">penalty</span><span class=\"o\">=</span><span class=\"s\">'l1'</span><span class=\"p\">,</span> <span class=\"n\">C</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">).</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">nX</span><span class=\"p\">,</span> <span class=\"n\">ny</span><span class=\"p\">).</span><span class=\"n\">coef_</span>\n\n<span class=\"c1\"># Scikit-Learn Stochastic Gradient Descent\n</span><span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">SGDClassifier</span><span class=\"p\">(</span><span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"s\">'log'</span><span class=\"p\">,</span>\n                       <span class=\"n\">penalty</span><span class=\"o\">=</span><span class=\"s\">'l1'</span><span class=\"p\">,</span>\n                       <span class=\"n\">l1_ratio</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span>\n                       <span class=\"n\">n_iter</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">,</span>\n                       <span class=\"n\">fit_intercept</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">).</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">nX</span><span class=\"p\">,</span> <span class=\"n\">ny</span><span class=\"p\">).</span><span class=\"n\">coef_</span>\n</code></pre></div></div>\n\n<p>We then compare with the $L_{\\infty}$ norm (largest different value).</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">abs</span><span class=\"p\">(</span><span class=\"n\">result</span> <span class=\"o\">-</span> <span class=\"n\">beta</span><span class=\"p\">).</span><span class=\"nb\">max</span><span class=\"p\">()</span>\n</code></pre></div></div>\n\n<p>Times and $L_\\infty$ distance from the true “generative beta” for these parameters are shown in the table below:</p>\n\n<table>\n<thead><tr>\n  <th>Algorithm</th>\n  <th>Error</th>\n  <th>Duration (s)</th>\n</tr></thead>\n<tbody>\n<tr>\n  <td>Proximal Gradient</td>\n  <td>0.0227</td>\n  <td>128</td>\n</tr>\n<tr>\n  <td>ADMM</td>\n  <td>0.0125</td>\n  <td>34.7</td>\n</tr>\n<tr>\n  <td>LogisticRegression</td>\n  <td>0.0132</td>\n  <td>79</td>\n</tr>\n<tr>\n  <td>SGDClassifier</td>\n  <td>0.0456</td>\n  <td>29.4</td>\n</tr>\n</tbody>\n</table>\n\n<p>Again, please don’t take these numbers too seriously: these algorithms all solve\nregularized problems, so we don’t expect the results to necessarily be close to the\nunderlying generative beta (even asymptotically).  The numbers above are meant to\ndemonstrate that they all return results which were roughly the same distance\nfrom the beta above.  Also, Dask-glm is using a full four-core laptop while\nSKLearn is restricted to use a single core.</p>\n\n<p>In the sections below we include profile plots for proximal gradient and ADMM.\nThese show the operations that each of eight threads was doing over time.  You\ncan mouse-over rectangles/tasks and zoom in using the zoom tools in the upper\nright.  You can see the difference in complexity of the algorithms.  ADMM is\nmuch simpler from Dask’s perspective but also saturates hardware better for\nthis chunksize.</p>\n\n<h4 id=\"profile-plot-for-proximal-gradient-descent\">Profile Plot for Proximal Gradient Descent</h4>\n\n<iframe src=\"https://cdn.rawgit.com/mrocklin/5dec93966e0daa2edb8fcde8449a5335/raw/b0f096cf72d31df02bda4e5ca3203a7347464808/dask-glm-proximal-grad-small.html\" width=\"800\" height=\"300\"></iframe>\n\n<h4 id=\"profile-plot-for-admm\">Profile Plot for ADMM</h4>\n\n<iframe src=\"https://cdn.rawgit.com/mrocklin/5dec93966e0daa2edb8fcde8449a5335/raw/52f81540fd969e3ff4c69f1f44a401d25d9134ff/dask-glm-admm.html\" width=\"800\" height=\"300\"></iframe>\n\n<p>The general takeaway here is that dask-glm performs comparably to Scikit-Learn\non a single machine.  If your problem fits in memory on a single machine you\nshould continue to use Scikit-Learn and Statsmodels.  The real benefit to the\ndask-glm algorithms is that they <em>scale</em> and can run efficiently on data that is\nlarger-than-memory by operating from disk on a single computer or on a\ncluster of computers working together.</p>\n\n<h3 id=\"cluster-computing\">Cluster Computing</h3>\n\n<p>As a demonstration, we run a larger version of the data above on a cluster of\neight <code class=\"language-plaintext highlighter-rouge\">m4.2xlarges</code> on EC2 (8 cores and 30GB of RAM each.)</p>\n\n<p>We create a larger dataset with 800,000,000 rows and 15 columns across eight\nprocesses.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">N</span> <span class=\"o\">=</span> <span class=\"mf\">8e8</span>\n<span class=\"n\">chunks</span> <span class=\"o\">=</span> <span class=\"mf\">1e7</span>\n<span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"mi\">20009</span>\n<span class=\"n\">beta</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">(</span><span class=\"mi\">15</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"mf\">0.5</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"mi\">3</span>\n\n<span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">da</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">((</span><span class=\"n\">N</span><span class=\"p\">,</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">beta</span><span class=\"p\">)),</span> <span class=\"n\">chunks</span><span class=\"o\">=</span><span class=\"n\">chunks</span><span class=\"p\">)</span>\n<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">make_y</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">beta</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">array</span><span class=\"p\">(</span><span class=\"n\">beta</span><span class=\"p\">),</span> <span class=\"n\">chunks</span><span class=\"o\">=</span><span class=\"n\">chunks</span><span class=\"p\">)</span>\n\n<span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">dask</span><span class=\"p\">.</span><span class=\"n\">persist</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>We then run the same <code class=\"language-plaintext highlighter-rouge\">proximal_grad</code> and <code class=\"language-plaintext highlighter-rouge\">admm</code> operations from before:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\"># Dask-GLM Proximal Gradient\n</span><span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">proximal_grad</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">lamduh</span><span class=\"o\">=</span><span class=\"n\">alpha</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Dask-GLM ADMM\n</span><span class=\"n\">X2</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"p\">.</span><span class=\"n\">rechunk</span><span class=\"p\">((</span><span class=\"mf\">1e6</span><span class=\"p\">,</span> <span class=\"bp\">None</span><span class=\"p\">)).</span><span class=\"n\">persist</span><span class=\"p\">()</span>  <span class=\"c1\"># ADMM prefers smaller chunks\n</span><span class=\"n\">y2</span> <span class=\"o\">=</span> <span class=\"n\">y</span><span class=\"p\">.</span><span class=\"n\">rechunk</span><span class=\"p\">(</span><span class=\"mf\">1e6</span><span class=\"p\">).</span><span class=\"n\">persist</span><span class=\"p\">()</span>\n<span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">admm</span><span class=\"p\">(</span><span class=\"n\">X2</span><span class=\"p\">,</span> <span class=\"n\">y2</span><span class=\"p\">,</span> <span class=\"n\">lamduh</span><span class=\"o\">=</span><span class=\"n\">alpha</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>Proximal grad completes in around seventeen minutes while ADMM completes in\naround four minutes.  Profiles for the two computations are included below:</p>\n\n<h4 id=\"profile-plot-for-proximal-gradient-descent-1\">Profile Plot for Proximal Gradient Descent</h4>\n\n<p>We include only the first few iterations here.  Otherwise this plot is several\nmegabytes.</p>\n\n<p><a href=\"https://cdn.rawgit.com/mrocklin/c9f1724285af29cfa50fd1430178c5af/raw/0d0edca0fa97a70c0e90699eb41c51e23f503ea6/dask-glm-proximal-grad-large.html\">Link to fullscreen plot</a></p>\n<iframe src=\"https://cdn.rawgit.com/mrocklin/c9f1724285af29cfa50fd1430178c5af/raw/0d0edca0fa97a70c0e90699eb41c51e23f503ea6/dask-glm-proximal-grad-large.html\" width=\"800\" height=\"400\"></iframe>\n\n<h4 id=\"profile-plot-for-admm-1\">Profile Plot for ADMM</h4>\n\n<p><a href=\"https://cdn.rawgit.com/mrocklin/c9f1724285af29cfa50fd1430178c5af/raw/0d0edca0fa97a70c0e90699eb41c51e23f503ea6/dask-glm-proximal-grad-large.html\">Link to fullscreen plot</a></p>\n\n<iframe src=\"https://cdn.rawgit.com/mrocklin/c9f1724285af29cfa50fd1430178c5af/raw/0d0edca0fa97a70c0e90699eb41c51e23f503ea6/dask-glm-admm-large.html\" width=\"800\" height=\"400\"></iframe>\n\n<p>These both obtained similar $L_{\\infty}$ errors to what we observed before.</p>\n\n<table>\n<thead><tr>\n  <th>Algorithm</th>\n  <th>Error</th>\n  <th>Duration (s)</th>\n</tr></thead>\n<tbody>\n<tr>\n  <td>Proximal Gradient</td>\n  <td>0.0306</td>\n  <td>1020</td>\n</tr>\n<tr>\n  <td>ADMM</td>\n  <td>0.00159</td>\n  <td>270</td>\n</tr>\n</tbody>\n</table>\n\n<p>Although this time we had to be careful about a couple of things:</p>\n\n<ol>\n  <li>We explicitly deleted the old data after rechunking (ADMM prefers different\nchunksizes than proximal_gradient) because our full dataset, 100GB, is\nclose enough to our total distributed RAM (240GB) that it’s a good idea to\navoid keeping replias around needlessly.  Things would have run fine, but\nspilling excess data to disk would have negatively affected performance.</li>\n  <li>We set the <code class=\"language-plaintext highlighter-rouge\">OMP_NUM_THREADS=1</code> environment variable to avoid\nover-subscribing our CPUs.  Surprisingly not doing so led both to worse\nperformance and to non-deterministic results.  An issue that we’re still\ntracking down.</li>\n</ol>\n\n<h3 id=\"analysis\">Analysis</h3>\n\n<p>The algorithms in Dask-GLM are new and need development, but are in a usable\nstate by people comfortable operating at this technical level.  Additionally,\nwe would like to attract other mathematical and algorithmic developers to this\nwork.  We’ve found that Dask provides a nice balance between being flexible\nenough to support interesting algorithms, while being managed enough to be\nusable by researchers without a strong background in distributed systems.  In\nthis section we’re going to discuss the things that we learned from both\nChris’ (mathematical algorithms) and Matt’s (distributed systems) perspective\nand then talk about possible future work.  We encourage people to pay attention\nto future work; we’re open to collaboration and think that this is a good\nopportunity for new researchers to meaningfully engage.</p>\n\n<h4 id=\"chriss-perspective\">Chris’s perspective</h4>\n\n<ol>\n  <li>Creating distributed algorithms with Dask was surprisingly easy; there is\nstill a small learning curve around when to call things like <code class=\"language-plaintext highlighter-rouge\">persist</code>,\n<code class=\"language-plaintext highlighter-rouge\">compute</code>, <code class=\"language-plaintext highlighter-rouge\">rebalance</code>, and so on, but that can’t be avoided.  Using Dask for\nalgorithm development has been a great learning environment for\nunderstanding the unique challenges associated with distributed algorithms\n(including communication costs, among others).</li>\n  <li>Getting the particulars of algorithms correct is non-trivial; there is still\nwork to be done in better understanding the tolerance settings vs. accuracy\ntradeoffs that are occurring in many of these algorithms, as well as\nfine-tuning the convergence criteria for increased precision.</li>\n  <li>On the software development side, reliably testing optimization algorithms\nis hard.  Finding provably correct optimality conditions that should be\nsatisfied <em>which are also numerically stable</em> has been a challenge for me.</li>\n  <li>Working on algorithms in isolation is not nearly as fun as collaborating on\nthem; please join the conversation and contribute!</li>\n  <li>Most importantly from my perspective, I’ve found there is a surprisingly\nlarge amount of misunderstanding in “the community” surrounding what\noptimization algorithms do in the world of predictive modeling, what\nproblems they each individually solve, and whether or not they are\ninterchangeable for a given problem.  For example, Newton’s method can’t be\nused to optimize an l1-regularized problem, and the coefficient estimates\nfrom an l1-regularized problem are fundamentally (and numerically) different\nfrom those of an l2-regularized problem (and from those of an unregularized\nproblem).  My own personal goal is that the API for <code class=\"language-plaintext highlighter-rouge\">dask-glm</code> exposes these\nsubtle distinctions more transparently and leads to more thoughtful modeling\ndecisions “in the wild”.</li>\n</ol>\n\n<h4 id=\"matts-perspective\">Matt’s perspective</h4>\n\n<p>This work triggered a number of concrete changes within the Dask library:</p>\n\n<ol>\n  <li>We can convert Dask.dataframes to Dask.arrays.  This is particularly\nimportant because people want to do pre-processing with dataframes but then\nswitch to efficient multi-dimensional arrays for algorithms.</li>\n  <li>We had to unify the single-machine scheduler and distributed scheduler APIs\na bit, notably adding a <code class=\"language-plaintext highlighter-rouge\">persist</code> function to the single machine\nscheduler.  This was particularly important because Chris\ngenerally prototyped on his laptop but we wanted to write code that was\neffective on clusters.</li>\n  <li>Scheduler overhead can be a problem for the iterative dask-array algorithms\n(gradient descent, proximal gradient descent, BFGS).  This is particularly\na problem because NumPy is very fast.  Often our tasks take only a few\nmilliseconds, which makes Dask’s overhead of 200us per task become very\nrelevant (this is why you see whitespace in the profile plots above).\nWe’ve started resolving this problem in a few ways like more aggressive\ntask fusion and lower overheads generally, but this will be a medium-term\nchallenge.  In practice for dask-glm we’ve started handling this just by\nchoosing chunksizes well.  I suspect that for the dask-glm in particular\nwe’ll just develop auto-chunksize heuristics that will mostly solve this\nproblem.  However we expect this problem to recur in other work with\nscientists on HPC systems who have similar situations.</li>\n  <li>A couple of things can be tricky for algorithmic users:\n    <ol>\n      <li>Placing the calls to asynchronously start computation (persist,\ncompute).  In practice Chris did a good job here and then I came through\nand tweaked things afterwards.  The web diagnostics ended up being\ncrucial to identify issues.</li>\n      <li>Avoiding accidentally calling NumPy functions on dask.arrays and vice\nversa.  We’ve improved this on the dask.array side, and they now operate\nintelligently when given numpy arrays.  Changing this on the NumPy side\nis harder until NumPy protocols change (which is planned).</li>\n    </ol>\n  </li>\n</ol>\n\n<h4 id=\"future-work\">Future work</h4>\n\n<p>There are a number of things we would like to do, both in terms of measurement\nand for the dask-glm project itself.  We welcome people to voice their opinions\n(and join development) on the following issues:</p>\n\n<ol>\n  <li><a href=\"https://github.com/dask/dask-glm/issues/5\">Asynchronous Algorithms</a></li>\n  <li><a href=\"https://github.com/dask/dask-glm/issues/11\">User APIs</a></li>\n  <li><a href=\"https://github.com/dask/dask-glm/issues/35\">Extend GLM families</a></li>\n  <li>Write more extensive rigorous algorithm testing - for <a href=\"https://github.com/dask/dask-glm/issues/7\">satisfying provable optimality criteria</a>, and for <a href=\"https://github.com/dask/dask-glm/issues/9\">robustness to various input data</a></li>\n  <li><a href=\"https://github.com/dask/dask-glm/issues/34\">Begin work on smart initialization routines</a></li>\n</ol>\n\n<p>What is your perspective here, gentle reader?  Both Matt and Chris can use help\non this project.  We hope that some of the issues above provide seeds for\ncommunity engagement.  We welcome other questions, comments, and contributions\neither as github issues or comments below.</p>\n\n<h2 id=\"acknowledgements\">Acknowledgements</h2>\n\n<p>Thanks also go to <a href=\"https://github.com/hussainsultan\">Hussain Sultan</a> (Capital\nOne) and <a href=\"https://github.com/TomAugspurger\">Tom Augspurger</a> for collaboration\non Dask-GLM and to <a href=\"https://github.com/electronwill\">Will Warner</a> (Continuum)\nfor reviewing and editing this post.</p>"
}