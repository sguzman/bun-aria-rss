{
  "title": "Dask Release 0.14.1",
  "link": "",
  "updated": "2017-03-23T00:00:00+00:00",
  "id": "https://mrocklin.github.io/blog/work/2017/03/23/dask-0.14.1",
  "content": "<p><em>This work is supported by <a href=\"http://continuum.io\">Continuum Analytics</a>,\nthe <a href=\"http://www.darpa.mil/program/XDATA\">XDATA Program</a>,\nand the Data Driven Discovery Initiative from the <a href=\"https://www.moore.org/\">Moore\nFoundation</a>.</em></p>\n\n<p>I’m pleased to announce the release of Dask version 0.14.1.  This release\ncontains a variety of performance and feature improvements.  This blogpost\nincludes some notable features and changes since the last release on February\n27th.</p>\n\n<p>As always you can conda install from conda-forge</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>conda install -c conda-forge dask distributed\n</code></pre></div></div>\n\n<p>or you can pip install from PyPI</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>pip install dask[complete] --upgrade\n</code></pre></div></div>\n\n<h2 id=\"arrays\">Arrays</h2>\n\n<p>Recent work in distributed computing and machine learning have motivated new\nperformance-oriented and usability changes to how we handle arrays.</p>\n\n<h3 id=\"automatic-chunking-and-operation-on-numpy-arrays\">Automatic chunking and operation on NumPy arrays</h3>\n\n<p>Many interactions between Dask arrays and NumPy arrays work smoothly.  NumPy\narrays are made lazy and are appropriately chunked to match the operation\nand the Dask array.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">ones</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">)</span>                 <span class=\"c1\"># a numpy array\n</span><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">da</span><span class=\"p\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">chunks</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">,))</span>  <span class=\"c1\"># a dask array\n</span><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">z</span> <span class=\"o\">=</span> <span class=\"n\">x</span> <span class=\"o\">+</span> <span class=\"n\">y</span>                       <span class=\"c1\"># combined become a dask.array\n</span><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">z</span>\n<span class=\"n\">dask</span><span class=\"p\">.</span><span class=\"n\">array</span><span class=\"o\">&lt;</span><span class=\"n\">add</span><span class=\"p\">,</span> <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">float64</span><span class=\"p\">,</span> <span class=\"n\">chunksize</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">,)</span><span class=\"o\">&gt;</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">z</span><span class=\"p\">.</span><span class=\"n\">compute</span><span class=\"p\">()</span>\n<span class=\"n\">array</span><span class=\"p\">([</span>  <span class=\"mf\">1.</span><span class=\"p\">,</span>   <span class=\"mf\">2.</span><span class=\"p\">,</span>   <span class=\"mf\">3.</span><span class=\"p\">,</span>   <span class=\"mf\">4.</span><span class=\"p\">,</span>   <span class=\"mf\">5.</span><span class=\"p\">,</span>   <span class=\"mf\">6.</span><span class=\"p\">,</span>   <span class=\"mf\">7.</span><span class=\"p\">,</span>   <span class=\"mf\">8.</span><span class=\"p\">,</span>   <span class=\"mf\">9.</span><span class=\"p\">,</span>  <span class=\"mf\">10.</span><span class=\"p\">])</span>\n</code></pre></div></div>\n\n<h3 id=\"reshape\">Reshape</h3>\n\n<p>Reshaping distributed arrays is simple in simple cases, and can be quite\ncomplex in complex cases.  Reshape now supports a much more broad set of shape\ntransformations where any dimension is collapsed or merged to other dimensions.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">da</span><span class=\"p\">.</span><span class=\"n\">ones</span><span class=\"p\">((</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">6</span><span class=\"p\">),</span> <span class=\"n\">chunks</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">))</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">x</span><span class=\"p\">.</span><span class=\"n\">reshape</span><span class=\"p\">((</span><span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">30</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n<span class=\"n\">dask</span><span class=\"p\">.</span><span class=\"n\">array</span><span class=\"o\">&lt;</span><span class=\"n\">reshape</span><span class=\"p\">,</span> <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">30</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">float64</span><span class=\"p\">,</span> <span class=\"n\">chunksize</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"o\">&gt;</span>\n</code></pre></div></div>\n\n<p>This operation ends up being quite useful in a number of distributed array\ncases.</p>\n\n<h3 id=\"optimize-slicing-to-minimize-communication\">Optimize Slicing to Minimize Communication</h3>\n\n<p>Dask.array slicing optimizations are now careful to produce graphs that avoid\nsituations that could cause excess inter-worker communication.  The details of\nhow they do this is a bit out of scope for a short blogpost, but the history\nhere is interesting.</p>\n\n<p>Historically dask.arrays were used almost exclusively by researchers with large\non-disk arrays stored as HDF5 or NetCDF files.  These users primarily used the\nsingle machine multi-threaded scheduler.  We heavily tailored Dask array\noptimizations to this situation and made that community pretty happy.\nNow as some of that community switches to cluster computing on larger datasets\nthe optimization goals shift a bit.  We have tons of distributed disk bandwidth\nbut really want to avoid communicating large results between workers.\nSupporting both use cases is possible and I think that we’ve achieved that in\nthis release so far, but it’s starting to require increasing levels of care.</p>\n\n<h3 id=\"micro-optimizations\">Micro-optimizations</h3>\n\n<p>With distributed computing also comes larger graphs and a growing importance of\ngraph-creation overhead.  This has been optimized somewhat in this release.  We\nexpect this to be a focus going forward.</p>\n\n<h2 id=\"dataframes\">DataFrames</h2>\n\n<h3 id=\"set_index\">Set_index</h3>\n\n<p>Set_index is smarter in two ways:</p>\n\n<ol>\n  <li>If you set_index on a column that happens to be sorted then we’ll identify\nthat and avoid a costly shuffle.  This was always possible with the <code class=\"language-plaintext highlighter-rouge\">sorted=</code>\nkeyword but users rarely used this feature.  Now this is automatic.</li>\n  <li>Similarly when setting the index we can look at the size of the data and\ndetermine if there are too many or too few partitions and rechunk the data\nwhile shuffling.  This can significantly improve performance if there are too\nmany partitions (a common case).</li>\n</ol>\n\n<ul>\n  <li><a href=\"https://github.com/dask/dask/pull/2025\">dask/dask #2025</a></li>\n  <li><a href=\"https://github.com/dask/dask/pull/2091\">dask/dask #2091</a></li>\n</ul>\n\n<h3 id=\"shuffle-performance\">Shuffle performance</h3>\n\n<p>We’ve micro-optimized some parts of dataframe shuffles.  Big thanks to the\nPandas developers for the help here.  This accelerates set_index, joins,\ngroupby-applies, and so on.</p>\n\n<ul>\n  <li><a href=\"https://github.com/dask/dask/pull/2032\">dask/dask #2032</a></li>\n</ul>\n\n<h3 id=\"fastparquet\">Fastparquet</h3>\n\n<p>The <a href=\"http://fastparquet.readthedocs.io/en/latest/\">fastparquet</a> library has\nseen a lot of use lately and has undergone a number of community bugfixes.</p>\n\n<p>Importantly, Fastparquet now supports Python 2.</p>\n\n<p>We strongly recommend Parquet as the standard data storage format for Dask\ndataframes (and Pandas DataFrames).</p>\n\n<p><a href=\"https://github.com/dask/fastparquet/pull/87\">dask/fastparquet #87</a></p>\n\n<h2 id=\"distributed-scheduler\">Distributed Scheduler</h2>\n\n<h3 id=\"replay-remote-exceptions\">Replay remote exceptions</h3>\n\n<p>Debugging is hard in part because exceptions happen on remote machines where\nnormal debugging tools like <code class=\"language-plaintext highlighter-rouge\">pdb</code> can’t reach.  Previously we were able to\nbring back the traceback and exception, but you couldn’t dive into the stack\ntrace to investigate what went wrong:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">def</span> <span class=\"nf\">div</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"n\">x</span> <span class=\"o\">/</span> <span class=\"n\">y</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">future</span> <span class=\"o\">=</span> <span class=\"n\">client</span><span class=\"p\">.</span><span class=\"n\">submit</span><span class=\"p\">(</span><span class=\"n\">div</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">future</span>\n<span class=\"o\">&lt;</span><span class=\"n\">Future</span><span class=\"p\">:</span> <span class=\"n\">status</span><span class=\"p\">:</span> <span class=\"n\">error</span><span class=\"p\">,</span> <span class=\"n\">key</span><span class=\"p\">:</span> <span class=\"n\">div</span><span class=\"o\">-</span><span class=\"mi\">4</span><span class=\"n\">a34907f5384bcf9161498a635311aeb</span><span class=\"o\">&gt;</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">future</span><span class=\"p\">.</span><span class=\"n\">result</span><span class=\"p\">()</span>  <span class=\"c1\"># getting result re-raises exception locally\n</span><span class=\"o\">&lt;</span><span class=\"n\">ipython</span><span class=\"o\">-</span><span class=\"nb\">input</span><span class=\"o\">-</span><span class=\"mi\">3</span><span class=\"o\">-</span><span class=\"mi\">398</span><span class=\"n\">a43a7781e</span><span class=\"o\">&gt;</span> <span class=\"ow\">in</span> <span class=\"n\">div</span><span class=\"p\">()</span>\n      <span class=\"mi\">1</span> <span class=\"k\">def</span> <span class=\"nf\">div</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">):</span>\n<span class=\"o\">----&gt;</span> <span class=\"mi\">2</span>     <span class=\"k\">return</span> <span class=\"n\">x</span> <span class=\"o\">/</span> <span class=\"n\">y</span>\n\n<span class=\"nb\">ZeroDivisionError</span><span class=\"p\">:</span> <span class=\"n\">division</span> <span class=\"n\">by</span> <span class=\"n\">zero</span>\n</code></pre></div></div>\n\n<p>Now Dask can bring a failing task and all necessary data back to the local\nmachine and rerun it so that users can leverage the normal Python debugging\ntoolchain.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">client</span><span class=\"p\">.</span><span class=\"n\">recreate_error_locally</span><span class=\"p\">(</span><span class=\"n\">future</span><span class=\"p\">)</span>\n<span class=\"o\">&lt;</span><span class=\"n\">ipython</span><span class=\"o\">-</span><span class=\"nb\">input</span><span class=\"o\">-</span><span class=\"mi\">3</span><span class=\"o\">-</span><span class=\"mi\">398</span><span class=\"n\">a43a7781e</span><span class=\"o\">&gt;</span> <span class=\"ow\">in</span> <span class=\"n\">div</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n      <span class=\"mi\">1</span> <span class=\"k\">def</span> <span class=\"nf\">div</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">):</span>\n<span class=\"o\">----&gt;</span> <span class=\"mi\">2</span>     <span class=\"k\">return</span> <span class=\"n\">x</span> <span class=\"o\">/</span> <span class=\"n\">y</span>\n<span class=\"nb\">ZeroDivisionError</span><span class=\"p\">:</span> <span class=\"n\">division</span> <span class=\"n\">by</span> <span class=\"n\">zero</span>\n</code></pre></div></div>\n\n<p>Now if you’re in IPython or a Jupyter notebook you can use the <code class=\"language-plaintext highlighter-rouge\">%debug</code> magic\nto jump into the stacktrace, investigate local variables, and so on.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">8</span><span class=\"p\">]:</span> <span class=\"o\">%</span><span class=\"n\">debug</span>\n<span class=\"o\">&gt;</span> <span class=\"o\">&lt;</span><span class=\"n\">ipython</span><span class=\"o\">-</span><span class=\"nb\">input</span><span class=\"o\">-</span><span class=\"mi\">3</span><span class=\"o\">-</span><span class=\"mi\">398</span><span class=\"n\">a43a7781e</span><span class=\"o\">&gt;</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">)</span><span class=\"n\">div</span><span class=\"p\">()</span>\n      <span class=\"mi\">1</span> <span class=\"k\">def</span> <span class=\"nf\">div</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">):</span>\n<span class=\"o\">----&gt;</span> <span class=\"mi\">2</span>     <span class=\"k\">return</span> <span class=\"n\">x</span> <span class=\"o\">/</span> <span class=\"n\">y</span>\n\n<span class=\"n\">ipdb</span><span class=\"o\">&gt;</span> <span class=\"n\">pp</span> <span class=\"n\">x</span>\n<span class=\"mi\">1</span>\n<span class=\"n\">ipdb</span><span class=\"o\">&gt;</span> <span class=\"n\">pp</span> <span class=\"n\">y</span>\n<span class=\"mi\">0</span>\n</code></pre></div></div>\n\n<p><a href=\"https://github.com/dask/distributed/pull/894\">dask/distributed #894</a></p>\n\n<h3 id=\"asyncawait-syntax\">Async/await syntax</h3>\n\n<p>Dask.distributed uses Tornado for network communication and Tornado coroutines\nfor concurrency.  Normal users rarely interact with Tornado coroutines; they\naren’t familiar to most people so we opted instead to copy the\nconcurrent.futures API.  However some complex situations are <em>much</em> easier to\nsolve if you know a little bit of async programming.</p>\n\n<p>Fortunately, the Python ecosystem seems to be embracing this change towards\nnative async code with the async/await syntax in Python 3.  In an effort to\nmotivate people to learn async programming and to gently nudge them towards\nPython 3 Dask.distributed we now support async/await in a few cases.</p>\n\n<p>You can wait on a dask Future</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">async</span> <span class=\"k\">def</span> <span class=\"nf\">f</span><span class=\"p\">():</span>\n    <span class=\"n\">future</span> <span class=\"o\">=</span> <span class=\"n\">client</span><span class=\"p\">.</span><span class=\"n\">submit</span><span class=\"p\">(</span><span class=\"n\">func</span><span class=\"p\">,</span> <span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">)</span>\n    <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">future</span>\n</code></pre></div></div>\n\n<p>You can put the <code class=\"language-plaintext highlighter-rouge\">as_completed</code> iterator into an async for loop</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">async</span> <span class=\"k\">for</span> <span class=\"n\">future</span> <span class=\"ow\">in</span> <span class=\"n\">as_completed</span><span class=\"p\">(</span><span class=\"n\">futures</span><span class=\"p\">):</span>\n    <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">future</span>\n    <span class=\"p\">...</span> <span class=\"n\">do</span> <span class=\"n\">stuff</span> <span class=\"k\">with</span> <span class=\"n\">result</span> <span class=\"p\">...</span>\n</code></pre></div></div>\n\n<p>And, because Tornado supports the await protocols you can also use the existing\nshadow concurrency API (everything prepended with an underscore) with await.\n(This was doable before.)</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">results</span> <span class=\"o\">=</span> <span class=\"n\">client</span><span class=\"p\">.</span><span class=\"n\">gather</span><span class=\"p\">(</span><span class=\"n\">futures</span><span class=\"p\">)</span>         <span class=\"c1\"># synchronous\n</span><span class=\"p\">...</span>\n<span class=\"n\">results</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">client</span><span class=\"p\">.</span><span class=\"n\">_gather</span><span class=\"p\">(</span><span class=\"n\">futures</span><span class=\"p\">)</span>  <span class=\"c1\"># asynchronous\n</span></code></pre></div></div>\n\n<p>If you’re in Python 2 you can always do this with normal <code class=\"language-plaintext highlighter-rouge\">yield</code> and\nthe <code class=\"language-plaintext highlighter-rouge\">tornado.gen.coroutine</code> decorator.</p>\n\n<p><a href=\"https://github.com/dask/distributed/pull/952\">dask/distributed #952</a></p>\n\n<h3 id=\"inproc-transport\">Inproc transport</h3>\n\n<p>In the last release we enabled Dask to communicate over more things than just\nTCP.  In practice this doesn’t come up (TCP is pretty useful).  However in this\nrelease we now support single-machine “clusters” where the clients, scheduler,\nand workers are all in the same process and transfer data cost-free over\nin-memory queues.</p>\n\n<p>This allows the in-memory user community to use some of the more advanced\nfeatures (asynchronous computation, spill-to-disk support, web-diagnostics)\nthat are only available in the distributed scheduler.</p>\n\n<p>This is on by default if you create a cluster with LocalCluster without using\nNanny processes.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">dask.distributed</span> <span class=\"kn\">import</span> <span class=\"n\">LocalCluster</span><span class=\"p\">,</span> <span class=\"n\">Client</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">cluster</span> <span class=\"o\">=</span> <span class=\"n\">LocalCluster</span><span class=\"p\">(</span><span class=\"n\">nanny</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">)</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">client</span> <span class=\"o\">=</span> <span class=\"n\">Client</span><span class=\"p\">(</span><span class=\"n\">cluster</span><span class=\"p\">)</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">client</span>\n<span class=\"o\">&lt;</span><span class=\"n\">Client</span><span class=\"p\">:</span> <span class=\"n\">scheduler</span><span class=\"o\">=</span><span class=\"s\">'inproc://192.168.1.115/8437/1'</span> <span class=\"n\">processes</span><span class=\"o\">=</span><span class=\"mi\">1</span> <span class=\"n\">cores</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"o\">&gt;</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">threading</span> <span class=\"kn\">import</span> <span class=\"n\">Lock</span>         <span class=\"c1\"># Not serializable\n</span><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">lock</span> <span class=\"o\">=</span> <span class=\"n\">Lock</span><span class=\"p\">()</span>                      <span class=\"c1\"># Won't survive going over a socket\n</span><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"p\">[</span><span class=\"n\">future</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">client</span><span class=\"p\">.</span><span class=\"n\">scatter</span><span class=\"p\">([</span><span class=\"n\">lock</span><span class=\"p\">])</span>  <span class=\"c1\"># Yet we can send to a worker\n</span><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">future</span><span class=\"p\">.</span><span class=\"n\">result</span><span class=\"p\">()</span>                    <span class=\"c1\"># ... and back\n</span><span class=\"o\">&lt;</span><span class=\"n\">unlocked</span> <span class=\"n\">_thread</span><span class=\"p\">.</span><span class=\"n\">lock</span> <span class=\"nb\">object</span> <span class=\"n\">at</span> <span class=\"mh\">0x7fb7f12d08a0</span><span class=\"o\">&gt;</span>\n</code></pre></div></div>\n\n<p><a href=\"https://github.com/dask/distributed/pull/919\">dask/distributed #919</a></p>\n\n<h3 id=\"connection-pooling-for-inter-worker-communications\">Connection pooling for inter-worker communications</h3>\n\n<p>Workers now maintain a pool of sustained connections between each other.  This\npool is of a fixed size and removes connections with a least-recently-used\npolicy.  It avoids re-connection delays when transferring data between workers.\nIn practice this shaves off a millisecond or two from every communication.</p>\n\n<p>This is actually a revival of an old feature that we had turned off last year\nwhen it became clear that the performance here wasn’t a problem.</p>\n\n<p>Along with other enhancements, this takes our round-trip latency down to 11ms\non my laptop.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">10</span><span class=\"p\">]:</span> <span class=\"o\">%%</span><span class=\"n\">time</span>\n    <span class=\"p\">...:</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">1000</span><span class=\"p\">):</span>\n    <span class=\"p\">...:</span>     <span class=\"n\">future</span> <span class=\"o\">=</span> <span class=\"n\">client</span><span class=\"p\">.</span><span class=\"n\">submit</span><span class=\"p\">(</span><span class=\"n\">inc</span><span class=\"p\">,</span> <span class=\"n\">i</span><span class=\"p\">)</span>\n    <span class=\"p\">...:</span>     <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">future</span><span class=\"p\">.</span><span class=\"n\">result</span><span class=\"p\">()</span>\n    <span class=\"p\">...:</span>\n<span class=\"n\">CPU</span> <span class=\"n\">times</span><span class=\"p\">:</span> <span class=\"n\">user</span> <span class=\"mf\">4.96</span> <span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"n\">sys</span><span class=\"p\">:</span> <span class=\"mi\">348</span> <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">total</span><span class=\"p\">:</span> <span class=\"mf\">5.31</span> <span class=\"n\">s</span>\n<span class=\"n\">Wall</span> <span class=\"n\">time</span><span class=\"p\">:</span> <span class=\"mf\">11.1</span> <span class=\"n\">s</span>\n</code></pre></div></div>\n\n<p>There may be room for improvement here though. For comparison here is the same\ntest with the <code class=\"language-plaintext highlighter-rouge\">concurent.futures.ProcessPoolExecutor</code>.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">14</span><span class=\"p\">]:</span> <span class=\"n\">e</span> <span class=\"o\">=</span> <span class=\"n\">ProcessPoolExecutor</span><span class=\"p\">(</span><span class=\"mi\">8</span><span class=\"p\">)</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">15</span><span class=\"p\">]:</span> <span class=\"o\">%%</span><span class=\"n\">time</span>\n    <span class=\"p\">...:</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">1000</span><span class=\"p\">):</span>\n    <span class=\"p\">...:</span>     <span class=\"n\">future</span> <span class=\"o\">=</span> <span class=\"n\">e</span><span class=\"p\">.</span><span class=\"n\">submit</span><span class=\"p\">(</span><span class=\"n\">inc</span><span class=\"p\">,</span> <span class=\"n\">i</span><span class=\"p\">)</span>\n    <span class=\"p\">...:</span>     <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">future</span><span class=\"p\">.</span><span class=\"n\">result</span><span class=\"p\">()</span>\n    <span class=\"p\">...:</span>\n<span class=\"n\">CPU</span> <span class=\"n\">times</span><span class=\"p\">:</span> <span class=\"n\">user</span> <span class=\"mi\">320</span> <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">sys</span><span class=\"p\">:</span> <span class=\"mi\">56</span> <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">total</span><span class=\"p\">:</span> <span class=\"mi\">376</span> <span class=\"n\">ms</span>\n<span class=\"n\">Wall</span> <span class=\"n\">time</span><span class=\"p\">:</span> <span class=\"mi\">442</span> <span class=\"n\">ms</span>\n</code></pre></div></div>\n\n<p>Also, just to be clear, this measures total roundtrip latency, not overhead.\nDask’s distributed scheduler overhead remains in the low hundreds of\nmicroseconds.</p>\n\n<p><a href=\"https://github.com/dask/distributed/pull/935\">dask/distributed #935</a></p>\n\n<h2 id=\"related-projects\">Related Projects</h2>\n\n<p>There has been activity around Dask and machine learning:</p>\n\n<ul>\n  <li><a href=\"https://github.com/dask/dask-learn\">dask-learn</a> is undergoing some\nperformance enhancements.  It turns out that when you offer distributed grid\nsearch people quickly want to scale up their computations to hundreds of\nthousands of trials.</li>\n  <li><a href=\"https://github.com/dask/dask-glm\">dask-glm</a> now has a few decent algorithms\nfor convex optimization.  The authors of this wrote a blogpost very recently\nif you’re interested:\n<a href=\"http://matthewrocklin.com/blog/work/2017/03/22/dask-glm-1\">Developing Convex Optimization Algorithms in Dask</a></li>\n  <li><a href=\"https://github.com/dask/dask-xgboost\">dask-xgboost</a> lets you hand off\ndistributed data in Dask dataframes or arrays and hand it directly to a\ndistributed XGBoost system (that Dask will nicely set up and tear down for\nyou).  This was a nice example of easy hand-off between two distributed\nservices running in the same processes.</li>\n</ul>\n\n<h2 id=\"acknowledgements\">Acknowledgements</h2>\n\n<p>The following people contributed to the dask/dask repository since the 0.14.0 release\non February 27th</p>\n\n<ul>\n  <li>Antoine Pitrou</li>\n  <li>Brian Martin</li>\n  <li>Elliott Sales de Andrade</li>\n  <li>Erik Welch</li>\n  <li>Francisco de la Peña</li>\n  <li>jakirkham</li>\n  <li>Jim Crist</li>\n  <li>Jitesh Kumar Jha</li>\n  <li>Julien Lhermitte</li>\n  <li>Martin Durant</li>\n  <li>Matthew Rocklin</li>\n  <li>Markus Gonser</li>\n  <li>Talmaj</li>\n</ul>\n\n<p>The following people contributed to the dask/distributed repository since the\n1.16.0 release on February 27th</p>\n\n<ul>\n  <li>Antoine Pitrou</li>\n  <li>Ben Schreck</li>\n  <li>Elliott Sales de Andrade</li>\n  <li>Martin Durant</li>\n  <li>Matthew Rocklin</li>\n  <li>Phil Elson</li>\n</ul>"
}