{
  "title": "First Convergence Bias",
  "link": "",
  "published": "2016-04-11T00:00:00-04:00",
  "updated": "2016-04-11T00:00:00-04:00",
  "author": {
    "name": "Silviu Pitis"
  },
  "id": "tag:r2rt.com,2016-04-11:/first-convergence-bias.html",
  "summary": "In this post, I offer the results of an experiment providing support for \"first convergence bias\", which includes the proposition that training a randomly initialized network via backpropagation may never converge to a global minimum, regardless of the intialization and number of trials.",
  "content": "<!DOCTYPE html>\n<html>\n<head>\n  <meta charset=\"utf-8\">\n  <meta name=\"generator\" content=\"pandoc\">\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, user-scalable=yes\">\n  <title></title>\n  <style type=\"text/css\">code{white-space: pre;}</style>\n  <style type=\"text/css\">\ndiv.sourceCode { overflow-x: auto; }\ntable.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {\n  margin: 0; padding: 0; vertical-align: baseline; border: none; }\ntable.sourceCode { width: 100%; line-height: 100%; }\ntd.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }\ntd.sourceCode { padding-left: 5px; }\ncode > span.kw { color: #007020; font-weight: bold; } /* Keyword */\ncode > span.dt { color: #902000; } /* DataType */\ncode > span.dv { color: #40a070; } /* DecVal */\ncode > span.bn { color: #40a070; } /* BaseN */\ncode > span.fl { color: #40a070; } /* Float */\ncode > span.ch { color: #4070a0; } /* Char */\ncode > span.st { color: #4070a0; } /* String */\ncode > span.co { color: #60a0b0; font-style: italic; } /* Comment */\ncode > span.ot { color: #007020; } /* Other */\ncode > span.al { color: #ff0000; font-weight: bold; } /* Alert */\ncode > span.fu { color: #06287e; } /* Function */\ncode > span.er { color: #ff0000; font-weight: bold; } /* Error */\ncode > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */\ncode > span.cn { color: #880000; } /* Constant */\ncode > span.sc { color: #4070a0; } /* SpecialChar */\ncode > span.vs { color: #4070a0; } /* VerbatimString */\ncode > span.ss { color: #bb6688; } /* SpecialString */\ncode > span.im { } /* Import */\ncode > span.va { color: #19177c; } /* Variable */\ncode > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */\ncode > span.op { color: #666666; } /* Operator */\ncode > span.bu { } /* BuiltIn */\ncode > span.ex { } /* Extension */\ncode > span.pp { color: #bc7a00; } /* Preprocessor */\ncode > span.at { color: #7d9029; } /* Attribute */\ncode > span.do { color: #ba2121; font-style: italic; } /* Documentation */\ncode > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */\ncode > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */\ncode > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */\n  </style>\n  <!--[if lt IE 9]>\n    <script src=\"//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js\"></script>\n  <![endif]-->\n</head>\n<body>\n<p>In my post <a href=\"https://r2rt.com/skill-vs-strategy.html\">Skill vs Strategy</a> I made the following proposition:</p>\n<blockquote>\n<p>Let’s say we retrain the network one million times, and each of the local minima reached leads to approximately the same performance. Is this enough for us to conclude that the resulting strategies are close to the best? I would answer in the negative; we cannot be certain that a random initialization will ever lead to an optimal strategy via backpropagation. It may be a situation like the chest shot, where in order to reach an optimal strategy, the network must be trained again after it has learned some useful hidden features.</p>\n</blockquote>\n<p>In this post, I offer the results of an experiment providing support for this proposition. The specific experiment was not designed to test this proposition, and in fact, the results I obtained are opposite of the results I expected.</p>\n<!--more-->\n<h3 id=\"motivation\">Motivation</h3>\n<p>In my post <a href=\"https://r2rt.com/representational-power-of-deeper-layers.html\">Representational Power of Deeper Layers</a>, I showed that progressively deeper layers serve as better representations of the original data. This motivated the thought that the better the first layer representation of the data, the better the last layer representation could be. Intuitively, this makes sense, if our first layer can get us 95% of the way to the best solution, then the second layer only has 5% of ground to cover, whereas if the first layer only gets 70% of the way there, the second layer has six times as much work to do.</p>\n<h3 id=\"experiment\">Experiment</h3>\n<p>I tested this hypothesis by setting up a neural network with 3-hidden layers to classify MNIST digits. The network is trained only on 1000 MNIST training examples, as opposed to the entire training set of 50000, and achieves a test accuracy of between 86% and 89%.</p>\n<p>Using a simple mini-batch gradient descent with a batch-size of 50, I compared the following two scenarios:</p>\n<ol type=\"1\">\n<li>Training the entire network for 1000 steps.</li>\n<li>Training the first layer for 200 steps, training the first and second layers together for 200 steps, and only then training the entire network for 1000 steps.</li>\n</ol>\n<p>My hypothesis was that the second option would lead to better results, because by the time the third layer starts training, the first and second layers are finely tuned to produce “good” representations of the data. This hypothesis was proven incorrect.</p>\n<h3 id=\"results\">Results</h3>\n<p>For each of the two training strategies above, I trained 500 randomly-initialized networks and recorded their final accuracies. Training the entire network at once yielded an average test accuracy of 87.87%, whereas training the two earlier layers before training the entire network yielded an average test accuracy of only 87.64%. While this is not that significant of a difference, the plot below clearly shows that the “local minima” that each strategy reaches is pulled from a different distribution.</p>\n<figure>\n<img src=\"https://r2rt.com/static/images/FCB_output_12_2.png\" />\n</figure>\n<h3 id=\"discussion\">Discussion</h3>\n<p>Although my hypothesis was invalidated, the result is nice because it supports my prior proposition that the local minima of first convergence may be biased. I.e., we have no guarantee of getting to the best local minima after training via backpropagation. Thus, the discussion in <a href=\"https://r2rt.com/skill-vs-strategy.html\">Skill vs Strategy</a> is very relevant. Convergence speed aside, which is the usual reason for preferring alternate training strategies, other training strategies (e.g., other optimization strategies like Adagrad) are worth exploring as they might be biased toward a superior quality of local minima.</p>\n<p>How this relates to “greedy layer-wise training of deep networks” (see <a href=\"https://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf\">this paper</a>) may also be interesting. I haven’t learned enough yet to dive into that paper or that method of training, but the gist, as I currently understand it, is that we can obtain better initializations of the weights in a deep network by first training each layer in an unsupervised fashion. In this experiment I did greedy layer-wise <em>supervised</em> training, which led to worse results. As an aside, the discussion in that paper also strongly supports the proposition that local minima of first convergence may be biased.</p>\n<h3 id=\"code\">Code</h3>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"im\">import</span> tensorflow <span class=\"im\">as</span> tf\n<span class=\"im\">import</span> numpy <span class=\"im\">as</span> np\n<span class=\"im\">import</span> load_mnist\n<span class=\"im\">import</span> matplotlib.pyplot <span class=\"im\">as</span> plt\n<span class=\"im\">import</span> seaborn <span class=\"im\">as</span> sns\nsns.<span class=\"bu\">set</span>(color_codes<span class=\"op\">=</span><span class=\"va\">True</span>)\n<span class=\"op\">%</span>matplotlib inline\nmnist <span class=\"op\">=</span> load_mnist.read_data_sets(<span class=\"st\">&#39;MNIST_data&#39;</span>, one_hot<span class=\"op\">=</span><span class=\"va\">True</span>)</code></pre></div>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"kw\">def</span> weight_variable(shape):\n  initial <span class=\"op\">=</span> tf.truncated_normal(shape, stddev<span class=\"op\">=</span><span class=\"fl\">0.1</span>)\n  <span class=\"cf\">return</span> tf.Variable(initial)\n\n<span class=\"kw\">def</span> bias_variable(shape):\n  initial <span class=\"op\">=</span> tf.constant(<span class=\"fl\">0.1</span>, shape<span class=\"op\">=</span>shape)\n  <span class=\"cf\">return</span> tf.Variable(initial)\n\n<span class=\"kw\">def</span> simple_fc_layer(input_layer, shape):\n    w <span class=\"op\">=</span> weight_variable(shape)\n    b <span class=\"op\">=</span> bias_variable([shape[<span class=\"dv\">1</span>]])\n    <span class=\"cf\">return</span> tf.nn.tanh(tf.matmul(input_layer,w) <span class=\"op\">+</span> b)\n\n<span class=\"kw\">def</span> cross_entropy_layer(input_layer,shape):\n    w <span class=\"op\">=</span> weight_variable(shape)\n    b <span class=\"op\">=</span> bias_variable([shape[<span class=\"dv\">1</span>]])\n    <span class=\"cf\">return</span> tf.nn.softmax(tf.matmul(input_layer,w) <span class=\"op\">+</span> b)\n\n<span class=\"kw\">def</span> accuracy(y, y_):\n    correct <span class=\"op\">=</span> tf.equal(tf.argmax(y,<span class=\"dv\">1</span>), tf.argmax(y_,<span class=\"dv\">1</span>))\n    <span class=\"cf\">return</span> tf.reduce_mean(tf.cast(correct, <span class=\"st\">&quot;float&quot;</span>))</code></pre></div>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">x <span class=\"op\">=</span> tf.placeholder(<span class=\"st\">&quot;float&quot;</span>, shape<span class=\"op\">=</span>[<span class=\"va\">None</span>, <span class=\"dv\">784</span>])\ny_ <span class=\"op\">=</span> tf.placeholder(<span class=\"st\">&quot;float&quot;</span>, shape<span class=\"op\">=</span>[<span class=\"va\">None</span>, <span class=\"dv\">10</span>])\nlr <span class=\"op\">=</span> tf.placeholder(<span class=\"st\">&quot;float&quot;</span>)\n\nl1 <span class=\"op\">=</span> simple_fc_layer(x, [<span class=\"dv\">784</span>,<span class=\"dv\">100</span>])\ny1 <span class=\"op\">=</span> cross_entropy_layer(l1,[<span class=\"dv\">100</span>,<span class=\"dv\">10</span>])\n\nl2 <span class=\"op\">=</span> simple_fc_layer(l1, [<span class=\"dv\">100</span>,<span class=\"dv\">100</span>])\ny2 <span class=\"op\">=</span> cross_entropy_layer(l2,[<span class=\"dv\">100</span>,<span class=\"dv\">10</span>])\n\nl3 <span class=\"op\">=</span> simple_fc_layer(l2, [<span class=\"dv\">100</span>,<span class=\"dv\">100</span>])\ny3 <span class=\"op\">=</span> cross_entropy_layer(l3,[<span class=\"dv\">100</span>,<span class=\"dv\">10</span>])\n\n\nce1 <span class=\"op\">=</span> <span class=\"op\">-</span>tf.reduce_sum(y_<span class=\"op\">*</span>tf.log(y1))\nce2 <span class=\"op\">=</span> <span class=\"op\">-</span>tf.reduce_sum(y_<span class=\"op\">*</span>tf.log(y2))\nce3 <span class=\"op\">=</span> <span class=\"op\">-</span>tf.reduce_sum(y_<span class=\"op\">*</span>tf.log(y3))\n\nts1 <span class=\"op\">=</span> tf.train.GradientDescentOptimizer(lr).minimize(ce1)\nts2 <span class=\"op\">=</span> tf.train.GradientDescentOptimizer(lr).minimize(ce2)\nts3 <span class=\"op\">=</span> tf.train.GradientDescentOptimizer(lr).minimize(ce3)\n\na1 <span class=\"op\">=</span> accuracy(y1,y_)\na2 <span class=\"op\">=</span> accuracy(y2,y_)\na3 <span class=\"op\">=</span> accuracy(y3,y_)</code></pre></div>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">train3 <span class=\"op\">=</span> []\ntrain123 <span class=\"op\">=</span> []\n\n<span class=\"cf\">for</span> run <span class=\"kw\">in</span> <span class=\"bu\">range</span>(<span class=\"dv\">400</span>):\n    <span class=\"cf\">with</span> tf.Session() <span class=\"im\">as</span> sess:\n        sess.run(tf.initialize_all_variables())\n        <span class=\"cf\">for</span> i <span class=\"kw\">in</span> <span class=\"bu\">range</span>(<span class=\"dv\">1000</span>):\n            start <span class=\"op\">=</span> (<span class=\"dv\">50</span><span class=\"op\">*</span>i) <span class=\"op\">%</span> <span class=\"dv\">1000</span>\n            end <span class=\"op\">=</span> start <span class=\"op\">+</span> <span class=\"dv\">50</span>\n            learning_rate <span class=\"op\">=</span> <span class=\"fl\">0.01</span> <span class=\"cf\">if</span> i <span class=\"op\">&lt;</span> <span class=\"dv\">750</span> <span class=\"cf\">else</span> <span class=\"fl\">0.003</span>\n            ts3.run(feed_dict<span class=\"op\">=</span>{x: mnist.train.images[start:end], y_: mnist.train.labels[start:end], lr:learning_rate})\n        res <span class=\"op\">=</span> a3.<span class=\"bu\">eval</span>(feed_dict<span class=\"op\">=</span>{x: mnist.test.images, y_: mnist.test.labels})\n        <span class=\"bu\">print</span>(res, end<span class=\"op\">=</span><span class=\"st\">&quot;</span><span class=\"ch\">\\r</span><span class=\"st\">&quot;</span>)\n        train3.append(res)</code></pre></div>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"cf\">for</span> run <span class=\"kw\">in</span> <span class=\"bu\">range</span>(<span class=\"dv\">400</span>):\n    <span class=\"cf\">with</span> tf.Session() <span class=\"im\">as</span> sess:\n        sess.run(tf.initialize_all_variables())\n        <span class=\"cf\">for</span> i <span class=\"kw\">in</span> <span class=\"bu\">range</span>(<span class=\"dv\">200</span>):\n            start <span class=\"op\">=</span> (<span class=\"dv\">50</span><span class=\"op\">*</span>i) <span class=\"op\">%</span> <span class=\"dv\">1000</span>\n            end <span class=\"op\">=</span> start <span class=\"op\">+</span> <span class=\"dv\">50</span>\n            ts1.run(feed_dict<span class=\"op\">=</span>{x: mnist.train.images[start:end], y_: mnist.train.labels[start:end], lr: <span class=\"fl\">0.01</span>})\n        <span class=\"cf\">for</span> i <span class=\"kw\">in</span> <span class=\"bu\">range</span>(<span class=\"dv\">200</span>):\n            start <span class=\"op\">=</span> (<span class=\"dv\">50</span><span class=\"op\">*</span>i) <span class=\"op\">%</span> <span class=\"dv\">1000</span>\n            end <span class=\"op\">=</span> start <span class=\"op\">+</span> <span class=\"dv\">50</span>\n            ts2.run(feed_dict<span class=\"op\">=</span>{x: mnist.train.images[start:end], y_: mnist.train.labels[start:end], lr: <span class=\"fl\">0.01</span>})\n        <span class=\"cf\">for</span> i <span class=\"kw\">in</span> <span class=\"bu\">range</span>(<span class=\"dv\">1000</span>):\n            start <span class=\"op\">=</span> (<span class=\"dv\">50</span><span class=\"op\">*</span>i) <span class=\"op\">%</span> <span class=\"dv\">1000</span>\n            end <span class=\"op\">=</span> start <span class=\"op\">+</span> <span class=\"dv\">50</span>\n            learning_rate <span class=\"op\">=</span> <span class=\"fl\">0.01</span> <span class=\"cf\">if</span> i <span class=\"op\">&lt;</span> <span class=\"dv\">750</span> <span class=\"cf\">else</span> <span class=\"fl\">0.003</span>\n            ts3.run(feed_dict<span class=\"op\">=</span>{x: mnist.train.images[start:end], y_: mnist.train.labels[start:end], lr:learning_rate})\n        res <span class=\"op\">=</span> a3.<span class=\"bu\">eval</span>(feed_dict<span class=\"op\">=</span>{x: mnist.test.images, y_: mnist.test.labels})\n        <span class=\"bu\">print</span>(res, end<span class=\"op\">=</span><span class=\"st\">&quot;</span><span class=\"ch\">\\r</span><span class=\"st\">&quot;</span>)\n        train123.append(res)</code></pre></div>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">sns.distplot(train3,label<span class=\"op\">=</span><span class=\"st\">&quot;Train all layers&quot;</span>)\nsns.distplot(train123,label<span class=\"op\">=</span><span class=\"st\">&quot;Train layer-by-layer&quot;</span>)\nplt.legend()</code></pre></div>\n<figure>\n<img src=\"https://r2rt.com/static/images/FCB_output_12_2.png\" />\n</figure>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"bu\">print</span>(<span class=\"st\">&quot;Mean and standard deviation of training all layers: &quot;</span> <span class=\"op\">+</span> <span class=\"bu\">str</span>(np.mean(train3)) <span class=\"op\">+</span> <span class=\"st\">&quot;, &quot;</span> <span class=\"op\">+</span> <span class=\"bu\">str</span>(np.std(train3)))\n<span class=\"bu\">print</span>(<span class=\"st\">&quot;Mean and standard deviation of layer-wise training: &quot;</span> <span class=\"op\">+</span> <span class=\"bu\">str</span>(np.mean(train123)) <span class=\"op\">+</span> <span class=\"st\">&quot;, &quot;</span> <span class=\"op\">+</span> <span class=\"bu\">str</span>(np.std(train123)))</code></pre></div>\n<pre><code>Mean and standard deviation of training all layers: 0.87867, 0.00274425\nMean and standard deviation of layer-wise training: 0.876439, 0.00234411</code></pre>\n</body>\n</html>"
}