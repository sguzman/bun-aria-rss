{
  "title": "How much compute do we need to train generative models?",
  "link": "http://dustintran.com/blog/how-much-compute-do-we-need-to-train-generative-models",
  "guid": "http://dustintran.com/blog/how-much-compute-do-we-need-to-train-generative-models",
  "description": "<p><em>Update (09/01/17): The post is written to be somewhat silly and numbers are not meant to be accurate. For example, there is a simplifying assumption that training time scales linearly with the # of bits to encode the output; and 5000 is chosen arbitrarily given only that the output’s range has 65K*3 dimensions and each takes one of 256 integers.</em></p>\n\n<p>Discriminative models can take weeks to train. It was only until a\nbreakthrough two months ago by Facebook <a href=\"#goyal2017accurate\">(Goyal et al., 2017)</a> that we could successfully train a neural net\nexceeding human accuracy (ResNet-50) on ImageNet in one hour. And this\nwas with 256 GPUs and a monstrous batch size of 8192.\n<!-- Unfortunately, most of us mortals have maybe 8 GPUs at most—or for the -->\n<!-- very fortunate, at most 8 GPUs per experiment—and do not have help     -->\n<!-- from the first authors of ResNets and Caffe. This means in 2017, each    -->\n<!-- ImageNet classifier can still take days to a week.                       --></p>\n\n<p>Contrast this with generative models.  We’ve made progress in\nstability and sample diversity with generative adversarial networks,\nwhere, say, Wasserstein GANs with gradient penalty\n<a href=\"#gulrajani2017improved\">(Gulrajani, Ahmed, Arjovsky, Dumoulin, &amp; Courville, 2017)</a> and\nCramer GANs\n<a href=\"#bellemare2017cramer\">(Bellemare et al., 2017)</a>\ncan get good results for generating LSUN bedrooms.\nBut in communication with\nIshaan Gulrajani, this took 3 days to train with 4 GPUs and 900,000\ntotal iterations; moreover, LSUN\nhas a resolution of 64x64 and is\nsignificantly less diverse than the 256x256 sized ImageNet.\n<!-- : this is especially the case as we do 5 discriminator                -->\n<!-- updates per generator update, which is already a 5x slowdown compared -->\n<!-- to vanilla GANs per-generator iteration.                              -->\nLet’s also not kid ourselves\nthat we perfected density estimation to learn the true distribution of\nLSUN bedrooms yet.</p>\n\n<p>Generative models for text are no different. The best results so far for the 1 billion\nlanguage modeling benchmark are an LSTM with 151 million parameters\n(excluding embedding and softmax layers)\nwhich took 3 weeks to train with 32 GPUs\n<a href=\"#jozefowicz2016exploring\">(Jozefowicz, Vinyals, Schuster, Shazeer, &amp; Wu, 2016)</a>\nand a mixture of experts LSTM with 4.3 billion parameters\n<a href=\"#shazeer2017outrageously\">(Shazeer et al., 2017)</a>.\n<!-- downsampled ImageNet. --></p>\n\n<p>This begs the question: how much compute <em>should</em> we expect in order\nto learn a generative model?</p>\n\n<p>Suppose we restrict ourselves to 256x256 ImageNet as a proxy for\nnatural images.\nA simple property in information theory says that the the entropy of\nthe conditional\n<script type=\"math/tex\">p(\\text{class label}\\mid \\text{natural image})</script>\nis upper bounded by at most <script type=\"math/tex\">\\log K</script> bits for <script type=\"math/tex\">K</script> classes.\nComparing this to the entropy of the unconditional\n<script type=\"math/tex\">p(\\text{natural image})</script>, whose\nnumber of bits is a function of <script type=\"math/tex\">256\\times 256=65,536</script>\npixels each of which take 3 values from <script type=\"math/tex\">[0, 255]</script>,\nthen a very modest guess would be that <script type=\"math/tex\">p(\\text{natural image})</script> has\n5000 times more bits. We also need to\naccount for the difference in training methods.  Let’s say that the\nmethod for generative models is only 6x slower than that of\ndiscriminative models (5 discriminative updates per generator update;\nwe’ll forget the fact that GAN and MMD objectives are actually more expensive\nthan maximum likelihood due to multiple forward and backward passes).</p>\n\n<p>Finally, let’s take Facebook’s result as a baseline for learning\n<script type=\"math/tex\">p(\\text{class label}\\mid \\text{natural image})</script> in 1 hour with 256 GPUs\nand a batch size of 8192. <strong>Then the distribution <script type=\"math/tex\">p(\\text{natural image})</script>\nwould require\n1 hour <script type=\"math/tex\">\\cdot</script> 5000 <script type=\"math/tex\">\\cdot</script> 6 <script type=\"math/tex\">=</script> 30,000 hours <script type=\"math/tex\">\\approx</script> 3.4 years to train.</strong>\nAnd this is assuming we have the right objective, architecture, and\nhyperparameters to set it and forget it: until then, let’s hope for\nbetter hardware.</p>\n\n<p><em>This short post is extracted from a fun conversation with Alec Radford today.</em></p>\n\n<h2 id=\"references\">References</h2>\n\n<ol class=\"bibliography\"><li><span id=\"bellemare2017cramer\">Bellemare, M. G., Danihelka, I., Dabney, W., Mohamed, S., Lakshminarayanan, B., Hoyer, S., &amp; Munos, R. (2017). The Cramer Distance as a Solution to Biased Wasserstein Gradients. <i>ArXiv Preprint ArXiv:1705.10743</i>.</span></li>\n<li><span id=\"goyal2017accurate\">Goyal, P., Dollár, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., … He, K. (2017). Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour. <i>ArXiv Preprint ArXiv:1706.02677</i>.</span></li>\n<li><span id=\"gulrajani2017improved\">Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., &amp; Courville, A. (2017). Improved Training of Wasserstein GANs. <i>ArXiv Preprint ArXiv:1704.00028</i>.</span></li>\n<li><span id=\"jozefowicz2016exploring\">Jozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., &amp; Wu, Y. (2016). Exploring the Limits of Language Modeling. <i>ArXiv Preprint ArXiv:1602.02410</i>.</span></li>\n<li><span id=\"shazeer2017outrageously\">Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., &amp; Dean, J. (2017). Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer. In <i>International Conference on Learning Representations</i>.</span></li></ol>",
  "pubDate": "Thu, 31 Aug 2017 00:00:00 -0700"
}