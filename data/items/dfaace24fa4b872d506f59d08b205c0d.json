{
  "title": "Reading and Writing Parquet Files on S3 with Pandas and PyArrow",
  "link": "",
  "published": "2022-04-10T00:00:00-05:00",
  "updated": "2022-04-10T00:00:00-05:00",
  "id": "http://janakiev.com/blog/pandas-pyarrow-parquet-s3",
  "content": "<p>When working with large amounts of data, a common approach is to store the data in S3 buckets. Instead of dumping the data as CSV files or plain text files, a good option is to use <a href=\"https://parquet.apache.org/\">Apache Parquet</a>. In this short guide you’ll see how to read and write Parquet files on S3 using Python, Pandas and <a href=\"https://arrow.apache.org/docs/python/index.html\">PyArrow</a>.</p>\n\n<p>This guide was tested using <a href=\"https://contabo.com/\">Contabo</a> object storage, <a href=\"https://min.io/\">MinIO</a>, and <a href=\"https://linode.gvw92c.net/LPg90o\">Linode</a> Object Storage. You should be able to use it on most S3-compatible providers and software.</p>\n\n<h1 id=\"prepare-connection\">Prepare Connection</h1>\n\n<p>Prepare the S3 environment variables in a file called <code class=\"language-plaintext highlighter-rouge\">.env</code> in the project folder with the following contents:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>S3_REGION=eu-central-1\nS3_ENDPOINT=https://eu-central-1.domain.com\nS3_ACCESS_KEY=XXXX\nS3_SECRET_KEY=XXXX\n</code></pre></div></div>\n\n<p>Prepare some S3 bucket that you want to use. In this case we’ll be using <code class=\"language-plaintext highlighter-rouge\">s3://s3-example</code> bucket to store and access our data. Next, prepare some random example data with:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"n\">np</span>\n<span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"n\">pd</span>\n\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"p\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">({</span><span class=\"s\">'data'</span><span class=\"p\">:</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">((</span><span class=\"mi\">1000</span><span class=\"p\">,))})</span>\n<span class=\"n\">df</span><span class=\"p\">.</span><span class=\"n\">to_parquet</span><span class=\"p\">(</span><span class=\"s\">\"data/data.parquet\"</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>Load the environment variables in your script with <a href=\"https://github.com/theskumar/python-dotenv\">python-dotenv</a>:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">dotenv</span> <span class=\"kn\">import</span> <span class=\"n\">load_dotenv</span>\n<span class=\"n\">load_dotenv</span><span class=\"p\">();</span>\n</code></pre></div></div>\n\n<p>Now, prepare the S3 connection with:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">import</span> <span class=\"nn\">os</span>\n<span class=\"kn\">import</span> <span class=\"nn\">s3fs</span>\n\n<span class=\"n\">fs</span> <span class=\"o\">=</span> <span class=\"n\">s3fs</span><span class=\"p\">.</span><span class=\"n\">S3FileSystem</span><span class=\"p\">(</span>\n    <span class=\"n\">anon</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">,</span>\n    <span class=\"n\">use_ssl</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span>\n    <span class=\"n\">client_kwargs</span><span class=\"o\">=</span><span class=\"p\">{</span>\n        <span class=\"s\">\"region_name\"</span><span class=\"p\">:</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s\">'S3_REGION'</span><span class=\"p\">],</span>\n        <span class=\"s\">\"endpoint_url\"</span><span class=\"p\">:</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s\">'S3_ENDPOINT'</span><span class=\"p\">],</span>\n        <span class=\"s\">\"aws_access_key_id\"</span><span class=\"p\">:</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s\">'S3_ACCESS_KEY'</span><span class=\"p\">],</span>\n        <span class=\"s\">\"aws_secret_access_key\"</span><span class=\"p\">:</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s\">'S3_SECRET_KEY'</span><span class=\"p\">],</span>\n        <span class=\"s\">\"verify\"</span><span class=\"p\">:</span> <span class=\"bp\">True</span><span class=\"p\">,</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">)</span>\n</code></pre></div></div>\n\n<h1 id=\"write-pandas-dataframe-to-s3-as-parquet\">Write Pandas DataFrame to S3 as Parquet</h1>\n\n<p>Save the DataFrame to S3 using s3fs and Pandas:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">with</span> <span class=\"n\">fs</span><span class=\"p\">.</span><span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s\">'s3-example/data.parquet'</span><span class=\"p\">,</span> <span class=\"s\">'wb'</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n    <span class=\"n\">df</span><span class=\"p\">.</span><span class=\"n\">to_parquet</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>Save the DataFrame to S3 using s3fs and PyArrow:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">import</span> <span class=\"nn\">pyarrow</span> <span class=\"k\">as</span> <span class=\"n\">pa</span>\n<span class=\"kn\">import</span> <span class=\"nn\">pyarrow.parquet</span> <span class=\"k\">as</span> <span class=\"n\">pq</span>\n<span class=\"kn\">from</span> <span class=\"nn\">pyarrow</span> <span class=\"kn\">import</span> <span class=\"n\">Table</span>\n\n<span class=\"n\">s3_filepath</span> <span class=\"o\">=</span> <span class=\"s\">'s3-example/data.parquet'</span>\n\n<span class=\"n\">pq</span><span class=\"p\">.</span><span class=\"n\">write_to_dataset</span><span class=\"p\">(</span>\n    <span class=\"n\">Table</span><span class=\"p\">.</span><span class=\"n\">from_pandas</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">),</span>\n    <span class=\"n\">s3_filepath</span><span class=\"p\">,</span>\n    <span class=\"n\">filesystem</span><span class=\"o\">=</span><span class=\"n\">fs</span><span class=\"p\">,</span>\n    <span class=\"n\">use_dictionary</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span>\n    <span class=\"n\">compression</span><span class=\"o\">=</span><span class=\"s\">\"snappy\"</span><span class=\"p\">,</span>\n    <span class=\"n\">version</span><span class=\"o\">=</span><span class=\"s\">\"2.4\"</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>You can also upload this file with <a href=\"https://s3tools.org/s3cmd\">s3cmd</a> by typing:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>s3cmd \\\n  --config ~/.s3cfg \\\n  put data/data.parquet s3://s3-example\n</code></pre></div></div>\n\n<h1 id=\"reading-parquet-file-from-s3-as-pandas-dataframe\">Reading Parquet File from S3 as Pandas DataFrame</h1>\n\n<p>Now, let’s have a look at the Parquet file by using PyArrow:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">s3_filepath</span> <span class=\"o\">=</span> <span class=\"s\">\"s3-example/data.parquet\"</span>\n\n<span class=\"n\">pf</span> <span class=\"o\">=</span> <span class=\"n\">pq</span><span class=\"p\">.</span><span class=\"n\">ParquetDataset</span><span class=\"p\">(</span>\n    <span class=\"n\">s3_filepath</span><span class=\"p\">,</span>\n    <span class=\"n\">filesystem</span><span class=\"o\">=</span><span class=\"n\">fs</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>Now, you can already explore the metadata with <code class=\"language-plaintext highlighter-rouge\">pf.metadata</code> or the schema with <code class=\"language-plaintext highlighter-rouge\">pf.schema</code>. To read the data set into Pandas type:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">pf</span><span class=\"p\">.</span><span class=\"n\">metadata</span>\n</code></pre></div></div>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">pf</span><span class=\"p\">.</span><span class=\"n\">schema</span>\n</code></pre></div></div>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>&lt;pyarrow._parquet.ParquetSchema object at 0x7f1c2fa4a300&gt;\nrequired group field_id=-1 schema {\n  optional double field_id=-1 data;\n}\n</code></pre></div></div>\n\n<p>When using <code class=\"language-plaintext highlighter-rouge\">ParquetDataset</code>, you can also use multiple paths. You can get those for example with:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">s3_filepath</span> <span class=\"o\">=</span> <span class=\"s\">'s3://s3-example'</span>\n<span class=\"n\">s3_filepaths</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">path</span> <span class=\"k\">for</span> <span class=\"n\">path</span> <span class=\"ow\">in</span> <span class=\"n\">fs</span><span class=\"p\">.</span><span class=\"n\">ls</span><span class=\"p\">(</span><span class=\"n\">s3_filepath</span><span class=\"p\">)</span>\n                <span class=\"k\">if</span> <span class=\"n\">path</span><span class=\"p\">.</span><span class=\"n\">endswith</span><span class=\"p\">(</span><span class=\"s\">'.parquet'</span><span class=\"p\">)]</span>\n<span class=\"n\">s3_filepaths</span>\n</code></pre></div></div>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>['s3-example/data.parquet', 's3-example/data.parquet']\n</code></pre></div></div>\n\n<h1 id=\"resources\">Resources</h1>\n\n<ul>\n  <li><a href=\"https://s3fs.readthedocs.io/en/latest/\">s3fs.readthedocs.io</a> - S3Fs Documentation</li>\n  <li><a href=\"https://arrow.apache.org/docs/python/index.html\">PyArrow</a> - Apache Arrow Python bindings</li>\n  <li><a href=\"https://parquet.apache.org/\">Apache Parquet</a></li>\n</ul>",
  "author": {
    "name": "Nikolai Janakiev"
  },
  "category": [
    "",
    "",
    "",
    "",
    "",
    "",
    ""
  ],
  "summary": "When working with large amounts of data, a common approach is to store the data in S3 buckets. Instead of dumping the data as CSV files or plain text files, a good option is to use Apache Parquet. In this short guide you’ll see how to read and write Parquet files on S3 using Python, Pandas and PyArrow.",
  "media:thumbnail": "",
  "media:content": ""
}