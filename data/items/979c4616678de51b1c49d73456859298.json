{
  "title": "Dask Development Log",
  "link": "",
  "updated": "2016-12-05T00:00:00+00:00",
  "id": "https://mrocklin.github.io/blog/work/2016/12/05/dask-dev-1",
  "content": "<p><em>This work is supported by <a href=\"http://continuum.io\">Continuum Analytics</a>\nthe <a href=\"http://www.darpa.mil/program/XDATA\">XDATA Program</a>\nand the Data Driven Discovery Initiative from the <a href=\"https://www.moore.org/\">Moore\nFoundation</a></em></p>\n\n<p>Dask has been active lately due to a combination of increased adoption and\nfunded feature development by private companies.  This increased activity\nis great, however an unintended side effect is that I have spent less time\nwriting about development and engaging with the broader community.  To address\nthis I hope to write one blogpost a week about general development.  These will\nnot be particularly polished, nor will they announce ready-to-use features for\nusers, however they should increase transparency and hopefully better engage\nthe developer community.</p>\n\n<p>So themes of last week</p>\n\n<ol>\n  <li>Embedded Bokeh servers for the Workers</li>\n  <li>Smarter workers</li>\n  <li>An overhauled scheduler that is slightly simpler overall (thanks to the\nsmarter workers) but with more clever work stealing</li>\n  <li>Fastparquet</li>\n</ol>\n\n<h3 id=\"embedded-bokeh-servers-in-dask-workers\">Embedded Bokeh Servers in Dask Workers</h3>\n\n<p>The distributed scheduler’s <a href=\"http://distributed.readthedocs.io/en/latest/web.html\">web diagnostic\npage</a> is one of Dask’s\nmore flashy features.  It shows the passage of every computation on the cluster\nin real time.  These diagnostics are invaluable for understanding performance\nboth for users and for core developers.</p>\n\n<p>I intend to focus on worker performance soon, so I decided to attach a Bokeh\nserver to every worker to serve web diagnostics about that worker.  To make\nthis easier, I also learned how to <em>embed</em> Bokeh servers inside of other\nTornado applications.  This has reduced the effort to create new visuals and\nexpose real time information considerably and I can now create a full live\nvisualization in around 30 minutes.  It is now <em>faster</em> for me to build\na new diagnostic than to grep through logs.  It’s pretty useful.</p>\n\n<p>Here are some screenshots.  Nothing too flashy, but this information is highly\nvaluable to me as I measure bandwidths, delays of various parts of the code,\nhow workers send data between each other, etc..</p>\n\n<p><a href=\"https://mrocklin.github.io/blog/images/bokeh-worker-system.png\">\n  <img src=\"https://mrocklin.github.io/blog/images/bokeh-worker-system.png\" alt=\"Dask Bokeh Worker system page\" width=\"30%\" /></a>\n<a href=\"https://mrocklin.github.io/blog/images/bokeh-worker-main.png\">\n  <img src=\"https://mrocklin.github.io/blog/images/bokeh-worker-main.png\" alt=\"Dask Bokeh Worker system page\" width=\"30%\" /></a>\n<a href=\"https://mrocklin.github.io/blog/images/bokeh-worker-crossfilter.png\">\n  <img src=\"https://mrocklin.github.io/blog/images/bokeh-worker-crossfilter.png\" alt=\"Dask Bokeh Worker system page\" width=\"30%\" /></a></p>\n\n<p>To be clear, these diagnostic pages aren’t polished in any way.  There’s lots\nmissing, it’s just what I could get done in a day.  Still, everyone running a\nTornado application should have an embedded Bokeh server running.  They’re\ngreat for rapidly pushing out visually rich diagnostics.</p>\n\n<h3 id=\"smarter-workers-and-a-simpler-scheduler\">Smarter Workers and a Simpler Scheduler</h3>\n\n<p>Previously the scheduler knew everything and the workers were fairly\nsimple-minded.  Now we’ve moved some of the knowledge and responsibility over\nto the workers.  Previously the scheduler would give just enough work to the\nworkers to keep them occupied.  This allowed the scheduler to make better\ndecisions about the state of the entire cluster.  By delaying committing a task\nto a worker until the last moment we made sure that we were making the right\ndecision.  However, this also means that the worker sometimes has idle\nresources, particularly network bandwidth, when it could be speculatively\npreparing for future work.</p>\n\n<p>Now we commit all ready-to-run tasks to a worker immediately and that worker\nhas the ability to pipeline those tasks as it sees fit.  This is better locally\nbut slightly worse globally.  To counter balance this we’re now being much more\naggressive about work stealing and, because the workers have more information,\nthey can manage some of the administrative costs of works stealing themselves.\nBecause this isn’t bound to run on just the scheduler we can use more expensive\nalgorithms than when when did everything on the scheduler.</p>\n\n<p>There were a few motivations for this change:</p>\n\n<ol>\n  <li>Dataframe performance was bound by keeping the worker hardware fully\noccupied, which we weren’t doing.  I expect that these changes will\neventually yield something like a 30% speedup.</li>\n  <li>Users on traditional job scheduler machines (SGE, SLURM, TORQUE) and users\nwho like GPUS, both wanted the ability to tag tasks with specific resource\nconstraints like “This consumes one GPU” or “This task requires a 5GB of RAM\nwhile running” and ensure that workers would respect those constraints when\nrunning tasks.  The old workers weren’t complex enough to reason about these\nconstraints.  With the new workers, adding this feature was trivial.</li>\n  <li>By moving logic from the scheduler to the worker we’ve actually made them\nboth easier to reason about.  This should lower barriers for contributors\nto get into the core project.</li>\n</ol>\n\n<h3 id=\"dataframe-algorithms\">Dataframe algorithms</h3>\n\n<p><a href=\"https://github.com/dask/dask/pull/1807\">Approximate nunique</a> and\nmultiple-output-partition groupbys landed in master last week.  These arose\nbecause some power-users had very large dataframes that weree running into\nscalability limits.  Thanks to Mike Graham for the approximate nunique\nalgorithm.  This has also pushed <a href=\"https://github.com/pandas-dev/pandas/pull/14729\">hashing\nchanges</a> upstream to Pandas.</p>\n\n<h3 id=\"fast-parquet\">Fast Parquet</h3>\n\n<p>Martin Durant has been working on a Parquet reader/writer for Python using\nNumba.   It’s pretty slick.  He’s been using it on internal Continuum projects\nfor a little while and has seen both good performance and a very Pythonic\nexperience for what was previously a format that was pretty inaccessible.</p>\n\n<p>He’s planning to write about this in the near future so I won’t steal his\nthunder.  Here is a link to the documentation:\n<a href=\"https://fastparquet.readthedocs.io/en/latest/\">fastparquet.readthedocs.io</a></p>"
}