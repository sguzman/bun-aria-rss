{
  "title": "Q-learning and DQN",
  "link": "",
  "published": "2020-04-06T00:00:00-07:00",
  "updated": "2020-04-06T00:00:00-07:00",
  "author": {
    "name": "Cathy Yeh"
  },
  "id": "tag:efavdb.com,2020-04-06:/dqn",
  "summary": "<p>[<span class=\"caps\">TOC</span>]</p>\n<p>Q-learning is a reinforcement learning (<span class=\"caps\">RL</span>) algorithm that is the basis for deep Q networks (<span class=\"caps\">DQN</span>), the algorithm by Google DeepMind that achieved human-level performance for a range of Atari games and kicked off the deep <span class=\"caps\">RL</span> revolution starting in&nbsp;2013-2015.</p>\n<p>We begin with some historical context, then provide …</p>",
  "content": "<p>[<span class=\"caps\">TOC</span>]</p>\n<p>Q-learning is a reinforcement learning (<span class=\"caps\">RL</span>) algorithm that is the basis for deep Q networks (<span class=\"caps\">DQN</span>), the algorithm by Google DeepMind that achieved human-level performance for a range of Atari games and kicked off the deep <span class=\"caps\">RL</span> revolution starting in&nbsp;2013-2015.</p>\n<p>We begin with some historical context, then provide an overview of value function methods / Q-learning, and conclude with a discussion of <span class=\"caps\">DQN</span>.</p>\n<p>If you want to skip straight to code, the implementation of <span class=\"caps\">DQN</span> that we used to train the agent playing Atari Breakout below is available <a href=\"https://github.com/frangipane/reinforcement-learning/tree/master/DQN\">here</a>.</p>\n<p align=\"center\">\n<img src=\"images/atari_breakout.gif\" alt=\"Atari Breakout\" style=\"width:250px;\">\n</p>\n\n<p>If you watch the video long enough, you&#8217;ll see the agent has learned a strategy that favors breaking bricks at the edges so the ball &#8220;breaks out&#8221; to the upper side, resulting in a cascade of&nbsp;points.</p>\n<h2>Historical&nbsp;context</h2>\n<p>The theories that underpin today’s reinforcement learning algorithms were developed decades ago.  For example, Watkins developed Q-learning, a value function method, in <a href=\"http://www.cs.rhul.ac.uk/~chrisw/thesis.html\">1989</a>, and Williams proposed the <span class=\"caps\">REINFORCE</span> policy gradient method in <a href=\"https://link.springer.com/content/pdf/10.1007%2FBF00992696.pdf\">1987</a>. So why the recent surge of interest in deep <span class=\"caps\">RL</span>?</p>\n<h3>Representational power from Neural&nbsp;Networks</h3>\n<p>Until 2013, most applications of <span class=\"caps\">RL</span> relied on hand engineered inputs for value function and policy representations, which drastically limited the scope of applicability to the real world.  Mnih et. al [1] made use of advances in computational power and neural network (<span class=\"caps\">NN</span>) architectures to use a deep <span class=\"caps\">NN</span> for <em>value function approximation</em>, showing that NNs can learn a useful representation from raw pixel inputs in Atari&nbsp;games.</p>\n<h3>Variations on a theme: vanilla <span class=\"caps\">RL</span> algorithms don’t work well&nbsp;out-of-the-box</h3>\n<p>The basic <span class=\"caps\">RL</span> algorithms that were developed decades ago do not work well in practice without modifications.  For example, <span class=\"caps\">REINFORCE</span> relies on Monte Carlo estimates of the performance gradient; such estimates of the performance gradient are high variance, resulting in unstable or impractically slow learning (poor sample efficiency).  The original Q-learning algorithm also suffers from instability due to correlated sequential training data and parameter updates affecting both the estimator and target, creating a “moving target” and hence&nbsp;divergences.</p>\n<p>We can think of these original <span class=\"caps\">RL</span> algorithms as the Wright Brothers plane.\n<p align=\"center\">\n<img src=\"images/wright_brothers_plane.png\" alt=\"Wright brothers plane\" style=\"width:500px;\">\n</p></p>\n<p>The foundational shape is there and recognizable in newer models.  However, the enhancements of newer algorithms aren&#8217;t just bells and whistles &#8212; they have enabled the move from toy problems into more functional&nbsp;territory.</p>\n<h2>Q-learning</h2>\n<h3>Background</h3>\n<p><span class=\"caps\">RL</span> models the sequential decision-making problem as a Markov Decision Process (<span class=\"caps\">MDP</span>): transitions from state to state involve both environment dynamics and an agent whose actions affect both the probability of transitioning to the next state and the reward&nbsp;received.</p>\n<p>The goal is to find a policy, a mapping from state to actions, that will maximize the agent’s expected returns, i.e. their cumulative future&nbsp;rewards.</p>\n<p>Q-learning is an algorithm for learning the eponymous <span class=\"math\">\\(Q(s,a)\\)</span> action-value function, defined as the expected returns for each state-action <span class=\"math\">\\((s,a)\\)</span> pair, corresponding to following the optimal&nbsp;policy.</p>\n<h3>Goal: solve the Bellman optimality&nbsp;equation</h3>\n<p>Recall that <span class=\"math\">\\(q_*\\)</span> is described by a self-consistent, recursive relation, the Bellman optimality equation, that falls out from the Markov property [6, 7] of&nbsp;MDPs</p>\n<div class=\"math\">\\begin{eqnarray}\\label{action-value-bellman-optimality} \\tag{1}\nq_*(s, a) &amp;=&amp; \\mathbb{E}_{\\pi*} [R_{t+1} + \\gamma \\max_{a'} q_*(S_{t+1}', a') | S_t = s, A_t = a] \\\\\n          &amp;=&amp; \\sum_{s', r} p(s', r | s, a) [r + \\gamma \\max_{a'} q_*(s', a') ]\n\\end{eqnarray}</div>\n<p>where <span class=\"math\">\\(0 \\leq \\gamma \\leq 1\\)</span> is the <em>discount rate</em> which characterizes how much we weight rewards now vs. later, <span class=\"math\">\\(R_{t+1}\\)</span> is the reward at timestep <span class=\"math\">\\(t+1\\)</span>, and <span class=\"math\">\\(p(s', r | s, a)\\)</span> is the environment transition&nbsp;dynamics.</p>\n<p>Our <a href=\"https://efavdb.com/intro-rl-toy-example.html\">introduction to <span class=\"caps\">RL</span></a> provides more background on the Bellman equations in case (\\ref{action-value-bellman-optimality}) looks&nbsp;unfamiliar.</p>\n<h3>The Q-learning approach to solving the Bellman&nbsp;equation</h3>\n<p>We use capitalized <span class=\"math\">\\(Q\\)</span> to denote an estimate and lowercase <span class=\"math\">\\(q\\)</span> to denote the real action-value function.  The Q-learning algorithm makes the following&nbsp;update:</p>\n<div class=\"math\">\\begin{eqnarray}\\label{q-learning} \\tag{2}\nQ(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a) - Q(S_t, A_t)]\n\\end{eqnarray}</div>\n<p>The quantity in square brackets in (\\ref{q-learning}) is exactly 0 for the optimal action-value, <span class=\"math\">\\(q*\\)</span>, based on (\\ref{action-value-bellman-optimality}).  We can think of it as an error term, “the Bellman error”, that describes how far off the target quantity <span class=\"math\">\\(R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a)\\)</span> is from our current estimate <span class=\"math\">\\(Q(S_t, A_t)\\)</span>.</p>\n<p>The goal with Q-learning is to iteratively calculate (\\ref{q-learning}), updating our estimate of <span class=\"math\">\\(Q\\)</span> to reduce the Bellman error, until we have converged on a&nbsp;solution.</p>\n<p><strong>Q-learning makes two&nbsp;approximations:</strong></p>\n<p>I. It replaces the expectation value in (\\ref{action-value-bellman-optimality}) with sampled estimates, similar to Monte Carlo estimates.  Unlike the dynamic programming approach we described in an earlier <a href=\"https://efavdb.com/dp-in-rl.html\">post</a>, sampling is necessary since we don&#8217;t have access to the model of the environment, i.e. the environment transition&nbsp;dynamics.</p>\n<p><span class=\"caps\">II</span>. It replaces the target <span class=\"math\">\\(R_{t+1} + \\max_a \\gamma q_*(s’,a’)\\)</span> in (\\ref{action-value-bellman-optimality}), which contains the true action-value function <span class=\"math\">\\(q_*\\)</span>, with the one-step temporal difference, <span class=\"caps\">TD</span>(0), target <span class=\"math\">\\(R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a)\\)</span>.  The <span class=\"caps\">TD</span>(0) target is an example of <em>bootstrapping</em> because it makes use of the current estimate of the action-value function, instead of, say the cumulative rewards from an entire episode, which would be a Monte Carlo target.  Temporal difference methods reduce variance that comes from sampling a single trajectory like Monte Carlo at the cost of introducing bias from using an approximate function in the target for&nbsp;updates.</p>\n<p>Figure 8.11 of [7] nicely summarizes the types of approximations and their limits in the following&nbsp;diagram:</p>\n<p><img alt=\"backup approximations\" src=\"https://efavdb.com/images/backup_limits_diagram_sutton_barto.png\"></p>\n<h2>Deep Q-Networks (<span class=\"caps\">DQN</span>)</h2>\n<h3>Key contributions to&nbsp;Q-learning</h3>\n<p>The <span class=\"caps\">DQN</span> authors made two key enhancements to the original Q-learning algorithm to actually make it&nbsp;work:</p>\n<ol>\n<li>\n<p><strong>Experience replay buffer</strong>: to reduce the instability caused by training on highly correlated sequential data, store samples (transition tuples <span class=\"math\">\\((s, a, s’, r)\\)</span>) in an “experience replay buffer”.  Cut down correlations by randomly sampling the buffer for minibatches of training data.  The idea of experience replay was introduced by <a href=\"http://www.incompleteideas.net/lin-92.pdf\">Lin in 1992</a>.</p>\n</li>\n<li>\n<p><strong>Freeze the target network</strong>: to address the instability caused by chasing a moving target, freeze the target network and only update it periodically with the latest parameters from the trained&nbsp;estimator.</p>\n</li>\n</ol>\n<p>These modifications enabled [1] to successfully train a deep Q-network, an action-value function approximated by a convolutional neural net, on the high dimensional visual inputs of a variety of Atari&nbsp;games.</p>\n<p>The authors also employed a number of tweaks / data preprocessing on top of the aforementioned key enhancements.  One preprocessing trick of note was the concatenation of the four most recent frames as input into the Q-network in order to provide some sense of velocity or trajectory, e.g. the trajectory of a ball in games such as Pong or Breakout.  This preprocessing decision helps uphold the assumption that the problem is a Markov Decision Process, which underlies the Bellman optimality equations and Q-learning algorithms; otherwise, the assumption is violated if the agent only observes some fraction of the state of the environment, turning the problem into a <a href=\"https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process\">partially observable <span class=\"caps\">MDP</span></a>.</p>\n<h3><span class=\"caps\">DQN</span> implementation in&nbsp;code</h3>\n<p>We’ve implemented <span class=\"caps\">DQN</span> <a href=\"https://github.com/frangipane/reinforcement-learning/blob/master/DQN/dqn.py\">here</a>, tested for (1) the <a href=\"https://gym.openai.com/envs/CartPole-v1/\">Cartpole</a> toy problem, which uses a multilayer perceptron <code>MLPCritic</code> as the Q-function approximator for non-visual input data, and (2) Atari Breakout, which uses a convolutional neural network <code>CNNCritic</code> as the Q-function approximator for the (visual) Atari pixel&nbsp;data.</p>\n<p>The Cartpole problem is trainable on the average modern laptop <span class=\"caps\">CPU</span>, but we recommend using a beefier setup with GPUs and lots of memory to do Q-learning on Atari.  Thanks to the OpenAI Scholars program and Microsoft, we were able to train <span class=\"caps\">DQN</span> on Breakout using an Azure <a href=\"https://docs.microsoft.com/en-us/azure/virtual-machines/nc-series\">Standard_NC24</a> consisting of 224 GiB <span class=\"caps\">RAM</span> and 2 K80&nbsp;GPUs.</p>\n<p>The values from the <span class=\"math\">\\(Q\\)</span> estimator and frozen target network are fed into the Huber loss that is used to update the parameters of the Q-function in this code&nbsp;snippet:</p>\n<div class=\"highlight\"><pre><span></span><span class=\"k\">def</span> <span class=\"nf\">compute_loss_q</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">):</span>\n        <span class=\"n\">o</span><span class=\"p\">,</span> <span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">r</span><span class=\"p\">,</span> <span class=\"n\">o2</span><span class=\"p\">,</span> <span class=\"n\">d</span> <span class=\"o\">=</span> <span class=\"n\">data</span><span class=\"p\">[</span><span class=\"s1\">&#39;obs&#39;</span><span class=\"p\">],</span> <span class=\"n\">data</span><span class=\"p\">[</span><span class=\"s1\">&#39;act&#39;</span><span class=\"p\">],</span> <span class=\"n\">data</span><span class=\"p\">[</span><span class=\"s1\">&#39;rew&#39;</span><span class=\"p\">],</span> <span class=\"n\">data</span><span class=\"p\">[</span><span class=\"s1\">&#39;obs2&#39;</span><span class=\"p\">],</span> <span class=\"n\">data</span><span class=\"p\">[</span><span class=\"s1\">&#39;done&#39;</span><span class=\"p\">]</span>\n\n        <span class=\"c1\"># Pick out q-values associated with / indexed by the action that was taken</span>\n        <span class=\"c1\"># for that observation</span>\n        <span class=\"n\">q</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">gather</span><span class=\"p\">(</span><span class=\"n\">ac</span><span class=\"o\">.</span><span class=\"n\">q</span><span class=\"p\">(</span><span class=\"n\">o</span><span class=\"p\">),</span> <span class=\"n\">dim</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">index</span><span class=\"o\">=</span><span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">view</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">long</span><span class=\"p\">())</span>\n\n        <span class=\"c1\"># Bellman backup for Q function</span>\n        <span class=\"k\">with</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">no_grad</span><span class=\"p\">():</span>\n                <span class=\"c1\"># Targets come from frozen target Q-network</span>\n                <span class=\"n\">q_target</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">max</span><span class=\"p\">(</span><span class=\"n\">target_q_network</span><span class=\"p\">(</span><span class=\"n\">o2</span><span class=\"p\">),</span> <span class=\"n\">dim</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">values</span>\n                <span class=\"n\">backup</span> <span class=\"o\">=</span> <span class=\"n\">r</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">d</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">gamma</span> <span class=\"o\">*</span> <span class=\"n\">q_target</span>\n\n        <span class=\"n\">loss_q</span> <span class=\"o\">=</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">smooth_l1_loss</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"p\">[:,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">backup</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">()</span>\n\n        <span class=\"k\">return</span> <span class=\"n\">loss_q</span>\n</pre></div>\n\n\n<p>The experience replay buffer was taken from OpenAI’s Spinning Up in <span class=\"caps\">RL</span> [6] code tutorials for the&nbsp;problem:</p>\n<div class=\"highlight\"><pre><span></span><span class=\"k\">class</span> <span class=\"nc\">ReplayBuffer</span><span class=\"p\">:</span>\n    <span class=\"sd\">&quot;&quot;&quot;</span>\n<span class=\"sd\">    A simple FIFO experience replay buffer for DDPG agents.</span>\n\n<span class=\"sd\">    Copied from: https://github.com/openai/spinningup/blob/master/spinup/algos/pytorch/ddpg/ddpg.py#L11,</span>\n<span class=\"sd\">    modified action buffer for discrete action space.</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">obs_dim</span><span class=\"p\">,</span> <span class=\"n\">act_dim</span><span class=\"p\">,</span> <span class=\"n\">size</span><span class=\"p\">):</span>\n        <span class=\"o\">...</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">store</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">obs</span><span class=\"p\">,</span> <span class=\"n\">act</span><span class=\"p\">,</span> <span class=\"n\">rew</span><span class=\"p\">,</span> <span class=\"n\">next_obs</span><span class=\"p\">,</span> <span class=\"n\">done</span><span class=\"p\">):</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">obs_buf</span><span class=\"p\">[</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">ptr</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">obs</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">obs2_buf</span><span class=\"p\">[</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">ptr</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">next_obs</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">act_buf</span><span class=\"p\">[</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">ptr</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">act</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">rew_buf</span><span class=\"p\">[</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">ptr</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">rew</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">done_buf</span><span class=\"p\">[</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">ptr</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">done</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">ptr</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">ptr</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">%</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">max_size</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">size</span> <span class=\"o\">=</span> <span class=\"nb\">min</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">max_size</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">sample_batch</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">32</span><span class=\"p\">):</span>\n        <span class=\"n\">idxs</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">choice</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">,</span> <span class=\"n\">size</span><span class=\"o\">=</span><span class=\"n\">batch_size</span><span class=\"p\">,</span> <span class=\"n\">replace</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n        <span class=\"n\">batch</span> <span class=\"o\">=</span> <span class=\"nb\">dict</span><span class=\"p\">(</span><span class=\"n\">obs</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">obs_buf</span><span class=\"p\">[</span><span class=\"n\">idxs</span><span class=\"p\">],</span>\n                     <span class=\"n\">obs2</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">obs2_buf</span><span class=\"p\">[</span><span class=\"n\">idxs</span><span class=\"p\">],</span>\n                     <span class=\"n\">act</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">act_buf</span><span class=\"p\">[</span><span class=\"n\">idxs</span><span class=\"p\">],</span>\n                     <span class=\"n\">rew</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">rew_buf</span><span class=\"p\">[</span><span class=\"n\">idxs</span><span class=\"p\">],</span>\n                     <span class=\"n\">done</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">done_buf</span><span class=\"p\">[</span><span class=\"n\">idxs</span><span class=\"p\">])</span>\n        <span class=\"k\">return</span> <span class=\"p\">{</span><span class=\"n\">k</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">as_tensor</span><span class=\"p\">(</span><span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">int32</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"p\">)</span> <span class=\"k\">if</span> <span class=\"n\">k</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;act&#39;</span>\n                <span class=\"k\">else</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">as_tensor</span><span class=\"p\">(</span><span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"p\">)</span> \n                <span class=\"k\">for</span> <span class=\"n\">k</span><span class=\"p\">,</span><span class=\"n\">v</span> <span class=\"ow\">in</span> <span class=\"n\">batch</span><span class=\"o\">.</span><span class=\"n\">items</span><span class=\"p\">()}</span>\n</pre></div>\n\n\n<p>Finally, we used OpenAI’s baselines <a href=\"https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py\">Atari wrappers</a> to handle the rather involved data preprocessing&nbsp;steps.</p>\n<p>You can see logs and plots like this plot of the mean raw returns per step in the environment for the Atari <span class=\"caps\">DQN</span> training run in our <a href=\"https://app.wandb.ai/frangipane/dqn/runs/30fhfv6y?workspace=user-frangipane\">wandb dashboard</a>.</p>\n<p><img alt=\"training curve\" src=\"https://efavdb.com/images/atari_training_returns.png\"></p>\n<h2>Conclusion</h2>\n<p>From a pedagogical point of view, Q-learning is a good study for someone getting off the ground with <span class=\"caps\">RL</span> since it pulls together many core <span class=\"caps\">RL</span> concepts,&nbsp;namely:</p>\n<ol>\n<li>Model the sequential decision making process as an <strong><span class=\"caps\">MDP</span></strong> where environment dynamics are&nbsp;unknown.</li>\n<li>Frame the problem as finding <strong>action-value functions</strong> that satisfy the Bellman&nbsp;equations.</li>\n<li>Iteratively solve the Bellman equations using <strong>bootstrapped  estimates</strong> from samples of an agent’s interactions with an&nbsp;environment.</li>\n<li>Use neural networks to <strong>approximate value functions</strong> to handle the more realistic situation of an observation space being too high-dimensional to be stored in&nbsp;table.</li>\n</ol>\n<p><span class=\"caps\">DQN</span> on top of vanilla Q-learning itself is noteworthy because the modifications &#8212; experience replay and frozen target networks &#8212; are what make Q-learning actually work, demonstrating that the devil is in the&nbsp;details.</p>\n<p>Furthermore, the <span class=\"caps\">DQN</span> tricks have been incorporated in many other <span class=\"caps\">RL</span> algorithms, e.g. see [6] for more examples.  The tricks aren’t necessarily “pretty”, but they come from understanding/intuition about shortcomings of the basic&nbsp;algorithms.</p>\n<h2>References</h2>\n<p><strong>Papers</strong></p>\n<ul>\n<li>[1] Mnih et al 2015 - <a href=\"https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#the-optimal-q-function-and-the-optimal-action\">Human-level control through deep reinforcement&nbsp;learning</a></li>\n</ul>\n<p><strong>Video&nbsp;lectures</strong></p>\n<ul>\n<li>[2] David Silver - <span class=\"caps\">RL</span> lecture 6 Value Function Approximation (<a href=\"https://www.youtube.com/watch?v=UoPei5o4fps\">video</a>, <a href=\"https://www.davidsilver.uk/wp-content/uploads/2020/03/FA.pdf\">slides</a>)</li>\n<li>[3] Sergey Levine’s lecture (<span class=\"caps\">CS285</span>) on value function methods (<a href=\"https://www.youtube.com/watch?v=doR5bMe-Wic&amp;list=PLkFD6_40KJIwhWJpGazJ9VSj9CFMkb79A&amp;index=8&amp;t=129s\">video</a>, <a href=\"http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-7.pdf\">slides</a>)</li>\n<li>[4] Sergey Levine’s lecture (<span class=\"caps\">CS285</span>) on deep <span class=\"caps\">RL</span> with Q-functions (<a href=\"https://www.youtube.com/watch?v=7Lwf-BoIu3M&amp;list=PLkFD6_40KJIwhWJpGazJ9VSj9CFMkb79A&amp;index=9&amp;t=0s\">video</a>, <a href=\"http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-8.pdf\">slides</a>)</li>\n<li>[5] Vlad Mnih - Berkeley Deep <span class=\"caps\">RL</span> Bootcamp 2017 - Core Lecture 3 <span class=\"caps\">DQN</span> + Variants (<a href=\"https://www.youtube.com/watch?v=fevMOp5TDQs\">video</a>, <a href=\"https://drive.google.com/open?id=0BxXI_RttTZAhVUhpbDhiSUFFNjg\">slides</a>)</li>\n</ul>\n<p><strong>Books /&nbsp;tutorials</strong></p>\n<ul>\n<li>[6] OpenAI - Spinning Up: <a href=\"https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#the-optimal-q-function-and-the-optimal-action\">The Optimal Q-Function and the Optimal&nbsp;Action</a></li>\n<li>[7] Sutton and Barto - <a href=\"http://incompleteideas.net/book/RLbook2018.pdf\">Reinforcement Learning: An Introduction (2nd Edition)</a>, section 6.5 “Q-learning: Off-policy <span class=\"caps\">TD</span> Control”, section 16.5 “Human-level Video Game&nbsp;Play”</li>\n</ul>\n<script type=\"text/javascript\">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}\n</script>",
  "category": [
    "",
    "",
    "",
    ""
  ]
}