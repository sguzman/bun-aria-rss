{
  "title": "Probabilistic and Bayesian Matrix Factorizations for Text Clustering",
  "link": "",
  "id": "https://www.georgeho.org/matrix-factorizations/",
  "updated": "2018-10-13T00:00:00Z",
  "published": "2018-10-13T00:00:00Z",
  "content": "<p>Natural language processing is in a curious place right now. It was always a\nlate bloomer (as far as machine learning subfields go), and it&rsquo;s not immediately\nobvious how close the field is to viable, large-scale, production-ready\ntechniques (in the same way that, say, <a href=\"https://clarifai.com/models/\">computer vision\nis</a>). For example, <a href=\"https://ruder.io\">Sebastian\nRuder</a> predicted that the field is <a href=\"https://thegradient.pub/nlp-imagenet/\">close to a watershed\nmoment</a>, and that soon we&rsquo;ll have\ndownloadable language models. However, <a href=\"https://thegradient.pub/author/ana/\">Ana\nMarasović</a> points out that there is <a href=\"https://thegradient.pub/frontiers-of-generalization-in-natural-language-processing/\">a\ntremendous amount of work demonstrating\nthat</a>:</p>\n<blockquote>\n<p>“despite good performance on benchmark datasets, modern NLP techniques are\nnowhere near the skill of humans at language understanding and reasoning when\nmaking sense of novel natural language inputs”.</p>\n</blockquote>\n<p>I am confident that I am <em>very</em> bad at making lofty predictions about the\nfuture. Instead, I&rsquo;ll talk about something I know a bit about: simple solutions\nto concrete problems, with some Bayesianism thrown in for good measure!</p>\n<p>This blog post summarizes some literature on probabilistic and Bayesian\nmatrix factorization methods, keeping an eye out for applications to one\nspecific task in NLP: text clustering. It&rsquo;s exactly what it sounds like, and\nthere&rsquo;s been a fair amount of success in applying text clustering to many other\nNLP tasks (e.g. check out these examples in <a href=\"https://www-users.cs.umn.edu/~hanxx023/dmclass/scatter.pdf\">document\norganization</a>,\n<a href=\"http://jmlr.csail.mit.edu/papers/volume3/bekkerman03a/bekkerman03a.pdf\">corpus</a>\n<a href=\"https://www.cs.technion.ac.il/~rani/el-yaniv-papers/BekkermanETW01.pdf\">summarization</a>\nand <a href=\"http://www.kamalnigam.com/papers/emcat-aaai98.pdf\">document\nclassification</a>).</p>\n<p>What follows is a literature review of three matrix factorization techniques for\nmachine learning: one classical, one probabilistic and one Bayesian. I also\nexperimented with applying these methods to text clustering: I gave a guest\nlecture on my results to a graduate-level machine learning class at The Cooper\nUnion (the slide deck is below). Dive in!</p>\n<h2 id=\"non-negative-matrix-factorization-nmf\">Non-Negative Matrix Factorization (NMF)</h2>\n<p>NMF is a <a href=\"https://en.wikipedia.org/wiki/Non-negative_matrix_factorization\">very\nwell-known</a>\n<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html\">matrix\nfactorization</a>\n<a href=\"https://arxiv.org/abs/1401.5226\">technique</a>, perhaps most famous for its\napplications in <a href=\"http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/\">collaborative filtering and the Netflix\nPrize</a>.</p>\n<p>Factorize your (entrywise non-negative) $m \\times n$ matrix $V$ as\n$V = WH$, where $W$ is $m \\times p$ and $H$ is $p \\times n$. $p$\nis the dimensionality of your latent space, and each latent dimension usually\ncomes to quantify something with semantic meaning. There are several algorithms\nto compute this factorization, but Lee and Seung&rsquo;s <a href=\"https://dl.acm.org/citation.cfm?id=3008829\">multiplicative update\nrule</a> (originally published in NIPS\n2000) is most popular.</p>\n<p>Fairly simple: enough said, I think.</p>\n<h2 id=\"probabilistic-matrix-factorization-pmf\">Probabilistic Matrix Factorization (PMF)</h2>\n<p>Originally introduced as a paper at <a href=\"https://papers.nips.cc/paper/3208-probabilistic-matrix-factorization\">NIPS\n2007</a>,\n<em>probabilistic matrix factorization</em> is essentially the exact same model as NMF,\nbut with uncorrelated (a.k.a. “spherical”) multivariate Gaussian priors placed\non the rows and columns of $U$ and $V$. Expressed as a graphical model, PMF\nwould look like this:</p>\n<figure>\n<a href=\"https://www.georgeho.org/assets/images/pmf.png\"><img style=\"float: middle\" src=\"https://www.georgeho.org/assets/images/pmf.png\" alt=\"Graphical model (using plate notation) for probabilistic matrix factorization (PMF)\"></a>\n</figure>\n<p>Note that the priors are placed on the <em>rows</em> of the $U$ and $V$ matrices.</p>\n<p>The authors then (somewhat disappointing) proceed to find the MAP estimate of\nthe $U$ and $V$ matrices. They show that maximizing the posterior is\nequivalent to minimizing the sum-of-squared-errors loss function with two\nquadratic regularization terms:</p>\n<p>$$\n\\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{M} {I_{ij} (R_{ij} - U_i^T V_j)^2} +\n\\frac{\\lambda_U}{2} \\sum_{i=1}^{N} |U|_{Fro}^2 +\n\\frac{\\lambda_V}{2} \\sum_{j=1}^{M} |V|_{Fro}^2\n$$</p>\n<p>where $|\\cdot|_{Fro}$ denotes the Frobenius norm, and $I_{ij}$ is 1 if document\n$i$ contains word $j$, and 0 otherwise.</p>\n<p>This loss function can be minimized via gradient descent, and implemented in\nyour favorite deep learning framework (e.g. Tensorflow or PyTorch).</p>\n<p>The problem with this approach is that while the MAP estimate is often a\nreasonable point in low dimensions, it becomes very strange in high dimensions,\nand is usually not informative or special in any way. Read <a href=\"https://www.inference.vc/high-dimensional-gaussian-distributions-are-soap-bubble/\">Ferenc Huszár’s blog\npost</a>\nfor more.</p>\n<h2 id=\"bayesian-probabilistic-matrix-factorization-bpmf\">Bayesian Probabilistic Matrix Factorization (BPMF)</h2>\n<p>Strictly speaking, PMF is not a Bayesian model. After all, there aren&rsquo;t any\npriors or posteriors, only fixed hyperparameters and a MAP estimate. <em>Bayesian\nprobabilistic matrix factorization</em>, originally published by <a href=\"https://dl.acm.org/citation.cfm?id=1390267\">researchers from\nthe University of Toronto</a> is a\nfully Bayesian treatment of PMF.</p>\n<p>Instead of saying that the rows/columns of U and V are normally distributed with\nzero mean and some precision matrix, we place hyperpriors on the mean vector and\nprecision matrices. The specific priors are Wishart priors on the covariance\nmatrices (with scale matrix $W_0$ and $\\nu_0$ degrees of freedom), and\nGaussian priors on the means (with mean $\\mu_0$ and covariance equal to the\ncovariance given by the Wishart prior). Expressed as a graphical model, BPMF\nwould look like this:</p>\n<figure>\n<a href=\"https://www.georgeho.org/assets/images/bpmf.png\"><img style=\"float: middle\" src=\"https://www.georgeho.org/assets/images/bpmf.png\" alt=\"Graphical model (using plate notation) for Bayesian probabilistic matrix factorization (BPMF)\"></a>\n</figure>\n<p>Note that, as above, the priors are placed on the <em>rows</em> of the $U$ and $V$\nmatrices, and that $n$ is the dimensionality of latent space (i.e. the number\nof latent dimensions in the factorization).</p>\n<p>The authors then sample from the posterior distribution of $U$ and $V$ using\na Gibbs sampler. Sampling takes several hours: somewhere between 5 to 180,\ndepending on how many samples you want. Nevertheless, the authors demonstrate\nthat BPMF can achieve more accurate and more robust results on the Netflix data\nset.</p>\n<p>I would propose two changes to the original paper:</p>\n<ol>\n<li>Use an LKJ prior on the covariance matrices instead of a Wishart prior.\n<a href=\"https://docs.pymc.io/notebooks/LKJ.html\">According to Michael Betancourt and the PyMC3 docs, this is more numerically\nstable</a>, and will lead to better\ninference.</li>\n<li>Use a more robust sampler such as NUTS (instead of a Gibbs sampler), or even\nresort to variational inference. The paper makes it clear that BPMF is a\ncomputationally painful endeavor, so any speedup to the method would be a\ngreat help. It seems to me that for practical real-world applications to\ncollaborative filtering, we would want to use variational inference. Netflix\nain&rsquo;t waiting 5 hours for their recommendations.</li>\n</ol>\n<h2 id=\"application-to-text-clustering\">Application to Text Clustering</h2>\n<p>Most of the work in these matrix factorization techniques focus on\ndimensionality reduction: that is, the problem of finding two factor matrices\nthat faithfully reconstruct the original matrix when multiplied together.\nHowever, I was interested in applying the exact same techniques to a separate\ntask: text clustering.</p>\n<p>A natural question is: why is matrix factorization<sup id=\"fnref:1\"><a href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\">1</a></sup> a good technique to use\nfor text clustering? Because it is simultaneously a clustering and a feature\nengineering technique: not only does it offer us a latent representation of the\noriginal data, but it also gives us a way to easily <em>reconstruct</em> the original\ndata from the latent variables! This is something that <a href=\"https://www.georgeho.org/lda-sucks\">latent Dirichlet\nallocation</a>, for instance, cannot do.</p>\n<p>Matrix factorization lives an interesting double life: clustering technique by\nday, feature transformation technique by night. <a href=\"http://charuaggarwal.net/text-cluster.pdf\">Aggarwal and\nZhai</a> suggest that chaining matrix\nfactorization with some other clustering technique (e.g. agglomerative\nclustering or topic modelling) is common practice and is called <em>concept\ndecomposition</em>, but I haven&rsquo;t seen any other source back this up.</p>\n<p>I experimented with using these techniques to cluster subreddits (<a href=\"https://www.georgeho.org/reddit-clusters\">sound\nfamiliar?</a>). In a nutshell, nothing seemed\nto work out very well, and I opine on why I think that&rsquo;s the case in the slide\ndeck below. This talk was delivered to a graduate-level course in frequentist\nmachine learning.</p>\n<blockquote class=\"embedly-card\"><h4><a href=\"https://speakerdeck.com/_eigenfoo/probabilistic-and-bayesian-matrix-factorizations-for-text-clustering\">Probabilistic and Bayesian Matrix Factorizations for Text Clustering</a></h4><p> I experimented with using these techniques to cluster subreddits. In a nutshell, nothing seemed to work out very well, and I opine on why I think that’s the case in this slide deck. This talk was delivered to a graduate-level course in frequentist machine learning. </p></blockquote>\n<script async src=\"//cdn.embedly.com/widgets/platform.js\" charset=\"UTF-8\"></script>\n<div class=\"footnotes\" role=\"doc-endnotes\">\n<hr>\n<ol>\n<li id=\"fn:1\">\n<p>which is, by the way, a <a href=\"http://scikit-learn.org/stable/modules/decomposition.html\">severely underappreciated technique in machine\nlearning</a>&#160;<a href=\"#fnref:1\" class=\"footnote-backref\" role=\"doc-backlink\">&#x21a9;&#xfe0e;</a></p>\n</li>\n</ol>\n</div>"
}