{
  "title": "My favourite papers from day one of ICML 2015",
  "link": "",
  "updated": "2015-07-07T00:00:00+00:00",
  "id": "http://daoudclarke.github.com/machine%20learning%20in%20practice/2015/07/07/icml2015-favourite-papers-day1",
  "content": "<p>Aargh! How can I possibly keep all the amazing things I learnt at ICML\ntoday in my head?! Clearly I can’t. This is a list of pointers to my\nfavourite papers from today, and why I think they are cool. This is\nmainly for my benefit, but you might like them too!</p>\n\n<h2 id=\"neural-nets--deep-learning\">Neural Nets / Deep Learning</h2>\n\n<h3 id=\"bilbowa-fast-bilingual-distributed-representations-without-word-alignments\"><a href=\"http://jmlr.org/proceedings/papers/v37/gouws15.pdf\">BilBOWA: Fast Bilingual Distributed Representations without Word Alignments</a></h3>\n\n<p><em>Stephan Gouws, Yoshua Bengio, Greg Corrado</em></p>\n\n<p><strong>Why this paper is cool:</strong> It simultaneously learns word vectors for\n  words in two languages without having to learn a mapping between\n  them.</p>\n\n<h3 id=\"compressing-neural-networks-with-the-hashing-trick\"><a href=\"http://jmlr.org/proceedings/papers/v37/chenc15.pdf\">Compressing Neural Networks with the Hashing Trick</a></h3>\n\n<p><em>Wenlin Chen, James Wilson, Stephen Tyree, Kilian Weinberger, Yixin Chen</em></p>\n\n<p><strong>Why this paper is cool:</strong> Gives a huge reduction (32x) in the amount\n  of memory needed to store a neural network. This means you can\n  potentially use it on low memory devices like mobile phones!</p>\n\n<h3 id=\"batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift\"><a href=\"http://jmlr.org/proceedings/papers/v37/ioffe15.pdf\">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></h3>\n\n<p><em>Sergey Ioffe, Christian Szegedy</em></p>\n\n<p><strong>Why this paper is cool:</strong> Makes deep neural network training super\n  fast, giving a new state of the art for some datasets.</p>\n\n<h3 id=\"deep-learning-with-limited-numerical-precision\"><a href=\"http://jmlr.org/proceedings/papers/v37/gupta15.pdf\">Deep Learning with Limited Numerical Precision</a></h3>\n\n<p><em>Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, Pritish Narayanan</em></p>\n\n<p><strong>Why this paper is cool:</strong> Train neural networks with very limited\n  fixed precision arithmetic instead of floating points. The key\n  insight is to use randomness to do the rounding. The goal is to\n  eventually build custom hardware to make learning much faster.</p>\n\n<h2 id=\"recommendations-etc\">Recommendations etc.</h2>\n\n<h3 id=\"fixed-point-algorithms-for-learning-determinantal-point-processes\"><a href=\"http://jmlr.org/proceedings/papers/v37/mariet15.pdf\">Fixed-point algorithms for learning determinantal point processes</a></h3>\n\n<p><em>Zelda Mariet, Suvrit Sra</em></p>\n\n<p><strong>Why this paper is cool</strong> If you want to recommend a set of things,\n  rather than just an individual thing, how do you choose the best\n  set? This will tell you.</p>\n\n<h3 id=\"surrogate-functions-for-maximizing-precision-at-the-top\"><a href=\"http://jmlr.org/proceedings/papers/v37/kar15.pdf\">Surrogate Functions for Maximizing Precision at the Top</a></h3>\n\n<p><strong>Why this paper is cool:</strong> If you only care about the top <em>n</em> things\n  you recommend, this technique works faster and better than other\n  approaches.</p>\n\n<p><em>Purushottam Kar, Harikrishna Narasimhan, Prateek Jain</em></p>\n\n<h2 id=\"and-finally\">And Finally…</h2>\n\n<h3 id=\"learning-to-search-better-than-your-teacher\"><a href=\"http://jmlr.org/proceedings/papers/v37/changb15.pdf\">Learning to Search Better than Your Teacher</a></h3>\n\n<p><em>Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agarwal, Hal Daume, John Langford</em></p>\n\n<p><strong>Why this paper is cool:</strong> A new, general way to do structured\n  prediction (tasks like dependency parsing or semantic parsing) which\n  works well even when there are errors in the training set. Thanks to\n  the authors for talking me through this one!</p>"
}