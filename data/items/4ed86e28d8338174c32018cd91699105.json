{
  "title": "Asynchronous Scraping with Python",
  "link": "",
  "published": "2016-10-16T00:00:00-07:00",
  "updated": "2022-11-03T04:16:07-07:00",
  "author": {
    "name": "Greg Reda"
  },
  "id": "tag:www.gregreda.com,2016-10-16:/2016/10/16/asynchronous-scraping-with-python/",
  "summary": "<p><em>This is part of a series of posts I have written about web scraping with Python.</em></p>\n<ol>\n<li><a href=\"http://www.gregreda.com/2013/03/03/web-scraping-101-with-python/\">Web Scraping 101 with Python</a>, which covers the basics of using Python for web scraping.</li>\n<li><a href=\"http://www.gregreda.com/2015/02/15/web-scraping-finding-the-api/\">Web Scraping 201: Finding the API</a>, which covers when sites load data client-side with Javascript.</li>\n<li><a href=\"http://www.gregreda.com/2016/10/16/asynchronous-scraping-with-python/\">Asynchronous Scraping with Python â€¦</a></li></ol>",
  "content": "<p><em>This is part of a series of posts I have written about web scraping with Python.</em></p>\n<ol>\n<li><a href=\"http://www.gregreda.com/2013/03/03/web-scraping-101-with-python/\">Web Scraping 101 with Python</a>, which covers the basics of using Python for web scraping.</li>\n<li><a href=\"http://www.gregreda.com/2015/02/15/web-scraping-finding-the-api/\">Web Scraping 201: Finding the API</a>, which covers when sites load data client-side with Javascript.</li>\n<li><a href=\"http://www.gregreda.com/2016/10/16/asynchronous-scraping-with-python/\">Asynchronous Scraping with Python</a>, showing how to use multithreading to speed things up.</li>\n<li><a href=\"http://www.gregreda.com/2020/11/17/scraping-pages-behind-login-forms/\">Scraping Pages Behind Login Forms</a>, which shows how to log into sites using Python.</li>\n</ol>\n<hr>\n\n<p>Previously, I've written about the <a href=\"http://www.gregreda.com/2013/03/03/web-scraping-101-with-python/\">basics of scraping</a> and how you can <a href=\"http://www.gregreda.com/2015/02/15/web-scraping-finding-the-api/\">find API calls</a> in order to fetch data that isn't easily downloadable.</p>\n<p>For simplicity, the code in these posts has always been synchronous -- given a list of URLs, we process one, then the next, then the next, and so on. While this makes for code that's straight-forward, it can also be slow.</p>\n<p>This doesn't have to be the case though. Scraping is often an example of code that is <a href=\"https://en.wikipedia.org/wiki/Embarrassingly_parallel\">embarrassingly parallel</a>. With some slight changes, our tasks can be done asynchronously, allowing us to process more than one URL at a time.</p>\n<p>In version 3.2, Python introduced the <a href=\"https://docs.python.org/3/library/concurrent.futures.html\"><code>concurrent.futures</code></a> module, which is a joy to use for parallelizing tasks like scraping. The rest of this post will show how we can use the module to make our previously synchronous code asynchronous.</p>\n<h3>Parallelizing your tasks</h3>\n<p>Imagine we have a list of several thousand URLs. In previous posts, we've always written something that looks like this:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">csv</span> <span class=\"kn\">import</span> <span class=\"n\">DictWriter</span>\n\n<span class=\"n\">URLS</span> <span class=\"o\">=</span> <span class=\"p\">[</span> <span class=\"o\">...</span> <span class=\"p\">]</span>  <span class=\"c1\"># thousands of urls for pages we&#39;d like to parse</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">parse</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">):</span>\n    <span class=\"c1\"># our logic for parsing the page</span>\n    <span class=\"k\">return</span> <span class=\"n\">data</span>  <span class=\"c1\"># probably a dict</span>\n\n<span class=\"n\">results</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"k\">for</span> <span class=\"n\">url</span> <span class=\"ow\">in</span> <span class=\"n\">URLS</span><span class=\"p\">:</span>  <span class=\"c1\"># go through each url one by one</span>\n    <span class=\"n\">results</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">parse</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">))</span>\n\n<span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s1\">&#39;results.csv&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;w&#39;</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n    <span class=\"n\">writer</span> <span class=\"o\">=</span> <span class=\"n\">DictWriter</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">,</span> <span class=\"n\">fieldnames</span><span class=\"o\">=</span><span class=\"n\">results</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">keys</span><span class=\"p\">())</span>\n    <span class=\"n\">writer</span><span class=\"o\">.</span><span class=\"n\">writeheader</span><span class=\"p\">()</span>\n    <span class=\"n\">writer</span><span class=\"o\">.</span><span class=\"n\">writerows</span><span class=\"p\">(</span><span class=\"n\">results</span><span class=\"p\">)</span>\n</code></pre></div>\n\n<p>The above is an example of synchronous code -- we're looping through a list of URLs, processing one at a time. If the list of URLs is relatively small or we're not concerned about execution time, there's little reason to <a href=\"https://en.wikipedia.org/wiki/Task_parallelism\">parallelize</a> these tasks -- we might as well keep things simple and wait it out.</p>\n<p>However, sometimes we have a huge list of URLs -- at least several thousand -- and we can't wait hours for them to finish.</p>\n<p>With <code>concurrent.futures</code>, we can work on multiple URLs at once by adding a <code>ProcessPoolExecutor</code> and making a slight change to how we fetch our results.</p>\n<p>But first, a reminder: <em>if you're scraping, don't be a jerk</em>. Space out your requests appropriately and don't hammer the site (i.e. use <code>time.sleep</code> to wait briefly between each request and set <code>max_workers</code> to a small number). Being a jerk runs the risk of getting your IP address blocked -- good luck getting that data now.</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">concurrent.futures</span> <span class=\"kn\">import</span> <span class=\"n\">ProcessPoolExecutor</span>\n<span class=\"kn\">import</span> <span class=\"nn\">concurrent.futures</span>\n\n<span class=\"n\">URLS</span> <span class=\"o\">=</span> <span class=\"p\">[</span> <span class=\"o\">...</span> <span class=\"p\">]</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">parse</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">):</span>\n    <span class=\"c1\"># our logic for parsing the page</span>\n    <span class=\"k\">return</span> <span class=\"n\">data</span>  <span class=\"c1\"># still probably a dict</span>\n\n<span class=\"k\">with</span> <span class=\"n\">ProcessPoolExecutor</span><span class=\"p\">(</span><span class=\"n\">max_workers</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">executor</span><span class=\"p\">:</span>\n    <span class=\"n\">future_results</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"n\">executor</span><span class=\"o\">.</span><span class=\"n\">submit</span><span class=\"p\">(</span><span class=\"n\">parse</span><span class=\"p\">,</span> <span class=\"n\">url</span><span class=\"p\">):</span> <span class=\"n\">url</span> <span class=\"k\">for</span> <span class=\"n\">url</span> <span class=\"ow\">in</span> <span class=\"n\">URLS</span><span class=\"p\">}</span>\n\n    <span class=\"n\">results</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"k\">for</span> <span class=\"n\">future</span> <span class=\"ow\">in</span> <span class=\"n\">concurrent</span><span class=\"o\">.</span><span class=\"n\">futures</span><span class=\"o\">.</span><span class=\"n\">as_completed</span><span class=\"p\">(</span><span class=\"n\">future_results</span><span class=\"p\">):</span>\n        <span class=\"n\">results</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">future</span><span class=\"o\">.</span><span class=\"n\">result</span><span class=\"p\">())</span>\n</code></pre></div>\n\n<p>In the above code, we're submitting tasks to the executor -- four workers -- each of which will execute the <code>parse</code> function against a URL. This execution does not happen immediately. For each submission, the executor returns an instance of a <code>Future</code>, which tells us that our task will be executed at some point in the ... well, future. The <code>as_completed</code> function watches our <code>future_results</code> for completion, upon which we'll be able to fetch each result via the <code>result</code> method.</p>\n<p>My favorite part about this module is the clarity of its API -- tasks are <em>submitted</em> to an <em>executor</em>, which is made up of one or more workers, each of which is churning through our tasks. Because our tasks are executed asynchronously, we are not waiting for a given task's completion before submitting another -- we are doing so at-will, with completion happening in the <em>future</em>. Once completed, we can get the task's <em>result</em>.</p>\n<h3>Closing up</h3>\n<p>With a few changes to your code and some <code>concurrent.futures</code> love, you no longer have to fetch those basketball stats one page at a time.</p>\n<p>But don't be a jerk either.</p>",
  "category": [
    "",
    "",
    "",
    ""
  ]
}