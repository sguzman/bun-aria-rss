{
  "title": "Bayesian P-Values",
  "link": "",
  "published": "2015-06-27T00:15:00+01:00",
  "updated": "2015-06-27T00:15:00+01:00",
  "author": {
    "name": "Sebastian Nowozin"
  },
  "id": "tag:www.nowozin.net,2015-06-27:/sebastian/blog/bayesian-p-values.html",
  "summary": "<p><a href=\"https://en.wikipedia.org/wiki/P-value\">P-Values</a> (see also <a href=\"https://stat.duke.edu/~berger/p-values.html\">Jim Berger's page\non p-values</a>) are probably one of\nthe most misunderstood concepts in statistics and certainly have been abused\nin statistical practice.\nOriginally proposed as an informal diagnostic by <a href=\"http://en.wikipedia.org/wiki/Ronald_Fisher\">Ronald Fisher</a>, there are many\nreasons for â€¦</p>",
  "content": "<p><a href=\"https://en.wikipedia.org/wiki/P-value\">P-Values</a> (see also <a href=\"https://stat.duke.edu/~berger/p-values.html\">Jim Berger's page\non p-values</a>) are probably one of\nthe most misunderstood concepts in statistics and certainly have been abused\nin statistical practice.\nOriginally proposed as an informal diagnostic by <a href=\"http://en.wikipedia.org/wiki/Ronald_Fisher\">Ronald Fisher</a>, there are many\nreasons for the bad reputation of p-values, and in many relevant situations\ngood alternatives such as <a href=\"http://amstat.tandfonline.com/doi/abs/10.1080/01621459.1995.10476572\">Bayes\nfactors</a>\ncan and should be used instead.\nOne key objection to p-values is that although they provide statistical\nevidence against an assumed hypothesis, this does not imply that the deviation\nfrom the hypothesis is large or relevant.\nIn practice, the largest criticisms are not related to the p-value itself but\nrelated to the widespread misunderstanding of p-values and the arbitrariness\nof accepting formal tests of significance based on p-values in\nscientific discourse.</p>\n<p>In this article I am not going to defend p-values, also because others have\ndone a good job at giving a modern explanation of their benefits in context,\nas well as refuting some common criticisms, for example the article <a href=\"http://www.dcscience.net/Senn-Two-cheers-2001.pdf\">Two\ncheers for P-values?</a> by\n<a href=\"http://www.gla.ac.uk/schools/mathematicsstatistics/staff/stephensenn/\">Stephen\nSenn</a>\nand the more recent <a href=\"http://science.oregonstate.edu/~murtaugh/files/ecol-2014.pdf\">In defense of P\nvalues</a> by <a href=\"http://stat.oregonstate.edu/content/murtaugh-paul\">Paul\nMurtaugh</a>.</p>\n<p>Instead, I will consider a situation which often arises in practice.</p>\n<h2>Setup</h2>\n<p>Suppose you have decided on a probabilistic model <span class=\"math\">\\(P(X)\\)</span> or <span class=\"math\">\\(P(X|\\theta)\\)</span>,\nwhere <span class=\"math\">\\(\\theta\\)</span> is some unknown parameter.\nWith <em>decided</em> I mean that we actually commit and ship our model and we no\nlonger entertain alternative models.\nAlternatives could be too expensive computationally or it could be too\ndifficult to accurately specify these alternative models.  (For example, a\nmore complicated model may involve additional latent variables for which it is\ndifficult to elicit prior beliefs.)</p>\n<p>Given such a model but no assumed alternative, and some observed data <span class=\"math\">\\(X\\)</span>, can\nwe identify whether \"the model fits the data\"?\nThis problem is the classic <a href=\"https://en.wikipedia.org/wiki/Goodness_of_fit\">goodness of\nfit</a> problem and classical\nstatistics has a repertoire of methods for standard models.  These methods\nhave their own problems in that they are often unsatisfactory case-by-case\nstudies or strong results are obtained only in asymptotia.\nHowever, it would be too easy to just criticise these methods.\nThe real question is whether the problem they address is an important one, and\nwhat alternatives should be used, especially from a Bayesian viewpoint.</p>\n<h2>Prediction versus Scientific Theories</h2>\n<p>In <em>machine learning</em>, at least in its widespread current industrial use, we\nare most often concerned with building predictive models that automatically\nmake decisions such as showing the right advertisements, classifying spam\nemails, etcetera.</p>\n<p>This current focus on prediction may shift in the future, for example due to a\nrevival in artificial intelligence systems or in general more autonomous agent\ntype systems which do not have a single clearly defined prediction task.</p>\n<p>But as it currently stands, model checking and goodness of fit is not so\nrelevant for building predictive models.</p>\n<p><em>First</em>, even when the observation does not comply with model assumptions, your\nprediction may still be correct, in which case the non-compliance does not\nmatter.  I.e. the p-value does not use a decision-theoretic viewpoint that\nincludes a task-dependent utility; cf. <a href=\"http://arxiv.org/abs/1402.6118\">Watson and\nHolmes</a>.\nTo know whether the model is \"correct\" or not may not be important at all for\nprediction, but even likewise within science, as summarized by Bruce Hill in\n<a href=\"http://www3.stat.sinica.edu.tw/statistica/oldpdf/A6n41.pdf\">this comment</a>,</p>\n<blockquote>\n<p>\"A major defect of the classical view of hypothesis testing, [...],\nis that it attempts to test only whether the model is true.\nThis came out of the tradition in physics, where models such as Newtonian\nmechanics, the gas laws, fluid dynamics, and so on, come so close to being\n\"true\" in the sense of fitting (much of) the data, that one tends to neglect\nissues about the use of the model. However, in typical statistical problems\n(especially in the biological and social sciences but not exclusively so)\none is almost certain a priori that the model taken literally is false in a\nnon-trivial way, and so one is instead concerned whether the magnitude of\ndiscrepancies is sufficiently small so that the model can be employed for\nsome specific purpose.\"</p>\n</blockquote>\n<p><em>Second</em>, if the deviation from modelling assumptions leads to incorrect\npredictions you would detect this through simple analysis of incorrect\npredictions using ground truth holdout data, not through fancy model checking.\nChecking accuracy of predictions is easy with annotated ground truth data, and\nis the bread-and-butter basic tool of machine learning.</p>\n<p>The only useful application of model checking for predictive systems that I\ncould think of are systems in which a conservative \"prefer-not-to-predict\"\noption exists, so that observations which are violating model assumptions\ncould be excluded from further automated processing.  Yet, much of this\npotential benefit may already be accessible through posterior uncertainty of\nthe model.  Only the subset of instances for which the model is certain but\nits predictions are wrong could profit from this special treatment.</p>\n<p>In contrast to prediction, in science we build models not purely for\nprediction, but as a formal approximation to reality.  Here I see that model\nchecking is crucial, because it allows falsification of <em>scientific</em>\nhypotheses, leading hopefully to improved scientific understanding in the form\nof new models.\nOne historically efficient method to falsify a scientific model is to check\nthe predictions it makes, so a scientific model must normally also be a\n\"predictive model\".\nThis viewpoint of establishing a model not just for making good predictions\nbut also to understand mechanisms of reality also seems closer to the field of\nstatistics.</p>\n<p>The above separation of prediction versus science is of course not a\nsimple dichotomy, but just a preference of the practitioner.</p>\n<h2>Bayesian Viewpoints?</h2>\n<p>So then, what is the Bayesian viewpoint here?\nThe answer is that some well respected figures in the field accept frequentist\ntests and p-values as a method to criticise and attempt to falsify Bayesian\nmodels.\nOne example can be seen in a <a href=\"http://www.stat.columbia.edu/~gelman/research/published/philosophy_chapter.pdf\">recent article</a>\nby <a href=\"http://andrewgelman.com/\">Andrew Gelman</a> and <a href=\"http://www.stat.cmu.edu/~cshalizi/\">Cosma\nShalizi</a> where mechanisms to falsify a\nBayesian model a discussed, stating</p>\n<blockquote>\n<p>\"The main point where we disagree with many Bayesians is that we do not\nthink that Bayesian methods are generally useful for giving the posterior\nprobability that a model is true, or the probability for preferring model A\nover model <em>B</em>, or whatever.  Bayesian inference is good for deductive\ninference within a model, but for evaluating a model, we prefer to compare\nit to data (what Cox and Hinkley , 1974, call \"pure significance testing\")\nwithout requiring that a new model be there to beat it.\"</p>\n</blockquote>\n<p>(They use pure significance tests and frequentist predictive checks, but no\np-values in that paper.)</p>\n<p>Another example is <a href=\"http://amstat.tandfonline.com/doi/abs/10.1080/01621459.2000.10474309\">an\narticle</a>\nby <a href=\"http://www.uv.es/bayarri/\">Susie Bayarri</a> and <a href=\"https://stat.duke.edu/~berger/\">James\nBerger</a>, where \"Bayesian p-values\" are\ndiscussed.</p>\n<p>A third and maybe more popular pragmatic Bayesian stance is summarized in\nBruce Hill's comment on <a href=\"http://www3.stat.sinica.edu.tw/statistica/oldpdf/A6n41.pdf\">Gelman, Meng, and Stern's article on posterior\npredictive\ntesting</a>,</p>\n<blockquote>\n<p>\"Like many others, I have come to regard the classical p-value as a useful\ndiagnostic device, particularly in screening large numbers of possibly\nmeaningful treatment comparisons. It is one of many ways quickly to alert\noneself to some of the important features of a data set. However, in my\nopinion it is not particularly suited for careful decision-making in serious\nproblems, or even for hypothesis testing.  Its primary function is to\nalert one to the need for making such a more careful analysis, and\nperhaps to search for better models. Whether one wishes actually to go\nbeyond the p-value depends upon, among other things, the importance of the\nproblem, whether the quality of the data and information about the model and\na priori distributions is sufficiently high for such an analysis to be\nworthwhile, and ultimately upon the perceived utility of such an analysis.\"</p>\n</blockquote>\n<p>Not arguing by reference to authorities, but given the broad spectrum of\ncontributions of Andrew Gelman, Bruce Hill, and James Berger (many of us\nlearned Bayesian methods from the books <a href=\"http://www.stat.columbia.edu/~gelman/book/\">Bayesian Data\nAnalysis</a> and <a href=\"http://www.springer.com/us/book/9780387960982\">Statistical\nDecision Theory and Bayesian\nAnalysis</a>), it should be clear\nthat if they take frequentist tests and p-values seriously in statistical\npractice, they may actually be useful.</p>\n<p>So let's look again at our goodness-of-fit problem.</p>\n<h2>Simple Models (simple null model)</h2>\n<p>The p-value can provide a useful diagnostic of goodness of fit.\nFor the case of a simple model <span class=\"math\">\\(P(X)\\)</span> with an observation <span class=\"math\">\\(X \\in \\mathcal{X}\\)</span>\nwe can pick a test statistic <span class=\"math\">\\(T: \\mathcal{X} \\to \\mathbb{R}\\)</span> where high values\nindicate unlikely outcomes, and then compute\n</p>\n<div class=\"math\">$$p_{\\textrm{classic}} = \\textrm{Pr}_{X' \\sim P}(T(X') \\geq T(X)),$$</div>\n<p>\nthat is, the probability of observing a <span class=\"math\">\\(T(X')\\)</span> greater than the actually\nobserved <span class=\"math\">\\(T(X)\\)</span>, given the assumed model <span class=\"math\">\\(P(X)\\)</span>.\nThis probability is the p-value and if the probability of observing a more\nextreme test statistic is small we should righly be suspicious of the assumed\nmodel.\nThe choice of test statistic <span class=\"math\">\\(T\\)</span> is the only degree of freedom and has to be\nmade given the model.</p>\n<p>This is the classic p-value and its formal definition is completely\nunambiguous.\nOne important observation is that if we assume the null hypothesis is true and\nwe treat the p-value as a random variable, then this random variable is\nuniformly distributed, for any sample size.</p>\n<h2>Latent Variable Models (composite null model)</h2>\n<p>Now assume a slightly more general setting, where we have a model\n<span class=\"math\">\\(P(X|\\theta)\\)</span>, and <span class=\"math\">\\(\\theta \\in \\Theta\\)</span> is some unknown parameter of the model\nwhich is not observed.</p>\n<p>Because it is not observed, the above definition does not apply.\nWe could apply the definition only if we knew <span class=\"math\">\\(\\theta\\)</span>.\nClassic methods assume that we have an estimator <span class=\"math\">\\(\\hat{\\theta}\\)</span> so that we can\nevaluate the p-value on <span class=\"math\">\\(P(X|\\hat{\\theta})\\)</span>, fixing the parameter to a value\nhopefully close to it's true value.\nThe key problem with this approach is that the p-value in general will no\nlonger be uniformly distributed.\nThis diminishes its value as a diagnostic for model misspecification.\n(Another alternative is to take the supremum probability over all possible\nparameters, again yielding a non-uniformly distributed p-value under the\nnull.)</p>\n<p>Bayesians to the rescue!  Twice!</p>\n<p>First, assume we would like to compute a p-value in the above setting.  What\nwould a Bayesian do?  Of course, he would integrate over the unknown\nparameter, using a prior.\nThis yields the so called <a href=\"http://projecteuclid.org/euclid.aos/1176325622\">posterior predictive\np-value</a> going back to\nthe <a href=\"http://www.jstor.org/stable/2984569\">work of Guttman</a>.\nAssuming a prior <span class=\"math\">\\(P(\\theta)\\)</span> we compute the <em>posterior predictive p-value</em> as\n</p>\n<div class=\"math\">$$p_{\\textrm{post}} = \\mathbb{E}_{\\theta \\sim P(\\theta|X)}[\n    \\textrm{Pr}_{X' \\sim P(X'|\\theta)}(T(X') \\geq T(X))],$$</div>\n<p>\nwhere <span class=\"math\">\\(P(\\theta|X) \\propto P(X|\\theta) P(\\theta)\\)</span> is the proper posterior.\nThe definition is simple: take the expectation of the ordinary p-value\nweighted by the parameter posterior.\nThis definition is very general and typically easy to compute during posterior\ninference, i.e. it is quite practical computationally.</p>\n<p>Unfortunately, it is also overly conservative, as explained in the JASA paper\n<a href=\"http://www.biostat.harvard.edu/robins/p-values.pdf\">``Asymptotic Distribution of P Values in Composite Null\nModels''</a> by Robins, van\nder Vaart, and Ventura from 2000.\nIntuitively this is because the observed data <span class=\"math\">\\(X\\)</span> is used twice, a violation\nof the <a href=\"https://projecteuclid.org/euclid.lnms/1215466210#toc\">likelihood principle of Bayesian\nstatistics</a>:\nfirst it is used to obtain the posterior <span class=\"math\">\\(P(\\theta|X)\\)</span>, and then it is used\nagain to compute the p-value.</p>\n<p>Bayesians to the rescue again!\nThis time it is Susie Bayarri and Jim Berger, and in their JASA paper <a href=\"http://amstat.tandfonline.com/doi/abs/10.1080/01621459.2000.10474309\">P\nvalues for Composite Null\nModels</a>\nthey introduce two alternative p-values which exactly \"undo\" the effect of\nusing the data twice by conditioning on the information already observed.\n(I will not discuss the <em>U-conditional predictive p-value</em> proposed by Bayarri\nand Berger.)\nHere is the basic idea: let <span class=\"math\">\\(X\\)</span> be the observed data and <span class=\"math\">\\(t=T(X)\\)</span> the test\nstatistic.  We then define the <em>partial posterior</em>,\n</p>\n<div class=\"math\">$$P(\\theta|X \\setminus t) \\propto \\frac{P(X|\\theta) P(\\theta)}{P(t|\\theta)}.$$</div>\n<p>\nTo understand this definition remember that random variables are functions\nfrom the sample space to another set.  Hence, conditioning on <span class=\"math\">\\(t\\)</span> means that\nwe condition on the event <span class=\"math\">\\(\\{X' \\in \\mathcal{X} | T(X') = t\\}\\)</span>.\nThe partial posterior predictive p-value is now defined as\n</p>\n<div class=\"math\">$$p_{\\textrm{ppost}} = \\mathbb{E}_{\\theta \\sim P(\\theta|X \\setminus t)}[\n    \\textrm{Pr}_{X' \\sim P(X'|\\theta)}(T(X') \\geq T(X))].$$</div>\n<p>\nBayarri and Berger, as well as Robins, van der Vaart, and Ventura analyze the\nproperties of this particular p-value and show that is asymptotically\nuniformly distributed and thus is neither conservative nor anti-conservative.</p>\n<p>If you are a Bayesian and consider providing a general model-fit diagnostic in\nthe absence of a formal alternative hypothesis this partial posterior\npredictive p-value is the method to use.</p>\n<p>However, there are two drawbacks I can see that have affected it's usefulness\nfor me:</p>\n<ol>\n<li>It is much harder to compute.  Whereas the posterior predictive p-value can\n   be well approximated even with naive Monte Carlo as soon as normal\n   posterior inference is achieved, this is not the case for the partial\n   posterior predictive p-value.  The reason is that <span class=\"math\">\\(P(t|\\theta)\\)</span>, although\n   typically an univariate density in the test statistic, is the integral over\n   potentially complicated sets in <span class=\"math\">\\(\\mathcal{X}\\)</span>, that is\n   <span class=\"math\">\\(P(t|\\theta) = \\int_{\\mathcal{X}} 1_{\\{T(X)=t\\}} P(X|\\theta) \\,\\textrm{d}X\\)</span>.\n   I have not seen generally applicable methods to compute\n   <span class=\"math\">\\(p_{\\textrm{ppost}}\\)</span> efficiently so far.</li>\n<li>The nice results of Bayarri and Berger do not extend to so called\n   <em>discrepancy statistics</em> as proposed by Xiaoli Meng in his <a href=\"http://projecteuclid.org/euclid.aos/1176325622\">1994\n   paper</a>.  These more general\n   test statistics include the parameter, i.e. we use <span class=\"math\">\\(T(X,\\theta)\\)</span> instead of\n   just <span class=\"math\">\\(T(X)\\)</span>.  Why is this useful?  For example, and I found this a very\n   useful test statistic, you can directly use the likelihood of the model\n   itself as a test statistic: <span class=\"math\">\\(T(X,\\theta) = -P(X|\\theta)\\)</span>.</li>\n</ol>\n<p>Enough thoughts, let's get our hands dirty with a simple experiment.</p>\n<h2>Experiment</h2>\n<p>We take a simple composite null setting as follows.  Our assumed model is\n</p>\n<div class=\"math\">$$X_i \\sim \\mathcal{N}(\\mu, \\sigma^2),\\qquad i=1,\\dots,n.$$</div>\n<p>\nWe get to observe <span class=\"math\">\\(X=(X_1,\\dots,X_n)\\)</span> and know <span class=\"math\">\\(\\sigma\\)</span> but consider <span class=\"math\">\\(\\mu\\)</span>\nunknown.</p>\n<p>After some observations we would like to assess whether our model is accurate\nin light of the data.\nTo this end we would like to use the P-values described above.\nWe will need two ingredients: we need to define a test statistic and we need\nto work out the posterior inference in our model.</p>\n<p>For the test statistic we actually use a generalized test statistic\n(discrepancy variable in Meng's vocabulary) as\n</p>\n<div class=\"math\">$$T(X,\\mu) = - \\prod_{i=1}^n p(X_i|\\mu)\n    = -\\prod_{i=1}^n \\mathcal{N}(X_i ; \\mu, \\sigma^2).$$</div>\n<p>For the posterior inference, as Bayesians we place a prior on <span class=\"math\">\\(\\mu\\)</span> and we\nselect\n</p>\n<div class=\"math\">$$\\mu \\sim \\mathcal{N}(\\mu_0, \\sigma_0).$$</div>\n<p>\nThe Bayesian analysis is particularly straightforward in this case, as this\n<a href=\"http://www.cs.ubc.ca/~murphyk/Papers/bayesGauss.pdf\">note by Kevin Murphy</a>\ndetails.\nIn particular, after observing <span class=\"math\">\\(n\\)</span> samples <span class=\"math\">\\(X=(X_1,\\dots,X_n)\\)</span> the posterior\non <span class=\"math\">\\(\\mu\\)</span> has a simple closed form as\n</p>\n<div class=\"math\">$$p(\\mu|X) = \\mathcal{N}(\\mu_n, \\sigma^2_n),$$</div>\n<p>\nwith\n</p>\n<div class=\"math\">$$\\sigma^2_n = \\frac{1}{\\frac{n}{\\sigma^2}+\\frac{1}{\\sigma^2_0}},$$</div>\n<p>\nand\n</p>\n<div class=\"math\">$$\\mu_n = \\sigma^2_n \\left(\\frac{\\mu_0}{\\sigma^2_0} +\n    \\frac{n \\bar{x}}{\\sigma^2}\\right),$$</div>\n<p>\nwhere <span class=\"math\">\\(\\bar{x} = \\frac{1}{n} \\sum_i X_i\\)</span> is the sample average.</p>\n<p>From this simple form of the posterior distribution we can derive the closed\nform partial posterior <span class=\"math\">\\(P(\\mu|X\\setminus t)\\)</span> as well (not shown here, but\nessentially using known properties of the <span class=\"math\">\\(\\chi^2\\)</span> distribution).\nHere is a picture of the posterior <span class=\"math\">\\(P(\\mu|X)\\)</span> and the partial posterior\n<span class=\"math\">\\(P(\\mu | X \\setminus t)\\)</span>, where the data <span class=\"math\">\\(X\\)</span> actually\ncomes from the assumed model with true <span class=\"math\">\\(\\mu=4.5\\)</span> and <span class=\"math\">\\(n=10\\)</span>.\nInterestingly the partial posterior is more concentrated (which makes sense\nfrom the theory derived in Robins et al.).</p>\n<p><img alt=\"Posterior and partial posterior in mu\" src=\"http://www.nowozin.net/sebastian/blog/images/pvalue-posterior.svg\"></p>\n<p>Let us generate data from the assumed prior and model and see how our p-values\nbehave.  Because the null model is then correct, we can hope that the\nresulting p-values will be uniformly distributed.\nIndeed, if they were perfectly uniformly distributed they would be proper\nfrequentist p-values.\nBecause of the paper of Robins et al. we know that they will only be\nasymptotically uniformly distributed as <span class=\"math\">\\(n \\to \\infty\\)</span>.  But here we are also\noutside the theory because our test statistic <span class=\"math\">\\(T(X,\\mu)\\)</span> includes the unknown\nparameter <span class=\"math\">\\(\\mu\\)</span>.\nSo, walking on thin theory, let's verify the distribution for <span class=\"math\">\\(n=10\\)</span> by taking\na histogram over <span class=\"math\">\\(10^6\\)</span> replicates.</p>\n<p><img alt=\"Distribution over P-values\" src=\"http://www.nowozin.net/sebastian/blog/images/pvalue-histogram.svg\"></p>\n<p>This looks good, and the partial posterior predictive p-value is more\nuniformly distributed obtaining better frequentist properties, in line with\nthe claims in Bayarri and Berger and in Robins et al.</p>\n<p>Finally, let us check with data from a model that is different from the\nassumed model.  Here I sample from <span class=\"math\">\\(\\mathcal{N}(\\mu, s^2)\\)</span>, where <span class=\"math\">\\(s \\in\n[0,2]\\)</span>.  For <span class=\"math\">\\(s=1\\)</span> this is the assumed model, but ideally we can refute the\nmodel for values that differ from one by detecting this deviation through a\np-value close to zero.\nThe plot below shows, for each <span class=\"math\">\\(s\\)</span>, the average p-value over 1000 replicates.</p>\n<p><img alt=\"Deviation experiment\" src=\"http://www.nowozin.net/sebastian/blog/images/pvalue-deviation-sensitivity.svg\"></p>\n<p>Clearly for <span class=\"math\">\\(s &lt; 0.6\\)</span> or so we can reliably discover that our assumed model is\nproblematic.  Interestingly the partial posterior predictive p-value has\nsignificantly more power, in line with the theory.</p>\n<p>For <span class=\"math\">\\(s &gt; 1\\)</span> however, our p-value goes to one!  How can this be?\nWell, remember that the choice of test statistic determines which deviations\nfrom our assumptions we can detect and that the p-value cannot verify the\ncorrectness of our assumed model but instead may only provide one-sided\nevidence against the model.\nWith our current test statistic clearly this significant deviation passes\nundetected.\nWe could replace our test statistic using the negative of our current test\nstatistic and would be able detect the above deviation for <span class=\"math\">\\(s &gt; 1\\)</span>, but this\nimplicitly more or less starts the process of <em>thinking about alternative\nmodels</em>, a point Bruce Hill mentioned above.</p>\n<p>If we would like to consider alternative models we should ideally consider\nthem in a formal way, and as a result we would be better off using a fully\nBayesian approach over an enlarged model class.</p>\n<script type=\"text/javascript\">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}\n</script>",
  "category": ""
}