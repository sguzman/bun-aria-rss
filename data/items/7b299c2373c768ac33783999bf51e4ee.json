{
  "title": "How good are your beliefs? Part 1: Scoring Rules",
  "link": "",
  "published": "2015-09-04T22:00:00+01:00",
  "updated": "2015-09-04T22:00:00+01:00",
  "author": {
    "name": "Sebastian Nowozin"
  },
  "id": "tag:www.nowozin.net,2015-09-04:/sebastian/blog/how-good-are-your-beliefs-part-1-scoring-rules.html",
  "summary": "<p>This article is the first of two on <em>proper scoring rules</em>,\na specific type of loss function defined on probability distributions or\nfunctions of probability distributions.</p>\n<p>If this article sparks your interest, I recommend the gentle introduction to\nscoring rules â€¦</p>",
  "content": "<p>This article is the first of two on <em>proper scoring rules</em>,\na specific type of loss function defined on probability distributions or\nfunctions of probability distributions.</p>\n<p>If this article sparks your interest, I recommend the gentle introduction to\nscoring rules in the context of decision theory in Chapter 10 of <a href=\"http://eu.wiley.com/WileyCDA/WileyTitle/productCd-047149657X.html\">Parmigiani\nand Inoue's \"Decision Theory\"\nbook</a>,\nwhich is a great book to have on your data science bookshelf in any case and\nit deservedly won the <a href=\"https://bayesian.org/awards/DeGrootPrize.html\">DeGroot\nprize</a> in 2009.</p>\n<h2>Scoring Rules</h2>\n<p>Consider the following forecasting setting.\nGiven a set of possible outcomes <span class=\"math\">\\(\\mathcal{X}\\)</span> and a class of probability\nmeasures <span class=\"math\">\\(\\mathcal{P}\\)</span> defined on a suitably constructed <span class=\"math\">\\(\\sigma\\)</span>-algebra,\nwe consider a <em>forecaster</em> which makes a forecast in the form of a probability\ndistribution <span class=\"math\">\\(P \\in \\mathcal{P}\\)</span>.\nAfter the forecast is fixed, a realization <span class=\"math\">\\(x \\in \\mathcal{X}\\)</span> is revealed and\nwe would like to assess quality of the prediction made by the forecaster.</p>\n<p>A <em>scoring rule</em> is a function <span class=\"math\">\\(S\\)</span> such that <span class=\"math\">\\(S(P,x)\\)</span> is taken to mean the\n<em>quality</em> of the forecast.  Hence the function has the form\n<span class=\"math\">\\(S: \\mathcal{P} \\times \\mathcal{X} \\to \\mathbb{R} \\cup \\{-\\infty,\\infty\\}\\)</span>.\nThere are two variants popular in the literature: the <em>positively-orientied</em>\nscoring rules assign higher values to better forecasts, the\n<em>negatively-oriented</em> scoring rules behave like loss functions, taking smaller\nvalues for better forecasts.</p>\n<p>A <em>proper</em> scoring rule has desirable behaviour, to be made precise shortly.\nLet us first think what could be desirable in a scoring rule.  Intuitively we\nwould like to make \"cheating\" difficult, that is, if we really subjectively\nbelieve in <span class=\"math\">\\(P\\)</span>, we should have no incentive to report any deviation from <span class=\"math\">\\(P\\)</span>\nin order to achieve a better score.\nFormally, we first define the <em>expected score</em> under distribution <span class=\"math\">\\(Q\\)</span>,</p>\n<div class=\"math\">$$S(P,Q) = \\mathbb{E}_{x \\sim Q}[S(P,x)].$$</div>\n<p>So that if we believe in any prediction <span class=\"math\">\\(P \\in \\mathcal{P}\\)</span>, then we should\ndemand that (for negatively-oriented scores)</p>\n<div class=\"math\">$$S(P,P) \\leq S(P,Q),\\qquad \\forall P,Q \\in \\mathcal{P}.$$</div>\n<p>For <em>strictly proper</em> scoring rules the above inequality holds strictly except\nfor <span class=\"math\">\\(Q=P\\)</span>.\nFor a proper scoring rule the above inequality means that in expectation the\nlowest possible score can be achieved by faithfully reporting our true\nbeliefs.  Therefore, a rational forecaster who aims to minimize expected score\n(loss) is going to report his beliefs.</p>\n<p>Key uses of scoring rules are:</p>\n<ul>\n<li>Evaluating the predictive performance of a model;</li>\n<li>Eliciting probabilities;</li>\n<li>Using them for parameter estimation.</li>\n</ul>\n<p>Let us look briefly at the different uses.</p>\n<h3>Model Evaluation</h3>\n<p>For <em>assessing the model performance</em>, we simply use the scoring rule as a loss\nfunction and measure the predictive performance on a holdout data set.</p>\n<h3>Probability Elicitation</h3>\n<p>For <em>probability elicitation</em> we can use a scoring rule as follows: we ask a\nuser to make predictions and we tell him that we will reward him\nproportionally to the value achieved by the scoring rule once the prediction\ncan be scored.  Assuming that the user is <em>rational</em> and aims to maximize his\nreward, if we use a proper scoring rule, then he can maximize his expected\nreward by making predictions according to the true beliefs he holds.\nHowever, while the existence of a strictly proper scoring rule roughly means\nthat elicitation of a quantity is possible, more efficient methods for\nprobability elicitation may exist.  Infact, <a href=\"http://www2.warwick.ac.uk/fac/sci/statistics/staff/academic-research/french/\">Simon\nFrench</a> and <a href=\"http://www.davidriosinsua.org/\">David Rios Insua</a>\nargue in their book <a href=\"http://eu.wiley.com/WileyCDA/WileyTitle/productCd-0470711051.html\">Statistical Decision\nTheory</a>,\npage 76, that</p>\n<blockquote>\n<p>\"de Finetti (1974; 1975) and others have championed the use of <em>scoring\nrules</em> to elicit probabilities of events. ...\nScoring rules are important in de Finetti's development of subjective\nprobability, but it is not clear that they have a practical use in\nstatistical or decision analysis. ...\nScoring rules could provide a very expensive method of eliciting\nprobabilities.  In training probability assessors, however, they can have a\npractical use.\"</p>\n</blockquote>\n<p>If you wonder what more efficient alternatives French and Insua have in mind,\nthey do propose several methods to elicit probabilities, such as an idealized\n\"probability wheel\" the user can configure and spin, and a sequence of\nproposed gambles in order to find a fair value accepted by the user.</p>\n<p>In general it seems to me (as an outsider of this field), that probability\nelicitation is as much about theoretically sound methods as it is about human\npsychology and biases, and how to avoid them.  The human aspect of probability\nelicitation is discussed in the <a href=\"http://www.rff.org/people/profile/roger-m-cooke\">Roger\nCooke</a>'s <a href=\"https://books.google.com/books?isbn=0195362373\">book-length\nmonograph</a> on the topic, and\nthe recent study of <a href=\"http://journal.sjdm.org/13/131029/jdm131029.pdf\">(Goldstein and Rothschild, \"Lay understanding of\nprobability distributions\",\n2014)</a> (thanks to Ian Kash\nfor pointing me to this study!).</p>\n<h3>Estimation</h3>\n<p>For <em>parameter estimation</em> we perform empirical risk minimization on a\nprobabilistic model using the scoring rule as a loss function, an approach\ndating back to <a href=\"http://link.springer.com/article/10.1007/BF02613654\">(Pfanzagl,\n1969)</a>.  This is a\nspecial case of <a href=\"https://en.wikipedia.org/wiki/M-estimator\">M-estimation</a> but\ngeneralizes maximum likelihood estimation (MLE), where the log-probability\nscoring rule is used.</p>\n<p>If the model class contains the true generating model this yields a\n<a href=\"https://en.wikipedia.org/wiki/Consistent_estimator\"><em>consistent estimator</em></a>\nbut for misspecified models this can yield answers different from the MLE, and\nthese answers may be preferable; for example, if model assumptions are\nviolated and for any choice of parameter the model would put have a low\ndensity on some observations these tend to influence the MLE severely because\nthe log-prob scoring rule assigns a large penalty to these observations.\nUsing a suitable scoring rule cannot prevent misspecification of course but\nthe consequences can be made less severe.</p>\n<p>It should also be said that for estimation problems the log-prob scoring rule\nis the most principled in that it is the only one that can be justified from\nthe <a href=\"https://projecteuclid.org/euclid.lnms/1215466210#toc\">likelihood principle</a>.</p>\n<h2>Scoring Rule Examples</h2>\n<p>Here are a few examples of common and not so common scoring rules both for\ndiscrete and continuous outcomes.</p>\n<h3>Scoring Rule Example: Brier Score</h3>\n<p>This scoring rule was historically the first, proposed by\n<a href=\"http://imsc.pacificclimate.org/awards_brier.shtml\">Glenn Wilson Brier</a>\n(1913-1998) in his seminal work\n<a href=\"http://docs.lib.noaa.gov/rescue/mwr/078/mwr-078-01-0001.pdf\">(Brier, \"Verification of Forecasts Expressed in Terms of Probability\",\n1950)</a>\nas a means to verify weather forecasts.</p>\n<p>Given a discrete outcome set <span class=\"math\">\\(\\{1,2,\\dots,K\\}\\)</span> the forecaster specifies a\ndistribution <span class=\"math\">\\(P=(p_1,\\dots,p_K)\\)</span> with <span class=\"math\">\\(p_i \\geq 0\\)</span> and <span class=\"math\">\\(\\sum_i p_i = 1\\)</span>.\nThen, when an outcome <span class=\"math\">\\(j\\)</span> is realized we score the forecaster according to\nthe <em>Brier score</em>,</p>\n<div class=\"math\">$$S_B(P,j) = \\sum_{i=1}^K (1_{\\{i=j\\}} - p_i)^2.$$</div>\n<p>The Brier score is extensively discussed in <a href=\"http://www.dtic.mil/cgi-bin/GetTRDoc?AD=ADA121924\">(DeGroot and Fienberg,\n1983)</a> and they show that\nit can be decomposed into two terms measuring <em>calibration</em> and <em>refinement</em>,\nrespectively.  Here, <em>refinement</em> measures the information\navailable to discriminate between different outcomes that is contained in the\nprediction.</p>\n<p>For the case with binary classes, the definite work is <a href=\"http://www-stat.wharton.upenn.edu/~buja/PAPERS/paper-proper-scoring.pdf\">(Buja, Stuetzle, Shen,\n2005)</a>\nin which a class of scoring rules is proposed based on the Beta distribution\nwhich generalizes both the Brier score and the log-probability score.</p>\n<h3>Scoring Rule Example: Log-Probability</h3>\n<p>The most common scoring rule in estimation problems is the log-probability,\nalso known as the log-loss in machine learning.\nMaximum likelihood estimation can be seen as optimizing the log-probability\nscoring rule.</p>\n<p>For the discrete outcome case it is given simply by</p>\n<div class=\"math\">$$S_{\\textrm{log}}(P,i) = -\\log p_i.$$</div>\n<p>If <span class=\"math\">\\(p_i = 0\\)</span> the score <span class=\"math\">\\(S_{\\textrm{log}}(P,i) = \\infty\\)</span>.\nThe log-probability is a proper scoring rule, but what really distinguishes it\nis that it is <em>local</em> in that when outcome <span class=\"math\">\\(j\\)</span> realizes only the predicted\nvalue <span class=\"math\">\\(p_j\\)</span> is used to compute the score.\nIntuitively this is a desirable property because if <span class=\"math\">\\(j\\)</span> happens, why should we\ncare about the precise distribution of probability mass for the other events?</p>\n<p>It turns out that this <em>local</em> property is unique to the log-probability\nscoring rule.  (For the result and proof see Theorem 10.1 in Parmigiani and\nInoue's book.)</p>\n<h3>Scoring Rule Example: Energy Statistic</h3>\n<p>This scoring rule is for predicting a distribution in <span class=\"math\">\\(\\mathbb{R}^d\\)</span> and is\ndefined for <span class=\"math\">\\(\\beta \\in (0,2)\\)</span>, realization <span class=\"math\">\\(x \\in \\mathbb{R}^d\\)</span>, and distribution <span class=\"math\">\\(P\\)</span> on <span class=\"math\">\\(\\mathbb{R}^d\\)</span> as</p>\n<div class=\"math\">$$S_E(P,x) = \\mathbb{E}_{X \\sim P}[\\|X-x\\|^\\beta] - \\frac{1}{2} \\mathbb{E}_{X,X' \\sim P}[\\|X-X'\\|^\\beta].$$</div>\n<p>This score has an intuitive interpretation: the score is the expected distance\nto the realization minus half the expected pairwise sample distance.\nLet us think about a few cases: if <span class=\"math\">\\(P\\)</span> is a point mass, then the first term is\njust the distance to the realization and the second term is zero; in\nparticular for <span class=\"math\">\\(\\beta \\to 2\\)</span> the score recovers the squared Euclidean norm\nloss.\nThe original definition is from <a href=\"https://www.csss.washington.edu/~raftery/Research/PDF/Gneiting2007jasa.pdf\">(Gneiting and Raftery,\n2007)</a> except for the sign change, but is based on\nSzekely's <a href=\"http://personal.bgsu.edu/~mrizzo/energy.htm\">energy statistic</a>\nwhich also independently found its way into machine learning through the\n<a href=\"http://kyb.mpg.de/fileadmin/user_upload/files/publications/attachments/indepHS140_3437%5B0%5D.pdf\">Hilbert-Schmidt independence\ncriterion</a>.</p>\n<p>For <span class=\"math\">\\(\\beta \\in (0,2)\\)</span> the energy score is a strictly proper scoring function\nfor all Borel measures with finite moment <span class=\"math\">\\(\\mathbb{E}_P[\\|X\\|^\\beta]\n&lt; \\infty\\)</span>.</p>\n<p>Here is a visualization, where <span class=\"math\">\\(P = \\mathcal{N}([0,0]^T, \\textrm{diag}([1/2,\n5/2]))\\)</span> is given by the 10k samples and the red marker corresponds to the\nrealization <span class=\"math\">\\(x\\)</span>.  Here we have <span class=\"math\">\\(\\beta=1\\)</span>.  We can see that the Euclidean\nnature of the scoring rule seems to dominate the anisotropic distribution <span class=\"math\">\\(P\\)</span>,\nthat is, a realization that is unlikely under our belief distribution\n(leftmost plot) achieves a lower score than a sample with higher density\n(second leftmost plot).</p>\n<p><img alt=\"Energy score for beta equal to\none\" src=\"http://www.nowozin.net/sebastian/blog/images/scoringrules-energyscore-beta10-80dpi.png\"></p>\n<p>As a practical manner, the energy score is simple to evaluate even when you\nhave only predictive Monte Carlo realizations of your model, compared to the\nlog-probability rule which requires the normalizer of the predictive\ndistribution.</p>\n<h3>Scoring Rule: Check Loss</h3>\n<p>The <em>check loss</em>, also known as <em>quantile loss</em> or <em>tick loss</em>, is a loss\nfunction used for <a href=\"https://en.wikipedia.org/wiki/Quantile_regression\">quantile\nregression</a>, where we would\nlike to learn a model that directly predicts a\n<a href=\"https://en.wikipedia.org/wiki/Quantile\">quantile</a> of a distribution, but we\nare given only samples of the distribution at training time.</p>\n<p>This scoring rule is somewhat different in that a specific property of a\nbelief distribution is scored, namely the quantile of the distribution.\nBeing <em>proper</em> here means that the lowest expected loss is achieved by\npredicting the corresponding quantile of your belief.\n(Interestingly proper scoring rules exist only for some functions of the\ndistribution, see <a href=\"http://arxiv.org/abs/0912.0902\">(Gneiting, 2009)</a>.)</p>\n<p>You may know a special case of the check loss already:\nwhen using an absolute value loss, your expected risk is minimized by taking\nthe median of your belief distribution, that is, the <span class=\"math\">\\(\\frac{1}{2}\\)</span>-quantile.\nThe <em>check loss</em> generalizes this to a richer family of loss functions such\nthat the expected minimizer corresponds to arbitrary quantiles, not just the\nmedian.\nThus, instead of scoring an entire belief distribution <span class=\"math\">\\(P\\)</span> we only score its\nquantile statistics.</p>\n<p>The check loss is defined as</p>\n<div class=\"math\">$$S_{\\textrm{c}}(r,x,\\alpha) = (x-r) (1_{\\{x \\leq r\\}} - \\alpha),$$</div>\n<p>where <span class=\"math\">\\(r\\)</span> is our predicted <span class=\"math\">\\(\\alpha\\)</span>-quantile and <span class=\"math\">\\(x \\sim Q\\)</span> is a sample from\nthe true unknown distribution <span class=\"math\">\\(Q\\)</span>.</p>\n<p>Plotting this loss explains the name <em>check loss</em> and <em>tick loss</em>, because it\nlooks like two tilted lines.\nI show it for a sample realization of <span class=\"math\">\\(x=5\\)</span>, and the horizontal axis denotes\nthe quantile estimate.</p>\n<p><img alt=\"Check loss, a popular quantile loss\" src=\"http://www.nowozin.net/sebastian/blog/images/scoringrules-quantile-rule.svg\"></p>\n<p>For any belief distribution, taking the minimum expected risk decision yields\nthe matching quantile.\nFor example, if your beliefs are distributed according to <span class=\"math\">\\(X \\sim N(5,1)\\)</span>,\nthen you would consider the expected risk</p>\n<div class=\"math\">$$R_{\\alpha}(r,\\alpha) = \\mathbb{E}_{X \\sim N(5,1)}[-S_c(r,X,\\alpha)].$$</div>\n<p>This convolves the check loss function with the belief distribution, in this\ncase corresponding to a Gaussian kernel.\nThe minimizer over <span class=\"math\">\\(r\\)</span> of this expected risk function would correspond to your\noptimal decision.</p>\n<p><img alt=\"Integrated risk under the check loss\" src=\"http://www.nowozin.net/sebastian/blog/images/scoringrules-quantile-rule-example.svg\"></p>\n<p>The above plot marks the 10/50/90 quantiles and these correspond to the\nminimizers of the expected risks of the respective check losses.</p>\n<h2>Conclusion</h2>\n<p>The above is only a small peek into the vast literature on scoring rules.\nIf you are mathematically inclined, I highly recommend <a href=\"https://www.csss.washington.edu/~raftery/Research/PDF/Gneiting2007jasa.pdf\">(Gneiting and Raftery,\n2007)</a>\nas an enjoyable further read and <a href=\"http://www.cs.colorado.edu/~raf/media/papers/vec-props.pdf\">(Frongillo and Kash,\n2015)</a> for the\nmost recent general results; everyone else may enjoy the book mentioned in the\nintroduction.</p>\n<p>In the second part we are going to put your forecasting skills to the test via\nan interactive quiz!</p>\n<p><em>Acknowledgements</em>.  I thank <a href=\"http://research.microsoft.com/en-us/people/iankash/\"><em>Ian\nKash</em></a> for further\ninsightful discussions on scoring rules and pointing me to relevant\nliterature.</p>\n<script type=\"text/javascript\">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}\n</script>",
  "category": ""
}