{
  "title": "Transfer learning for TensorFlow object detection models in Amazon SageMaker",
  "link": "https://aws.amazon.com/blogs/machine-learning/transfer-learning-for-tensorflow-object-detection-models-in-amazon-sagemaker/",
  "dc:creator": "Vivek Madan",
  "pubDate": "Fri, 04 Nov 2022 15:28:40 +0000",
  "category": [
    "Amazon SageMaker",
    "Artificial Intelligence",
    "Foundational (100)"
  ],
  "guid": "45c3490a3a95f32ace4caa7ecf706dad9773ff62",
  "description": "Amazon SageMaker provides a suite of built-in algorithms, pre-trained models, and pre-built solution templates to help data scientists and machine learning (ML) practitioners get started on training and deploying ML models quickly. You can use these algorithms and models for both supervised and unsupervised learning. They can process various types of input data, including tabular, […]",
  "content:encoded": "<p><a href=\"https://aws.amazon.com/sagemaker/\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker</a> provides a suite of <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html\" target=\"_blank\" rel=\"noopener\">built-in algorithms</a>, <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html#jumpstart-models\" target=\"_blank\" rel=\"noopener\">pre-trained models</a>, and <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-solutions.html\" target=\"_blank\" rel=\"noopener\">pre-built solution templates</a> to help data scientists and machine learning (ML) practitioners get started on training and deploying ML models quickly. You can use these algorithms and models for both supervised and unsupervised learning. They can process various types of input data, including tabular, image, and text.</p> \n<p>This post is the second in a series on the new built-in algorithms in SageMaker. In the <a href=\"https://aws.amazon.com/blogs/machine-learning/transfer-learning-for-tensorflow-image-classification-models-in-amazon-sagemaker/\" target=\"_blank\" rel=\"noopener\">first post</a>, we showed how SageMaker provides a built-in algorithm for image classification. Today, we announce that SageMaker provides a new built-in algorithm for object detection using TensorFlow. This supervised learning algorithm supports transfer learning for many pre-trained models available in TensorFlow. It takes an image as input and outputs the objects present in the image along with the bounding boxes. You can fine-tune these pre-trained models using transfer learning even when a large number of training images aren’t available. It’s available through the SageMaker <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html\" target=\"_blank\" rel=\"noopener\">built-in algorithms</a> as well as through the <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html\" target=\"_blank\" rel=\"noopener\">SageMaker JumpStart UI</a> in <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker Studio</a>. For more information, refer to <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection-tensorflow.html\" target=\"_blank\" rel=\"noopener\">Object Detection Tensorflow</a> and the example notebook <a href=\"https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/object_detection_tensorflow/Amazon_Tensorflow_Object_Detection.ipynb\" target=\"_blank\" rel=\"noopener\">Introduction to SageMaker Tensorflow – Object Detection</a>.</p> \n<p>Object detection with TensorFlow in SageMaker provides transfer learning on many pre-trained models available in TensorFlow Hub. According to the number of class labels in the training data, a new randomly initialized object detection head replaces the existing head of the TensorFlow model. Either the whole network, including the pre-trained model, or only the top layer (object detection head) can be fine-tuned on the new training data. In this transfer learning mode, you can achieve training even with a smaller dataset.</p> \n<h2>How to use the new TensorFlow object detection algorithm</h2> \n<p>This section describes how to use the TensorFlow object detection algorithm with the <a href=\"https://sagemaker.readthedocs.io/en/stable/\" target=\"_blank\" rel=\"noopener\">SageMaker Python SDK</a>. For information on how to use it from the Studio UI, see <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html\" target=\"_blank\" rel=\"noopener\">SageMaker JumpStart</a>.</p> \n<p>The algorithm supports transfer learning for the pre-trained models listed in <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection-tensorflow-Models.html\" target=\"_blank\" rel=\"noopener\">TensorFlow models</a>. Each model is identified by a unique <code>model_id</code>. The following code shows how to fine-tune a ResNet50 V1 FPN model identified by <code>model_id</code> <code>tensorflow-od1-ssd-resnet50-v1-fpn-640x640-coco17-tpu-8</code> on a custom training dataset. For each <code>model_id</code>, in order to launch a SageMaker training job through the <a href=\"https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html\" target=\"_blank\" rel=\"noopener\">Estimator</a> class of the SageMaker Python SDK, you need to fetch the Docker image URI, training script URI, and pre-trained model URI through the utility functions provided in SageMaker. The training script URI contains all the necessary code for data processing, loading the pre-trained model, model training, and saving the trained model for inference. The pre-trained model URI contains the pre-trained model architecture definition and the model parameters. Note that the Docker image URI and the training script URI are the same for all the TensorFlow object detection models. The pre-trained model URI is specific to the particular model. The pre-trained model tarballs have been pre-downloaded from TensorFlow and saved with the appropriate model signature in <a href=\"http://aws.amazon.com/s3\" target=\"_blank\" rel=\"noopener\">Amazon Simple Storage Service</a> (Amazon S3) buckets, such that the training job runs in network isolation. See the following code:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">from sagemaker import image_uris, model_uris, script_urisfrom sagemaker.estimator import Estimator\n\nmodel_id, model_version = \"tensorflow-od1-ssd-resnet50-v1-fpn-640x640-coco17-tpu-8\", \"*\"\ntraining_instance_type = \"ml.p3.2xlarge\"\n# Retrieve the docker image\ntrain_image_uri = image_uris.retrieve(model_id=model_id,model_version=model_version,image_scope=\"training\",instance_type=training_instance_type,region=None,framework=None)# Retrieve the training script\ntrain_source_uri = script_uris.retrieve(model_id=model_id, model_version=model_version, script_scope=\"training\")# Retrieve the pre-trained model tarball for transfer learning\ntrain_model_uri = model_uris.retrieve(model_id=model_id, model_version=model_version, model_scope=\"training\")\n\noutput_bucket = sess.default_bucket()\noutput_prefix = \"jumpstart-example-tensorflow-od-training\"\ns3_output_location = f\"s3://{output_bucket}/{output_prefix}/output\"</code></pre> \n</div> \n<p>With these model-specific training artifacts, you can construct an object of the <a href=\"https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html\" target=\"_blank\" rel=\"noopener\">Estimator</a> class:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\"># Create SageMaker Estimator instance\ntf_od_estimator = Estimator(\n    role=aws_role,\n    image_uri=train_image_uri,\n    source_dir=train_source_uri,\n    model_uri=train_model_uri,\n    entry_point=\"transfer_learning.py\",\n    instance_count=1,\n    instance_type=training_instance_type,\n    max_run=360000,\n    hyperparameters=hyperparameters,\n    output_path=s3_output_location,)</code></pre> \n</div> \n<p>Next, for transfer learning on your custom dataset, you might need to change the default values of the training hyperparameters, which are listed in <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection-tensorflow-Hyperparameter.html\" target=\"_blank\" rel=\"noopener\">Hyperparameters</a>. You can fetch a Python dictionary of these hyperparameters with their default values by calling <code>hyperparameters.retrieve_default</code>, update them as needed, and then pass them to the Estimator class. Note that the default values of some of the hyperparameters are different for different models. For large models, the default batch size is smaller and the <code>train_only_top_layer</code> hyperparameter is set to <code>True</code>. The hyperparameter <code>train_only_top_layer</code> defines which model parameters change during the fine-tuning process. If <code>train_only_top_layer</code> is <code>True</code>, parameters of the classification layers change and the rest of the parameters remain constant during the fine-tuning process. On the other hand, if <code>train_only_top_layer</code> is <code>False</code>, all parameters of the model are fine-tuned. See the following code:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">from sagemaker import hyperparameters# Retrieve the default hyper-parameters for fine-tuning the model\nhyperparameters = hyperparameters.retrieve_default(model_id=model_id, model_version=model_version)# [Optional] Override default hyperparameters with custom values\nhyperparameters[\"epochs\"] = \"5\"</code></pre> \n</div> \n<p>We provide the <a href=\"https://www.cis.upenn.edu/~jshi/ped_html/&quot; \\l &quot;pub1\" target=\"_blank\" rel=\"noopener\">PennFudanPed dataset</a> as a default dataset for fine-tuning the models. The dataset comprises images of pedestrians. The following code provides the default training dataset hosted in S3 buckets:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\"># Sample training data is available in this bucket\ntraining_data_bucket = f\"jumpstart-cache-prod-{aws_region}\"\ntraining_data_prefix = \"training-datasets/PennFudanPed_COCO_format/\"\n\ntraining_dataset_s3_path = f\"s3://{training_data_bucket}/{training_data_prefix}\"</code></pre> \n</div> \n<p>Finally, to launch the SageMaker training job for fine-tuning the model, call <code>.fit</code> on the object of the Estimator class, while passing the S3 location of the training dataset:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\"># Launch a SageMaker Training job by passing s3 path of the training data\ntf_od_estimator.fit({\"training\": training_dataset_s3_path}, logs=True)</code></pre> \n</div> \n<p>For more information about how to use the new SageMaker TensorFlow object detection algorithm for transfer learning on a custom dataset, deploy the fine-tuned model, run inference on the deployed model, and deploy the pre-trained model as is without first fine-tuning on a custom dataset, see the following example notebook: <a href=\"https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/object_detection_tensorflow/Amazon_Tensorflow_Object_Detection.ipynb\" target=\"_blank\" rel=\"noopener\">Introduction to SageMaker TensorFlow – Object Detection</a>.</p> \n<h2>Input/output interface for the TensorFlow object detection algorithm</h2> \n<p>You can fine-tune each of the pre-trained models listed in <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection-tensorflow-Models.html\" target=\"_blank\" rel=\"noopener\">TensorFlow Models</a> to any given dataset comprising images belonging to any number of classes. The objective is to minimize prediction error on the input data. The model returned by fine-tuning can be further deployed for inference. The following are the instructions for how the training data should be formatted for input to the model:</p> \n<ul> \n <li><strong>Input</strong> – A directory with sub-directory images and a file <code>annotations.json</code>.</li> \n <li><strong>Output</strong> – There are two outputs. First is a fine-tuned model, which can be deployed for inference or further trained using incremental training. Second is a file which maps class indexes to class labels; this is saved along with the model.</li> \n</ul> \n<p>The input directory should look like the following example:</p> \n<div class=\"hide-language\"> \n <div class=\"hide-language\"> \n  <pre><code class=\"lang-code\">input_directory\n      | -- images\n            |--abc.png\n            |--def.png\n      |--annotations.json</code></pre> \n </div> \n</div> \n<p>The <code>annotations.json</code> file should have information for <code>bounding_boxes</code> and their class labels. It should have a dictionary with the keys <code>\"images\"</code> and <code>\"annotations\"</code>. The value for the <code>\"images\"</code> key should be a list of entries, one for each image of the form <code>{\"file_name\": image_name, \"height\": height, \"width\": width, \"id\": image_id}</code>. The value of the <code>\"annotations\"</code> key should be a list of entries, one for each bounding box of the form <code>{\"image_id\": image_id, \"bbox\": [xmin, ymin, xmax, ymax], \"category_id\": bbox_label}</code>.</p> \n<h2>Inference with the TensorFlow object detection algorithm</h2> \n<p>The generated models can be hosted for inference and support encoded .jpg, .jpeg, and .png image formats as the <code>application/x-image</code> content type. The input image is resized automatically. The output contains the boxes, predicted classes, and scores for each prediction. The TensorFlow object detection model processes a single image per request and outputs only one line in the JSON. The following is an example of a response in JSON:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">accept: application/json;verbose\n\n{\"normalized_boxes\":[[xmin1, xmax1, ymin1, ymax1],....], \"classes\":[classidx1, class_idx2,...], \"scores\":[score_1, score_2,...], \"labels\": [label1, label2, ...], \"tensorflow_model_output\":<span style=\"color: #ff0000\">&lt;original output of the model&gt;</span>}</code></pre> \n</div> \n<p>If <code>accept</code> is set to <code>application/json</code>, then the model only outputs predicted boxes, classes, and scores. For more details on training and inference, see the sample notebook <a href=\"https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/object_detection_tensorflow/Amazon_Tensorflow_Object_Detection.ipynb\" target=\"_blank\" rel=\"noopener\">Introduction to SageMaker TensorFlow – Object Detection</a>.</p> \n<h2>Use SageMaker built-in algorithms through the JumpStart UI</h2> \n<p>You can also use SageMaker TensorFlow object detection and any of the other built-in algorithms with a few clicks via the JumpStart UI. JumpStart is a SageMaker feature that allows you to train and deploy built-in algorithms and pre-trained models from various ML frameworks and model hubs through a graphical interface. It also allows you to deploy fully fledged ML solutions that string together ML models and various other AWS services to solve a targeted use case.</p> \n<p>Following are two videos that show how you can replicate the same fine-tuning and deployment process we just went through with a few clicks via the JumpStart UI.</p> \n<h2>Fine-tune the pre-trained model</h2> \n<p>Here is the process to fine-tune the same pre-trained object detection model.</p> \n<div style=\"width: 640px;\" class=\"wp-video\">\n <video class=\"wp-video-shortcode\" id=\"video-45329-1\" width=\"640\" height=\"360\" preload=\"metadata\" controls=\"controls\">\n  <source type=\"video/mp4\" src=\"https://d2908q01vomqb2.cloudfront.net/artifacts/DBSBlogs/ML-11671/train_od.mp4?_=1\">\n </video>\n</div> \n<h2>Deploy the finetuned model</h2> \n<p>After model training is finished, you can directly deploy the model to a persistent, real-time endpoint with one click.</p> \n<div style=\"width: 640px;\" class=\"wp-video\">\n <video class=\"wp-video-shortcode\" id=\"video-45329-2\" width=\"640\" height=\"360\" preload=\"metadata\" controls=\"controls\">\n  <source type=\"video/mp4\" src=\"https://d2908q01vomqb2.cloudfront.net/artifacts/DBSBlogs/ML-11671/deploy_od.mp4?_=2\">\n </video>\n</div> \n<h2>Conclusion</h2> \n<p>In this post, we announced the launch of the SageMaker TensorFlow object detection built-in algorithm. We provided example code on how to do transfer learning on a custom dataset using a pre-trained model from TensorFlow using this algorithm.</p> \n<p>For more information, check out <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection-tensorflow.html\" target=\"_blank\" rel=\"noopener\">documentation</a> and the <a href=\"https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/object_detection_tensorflow/Amazon_Tensorflow_Object_Detection.ipynb\" target=\"_blank\" rel=\"noopener\">example notebook</a>.</p> \n<hr> \n<h3>About the authors</h3> \n<p style=\"clear: both\"><strong><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/08/09/Dr.-Vivek-Madan-.jpg\"><img loading=\"lazy\" class=\"size-full wp-image-40837 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/08/09/Dr.-Vivek-Madan-.jpg\" alt=\"\" width=\"100\" height=\"133\"></a>Dr. Vivek Madan</strong> is an Applied Scientist with the <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker JumpStart team</a>. He got his PhD from University of Illinois at Urbana-Champaign and was a Post Doctoral Researcher at Georgia Tech. He is an active researcher in machine learning and algorithm design and has published papers in EMNLP, ICLR, COLT, FOCS, and SODA conferences.</p> \n<p style=\"clear: both\"><strong><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/05/10/João-Moura.jpg\"><img loading=\"lazy\" class=\"size-full wp-image-36285 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/05/10/João-Moura.jpg\" alt=\"\" width=\"100\" height=\"150\"></a>João Moura</strong> is an AI/ML Specialist Solutions Architect at Amazon Web Services. He is mostly focused on NLP use cases and helping customers optimize deep learning model training and deployment. He is also an active proponent of low-code ML solutions and ML-specialized hardware.</p> \n<p style=\"clear: both\"><strong><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/05/10/Ashish-Khetan.jpg\"><img loading=\"lazy\" class=\"size-full wp-image-36284 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/05/10/Ashish-Khetan.jpg\" alt=\"\" width=\"100\" height=\"132\"></a>Dr. Ashish Khetan</strong> is a Senior Applied Scientist with <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker built-in algorithms</a> and helps develop machine learning algorithms. He got his PhD from University of Illinois Urbana Champaign. He is an active researcher in machine learning and statistical inference and has published many papers in NeurIPS, ICML, ICLR, JMLR, ACL, and EMNLP conferences.</p>",
  "enclosure": [
    "",
    ""
  ]
}