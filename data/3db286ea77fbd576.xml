<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>The SAS Data Science Blog</title>
	<atom:link href="https://blogs.sas.com/content/subconsciousmusings/feed/" rel="self" type="application/rss+xml" />
	<link>https://blogs.sas.com/content/subconsciousmusings/</link>
	<description>Advanced analytics from SAS data scientists</description>
	<lastBuildDate>Tue, 01 Nov 2022 16:15:25 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.9.3</generator>
	<item>
		<title>Risk driver analysis for complex supply chains</title>
		<link>https://blogs.sas.com/content/subconsciousmusings/2022/11/01/risk-driver-analysis-for-complex-supply-chains/</link>
					<comments>https://blogs.sas.com/content/subconsciousmusings/2022/11/01/risk-driver-analysis-for-complex-supply-chains/#respond</comments>
		
		<dc:creator><![CDATA[Xi Jiang]]></dc:creator>
		<pubDate>Tue, 01 Nov 2022 16:15:50 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[Analytics R&D]]></category>
		<category><![CDATA[sensitivity analysis]]></category>
		<category><![CDATA[supply chain]]></category>
		<guid isPermaLink="false">https://blogs.sas.com/content/subconsciousmusings/?p=14087</guid>

					<description><![CDATA[<p>SAS' Bahar Biller and Xi Jiang use the example of a semiconductor manufacturing plant to illustrate the role of sensitivity analysis in assessing supply chain risk.</p>
<p>The post <a rel="nofollow" href="https://blogs.sas.com/content/subconsciousmusings/2022/11/01/risk-driver-analysis-for-complex-supply-chains/">Risk driver analysis for complex supply chains</a> appeared first on <a rel="nofollow" href="https://blogs.sas.com/content/subconsciousmusings">The SAS Data Science Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p>Authors: <a href="https://blogs.sas.com/content/author/baharbiller/">Bahar Biller</a> and <a href="https://blogs.sas.com/content/author/xijiang/">Xi Jiang</a></p>
<p>The ability to identify the key drivers of risk is critical for the successful management of all supply chains. A technique enabling the identification of risk drivers in complex supply chains is sensitivity analysis. Specifically, sensitivity analysis (SA) quantifies the impact of variations in system inputs on key performance indicators (KPIs). Thus, when coupled with supply chain KPI prediction, SA is the tool used to empower supply chain practitioners to identify the key supply chain risk drivers. It further enhances the understanding of supply chain performance. It also helps determine what drives the cost and presents guidance on input management.</p>
<h2>Use case: semiconductor manufacturing plant (fab)</h2>
<p>Consider a semiconductor wafer fab with the process-flow illustrated in Figure 1. The process consists of two basic steps, Diffusion and Lithography. Each contains multiple sub-steps. In this wafer fab, which operates 24/7, cassettes are released at a constant rate of one per hour. The raw cassette begins at the Clean station. It passes through a full cycle of diffusion and lithography steps multiple times before leaving the system. The times spent at Oxidize, Coat, and Stepper are assumed to be deterministic. The processing times at Clean, Load, Unload, and Develop stations (marked with a star in Figure 1) are variable. Their distribution types and mean values are presented in Table 1.</p>
<div id="attachment_14096" style="width: 951px" class="wp-caption aligncenter"><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/october-biller-complex-supply-chains-figure-1.png"><img aria-describedby="caption-attachment-14096" class="wp-image-14096 size-full" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/october-biller-complex-supply-chains-figure-1.png" alt="Supply chains Figure 1 Process flow diagram of a semiconductor manufacturing plant" width="941" height="479" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/october-biller-complex-supply-chains-figure-1.png 941w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/october-biller-complex-supply-chains-figure-1-300x153.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/october-biller-complex-supply-chains-figure-1-768x391.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/october-biller-complex-supply-chains-figure-1-164x82.png 164w" sizes="(max-width: 941px) 100vw, 941px" /></a><p id="caption-attachment-14096" class="wp-caption-text">Figure 1 Process flow diagram of a semiconductor manufacturing plant</p></div>
<p>A KPI of interest in this use case is the flow time. Flow time is the amount of time it takes to complete the manufacturing processes from the release of a cassette to the finish. We mimic how cassettes flow through this manufacturing facility with the help of a stochastic simulation. We use the simulated process-flow output data to obtain the risk profile of the flow time. Then we execute the simulation for 900 replications, each with a run length of seven days (168 hours).</p>
<table width="100%">
<tbody>
<tr>
<td><strong>Station</strong></td>
<td><strong>Machine Count</strong></td>
<td><strong>Distribution</strong></td>
<td><strong>Mean (Hours)</strong></td>
</tr>
<tr>
<td>Clean</td>
<td>9</td>
<td>Exponential</td>
<td>1.821</td>
</tr>
<tr>
<td>Load</td>
<td>1</td>
<td>Lognormal</td>
<td>0.283</td>
</tr>
<tr>
<td>Oxidize</td>
<td>11</td>
<td>Constant</td>
<td>2</td>
</tr>
<tr>
<td>Unload</td>
<td>1</td>
<td>Lognormal</td>
<td>0.283</td>
</tr>
<tr>
<td>Coat</td>
<td>7</td>
<td>Constant</td>
<td>1.5</td>
</tr>
<tr>
<td>Stepper</td>
<td>6</td>
<td>Constant</td>
<td>1.5</td>
</tr>
<tr>
<td>Develop</td>
<td>3</td>
<td>Gamma</td>
<td>0.494</td>
</tr>
</tbody>
</table>
<p style="text-align: center">Table 1 Semiconductor manufacturing plant inputs</p>
<p>After quantifying the risk profile of the flow time KPI, it is important to study its sensitivity to means and standard deviations of uncertain processing times. As an example, if the mean processing time spent at the Clean station decreased by one hour, what would be the impact on the mean flow time? Or what would be the impact if the standard deviation of the cleaning time decreased by one hour? A way to answer these questions is by using a specific type of SA technique. This is known as the local sensitivity analysis.</p>
<h2>Sensitivity analysis: going beyond the traditional manufacturing KPI prediction</h2>
<p>Inputs of a simulation model represent real-world uncertainty at its most basic level. In our semiconductor manufacturing plant simulation, inputs include the number of machines available at each step. Also included are the processing times of these machines (that is, Machine Count, Distribution, and Mean (Hours)). Processing time can be deterministic or variable due to lack of information, errors of measurement, or estimation errors. Uncertainties in the inputs (for example, processing times) will imply uncertainty in the outputs (for example, flow time). SA studies how the model output responses are affected by the inputs to better understand system performance, quantify risk, or indicate where input change or management might be desirable.</p>
<p>Depending on the type of input and the goals of the analysis, SA methods can be grouped into two categories, global SA and local SA. Further subdivisions can exist within each. Here we focus on the local SA:</p>
<ul>
<li>Global SA imposes a distribution on each input factor based on prior knowledge or data. Then, the measured output variability caused by variation in the input factors (over the entire range of the possible values) is apportioned to each input factor as a measure of its contribution to output uncertainty. Consequently, measures of global SA provide qualitative guidance as to which inputs to control or study to reduce their uncertainty. And which inputs are not significant sources of output uncertainty to reduce the risk in estimating the output.</li>
</ul>
<p style="padding-left: 40px">In the case of the semiconductor wafer fab use case, global SA would apply if we know their ranges, for example, 1&lt;α&lt;10 and 10&lt;β&lt;15 but don’t know the values of the shape (α) and rate (β) of the gamma distribution modeling the processing time at the Develop station. Global SA would tell you which of these two parameters matters more qualitatively. However, if the goal were to conduct a what-if scenario analysis, then local SA, instead of global SA, would have to be used.</p>
<ul>
<li>Local SA focuses on the impact of small perturbations of the input on the outputs. This is often in the form of a partial derivative of the output with respect to the input. Because this type of sensitivity analysis applies only to an infinitesimal perturbation around the nominal setting, it is truly local. Consequently, it identifies the key risk drivers in the current system state and allows management to focus on a few of the many uncertain inputs. Through local SA, at SAS, we equip our customers with the ability to quantify the expected change in the mean KPI per unit change in the mean or standard deviation of the corresponding uncertain input. Within the context of the semiconductor wafer fab use case, we could compute the change in mean flow time in response to a change of one hour in the mean or standard deviation of the processing time spent at any of its stations.</li>
</ul>
<p style="padding-left: 40px">However, doing so is not a straightforward task when there are multiple ways to achieve a change in mean (μ) or standard deviation (α) because different changes in the input parameter vector might lead to the same change in μ  or α but different changes in the expected flow time. For example, this is the case with the gamma-distributed processing time at the Develop station. Fortunately, there exists a new family of local sensitivity measures that quantify the sensitivity of the output mean to a change in the input’s mean or standard deviation along a specified direction in the input-parameter space. A mathematical description of this quantification is available in a <a href="https://communities.sas.com/t5/Research-and-Science-from-SAS/SAS-Develops-Powerful-Tool-For-CROs-and-Pharmaceutical-Companies/ta-p/831448">technical paper</a> recently featured on the <a href="https://communities.sas.com/t5/Research-and-Science-from-SAS/tkb-p/science">SAS Science Community</a>. Next, we discuss the results of the local SA implementation in SAS for our semiconductor manufacturing use case.</p>
<h2>Results: flow time risk profile and sensitivity measures</h2>
<p>As illustrated in Figure 2, we have the analysis of the 900-replication simulation output data results in the flow time risk profile. It shows that the expected flow time is 24.07 hours with a standard deviation of 0.29 hours. This implies that the average flow time falls between 24.05 hours (that is., 24.07-1.96*0.29*(900)<sup>-1/2</sup>) and 24.09 hours (that is, 24.07+1.96*0.29*(900)<sup>-1/2</sup>) for 95% of the time. However, there is a 25% chance that the flow time falls below 23.88 hours as well as a 25% chance that the flow time exceeds 24.26 hours.</p>
<div id="attachment_14099" style="width: 449px" class="wp-caption aligncenter"><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/october-biller-complex-supply-chains-figure-2.png"><img aria-describedby="caption-attachment-14099" loading="lazy" class="wp-image-14099 size-full" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/october-biller-complex-supply-chains-figure-2.png" alt="Supply chains Figure 2 Flow time risk profile" width="439" height="299" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/october-biller-complex-supply-chains-figure-2.png 439w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/october-biller-complex-supply-chains-figure-2-300x204.png 300w" sizes="(max-width: 439px) 100vw, 439px" /></a><p id="caption-attachment-14099" class="wp-caption-text">Figure 2 Flow time risk profile</p></div>
<p>Next, we ask the following question. If we could improve the performance of a single station, which one of the Clean, Load, Unload, and Develop stations should we focus our effort on? Quantitatively, we pose eight different questions, two of which are as follows for the Clean station:</p>
<ol>
<li>If mean cleaning time decreased by one hour, what would be the impact on mean flow time?</li>
<li>If the standard deviation of the cleaning time decreased by one hour, what would be the impact?</li>
</ol>
<p>We repeat these questions for the processing times of the remaining three (Load, Unload, and Develop) stations. What is important to note here is that, instead of conducting eight different simulation experiments, the answers to these questions are obtained all at once by applying the local SA technique to the simulation output data. These have already led to the risk profile in Figure 2. We visualize the resulting sensitivity measures in Figure 3. These results have been obtained from the local sensitivity analyzer we have developed by using SAS <a href="https://go.documentation.sas.com/doc/en/pgmsascdc/v_032/ormpug/ormpug_optmodel_overview.htm">OPTMODEL</a> together with SAS <a href="https://go.documentation.sas.com/doc/en/pgmsascdc/9.4_3.3/statug/statug_nlmixed_overview.htm">NLMIXED</a>, <a href="https://go.documentation.sas.com/doc/en/pgmsascdc/9.4_3.3/statug/statug_reg_overview.htm">REG</a>, and <a href="https://go.documentation.sas.com/doc/en/pgmsascdc/v_031/imlug/imlug_imlstart_sect012.htm=">IML</a> procedures.</p>
<div id="attachment_14102" style="width: 712px" class="wp-caption aligncenter"><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/october-biller-complex-supply-chains-figure-3.png"><img aria-describedby="caption-attachment-14102" loading="lazy" class="size-large wp-image-14102" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/october-biller-complex-supply-chains-figure-3-1024x465.png" alt="" width="702" height="319" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/october-biller-complex-supply-chains-figure-3-1024x465.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/october-biller-complex-supply-chains-figure-3-300x136.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/october-biller-complex-supply-chains-figure-3-768x349.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/october-biller-complex-supply-chains-figure-3.png 1035w" sizes="(max-width: 702px) 100vw, 702px" /></a><p id="caption-attachment-14102" class="wp-caption-text">Figure 3: Flow time sensitivity measures</p></div>
<p>Figure 3 contains two plots, each of which presents sensitivity measures using tornado plots. Plot A presents the mean flow time sensitivity to the mean processing time. Plot B summarizes the mean flow time sensitivity to the standard deviation of processing time. Furthermore, an illustration of each measure is accompanied by an error bar showing the standard deviation interval on the estimated sensitivity.</p>
<p>We immediately see from Plot A that Develop and Clean are the two manufacturing steps that would reduce the mean flow time the most with improved mean processing times. A one-hour decrease in the Develop processing time would lead to a reduction of 3.6 hours in the mean flow time. A one-hour decrease in the Clean processing time would lead to a reduction of 3.2 hours in the mean flow time. On the other hand, improving the processing times at Load and Unload stations has a much smaller impact on flow time.</p>
<p>Plot B hints that mean flow time is not sensitive to the change in the standard deviations of the processing times at the Load and Unload stations. Therefore, these two stations are excluded from Plot B. It also shows that an hour reduction in the standard deviation of the processing time at the Develop station decreases the mean flow time by eight hours.</p>
<p>The same improvement in the processing time of the Clean station is expected to reduce the mean flow time by approximately three hours. On one hand, these results indicate the importance of quantifying the impact of input uncertainty on the flow time KPI. On the other hand, they provide us the last piece of information that will answer the question posed earlier. If there would be an opportunity to improve the processing time of a single manufacturing step in the fab, we would recommend restricting focus to the Develop station.</p>
<h2>Summary</h2>
<p>This post discusses the critical role of sensitivity analysis in supply chain risk analysis. As an example, we use a simplified semiconductor manufacturing plant. It helps explain that sensitivity quantification is critical. It answers what-if questions for business systems with complex process flows and exposure to high levels of input risk. The sensitivity measures suggest smart resource management and effort allocation strategies that are both time and cost-efficient for customers that might face supply chain disruptions brought on by the pandemic. They further provide guidance in minimizing risk and improving productivity as an important component of <a href="https://www.sas.com/en_ph/industry/manufacturing.html">manufacturing analytics</a>. At SAS, we are committed to equipping our customers with the power of <a href="https://www.sas.com/en_ph/solutions/analytics.html">advanced analytics</a> to help them succeed on this journey.</p>
<p>The post <a rel="nofollow" href="https://blogs.sas.com/content/subconsciousmusings/2022/11/01/risk-driver-analysis-for-complex-supply-chains/">Risk driver analysis for complex supply chains</a> appeared first on <a rel="nofollow" href="https://blogs.sas.com/content/subconsciousmusings">The SAS Data Science Blog</a>.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://blogs.sas.com/content/subconsciousmusings/2022/11/01/risk-driver-analysis-for-complex-supply-chains/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			<enclosure url="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/bahar-semiconductor-plant-150x150.jpg" />
	</item>
		<item>
		<title>Deployment of a Multi-stage Computer Vision model with SAS Event Stream Processing</title>
		<link>https://blogs.sas.com/content/subconsciousmusings/2022/10/28/deployment-multi-stage-computer-vision-model-with-sas-esp/</link>
					<comments>https://blogs.sas.com/content/subconsciousmusings/2022/10/28/deployment-multi-stage-computer-vision-model-with-sas-esp/#respond</comments>
		
		<dc:creator><![CDATA[Dragos Coles]]></dc:creator>
		<pubDate>Fri, 28 Oct 2022 14:13:33 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[aerial imagery]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[CASL]]></category>
		<category><![CDATA[computer vision]]></category>
		<category><![CDATA[deep learning]]></category>
		<category><![CDATA[Drone]]></category>
		<category><![CDATA[event stream processing]]></category>
		<category><![CDATA[Image Classification]]></category>
		<category><![CDATA[machine learning]]></category>
		<category><![CDATA[object detection]]></category>
		<category><![CDATA[python]]></category>
		<guid isPermaLink="false">https://blogs.sas.com/content/subconsciousmusings/?p=13950</guid>

					<description><![CDATA[<p>I will show you how to deploy multi-stage deep learning (DL) models in SAS Event Stream Processing (ESP) and leverage ESP on Edge via Docker containers to identify events of interest.</p>
<p>The post <a rel="nofollow" href="https://blogs.sas.com/content/subconsciousmusings/2022/10/28/deployment-multi-stage-computer-vision-model-with-sas-esp/">Deployment of a Multi-stage Computer Vision model with SAS Event Stream Processing</a> appeared first on <a rel="nofollow" href="https://blogs.sas.com/content/subconsciousmusings">The SAS Data Science Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p><em>This article is a follow-up to <a href="https://blogs.sas.com/content/subconsciousmusings/2022/08/12/multi-stage-computer-vision-model-detect-aerial-objects/">Creating a Multi-stage Computer Vision model to detect objects on high-resolution imagery</a> by <a href="https://blogs.sas.com/content/author/neelav/">Neela Vengateshwaran</a> and <a href="https://blogs.sas.com/content/author/robertblanchard/">Robert Blanchard</a>.</em></p>
<h2>Overview</h2>
<p>If you’ve read <a href="https://blogs.sas.com/content/subconsciousmusings/2022/08/12/multi-stage-computer-vision-model-detect-aerial-objects/">Robert and Neela's first article in this series</a>, you already know that the star of our show is Dr. Taco. He is a mischievous little dog who is always up to something fun like stealing a treat… even chocolate! We all know that chocolate is not good for dogs, so we need to stop him and keep him safe. Dr. Taco also loves accessorizing with different colored collars and scarves and we can use his fashion sense to our advantage. In this article, I will show you how to deploy multi-stage deep learning (DL) models in <a href="https://www.sas.com/en_us/software/event-stream-processing.html">SAS Event Stream Processing (ESP)</a> and leverage ESP on Edge via Docker containers to identify events of interest. Then, we will talk about potential improvements and future possibilities.</p>
<h2>Intro/review of the DL models</h2>
<p>In the first part of this series, we saw how Neela and Robert used <a href="https://www.sas.com/en_us/software/visual-data-mining-machine-learning.html">SAS Visual Data Mining and Machine Learning Software (VDMML)</a> capabilities to develop two deep learning models: a YOLOv2 model to first find Dr. Taco inside an image and a ResNet-50 model to identify Dr. Taco regardless of if he's wearing a different color scarf. We now have a video source that comes from a high-resolution drone camera that records Dr. Taco's activities and watches out for his scarf color. Our goal is to deploy these 2-stage DL models to process a high-resolution drone video capture. In stage one (the YOLOv2 model), we will find Dr. Taco, frame him, extract the frame and shrink it. Then, we will apply the second stage (the ResNet-50 model)  to catch the little thief in the act of stealing a chocolate treat and label the action according to Dr. Taco's collar color. If his collar is red, white or blue then he stole the treat, otherwise he hasn't stolen yet). To process and visualize the video stream, we will use SAS ESP deployed in a Docker container (with and without GPU support).</p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/projectSetup-screenshot.png"><img loading="lazy" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/projectSetup-screenshot-1024x579.png" alt="" width="702" height="397" class="aligncenter size-large wp-image-14342" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/projectSetup-screenshot-1024x579.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/projectSetup-screenshot-300x170.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/projectSetup-screenshot-768x434.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/projectSetup-screenshot.png 1401w" sizes="(max-width: 702px) 100vw, 702px" /></a></p>
<h2>How are we going to deploy the SAS DL models?</h2>
<p>We will use a SAS ESP Engine to create an event stream processing project, deploy our 2-stage models and score the video stream while annotating. To build this project we can use XML code or we can use the SAS ESP Studio user-friendly UI and build the project with point-and-click functions. This ESP engine can easily process many modeling objects including DL model ASTOREs.</p>
<h2>How are we going to deploy the SAS ESP Engine?</h2>
<p>As with all projects, the setup is the most important part. For Dr. Taco, the setup would just be us forgetting some chocolate on the table; stealing it would be so easy for him! In our case, setting up the containers with ESP is the most important step. We must ask ourselves some key questions. Do we need a GPU to process this? Do we have a high enough resolution and framerate? These factors will also determine if it will be better to run the container locally or in the cloud (Azure in this case). Our video source is 60 FPS with a resolution over 3000x2000 and we will deploy both locally and in the cloud, just to get a sense of the difference. We have a couple of deployment options to decide between…</p>
<h3>Option 1</h3>
<p>SAS ESP Engine in an Azure NVIDIA GPU Virtual Machine deployed in a Docker container. The stream viewer is the video source that pushes the video stream through the ESP engine and allows us to view both the original video and the annotated video stream side-by-side.</p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Option1.png"><img loading="lazy" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Option1-1024x579.png" alt="" width="702" height="397" class="aligncenter size-large wp-image-14204" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Option1-1024x579.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Option1-300x170.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Option1-768x434.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Option1-1536x869.png 1536w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Option1.png 1574w" sizes="(max-width: 702px) 100vw, 702px" /></a></p>
<p>We have a container for SAS ESP Studio UI which allows us to connect to different ESP servers, build our ESP project, deploy the DL models and use other stream processing features on events identified in the stream. We will deploy the SAS ESP Engine in a container inside an Azure NVIDIA GPU Virtual Machine and we will leverage the GPUs for faster processing. With this setup, the webUI that we use for the stream viewer will also be the source of our video stream, as the browser will be pushing the video through the ESP project via a little Python code.</p>
<p>For testing the ESP project, we will capture some "event" frames and use those as the source to ensure that our models are deployed correctly and that the events of interest are identified and processed correctly.</p>
<h3>Option 2</h3>
<p>SAS ESP Engine in a local Docker container using a custom videoCapture plugin to stream the process video directly from the raw MP4 video source, allowing us to view the annotated stream through an HTML viewer.</p>
<p>With this deployment option, the ESP project setup is almost identical to what we see in Option 1. The difference is that we are also defining an Input Publisher Connector to use the raw video as the video source instead of having an intermediary, the stream viewer, as the video source for the ESP project. We are not leveraging GPUs and therefore, we would have to accept that stream processing for this high-resolution video would be a bit slower. Using the input publisher will also allow us to view the stream in test mode.</p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Option2.png"><img loading="lazy" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Option2-1024x579.png" alt="" width="702" height="397" class="aligncenter size-large wp-image-14213" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Option2-1024x579.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Option2-300x170.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Option2-768x434.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Option2.png 1436w" sizes="(max-width: 702px) 100vw, 702px" /></a></p>
<p>With both options considered, we will proceed with Option 1.</p>
<h2>Configure the ESP project step by step</h2>
<h3>Step 1: ESP server connection and project variables</h3>
<p><strong>Connect to the ESP server on the VM</strong>, go to 'ESP Servers' and add new:</p>
<ul>
<li>name = <your name></li>
<li>host = FQDN from you Azure resource</li>
<li>port = <your http port></li>
</ul>
<p><strong>Create project variables</strong> because they will help us configure nodes more easily and change project parameters without having to remember full file system paths on the VM. Ensure that no node is selected in your project (if you have an empty screen, you're good to go), click on the 'project' icon button on the top right corner, go to 'User-Defined Properties' and create the following variables. Make sure to assign your model names to the model variables and the file_input_name.</p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step1.png"><img loading="lazy" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step1.png" alt="" width="864" height="552" class="aligncenter size-full wp-image-14225" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step1.png 864w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step1-300x192.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step1-768x491.png 768w" sizes="(max-width: 864px) 100vw, 864px" /></a></p>
<h3>Step 2: Building the ESP Studio Project</h3>
<h4>Step 2.1 - Source Window -1- 'droneFootage'</h4>
<p>We need this window for testing purposes only. To configure, add a source window node and assign the following properties:</p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.1.png"><img loading="lazy" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.1-1024x363.png" alt="" width="702" height="249" class="aligncenter size-large wp-image-14228" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.1-1024x363.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.1-300x106.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.1-768x272.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.1-1536x545.png 1536w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.1.png 1890w" sizes="(max-width: 702px) 100vw, 702px" /></a></p>
<h4>Step 2.2 - Functional Window -2- ‘addTimeStamp’ (optional)</h4>
<p>This is to capture the timestamp for the event identified. To configure, add a functional window node and assign the following properties:</p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.2.png"><img loading="lazy" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.2-1024x419.png" alt="" width="702" height="287" class="aligncenter size-large wp-image-14231" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.2-1024x419.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.2-300x123.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.2-768x314.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.2-1536x629.png 1536w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.2.png 1940w" sizes="(max-width: 702px) 100vw, 702px" /></a></p>
<h4>Step 2.3 - Calculate Window -3- ‘resize’</h4>
<p>We need to resize each high-resolution frame to adhere to YOLOv2's maximum of 416x416. The Calculate windows support multiple image processing functions and 'resize' and 'crop' are 2 of the functions supported. To configure, add a calculate window and assign the following properties:</p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.3.png"><img loading="lazy" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.3-1024x490.png" alt="" width="702" height="336" class="aligncenter size-large wp-image-14234" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.3-1024x490.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.3-300x144.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.3-768x368.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.3-1536x735.png 1536w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.3-1078x516.png 1078w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.3-702x336.png 702w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.3.png 1822w" sizes="(max-width: 702px) 100vw, 702px" /></a></p>
<h4>Step 2.4 - Model Reader Window -4- 'yoloReader'</h4>
<p>This is where we configure the use of our first ASTORE model in the 2-stage model process. The YOLOv2 model will score and help us find Dr. Taco and put a rectangular frame around the 'event' in the subsequent scoring step. To configure, add a model reader window and assign the following properties:</p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.4.png"><img loading="lazy" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.4-1024x491.png" alt="" width="702" height="337" class="aligncenter size-large wp-image-14237" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.4-1024x491.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.4-300x144.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.4-768x368.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.4-1536x737.png 1536w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.4-1078x516.png 1078w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.4-702x336.png 702w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.4.png 1960w" sizes="(max-width: 702px) 100vw, 702px" /></a></p>
<h4>Step 2.5 - Scoring Window -5-  'isTacoThere'</h4>
<p>This is where we start scoring the video feed against our first model (YOLOv2) to find Dr. Taco. To configure, add a score window and assign the following properties:</p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.5.png"><img loading="lazy" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.5-1024x580.png" alt="" width="702" height="398" class="aligncenter size-large wp-image-14240" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.5-1024x580.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.5-300x170.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.5-768x435.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.5.png 1532w" sizes="(max-width: 702px) 100vw, 702px" /></a></p>
<p>If we test our project now, we will be able to find Dr. Taco (thank you YOLOv2 model) and track him in the video stream:</p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/TrackingDrTaco.png"><img loading="lazy" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/TrackingDrTaco-1024x490.png" alt="" width="702" height="336" class="aligncenter size-large wp-image-14243" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/TrackingDrTaco-1024x490.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/TrackingDrTaco-300x143.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/TrackingDrTaco-768x367.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/TrackingDrTaco-1536x735.png 1536w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/TrackingDrTaco-1078x516.png 1078w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/TrackingDrTaco-702x336.png 702w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/TrackingDrTaco.png 1560w" sizes="(max-width: 702px) 100vw, 702px" /></a></p>
<h4>Step 2.6 - Calculate Window – ‘cropTaco’</h4>
<p>Now, we need to crop out Dr. Taco around the rectangle frame identified and set up by the YOLOv2 model. We will use YOLO coordinates because it will normalize the data so we don’t have to worry about resolution. The Calculate windows support multiple image processing functions and 'resize' and 'crop' are 2 of the functions supported. To configure, add a calculate window and assign the following properties:</p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.6.png"><img loading="lazy" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.6-1024x624.png" alt="" width="702" height="428" class="aligncenter size-large wp-image-14246" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.6-1024x624.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.6-300x183.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.6-768x468.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.6-1536x937.png 1536w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.6.png 1748w" sizes="(max-width: 702px) 100vw, 702px" /></a></p>
<h4>Step 2.7 - Calculate Window -7- 'resnetResize'</h4>
<p> we need to resize each high-resolution frame to adhere to ResNet-50's maximum of 224x224 using the RECT coordinate type. The Calculate windows support multiple image processing functions and 'resize' and 'crop' are 2 of the functions supported. To configure, add a calculate window and assign the following properties:</p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.7.png"><img loading="lazy" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.7-1024x662.png" alt="" width="702" height="454" class="aligncenter size-large wp-image-14249" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.7-1024x662.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.7-300x194.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.7-768x496.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.7-1536x993.png 1536w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.7.png 1584w" sizes="(max-width: 702px) 100vw, 702px" /></a></p>
<h4>Step 2.8 - Model Reader Window -8- 'resnetReader'</h4>
<p>This is where we configure the use of our second ASTORE model in the 2-stage model process. The ResNet-50 model will score and help us find what Dr. Taco is wearing and it will generate the second label for our annotation in the subsequent scoring step. To configure, add a calculate window and assign the following properties:</p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.8.png"><img loading="lazy" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.8-1024x708.png" alt="" width="702" height="485" class="aligncenter size-large wp-image-14252" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.8-1024x708.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.8-300x207.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.8-768x531.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.8.png 1498w" sizes="(max-width: 702px) 100vw, 702px" /></a></p>
<h4>Step 2.9 - Scoring Window -9- 'findTheScarf'</h4>
<p>This is where we start scoring the video feed against our second model (ResNet-50) to find the color of Dr. Taco's scarf. To configure, add a calculate window and assign the following properties:</p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.9.png"><img loading="lazy" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.9-1024x704.png" alt="" width="702" height="483" class="aligncenter size-large wp-image-14255" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.9-1024x704.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.9-300x206.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.9-768x528.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.9-1536x1056.png 1536w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.9.png 1556w" sizes="(max-width: 702px) 100vw, 702px" /></a></p>
<p>Testing the model now and outputting the stream from the ‘findTheScarf’ window, we see still only what the YOLO model gives us: Dr. Taco without the additional annotation of the scarf color. To get the classification from the ResNet model on the scarf Dr. Taco is wearing, we need one more functional window in the project…</p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/FindTheScarf.png"><img loading="lazy" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/FindTheScarf-1024x571.png" alt="" width="702" height="391" class="aligncenter size-large wp-image-14258" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/FindTheScarf-1024x571.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/FindTheScarf-300x167.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/FindTheScarf-768x428.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/FindTheScarf.png 1356w" sizes="(max-width: 702px) 100vw, 702px" /></a></p>
<h4>Step 2.10 - Functional Window -10- 'annotateTaco'</h4>
<p>Apply a concatenate function between the labels of the 2 models: 'drTaco' and 'scarf color'. To configure, add a calculate window and assign the following properties:</p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.10.png"><img loading="lazy" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.10-1024x715.png" alt="" width="702" height="490" class="aligncenter size-large wp-image-14261" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.10-1024x715.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.10-300x209.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.10-768x536.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.10-1536x1072.png 1536w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Step2.10.png 1642w" sizes="(max-width: 702px) 100vw, 702px" /></a></p>
<p>In the final test, we now know what Dr. Taco is doing!</p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/dr_taco_white_scarf.png"><img loading="lazy" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/dr_taco_white_scarf.png" alt="" width="936" height="423" class="aligncenter size-full wp-image-14264" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/dr_taco_white_scarf.png 1022w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/dr_taco_white_scarf-300x136.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/dr_taco_white_scarf-768x347.png 768w" sizes="(max-width: 936px) 100vw, 936px" /></a></p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/dr_taco_no_scarf.png"><img loading="lazy" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/dr_taco_no_scarf.png" alt="" width="936" height="525" class="aligncenter size-full wp-image-14267" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/dr_taco_no_scarf.png 1042w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/dr_taco_no_scarf-300x168.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/dr_taco_no_scarf-1024x574.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/dr_taco_no_scarf-768x430.png 768w" sizes="(max-width: 936px) 100vw, 936px" /></a></p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/dr_taco_blue_scarf.png"><img loading="lazy" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/dr_taco_blue_scarf.png" alt="" width="936" height="456" class="aligncenter size-full wp-image-14273" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/dr_taco_blue_scarf.png 936w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/dr_taco_blue_scarf-300x146.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/dr_taco_blue_scarf-768x374.png 768w" sizes="(max-width: 936px) 100vw, 936px" /></a></p>
<h2>Step 3: Testing</h2>
<p>To test this ESP project before deploying it to stream and score through the webUI, we need a different data source. We will use the same 'droneFootage' source window, but we will activate the input data publisher connector that we defined earlier in Step 2.1 which will make the source a Base64 converted file of the screenshot created in Step 0. The Base64 file should be at this location - '…/ESPAssets/Input'.</p>
<p><strong>Run test:</strong></p>
<ul>
<li>Set publisher to 'Active'</li>
<li>Enter test mode</li>
<li>Run and monitor the source and the annotate windows.</li>
</ul>
<h2>Step 4: Running the ESP project</h2>
<p>To deploy a new demo within the same infrastructure the IoT repo:</p>
<ul>
<li>All new assets will have been moved under "…/ComputerVisionUI" locations</li>
<li>The ESP project XML file will need to be named 'Model.xml' and place under new folder location for the project under<br />
“…/ComputerVisionUI/WebUI/Projects" (this should have already been completed in Step 0)</li>
<li><strong>Important!!!</strong> Before exporting the project XML from ESP Studio, uncheck the 'Activate' option for the input publisher connector defined in the source window</li>
<li>To add a new project to the webUI, make sure it can read from the new project Example</li>
<li>Place videos to stream in the video source location under “…/ComputerVisionUI/WebUI"</li>
</ul>
<p>Now, we are ready to run our project:  </p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Run-Project.gif"><img loading="lazy" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Run-Project.gif" alt="" width="800" height="429" class="aligncenter size-full wp-image-14300" /></a></p>
<h2>Final stream and conclusions</h2>
<p>A quick recap:</p>
<ul>
<li>Neela and Robert used SAS Deep Learning technology to develop a 2-stage model to track down the thief Dr. Taco. They created a YOLO model to find and box-bind Dr. Taco’s activities. Once Dr. Taco was identified, they used a ResNet deep learning model to determine what Dr. Taco did by the color of the scarf he was wearing (we are going to great lengths to connect these dots; whatever color scarf Dr. Taco is wearing, that is the color of the chocolate that he stole).</li>
<li>Once these models were developed, our expert modelers created the analytic stores of the models that they shared with me to implement and deploy through SAS ESP.</li>
<li>To deploy those models, we first needed access to an ESP environment, and luckily, we have access to a couple. We have an ESP Engine on Edge container via a GPU-enabled Azure VM which gives us the option to experiment with GPUs or CPUs for processing the video stream. We also deployed ESP on Edge on a local container via Docker (no GPUs here though).</li>
<li>We also have ESP Studio deployed in a local container which allows us to configure and connect to any ESP engine. ESP studio simplifies the creation of the ESP project, the deployment of the DL models and has point-and-click functions specific to manipulating and processing a video stream.</li>
<li>Now, we are finally able to find Dr. Taco, track him and annotate his actions to the point that we now know exactly what he is up to.</li>
</ul>
<h3>Learn more</h3>
<p>For more information on SAS Deep Learning and Event Stream Processing capabilities, please follow these links and don’t hesitate to contact us if you have any questions. Thanks for reading!</p>
<ul>
<li><a href="https://blogs.sas.com/content/subconsciousmusings/2020/04/06/getting-started-with-deep-learning-using-the-sas-language/">Getting started with deep learning using the SAS Language</a></li>
<li><a href="https://go.documentation.sas.com/doc/en/espcdc/6.2/espov/home.htm">What Is SAS Event Stream Processing?</a></li>
<li><a href="https://support.sas.com/documentation/prod-p/vdmml/zip/">Deep Learning Models and Tools</a></li>
</ul>
<p>The post <a rel="nofollow" href="https://blogs.sas.com/content/subconsciousmusings/2022/10/28/deployment-multi-stage-computer-vision-model-with-sas-esp/">Deployment of a Multi-stage Computer Vision model with SAS Event Stream Processing</a> appeared first on <a rel="nofollow" href="https://blogs.sas.com/content/subconsciousmusings">The SAS Data Science Blog</a>.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://blogs.sas.com/content/subconsciousmusings/2022/10/28/deployment-multi-stage-computer-vision-model-with-sas-esp/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			<enclosure url="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/dr_taco_blue_scarf-150x150.png" />
	</item>
		<item>
		<title>Find similar words in ESG data with Natural Language Processing</title>
		<link>https://blogs.sas.com/content/subconsciousmusings/2022/10/20/similar-words-esg-data-nlp/</link>
					<comments>https://blogs.sas.com/content/subconsciousmusings/2022/10/20/similar-words-esg-data-nlp/#respond</comments>
		
		<dc:creator><![CDATA[Meilan Ji]]></dc:creator>
		<pubDate>Thu, 20 Oct 2022 13:00:30 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[Analytics R&D]]></category>
		<category><![CDATA[Corpus Analysis]]></category>
		<category><![CDATA[data culture and fluency]]></category>
		<category><![CDATA[data scientist]]></category>
		<category><![CDATA[ESG]]></category>
		<category><![CDATA[Natural Language Generation (NLG)]]></category>
		<category><![CDATA[natural language processing]]></category>
		<category><![CDATA[Text analytics]]></category>
		<guid isPermaLink="false">https://blogs.sas.com/content/subconsciousmusings/?p=13956</guid>

					<description><![CDATA[<p>Using such features and Natural Language Processing capabilities like text parsing and information extraction in SAS Visual Text Analytics (VTA) helps us uncover emerging trends and unlock the value of unstructured text data. </p>
<p>The post <a rel="nofollow" href="https://blogs.sas.com/content/subconsciousmusings/2022/10/20/similar-words-esg-data-nlp/">Find similar words in ESG data with Natural Language Processing</a> appeared first on <a rel="nofollow" href="https://blogs.sas.com/content/subconsciousmusings">The SAS Data Science Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p>When building information extraction models using rule-based approaches, one challenge is discovering the right vocabulary to add to the rule set. What if we could use a similarity metric to discover such words and quickly expand our model coverage? Using such features and Natural Language Processing capabilities like text parsing and information extraction in <a href="https://support.sas.com/en/software/visual-text-analytics-support.html">SAS Visual Text Analytics</a> helps us uncover emerging trends and unlock the value of unstructured text data.</p>
<p>As a data scientist, I want to build a model to extract information about company performance related to ESG metrics. ESG stands for Environmental, Social, and Governance, and these metrics and related strategies go hand in hand with strong financial performance. Investors want to know what ESG areas that companies are focusing on and what improvements they are targeting. I will build a model to discover this information from corporate ESG reports. In this blog post, I will explore how to use SAS Visual Text Analytics and <a href="https://support.sas.com/en/software/studio-support.html">SAS Studio</a> interactively to achieve this goal.</p>
<h2>Prepare the data and term list</h2>
<p>The data consists of various company reports, such as CRI reports, CSR reports, Env Reports, 10K reports, etc. obtained from their official websites. Before feeding them to the model, the data was pre-processed including file conversion (convert PDF into text), splitting into paragraphs, and removing duplicates and noises. For data variables, I extracted and kept the “Company” and “Year” variables in the data for trend analysis. Finally, I came up with a 26MB-sized dataset, comprised of 157 reports for the companies of interest. I uploaded this data to <a href="https://support.sas.com/en/software/sas-data-preparation-support.html">SAS Data Explorer</a> and visualized it using <a href="https://support.sas.com/en/software/visual-analytics-support.html">SAS Visual Analytics</a>.</p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Data-overview.png"><img loading="lazy" class="aligncenter size-large wp-image-13991" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Data-overview-1024x754.png" alt="" width="702" height="517" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Data-overview-1024x754.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Data-overview-300x221.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Data-overview-768x566.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Data-overview.png 1472w" sizes="(max-width: 702px) 100vw, 702px" /></a></p>
<p>I have a rough idea of how the data is distributed and want to gain more insights from the text data. For example, what areas of ESG do companies focus on most? How has their reporting changed in the past 5 years?</p>
<p>To answer the questions, I collected a starting list of ESG terms from ESG-related websites.</p>
<table>
<tbody>
<tr>
<td><i><span style="padding-right: 20px">Environmental</span><br />
(21)</i></td>
<td><i>environment, environmental, sustainable, sustainability, biodiversity, renewable, green, climate, material, energy, wind, solar, forest, water, waste, carbon, emission, effluent, pollutant, hazardous, disposal</i></td>
</tr>
<tr>
<td><i>Social<br />
(26)<br />
</i></td>
<td><i>reputation, ethic, ethics, labor, employment, employee, compensation, pay, occupational, health, safety, equity, equal, fairness, transparent, transparency, bias, training, education, diverse, diversity, discrimination, nondiscrimination, freedom, minority, woman</i></td>
</tr>
<tr>
<td><i>Governance<br />
(17)<br />
</i></td>
<td><i>compliance, regulation, planning, value, economic, financial, business, strategy, performance, risk, innovation, stakeholder, management, process, opportunity, responsible, responsibilityl</i></td>
</tr>
</tbody>
</table>
<p>&nbsp;</p>
<p>This term list is not large. And the terms like "emission" and "energy" may have hyponyms such as "air emission, co2 emission", "solar energy, wind energy", etc. Also, when checking the terms on the Text Parsing node I found some high-frequency terms that were not on my list, such as "recycle, electricity, security". It seems that this list is not sufficient to cover the breadth of the ESG topic areas. I want to extend the term lists in the model with the relevant ones used in their reports without reading through all the data we collected.</p>
<p>Next, let’s explore how to expand the terms on SAS Visual Text Analytics and SAS Studio.</p>
<h2>Find similar words on SAS Visual Text Analytics</h2>
<p>The Text Parsing node on SAS Visual Text Analytics finds all the terms in my data and groups the variants of terms via word tokenization and lemmatization. In addition, multiword concepts are detected as noun groups and added to the terms list. Another function, “Find similar”, finds terms in the training data that share a similar context with the target vocabulary and assigns them a similarity score. The similarity scores indicate how likely it is that other terms appear in the same context as a selected term.</p>
<p>Now, I am using this function to expand our vocabulary. The table below shows the similar words for the search term “carbon” sorted by similarity score. We can see that similar words for the word ‘carbon’ contain many carbon-related phrases used in the data, such as “carbon offset, carbon emission, carbon footprint, carbon neutrality” and so on. Also, we can find the words that do not contain “carbon”, such as “footprint, net zero goal, removal solution”, etc., but are somewhat related.</p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Similarities-for-the-term-carbon.png"><img loading="lazy" class="aligncenter size-large wp-image-13997" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Similarities-for-the-term-carbon-1024x478.png" alt="" width="702" height="328" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Similarities-for-the-term-carbon-1024x478.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Similarities-for-the-term-carbon-300x140.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Similarities-for-the-term-carbon-768x359.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Similarities-for-the-term-carbon.png 1512w" sizes="(max-width: 702px) 100vw, 702px" /></a></p>
<p>The similar words generated by the “Find Similar” function behind the SAS Visual Text Analytics look helpful although we need to manually filter out some less relevant words.</p>
<p>Can we utilize this function to programmatically get similar words for a word list in batch? Before answering this question, let’s see how it is working behind the scenes. You’ll find the key points in the log for the “Text Parsing” node. Here, the tpAccumulate action outputs a 'parent' table that contains a compressed representation of the sparse term-by-document matrix and a 'term' table that contains the summary information about the terms in the document collection. The tmSvd reduces the dimensionality of the matrix into reduced dimensions using these two tables. Then, the tmFindSimilar action computes similarity scores of terms based on singular value decomposition (SVD) projections.</p>
<h2>Find similar words programmatically in SAS Studio</h2>
<p>Armed with this understanding, we can move to SAS Studio for programming. Here, you can write SAS or Python programs as you like. I will use Python to demonstrate.</p>
<h3>Import libraries and connect to CAS server</h3>
<p>Connect to the CAS server of the working environment and import the Python SWAT package to run CAS actions.</p>

<div class="wp_syntax"><table><tr><td class="code"><pre class="sas" style="font-family:monospace;"># Import libraries
<span style="color: #0000ff;">from</span> swat import <span style="color: #006400; font-style: italic;">*
import pandas as pd
&nbsp;
# Connect to CAS server
host = 'sas-cas-server-default-client'
&nbsp;
s = CAS(host , 5570, username, password, timeout=1e7)
s.sessionprop.setsessopt(caslib='CASUSER', timeout=1e7)
&nbsp;
# Load CAS action sets
s.loadactionset('textManagement')
s.loadactionset('textMining')
s.loadactionset('textUtil')
s.loadactionset('fedSql')
s.loadactionset('textParse')</span></pre></td></tr></table></div>

<h3>Generate Document Term Matrix using tmMine action</h3>
<p>You can write the program by referring to the code snippet from the log. I used the tmMine action which combines the tpParse, tpAccumulate, and SVD functionality into one. This action helps to make the code more concise.</p>

<div class="wp_syntax"><table><tr><td class="code"><pre class="sas" style="font-family:monospace;"># Terms <span style="color: #0000ff;">and</span> word projection tables are created using the tmMine action
s.tmMine<span style="color: #66cc66;">&#40;</span>docId=<span style="color: #a020f0;">'did'</span>,
         text=<span style="color: #a020f0;">'text'</span>,
         documents=dict<span style="color: #66cc66;">&#40;</span>name=<span style="color: #a020f0;">'ALL_REPORTS_ID'</span><span style="color: #66cc66;">&#41;</span>,
         terms=dict<span style="color: #66cc66;">&#40;</span>name=<span style="color: #a020f0;">'terms'</span>, <span style="color: #0000ff;">replace</span>=True<span style="color: #66cc66;">&#41;</span>,
         parent=dict<span style="color: #66cc66;">&#40;</span>name=<span style="color: #a020f0;">'parent'</span>, <span style="color: #0000ff;">replace</span>=True<span style="color: #66cc66;">&#41;</span>,
         reduce=<span style="color: #2e8b57; font-weight: bold;">4</span>,
         k=<span style="color: #2e8b57; font-weight: bold;">160</span>,
         wordPro=dict<span style="color: #66cc66;">&#40;</span>name=<span style="color: #a020f0;">'svdout'</span>, <span style="color: #0000ff;">replace</span>=True<span style="color: #66cc66;">&#41;</span><span style="color: #66cc66;">&#41;</span></pre></td></tr></table></div>

<h3>Convert Terms table to SAS data frame and create term to termnum map</h3>
<p>Because the tmFindSimilar action searches by term ID, we need to get the term IDs for the seed term list. To do this, I converted the ‘terms’ table to a SASDataFrame and created a dictionary to map terms and term IDs. The terms have different term IDs for their different part-of-speech tags. Thus, one term may have more than one term ID.</p>

<div class="wp_syntax"><table><tr><td class="code"><pre class="sas" style="font-family:monospace;"># <span style="color: #0000ff;">Create</span> term-to-termnum map
df_terms = s.CASTable<span style="color: #66cc66;">&#40;</span>name=<span style="color: #a020f0;">'terms'</span><span style="color: #66cc66;">&#41;</span>.to_frame<span style="color: #66cc66;">&#40;</span><span style="color: #66cc66;">&#41;</span>
dict_termnum = <span style="color: #66cc66;">&#123;</span><span style="color: #66cc66;">&#125;</span>
for term, termnum <span style="color: #0000ff;">in</span> <span style="color: #0000ff;">list</span><span style="color: #66cc66;">&#40;</span>zip<span style="color: #66cc66;">&#40;</span>df_terms._Term_, df_terms._Termnum_<span style="color: #66cc66;">&#41;</span><span style="color: #66cc66;">&#41;</span>:
    <span style="color: #0000ff;">if</span> term <span style="color: #0000ff;">in</span> dict_termnum:
        dict_termnum<span style="color: #66cc66;">&#91;</span>term<span style="color: #66cc66;">&#93;</span>.<span style="color: #0000ff;">add</span><span style="color: #66cc66;">&#40;</span>termnum<span style="color: #66cc66;">&#41;</span>
    <span style="color: #0000ff;">else</span>:
        dict_termnum<span style="color: #66cc66;">&#91;</span>term<span style="color: #66cc66;">&#93;</span> = <span style="color: #0000ff;">set</span><span style="color: #66cc66;">&#40;</span><span style="color: #66cc66;">&#41;</span>
        dict_termnum<span style="color: #66cc66;">&#91;</span>term<span style="color: #66cc66;">&#93;</span>.<span style="color: #0000ff;">add</span><span style="color: #66cc66;">&#40;</span>termnum<span style="color: #66cc66;">&#41;</span></pre></td></tr></table></div>

<h3>Find top N similar words</h3>
<p>Next, I created a findSimilar function that utilizes the tmFindSimilar action to generate the top N similar words for a targeted word. I also have some ideas on narrowing down the candidate terms given that:</p>
<ul>
<li>They are likely not <a href="https://en.wikipedia.org/wiki/Stop_word">stop words</a></li>
<li>They are more likely to be nouns, noun phrases, adjectives or verbs</li>
</ul>

<div class="wp_syntax"><table><tr><td class="code"><pre class="sas" style="font-family:monospace;">def findSimilar<span style="color: #66cc66;">&#40;</span>termid, model=<span style="color: #a020f0;">'svdout'</span>, <span style="color: #0000ff;">n</span>=<span style="color: #2e8b57; font-weight: bold;">20</span><span style="color: #66cc66;">&#41;</span>:
    s.tmFindSimilar<span style="color: #66cc66;">&#40;</span><span style="color: #0000ff;">table</span>=dict<span style="color: #66cc66;">&#40;</span>name=model<span style="color: #66cc66;">&#41;</span>,
                    termnum=termid,
                    nSVD=<span style="color: #2e8b57; font-weight: bold;">160</span>,
                    casout=dict<span style="color: #66cc66;">&#40;</span>name=<span style="color: #a020f0;">'casout'</span>, <span style="color: #0000ff;">replace</span>=True<span style="color: #66cc66;">&#41;</span>,
                    prefix=<span style="color: #a020f0;">'col'</span><span style="color: #66cc66;">&#41;</span>
&nbsp;
    s.fedSql.execDirect<span style="color: #66cc66;">&#40;</span>casout=dict<span style="color: #66cc66;">&#40;</span>name=<span style="color: #a020f0;">'outsimilarTerms'</span>, <span style="color: #0000ff;">replace</span>=True<span style="color: #66cc66;">&#41;</span>,
                    query=<span style="color: #a020f0;">&quot;select a._term_, a._role_, b.* from (select * from terms where _ispar_ != '.' and _keep_='Y' and _role_ in ('N','nlpNounGroup','A', 'V')) a join casout b on a._termnum_ =b._termnum_ &quot;</span><span style="color: #66cc66;">&#41;</span>
&nbsp;
    df_similar = s.CASTable<span style="color: #66cc66;">&#40;</span>name=<span style="color: #a020f0;">'outsimilarTerms'</span>,<span style="color: #66cc66;">&#41;</span>.sort_values<span style="color: #66cc66;">&#40;</span><span style="color: #0000ff;">by</span>=<span style="color: #a020f0;">&quot;_Similar_&quot;</span>, ascending=False<span style="color: #66cc66;">&#41;</span>.to_frame<span style="color: #66cc66;">&#40;</span><span style="color: #66cc66;">&#41;</span>.head<span style="color: #66cc66;">&#40;</span><span style="color: #0000ff;">n</span><span style="color: #66cc66;">&#41;</span>
&nbsp;
    <span style="color: #0000ff;">return</span> df_similar</pre></td></tr></table></div>

<p>Now, I can use the findSimilar function to generate similar words for a word list and rank them by the times they are recommended. Also, I can exclude the candidates belonging to the seed list and those where the length is shorter than 4 characters.</p>

<div class="wp_syntax"><table><tr><td class="code"><pre class="sas" style="font-family:monospace;">def findSimilarSet<span style="color: #66cc66;">&#40;</span>word_list, dict_termnum<span style="color: #66cc66;">&#41;</span>:
    cand_dic = dict<span style="color: #66cc66;">&#40;</span><span style="color: #66cc66;">&#41;</span>
    for word <span style="color: #0000ff;">in</span> word_list:
        <span style="color: #0000ff;">if</span> word <span style="color: #0000ff;">in</span> dict_termnum:
            termids = dict_termnum<span style="color: #66cc66;">&#91;</span>word<span style="color: #66cc66;">&#93;</span>
            for termid <span style="color: #0000ff;">in</span> termids:
                try:
                    df_tmp = findSimilar<span style="color: #66cc66;">&#40;</span>termid<span style="color: #66cc66;">&#41;</span>
                    for term <span style="color: #0000ff;">in</span> <span style="color: #0000ff;">list</span><span style="color: #66cc66;">&#40;</span>df_tmp._Term_<span style="color: #66cc66;">&#41;</span>:
                        <span style="color: #0000ff;">if</span> <span style="color: #0000ff;">not</span> term <span style="color: #0000ff;">in</span> word_list <span style="color: #0000ff;">and</span> len<span style="color: #66cc66;">&#40;</span>term<span style="color: #66cc66;">&#41;</span><span style="color: #0000ff; font-weight: bold;">&amp;gt</span>;<span style="color: #2e8b57; font-weight: bold;">4</span>:
                            <span style="color: #0000ff;">if</span> term <span style="color: #0000ff;">in</span> cand_dic:
                                cand_dic<span style="color: #66cc66;">&#91;</span>term<span style="color: #66cc66;">&#93;</span> += <span style="color: #2e8b57; font-weight: bold;">1</span>
                            <span style="color: #0000ff;">else</span>:
                                cand_dic<span style="color: #66cc66;">&#91;</span>term<span style="color: #66cc66;">&#93;</span> = <span style="color: #2e8b57; font-weight: bold;">1</span>
                except:
                    pass
    cand_dic = dict<span style="color: #66cc66;">&#40;</span>sorted<span style="color: #66cc66;">&#40;</span>cand_dic.items<span style="color: #66cc66;">&#40;</span><span style="color: #66cc66;">&#41;</span>, <span style="color: #0000ff;">key</span>=lambda item: item<span style="color: #66cc66;">&#91;</span><span style="color: #2e8b57; font-weight: bold;">1</span><span style="color: #66cc66;">&#93;</span>, <span style="color: #0000ff;">reverse</span>=True<span style="color: #66cc66;">&#41;</span><span style="color: #66cc66;">&#41;</span>
    <span style="color: #0000ff;">return</span> cand_dic</pre></td></tr></table></div>

<p>All set. Let’s try with the environmental term list.</p>

<div class="wp_syntax"><table><tr><td class="code"><pre class="sas" style="font-family:monospace;">ESG_terms = <span style="color: #66cc66;">&#91;</span><span style="color: #a020f0;">&quot;environment&quot;</span>, <span style="color: #a020f0;">&quot;environmental&quot;</span>, <span style="color: #a020f0;">&quot;sustainable&quot;</span>, 
             <span style="color: #a020f0;">&quot;sustainability&quot;</span>, <span style="color: #a020f0;">&quot;biodiversity&quot;</span>, <span style="color: #a020f0;">&quot;renewable&quot;</span>, <span style="color: #a020f0;">&quot;green&quot;</span>, 
             <span style="color: #a020f0;">&quot;climate&quot;</span>, <span style="color: #a020f0;">&quot;material&quot;</span>, <span style="color: #a020f0;">&quot;energy&quot;</span>, <span style="color: #a020f0;">&quot;wind&quot;</span>, <span style="color: #a020f0;">&quot;solar&quot;</span>, <span style="color: #a020f0;">&quot;forest&quot;</span>, 
             <span style="color: #a020f0;">&quot;water&quot;</span>, <span style="color: #a020f0;">&quot;waste&quot;</span>, <span style="color: #a020f0;">&quot;carbon&quot;</span>, <span style="color: #a020f0;">&quot;emission&quot;</span>, <span style="color: #a020f0;">&quot;effluent&quot;</span>, <span style="color: #a020f0;">&quot;pollutant&quot;</span>, 
              <span style="color: #a020f0;">&quot;hazardous&quot;</span>, <span style="color: #a020f0;">&quot;disposal&quot;</span><span style="color: #66cc66;">&#93;</span>
&nbsp;
cand_dic=findSimilarSet<span style="color: #66cc66;">&#40;</span>ESG_terms, dict_termnum<span style="color: #66cc66;">&#41;</span>
&nbsp;
print<span style="color: #66cc66;">&#40;</span><span style="color: #a020f0;">'%s similar terms generated in total'</span> %len<span style="color: #66cc66;">&#40;</span>cand_dic.items<span style="color: #66cc66;">&#40;</span><span style="color: #66cc66;">&#41;</span><span style="color: #66cc66;">&#41;</span><span style="color: #66cc66;">&#41;</span>
similar_dic = <span style="color: #66cc66;">&#123;</span><span style="color: #a020f0;">&quot;similar_term&quot;</span>:<span style="color: #66cc66;">&#91;</span><span style="color: #66cc66;">&#93;</span>, <span style="color: #a020f0;">&quot;recommended_times&quot;</span>:<span style="color: #66cc66;">&#91;</span><span style="color: #66cc66;">&#93;</span>, <span style="color: #a020f0;">&quot;term_frequency&quot;</span>:<span style="color: #66cc66;">&#91;</span><span style="color: #66cc66;">&#93;</span><span style="color: #66cc66;">&#125;</span>
for word, freq <span style="color: #0000ff;">in</span> cand_dic.items<span style="color: #66cc66;">&#40;</span><span style="color: #66cc66;">&#41;</span>:
    similar_dic<span style="color: #66cc66;">&#91;</span><span style="color: #a020f0;">&quot;similar_term&quot;</span><span style="color: #66cc66;">&#93;</span>.append<span style="color: #66cc66;">&#40;</span>word<span style="color: #66cc66;">&#41;</span>
    similar_dic<span style="color: #66cc66;">&#91;</span><span style="color: #a020f0;">&quot;recommended_times&quot;</span><span style="color: #66cc66;">&#93;</span>.append<span style="color: #66cc66;">&#40;</span>freq<span style="color: #66cc66;">&#41;</span>
&nbsp;
print<span style="color: #66cc66;">&#40;</span>pd.DataFrame.from_dict<span style="color: #66cc66;">&#40;</span>similar_dic<span style="color: #66cc66;">&#41;</span>.head<span style="color: #66cc66;">&#40;</span><span style="color: #2e8b57; font-weight: bold;">20</span><span style="color: #66cc66;">&#41;</span><span style="color: #66cc66;">&#41;</span></pre></td></tr></table></div>

<p>Output:</p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Output.png"><img loading="lazy" class="alignnone size-full wp-image-14027" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Output.png" alt="" width="544" height="392" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Output.png 1008w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Output-300x216.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Output-768x553.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Output-536x386.png 536w" sizes="(max-width: 544px) 100vw, 544px" /></a></p>
<p>It generated 635 similar words for this term list. “Renewable energy” was recommended 8 times and ‘solar energy’ was recommended 6 times. The table below shows similar words with a frequency greater than 2, which means they are recommended more than two times.</p>
<h3>Similar words for Environmental term list (Frequency &gt; 2)</h3>
<table>
<tbody>
<tr>
<td><i>renewable energy<br />
energy generation<br />
solar energy<br />
unplanned water<br />
energy project<br />
utility-scale<br />
power capacity<br />
energy initiative<br />
neutral<br />
<span style="padding-right: 20px">corporate renewable energy</span><br />
renewable power<br />
nonhazardous waste</i></td>
<td><i>wind farm<br />
solar array<br />
solar farm<br />
energy match<br />
clean<br />
<span style="padding-right: 20px">carbon reduction</span><br />
carbon emission<br />
surface water<br />
freshwater<br />
water stress<br />
landfill<br />
final disposal<br />
</i></td>
<td><i>non-hazardous<br />
nonhazardous<br />
hazardous waste<br />
air emission<br />
sustainability goal<br />
electricity demand<br />
grid mix<br />
geothermal<br />
<span style="padding-right: 20px">renewable electricity</span><br />
energy deal<br />
new wind<br />
wind energy</i></td>
<td><i>new solar farm<br />
solar power<br />
water-related<br />
water consumption<br />
water withdrawal<br />
water usage<br />
<span style="padding-right: 20px">waste management program</span><br />
carbon offset<br />
carbon footprint<br />
dioxide<br />
carbon negative<br />
footprint</i></td>
</tr>
</tbody>
</table>
<p>The similar words generated for the environmental word list look promising! In addition, using a word list instead of a single word to generate similar words allows more “important” words to come first. I only need to review the suggested vocabulary list and filter out some irrelevant words.</p>
<p>Similarly, I ended up with 717 and 542 similar words for my Governance and Social term list respectively.</p>
<h2>Information extraction model and visualization</h2>
<p>Now I have a richer vocabulary after a quick check and filtering out irrelevant words. Next, I want to build an information extraction model on the SAS Visual Text Analytics Concept node to continue the exploration. For such unstructured text data without any labels, the LITI rule-based model is a good choice. I can also take advantage of the predefined concepts such as nlpPercent, nlpMoney, and nlpMeasure to help identify ESG-related metrics. Then, create fact rules using ESG terms and metrics concepts. With expanded ESG-related terms, the model returned 1.7k more fact matches. This number wasn't noticeable since the refined terms didn't contribute much to the number. For example, the "energy" initially returned 825 results, while the refined model only returned 26 more results. But what I see in the results are more granular terms, such as "renewable energy, clean energy, solar energy, carbon-free energy", etc.</p>
<p>After some post-processing of the results, I was ready to create a report in Visual Analytics. On the left part, I added a bar chart with the frequency of company on the ESG report and a pie chart with the frequency of ESG category. On the right part, I added a line chart to show how their reports changed over the years, and a word cloud with the frequency of ESG terms. Then connected these two charts with the bar chart and pie chart. Also, I added and connected a list table with the text, which was the ESG category, terms and metrics arguments, and the matched text span.</p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Frequency-of-ESG.png"><img loading="lazy" class="aligncenter size-large wp-image-14000" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Frequency-of-ESG-1024x468.png" alt="" width="702" height="321" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Frequency-of-ESG-1024x468.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Frequency-of-ESG-300x137.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Frequency-of-ESG-768x351.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Frequency-of-ESG-1536x702.png 1536w, https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Frequency-of-ESG-2048x936.png 2048w" sizes="(max-width: 702px) 100vw, 702px" /></a></p>
<p>Based on the visualization, IBM and Microsoft reported the most ESG-related metrics. This is somewhat consistent with the data distribution. When clicking on the companies in the bar chart to view the details, I found IBM focused most on "energy" and "electricity" and Microsoft focused most on "business" and "indenture". Also, I can see how the reports changed in the past 5 years from the line chart. By clicking on the ESG pie chart, I can check which areas the companies are most concerned about in each category.</p>
<h2>Summary</h2>
<p>This blog post aimed to provide a strategy for expanding terms using the SAS similar word detection technique to facilitate model development. The exploration with ESG data showed this approach works well in expanding domain-related vocabularies even on a relatively small amount of data. When I experimented with different data volumes with this method of term expanding, it showed that larger data is more likely to yield better results. Moreover, you can consider such extensions by leveraging open-source pre-trained word embedding models that are trained on large corpora. For expanding a word list that is not domain-related, for example, the expansion of a sentiment word list, using a word embedding model trained on more extensive data will achieve better results. To leverage the pre-trained models, you only need to import the pre-trained word embedding to SAS Data Explorer and use the example code shown above to find similar words. My experiments showed that this method also works well in expanding sentiment vocabularies. Try it out on your project and let us know what you find!</p>
<h3>Learn more</h3>
<ul>
<li>Check out additional documentation on <a href="https://go.documentation.sas.com/doc/us/pgmsascdc/v_021/casanpg/p1e93w56dte67qn14kcy9njkxs1p.htm">corpus analysis</a> and for <a href="https://support.sas.com/en/software/visual-text-analytics-support.html#documentation">SAS Visual Text Analytics</a>.</li>
<li>Keep exploring by checking out the ebook “<a href="https://www.sas.com/en/whitepapers/natural-language-processing-110641.html">Make Every Voice Heard with Natural Language Processing</a>” or try <a href="https://www.sas.com/en_us/trials/software/viya/viya-trial-form.html">SAS Viya</a> for free.</li>
</ul>
<p>The post <a rel="nofollow" href="https://blogs.sas.com/content/subconsciousmusings/2022/10/20/similar-words-esg-data-nlp/">Find similar words in ESG data with Natural Language Processing</a> appeared first on <a rel="nofollow" href="https://blogs.sas.com/content/subconsciousmusings">The SAS Data Science Blog</a>.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://blogs.sas.com/content/subconsciousmusings/2022/10/20/similar-words-esg-data-nlp/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			<enclosure url="https://blogs.sas.com/content/subconsciousmusings/files/2022/10/Frequency-of-ESG-150x150.png" />
	</item>
		<item>
		<title>Find duplicates and near-duplicates in a corpus with Natural Language Processing</title>
		<link>https://blogs.sas.com/content/subconsciousmusings/2022/10/03/find-duplicates-with-nlp/</link>
					<comments>https://blogs.sas.com/content/subconsciousmusings/2022/10/03/find-duplicates-with-nlp/#comments</comments>
		
		<dc:creator><![CDATA[Estelle Wang]]></dc:creator>
		<pubDate>Mon, 03 Oct 2022 15:32:05 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[Analytics R&D]]></category>
		<category><![CDATA[Corpus Analysis]]></category>
		<category><![CDATA[data culture and fluency]]></category>
		<category><![CDATA[data scientist]]></category>
		<category><![CDATA[Natural Language Generation (NLG)]]></category>
		<category><![CDATA[natural language processing]]></category>
		<category><![CDATA[SAS Visual Text Analytics]]></category>
		<category><![CDATA[Text analytics]]></category>
		<guid isPermaLink="false">https://blogs.sas.com/content/subconsciousmusings/?p=13791</guid>

					<description><![CDATA[<p>To find exact duplicates, matching all string pairs is the simplest approach, but it is not a very efficient or sufficient technique. Using the MD5 or SHA-1 hash algorithms can get us a correct outcome with a faster speed, yet near-duplicates would still not be on the radar. Text similarity is useful for finding files that look alike. There are various approaches to this and each of them has its own way to define documents that are considered duplicates. Furthermore, the definition of duplicate documents has implications for the type of processing and the results produced. Below are some of the options.</p>
<p>Using SAS Visual Text Analytics, you can customize and accomplish this task during your corpus analysis journey either with Python SWAT package or with PROC SQL in SAS.</p>
<p>The post <a rel="nofollow" href="https://blogs.sas.com/content/subconsciousmusings/2022/10/03/find-duplicates-with-nlp/">Find duplicates and near-duplicates in a corpus with Natural Language Processing</a> appeared first on <a rel="nofollow" href="https://blogs.sas.com/content/subconsciousmusings">The SAS Data Science Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p>Building a large high-quality corpus for <a href="https://www.sas.com/en_us/insights/analytics/what-is-natural-language-processing-nlp.html">Natural Language Processing (NLP)</a> is not for the faint of heart. Text data can be large, cumbersome, and unwieldy and unlike clean numbers or categorical data in rows and columns, discerning differences between documents can be challenging. In organizations where documents are shared, modified, and shared again before being saved in an archive, the problem of duplication can become overwhelming.</p>
<p>To find exact duplicates, matching all string pairs is the simplest approach, but it is not a very efficient or sufficient technique. Using the <a href="https://en.wikipedia.org/wiki/MD5">MD5</a> or <a href="https://en.wikipedia.org/wiki/SHA-1">SHA-1</a> hash algorithms can get us a correct outcome with a faster speed, yet near-duplicates would still not be on the radar. Text similarity is useful for finding files that look alike. There are various approaches to this and each of them has its own way to define documents that are considered duplicates. Furthermore, the definition of duplicate documents has implications for the type of processing and the results produced. Below are some of the options.</p>
<p>Using <a href="https://www.sas.com/en_us/software/visual-text-analytics.html">SAS Visual Text Analytics</a>, you can customize and accomplish this task during your corpus analysis journey either with Python SWAT package or with PROC SQL in SAS.</p>
<h2>Work with Python SWAT</h2>
<p>The Python SWAT package provides us with a Python interface to <a href="https://documentation.sas.com/doc/en/pgmsascdc/9.4_3.5/lrcon/n0bc1qjcm0ieegn15og9lfelk7pm.htm">SAS Cloud Analytic Services (CAS)</a>. In this article, we’ll call the profileText action, pull down output tables, and perform duplicate identification in Python.</p>
<h3>Prepare the data</h3>
<p>The corpus we’re going to explore is <a href="https://anc.org/">Second Release of the American National Corpus (ANC2)</a>. It’s also one of the reference corpora for the profileText action. The corpus contains over 22,000,000 words of written and spoken texts and comes with both annotated data and their plain texts.</p>
<p>We put all 13295 plain text files under /home/anc2. After connecting to the CAS server, we create the table TESTDATA with ANC2 data.</p>

<div class="wp_syntax"><table><tr><td class="code"><pre class="python" style="font-family:monospace;"><span style="color: #808080; font-style: italic;"># Import libraries</span>
<span style="color: #ff7700;font-weight:bold;">import</span> swat
<span style="color: #ff7700;font-weight:bold;">from</span> <span style="color: #dc143c;">collections</span> <span style="color: #ff7700;font-weight:bold;">import</span> Counter
<span style="color: #ff7700;font-weight:bold;">import</span> pandas <span style="color: #ff7700;font-weight:bold;">as</span> pd
<span style="color: #ff7700;font-weight:bold;">import</span> <span style="color: #dc143c;">itertools</span>
<span style="color: #ff7700;font-weight:bold;">import</span> <span style="color: #dc143c;">random</span>
&nbsp;
<span style="color: #808080; font-style: italic;"># Connect to CAS server</span>
s <span style="color: #66cc66;">=</span> swat.<span style="color: black;">CAS</span><span style="color: black;">&#40;</span><span style="color: #483d8b;">&quot;cloud.example.com&quot;</span><span style="color: #66cc66;">,</span> <span style="color: #ff4500;">5570</span><span style="color: black;">&#41;</span>
&nbsp;
<span style="color: #808080; font-style: italic;"># Add the caslib mycas with the path to corpus directory</span>
s.<span style="color: black;">addCaslib</span><span style="color: black;">&#40;</span>caslib<span style="color: #66cc66;">=</span><span style="color: #483d8b;">'mycas'</span><span style="color: #66cc66;">,</span>
            datasource<span style="color: #66cc66;">=</span><span style="color: black;">&#123;</span><span style="color: #483d8b;">&quot;srcType&quot;</span>:<span style="color: #483d8b;">&quot;path&quot;</span><span style="color: black;">&#125;</span><span style="color: #66cc66;">,</span>
            session<span style="color: #66cc66;">=</span><span style="color: #008000;">False</span><span style="color: #66cc66;">,</span>
            path<span style="color: #66cc66;">=</span><span style="color: #483d8b;">&quot;/home&quot;</span><span style="color: #66cc66;">,</span>
            subdirectories<span style="color: #66cc66;">=</span><span style="color: #483d8b;">&quot;yes&quot;</span><span style="color: black;">&#41;</span>
&nbsp;
<span style="color: #808080; font-style: italic;"># Load txt files under anc/ to the CASTable testdata</span>
s.<span style="color: black;">loadTable</span><span style="color: black;">&#40;</span>casout<span style="color: #66cc66;">=</span><span style="color: black;">&#123;</span><span style="color: #483d8b;">&quot;name&quot;</span>:<span style="color: #483d8b;">&quot;TESTDATA&quot;</span><span style="color: #66cc66;">,</span> <span style="color: #483d8b;">&quot;replace&quot;</span>:<span style="color: #008000;">True</span><span style="color: black;">&#125;</span><span style="color: #66cc66;">,</span>
            caslib<span style="color: #66cc66;">=</span><span style="color: #483d8b;">&quot;mycas&quot;</span><span style="color: #66cc66;">,</span>
            importOptions<span style="color: #66cc66;">=</span><span style="color: black;">&#123;</span><span style="color: #483d8b;">&quot;fileType&quot;</span>:<span style="color: #483d8b;">&quot;Document&quot;</span><span style="color: black;">&#125;</span><span style="color: #66cc66;">,</span>
            path<span style="color: #66cc66;">=</span><span style="color: #483d8b;">&quot;anc2&quot;</span><span style="color: black;">&#41;</span></pre></td></tr></table></div>

<p>Out:</p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out1.png"><img loading="lazy" class="aligncenter size-large wp-image-13794" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out1-1024x276.png" alt="" width="702" height="189" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out1-1024x276.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out1-300x81.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out1-768x207.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out1.png 1380w" sizes="(max-width: 702px) 100vw, 702px" /></a></p>
<p>We can check on the table easily, such as by using columnInfo() or head().</p>

<div class="wp_syntax"><table><tr><td class="code"><pre class="python" style="font-family:monospace;"><span style="color: #808080; font-style: italic;"># View column summary for testdata</span>
anc2 <span style="color: #66cc66;">=</span> s.<span style="color: black;">CASTable</span><span style="color: black;">&#40;</span><span style="color: #483d8b;">&quot;TESTDATA&quot;</span><span style="color: #66cc66;">,</span> replace<span style="color: #66cc66;">=</span><span style="color: #008000;">True</span><span style="color: black;">&#41;</span>
anc2.<span style="color: black;">columninfo</span><span style="color: black;">&#40;</span><span style="color: black;">&#41;</span></pre></td></tr></table></div>

<p>Out:</p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out2.png"><img loading="lazy" class="aligncenter size-large wp-image-13797" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out2-1024x282.png" alt="" width="702" height="193" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out2-1024x282.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out2-300x82.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out2-768x211.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out2-1536x422.png 1536w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out2.png 1564w" sizes="(max-width: 702px) 100vw, 702px" /></a></p>

<div class="wp_syntax"><table><tr><td class="code"><pre class="python" style="font-family:monospace;"><span style="color: #808080; font-style: italic;"># Check on the first five rows</span>
anc2.<span style="color: black;">head</span><span style="color: black;">&#40;</span><span style="color: black;">&#41;</span></pre></td></tr></table></div>

<p>Out:</p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out3.png"><img loading="lazy" class="aligncenter size-large wp-image-13800" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out3-1024x172.png" alt="" width="702" height="118" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out3-1024x172.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out3-300x50.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out3-768x129.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out3-1536x258.png 1536w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out3-2048x344.png 2048w" sizes="(max-width: 702px) 100vw, 702px" /></a></p>
<h3>Profile the data</h3>
<p>We load the Text Management action set and call the profileText action to profile the ANC2 data. The casOut parameter is required to run the action. This output table contains information complexity, information density, and vocabulary diversity statistics. For duplicate identification we need the results from two output tables, documentOut and intermediateOut. A CASTable can be converted to a SASDataFrame by using the CASTable.to_frame() method. This method helps us pull all of the data down for further exploration.</p>

<div class="wp_syntax"><table><tr><td class="code"><pre class="python" style="font-family:monospace;"><span style="color: #808080; font-style: italic;"># Load the action set textManagement</span>
s.<span style="color: black;">loadactionset</span><span style="color: black;">&#40;</span><span style="color: #483d8b;">'textManagement'</span><span style="color: black;">&#41;</span>
&nbsp;
<span style="color: #808080; font-style: italic;"># Call the action profileText</span>
results <span style="color: #66cc66;">=</span> s.<span style="color: black;">profileText</span><span style="color: black;">&#40;</span>table<span style="color: #66cc66;">=</span><span style="color: #008000;">dict</span><span style="color: black;">&#40;</span>caslib<span style="color: #66cc66;">=</span><span style="color: #483d8b;">&quot;mycas&quot;</span><span style="color: #66cc66;">,</span> name<span style="color: #66cc66;">=</span><span style="color: #483d8b;">&quot;testdata&quot;</span><span style="color: black;">&#41;</span><span style="color: #66cc66;">,</span>
                        documentid<span style="color: #66cc66;">=</span><span style="color: #483d8b;">&quot;fileName&quot;</span><span style="color: #66cc66;">,</span>
                        text<span style="color: #66cc66;">=</span><span style="color: #483d8b;">&quot;content&quot;</span><span style="color: #66cc66;">,</span>
                        language<span style="color: #66cc66;">=</span><span style="color: #483d8b;">&quot;english&quot;</span><span style="color: #66cc66;">,</span>
                        casOut<span style="color: #66cc66;">=</span><span style="color: #008000;">dict</span><span style="color: black;">&#40;</span>name<span style="color: #66cc66;">=</span><span style="color: #483d8b;">&quot;casOut&quot;</span><span style="color: #66cc66;">,</span> replace<span style="color: #66cc66;">=</span><span style="color: #008000;">True</span><span style="color: black;">&#41;</span><span style="color: #66cc66;">,</span>
                        documentOut<span style="color: #66cc66;">=</span><span style="color: #008000;">dict</span><span style="color: black;">&#40;</span>name<span style="color: #66cc66;">=</span><span style="color: #483d8b;">&quot;docOut&quot;</span><span style="color: #66cc66;">,</span> replace<span style="color: #66cc66;">=</span><span style="color: #008000;">True</span><span style="color: black;">&#41;</span><span style="color: #66cc66;">,</span>
                        intermediateOut<span style="color: #66cc66;">=</span><span style="color: #008000;">dict</span><span style="color: black;">&#40;</span>name<span style="color: #66cc66;">=</span><span style="color: #483d8b;">&quot;interOut&quot;</span><span style="color: #66cc66;">,</span> replace<span style="color: #66cc66;">=</span><span style="color: #008000;">True</span><span style="color: black;">&#41;</span><span style="color: black;">&#41;</span></pre></td></tr></table></div>

<p>The documentOut contains document-level information complexity statistics. For each file, we know their total number of sentences and maximum number of tokens in these sentences.</p>

<div class="wp_syntax"><table><tr><td class="code"><pre class="python" style="font-family:monospace;"><span style="color: #808080; font-style: italic;"># Convert the CASTable docOut to SASDataFrame</span>
df_docout <span style="color: #66cc66;">=</span> s.<span style="color: black;">CASTable</span><span style="color: black;">&#40;</span><span style="color: #483d8b;">'docOut'</span><span style="color: black;">&#41;</span>.<span style="color: black;">to_frame</span><span style="color: black;">&#40;</span><span style="color: black;">&#41;</span>
df_docout.<span style="color: black;">head</span><span style="color: black;">&#40;</span><span style="color: black;">&#41;</span></pre></td></tr></table></div>

<p>Out:</p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out4.png"><img loading="lazy" class="aligncenter size-large wp-image-13803" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out4-1024x177.png" alt="" width="702" height="121" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out4-1024x177.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out4-300x52.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out4-768x133.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out4-1536x265.png 1536w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out4-2048x354.png 2048w" sizes="(max-width: 702px) 100vw, 702px" /></a></p>
<p>The other output, intermediateOut, contains the token count of each sentence in each document.</p>

<div class="wp_syntax"><table><tr><td class="code"><pre class="python" style="font-family:monospace;"><span style="color: #808080; font-style: italic;"># Convert the CASTable interOut to SASDataFrame</span>
df_interout <span style="color: #66cc66;">=</span> s.<span style="color: black;">CASTable</span><span style="color: black;">&#40;</span><span style="color: #483d8b;">'interOut'</span><span style="color: black;">&#41;</span>.<span style="color: black;">to_frame</span><span style="color: black;">&#40;</span><span style="color: black;">&#41;</span>
df_interout.<span style="color: black;">head</span><span style="color: black;">&#40;</span><span style="color: black;">&#41;</span></pre></td></tr></table></div>

<p>Out:</p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out5.png"><img loading="lazy" class="aligncenter size-large wp-image-13806" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out5-1024x182.png" alt="" width="702" height="125" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out5-1024x182.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out5-300x53.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out5-768x136.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out5-1536x273.png 1536w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out5-2048x364.png 2048w" sizes="(max-width: 702px) 100vw, 702px" /></a></p>
<h3>Filter the data</h3>
<p>Our goal is to locate both identical documents and documents that are not identical but substantially similar. To narrow our search results for good candidates, we introduce an assumption that if two files have the same number of sentences and the maximum number of tokens for a sentence, they have a higher possibility to be duplicates or near-duplicates.</p>
<p>Having this assumption, we keep the documents with their value pair of _NUM_SENTENCES_ and _MAX_TOKENS_SENTENCES_ occurring more than once, leaving us 8972 out of 13295 files.</p>

<div class="wp_syntax"><table><tr><td class="code"><pre class="python" style="font-family:monospace;"><span style="color: #808080; font-style: italic;"># Filter out docs with their column values appearing more than once</span>
df_docout_selected <span style="color: #66cc66;">=</span> df_docout<span style="color: black;">&#91;</span>df_docout.<span style="color: black;">groupby</span><span style="color: black;">&#40;</span><span style="color: black;">&#91;</span><span style="color: #483d8b;">'_NUM_SENTENCES_'</span><span style="color: #66cc66;">,</span><span style="color: #483d8b;">'_MAX_TOKENS_SENTENCE_'</span><span style="color: black;">&#93;</span><span style="color: black;">&#41;</span>
                                                 <span style="color: black;">&#91;</span><span style="color: #483d8b;">'_NUM_SENTENCES_'</span><span style="color: black;">&#93;</span>.<span style="color: black;">transform</span><span style="color: black;">&#40;</span><span style="color: #483d8b;">'size'</span><span style="color: black;">&#41;</span>&amp;gt<span style="color: #66cc66;">;</span><span style="color: #ff4500;">1</span><span style="color: black;">&#93;</span>
<span style="color: #ff7700;font-weight:bold;">print</span><span style="color: black;">&#40;</span>f<span style="color: #483d8b;">&quot;Number of rows after selection: {len(df_docout_selected)}&quot;</span><span style="color: black;">&#41;</span>
df_docout_selected.<span style="color: black;">head</span><span style="color: black;">&#40;</span><span style="color: black;">&#41;</span></pre></td></tr></table></div>

<p>Out:</p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out6.png"><img loading="lazy" class="aligncenter size-large wp-image-13809" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out6-1024x215.png" alt="" width="702" height="147" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out6-1024x215.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out6-300x63.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out6-768x162.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out6-1536x323.png 1536w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out6.png 1844w" sizes="(max-width: 702px) 100vw, 702px" /></a></p>
<p>You can further reduce results if there’s a focus in your search by providing conditions like, only selecting documents with a total number of sentences of more than 200, or selecting those with a maximum number of tokens in a sentence of more than 80.</p>

<div class="wp_syntax"><table><tr><td class="code"><pre class="python" style="font-family:monospace;"><span style="color: #808080; font-style: italic;"># (Optional) Reduce search results by filtering out docs by condition</span>
df_docout_selected<span style="color: #66cc66;">=</span>df_docout_selected<span style="color: black;">&#91;</span>df_docout_selected._NUM_SENTENCES_&amp;gt<span style="color: #66cc66;">;</span><span style="color: #ff4500;">200</span><span style="color: black;">&#93;</span>
df_docout_selected<span style="color: #66cc66;">=</span>df_docout_selected<span style="color: black;">&#91;</span>df_docout_selected._MAX_TOKENS_SENTENCE_&amp;gt<span style="color: #66cc66;">;</span><span style="color: #ff4500;">80</span><span style="color: black;">&#93;</span></pre></td></tr></table></div>

<p>Next, we prepare pair combinations of files that share the values for _NUM_SENTENCES_ and _MAX_TOKENS_SENTENCES_. Notice that sometimes more than 2 files share the same values. The total number of unique pairs is 14617.</p>

<div class="wp_syntax"><table><tr><td class="code"><pre class="python" style="font-family:monospace;"><span style="color: #808080; font-style: italic;"># Keep only the interout data for files that are selected</span>
search_dict <span style="color: #66cc66;">=</span> df_docout_selected.<span style="color: black;">set_index</span><span style="color: black;">&#40;</span><span style="color: #483d8b;">'fileName'</span><span style="color: black;">&#41;</span>.<span style="color: black;">T</span>.<span style="color: black;">to_dict</span><span style="color: black;">&#40;</span><span style="color: #483d8b;">'list'</span><span style="color: black;">&#41;</span>
df_interout_selected <span style="color: #66cc66;">=</span> df_interout<span style="color: black;">&#91;</span>df_interout<span style="color: black;">&#91;</span><span style="color: #483d8b;">'fileName'</span><span style="color: black;">&#93;</span>.<span style="color: black;">isin</span><span style="color: black;">&#40;</span>search_dict.<span style="color: black;">keys</span><span style="color: black;">&#40;</span><span style="color: black;">&#41;</span><span style="color: black;">&#41;</span><span style="color: black;">&#93;</span>
&nbsp;
<span style="color: #808080; font-style: italic;"># Get all unique combinations of every two docs</span>
check_tmp_dict <span style="color: #66cc66;">=</span> Counter<span style="color: black;">&#40;</span><span style="color: black;">&#91;</span><span style="color: #008000;">tuple</span><span style="color: black;">&#40;</span>s<span style="color: black;">&#41;</span> <span style="color: #ff7700;font-weight:bold;">for</span> s <span style="color: #ff7700;font-weight:bold;">in</span> search_dict.<span style="color: black;">values</span><span style="color: black;">&#40;</span><span style="color: black;">&#41;</span><span style="color: black;">&#93;</span><span style="color: black;">&#41;</span>
file_pair_lst <span style="color: #66cc66;">=</span> <span style="color: black;">&#91;</span><span style="color: black;">&#93;</span>
<span style="color: #ff7700;font-weight:bold;">for</span> c <span style="color: #ff7700;font-weight:bold;">in</span> check_tmp_dict:
    file_pair <span style="color: #66cc66;">=</span> <span style="color: black;">&#91;</span>k <span style="color: #ff7700;font-weight:bold;">for</span> k<span style="color: #66cc66;">,</span>v <span style="color: #ff7700;font-weight:bold;">in</span> search_dict.<span style="color: black;">items</span><span style="color: black;">&#40;</span><span style="color: black;">&#41;</span> <span style="color: #ff7700;font-weight:bold;">if</span> <span style="color: #008000;">tuple</span><span style="color: black;">&#40;</span>v<span style="color: black;">&#41;</span><span style="color: #66cc66;">==</span>c<span style="color: black;">&#93;</span>
    <span style="color: #ff7700;font-weight:bold;">if</span> <span style="color: #008000;">len</span><span style="color: black;">&#40;</span>file_pair<span style="color: black;">&#41;</span> <span style="color: #66cc66;">==</span> <span style="color: #ff4500;">2</span>:
        file_pair_lst.<span style="color: black;">append</span><span style="color: black;">&#40;</span><span style="color: #008000;">tuple</span><span style="color: black;">&#40;</span>file_pair<span style="color: black;">&#41;</span><span style="color: black;">&#41;</span>
    <span style="color: #ff7700;font-weight:bold;">else</span>:
        pair_lst <span style="color: #66cc66;">=</span> <span style="color: #008000;">list</span><span style="color: black;">&#40;</span><span style="color: #dc143c;">itertools</span>.<span style="color: black;">combinations</span><span style="color: black;">&#40;</span>file_pair<span style="color: #66cc66;">,</span> <span style="color: #ff4500;">2</span><span style="color: black;">&#41;</span><span style="color: black;">&#41;</span>
        file_pair_lst +<span style="color: #66cc66;">=</span> pair_lst
&nbsp;
<span style="color: #ff7700;font-weight:bold;">print</span><span style="color: black;">&#40;</span>f<span style="color: #483d8b;">&quot;Number of unique pairs is: {len(file_pair_lst)}<span style="color: #000099; font-weight: bold;">\n</span>&quot;</span><span style="color: black;">&#41;</span>
<span style="color: #ff7700;font-weight:bold;">print</span><span style="color: black;">&#40;</span>f<span style="color: #483d8b;">&quot;The first five pairs are: {file_pair_lst[:5]}&quot;</span><span style="color: black;">&#41;</span></pre></td></tr></table></div>

<p>Out:</p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out7.png"><img loading="lazy" class="aligncenter size-large wp-image-13812" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out7-1024x120.png" alt="" width="702" height="82" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out7-1024x120.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out7-300x35.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out7-768x90.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out7-1536x180.png 1536w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out7-2048x240.png 2048w" sizes="(max-width: 702px) 100vw, 702px" /></a></p>
<h3>Compare the data</h3>
<p>Finding textual near duplicates is more complicated than duplicates. There is no gold standard on the similarity threshold of two considered near-duplicates. Based on the _NUM_TOKENS_ by _SENTENCE_ID_ from the table interOut earlier, we add the assumption that two documents have a very high possibility to be near-duplicates if they share the same number of tokens for the sentences ordered in a list with their indices randomly picked by a defined ratio to total sentence number.</p>
<p>For example, fileA and fileB have 20 sentences each and the defined ratio is 0.5. We use pandas.Series.sample to randomly select 10 sentences from two files each. The random_state value is required to make sure that sentences from two files are picked up in parallel. If the two sentences have the same number of tokens for every pair we sampled, fileA and fileB are considered near-duplicates.</p>
<p>Now we are ready for comparison.</p>

<div class="wp_syntax"><table><tr><td class="code"><pre class="python" style="font-family:monospace;"><span style="color: #808080; font-style: italic;"># Compare doc pairs</span>
possibleDuplicate <span style="color: #66cc66;">=</span> <span style="color: black;">&#91;</span><span style="color: black;">&#93;</span>
<span style="color: #ff7700;font-weight:bold;">for</span> <span style="color: black;">&#40;</span>a<span style="color: #66cc66;">,</span> b<span style="color: black;">&#41;</span> <span style="color: #ff7700;font-weight:bold;">in</span> file_pair_lst:
    <span style="color: #808080; font-style: italic;"># Keep only the column _NUM_TOKENS_</span>
    tmp_a <span style="color: #66cc66;">=</span> df_interout_selected<span style="color: black;">&#91;</span>df_interout_selected<span style="color: black;">&#91;</span><span style="color: #483d8b;">'fileName'</span><span style="color: black;">&#93;</span><span style="color: #66cc66;">==</span>a<span style="color: black;">&#93;</span>.<span style="color: black;">loc</span><span style="color: black;">&#91;</span>:<span style="color: #66cc66;">,</span><span style="color: #483d8b;">&quot;_NUM_TOKENS_&quot;</span><span style="color: black;">&#93;</span>
    tmp_b <span style="color: #66cc66;">=</span> df_interout_selected<span style="color: black;">&#91;</span>df_interout_selected<span style="color: black;">&#91;</span><span style="color: #483d8b;">'fileName'</span><span style="color: black;">&#93;</span><span style="color: #66cc66;">==</span>b<span style="color: black;">&#93;</span>.<span style="color: black;">loc</span><span style="color: black;">&#91;</span>:<span style="color: #66cc66;">,</span><span style="color: #483d8b;">&quot;_NUM_TOKENS_&quot;</span><span style="color: black;">&#93;</span>
    <span style="color: #808080; font-style: italic;"># Drop the index column to use pandas.Series.compare</span>
    tmp_a.<span style="color: black;">reset_index</span><span style="color: black;">&#40;</span>drop<span style="color: #66cc66;">=</span><span style="color: #008000;">True</span><span style="color: #66cc66;">,</span> inplace<span style="color: #66cc66;">=</span><span style="color: #008000;">True</span><span style="color: black;">&#41;</span>
    tmp_b.<span style="color: black;">reset_index</span><span style="color: black;">&#40;</span>drop<span style="color: #66cc66;">=</span><span style="color: #008000;">True</span><span style="color: #66cc66;">,</span> inplace<span style="color: #66cc66;">=</span><span style="color: #008000;">True</span><span style="color: black;">&#41;</span>
    <span style="color: #808080; font-style: italic;"># Select sentences by pandas.Series.sample with the defined ratio</span>
    num_sent<span style="color: #66cc66;">,</span> num_sent_tocheck <span style="color: #66cc66;">=</span> <span style="color: #008000;">len</span><span style="color: black;">&#40;</span>tmp_a<span style="color: black;">&#41;</span><span style="color: #66cc66;">,</span> <span style="color: #008000;">round</span><span style="color: black;">&#40;</span>ratio_tocheck*<span style="color: #008000;">len</span><span style="color: black;">&#40;</span>tmp_a<span style="color: black;">&#41;</span><span style="color: black;">&#41;</span>
    tmp_a <span style="color: #66cc66;">=</span> tmp_a.<span style="color: black;">sample</span><span style="color: black;">&#40;</span>num_sent_tocheck<span style="color: #66cc66;">,</span> random_state<span style="color: #66cc66;">=</span><span style="color: #ff4500;">1</span><span style="color: black;">&#41;</span>
    tmp_b <span style="color: #66cc66;">=</span> tmp_b.<span style="color: black;">sample</span><span style="color: black;">&#40;</span>num_sent_tocheck<span style="color: #66cc66;">,</span> random_state<span style="color: #66cc66;">=</span><span style="color: #ff4500;">1</span><span style="color: black;">&#41;</span>
    <span style="color: #808080; font-style: italic;"># Detect duplicates by checking whether it is an empty dataframe (with a shape of (0,2))</span>
    <span style="color: #ff7700;font-weight:bold;">if</span> tmp_a.<span style="color: black;">compare</span><span style="color: black;">&#40;</span>tmp_b<span style="color: black;">&#41;</span>.<span style="color: black;">shape</span> <span style="color: #66cc66;">!=</span> <span style="color: black;">&#40;</span><span style="color: #ff4500;">0</span><span style="color: #66cc66;">,</span><span style="color: #ff4500;">2</span><span style="color: black;">&#41;</span>:
        <span style="color: #ff7700;font-weight:bold;">pass</span>
    <span style="color: #ff7700;font-weight:bold;">else</span>:
        possibleDuplicate.<span style="color: black;">append</span><span style="color: black;">&#40;</span><span style="color: black;">&#91;</span>a<span style="color: #66cc66;">,</span> b<span style="color: black;">&#93;</span><span style="color: black;">&#41;</span></pre></td></tr></table></div>

<p>The possibleDuplicate list contains 188 pairs of file names.</p>

<div class="wp_syntax"><table><tr><td class="code"><pre class="python" style="font-family:monospace;"><span style="color: #808080; font-style: italic;"># View the result</span>
view <span style="color: #66cc66;">=</span> <span style="color: #483d8b;">'======<span style="color: #000099; font-weight: bold;">\n</span>'</span>+<span style="color: #483d8b;">'<span style="color: #000099; font-weight: bold;">\n</span>'</span>.<span style="color: black;">join</span><span style="color: black;">&#40;</span><span style="color: black;">&#91;</span><span style="color: #483d8b;">&quot; &quot;</span>.<span style="color: black;">join</span><span style="color: black;">&#40;</span>p<span style="color: black;">&#41;</span> <span style="color: #ff7700;font-weight:bold;">for</span> p <span style="color: #ff7700;font-weight:bold;">in</span> possibleDuplicate<span style="color: black;">&#93;</span><span style="color: black;">&#41;</span>+<span style="color: #483d8b;">'<span style="color: #000099; font-weight: bold;">\n</span>======'</span>
<span style="color: #ff7700;font-weight:bold;">print</span><span style="color: black;">&#40;</span>f<span style="color: #483d8b;">&quot;NOTE: [ {len(possibleDuplicate) } ] possible duplicate pairs -&amp;gt; <span style="color: #000099; font-weight: bold;">\n</span>{view}&quot;</span><span style="color: black;">&#41;</span></pre></td></tr></table></div>

<p>Out:</p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out8.png"><img loading="lazy" class="aligncenter size-large wp-image-13815" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out8-1024x360.png" alt="" width="702" height="247" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out8-1024x360.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out8-300x105.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out8-768x270.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Out8.png 1332w" sizes="(max-width: 702px) 100vw, 702px" /></a></p>
<h3>Verify the results</h3>
<p>Now it’s time to see how far we went for our duplicate search. By checking the content of each pair, it’s not hard to find 133 being duplicates and 55 being near duplicates. Let’s take a look at two near-duplicate pairs we find. These documents have around 50 sentences and the differences occur just between 2 sentences.</p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Diff1.png"><img loading="lazy" class="aligncenter size-large wp-image-13818" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Diff1-1024x179.png" alt="" width="702" height="123" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Diff1-1024x179.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Diff1-300x53.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Diff1-768x134.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Diff1-1536x269.png 1536w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Diff1.png 1736w" sizes="(max-width: 702px) 100vw, 702px" /></a></p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Diff2.png"><img loading="lazy" class="aligncenter size-large wp-image-13821" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Diff2-1024x260.png" alt="" width="702" height="178" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Diff2-1024x260.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Diff2-300x76.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Diff2-768x195.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Diff2-1536x389.png 1536w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Diff2.png 1562w" sizes="(max-width: 702px) 100vw, 702px" /></a></p>
<h2>Work with PROC SQL in SAS</h2>
<p>SQL is one of the many languages built into the SAS system. Using PROC SQL, you have access to a robust data manipulation and query tool.</p>
<h3>Prepare the data</h3>
<p>We load the folder /home/anc2 with all plain text files to the table TESTDATA.</p>

<div class="wp_syntax"><table><tr><td class="code"><pre class="sas" style="font-family:monospace;"><span style="color: #0000ff;">libname</span> mycas cas;
&nbsp;
<span style="color: #000080; font-weight: bold;">proc cas</span>;
  <span style="color: #0000ff;">table</span>.addcaslib /
    caslib = <span style="color: #a020f0;">&quot;mycas&quot;</span>
    datasource = <span style="color: #66cc66;">&#123;</span>srctype=<span style="color: #a020f0;">&quot;path&quot;</span><span style="color: #66cc66;">&#125;</span>
    session = False
    path = <span style="color: #a020f0;">&quot;/home&quot;</span>
    subdirectories = <span style="color: #a020f0;">&quot;yes&quot;</span>;
<span style="color: #000080; font-weight: bold;">run</span>;
&nbsp;
  <span style="color: #0000ff;">table</span>.loadTable /
    casout = <span style="color: #66cc66;">&#123;</span>name=<span style="color: #a020f0;">&quot;testdata&quot;</span>, <span style="color: #0000ff;">replace</span>=True<span style="color: #66cc66;">&#125;</span>
    caslib = <span style="color: #a020f0;">&quot;mycas&quot;</span>
    importOptions = <span style="color: #66cc66;">&#123;</span>fileType=<span style="color: #a020f0;">&quot;DOCUMENT&quot;</span><span style="color: #66cc66;">&#125;</span>
    path = <span style="color: #a020f0;">&quot;anc2&quot;</span>;
<span style="color: #000080; font-weight: bold;">run</span>;
<span style="color: #000080; font-weight: bold;">quit</span>;</pre></td></tr></table></div>

<p>You can load it directly if you have already saved them to a .sashdata file.</p>

<div class="wp_syntax"><table><tr><td class="code"><pre class="sas" style="font-family:monospace;"><span style="color: #000080; font-weight: bold;">proc cas</span>;
  <span style="color: #0000ff;">table</span>.save /
    <span style="color: #0000ff;">table</span> = <span style="color: #66cc66;">&#123;</span>name=<span style="color: #a020f0;">&quot;testdata&quot;</span><span style="color: #66cc66;">&#125;</span>
    caslib = <span style="color: #a020f0;">&quot;mycas&quot;</span>
    name = <span style="color: #a020f0;">&quot;ANC2.sashdat&quot;</span>;
<span style="color: #000080; font-weight: bold;">run</span>;
&nbsp;
  <span style="color: #0000ff;">table</span>.loadtable /
    casout = <span style="color: #66cc66;">&#123;</span>name=<span style="color: #a020f0;">&quot;testdata&quot;</span>, <span style="color: #0000ff;">replace</span>=true<span style="color: #66cc66;">&#125;</span>
    path = <span style="color: #a020f0;">&quot;ANC2.sashdat&quot;</span>
    caslib = <span style="color: #a020f0;">&quot;mycas&quot;</span>;
<span style="color: #000080; font-weight: bold;">run</span>;
<span style="color: #000080; font-weight: bold;">quit</span>;</pre></td></tr></table></div>

<h3>Profile the data</h3>
<p>We call the profileText action in the textManagement action set to profile the data.</p>

<div class="wp_syntax"><table><tr><td class="code"><pre class="sas" style="font-family:monospace;"><span style="color: #000080; font-weight: bold;">proc cas</span>;
  textManagement.profiletext /
    <span style="color: #0000ff;">table</span> = <span style="color: #66cc66;">&#123;</span>name=<span style="color: #a020f0;">&quot;testdata&quot;</span><span style="color: #66cc66;">&#125;</span>
    documentid = <span style="color: #a020f0;">&quot;fileName&quot;</span>
    text = <span style="color: #a020f0;">&quot;content&quot;</span>
    language = <span style="color: #a020f0;">&quot;english&quot;</span>
    casOut = <span style="color: #66cc66;">&#123;</span>name=<span style="color: #a020f0;">&quot;casOut&quot;</span>, <span style="color: #0000ff;">replace</span>=True<span style="color: #66cc66;">&#125;</span>
    documentOut = <span style="color: #66cc66;">&#123;</span>name=<span style="color: #a020f0;">&quot;docOut&quot;</span>, <span style="color: #0000ff;">replace</span>=True<span style="color: #66cc66;">&#125;</span>
    intermediateOut = <span style="color: #66cc66;">&#123;</span>name=<span style="color: #a020f0;">&quot;interOut&quot;</span>, <span style="color: #0000ff;">replace</span>=True<span style="color: #66cc66;">&#125;</span>;
  <span style="color: #000080; font-weight: bold;">run</span>;
&nbsp;
  <span style="color: #0000ff;">table</span>.<span style="color: #0000ff;">fetch</span> /
    <span style="color: #0000ff;">table</span> = <span style="color: #66cc66;">&#123;</span>name=<span style="color: #a020f0;">&quot;docOut&quot;</span><span style="color: #66cc66;">&#125;</span>;
  <span style="color: #000080; font-weight: bold;">run</span>;
&nbsp;
  <span style="color: #0000ff;">table</span>.<span style="color: #0000ff;">fetch</span> /
    <span style="color: #0000ff;">table</span> = <span style="color: #66cc66;">&#123;</span>name=<span style="color: #a020f0;">&quot;interOut&quot;</span><span style="color: #66cc66;">&#125;</span>;
  <span style="color: #000080; font-weight: bold;">run</span>;
<span style="color: #000080; font-weight: bold;">quit</span>;</pre></td></tr></table></div>

<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/System1.png"><img loading="lazy" class="aligncenter size-large wp-image-13824" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/System1-1024x460.png" alt="" width="702" height="315" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/System1-1024x460.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/System1-300x135.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/System1-768x345.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/System1.png 1272w" sizes="(max-width: 702px) 100vw, 702px" /></a></p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/System2.png"><img loading="lazy" class="aligncenter size-large wp-image-13827" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/System2-1024x482.png" alt="" width="702" height="330" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/System2-1024x482.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/System2-300x141.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/System2-768x361.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/System2.png 1148w" sizes="(max-width: 702px) 100vw, 702px" /></a></p>
<h3>Filter the data</h3>
<p>We keep the documents given that their value pair occurs more than once.</p>

<div class="wp_syntax"><table><tr><td class="code"><pre class="sas" style="font-family:monospace;"><span style="color: #000080; font-weight: bold;">proc sql</span>;
  <span style="color: #0000ff;">create</span> <span style="color: #0000ff;">table</span> search1 <span style="color: #0000ff;">as</span>
    <span style="color: #0000ff;">select</span> <span style="color: #006400; font-style: italic;">* from mycas.docout
    group by _NUM_SENTENCES_, _MAX_TOKENS_SENTENCE_
    having count(*) &amp;gt;</span> <span style="color: #2e8b57; font-weight: bold;">1</span>;
<span style="color: #000080; font-weight: bold;">quit</span>;</pre></td></tr></table></div>

<p>We prepare all pair combinations of files that share the same values.</p>

<div class="wp_syntax"><table><tr><td class="code"><pre class="sas" style="font-family:monospace;"><span style="color: #000080; font-weight: bold;">proc sql</span>;
  <span style="color: #0000ff;">create</span> <span style="color: #0000ff;">table</span> search2 <span style="color: #0000ff;">as</span>
    <span style="color: #0000ff;">select</span> a.<span style="color: #0000ff;">fileName</span> <span style="color: #0000ff;">as</span> fileA , b.<span style="color: #0000ff;">fileName</span> <span style="color: #0000ff;">as</span> fileB
    <span style="color: #0000ff;">from</span>
      <span style="color: #66cc66;">&#40;</span><span style="color: #0000ff;">select</span> <span style="color: #006400; font-style: italic;">* from search1 ) a
        cross join
      (select * from search1 ) b
    where a._NUM_SENTENCES_ = b._NUM_SENTENCES_ and
      a._MAX_TOKENS_SENTENCE_ = b._MAX_TOKENS_SENTENCE_ and
      a.fileName &amp;lt;</span><span style="color: #0000ff; font-weight: bold;">&amp;gt</span>; b.<span style="color: #0000ff;">fileName</span>;
<span style="color: #000080; font-weight: bold;">quit</span>;
&nbsp;
<span style="color: #000080; font-weight: bold;">proc print</span> <span style="color: #000080; font-weight: bold;">data</span>=search2<span style="color: #66cc66;">&#40;</span>obs=<span style="color: #2e8b57; font-weight: bold;">5</span><span style="color: #66cc66;">&#41;</span>;
<span style="color: #000080; font-weight: bold;">run</span>;</pre></td></tr></table></div>

<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/System3.png"><img loading="lazy" class="aligncenter size-large wp-image-13830" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/System3-1024x436.png" alt="" width="702" height="299" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/System3-1024x436.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/System3-300x128.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/System3-768x327.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/System3.png 1180w" sizes="(max-width: 702px) 100vw, 702px" /></a></p>
<p>With a glimpse of table search2, we notice that it would be better to get just unique pairs to avoid repeating comparing those with the same file names.</p>

<div class="wp_syntax"><table><tr><td class="code"><pre class="sas" style="font-family:monospace;"><span style="color: #000080; font-weight: bold;">proc sql</span>;
  <span style="color: #0000ff;">create</span> <span style="color: #0000ff;">table</span> search3 <span style="color: #0000ff;">as</span>
    <span style="color: #0000ff;">select</span> <span style="color: #0000ff;">distinct</span> fileA, fileB <span style="color: #0000ff;">from</span> search2
    <span style="color: #0000ff;">where</span> search2.fileA <span style="color: #0000ff; font-weight: bold;">&amp;lt</span>; search2.fileB;
<span style="color: #000080; font-weight: bold;">quit</span>;
&nbsp;
<span style="color: #000080; font-weight: bold;">proc print</span> <span style="color: #000080; font-weight: bold;">data</span>=search3<span style="color: #66cc66;">&#40;</span>obs=<span style="color: #2e8b57; font-weight: bold;">5</span><span style="color: #66cc66;">&#41;</span>;
<span style="color: #000080; font-weight: bold;">run</span>;</pre></td></tr></table></div>

<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/System4.png"><img loading="lazy" class="aligncenter size-large wp-image-13833" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/System4-1024x417.png" alt="" width="702" height="286" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/System4-1024x417.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/System4-300x122.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/System4-768x313.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/System4.png 1246w" sizes="(max-width: 702px) 100vw, 702px" /></a></p>
<h3>Compare the data</h3>
<p>Given the assumption that two documents have a very high possibility to be near-duplicates if they share the same number of tokens for the sentences ordered in a list with their indices randomly picked by a defined ratio to total sentence number. Here we use the rand(‘uniform’) function to generate an observation from the continuous uniform distribution in the interval (0,1) as default. Setting it with ‘between .2 and .7’ helps us randomly get 50% of sentences. The similarity threshold can be customized by changing the range, say “where rand(‘uniform’) between .2 and .9” which means 70% of sentences in the documents would be examined.</p>

<div class="wp_syntax"><table><tr><td class="code"><pre class="sas" style="font-family:monospace;"><span style="color: #000080; font-weight: bold;">proc sql</span>;
  <span style="color: #0000ff;">create</span> <span style="color: #0000ff;">table</span> search4 <span style="color: #0000ff;">as</span>
    <span style="color: #0000ff;">select</span> fileA <span style="color: #0000ff;">as</span> f1, fileB <span style="color: #0000ff;">as</span> f2 <span style="color: #0000ff;">from</span> search3 
    <span style="color: #0000ff;">where</span> <span style="color: #0000ff;">not</span> exists <span style="color: #66cc66;">&#40;</span>
      <span style="color: #0000ff;">select</span> <span style="color: #006400; font-style: italic;">* from (
        select tmp1A, tmp2A from (
          select tmp1._NUM_TOKENS_ as tmp1A, tmp1._SENTENCE_ID_ as tmp1B,
                 tmp2._NUM_TOKENS_ as tmp2A, tmp2._SENTENCE_ID_ as tmp2B from
            (select * from sasout1.interout interout1 where interout1.fileName = f1) tmp1,
            (select * from sasout1.interout interout2 where interout2.fileName = f2) tmp2 
	    where tmp1B = tmp2B)
          where rand('uniform') between .2 and .7)
        where tmp1A &amp;lt;</span><span style="color: #0000ff; font-weight: bold;">&amp;gt</span>; tmp2A<span style="color: #66cc66;">&#41;</span>;
<span style="color: #000080; font-weight: bold;">quit</span>;</pre></td></tr></table></div>

<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/System5.png"><img loading="lazy" class="aligncenter size-large wp-image-13836" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/System5-1024x391.png" alt="" width="702" height="268" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/System5-1024x391.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/System5-300x114.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/System5-768x293.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/System5.png 1400w" sizes="(max-width: 702px) 100vw, 702px" /></a></p>
<h3>Verify the results</h3>
<p>We use the table testdata to verify the results. There are 133 duplicates and 39 near duplicates out of 172 pairs.</p>

<div class="wp_syntax"><table><tr><td class="code"><pre class="sas" style="font-family:monospace;"><span style="color: #000080; font-weight: bold;">proc sql</span>;
  <span style="color: #0000ff;">create</span> <span style="color: #0000ff;">table</span> Duplicates <span style="color: #0000ff;">as</span>
    <span style="color: #0000ff;">select</span> f1, f2 <span style="color: #0000ff;">from</span> search4
    <span style="color: #0000ff;">where</span> <span style="color: #0000ff;">not</span> exists <span style="color: #66cc66;">&#40;</span>
      <span style="color: #66cc66;">&#40;</span><span style="color: #0000ff;">select</span> content <span style="color: #0000ff;">from</span> mycas.testdata tmp <span style="color: #0000ff;">where</span> tmp.<span style="color: #0000ff;">fileName</span> = f1<span style="color: #66cc66;">&#41;</span>
      except
      <span style="color: #66cc66;">&#40;</span><span style="color: #0000ff;">select</span> content <span style="color: #0000ff;">from</span> mycas.testdata tmp <span style="color: #0000ff;">where</span> tmp.<span style="color: #0000ff;">fileName</span> = f2<span style="color: #66cc66;">&#41;</span>
    <span style="color: #66cc66;">&#41;</span>;
<span style="color: #000080; font-weight: bold;">quit</span>;
&nbsp;
<span style="color: #000080; font-weight: bold;">proc sql</span>;
  <span style="color: #0000ff;">create</span> <span style="color: #0000ff;">table</span> nearDuplicates <span style="color: #0000ff;">as</span>
    <span style="color: #0000ff;">select</span> f1, f2 <span style="color: #0000ff;">from</span> search4
    <span style="color: #0000ff;">where</span> exists <span style="color: #66cc66;">&#40;</span>
      <span style="color: #66cc66;">&#40;</span><span style="color: #0000ff;">select</span> content <span style="color: #0000ff;">from</span> mycas.testdata tmp <span style="color: #0000ff;">where</span> tmp.<span style="color: #0000ff;">fileName</span> = f1<span style="color: #66cc66;">&#41;</span>
      except
      <span style="color: #66cc66;">&#40;</span><span style="color: #0000ff;">select</span> content <span style="color: #0000ff;">from</span> mycas.testdata tmp <span style="color: #0000ff;">where</span> tmp.<span style="color: #0000ff;">fileName</span> = f2<span style="color: #66cc66;">&#41;</span>
    <span style="color: #66cc66;">&#41;</span>;
<span style="color: #000080; font-weight: bold;">quit</span>;</pre></td></tr></table></div>

<h2>Conclusions</h2>
<p>Exploring derived statistics from the profileText action provides a practical perspective to gain insights not only by comparing to a reference corpus, but at token, sentence, and document levels within a corpus itself. With the randomness in selecting which sentences to compare, we might observe different results after performing this duplication identification method. The smaller ratio we define, the more near-duplicate pairs we get. And you would probably be surprised by the fact that if we set the ratio to 0.1, the result would still be around 207 pairs, just a little bit more than 172 pairs when the ratio is set to 0.5. The method doesn’t seem to overfire because two files are required to have the same number of sentences and the same maximum number of tokens in a sentence before we pair them up. This requirement gives us a safer place to start our search.</p>
<p>Textual near duplicate identification is simple to understand but not easy to develop standards to include every type of near duplicates. In this article, we provide one way to describe near duplicates in which distance is between several sentences or words in order, yet not including cases like sentences of two documents are not arranged in the same order or some chunks are glued together so that the results are affected by different sentence indexing. These are fun to think about and they might be turned into the next level detection.</p>
<p>How would you define the similarity for near duplicates?</p>
<h3>Learn more</h3>
<ul>
<li>Check out <a href="https://go.documentation.sas.com/doc/us/pgmsascdc/v_021/casanpg/p1e93w56dte67qn14kcy9njkxs1p.htm">additional documentation on corpus analysis</a> and  <a href="https://support.sas.com/en/software/visual-text-analytics-support.html#documentation">SAS</a> <a href="https://support.sas.com/en/software/visual-text-analytics-support.html#documentation">Visual Text Analytics.</a></li>
<li>Keep exploring by checking out the ebook “<a href="https://www.sas.com/en/whitepapers/natural-language-processing-110641.html">Make Every Voice Heard with Natural Language Processing</a>” or visit <a href="https://www.sas.com/en_us/software/visual-text-analytics.html">SAS Visual Text Analytics</a>.</li>
</ul>
<p>The post <a rel="nofollow" href="https://blogs.sas.com/content/subconsciousmusings/2022/10/03/find-duplicates-with-nlp/">Find duplicates and near-duplicates in a corpus with Natural Language Processing</a> appeared first on <a rel="nofollow" href="https://blogs.sas.com/content/subconsciousmusings">The SAS Data Science Blog</a>.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://blogs.sas.com/content/subconsciousmusings/2022/10/03/find-duplicates-with-nlp/feed/</wfw:commentRss>
			<slash:comments>2</slash:comments>
		
		
			<enclosure url="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/637278988-150x150.jpg" />
	</item>
		<item>
		<title>How advancements in automated computer vision are keeping us safe</title>
		<link>https://blogs.sas.com/content/subconsciousmusings/2022/09/30/automated-computer-vision-keeping-us-safe/</link>
					<comments>https://blogs.sas.com/content/subconsciousmusings/2022/09/30/automated-computer-vision-keeping-us-safe/#respond</comments>
		
		<dc:creator><![CDATA[Federica Citterio]]></dc:creator>
		<pubDate>Fri, 30 Sep 2022 13:00:02 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[computer vision]]></category>
		<category><![CDATA[image matching]]></category>
		<category><![CDATA[object detection]]></category>
		<category><![CDATA[python]]></category>
		<category><![CDATA[SAS Viya]]></category>
		<guid isPermaLink="false">https://blogs.sas.com/content/subconsciousmusings/?p=13884</guid>

					<description><![CDATA[<p>Using SAS Viya in combination with open-source capabilities, we were able to develop an automated solution for logo detection that does not require any manual data labeling.</p>
<p>The post <a rel="nofollow" href="https://blogs.sas.com/content/subconsciousmusings/2022/09/30/automated-computer-vision-keeping-us-safe/">How advancements in automated computer vision are keeping us safe</a> appeared first on <a rel="nofollow" href="https://blogs.sas.com/content/subconsciousmusings">The SAS Data Science Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p><em><a href="https://blogs.sas.com/content/author/augustazhang/">Augusta Zhang</a> and Jacek Dobrowolski also contributed to this article.</em></p>
<p>Automated computer vision technology is helping intelligence analysts stay one step ahead of gangs, terrorist groups, and other suspicious organizations.</p>
<p>Automation is at the heart of artificial intelligence. By helping humans automate repetitive tasks and make the most of their time, AI has resulted in remarkable successes that range from <a href="https://www.sas.com/en_us/customers/georgia-pacific.html">optimizing manufacturing supply chains</a> to saving lives through <a href="https://www.sas.com/en_us/customers/amsterdam-umc.html">improved cancer treatment strategies</a>. Automation is embedded in many everyday processes to accelerate how we handle different tasks. A lot of research is conducted in the fields of AutoML (automated machine learning), AutoDL (automated deep learning), autoNLP (automated language processing), autoSpeech (automated speech classification), and autoCV (automated computer vision) to determine how machines can help us quickly solve challenges by interpreting our language, identifying images, and more.</p>
<p>At the core, these fields all share a similar process of learning from data. Machines can analyze data in depth thanks to hidden layers in neural networks, which find structures and regularities in the data and set weights so that an algorithm can acquire the skill to classify information or predict outcomes. </p>
<h2>The challenge? Collecting high-quality data, and a lot of it.</h2>
<p>Although AI is incredibly useful for extracting insights from our data, humans are still essential to set up the systems and providing the right foundation for machines to do the work. One of the biggest challenges is providing enough unbiased, correct, labeled data for analysis.</p>
<p>For example, in order to train an object detection model that uses computer vision to find objects in images, a user must first provide many sample images. Traditionally, a user manually labels hundreds or thousands of images and stores information such as the position, size and class of the objects of interest. This task requires lots of time and tedious effort, which is a huge investment before getting started. As data scientists, our question is, can we automate the image annotation process?</p>
<p>Well, in some cases, yes!</p>
<p>Jacek and I were recently approached to develop a solution to detect logos of interest in the area of security intelligence. Intelligence analysts piece together information from a variety of sources in order to assess threats and protect national security and economic wellbeing, including millions of images collected from social media. When a new, suspicious organization comes on their radar, it is important to keep track of the content they share, including images that may be identified thanks to the presence of a specific logo. This is a perfect use case for object detection, but it’s a huge effort to collect and label enough data to train a computer vision model to recognize a logo. Plus, the process must be repeated many times because new logos of interest can potentially come up every day. How can we develop models fast enough to stay ahead of these quickly evolving threats?</p>
<h2>Our solution and how it works</h2>
<p> We knew that the fastest way we could get accurate models into production was to automate as much of the process as possible. We developed an automated pipeline that accelerates the creation of logo detection models by executing the following steps:</p>
<h3>1. Create an artificial training set</h3>
<p>We ask the analyst to provide the logo of interest and any variations of it, and we overlay the logo on a set of background images. We randomly choose the size, position, rescaling factor, and opacity of the logo in each image to introduce some variation, and we keep track of where the logos are located through generated bounding boxes. The background images can be taken from publicly available datasets, like ImageNet, or can be more specific to the domain. In our case, they were images collected from social media. By creating an artificial training set this way, we can take a single logo and generate hundreds or thousands of images to train a model automatically! Here is an example of artificially generated images that contain a few variations of the SAS logo:</p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/logo-detection.png"><img loading="lazy" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/logo-detection.png" alt="" width="979" height="877" class="aligncenter size-full wp-image-13893" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/logo-detection.png 979w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/logo-detection-300x269.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/logo-detection-768x688.png 768w" sizes="(max-width: 979px) 100vw, 979px" /></a></p>
<h3>2. Train an object detection model on the artificial training set</h3>
<p>For this task, we used a Tiny-YOLO model, since it showed a good balance between model size and accuracy. We followed the example available on <a href="https://github.com/sassoftware/python-dlpy/blob/master/examples/object_detection/SimpleObjectDetection.ipynb">the SAS Deep Learning Python (DLPy) GitHub page</a>. DLPy is a Python library that allows you to quickly create deep learning models in SAS, with pre-trained weights provided, so it was perfect for accelerating our modeling process!</p>
<h2>The results</h2>
<p>The idea is pretty simple, but it proved to be effective. To test it, we generated an artificial training set using the SAS logo. We were able to use a single image of the SAS logo to generate hundreds of new images in just a few minutes. Then, we trained an object detection model using the training set, and we scored it on new images, which weren’t used during model training. These test images came from SAS social media accounts. After training for just 13 minutes, we were able to achieve a very accurate model, with a recall of 93% and a precision of 93%. Here, you can some sample results. Each purple bounding box shows where the model predicts a SAS logo to be present, along with a confidence score between 0.00 and 1.00:</p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/logo-detection-score.png"><img loading="lazy" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/logo-detection-score.png" alt="" width="795" height="821" class="aligncenter size-full wp-image-13896" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/logo-detection-score.png 795w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/logo-detection-score-290x300.png 290w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/logo-detection-score-768x793.png 768w" sizes="(max-width: 795px) 100vw, 795px" /></a></p>
<p>Surprisingly, the model proved to work really well not only on the logos found on neat marketing images, but also on photographs of real objects that include a SAS logo. For example, here we have business cards, a water bottle, a glass, a pen, and more.</p>
<p>On this dataset, both recall and precision are at about 94%.</p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/logo-detection-object-score.png"><img loading="lazy" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/logo-detection-object-score.png" alt="" width="979" height="864" class="aligncenter size-full wp-image-13899" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/logo-detection-object-score.png 979w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/logo-detection-object-score-300x265.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/logo-detection-object-score-768x678.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/logo-detection-object-score-110x96.png 110w" sizes="(max-width: 979px) 100vw, 979px" /></a></p>
<p>Using SAS Viya in combination with open-source capabilities, we were able to develop an automated solution for logo detection that does not require any manual data labeling. The input is just a single image of the logo of interest, and the output is a powerful computer vision model that can be used to identify new images, whether you’re analyzing instances of a corporate brand or tracking the logos of suspicious organizations. In this way, we can speed up the analysis immensely and let the experts focus on what matters most--discovering hidden connections in the data to solve analytics challenges and keep all of us safe.</p>
<h2>Want to learn more?</h2>
<p>Check out this <a href="https://blogs.sas.com/content/subconsciousmusings/2022/08/12/multi-stage-computer-vision-model-detect-aerial-objects/">article</a> to see how two data scientists at SAS built a multi-stage computer vision model to locate their canine friend Dr. Taco!</p>
<p>The post <a rel="nofollow" href="https://blogs.sas.com/content/subconsciousmusings/2022/09/30/automated-computer-vision-keeping-us-safe/">How advancements in automated computer vision are keeping us safe</a> appeared first on <a rel="nofollow" href="https://blogs.sas.com/content/subconsciousmusings">The SAS Data Science Blog</a>.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://blogs.sas.com/content/subconsciousmusings/2022/09/30/automated-computer-vision-keeping-us-safe/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			<enclosure url="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/logo-detection-object-score-150x150.png" />
	</item>
		<item>
		<title>Three data scientist jobs and how to land them</title>
		<link>https://blogs.sas.com/content/subconsciousmusings/2022/09/20/data-scientist-jobs/</link>
					<comments>https://blogs.sas.com/content/subconsciousmusings/2022/09/20/data-scientist-jobs/#comments</comments>
		
		<dc:creator><![CDATA[Suzanne Morgen]]></dc:creator>
		<pubDate>Tue, 20 Sep 2022 18:08:58 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[data scientist]]></category>
		<category><![CDATA[data visualization]]></category>
		<category><![CDATA[machine learning]]></category>
		<guid isPermaLink="false">https://blogs.sas.com/content/subconsciousmusings/?p=13695</guid>

					<description><![CDATA[<p>The question to ask is no longer, “Do you want to be a data scientist?” But rather, “What kind of data scientist do you want to be?”</p>
<p>The post <a rel="nofollow" href="https://blogs.sas.com/content/subconsciousmusings/2022/09/20/data-scientist-jobs/">Three data scientist jobs and how to land them</a> appeared first on <a rel="nofollow" href="https://blogs.sas.com/content/subconsciousmusings">The SAS Data Science Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p>There has been a lot of buzz about data scientist jobs recently. And for good reason! Since 2016, data scientist has been at or near the top of Glassdoor’s <a href="https://www.glassdoor.com/List/Best-Jobs-in-America-LST_KQ0,20.htm">Best Jobs in America</a> list. But since the job hit the top spot in the list, the field has exploded both in popularity and complexity. (Our data shows that data scientist job postings have more than tripled in the last five years!) In this quickly evolving field, new opportunities are always on the horizon.  Perhaps the question to ask is no longer, “Do you want to be a data scientist?” But rather, “What kind of data scientist do you want to be?”</p>
<p>In addition to the core set of skills that all data scientists must master, there are many specializations within the field because business problems can be approached in multiple ways using multiple tools. A lot of infrastructure is required, especially in large companies, in order to store, move, and analyze data. Employers are often looking for data scientists with specific areas of focus. Depending on the industry that you want to work in or the size of your team, you may need to strengthen your data visualization, machine learning, or data curation talents.</p>
<p>In this article, we will look at three trending data scientist jobs and what it will take to land them.</p>
<h2><font color="#0378CD">Data Visualization Engineer</font></h2>
<p>Data, data everywhere and not a drop to drink. The world is awash in data, and companies need experts to handle data so that they don’t drown in it. Data visualization engineers package all that fluid data into a form that can be easily digested and understood. Data visualization engineers straddle the data engineering and design worlds by marrying an understanding of the data with the ability to display that information beautifully. According to Lightcast, job postings requesting skills in data analysis and data visualization more than doubled in the last two years.</p>
<p>Data visualization engineers create dashboards, charts, and other visualizations that different stakeholders in a company can use. Data visualization engineers may also use visual tools to identify anomalies in data to support data scientists as part of the data engineering process. They work with many different people in an organization and use a mix of programming and software tools to fulfill requests. They need to understand how data is structured and stored and how to access it. The role is an unusual blend of art and science that may appeal to people who don’t have the extensive computer science or statistics background required to be a data scientist.</p>
<h3>Skills needed</h3>
<p>In addition to an educational background in graphic design and data science, to become a data visualization engineer you will need proficiency in the following:</p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Data-Visualization-Engineer.jpg"><img loading="lazy" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Data-Visualization-Engineer-300x200.jpg" alt="" width="300" height="200" class="alignright size-medium wp-image-13731" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Data-Visualization-Engineer-300x200.jpg 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Data-Visualization-Engineer-768x513.jpg 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Data-Visualization-Engineer.jpg 800w" sizes="(max-width: 300px) 100vw, 300px" /></a></p>
<ul>
<li>data visualization and dashboarding tools like Tableau or SAS® Visual Analytics</li>
<li>graphic design principles and best practices</li>
<li>coding knowledge for data visualization libraries from Python, R, SAS, and Javascript</li>
<li>leadership, communication, and presentation skills</li>
</ul>
<h3>Recommended training</h3>
<p>If you are interested in becoming a data visualization engineer, getting experience with SAS Visual Analytics through <a href="https://support.sas.com/training/us/paths/va.html">free tutorials and other courses</a> is a good place to start. Also check out the <a href="https://www.sas.com/en_us/certification/credentials/bi-analytics/visual-business-analytics.geo.html">SAS® Visual Business Analytics Specialist credential</a> to prove your proficiency in designing reports and dashboards with SAS Visual Analytics.</p>
<h2><font color="#0378CD">Machine Learning Engineer</font></h2>
<p>A machine learning engineer uses artificial intelligence to solve complex data problems by developing models that predict outcomes based on past behavior. A machine learning specialist or machine learning engineer is a vital part of a data science team. After the data sources have been identified and cleansed, the algorithms that analyze the data need to be built. This is where the machine learning specialist comes in. They develop and deploy the models that will extract insights from the data. Demand is rising quickly for this specialized skill set with over 14,000 jobs posted in the U.S. last year.</p>
<h3>Skills needed</h3>
<p>Machine learning specialists still need to be well-versed in core data science principles because they may be called on to consult during the data preparation and software selection phases. In addition, machine learning specialists will need experience with the following in order to be an outstanding candidate:</p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Machine-Learning-Engineer.jpg"><img loading="lazy" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Machine-Learning-Engineer-300x183.jpg" alt="" width="300" height="183" class="alignright size-medium wp-image-13749" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Machine-Learning-Engineer-300x183.jpg 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Machine-Learning-Engineer-768x469.jpg 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Machine-Learning-Engineer.jpg 800w" sizes="(max-width: 300px) 100vw, 300px" /></a></p>
<ul>
<li>an educational background in software engineering or computer science</li>
<li>programming skills in multiple languages like Python, Java, SAS, and C++</li>
<li>knowledge of statistics and machine learning techniques</li>
<li>the ability to collaborate on a large, cross-functional team</li>
<li>communication skills to explain difficult concepts to nontechnical people</li>
</ul>
<h3>Recommended training</h3>
<p>One way to get some of these skills is through the <a href="https://www.sas.com/en_us/training/courses/machine-learning-using-sas-viya.geo.html">Machine Learning with SAS® Viya® course</a> that will help you prepare for the <a href="https://www.sas.com/en_us/certification/credentials/advanced-analytics/machine-learning-specialist.geo.html">Machine Learning Specialist exam</a>. For some quick hits, check out <a href="https://www.youtube.com/watch?v=S4PeFiqc19k&list=PLVV6eZFA22QxiVFi6BotaMEAyp6NHo_9Y">Cat Truxillo’s curated playlist</a> on the <a href="https://www.youtube.com/sasusers">SAS Users YouTube Channel</a>.</p>
<h2><font color="#0378CD">Data Engineer</font></h2>
<p>Not to be confused with a data visualization engineer, data engineer is climbing the ranks of best jobs lists and is poised to have a breakout moment. Over 90,000 jobs for machine learning engineers were posted in the U.S. just last year, making data engineer the second most popular job title that requests skills in the data science field. Data engineering is an essential role that builds and manages the systems that store and move data. Organizations who want to invest in data analytics need to ensure that the data is available, clean, and ready to use. Data engineers create the data infrastructure and work closely with data scientists and machine learning specialists to understand their needs. They may also be responsible for educating others in the company about good data practices. No longer the unsung heroes of the data department, data engineering is a great career path for those who are drawn to data management more than data analysis.</p>
<h3>Skills needed</h3>
<p>Data engineers need to love data and be up to date on the latest tools. Software engineers often have the right background to transition into data engineering, but a broad understanding of the analytics lifecycle is also important. A successful data engineer who can create and maintain a secure data infrastructure needs the following skills:</p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Data-Engineer.jpg"><img loading="lazy" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Data-Engineer-300x200.jpg" alt="" width="300" height="200" class="alignright size-medium wp-image-13752" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Data-Engineer-300x200.jpg 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Data-Engineer-768x512.jpg 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Data-Engineer.jpg 800w" sizes="(max-width: 300px) 100vw, 300px" /></a></p>
<ul>
<li>experience with ETL platforms</li>
<li>advanced coding skills in database and statistical languages like SQL, Python, SAS, or Scala</li>
<li>can solve complex data problems</li>
<li>is a data evangelist who can explain the importance of good data governance</li>
</ul>
<h3>Recommended training</h3>
<p>Several companies offer professional certifications in data engineering. The <a href="https://www.sas.com/en_us/training/academy-data-science/data-curation-certification.html">Data Curation Professional learning path</a> at the <a href="https://www.sas.com/en_us/training/academy-data-science.geo.html">SAS Academy for Data Science</a> is a great place to gain the skills you need to become a data engineer.</p>
<p>No matter where you are in your data science journey, SAS has training through the <a href="https://www.sas.com/en_us/training/academy-data-science.geo.html">SAS Academy for Data Science</a> and <a href="https://www.sas.com/en_us/training/academy-data-science/data-science-resources.geo.html">resources</a> to support you as you become exactly the kind of data scientist you want to be. Are you working in any of these jobs and have insights to share? Let us know in the comments below.</p>
<h2>Related content</h2>
<ul>
<li><a href="https://blogs.sas.com/content/sascom/2022/08/05/4-reasons-to-build-your-data-analytics-skills-with-sas/">4 reasons to build your data analytics skills with SAS</a></li>
<li><a href="https://www.sas.com/gms/redirect.jsp?detail=GMS207125_328288">The value of SAS certification</a></li>
<li><a href="https://www.sas.com/en_us/explore/for-data-scientists.html">SAS® for data scientists</a></li>
<li><a href="https://www.sas.com/en/offers/data-science-experience.html#industry">The Data Science Experience</a></li>
</ul>
<p>The post <a rel="nofollow" href="https://blogs.sas.com/content/subconsciousmusings/2022/09/20/data-scientist-jobs/">Three data scientist jobs and how to land them</a> appeared first on <a rel="nofollow" href="https://blogs.sas.com/content/subconsciousmusings">The SAS Data Science Blog</a>.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://blogs.sas.com/content/subconsciousmusings/2022/09/20/data-scientist-jobs/feed/</wfw:commentRss>
			<slash:comments>3</slash:comments>
		
		
			<enclosure url="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Three-data-scientist-jobs-and-how-to-land-them-150x150.jpg" />
	</item>
		<item>
		<title>Intelligent Decisioning: Ensuring fairness in analytically-driven decision making</title>
		<link>https://blogs.sas.com/content/subconsciousmusings/2022/09/14/intelligent-decisioning-ensuring-fairness-in-analytically-driven-decision-making/</link>
					<comments>https://blogs.sas.com/content/subconsciousmusings/2022/09/14/intelligent-decisioning-ensuring-fairness-in-analytically-driven-decision-making/#respond</comments>
		
		<dc:creator><![CDATA[Diana Maris]]></dc:creator>
		<pubDate>Wed, 14 Sep 2022 13:00:00 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[intelligent decisioning]]></category>
		<category><![CDATA[ModelOps]]></category>
		<category><![CDATA[SAS Explore]]></category>
		<category><![CDATA[sas model manager]]></category>
		<category><![CDATA[SAS Visual Analytics]]></category>
		<category><![CDATA[SAS Viya]]></category>
		<guid isPermaLink="false">https://blogs.sas.com/content/subconsciousmusings/?p=13635</guid>

					<description><![CDATA[<p>Attend this session during the SAS Explore event on Sept 27-29 or view the recording at your convenience. We will showcase the use of SAS Intelligent Decisioning, SAS Model Manager, and SAS Visual Analytics on the SAS Viya platform for a solution that helps mitigate inequitable credit decisions.</p>
<p>The post <a rel="nofollow" href="https://blogs.sas.com/content/subconsciousmusings/2022/09/14/intelligent-decisioning-ensuring-fairness-in-analytically-driven-decision-making/">Intelligent Decisioning: Ensuring fairness in analytically-driven decision making</a> appeared first on <a rel="nofollow" href="https://blogs.sas.com/content/subconsciousmusings">The SAS Data Science Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p><a href="http://explore.sas.com/"><img loading="lazy" class="aligncenter size-full wp-image-13644" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/SAS-Explore-Header-scaled.jpg" alt="" width="2560" height="335" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/SAS-Explore-Header-scaled.jpg 2560w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/SAS-Explore-Header-300x39.jpg 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/SAS-Explore-Header-1024x134.jpg 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/SAS-Explore-Header-768x100.jpg 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/SAS-Explore-Header-1536x201.jpg 1536w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/SAS-Explore-Header-2048x268.jpg 2048w" sizes="(max-width: 2560px) 100vw, 2560px" /></a></p>
<p>The realities of today’s digital transformation are pushing organizations across all industries to expand and accelerate their decision-making processes. Adaptability and precision remain essential, while decision complexity is accelerating.</p>
<p>In response, organizations are changing the way they approach decisioning – leveraging technology in concert with human potential for obtaining the highest value. There are two approaches: the most familiar one is augmented decision making – where humans take analytically driven insights to make a decision, for example in call centers; and the second - automated decision making – where the machine makes the decisions, for high-volume transactional systems like credit origination, next best offers, and logistical routing.</p>
<p>As these analytically driven approaches are embedded into decisioning operations, the impact on people and companies is far-reaching including the need to ensure fairness and eliminate bias in the process. This need is highlighted by Federal institutions, like the Consumer Financial Protection Bureau, who, as recent as May 2022, issued a reminder for creditors of their legal responsibility to provide specific and accurate reasons for adverse actions when using AI technologies in credit decisioning applications. <strong>(See details of the Consumer Financial Protection Circular 2022-03 <a href="https://www.consumerfinance.gov/compliance/circulars/circular-2022-03-adverse-action-notification-requirements-in-connection-with-credit-decisions-based-on-complex-algorithms/#3">here</a>)</strong></p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Code-of-Federal-Regulations.png"><img loading="lazy" class="aligncenter size-full wp-image-13638" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Code-of-Federal-Regulations.png" alt="Code of Federal Regulations: Statement of specific reasons. The statement of reasons for adverse action required by paragraph (a)(2)(i) of this section must be specific and indicate the principal reason(s) for the adverse action. Statements that the adverse action was based on the creditor's internal standards or policies or that the applicant, ioint applicant. or similar party failed to achieve a qualifying score on the creditor's credit scoring system are insufficient." width="1280" height="397" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Code-of-Federal-Regulations.png 1280w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Code-of-Federal-Regulations-300x93.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Code-of-Federal-Regulations-1024x318.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/Code-of-Federal-Regulations-768x238.png 768w" sizes="(max-width: 1280px) 100vw, 1280px" /></a></p>
<p>This prompts decisioning software providers, to ensure their data-driven systems are designed with clear bias mitigation and fairness in place.</p>
<p>SAS applications already include explainability and fairness features as part of a well-functioning analytics lifecycle. Navigating from customer questions to data-driven decisions is part of the <a href="https://www.sas.com/en_us/insights/articles/analytics/modelops.html">ModelOps</a> implementation at SAS.</p>
<p><a href="https://www.sas.com/en_us/software/intelligent-decisioning.html">SAS Intelligent Decisioning</a> provides users the ability to create, manage and govern robust analytically driven decisioning at scale, with the use of a low-code, user friendly decision builder, model management integration, custom code integration, open-source support (Python), and a variety of publishing destination options, for both batch and real-time processing.</p>
<p><a href="https://support.sas.com/en/software/intelligent-decisioning-support.html"><img loading="lazy" class="alignright size-medium wp-image-13653" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/SAS-Intelligent-Decisioning-300x169.jpg" alt="" width="300" height="169" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/SAS-Intelligent-Decisioning-300x169.jpg 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/SAS-Intelligent-Decisioning-1024x576.jpg 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/SAS-Intelligent-Decisioning-768x432.jpg 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/SAS-Intelligent-Decisioning.jpg 1280w" sizes="(max-width: 300px) 100vw, 300px" /></a></p>
<p>SAS Intelligent Decisioning on <a href="https://www.sas.com/en_us/software/viya.html">Viya</a> can be used to satisfy an extensive number of diverse operational decisioning use-cases that span an organization. Different domains, like Fraud, Underwriting, Compliance, and Marketing all benefit from the use of SAS Intelligent Decisioning to meet risk and compliance requirements.</p>
<p>SAS Intelligent Decisioning is part of a unified stack of applications on top of the cloud-native SAS Viya platform, which enable companies to make the best decision, in the moment, for every moment, while ensuring fairness and bias mitigation.</p>
<p>To see this in action, you can attend the <a href="https://explore.sas.com/">SAS Explore event</a> on Sept 27-29 or view the recording at your convenience. This <a href="https://explore.sas.com/event/8c314b18-f97b-4156-ae95-2358df0ba862/websitePage:474f0cb8-85ae-4277-81d5-a3014d74167f">session</a> will showcase the use of SAS Intelligent Decisioning, <a href="https://www.sas.com/en_us/software/model-manager.html">SAS Model Manager</a>, and <a href="https://www.sas.com/en_us/software/visual-analytics.html">SAS Visual Analytics</a> on the SAS Viya platform for a solution that helps mitigate inequitable credit decisions.</p>
<p>The post <a rel="nofollow" href="https://blogs.sas.com/content/subconsciousmusings/2022/09/14/intelligent-decisioning-ensuring-fairness-in-analytically-driven-decision-making/">Intelligent Decisioning: Ensuring fairness in analytically-driven decision making</a> appeared first on <a rel="nofollow" href="https://blogs.sas.com/content/subconsciousmusings">The SAS Data Science Blog</a>.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://blogs.sas.com/content/subconsciousmusings/2022/09/14/intelligent-decisioning-ensuring-fairness-in-analytically-driven-decision-making/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			<enclosure url="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/SAS-Explore-Header-scaled-e1663353730876-139x150.jpg" />
	</item>
		<item>
		<title>Using PROC DEEPCAUSAL to optimize revenue through policy evaluation</title>
		<link>https://blogs.sas.com/content/subconsciousmusings/2022/09/12/using-proc-deepcausal-to-optimize-revenue-through-policy-evaluation/</link>
					<comments>https://blogs.sas.com/content/subconsciousmusings/2022/09/12/using-proc-deepcausal-to-optimize-revenue-through-policy-evaluation/#respond</comments>
		
		<dc:creator><![CDATA[Gunce E. Walton]]></dc:creator>
		<pubDate>Mon, 12 Sep 2022 14:00:12 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[Analytics R&D]]></category>
		<category><![CDATA[PROC DEEPCAUSAL]]></category>
		<category><![CDATA[SAS Econometrics]]></category>
		<guid isPermaLink="false">https://blogs.sas.com/content/subconsciousmusings/?p=13536</guid>

					<description><![CDATA[<p>SAS' Gunce Walton introduces to you a new scoring capability, how it utilizes Deep Neural Networks (DNNs) and shares use cases with PROC DEEPCAUSAL.</p>
<p>The post <a rel="nofollow" href="https://blogs.sas.com/content/subconsciousmusings/2022/09/12/using-proc-deepcausal-to-optimize-revenue-through-policy-evaluation/">Using PROC DEEPCAUSAL to optimize revenue through policy evaluation</a> appeared first on <a rel="nofollow" href="https://blogs.sas.com/content/subconsciousmusings">The SAS Data Science Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p>When it comes to causal inference, scoring capability is particularly beneficial. It can be used in unique ways that result in an improved decision-making process, such as gaining optimal revenue using the least number of resources. In this post, I will introduce to you a new scoring capability and its use cases with <a href="https://documentation.sas.com/doc/en/pgmsascdc/v_030/casecon/casecon_deepcausal_toc.htm">PROC DEEPCAUSAL</a>. I will also show you how it utilizes Deep Neural Networks (DNNs) to perform causal inference as well as policy evaluation and comparison.</p>
<h2>The powerful, state-of-the-art and easy-to-use PROC DEEPCAUSAL</h2>
<p>Inference is not valid for the estimators when the estimates from machine learning methods are directly plugged into an econometric model. This way creates highly biased estimators, so econometrics methods need to correct for this bias. PROC DEEPCAUSAL does this by implementing the doubly robust estimation method (suggested by <a href="https://doi.org/10.3982/ECTA16901">Max H. Farrell, Tengyuan Liang, and Sanjog Misra (2021</a>). This method applies DNNs, a powerful machine learning technique, and provides estimates for various causal effect parameters. As well, it performs policy evaluation and policy comparison. DNNs overcome several technical difficulties. These include big data problems, such as high-dimensional covariates; having a mix of discrete or continuous covariates: the unknown nonlinear relationships among the covariates, the outcome, and the treatment assignment. This makes PROC DEEPCAUSAL a very powerful tool for causal inference or analyzing a treatment effect.</p>
<p>PROC DEEPCAUSAL has two main statements for specifying the causal model. First, there is the <a href="https://documentation.sas.com/doc/en/pgmsascdc/v_030/casecon/casecon_deepcausal_syntax07.htm">PSMODEL statement</a> for specifying the propensity score model. Second is the <a href="https://documentation.sas.com/doc/en/pgmsascdc/v_030/casecon/casecon_deepcausal_syntax06.htm">MODEL statement</a> for specifying the outcome model. The propensity score model estimates the treatment by assigning probability conditional on some covariates. The outcome model describes how the outcome depends on the treatment and some covariates. It also consists of two unknown functions, α( ⋅) and β(⋅), that are estimated by a DNN.</p>
<p>If you are wondering what causal inference is, check out this <a href="https://blogs.sas.com/content/subconsciousmusings/2021/09/07/causal-inference-and-policy-evaluation-with-deep-neural-networks/">post</a>. It provides preliminary knowledge that makes it easier to understand the concepts explained here.</p>
<h2>The new SCORE statement in PROC DEEPCAUSAL</h2>
<p>What exactly is scoring? Scoring is the act of producing predictions for a target variable from a predictive model. The benefit of scoring is to avoid the cost of refitting the model when new predictions are needed. Scoring new data to compute predictions for an existing model is a fundamental stage in the analytics life cycle. For example, in industrial process control or monitoring of atmospheric pollutant levels, a sequential data stream is monitored continuously to flag unexpected changes in the normal behavior of the process as soon as possible. Scoring in these situations can be quite useful.</p>
<p>In the context of causal inference, computing predictions for new data or testing the precision of prediction on a test data set are only a part of the advantages of scoring. The scoring capability in causal inference can be useful in many different and important ways that I will be discussing in the next sections.</p>
<p>You can access all the benefits of scoring capability in PROC DEEPCAUSAL by using the recently added <a href="https://documentation.sas.com/doc/en/pgmsascdc/v_030/casecon/casecon_deepcausal_syntax08.htm">SCORE statement</a>.</p>
<p>Let’s look at an example that demonstrates the various advantages of PROC DEEPCAUSAL scoring capability. The causal model in this example is an online music subscription service estimating the effect of targeted customer discounts on the company’s revenue.</p>
<p>The original data set is provided by the Microsoft research project ALICE. For more details, see <a href="https://go.documentation.sas.com/doc/en/pgmsascdc/v_030/casecon/casecon_deepcausal_examples02.htm">Example 14.2</a> in the PROC DEEPCAUSAL documentation. The data are synthetically generated to protect the privacy of the company, but the model variable names and their meanings are preserved. The data have 10,000 observations and include customers’ personal characteristics, such as age and log-income. The data also includes online behavior history, such as previous purchases and previous online times per week. The treatment variable t is the binary variable of whether or not the discount is applied. The output variable is revenue. Table 1 shows the names of the variables that are used in the model, their types, and their definitions.</p>
<table width="100%">
<tbody>
<tr>
<td width="25%"><strong>Name</strong></td>
<td width="25%"><strong>Variable Type</strong></td>
<td width="75%"><strong>Details</strong></td>
</tr>
<tr>
<td width="25%">account_age</td>
<td width="25%">covariate</td>
<td width="75%">User’s account age</td>
</tr>
<tr>
<td width="25%">age</td>
<td width="25%">covariate</td>
<td width="75%">User’s age</td>
</tr>
<tr>
<td width="25%">avg_hours</td>
<td width="25%">covariate</td>
<td width="75%">Average number of hours user was online per week in the past</td>
</tr>
<tr>
<td width="25%">days_visited</td>
<td width="25%">covariate</td>
<td width="75%">Average number of days user visited website per week in the past</td>
</tr>
<tr>
<td width="25%">friend_count</td>
<td width="25%">covariate</td>
<td width="75%">Number of friends user connected to in account</td>
</tr>
<tr>
<td width="25%">has_membership</td>
<td width="25%">covariate</td>
<td width="75%">Whether user has membership</td>
</tr>
<tr>
<td width="25%">is_US</td>
<td width="25%">covariate</td>
<td width="75%">Whether user accesses website from US</td>
</tr>
<tr>
<td width="25%">songs_purchased</td>
<td width="25%">covariate</td>
<td width="75%">Average number of songs user purchased per week in the past</td>
</tr>
<tr>
<td width="25%">income</td>
<td width="25%">covariate</td>
<td width="75%">User’s income</td>
</tr>
<tr>
<td width="25%">t</td>
<td width="25%">treatment</td>
<td width="75%">Whether a discount is applied</td>
</tr>
<tr>
<td width="25%">revenue</td>
<td width="25%">outcome</td>
<td width="75%">Number of songs purchased during discount season times price paid</td>
</tr>
</tbody>
</table>
<p><em>Table 1: Model Variables</em></p>
<p>The data are split into three data tables - the training, testing, and “new” data sets. The “new” data observations for the treatment (discount) and the outcome variables (revenue) are set to missing. So, for these customers, you need to figure out the treatment assignment.</p>
<p>The SAS code below demonstrates how to estimate the treatment effect, that is the effect of discount, and saves the estimation details. These details will be used as input for scoring.</p>

<div class="wp_syntax"><table><tr><td class="code"><pre class="sas" style="font-family:monospace;">   <span style="color: #006400; font-style: italic;">* estimate the treatment effect and save the estimation details;</span>
   <span style="color: #000080; font-weight: bold;">proc deepcausal</span> <span style="color: #000080; font-weight: bold;">data</span>=mycas.pricing_sample_train;
      id rowindex;
      psmodel t = account_age age avg_hours days_visited friends_count
                  has_membership is_US songs_purchased income /
                  dnn=<span style="color: #66cc66;">&#40;</span>nodes=<span style="color: #66cc66;">&#40;</span><span style="color: #2e8b57; font-weight: bold;">32</span> <span style="color: #2e8b57; font-weight: bold;">32</span> <span style="color: #2e8b57; font-weight: bold;">32</span> <span style="color: #2e8b57; font-weight: bold;">32</span><span style="color: #66cc66;">&#41;</span>
                  train=<span style="color: #66cc66;">&#40;</span>optimizer=<span style="color: #66cc66;">&#40;</span>miniBatchSize=<span style="color: #2e8b57; font-weight: bold;">500</span> regL1=<span style="color: #2e8b57; font-weight: bold;">0.0001</span> maxEpochs=<span style="color: #2e8b57; font-weight: bold;">32000</span>
                  algorithm=adam<span style="color: #66cc66;">&#41;</span> nthreads=<span style="color: #2e8b57; font-weight: bold;">20</span> seed=<span style="color: #2e8b57; font-weight: bold;">12345</span> recordseed=<span style="color: #2e8b57; font-weight: bold;">67890</span><span style="color: #66cc66;">&#41;</span><span style="color: #66cc66;">&#41;</span>;
      model revenue = account_age age avg_hours days_visited friends_count
                  has_membership is_US songs_purchased income /
                  dnn=<span style="color: #66cc66;">&#40;</span>nodes=<span style="color: #66cc66;">&#40;</span><span style="color: #2e8b57; font-weight: bold;">32</span> <span style="color: #2e8b57; font-weight: bold;">32</span> <span style="color: #2e8b57; font-weight: bold;">32</span> <span style="color: #2e8b57; font-weight: bold;">32</span><span style="color: #66cc66;">&#41;</span>
                  train=<span style="color: #66cc66;">&#40;</span>optimizer=<span style="color: #66cc66;">&#40;</span>miniBatchSize=<span style="color: #2e8b57; font-weight: bold;">500</span> regL1=<span style="color: #2e8b57; font-weight: bold;">0.001</span> maxEpochs=<span style="color: #2e8b57; font-weight: bold;">32000</span>
                  algorithm=adam<span style="color: #66cc66;">&#41;</span> nthreads=<span style="color: #2e8b57; font-weight: bold;">20</span> seed=<span style="color: #2e8b57; font-weight: bold;">12345</span> recordseed=<span style="color: #2e8b57; font-weight: bold;">67890</span><span style="color: #66cc66;">&#41;</span><span style="color: #66cc66;">&#41;</span>;
      infer out=mycas.oest outdetails=mycas.odetails;
      score outps=mycas.outpsdata outa=mycas.outadata outb=mycas.outbdata;
   <span style="color: #000080; font-weight: bold;">run</span>;</pre></td></tr></table></div>

<p>The output from this estimation is below.</p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/sept-walton-code-1-output-estimate-treatment-effect.png"><img loading="lazy" class="aligncenter size-full wp-image-13569" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/sept-walton-code-1-output-estimate-treatment-effect.png" alt="PROC DEEPCAUSAL Output estimation" width="687" height="532" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/sept-walton-code-1-output-estimate-treatment-effect.png 687w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/sept-walton-code-1-output-estimate-treatment-effect-300x232.png 300w" sizes="(max-width: 687px) 100vw, 687px" /></a></p>
<p>The parameter average potential outcome for an untreated customer is about 14.32, indicating the average effect on the revenue from this customer type. Similarly, the average potential outcome for a treated customer is about 13.6, indicating the average effect on the revenue from this customer type. The parameter ATE (average treatment effect) is usually the main parameter of interest in causal inference. The value of ATE is about -0.72.</p>
<p>This is statistically significant, suggesting that the discount, on average, causes revenue to decrease. However, the estimate for ATT (average treatment effect on the treated) reflects a positive effect of the discount on revenue. The value is small but significant. This could mean that the discount was given to some customers who didn’t care about it. Or the discount wasn’t given to some customers who would have used it. Identifying the characteristics of customers to whom the discount matters helps construct the optimum policy to use the least amount of resources to gain the most profit. The scoring ability can help with this quite efficiently.</p>
<h2>Scoring on testing data</h2>
<p>Testing the precision of prediction is important. However, in the causal inference context, two other points are also important to consider. The first would be making sure that the observations in the two data sets are coming from the same data generating process (DGP). This would help you understand whether the two policies are the same or not. The second would be to see if there are any outliers in the new data set. You can use the scoring capability of PROC DEEPCAUSAL to handle these three important points. This is how the SAS code might look.</p>

<div class="wp_syntax"><table><tr><td class="code"><pre class="sas" style="font-family:monospace;"><span style="color: #006400; font-style: italic;">* scoring 1: on testing data:
   * (1) testing prediction precision (for outcome of regression or treatment of classification);</span>
   <span style="color: #006400; font-style: italic;">* (2) finding out whether the DGPs are the same (e.g., is there a policy difference?) and the outliers;</span>
   <span style="color: #000080; font-weight: bold;">proc deepcausal</span> <span style="color: #000080; font-weight: bold;">data</span>=mycas.pricing_sample_test;
      id rowindex;
      infer out=mycas.oest2 outdetails=mycas.odetails2;
      score inps=mycas.outpsdata ina=mycas.outadata inb=mycas.outbdata;
   <span style="color: #000080; font-weight: bold;">run</span>;</pre></td></tr></table></div>

<p>Note that the MODEL and PSMODEL statements are not included. With the SCORE statement, the model estimation information is already included. So, there is no need to reestimate the model. This saves you a considerable amount of time.</p>
<p><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/sept-walton-code-2-output-scoring-testing-data.png"><img loading="lazy" class="aligncenter size-full wp-image-13572" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/sept-walton-code-2-output-scoring-testing-data.png" alt="PROC DEEPCAUSAL Output 2 testing data " width="765" height="533" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/sept-walton-code-2-output-scoring-testing-data.png 765w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/sept-walton-code-2-output-scoring-testing-data-300x209.png 300w" sizes="(max-width: 765px) 100vw, 765px" /></a></p>
<p>The results are interesting. The estimates for the ATE and many of the other parameters are completely different for the test data set as compared to the training data set. In fact, they are so “off” that it’s hard to make sense out of them. What is going on? There are two explanations for this. Either the two data sets are not generated from the same DGP or there are some outliers. As I mentioned before, this example uses simulated data, eliminating the first possibility. So, there must be some outliers. You can easily identify and eliminate them. Then you can confirm that scoring again produces more similar results to those obtained using the training data set.</p>
<h2>Scoring for determining the treatment of new customers</h2>
<p>So how would you deal with new customers? How would you determine whether to give them a discount or not given the characteristics that you observe about them?</p>
<p>The third data set contains the information for these new customers. Note that the new observations are missing the outcome and the treatment variable values. Therefore, it is impossible to incorporate the new data into the existing one and reestimate the model. However, we can still use these data to determine who receives the discount and who doesn’t by using the score action of the aStore action set.</p>
<p>One way of approaching this problem is to follow the existing policy we observed in the data set. For this, you can use the analytic store for the propensity score model, outpsdata, that is generated using the OUTPS= option in the SCORE statement during the first call to PROC DEEPCAUSAL. Then calculate the logit of the transformed propensity score and construct a policy based on this propensity score. The SAS code might look like this.</p>

<div class="wp_syntax"><table><tr><td class="code"><pre class="sas" style="font-family:monospace;">   <span style="color: #006400; font-style: italic;">* Scoring 2: Determine the treatment on new customers;</span>
   <span style="color: #006400; font-style: italic;">* 1) follow the existing policy;</span>
   <span style="color: #000080; font-weight: bold;">proc cas</span>;
      aStore.score / <span style="color: #0000ff;">table</span>=<span style="color: #a020f0;">&quot;pricing_sample_new_customers&quot;</span> rstore=<span style="color: #a020f0;">&quot;outpsdata&quot;</span> casout=<span style="color: #66cc66;">&#123;</span>name=<span style="color: #a020f0;">&quot;oldway&quot;</span>,<span style="color: #0000ff;">replace</span>=true<span style="color: #66cc66;">&#125;</span> copyvars=<span style="color: #66cc66;">&#123;</span><span style="color: #a020f0;">&quot;rowindex&quot;</span><span style="color: #66cc66;">&#125;</span>;
   <span style="color: #000080; font-weight: bold;">quit</span>;
&nbsp;
   <span style="color: #000080; font-weight: bold;">data</span> oldway;
      <span style="color: #0000ff;">set</span> mycas.oldway;
      <span style="color: #0000ff;">call</span> streaminit<span style="color: #66cc66;">&#40;</span><span style="color: #2e8b57; font-weight: bold;">12345</span><span style="color: #66cc66;">&#41;</span>;
      u = rand<span style="color: #66cc66;">&#40;</span><span style="color: #a020f0;">&quot;uniform&quot;</span><span style="color: #66cc66;">&#41;</span>;
      t_old = <span style="color: #66cc66;">&#40;</span>u &lt;= <span style="color: #66cc66;">&#40;</span><span style="color: #2e8b57; font-weight: bold;">1</span>/<span style="color: #66cc66;">&#40;</span><span style="color: #2e8b57; font-weight: bold;">1</span>+<span style="color: #0000ff;">exp</span><span style="color: #66cc66;">&#40;</span>-P_t<span style="color: #66cc66;">&#41;</span><span style="color: #66cc66;">&#41;</span><span style="color: #66cc66;">&#41;</span><span style="color: #66cc66;">&#41;</span>;
   <span style="color: #000080; font-weight: bold;">run</span>;</pre></td></tr></table></div>

<p>However, this approach does not necessarily optimize the revenue from these new customers. The optimum policy, the one that maximizes the revenue, should always lead to a positive treatment effect. In our framework, this translates into treating everyone whose Individual Treatment Effect (ITE) is positive. The function β(x) measures ITE and this function is unknown. Therefore, in practice, an estimate of it is used. This, in effect, translates into a policy giving discounts to those whose predicted beta values are positive. For this, you can use the astore for the information for the beta function estimation, outbdata (similarly generated using the OUTB= option), and create a new policy variable that takes the value of 1 if the predicted revenue for these new customers is positive and the value 0 if it is negative.</p>

<div class="wp_syntax"><table><tr><td class="code"><pre class="sas" style="font-family:monospace;"><span style="color: #006400; font-style: italic;">* 2) optimize the policy;</span>
   <span style="color: #000080; font-weight: bold;">proc cas</span>;
      aStore.score / <span style="color: #0000ff;">table</span>=<span style="color: #a020f0;">&quot;pricing_sample_new_customers&quot;</span> rstore=<span style="color: #a020f0;">&quot;outbdata&quot;</span> casout=<span style="color: #66cc66;">&#123;</span>name=<span style="color: #a020f0;">&quot;newway&quot;</span>,<span style="color: #0000ff;">replace</span>=true<span style="color: #66cc66;">&#125;</span> copyvars=<span style="color: #66cc66;">&#123;</span><span style="color: #a020f0;">&quot;rowindex&quot;</span><span style="color: #66cc66;">&#125;</span>;
   <span style="color: #000080; font-weight: bold;">quit</span>;
&nbsp;
   <span style="color: #000080; font-weight: bold;">data</span> newway;
      <span style="color: #0000ff;">set</span> mycas.newway;
      t_new = <span style="color: #66cc66;">&#40;</span><span style="color: #66cc66;">&#40;</span>P__revenue_beta_<span style="color: #66cc66;">&#41;</span> &gt; <span style="color: #2e8b57; font-weight: bold;">0</span><span style="color: #66cc66;">&#41;</span>;
   <span style="color: #000080; font-weight: bold;">run</span>;</pre></td></tr></table></div>

<p>The last two columns of Table 2 show the resulting two policies, t_old and t_new, which is also the optimal policy, for the first 20 observations.</p>
<div id="attachment_13575" style="width: 435px" class="wp-caption aligncenter"><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/sept-walton-code-3-table-old-new-policies-customers.png"><img aria-describedby="caption-attachment-13575" loading="lazy" class="size-full wp-image-13575" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/sept-walton-code-3-table-old-new-policies-customers.png" alt="Table 1: Old and New (Optimized) Policies for the New Customers" width="425" height="522" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/sept-walton-code-3-table-old-new-policies-customers.png 425w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/sept-walton-code-3-table-old-new-policies-customers-244x300.png 244w" sizes="(max-width: 425px) 100vw, 425px" /></a><p id="caption-attachment-13575" class="wp-caption-text">Table 2: Old and New (Optimized) Policies for the New Customers</p></div>
<p>The design of this optimal policy runs into the problem of interpretability. The function β(x) is unknown,  and a DNN is used to obtain an estimate for it. It’s not known how individual covariates affect the design of the policy. This interpretability issue might be a problem, particularly for policymakers at the government level. In practice, the optimized policy should be as simple as possible and easily explainable. It’s better to use just a small subset of the covariates and easy-to-follow rules. Of course, this raises the question of which subset. Fortunately, the scoring capability of PROC DEEPCAUSAL can help!</p>
<h2>Scoring for policy evaluation/comparison/optimization</h2>
<p>The optimal policy in application, that is giving discounts to customers for whom _BETA_ is positive, is hard to beat. But at least you can come up with a policy that can come close to the optimal policy and is interpretable. Consider a way of doing personalized discounts based on income. Let’s say you’d like to target customers with the log-income level of at least a threshold level that you pick, say, values starting from 0 to 6.25 by 0.25 increments. Here are the data steps that take care of creating this set of policies.</p>

<div class="wp_syntax"><table><tr><td class="code"><pre class="sas" style="font-family:monospace;">   <span style="color: #006400; font-style: italic;">* scoring 3: policy learning, the evaluation and comparison;</span>
   <span style="color: #006400; font-style: italic;">* create policies based on income;</span>
   <span style="color: #000080; font-weight: bold;">data</span> mycas.discountPolicy_income;
      <span style="color: #0000ff;">set</span> mycas.odetails;
      <span style="color: #0000ff;">array</span> pi<span style="color: #66cc66;">&#91;</span><span style="color: #2e8b57; font-weight: bold;">26</span><span style="color: #66cc66;">&#93;</span> pi0 - pi25;
      <span style="color: #0000ff;">do</span> k = <span style="color: #2e8b57; font-weight: bold;">1</span> to <span style="color: #2e8b57; font-weight: bold;">26</span>;
         pi<span style="color: #66cc66;">&#91;</span>k<span style="color: #66cc66;">&#93;</span> = <span style="color: #66cc66;">&#40;</span>income &lt;= <span style="color: #2e8b57; font-weight: bold;">0.25</span><span style="color: #006400; font-style: italic;">*(k-1));</span> 
         <span style="color: #0000ff;">end</span>; 
         t_opt = <span style="color: #66cc66;">&#40;</span>_beta_&gt;<span style="color: #2e8b57; font-weight: bold;">0</span><span style="color: #66cc66;">&#41;</span>;
      <span style="color: #0000ff;">keep</span> rowindex id t account_age age avg_hours days_visited
           friends_count has_membership is_US 
           songs_purchased income revenue pi0-pi25 t_opt;
   <span style="color: #000080; font-weight: bold;">run</span>;
&nbsp;
   <span style="color: #000080; font-weight: bold;">proc deepcausal</span> <span style="color: #000080; font-weight: bold;">data</span>=mycas.discountPolicy_income;
      id id;
      infer policy=<span style="color: #66cc66;">&#40;</span>t_opt pi0-pi25<span style="color: #66cc66;">&#41;</span> policyComparison=<span style="color: #66cc66;">&#40;</span>base=<span style="color: #66cc66;">&#40;</span>t_opt<span style="color: #66cc66;">&#41;</span> compare=<span style="color: #66cc66;">&#40;</span>t pi0-pi25<span style="color: #66cc66;">&#41;</span><span style="color: #66cc66;">&#41;</span>
            out=mycas.oest4 outdetails=mycas.odetails4;
      score inps=mycas.outpsdata ina=mycas.outadata inb=mycas.outbdata;
   <span style="color: #000080; font-weight: bold;">run</span>;</pre></td></tr></table></div>

<p>There are 26 policies, pi0 through pi25. Policies pi0 and pi25 have special meanings. Policy pi0 corresponds to the policy of giving no discount, as no one’s income is less than 0. Policy pi25 corresponds to giving a discount to everyone, as everyone in this data set has income less than 6.25. Now you can run PROC DEEPCAUSAL including this new set of policies in the POLICY= option in the INFER statement. Naturally, comparing this policy with the optimal policy is desired, therefore, t_opt, the optimal policy, is specified in the BASE= suboption of the POLICYCOMPARISON= option. For policy comparison, a negative value for a given policy indicates that it achieves a worse result than the base policy, and a value close to 0 indicates that it is very similar to the base policy. A plot of policy evaluation output is in Figure 1.</p>
<div id="attachment_13578" style="width: 658px" class="wp-caption aligncenter"><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/sept-walton-code-4-plot.png"><img aria-describedby="caption-attachment-13578" loading="lazy" class="size-full wp-image-13578" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/sept-walton-code-4-plot.png" alt="PROC DEEPCAUSAL Figure 1: Plot of the effect of new set of policies on revenue" width="648" height="489" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/sept-walton-code-4-plot.png 648w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/sept-walton-code-4-plot-300x226.png 300w" sizes="(max-width: 648px) 100vw, 648px" /></a><p id="caption-attachment-13578" class="wp-caption-text">Figure 1: Plot of the effect of a new set of policies on revenue</p></div>
<p>The policy pi4 is almost as good as the optimal policy. Policy pi4 corresponds to threshold 1, that is the policy to provide discounts to customers whose log-income is less than or equal to 1. An important advantage of the policy pi4 is its easy interpretability, unlike the optimal policy.</p>
<p>So, can we do even better? Can we find another set of policies that can outperform pi4 and come even closer to the optimal policy t_opt? This is where the scoring capability comes in handy (again). You can try various sets of policies and compare them with the optimal policy as the base policy by using the POLICYCOMPARISON= option. Thanks to the SCORE statement, each call to PROC DEEPCAUSAL takes only seconds!</p>
<h2>Conclusion</h2>
<p>Hopefully, you can now see how the scoring capability of PROC DEEPCAUSAL can contribute to making better business decisions. In the causal inference framework, the scoring extends beyond testing prediction. You can also benefit from the scoring capability in many ways that are unique to causal inference. As I demonstrated, you could use it for checking the data accuracy. You can determine if the observations from the two data sets come from the same DGP as well as uncover potential outliers.</p>
<p>I also showed how you can use scoring to determine the treatment on completely new observations. As well, you can also use it for policy evaluation, comparison, and optimization. The latter advantage is particularly important for businesses. They are increasingly interested in understanding the different responses from customers so they can tailor their actions to the customer's needs. Understanding customer characteristics helps businesses to construct policies that use the least number of resources to optimize revenue. The scoring capability in PROC DEEPCAUSAL can most definitely help with that goal.</p>
<p>The post <a rel="nofollow" href="https://blogs.sas.com/content/subconsciousmusings/2022/09/12/using-proc-deepcausal-to-optimize-revenue-through-policy-evaluation/">Using PROC DEEPCAUSAL to optimize revenue through policy evaluation</a> appeared first on <a rel="nofollow" href="https://blogs.sas.com/content/subconsciousmusings">The SAS Data Science Blog</a>.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://blogs.sas.com/content/subconsciousmusings/2022/09/12/using-proc-deepcausal-to-optimize-revenue-through-policy-evaluation/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			<enclosure url="https://blogs.sas.com/content/subconsciousmusings/files/2022/09/sept-walton-banner-c-150x150.jpg" />
	</item>
		<item>
		<title>Analyzing demographics and patterns-of-life using SAS Visual Analytics</title>
		<link>https://blogs.sas.com/content/subconsciousmusings/2022/09/07/analyzing-demographics-and-patterns-of-life-using-sas-visual-analytics/</link>
					<comments>https://blogs.sas.com/content/subconsciousmusings/2022/09/07/analyzing-demographics-and-patterns-of-life-using-sas-visual-analytics/#respond</comments>
		
		<dc:creator><![CDATA[Falko Schulz]]></dc:creator>
		<pubDate>Thu, 08 Sep 2022 02:54:27 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[advanced analytics]]></category>
		<category><![CDATA[analytics]]></category>
		<category><![CDATA[data visualization]]></category>
		<category><![CDATA[machine learning]]></category>
		<category><![CDATA[SAS Visual Analytics]]></category>
		<category><![CDATA[SAS Viya]]></category>
		<guid isPermaLink="false">https://blogs.sas.com/content/subconsciousmusings/?p=13360</guid>

					<description><![CDATA[<p>The IEEE Visual Analytics Science and Technology (VAST) Challenge provides a great opportunity to validate our software against real-world scenarios using complex data sets. Not only do we learn from these projects, but we also send feedback to our development teams to further improve product capabilities for customers.</p>
<p>The post <a rel="nofollow" href="https://blogs.sas.com/content/subconsciousmusings/2022/09/07/analyzing-demographics-and-patterns-of-life-using-sas-visual-analytics/">Analyzing demographics and patterns-of-life using SAS Visual Analytics</a> appeared first on <a rel="nofollow" href="https://blogs.sas.com/content/subconsciousmusings">The SAS Data Science Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[<h4>Increased access to data provided by social channels helps industries not only understand demographics and relationships but also analyze historic and future market growth by looking at the patterns of a given target population's daily lives.</h4>
<p>This year's <a href="https://vast-challenge.github.io/2022/">IEEE Visual Analytics Science and Technology (VAST) Challenge</a> covered such a scenario and a team of SAS data scientists decided to put <a href="https://www.sas.com/en_us/software/viya.html">SAS® Viya®</a> to the test and submit a solution for both Challenge 1 and 2. The mission was to analyze the demographics and relationships of a city in the United States with access to detailed characteristics of about 1,000 residents over a period of one year. While Challenge 1 focuses on the city's demographics and social relationships of the residents, Challenge 2 considers the patterns of daily life throughout the city. The challenge was to describe the daily routines of some of the representative people, characterize their travel patterns and analyze changes over time and across seasons.</p>
<p>If you are interested in reading about our submission for last year's challenge, you can find all the details in this related <a href="https://blogs.sas.com/content/subconsciousmusings/2021/09/02/analyzing-movement-and-tracking-data-using-sas-visual-analytics/">blog post</a>.</p>
<h2>The process</h2>
<p>We used tools provided by <a href="https://www.sas.com/en_us/software/viya.html">SAS® Viya</a> to import, adjust and format the provided data. We had access to residents' home locations, buildings, employers and financial records. Some of the data files, e.g. individual transaction status updates, exceeded well above 100mio observations. We used <a href="https://www.sas.com/en_us/software/visual-analytics.html">SAS® Visual Analytics</a> to explore and understand the cities characteristics and social patterns. Being able to understand where particular groups of residents live and work is important when analyzing travel patterns. We also utilized Esri's <a href="https://www.arcgis.com/">ArcGIS Online</a> services to create a custom map visualizing the cities boundaries and landmarks. SAS Viya's support for machine learning (ML) and network analytics was used to analyze social patterns and relationships.</p>
<h3>The tools</h3>
<p>The majority of work was done using SAS Studio and SAS Visual Analytics. We leveraged some core capabilities in SAS data steps to prepare and adjust the provided data for easier analysis. In particular, we stored all provided CSV files in SAS Viya and aggregated some for easier analysis. Additional attributes such as home locations, detail building types, employer visits or turnovers were calculated. We also used Esri's ArcGIS mapping tools to create a custom map tile service representing the city's boundaries, buildings and streets. Custom maps like these can be embedded in SAS Visual Analytics for any geospatial analysis.</p>
<h3>The solution - Challenge 1</h3>
<p>The first challenge focuses on the city's characteristics and demographics. We identified that the average age is 39 among our representatives with some as young as 18. To better understand the geographical distribution based on age groups, we used a heat map visualization:</p>
<div id="attachment_13372" style="width: 712px" class="wp-caption aligncenter"><img aria-describedby="caption-attachment-13372" loading="lazy" class=".border-image wp-image-13372 size-large" style="border-radius: 1rem;" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c1-q1-1-1024x877.png" alt="" width="702" height="601" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c1-q1-1-1024x877.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c1-q1-1-300x257.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c1-q1-1-768x658.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c1-q1-1.png 1360w" sizes="(max-width: 702px) 100vw, 702px" /><p id="caption-attachment-13372" class="wp-caption-text">Age group distribution throughout the city</p></div>
<p>Data shows that we have residents living as single or as a family of 3. Families with children represent about 28% of the population. Most residents (about 50%) hold a high school or college degree.</p>
<div id="attachment_13378" style="width: 712px" class="wp-caption aligncenter"><img aria-describedby="caption-attachment-13378" loading="lazy" class="size-large wp-image-13378" style="border-radius: 1rem;" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c1-q1-3-998x1024.png" alt="" width="702" height="720" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c1-q1-3-998x1024.png 998w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c1-q1-3-292x300.png 292w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c1-q1-3-768x788.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c1-q1-3.png 1359w" sizes="(max-width: 702px) 100vw, 702px" /><p id="caption-attachment-13378" class="wp-caption-text">Education level, household size and families with children distribution</p></div>
<p>There are various schools, restaurants and pubs located throughout the city. Some are also employers for the residents. There is an average of about 5 jobs per employer in the city with some providing up to 9 jobs. Most employers (about 70%) require at least a high school or college degree.</p>
<div id="attachment_13380" style="width: 712px" class="wp-caption aligncenter"><img aria-describedby="caption-attachment-13380" loading="lazy" class="size-large wp-image-13380" style="border-radius: 1rem;" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c1-q1-5-986x1024.png" alt="" width="702" height="729" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c1-q1-5-986x1024.png 986w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c1-q1-5-289x300.png 289w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c1-q1-5-768x798.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c1-q1-5.png 1342w" sizes="(max-width: 702px) 100vw, 702px" /><p id="caption-attachment-13380" class="wp-caption-text">Job availability and education requirements</p></div>
<p>Using SAS Viya's network analytics procedure, we searched for various travel patterns to better understand the daily social activities of the participants. When analyzing patterns, we took into account the travel end location, purpose and check-in time. Using the pattern match algorithm, we were able to determine the frequency of given patterns in the data.</p>
<div id="attachment_13382" style="width: 712px" class="wp-caption aligncenter"><img aria-describedby="caption-attachment-13382" loading="lazy" class="size-large wp-image-13382" style="border-radius: 1rem;" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c1-q2-2-1024x840.png" alt="" width="702" height="576" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c1-q2-2-1024x840.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c1-q2-2-300x246.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c1-q2-2-768x630.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c1-q2-2-168x137.png 168w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c1-q2-2.png 1377w" sizes="(max-width: 702px) 100vw, 702px" /><p id="caption-attachment-13382" class="wp-caption-text">Comparison weekday and weekend travel</p></div>
<p>Eating out is a common activity after work and in general. The median participant ate out 469 times in total and ate out 130 times after a work day (40% of the workdays).</p>
<div id="attachment_13384" style="width: 653px" class="wp-caption aligncenter"><img aria-describedby="caption-attachment-13384" loading="lazy" class="size-large wp-image-13384" style="border-radius: 1rem;" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c1-q2-4-643x1024.png" alt="" width="643" height="1024" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c1-q2-4-643x1024.png 643w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c1-q2-4-188x300.png 188w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c1-q2-4-768x1223.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c1-q2-4-964x1536.png 964w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c1-q2-4-1286x2048.png 1286w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c1-q2-4.png 1392w" sizes="(max-width: 643px) 100vw, 643px" /><p id="caption-attachment-13384" class="wp-caption-text">Restaurant visits</p></div>
<div id="attachment_13386" style="width: 306px" class="wp-caption alignright"><img aria-describedby="caption-attachment-13386" loading="lazy" class="wp-image-13386 size-medium" style="border-radius: 1rem;" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c1-q3-1-296x300.png" alt="" width="296" height="300" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c1-q3-1-296x300.png 296w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c1-q3-1-1011x1024.png 1011w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c1-q3-1-768x778.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c1-q3-1.png 1374w" sizes="(max-width: 296px) 100vw, 296px" /><p id="caption-attachment-13386" class="wp-caption-text">Pub and restaurant sales across the city</p></div>
<p>The citizens in the study primarily went to two locations during their free time: pubs and restaurants. Pubs were the dominant place to engage in social gatherings. There are five primary locations of pubs and restaurants:</p>
<ul>
<li>Central</li>
<li>North Central</li>
<li>Northwest</li>
<li>East</li>
<li>South</li>
</ul>
<p>&nbsp;</p>
<p>We discovered 6 clusters of weekly sale patterns by restaurants and pubs. There is no clear relationship between weekly sales patterns and location within the city. The y-axis represents normalized total sales. The higher the value, the greater the sales at that time. The clusters below are zoomed into a representative date range and are designed to show the general shape of sales patterns over time.</p>
<div id="attachment_13388" style="width: 310px" class="wp-caption alignright"><img aria-describedby="caption-attachment-13388" loading="lazy" class="size-medium wp-image-13388" style="border-radius: 1rem;" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c1-q3-10_cluster1-300x159.png" alt="" width="300" height="159" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c1-q3-10_cluster1-300x159.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c1-q3-10_cluster1-1024x544.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c1-q3-10_cluster1-768x408.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c1-q3-10_cluster1-351x185.png 351w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c1-q3-10_cluster1.png 1381w" sizes="(max-width: 300px) 100vw, 300px" /><p id="caption-attachment-13388" class="wp-caption-text">Weekly sale patterns (Cluster 1)</p></div>
<ul>
<li><strong>Cluster 1</strong>: Consists almost entirely of pubs with the exception of a single restaurant (895) that has similar demand patterns to pubs. Saturday and Sunday both have sharp increases in sales. Weekdays tend to have much fewer sales.</li>
<li><strong>Cluster 2</strong>: Restaurants whose sales tend to be greatest during weekdays. Weekends have fewer sales.</li>
<li><strong>Cluster 3</strong>: Restaurants whose sales tend to be greatest on Saturday followed by Sunday. Weekdays have fewer sales.</li>
<li><strong>Cluster 4</strong>: Two restaurants whose sales are relatively evenly distributed across weekdays with few peaks in between.</li>
<li><strong>Cluster 5</strong>: Four restaurants whose sales are generally higher on weekdays but tend to have slightly higher sales on weekends compared to clusters 2, 3, and 4. Sales tend to be more erratic throughout the week compared to other clusters.</li>
<li><strong>Cluster 6</strong>: Four restaurants whose sales tend to peak on Sundays, with Saturday close behind.</li>
</ul>
<h3>The solution - Challenge 2</h3>
<p>The second challenge focuses on the individuals and their patterns of daily life. We used transactional data from the participants' daily routines to identify various distinct areas in the city. Given our analysis, we were able to divide the city into areas for residential and business use.</p>
<div id="attachment_13392" style="width: 712px" class="wp-caption aligncenter"><img aria-describedby="caption-attachment-13392" loading="lazy" class="size-large wp-image-13392" style="border-radius: 1rem;" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c2-q1-1-914x1024.png" alt="" width="702" height="786" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c2-q1-1-914x1024.png 914w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c2-q1-1-268x300.png 268w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c2-q1-1-768x860.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c2-q1-1-1371x1536.png 1371w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c2-q1-1.png 1508w" sizes="(max-width: 702px) 100vw, 702px" /><p id="caption-attachment-13392" class="wp-caption-text">Residential and business districts</p></div>
<div id="attachment_13394" style="width: 289px" class="wp-caption alignright"><img aria-describedby="caption-attachment-13394" loading="lazy" class="size-medium wp-image-13394" style="border-radius: 1rem;" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c2-q1-2-279x300.png" alt="" width="279" height="300" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c2-q1-2-279x300.png 279w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c2-q1-2-951x1024.png 951w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c2-q1-2-768x827.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c2-q1-2.png 1283w" sizes="(max-width: 279px) 100vw, 279px" /><p id="caption-attachment-13394" class="wp-caption-text">Residential and business activity</p></div>
<p>Most of the residential buildings (shown as yellow in the figure to the right) are in the outer areas of the city which means long commutes to business locations (shown as red). Combining the business district heat map and residential information reveals a potential source of bottlenecks during peak hours as employees travel from the outer city areas into each of the business districts.</p>
<div id="attachment_13396" style="width: 310px" class="wp-caption alignleft"><img aria-describedby="caption-attachment-13396" loading="lazy" class="size-medium wp-image-13396" style="border-radius: 1rem;" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c2-q1-4-300x280.png" alt="" width="300" height="280" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c2-q1-4-300x280.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c2-q1-4-1024x954.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c2-q1-4-768x716.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c2-q1-4.png 1324w" sizes="(max-width: 300px) 100vw, 300px" /><p id="caption-attachment-13396" class="wp-caption-text">Clustering happiness score</p></div>
<p>Looking into the happiness score for participants shows that the majority of people are happier if they live close to the center of the city. Likely shorter commutes, better access to facilities, and employment options contribute.</p>
<p>&nbsp;</p>
<p>The traffic patterns of the city depend very much upon the time of the day. There are several potential areas of interest. Many people are commuting to work and increased traffic is measurable during 8-9AM and 4-5PM each day. In particular, the two connecting gateways in the north and southeast show consistent traffic bottlenecks at peak hours.</p>
<div style="width: 1874px" class="wp-caption alignnone"><img loading="lazy" class="size-medium" style="border-radius: 1rem;" src="https://falkoschulz.github.io/VAST/2022/SAS-SCHULZ-MC2/img/city_traffic_daily.gif" width="1864" height="896" /><p class="wp-caption-text">Traffic congestions by time of day</p></div>
<p>Breaking down the traffic patterns based on the business district they are traveling to reveal more nuanced patterns. For example, almost 50% of all employees traveling to the Eastern Business District do so from the western parts of the city, which can take over an hour.</p>
<div id="attachment_13398" style="width: 712px" class="wp-caption aligncenter"><img aria-describedby="caption-attachment-13398" loading="lazy" class="wp-image-13398 size-large" style="border-radius: 1rem;" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c2-q2-3-1024x1024.png" alt="" width="702" height="702" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c2-q2-3-1024x1024.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c2-q2-3-300x300.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c2-q2-3-150x150.png 150w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c2-q2-3-768x769.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c2-q2-3.png 1362w" sizes="(max-width: 702px) 100vw, 702px" /><p id="caption-attachment-13398" class="wp-caption-text">Commutes to Eastern business district</p></div>
<p>We have selected participant <strong>40</strong> as our first case study. This 55-year-old lives in a household of three with children and works for two employers during the year. Initially working at job #228, this participant tried three other jobs for a few days until job #534 started in April of 2022. This employment change is also reflected in the change of daily travel patterns. Workdays changed from Monday through Friday at job #228 to Friday through Wednesday at job #534. The participant also received a pay increase ($2) for job #534.</p>
<div style="width: 1430px" class="wp-caption alignnone"><img loading="lazy" class="size-medium" style="border-radius: 1rem;" src="https://falkoschulz.github.io/VAST/2022/SAS-SCHULZ-MC2/img/participant_40_01MAR22_travel.gif" width="1420" height="1048" /><p class="wp-caption-text">Participant 40 activity</p></div>
<p>Participant 40's activity dashboard reveals their daily life. The participant was typically at home between 1-7pm or engaged in recreational activities between 6-11pm. Lunches were usually eaten at a restaurant at 2pm. During the year the overall activity changes as shown in the visualization animation.</p>
<div style="width: 810px" class="wp-caption aligncenter"><img loading="lazy" class="size-medium" style="border-radius: 1rem;" src="https://falkoschulz.github.io/VAST/2022/SAS-SCHULZ-MC2/img/activity_change_animation.gif" width="800" height="1008" /><p class="wp-caption-text">Activity changes over time</p></div>
<p>You can find the complete set of visualizations and findings in the submission form linked below.</p>
<h3>Results</h3>
<p>We provided our findings in video format highlighting some of the approaches taken when analyzing the VAST challenge data:</p>
<table class="center" style="width: 100%">
<tbody>
<tr>
<td>
<iframe loading="lazy" width="560" height="315" src="https://www.youtube.com/embed/lzx_9zhJVAU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>Challenge 1</td>
<td>
<iframe loading="lazy" width="560" height="315" src="https://www.youtube.com/embed/wEYdCOXkduc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>Challenge 2
<td>
</tr>
</tbody>
</table>
<p>&nbsp;</p>
<p>We also utilized the <a href="https://developer.sas.com/guides/visual-analytics-sdk.html">SAS Visual Analytics SDK</a> to provide interactive access to all visualization in the submission. You can view all visualizations in the submission by exploring the related forms for <a href="https://falkoschulz.github.io/VAST/2022/SAS-SCHULZ-MC1/">Challenge 1</a> or <a href="https://falkoschulz.github.io/VAST/2022/SAS-SCHULZ-MC2/">Challenge 2</a>.</p>
<p>The VAST Challenge provides a great opportunity to validate our software against real-world scenarios using complex data sets. Not only do we learn from these projects, but we also send feedback to our development teams to further improve product capabilities for customers.</p>
<h3>The team</h3>
<p>Spending time on VAST challenges is always fun but also requires a team with lot of commitment and technical knowledge in various areas of technology. This team was led by <a href="https://blogs.sas.com/content/author/falkoschulz/">Falko Schulz</a>, with <strong>Stu Sztukowski</strong>, <strong>Cheryl LeSaint</strong>, <strong>Steven Harenberg</strong>, and <strong>Don Chapman</strong> all making significant contributions. Also huge thanks to <strong>Chelsea Mayse</strong> for the willingness and thoroughness in producing beautiful video summaries. None of this would have been possible without each of you.</p>
<p>Thanks again to the entire SAS team!</p>
<h2>References</h2>
<ul>
<li><a href="https://vast-challenge.github.io/2022/">VAST Challenge 2022</a></li>
<li><a href="http://visualdata.wustl.edu/varepository/benchmarks.php">Visual Analytics Benchmark Repository</a></li>
<li>Challenge 1: YouTube <a href="https://youtu.be/lzx_9zhJVAU">Submission Video</a> - <a href="https://falkoschulz.github.io/VAST/2022/SAS-SCHULZ-MC1/">Submission Form</a></li>
<li>Challenge 2: YouTube <a href="https://youtu.be/wEYdCOXkduc">Submission Video</a><span data-contrast="auto"> - <a href="https://falkoschulz.github.io/VAST/2022/SAS-SCHULZ-MC2/">Submission Form</a></span></li>
<li><span data-contrast="auto">SAS Institute</span><span data-contrast="auto"> Inc</span><span data-contrast="auto">. <a href="https://www.sas.com/en_us/software/visual-analytics.html.">SAS Visual Analytics</a>. </span><span data-contrast="auto">(</span><span data-contrast="auto">Online</span><span data-contrast="auto">.)</span> <span data-contrast="auto">2022.</span></li>
</ul>
<p>The post <a rel="nofollow" href="https://blogs.sas.com/content/subconsciousmusings/2022/09/07/analyzing-demographics-and-patterns-of-life-using-sas-visual-analytics/">Analyzing demographics and patterns-of-life using SAS Visual Analytics</a> appeared first on <a rel="nofollow" href="https://blogs.sas.com/content/subconsciousmusings">The SAS Data Science Blog</a>.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://blogs.sas.com/content/subconsciousmusings/2022/09/07/analyzing-demographics-and-patterns-of-life-using-sas-visual-analytics/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			<enclosure url="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/fig-c1-q1-2-150x150.png" />
	</item>
		<item>
		<title>Identifying influential users in a social network to aid public health</title>
		<link>https://blogs.sas.com/content/subconsciousmusings/2022/08/29/identifying-influential-users-in-a-social-network-to-aid-public-health/</link>
					<comments>https://blogs.sas.com/content/subconsciousmusings/2022/08/29/identifying-influential-users-in-a-social-network-to-aid-public-health/#respond</comments>
		
		<dc:creator><![CDATA[Damian Herrick]]></dc:creator>
		<pubDate>Mon, 29 Aug 2022 18:05:27 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[Analytics R&D]]></category>
		<category><![CDATA[SAS Viya]]></category>
		<category><![CDATA[social network analysis]]></category>
		<guid isPermaLink="false">https://blogs.sas.com/content/subconsciousmusings/?p=13413</guid>

					<description><![CDATA[<p>SAS' Damian Herrick chronicles the refresh of a 2002 social-network analysis aimed at identifying influential peer educators among former drug users.</p>
<p>The post <a rel="nofollow" href="https://blogs.sas.com/content/subconsciousmusings/2022/08/29/identifying-influential-users-in-a-social-network-to-aid-public-health/">Identifying influential users in a social network to aid public health</a> appeared first on <a rel="nofollow" href="https://blogs.sas.com/content/subconsciousmusings">The SAS Data Science Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p>In this post, we examine a <a href="https://scholars.uky.edu/en/publications/social-networks-of-drug-users-in-high-risk-sites-finding-the-conn">network data set study</a> released in 2002 that documented efforts identifying influential users in a social network. The goal of the study was to find the most connected social network users who used drugs themselves and train them as peer educators to help reduce needle sharing and other high-risk behaviors among drug users. The study wanted to find the fewest number of peer educator candidates that could reach the greatest number of other users in the network. To achieve this, the 2002 study utilized degree centrality measures, supplemented with demographic, gender, and racial attributes. Our study demonstrates three methods to find peer educators using only anonymized network data.</p>
<h2>Hartford drug user social network</h2>
<p>In their 2002 paper “<a href="https://scholars.uky.edu/en/publications/social-networks-of-drug-users-in-high-risk-sites-finding-the-conn">Social Networks of Drug Users in High-Risk Sides: Finding the Connections</a>,” the authors presented a social network of drug users in Hartford, CT. The research team collected data through interviews with a community of drug users. Users provided information about the types of drugs used, needle-sharing activities, recency of usage, and demographic information such as age, gender, race, and housing status.</p>
<p>The research team then built a directed network (one in which node connections have a known direction) from the data, where links represented needle-sharing. See Figure 1.</p>
<div id="attachment_13416" style="width: 620px" class="wp-caption aligncenter"><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/august-herrick-social-netwrok-figure1.png"><img aria-describedby="caption-attachment-13416" loading="lazy" class="size-full wp-image-13416" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/august-herrick-social-netwrok-figure1.png" alt="" width="610" height="132" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/august-herrick-social-netwrok-figure1.png 610w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/august-herrick-social-netwrok-figure1-300x65.png 300w" sizes="(max-width: 610px) 100vw, 610px" /></a><p id="caption-attachment-13416" class="wp-caption-text">Figure 1: Sample relationship between users in the Hartford Drug User Social Network. In this example, user 1 shares a needle with user 2.</p></div>
<p>The team chose to focus on the largest connected component, which represented connections between 193 individuals. This component was later released as an anonymized data set representing links <em>from</em> and <em>to</em> terminals. Each user was assigned a unique number between 1 and 193. So, the primary information content of this network existed within the linkages. The network itself, while connected, was sparse. There were 273 links between nodes. The density was only 0.007. Network density, which ranges from 0 to 1, is defined as actual links divided by potential links. The average links per node was only 1.4. 80 of the 193 nodes were leaf nodes. This means they contained in-links but no out-links.</p>
<p>A network diagram representing only the raw network is shown in Figure 2. Initial examination of this network shows that the fully connected component is a set of two loosely connected components. In Figure 2, each subcomponent is enclosed by ellipses. The network can be further divided into smaller subgroups, which is described later in Method B.</p>
<div id="attachment_13419" style="width: 686px" class="wp-caption aligncenter"><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/august-herrick-social-netwrok-figure2.png"><img aria-describedby="caption-attachment-13419" loading="lazy" class="size-full wp-image-13419" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/august-herrick-social-netwrok-figure2.png" alt="" width="676" height="701" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/august-herrick-social-netwrok-figure2.png 676w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/august-herrick-social-netwrok-figure2-289x300.png 289w" sizes="(max-width: 676px) 100vw, 676px" /></a><p id="caption-attachment-13419" class="wp-caption-text">Figure 2: Initial plot of the full network. Note that the network is a single connected component but can also be viewed as containing two weakly connected subcomponents. Each subcomponent is approximately enclosed by purple and yellow ellipses.</p></div>
<h2>Three methods to identify influential users</h2>
<p>In our attempt to identify important users in the network, we chose three approaches to identify potential peer educator candidates. Once the candidates were identified, we tested their influence by calculating each candidate’s reach (ego) network over two hops. Simply put, we defined a peer educator’s potential influence by counting their friends, and friends of friends. This differs slightly from the efforts of the original 2002 study. Their team relied only on direct (one-hop) connections. Two-hop reach networks were chosen because we didn’t have access to key data available to the original authors. In each method, we attempted to minimize the number of peer educators necessary to reach half of the network (97 nodes).</p>
<h3>Method A: degree centrality influence</h3>
<p>The first method we used to identify peer educators was the simplest. Here, we attempted the methods of the original study albeit without some important information about relationships that were used in the paper. For our tests, we could only look at the degree centrality of individual nodes. Degree centrality is a simple-to-understand measure that defines a node’s importance. It is found by counting the total links (both <em>from</em> and <em>to</em>) for each node. A higher degree centrality indicates greater importance.</p>
<p>We can separate total degree centrality into two categories. The first is in-degree which counts links pointing to a node. The second is out-degree which counts links originating from a node. In our case, we defined a “good” candidate as someone who has a high total degree centrality and a high out-degree centrality. In our test, we identified 19 potential peer educators who have at least five total connections, as well as at least four connections that originate from the node.</p>
<p>After calculating the two-hop reach (ego) network, we found that these 19 peer educator candidates could potentially reach 132 additional users. This gave us a total network coverage of 151 nodes. Figure 3 shows the extent of influence from the selected peer educators.</p>
<div id="attachment_13422" style="width: 712px" class="wp-caption aligncenter"><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/august-herrick-social-netwrok-figure3.png"><img aria-describedby="caption-attachment-13422" loading="lazy" class="wp-image-13422 size-large" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/august-herrick-social-netwrok-figure3-1024x964.png" alt="Identifying Influential Users in a Social Network figure 3" width="702" height="661" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/august-herrick-social-netwrok-figure3-1024x964.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/august-herrick-social-netwrok-figure3-300x282.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/august-herrick-social-netwrok-figure3-768x723.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/august-herrick-social-netwrok-figure3.png 1417w" sizes="(max-width: 702px) 100vw, 702px" /></a><p id="caption-attachment-13422" class="wp-caption-text">Figure 3: Influence diagram for Method A - degree centrality influence. Peer educators are yellow nodes, and the nodes they influence are maroon and teal. Unreached nodes are colored blue.</p></div>
<h3>Method B: degree centrality within the community influence</h3>
<p>The second method refined the first approach by first estimating the communities present in the network. Then we calculated the most “important” nodes within each community from the degree centrality.</p>
<p>For community detection, we used the Louvain method. Then we tested various total community assignments. See Table 1. After reviewing the results, we chose an arrangement that returns five communities, as shown in Figure 4.</p>
<table width="100%">
<tbody>
<tr>
<td style="text-align: center" width="33%"><strong>Resolution</strong></td>
<td style="text-align: center" width="33%"><strong>Communities</strong></td>
<td style="text-align: center" width="33%"><strong>Modularity</strong></td>
</tr>
<tr>
<td width="33%">1.5</td>
<td width="33%">16</td>
<td width="33%">0.722</td>
</tr>
<tr>
<td width="33%">1.0</td>
<td width="33%">12</td>
<td width="33%">0.727</td>
</tr>
<tr>
<td width="33%">0.5</td>
<td width="33%">7</td>
<td width="33%">0.675</td>
</tr>
<tr>
<td width="33%">0.25</td>
<td width="33%">5</td>
<td width="33%">0.630</td>
</tr>
</tbody>
</table>
<div id="attachment_13425" style="width: 712px" class="wp-caption aligncenter"><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/august-herrick-social-netwrok-figure4.png"><img aria-describedby="caption-attachment-13425" loading="lazy" class="wp-image-13425 size-large" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/august-herrick-social-netwrok-figure4-1024x979.png" alt="Identifying Influential Users in a Social Network figure 4" width="702" height="671" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/august-herrick-social-netwrok-figure4-1024x979.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/august-herrick-social-netwrok-figure4-300x287.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/august-herrick-social-netwrok-figure4-768x734.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/august-herrick-social-netwrok-figure4.png 1432w" sizes="(max-width: 702px) 100vw, 702px" /></a><p id="caption-attachment-13425" class="wp-caption-text">Figure 4: Community assignment from the Louvain method. In this example, we used a resolution of 0.25, which resulted in a modularity of 0.63.</p></div>
<p>Once community detection was completed, we proceeded with the selection of peer educator candidates. We again used degree centrality as the primary tool to determine influence. For each community, we determined the distribution of out-degree centrality among the community members. In each community, we standardized by z-score and calculated the kernel density estimate as well as the cumulative distribution (CDF) function. From the CDF we derived a score for each node between 0 and 1. Any node with a score of at least 0.85 was selected as a candidate for peer educator. As in Method A, we then calculated the two-hop reach (ego) network for these candidates and determined their total potential influence on the network.</p>
<p>Method B identified 13 peer educator candidates who could potentially reach 133 additional users for a total network coverage of 146 nodes. Figure 5 shows the extent of influence from the selected peer educators.</p>
<div id="attachment_13428" style="width: 712px" class="wp-caption aligncenter"><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/august-herrick-social-netwrok-figure5.png"><img aria-describedby="caption-attachment-13428" loading="lazy" class="wp-image-13428 size-large" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/august-herrick-social-netwrok-figure5-1024x993.png" alt="Identifying Influential Users in a Social Network figure 5" width="702" height="681" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/august-herrick-social-netwrok-figure5-1024x993.png 1024w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/august-herrick-social-netwrok-figure5-300x291.png 300w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/august-herrick-social-netwrok-figure5-768x745.png 768w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/august-herrick-social-netwrok-figure5.png 1440w" sizes="(max-width: 702px) 100vw, 702px" /></a><p id="caption-attachment-13428" class="wp-caption-text">Figure 5: Influence diagram for Method B - degree centrality within community influence. Peer educators are yellow nodes, and the nodes they influence are maroon and teal. Unreached nodes are colored blue.</p></div>
<h3>Method C: influence potential based on two-hop reach networks</h3>
<p>In Method C, we used a different approach to identify peer educator candidates. Instead of degree centrality, we first calculated all two-hop reach networks over all nodes.  We then submitted these networks into an optimization solver to determine the fewest peer educators that could influence at least half of the nodes in the network. We used the SAS documentation example <a href="https://go.documentation.sas.com/doc/en/pgmsascdc/v_028/procgralg/procgralg_optgraph_examples14.htm">Reach Networks for Computation of Market Coverage of a Terrorist Network</a> as inspiration.</p>
<p>Relying on reach networks only, Method C identified 25 peer educator candidates who could potentially reach 114 additional users. This gave us a total network coverage of 139 nodes. Figure 6 shows the extent of influence from the selected peer educators.</p>
<div id="attachment_13431" style="width: 608px" class="wp-caption aligncenter"><a href="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/august-herrick-social-netwrok-figure6.png"><img aria-describedby="caption-attachment-13431" loading="lazy" class="size-full wp-image-13431" src="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/august-herrick-social-netwrok-figure6.png" alt="" width="598" height="565" srcset="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/august-herrick-social-netwrok-figure6.png 598w, https://blogs.sas.com/content/subconsciousmusings/files/2022/08/august-herrick-social-netwrok-figure6-300x283.png 300w" sizes="(max-width: 598px) 100vw, 598px" /></a><p id="caption-attachment-13431" class="wp-caption-text">Figure 6: Influence diagram for Method C - influence potential from two-hop reach networks. Peer educators are yellow nodes, and the nodes they influence are maroon and teal. Unreached nodes are colored blue.</p></div>
<h2>Results</h2>
<p>Table 2 shows the peer educator candidates and potentially reached users for each of the three methods described above.</p>
<table width="100%">
<tbody>
<tr>
<td style="text-align: center" width="40%"><strong>Method</strong></td>
<td style="text-align: center" width="15%"><strong>Peer Educators</strong></td>
<td style="text-align: center" width="15%"><strong>Users Reached</strong></td>
<td style="text-align: center" width="15%"><strong>Total Reached</strong></td>
<td style="text-align: center" width="15%"><strong>Users Not Reached</strong></td>
</tr>
<tr>
<td width="40%">Degree Centrality Influence</td>
<td width="15%">19</td>
<td width="15%">132</td>
<td width="15%">151</td>
<td width="15%">42</td>
</tr>
<tr>
<td width="40%">Degree Centrality within Community Influence</td>
<td width="10%">13</td>
<td width="10%">133</td>
<td width="10%">146</td>
<td width="10%">47</td>
</tr>
<tr>
<td width="40%">Two-Hop Reach Network Influence Potential</td>
<td width="15%">25</td>
<td width="15%">114</td>
<td width="15%">139</td>
<td width="15%">54</td>
</tr>
</tbody>
</table>
<p>Recall that the original study attempted to minimize the total number of peer educators while reaching the most users in the network. They identified 13 candidates who could reach 97 other users in the network. This would be equivalent to reaching 50% of the network. We were unable to match their methods directly. However, Method A (degree centrality influence) represents the closest approximation to the original study. In our case, Method A identifies 19 candidates who could reach 132 others in the network. Method B has a slightly smaller total reach (146 users as opposed to 151 for Method A). But it achieves this with only 13 peer educator candidates. In Method C, when only two-hop reach networks are considered, then optimized for as close to 100% network coverage, we find that 25 candidates can only reach 114 individuals.</p>
<p>Of the three methods presented here, only one node (115) was identified as a peer educator candidate by all three methods. Methods A and B identify seven common peer educator candidates (nodes 115, 27, 34, 43, 61, 69, and 77). Methods A and C share only three nodes (115, 28, and 53). Unfortunately, peer educator candidates identified by the original study authors are not available, so we cannot directly compare our results with theirs.</p>
<h2>Summary</h2>
<p>This post attempts to replicate results from a <a href="https://scholars.uky.edu/en/publications/social-networks-of-drug-users-in-high-risk-sites-finding-the-conn">2002 paper</a> that identifies connections between drug users in Hartford, CT. In the paper, peer educators are identified by a combination of network analytics methods and prior known information about demographics, gender, race, and behaviors. In our study, we used only network analytics methods without the inclusion of information that could introduce bias into the results.</p>
<p>Even though we were unable to directly replicate the results from the study, the three methodologies shown provide similar network coverage to the original results. This indicates that it is possible to use only mathematical techniques to identify peer educator candidates in a social network. Method B generated the most favorable results of the fewest peer educators that reached the most members of the network.</p>
<p>The analyses for this post were developed using <a href="https://support.sas.com/en/software/sas-viya.html">SAS Viya</a>, Jupyter, and Python. If you are interested in learning more about the technical details of this post, and seeing the code used, go to the <a href="https://github.com/sassoftware/sas-network-analysis">SAS Software Network Analysis Github repository</a>. This specific example is located in <a href="https://github.com/sassoftware/sas-network-analysis/blob/main/applications/hartford/notebooks/Hartford%20Drug%20User%20Network%20Analysis.ipynb">applications/Hartford</a>.</p>
<p>The post <a rel="nofollow" href="https://blogs.sas.com/content/subconsciousmusings/2022/08/29/identifying-influential-users-in-a-social-network-to-aid-public-health/">Identifying influential users in a social network to aid public health</a> appeared first on <a rel="nofollow" href="https://blogs.sas.com/content/subconsciousmusings">The SAS Data Science Blog</a>.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://blogs.sas.com/content/subconsciousmusings/2022/08/29/identifying-influential-users-in-a-social-network-to-aid-public-health/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			<enclosure url="https://blogs.sas.com/content/subconsciousmusings/files/2022/08/august-herrick-social-media-150x150.png" />
	</item>
	</channel>
</rss>
