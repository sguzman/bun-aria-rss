<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>stat.ME updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Statistics -- Methodology (stat.ME) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2022-11-03T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Methodology</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01429" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01492" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01521" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01547" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01575" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01591" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01627" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01645" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01686" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01727" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01746" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01799" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01938" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01939" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.02039" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.02045" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.02046" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1701.06686" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2010.13599" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2108.12515" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.04065" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.04539" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.07330" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.01757" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.13133" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.15264" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01170" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01227" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2211.01429">
<title>Fast Bayesian estimation of brain activation with cortical surface fMRI data using EM. (arXiv:2211.01429v1 [stat.ME])</title>
<link>http://arxiv.org/abs/2211.01429</link>
<description rdf:parseType="Literal">&lt;p&gt;Task functional magnetic resonance imaging (fMRI) is a type of neuroimaging
data used to identify areas of the brain that activate during specific tasks or
stimuli. These data are conventionally modeled using a massive univariate
approach across all data locations, which ignores spatial dependence at the
cost of model power. We previously developed and validated a spatial Bayesian
model leveraging dependencies along the cortical surface of the brain in order
to improve accuracy and power. This model utilizes stochastic partial
differential equation spatial priors with sparse precision matrices to allow
for appropriate modeling of spatially-dependent activations seen in the
neuroimaging literature, resulting in substantial increases in model power. Our
original implementation relies on the computational efficiencies of the
integrated nested Laplace approximation (INLA) to overcome the computational
challenges of analyzing high-dimensional fMRI data while avoiding issues
associated with variational Bayes implementations. However, this requires
significant memory resources, extra software, and software licenses to run. In
this article, we develop an exact Bayesian analysis method for the general
linear model, employing an efficient expectation-maximization algorithm to find
maximum a posteriori estimates of task-based regressors on cortical surface
fMRI data. Through an extensive simulation study of cortical surface-based fMRI
data, we compare our proposed method to the existing INLA implementation, as
well as a conventional massive univariate approach employing ad-hoc spatial
smoothing. We also apply the method to task fMRI data from the Human Connectome
Project and show that our proposed implementation produces similar results to
the validated INLA implementation. Both the INLA and EM-based implementations
are available through our open-source BayesfMRI R package.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Spencer_D/0/1/0/all/0/1&quot;&gt;Daniel A. Spencer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bolin_D/0/1/0/all/0/1&quot;&gt;David Bolin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mejia_A/0/1/0/all/0/1&quot;&gt;Amanda F. Mejia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01492">
<title>Fast, effective, and coherent time series modeling using the sparsity-ranked lasso. (arXiv:2211.01492v1 [stat.ME])</title>
<link>http://arxiv.org/abs/2211.01492</link>
<description rdf:parseType="Literal">&lt;p&gt;The sparsity-ranked lasso (SRL) has been developed for model selection and
estimation in the presence of interactions and polynomials. The main tenet of
the SRL is that an algorithm should be more skeptical of higher-order
polynomials and interactions *a priori* compared to main effects, and hence the
inclusion of these more complex terms should require a higher level of
evidence. In time series, the same idea of ranked prior skepticism can be
applied to the possibly seasonal autoregressive (AR) structure of the series
during the model fitting process, becoming especially useful in settings with
uncertain or multiple modes of seasonality. The SRL can naturally incorporate
exogenous variables, with streamlined options for inference and/or feature
selection. The fitting process is quick even for large series with a
high-dimensional feature set. In this work, we discuss both the formulation of
this procedure and the software we have developed for its implementation via
the **srlTS** R package. We explore the performance of our SRL-based approach
in a novel application involving the autoregressive modeling of hourly
emergency room arrivals at the University of Iowa Hospitals and Clinics. We
find that the SRL is considerably faster than its competitors, while producing
more accurate predictions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Peterson_R/0/1/0/all/0/1&quot;&gt;Ryan Peterson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cavanaugh_J/0/1/0/all/0/1&quot;&gt;Joseph Cavanaugh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01521">
<title>Inferring independent sets of Gaussian variables after thresholding correlations. (arXiv:2211.01521v1 [stat.ME])</title>
<link>http://arxiv.org/abs/2211.01521</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider testing whether a set of Gaussian variables, selected from the
data, is independent of the remaining variables. We assume that this set is
selected via a very simple approach that is commonly used across scientific
disciplines: we select a set of variables for which the correlation with all
variables outside the set falls below some threshold. Unlike other settings in
selective inference, failure to account for the selection step leads, in this
setting, to excessively conservative (as opposed to anti-conservative) results.
Our proposed test properly accounts for the fact that the set of variables is
selected from the data, and thus is not overly conservative. To develop our
test, we condition on the event that the selection resulted in the set of
variables in question. To achieve computational tractability, we develop a new
characterization of the conditioning event in terms of the canonical
correlation between the groups of random variables. In simulation studies and
in the analysis of gene co-expression networks, we show that our approach has
much higher power than a ``naive&apos;&apos; approach that ignores the effect of
selection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Saha_A/0/1/0/all/0/1&quot;&gt;Arkajyoti Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Witten_D/0/1/0/all/0/1&quot;&gt;Daniela Witten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bien_J/0/1/0/all/0/1&quot;&gt;Jacob Bien&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01547">
<title>A Systematic Paradigm for Detecting, Surfacing, and Characterizing Heterogeneous Treatment Effects (HTE). (arXiv:2211.01547v1 [stat.ME])</title>
<link>http://arxiv.org/abs/2211.01547</link>
<description rdf:parseType="Literal">&lt;p&gt;To effectively optimize and personalize treatments, it is necessary to
investigate the heterogeneity of treatment effects. With the wide range of
users being treated over many online controlled experiments, the typical
approach of manually investigating each dimension of heterogeneity becomes
overly cumbersome and prone to subjective human biases. We need an efficient
way to search through thousands of experiments with hundreds of target
covariates and hundreds of breakdown dimensions. In this paper, we propose a
systematic paradigm for detecting, surfacing and characterizing heterogeneous
treatment effects. First, we detect if treatment effect variation is present in
an experiment, prior to specifying any breakdowns. Second, we surface the most
relevant dimensions for heterogeneity. Finally, we characterize the
heterogeneity beyond just the conditional average treatment effects (CATE) by
studying the conditional distributions of the estimated individual treatment
effects. We show the effectiveness of our methods using simulated data and
empirical studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cai_J/0/1/0/all/0/1&quot;&gt;John Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weinan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01575">
<title>Are Synthetic Control Weights Balancing Score?. (arXiv:2211.01575v1 [stat.ME])</title>
<link>http://arxiv.org/abs/2211.01575</link>
<description rdf:parseType="Literal">&lt;p&gt;In this short note, I outline conditions under which conditioning on
Synthetic Control (SC) weights emulates a randomized control trial where the
treatment status is independent of potential outcomes. Specifically, I
demonstrate that if there exist SC weights such that (i) the treatment effects
are exactly identified and (ii) these weights are uniformly and cumulatively
bounded, then SC weights are balancing scores.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Parikh_H/0/1/0/all/0/1&quot;&gt;Harsh Parikh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01591">
<title>A Bayesian Semiparametric Method For Estimating Causal Quantile Effects. (arXiv:2211.01591v1 [stat.ME])</title>
<link>http://arxiv.org/abs/2211.01591</link>
<description rdf:parseType="Literal">&lt;p&gt;Standard causal inference characterizes treatment effect through averages,
but the counterfactual distributions could be different in not only the central
tendency but also spread and shape. To provide a comprehensive evaluation of
treatment effects, we focus on estimating quantile treatment effects (QTEs).
Existing methods that invert a nonsmooth estimator of the cumulative
distribution functions forbid inference on probability density functions
(PDFs), but PDFs can reveal more nuanced characteristics of the counterfactual
distributions. We adopt a semiparametric conditional distribution regression
model that allows inference on any functionals of counterfactual distributions,
including PDFs and multiple QTEs. To account for the observational nature of
the data and ensure an efficient model, we adjust for a double balancing score
that augments the propensity score with individual covariates. We provide a
Bayesian estimation framework that appropriately propagates modeling
uncertainty. We show via simulations that the use of double balancing score for
confounding adjustment improves performance over adjusting for any single score
alone, and the proposed semiparametric model estimates QTEs more accurately
than other semiparametric methods. We apply the proposed method to the North
Carolina birth weight dataset to analyze the effect of maternal smoking on
infant&apos;s birth weight.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Steven G. Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Reich_B/0/1/0/all/0/1&quot;&gt;Brian J. Reich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01627">
<title>Inverting Regional Sensitivity Analysis to reveal sensitive model behaviors. (arXiv:2211.01627v1 [math.ST])</title>
<link>http://arxiv.org/abs/2211.01627</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the question of sensitivity analysis for model outputs of any
dimension using Regional Sensitivity Analysis (RSA). Classical RSA computes
sensitivity indices related to the impact of model inputs variations on the
occurrence of a target region of the model output space. In this work, we
invert this perspective by proposing to find, for a given target model input,
the region whose occurrence is best explained by the variations of this input.
When it exists, this region can be seen as a model behavior which is
particularly sensitive to the variations of the model input under study. We
name this method iRSA (for inverse RSA). iRSA is formalized as an optimization
problem using region-based sensitivity indices and solved using dedicated
numerical algorithms. Using analytical and numerical examples, including an
environmental model producing time series, we show that iRSA can provide a new
graphical and interpretable characterization of sensitivity for model outputs
of various dimensions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Roux_S/0/1/0/all/0/1&quot;&gt;S&amp;#xe9;bastien Roux&lt;/a&gt; (MISTEA), &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Loisel_P/0/1/0/all/0/1&quot;&gt;Patrice Loisel&lt;/a&gt; (MISTEA), &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Buis_S/0/1/0/all/0/1&quot;&gt;Samuel Buis&lt;/a&gt; (EMMAH)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01645">
<title>Towards federated multivariate statistical process control (FedMSPC). (arXiv:2211.01645v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2211.01645</link>
<description rdf:parseType="Literal">&lt;p&gt;The ongoing transition from a linear (produce-use-dispose) to a circular
economy poses significant challenges to current state-of-the-art information
and communication technologies. In particular, the derivation of integrated,
high-level views on material, process, and product streams from (real-time)
data produced along value chains is challenging for several reasons. Most
importantly, sufficiently rich data is often available yet not shared across
company borders because of privacy concerns which make it impossible to build
integrated process models that capture the interrelations between input
materials, process parameters, and key performance indicators along value
chains. In the current contribution, we propose a privacy-preserving, federated
multivariate statistical process control (FedMSPC) framework based on Federated
Principal Component Analysis (PCA) and Secure Multiparty Computation to foster
the incentive for closer collaboration of stakeholders along value chains. We
tested our approach on two industrial benchmark data sets - SECOM and ST-AWFD.
Our empirical results demonstrate the superior fault detection capability of
the proposed approach compared to standard, single-party (multiway) PCA.
Furthermore, we showcase the possibility of our framework to provide
privacy-preserving fault diagnosis to each data holder in the value chain to
underpin the benefits of secure data sharing and federated process modeling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Duy_D/0/1/0/all/0/1&quot;&gt;Du Nguyen Duy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gabauer_D/0/1/0/all/0/1&quot;&gt;David Gabauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nikzad_Langerodi_R/0/1/0/all/0/1&quot;&gt;Ramin Nikzad-Langerodi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01686">
<title>Principal Balances of Compositional Data for Regression and Classification using Partial Least Squares. (arXiv:2211.01686v1 [stat.ME])</title>
<link>http://arxiv.org/abs/2211.01686</link>
<description rdf:parseType="Literal">&lt;p&gt;High-dimensional compositional data are commonplace in the modern omics
sciences amongst others. Analysis of compositional data requires a proper
choice of orthonormal coordinate representation as their relative nature is not
compatible with the direct use of standard statistical methods. Principal
balances, a specific class of log-ratio coordinates, are well suited to this
context since they are constructed in such a way that the first few coordinates
capture most of the variability in the original data. Focusing on regression
and classification problems in high dimensions, we propose a novel Partial
Least Squares (PLS) based procedure to construct principal balances that
maximize explained variability of the response variable and notably facilitates
interpretability when compared to the ordinary PLS formulation. The proposed
PLS principal balance approach can be understood as a generalized version of
common logcontrast models, since multiple orthonormal (instead of one)
logcontrasts are estimated simultaneously. We demonstrate the performance of
the method using both simulated and real data sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nesrstova_V/0/1/0/all/0/1&quot;&gt;V. Nesrstov&amp;#xe1;&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wilms_I/0/1/0/all/0/1&quot;&gt;I. Wilms&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Palarea_Albaladejo_J/0/1/0/all/0/1&quot;&gt;J. Palarea-Albaladejo&lt;/a&gt; (3), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Filzmoser_P/0/1/0/all/0/1&quot;&gt;P. Filzmoser&lt;/a&gt; (4), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Martin_Fernandez_J/0/1/0/all/0/1&quot;&gt;J. A. Mart&amp;#xed;n-Fern&amp;#xe1;ndez&lt;/a&gt; (3), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Friedecky_D/0/1/0/all/0/1&quot;&gt;D. Friedeck&amp;#xfd;&lt;/a&gt; (5), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hron_K/0/1/0/all/0/1&quot;&gt;K. Hron&lt;/a&gt; (1) ((1) Department of Mathematical Analysis and Applications of Mathematics, Palack&amp;#xfd; University Olomouc, Faculty of Science, the Czech Republic (2) Department of Quantitative Economics, Maastricht University, Maastricht, the Netherlands (3) Department of Computer Science, Applied Mathematics and Statistics, University of Girona, Girona, Spain (4) Department of Statistics and Probability Theory, Vienna University of Technology, Vienna, Austria (5) Laboratory for Inherited Metabolic Disorders, Department of Clinical Biochemistry, University Hospital Olomouc and Faculty of Medicine and Dentistry, Palack&amp;#xfd; University Olomouc, Olomouc, the Czech Republic)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01727">
<title>Bayesian methods of vector autoregressions with tensor decompositions. (arXiv:2211.01727v1 [stat.ME])</title>
<link>http://arxiv.org/abs/2211.01727</link>
<description rdf:parseType="Literal">&lt;p&gt;Vector autoregressions (VARs) are popular in analyzing economic time series.
However, VARs can be over-parameterized if the numbers of variables and lags
are moderately large. Tensor VAR, a recent solution to overparameterization,
treats the coefficient matrix as a third-order tensor and estimates the
corresponding tensor decomposition to achieve parsimony. In this paper, the
inference of Tensor VARs is inspired by the literature on factor models.
Firstly, we determine the rank by imposing the Multiplicative Gamma Prior to
margins, i.e. elements in the decomposition, and accelerate the computation
with an adaptive inferential scheme. Secondly, to obtain interpretable margins,
we propose an interweaving algorithm to improve the mixing of margins and
introduce a post-processing procedure to solve column permutations and
sign-switching issues. In the application of the US macroeconomic data, our
models outperform standard VARs in point and density forecasting and yield
interpretable results consistent with the US economic history.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yiyong Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Griffin_J/0/1/0/all/0/1&quot;&gt;Jim E. Griffin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01746">
<title>Log-density gradient covariance and automatic metric tensors for Riemann manifold Monte Carlo methods. (arXiv:2211.01746v1 [stat.CO])</title>
<link>http://arxiv.org/abs/2211.01746</link>
<description rdf:parseType="Literal">&lt;p&gt;A metric tensor for Riemann manifold Monte Carlo particularly suited for
non-linear Bayesian hierarchical models is proposed. The metric tensor is built
from here proposed symmetric positive semidefinite log-density gradient
covariance (LGC) matrices. The LGCs measure the joint information content and
dependence structure of both a random variable and the parameters of said
variable. The proposed methodology is highly automatic and allows for
exploitation of any sparsity associated with the model in question. When
implemented in conjunction with a Riemann manifold variant of the recently
proposed numerical generalized randomized Hamiltonian Monte Carlo processes,
the proposed methodology is highly competitive, in particular for the more
challenging target distributions associated with Bayesian hierarchical models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kleppe_T/0/1/0/all/0/1&quot;&gt;Tore Selland Kleppe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01799">
<title>Statistical Inference for Scale Mixture Models via Mellin Transform Approach. (arXiv:2211.01799v1 [stat.ME])</title>
<link>http://arxiv.org/abs/2211.01799</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper deals with statistical inference for the scale mixture models. We
study an estimation approach based on the Mellin - Stieltjes transform that can
be applied to both discrete and absolute continuous mixing distributions. The
accuracy of the corresponding estimate is analysed in terms of its expected
pointwise error. As an important technical result, we prove the analogue of the
Berry - Esseen inequality for the Mellin transforms. The proposed statistical
approach is illustrated by numerical examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Belomestny_D/0/1/0/all/0/1&quot;&gt;Denis Belomestny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Morozova_E/0/1/0/all/0/1&quot;&gt;Ekaterina Morozova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Panov_V/0/1/0/all/0/1&quot;&gt;Vladimir Panov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01938">
<title>betaclust: a family of mixture models for beta valued DNA methylation data. (arXiv:2211.01938v1 [stat.ME])</title>
<link>http://arxiv.org/abs/2211.01938</link>
<description rdf:parseType="Literal">&lt;p&gt;The DNA methylation process has been extensively studied for its role in
cancer. Promoter cytosine-guanine dinucleotide (CpG) island hypermethylation
has been shown to silence tumour suppressor genes. Identifying the
differentially methylated CpG (DMC) sites between benign and tumour samples can
help understand the disease. The EPIC microarray quantifies the methylation
level at a CpG site as a beta value which lies within [0,1). There is a lack of
suitable methods for modelling the beta values in their innate form. The DMCs
are identified via multiple t-tests but this can be computationally expensive.
Also, arbitrary thresholds are often selected and used to identify the
methylation state of a CpG site. We propose a family of novel beta mixture
models (BMMs) which use a model-based clustering approach to cluster the CpG
sites in their innate beta form to (i) objectively identify methylation state
thresholds and (ii) identify the DMCs between different samples. The family of
BMMs employs different parameter constraints that are applicable to different
study settings. Parameter estimation proceeds via an EM algorithm, with a novel
approximation during the M-step providing tractability and computational
feasibility. Performance of the BMMs is assessed through a thorough simulation
study, and the BMMs are used to analyse a prostate cancer dataset and an
esophageal squamous cell carcinoma dataset. The BMM approach objectively
identifies methylation state thresholds and identifies more DMCs between the
benign and tumour samples in both cancer datasets than conventional methods, in
a computationally efficient manner. The empirical cumulative distribution
function of the DMCs related to genes implicated in carcinogenesis indicates
hypermethylation of CpG sites in the tumour samples in both cancer settings. An
R package betaclust is provided to facilitate the use of the developed BMMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Majumdar_K/0/1/0/all/0/1&quot;&gt;Koyel Majumdar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Silva_R/0/1/0/all/0/1&quot;&gt;Romina Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Perry_A/0/1/0/all/0/1&quot;&gt;Antoinette Sabrina Perry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Watson_R/0/1/0/all/0/1&quot;&gt;Ronald William Watson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Murphy_T/0/1/0/all/0/1&quot;&gt;Thomas Brendan Murphy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gormley_I/0/1/0/all/0/1&quot;&gt;Isobel Claire Gormley&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01939">
<title>Empirical Analysis of Model Selection for Heterogenous Causal Effect Estimation. (arXiv:2211.01939v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2211.01939</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of model selection in causal inference, specifically for
the case of conditional average treatment effect (CATE) estimation under binary
treatments. Unlike model selection in machine learning, we cannot use the
technique of cross-validation here as we do not observe the counterfactual
potential outcome for any data point. Hence, we need to design model selection
techniques that do not explicitly rely on counterfactual data. As an
alternative to cross-validation, there have been a variety of proxy metrics
proposed in the literature, that depend on auxiliary nuisance models also
estimated from the data (propensity score model, outcome regression model).
However, the effectiveness of these metrics has only been studied on synthetic
datasets as we can observe the counterfactual data for them. We conduct an
extensive empirical analysis to judge the performance of these metrics, where
we utilize the latest advances in generative modeling to incorporate multiple
realistic datasets. We evaluate 9 metrics on 144 datasets for selecting between
415 estimators per dataset, including datasets that closely mimic real-world
datasets. Further, we use the latest techniques from AutoML to ensure
consistent hyperparameter selection for nuisance models for a fair comparison
across metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahajan_D/0/1/0/all/0/1&quot;&gt;Divyat Mahajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitliagkas_I/0/1/0/all/0/1&quot;&gt;Ioannis Mitliagkas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neal_B/0/1/0/all/0/1&quot;&gt;Brady Neal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Syrgkanis_V/0/1/0/all/0/1&quot;&gt;Vasilis Syrgkanis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.02039">
<title>The Projected Covariance Measure for assumption-lean variable significance testing. (arXiv:2211.02039v1 [math.ST])</title>
<link>http://arxiv.org/abs/2211.02039</link>
<description rdf:parseType="Literal">&lt;p&gt;Testing the significance of a variable or group of variables $X$ for
predicting a response $Y$, given additional covariates $Z$, is a ubiquitous
task in statistics. A simple but common approach is to specify a linear model,
and then test whether the regression coefficient for $X$ is non-zero. However,
when the model is misspecified, the test may have poor power, for example when
$X$ is involved in complex interactions, or lead to many false rejections. In
this work we study the problem of testing the model-free null of conditional
mean independence, i.e. that the conditional mean of $Y$ given $X$ and $Z$ does
not depend on $X$. We propose a simple and general framework that can leverage
flexible nonparametric or machine learning methods, such as additive models or
random forests, to yield both robust error control and high power. The
procedure involves using these methods to perform regressions, first to
estimate a form of projection of $Y$ on $X$ and $Z$ using one half of the data,
and then to estimate the expected conditional covariance between this
projection and $Y$ on the remaining half of the data. While the approach is
general, we show that a version of our procedure using spline regression
achieves what we show is the minimax optimal rate in this nonparametric testing
problem. Numerical experiments demonstrate the effectiveness of our approach
both in terms of maintaining Type I error control, and power, compared to
several existing approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lundborg_A/0/1/0/all/0/1&quot;&gt;Anton Rask Lundborg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kim_I/0/1/0/all/0/1&quot;&gt;Ilmun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Shah_R/0/1/0/all/0/1&quot;&gt;Rajen D. Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Samworth_R/0/1/0/all/0/1&quot;&gt;Richard J. Samworth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.02045">
<title>Fast and robust Bayesian Inference using Gaussian Processes with GPry. (arXiv:2211.02045v1 [astro-ph.CO])</title>
<link>http://arxiv.org/abs/2211.02045</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the GPry algorithm for fast Bayesian inference of general
(non-Gaussian) posteriors with a moderate number of parameters. GPry does not
need any pre-training, special hardware such as GPUs, and is intended as a
drop-in replacement for traditional Monte Carlo methods for Bayesian inference.
Our algorithm is based on generating a Gaussian Process surrogate model of the
log-posterior, aided by a Support Vector Machine classifier that excludes
extreme or non-finite values. An active learning scheme allows us to reduce the
number of required posterior evaluations by two orders of magnitude compared to
traditional Monte Carlo inference. Our algorithm allows for parallel
evaluations of the posterior at optimal locations, further reducing wall-clock
times. We significantly improve performance using properties of the posterior
in our active learning scheme and for the definition of the GP prior. In
particular we account for the expected dynamical range of the posterior in
different dimensionalities. We test our model against a number of synthetic and
cosmological examples. GPry outperforms traditional Monte Carlo methods when
the evaluation time of the likelihood (or the calculation of theoretical
observables) is of the order of seconds; for evaluation times of over a minute
it can perform inference in days that would take months using traditional
methods. GPry is distributed as an open source Python package (pip install
gpry) and can also be found at https://github.com/jonaselgammal/GPry.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Gammal_J/0/1/0/all/0/1&quot;&gt;Jonas El Gammal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Schoneberg_N/0/1/0/all/0/1&quot;&gt;Nils Sch&amp;#xf6;neberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Torrado_J/0/1/0/all/0/1&quot;&gt;Jes&amp;#xfa;s Torrado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Fidler_C/0/1/0/all/0/1&quot;&gt;Christian Fidler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.02046">
<title>Seamless Phase 2-3 Design: A Useful Strategy to Reduce the Sample Size for Dose Optimization. (arXiv:2211.02046v1 [stat.ME])</title>
<link>http://arxiv.org/abs/2211.02046</link>
<description rdf:parseType="Literal">&lt;p&gt;The traditional more-is-better dose selection paradigm, developed based on
cytotoxic chemotherapeutics, is often problematic When applied to the
development of novel molecularly targeted agents (e.g., kinase inhibitors,
monoclonal antibodies, and antibody-drug conjugates). The US Food and Drug
Administration (FDA) initiated Project Optimus to reform the dose optimization
and dose selection paradigm in oncology drug development and call for more
attention to benefit-risk consideration.
&lt;/p&gt;
&lt;p&gt;We systematically investigated the operating characteristics of the seamless
phase 2-3 design as a strategy for dose optimization, where in stage 1
(corresponding to phase 2) patients are randomized to multiple doses, with or
without a control; and in stage 2 (corresponding to phase 3) the efficacy of
the selected optimal dose is evaluated with a randomized concurrent control or
historical control. Depending on whether the concurrent control is included and
the type of endpoints used in stages 1 and 2, we describe four types of
seamless phase 2-3 dose-optimization designs, which are suitable for different
clinical settings. The statistical and design considerations that pertain to
dose optimization are discussed. Simulation shows that dose optimization phase
2-3 designs are able to control the familywise type I error rates and yield
appropriate statistical power with substantially smaller sample size than the
conventional approach. The sample size savings range from 16.6% to 27.3%,
depending on the design and scenario, with a mean savings of 22.1%. Due to the
interim dose selection, the phase 2-3 dose-optimization design is logistically
and operationally more challenging, and should be carefully planned and
implemented to ensure trial integrity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jiang_L/0/1/0/all/0/1&quot;&gt;Liyun Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Ying Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1701.06686">
<title>Nested Markov Properties for Acyclic Directed Mixed Graphs. (arXiv:1701.06686v4 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1701.06686</link>
<description rdf:parseType="Literal">&lt;p&gt;Conditional independence models associated with directed acyclic graphs
(DAGs) may be characterized in at least three different ways: via a
factorization, the global Markov property (given by the d-separation
criterion), and the local Markov property. Marginals of DAG models also imply
equality constraints that are not conditional independences; the well-known
`Verma constraint&apos; is an example. Constraints of this type are used for testing
edges, and in a computationally efficient marginalization scheme via variable
elimination.
&lt;/p&gt;
&lt;p&gt;We show that equality constraints like the `Verma constraint&apos; can be viewed
as conditional independences in kernel objects obtained from joint
distributions via a fixing operation that generalizes conditioning and
marginalization. We use these constraints to define, via ordered local and
global Markov properties, and a factorization, a graphical model associated
with acyclic directed mixed graphs (ADMGs). We prove that marginal
distributions of DAG models lie in this model, and that a set of these
constraints given by Tian provides an alternative definition of the model.
Finally, we show that the fixing operation used to define the model leads to a
particularly simple characterization of identifiable causal effects in hidden
variable causal DAG models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Richardson_T/0/1/0/all/0/1&quot;&gt;Thomas S. Richardson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Evans_R/0/1/0/all/0/1&quot;&gt;Robin J. Evans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Robins_J/0/1/0/all/0/1&quot;&gt;James M. Robins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shpitser_I/0/1/0/all/0/1&quot;&gt;Ilya Shpitser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2010.13599">
<title>Design-Based Inference for Spatial Experiments with Interference. (arXiv:2010.13599v3 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/2010.13599</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider design-based causal inference in settings where randomized
treatments have effects that bleed out into space in complex ways that overlap
and in violation of the standard &quot;no interference&quot; assumption for many causal
inference methods. We define a spatial &quot;average marginalized response,&quot; which
characterizes how, in expectation, units of observation that are a specified
distance from an intervention point are affected by treatments at that point,
averaging over effects emanating from other intervention points. We establish
conditions for non-parametric identification, asymptotic distributions of
estimators, and recovery of structural effects. We propose methods for both
sample-theoretic and permutation-based inference. We provide illustrations
using randomized field experiments on forest conservation and health.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Ye Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Samii_C/0/1/0/all/0/1&quot;&gt;Cyrus Samii&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chang_H/0/1/0/all/0/1&quot;&gt;Haoge Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Aronow_P/0/1/0/all/0/1&quot;&gt;P.M. Aronow&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2108.12515">
<title>Convergence Rates for Learning Linear Operators from Noisy Data. (arXiv:2108.12515v3 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/2108.12515</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies the learning of linear operators between
infinite-dimensional Hilbert spaces. The training data comprises pairs of
random input vectors in a Hilbert space and their noisy images under an unknown
self-adjoint linear operator. Assuming that the operator is diagonalizable in a
known basis, this work solves the equivalent inverse problem of estimating the
operator&apos;s eigenvalues given the data. Adopting a Bayesian approach, the
theoretical analysis establishes posterior contraction rates in the infinite
data limit with Gaussian priors that are not directly linked to the forward map
of the inverse problem. The main results also include learning-theoretic
generalization error guarantees for a wide range of distribution shifts. These
convergence rates quantify the effects of data smoothness and true eigenvalue
decay or growth, for compact or unbounded operators, respectively, on sample
complexity. Numerical evidence supports the theory in diagonal and non-diagonal
settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hoop_M/0/1/0/all/0/1&quot;&gt;Maarten V. de Hoop&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kovachki_N/0/1/0/all/0/1&quot;&gt;Nikola B. Kovachki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Nelsen_N/0/1/0/all/0/1&quot;&gt;Nicholas H. Nelsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Stuart_A/0/1/0/all/0/1&quot;&gt;Andrew M. Stuart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.04065">
<title>Honest calibration assessment for binary outcome predictions. (arXiv:2203.04065v2 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/2203.04065</link>
<description rdf:parseType="Literal">&lt;p&gt;Probability predictions from binary regressions or machine learning methods
ought to be calibrated: If an event is predicted to occur with probability $x$,
it should materialize with approximately that frequency, which means that the
so-called calibration curve $p(\cdot)$ should equal the identity, $p(x) = x$
for all $x$ in the unit interval. We propose honest calibration assessment
based on novel confidence bands for the calibration curve, which are valid only
subject to the natural assumption of isotonicity. Besides testing the classical
goodness-of-fit null hypothesis of perfect calibration, our bands facilitate
inverted goodness-of-fit tests whose rejection allows for the sought-after
conclusion of a sufficiently well specified model. We show that our bands have
a finite sample coverage guarantee, are narrower than existing approaches, and
adapt to the local smoothness of the calibration curve $p$ and the local
variance of the binary observations. In an application to model predictions of
an infant having a low birth weight, the bounds give informative insights on
model calibration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Dimitriadis_T/0/1/0/all/0/1&quot;&gt;Timo Dimitriadis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Duembgen_L/0/1/0/all/0/1&quot;&gt;Lutz Duembgen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Henzi_A/0/1/0/all/0/1&quot;&gt;Alexander Henzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Puke_M/0/1/0/all/0/1&quot;&gt;Marius Puke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ziegel_J/0/1/0/all/0/1&quot;&gt;Johanna Ziegel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.04539">
<title>Efficient algorithms for building representative matched pairs with enhanced generalizability. (arXiv:2205.04539v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/2205.04539</link>
<description rdf:parseType="Literal">&lt;p&gt;Many recent efforts center on assessing the ability of real-world evidence
(RWE) generated from non-randomized, observational data to produce results
compatible with those from randomized controlled trials (RCTs). One noticeable
endeavor is the RCT DUPLICATE initiative (Franklin et al., 2020, 2021). To
better reconcile findings from an observational study and an RCT, or two
observational studies based on different databases, it is desirable to
eliminate differences between study populations. We outline an efficient,
network-flow-based statistical matching algorithm that designs well-matched
pairs from observational data that resemble the covariate distributions of a
target population, for instance, the target-RCT-eligible population in the RCT
DUPLICATE initiative studies or a generic population of scientific interest. We
demonstrate the usefulness of the method by revisiting the inconsistency
regarding a cardioprotective effect of the hormone replacement therapy (HRT) in
the Women&apos;s Health Initiative (WHI) clinical trial and corresponding
observational study. We found that the discrepancy between the trial and
observational study persisted in a design that adjusted for study populations&apos;
cardiovascular risk profile, but seemed to disappear in a study design that
further adjusted for the HRT initiation age and previous
estrogen-plus-progestin use. The proposed method is integrated into the R
package match2C.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bo Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.07330">
<title>Semiparametric Best Arm Identification with Contextual Information. (arXiv:2209.07330v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.07330</link>
<description rdf:parseType="Literal">&lt;p&gt;We study best-arm identification with a fixed budget and contextual
(covariate) information in stochastic multi-armed bandit problems. In each
round, after observing contextual information, we choose a treatment arm using
past observations and current context. Our goal is to identify the best
treatment arm, a treatment arm with the maximal expected reward marginalized
over the contextual distribution, with a minimal probability of
misidentification. First, we derive semiparametric lower bounds of the
misidentification probability for this problem, where we regard the gaps
between the expected rewards of the best and suboptimal treatment arms as
parameters of interest, and all other parameters, such as the expected rewards
conditioned on contexts, as the nuisance parameters. We then develop the
``Contextual RS-AIPW strategy,&apos;&apos; which consists of the random sampling (RS)
rule tracking a target allocation ratio and the recommendation rule using the
augmented inverse probability weighting (AIPW) estimator. Our proposed
Contextual RS-AIPW strategy is optimal because the upper bound for the
probability of misidentification by the strategy matches the semiparametric
lower bound, when the budget goes to infinity and the gaps converge to zero.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kato_M/0/1/0/all/0/1&quot;&gt;Masahiro Kato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Imaizumi_M/0/1/0/all/0/1&quot;&gt;Masaaki Imaizumi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ishihara_T/0/1/0/all/0/1&quot;&gt;Takuya Ishihara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kitagawa_T/0/1/0/all/0/1&quot;&gt;Toru Kitagawa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.01757">
<title>Purely prognostic variables may modify marginal treatment effects for non-collapsible effect measures. (arXiv:2210.01757v3 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/2210.01757</link>
<description rdf:parseType="Literal">&lt;p&gt;In evidence synthesis, effect measure modifiers are typically described as
variables that induce treatment effect heterogeneity at the individual level,
through treatment-covariate interactions in an outcome model parametrized at
such level. As such, effect modification is defined with respect to a
conditional measure. However, marginal effect estimates are required for
population-level decisions in health technology assessment. For non-collapsible
effect measures, purely prognostic variables that do not predict response to
treatment at the individual level may modify marginal treatment effects at the
population level. This has important implications for recommended practices for
evidence synthesis. Firstly, unadjusted indirect comparisons of marginal
effects may be biased in the absence of individual-level treatment effect
heterogeneity. Secondly, covariate adjustment may be necessary to account for
cross-study imbalances in purely prognostic variables. Popular summary measures
in meta-analysis such as odds ratios and hazard ratios are non-collapsible.
Collapsible measures would facilitate the transportability of marginal effects
between studies by: (1) removing dependence on model-based covariate adjustment
when there is treatment effect homogeneity at the individual level; and (2)
facilitating the selection of baseline characteristics for covariate adjustment
when there is heterogeneity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Remiro_Azocar_A/0/1/0/all/0/1&quot;&gt;Antonio Remiro-Az&amp;#xf3;car&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.13133">
<title>Markov-modulated marked Poisson processes for modelling disease dynamics based on medical claims data. (arXiv:2210.13133v2 [stat.AP] UPDATED)</title>
<link>http://arxiv.org/abs/2210.13133</link>
<description rdf:parseType="Literal">&lt;p&gt;We explore Markov-modulated marked Poisson processes (MMMPPs) as a natural
framework for modelling patients&apos; disease dynamics over time based on medical
claims data. In claims data, observations do not only occur at random points in
time but are also informative, i.e. driven by unobserved disease levels, as
poor health conditions usually lead to more frequent interactions with the
healthcare system. Therefore, we model the observation process as a
Markov-modulated Poisson process, where the rate of healthcare interactions is
governed by a continuous-time Markov chain. Its states serve as proxies for the
patients&apos; latent disease levels and further determine the distribution of
additional data collected at each observation time, the so-called marks.
Overall, MMMPPs jointly model observations and their informative time points by
comprising two state-dependent processes: the observation process
(corresponding to the event times) and the mark process (corresponding to
event-specific information), which both depend on the underlying states. The
approach is illustrated using claims data from patients diagnosed with chronic
obstructive pulmonary disease (COPD) by modelling their drug use and the
interval lengths between consecutive physician consultations. The results
indicate that MMMPPs are able to detect distinct patterns of healthcare
utilisation related to disease processes and reveal inter-individual
differences in the state-switching dynamics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mews_S/0/1/0/all/0/1&quot;&gt;Sina Mews&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Surmann_B/0/1/0/all/0/1&quot;&gt;Bastian Surmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hasemann_L/0/1/0/all/0/1&quot;&gt;Lena Hasemann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Elkenkamp_S/0/1/0/all/0/1&quot;&gt;Svenja Elkenkamp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.15264">
<title>Generating the right evidence at the right time: Principles of a new class of flexible augmented clinical trial designs. (arXiv:2210.15264v3 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/2210.15264</link>
<description rdf:parseType="Literal">&lt;p&gt;The past few years have seen an increasing number of initiatives aimed at
integrating information generated outside of confirmatory randomised clinical
trials (RCTs) into drug development. However, data generated non-concurrently
and through observational studies can provide results that are difficult to
compare with randomised trial data. Moreover, the scientific questions these
data can serve to answer often remain vague. Our starting point is to use
clearly defined objectives for evidence generation, which are formulated
towards early discussion with health technology assessment (HTA) bodies and are
additional to regulatory requirements for authorisation of a new treatment. We
propose FACTIVE (Flexible Augmented Clinical Trial for Improved eVidencE
generation), a new class of study designs enabling flexible augmentation of
confirmatory randomised controlled trials with concurrent and
close-to-real-world elements. These enabling designs facilitate estimation of
certain treatment effects in the confirmatory part and other, complementary
treatment effects in a concurrent real-world part. Each stakeholder should use
the evidence that is relevant within their own decision-making framework. High
quality data are generated under one single protocol and the use of
randomisation ensures rigorous statistical inference and interpretation within
and between the different parts of the experiment. Evidence for the
decision-making of HTA bodies could be available earlier than is currently the
case.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dunger_Baldauf_C/0/1/0/all/0/1&quot;&gt;Cornelia Dunger-Baldauf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hemmings_R/0/1/0/all/0/1&quot;&gt;Rob Hemmings&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bretz_F/0/1/0/all/0/1&quot;&gt;Frank Bretz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jones_B/0/1/0/all/0/1&quot;&gt;Byron Jones&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schiel_A/0/1/0/all/0/1&quot;&gt;Anja Schiel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Holmes_C/0/1/0/all/0/1&quot;&gt;Chris Holmes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01170">
<title>Estimating intracluster correlation for ordinal data. (arXiv:2211.01170v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/2211.01170</link>
<description rdf:parseType="Literal">&lt;p&gt;Purpose: In this paper we consider the estimation of intracluster correlation
for ordinal data. We focus on pure-tone audiometry hearing threshold data,
where thresholds are measured in 5 decibel increments. We estimate the
intracluster correlation for tests from iPhone-based hearing assessment
application as a measure of test/retest reliability. Methods: We present a
method to estimate the intracluster correlation using mixed effects cumulative
logistic and probit models, which assume the outcome data are ordinal. This
contrasts with using a mixed effects linear model which assumes that the
outcome data are continuous. Results: In simulation studies we show that using
a mixed effects linear model to estimate the intracluster correlation for
ordinal data results in a negative finite sample bias, while using mixed
effects cumulative logistic or probit models reduces this bias. The estimated
intracluster correlation for the iPhone-based hearing assessment application is
higher when using the mixed effects cumulative logistic and probit models
compared to using a mixed effects linear model. Conclusion: When data are
ordinal, using mixed effects cumulative logistic or probit models reduces the
bias of intracluster correlation estimates relative to using a mixed effects
linear model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Langworthy_B/0/1/0/all/0/1&quot;&gt;Benjamin W. Langworthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hou_Z/0/1/0/all/0/1&quot;&gt;Zhaoxun Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Curhan_G/0/1/0/all/0/1&quot;&gt;Gary C. Curhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Curhan_S/0/1/0/all/0/1&quot;&gt;Sharon G. Curhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Molin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01227">
<title>Conformalized survival analysis with adaptive cutoffs. (arXiv:2211.01227v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/2211.01227</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a method that constructs valid and efficient lower
predictive bounds (LPBs) for survival times with censored data. Traditional
methods for survival analysis often assume a parametric model for the
distribution of survival time as a function of the measured covariates, or
assume that this conditional distribution is captured well with a
non-parametric method such as random forests; however, these methods may lead
to undercoverage if their assumptions are not satisfied. In this paper, we
build on recent work by Cand\`es et al. (2021), which offers a more
assumption-lean approach to the problem. Their approach first subsets the data
to discard any data points with early censoring times and then uses a
reweighting technique (namely, weighted conformal inference (Tibshirani et al.,
2019)) to correct for the distribution shift introduced by this subsetting
procedure. For our new method, instead of constraining to a fixed threshold for
the censoring time when subsetting the data, we allow for a covariate-dependent
and data-adaptive subsetting step, which is better able to capture the
heterogeneity of the censoring mechanism. As a result, our method can lead to
LPBs that are less conservative and give more accurate information. We show
that in the Type I right-censoring setting, if either of the censoring
mechanism or the conditional quantile of survival time is well estimated, our
proposed procedure achieves approximately exact marginal coverage, where in the
latter case we additionally have approximate conditional coverage. We evaluate
the validity and efficiency of our proposed algorithm in numerical experiments,
illustrating its advantage when compared with other competing methods. Finally,
our method is applied to a real dataset to generate LPBs for users&apos; active
times on a mobile app.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gui_Y/0/1/0/all/0/1&quot;&gt;Yu Gui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hore_R/0/1/0/all/0/1&quot;&gt;Rohan Hore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ren_Z/0/1/0/all/0/1&quot;&gt;Zhimei Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Barber_R/0/1/0/all/0/1&quot;&gt;Rina Foygel Barber&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>