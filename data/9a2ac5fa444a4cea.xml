<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>YerevaNN</title>
 <link href="http://yerevann.github.io//atom.xml" rel="self"/>
 <link href="http://yerevann.github.io//"/>
 <updated>2017-11-08T14:53:49+00:00</updated>
 <id>http://yerevann.github.io/</id>
 <author>
   <name>Hrant Khachatrian</name>
   <email>hrant.khachatrian@ysu.am</email>
 </author>

 
 <entry>
   <title>Challenges of reproducing R-NET neural network using Keras</title>
   <link href="http://yerevann.github.io//2017/08/25/challenges-of-reproducing-r-net-neural-network-using-keras/"/>
   <updated>2017-08-25T00:00:00+00:00</updated>
   <id>http://yerevann.github.io//2017/08/25/challenges-of-reproducing-r-net-neural-network-using-keras</id>
   <content type="html">&lt;p&gt;By &lt;a href=&quot;https://github.com/MartinXPN&quot;&gt;Martin Mirakyan&lt;/a&gt;, &lt;a href=&quot;https://github.com/mahnerak&quot;&gt;Karen Hambardzumyan&lt;/a&gt; and
 &lt;a href=&quot;https://github.com/Hrant-Khachatrian&quot;&gt;Hrant Khachatrian&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this post we describe our attempt to re-implement a neural architecture for automated question answering called &lt;a href=&quot;https://www.microsoft.com/en-us/research/publication/mrc/&quot;&gt;R-NET&lt;/a&gt;, which is developed by the Natural Language Computing Group of Microsoft Research Asia. This architecture demonstrates the best performance among single models (not ensembles) on The Stanford Question Answering Dataset (as of August 25, 2017). MSR researchers released a &lt;a href=&quot;https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf&quot;&gt;technical report&lt;/a&gt; describing the model but did not release the code. We tried to implement the architecture in Keras framework and reproduce their results. This post describes the model and the challenges we faced while implementing it &lt;a class=&quot;nav-link&quot; href=&quot;https://github.com/YerevaNN/R-NET-in-Keras&quot;&gt;[&lt;span class=&quot;hidden-xs-down&quot;&gt;View on GitHub &lt;/span&gt;&lt;svg version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; viewBox=&quot;0 0 16 16&quot; class=&quot;octicon octicon-mark-github&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; fill=&quot;#268bd2&quot; d=&quot;M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;]&lt;/a&gt;.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 class=&quot;no_toc&quot; id=&quot;contents&quot;&gt;Contents&lt;/h2&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#problem-statement&quot; id=&quot;markdown-toc-problem-statement&quot;&gt;Problem statement&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#the-architecture-of-r-net&quot; id=&quot;markdown-toc-the-architecture-of-r-net&quot;&gt;The architecture of R-NET&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#drawing-complex-recurrent-networks&quot; id=&quot;markdown-toc-drawing-complex-recurrent-networks&quot;&gt;Drawing complex recurrent networks&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#1-question-and-passage-encoder&quot; id=&quot;markdown-toc-1-question-and-passage-encoder&quot;&gt;1. Question and passage encoder&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#2-obtain-question-aware-representation-for-the-passage&quot; id=&quot;markdown-toc-2-obtain-question-aware-representation-for-the-passage&quot;&gt;2. Obtain question aware representation for the passage&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#3-apply-self-matching-attention-on-the-passage-to-get-its-final-representation&quot; id=&quot;markdown-toc-3-apply-self-matching-attention-on-the-passage-to-get-its-final-representation&quot;&gt;3. Apply self-matching attention on the passage to get its final representation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#4-predict-the-interval-which-contains-the-answer-of-a-question&quot; id=&quot;markdown-toc-4-predict-the-interval-which-contains-the-answer-of-a-question&quot;&gt;4. Predict the interval which contains the answer of a question&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#implementation-details&quot; id=&quot;markdown-toc-implementation-details&quot;&gt;Implementation details&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#layers-with-masking-support&quot; id=&quot;markdown-toc-layers-with-masking-support&quot;&gt;Layers with masking support&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#slice-layer&quot; id=&quot;markdown-toc-slice-layer&quot;&gt;Slice layer&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#generators&quot; id=&quot;markdown-toc-generators&quot;&gt;Generators&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#bidirectional-grus&quot; id=&quot;markdown-toc-bidirectional-grus&quot;&gt;Bidirectional GRUs&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#dropout&quot; id=&quot;markdown-toc-dropout&quot;&gt;Dropout&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#weight-sharing&quot; id=&quot;markdown-toc-weight-sharing&quot;&gt;Weight sharing&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#hyperparameters&quot; id=&quot;markdown-toc-hyperparameters&quot;&gt;Hyperparameters&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#weight-initialization&quot; id=&quot;markdown-toc-weight-initialization&quot;&gt;Weight initialization&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#training&quot; id=&quot;markdown-toc-training&quot;&gt;Training&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#results-and-comparison-with-r-net-technical-report&quot; id=&quot;markdown-toc-results-and-comparison-with-r-net-technical-report&quot;&gt;Results and comparison with &lt;a href=&quot;https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf&quot;&gt;R-NET technical report&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#challenges-of-reproducibility&quot; id=&quot;markdown-toc-challenges-of-reproducibility&quot;&gt;Challenges of reproducibility&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;problem-statement&quot;&gt;Problem statement&lt;/h2&gt;

&lt;p&gt;Given a passage and a question, the task is to predict an answer to the question based on the information found in the passage. The SQuAD dataset further constrains the answer to be a continuous sub-span of the provided passage. Answers usually include non-entities and can be long phrases. The neural network needs to “understand” both the passage and the question in order to be able to give a valid answer. Here is an example from the dataset.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Passage:&lt;/strong&gt; Tesla later approached Morgan to ask for more funds to build a more powerful transmitter. When asked where all the money had gone, Tesla responded by saying that he was affected by the Panic of 1901, which he (Morgan) had caused. Morgan was shocked by the reminder of his part in the stock market crash and by Tesla’s breach of contract by asking for more funds. Tesla wrote another plea to Morgan, but it was also fruitless. Morgan still owed Tesla money on the original agreement, and Tesla had been facing foreclosure even before construction of the tower began.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; On what did Tesla blame for the loss of the initial money?
&lt;strong&gt;Answer:&lt;/strong&gt; Panic of 1901&lt;/p&gt;

&lt;h2 id=&quot;the-architecture-of-r-net&quot;&gt;The architecture of R-NET&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/YerevaNN/R-NET-in-Keras/blob/master/model.py&quot;&gt;architecture&lt;/a&gt; of R-NET network is designed to take the question and the passage as inputs and to output an interval on the passage that contains the answer. The process consists of several steps:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Encode the question and the passage&lt;/li&gt;
  &lt;li&gt;Obtain question aware representation for the passage&lt;/li&gt;
  &lt;li&gt;Apply self-matching attention on the passage to get its final representation.&lt;/li&gt;
  &lt;li&gt;Predict the interval which contains the answer of the question.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Each of these steps is implemented as some sort of recurrent neural network. The model is trained end-to-end.&lt;/p&gt;

&lt;h3 id=&quot;drawing-complex-recurrent-networks&quot;&gt;Drawing complex recurrent networks&lt;/h3&gt;

&lt;p&gt;We are using &lt;a href=&quot;https://arxiv.org/abs/1412.3555&quot;&gt;GRU&lt;/a&gt; cells (Gated Recurrent Unit) for all RNNs. The authors claim that their performance is similar to LSTM, but they are computationally cheaper.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://rawgit.com/YerevaNN/yerevann.github.io/master/public/2017-08-22/GRU.svg&quot; alt=&quot;GRU network&quot; title=&quot;GRU network&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Most of the modules of R-NET are implemented as recurrent networks with  complex cells. We draw these cells using colorful charts. Here is a chart that corresponds to the original GRU cell.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://rawgit.com/YerevaNN/yerevann.github.io/master/public/2017-08-22/GRUcell.svg&quot; alt=&quot;GRU cell&quot; title=&quot;GRU cell&quot; /&gt;&lt;/p&gt;

&lt;p&gt;White rectangles represent operations on tensors (dot product, sum, etc.). Yellow rectangles are activations (tanh, softmax or sigmoid). Orange circles are the weights of the network. Compare this to the formula of GRU cell (taken from &lt;a href=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;&gt;Olah’s famous blogpost&lt;/a&gt;):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\large
z_t &amp;=\sigma(W_z \cdot [h_{t-1}, x_t]) \\
r_t &amp;=\sigma(W_r \cdot [h_{t-1}, x_t]) \\
\tilde{h}_t &amp;= tanh(W \cdot [r_t \circ h_{t-1}, x_t]) \\
h_t &amp;= (1 - z_t) \circ h_{t-1} + z_t \circ \tilde{h}_t
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Some parts of R-NET architecture require to use tensors that are neither part of a GRU state nor part of an input at time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;. These are “global” variables that are used in all timesteps. Following &lt;a href=&quot;http://deeplearning.net/software/theano/library/scan.html&quot;&gt;Theano’s terminology&lt;/a&gt;, we call these global variables &lt;em&gt;non-sequences&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;To make it easier to create GRU cells with additional features and operations we’ve created a &lt;a href=&quot;https://github.com/YerevaNN/R-NET-in-Keras/blob/master/layers/WrappedGRU.py&quot;&gt;utility class called &lt;strong&gt;WrappedGRU&lt;/strong&gt;&lt;/a&gt; which is a base class for all GRU modules. WrappedGRU supports operations with non-sequences and sharing weights between modules. Keras doesn’t directly support weight sharing, but instead it supports layer sharing and we use &lt;a href=&quot;https://github.com/YerevaNN/R-NET-in-Keras/blob/master/layers/SharedWeight.py&quot;&gt;SharedWeight layer&lt;/a&gt; to solve this problem (SharedWeight is a layer that has no inputs and returns tensor of weights). WrappedGRU supports taking SharedWeight as an input.&lt;/p&gt;

&lt;h3 id=&quot;1-question-and-passage-encoder&quot;&gt;1. Question and passage encoder&lt;/h3&gt;

&lt;p&gt;This step consists of two parts: &lt;a href=&quot;https://github.com/YerevaNN/R-NET-in-Keras/blob/master/preprocessing.py&quot;&gt;preprocessing&lt;/a&gt; and text encoding. The preprocessing is done in a separate process and is not part of the neural network. First we preprocess the data by splitting it into parts, and then we convert all the words to corresponding vectors. Word-vectors are generated using &lt;a href=&quot;https://github.com/YerevaNN/R-NET-in-Keras/blob/master/preprocessing.py#L35&quot;&gt;gensim&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The next steps are already part of the model. Each word is represented by a concatenation of two vectors: its GloVe vector and another vector that holds character level information. To obtain character level embeddings we use an Embedding layer followed by a Bidirectional GRU cell wrapped inside a TimeDistributed layer. Basically, each character is embedded in &lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt; dimensional space, and a BiGRU runs over those embeddings to produce a vector for the word. The process is repeated for all the words using TimeDistributed layer.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/YerevaNN/R-NET-in-Keras/blob/master/model.py#L62&quot;&gt;Code on GitHub&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;TimeDistributed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;InputLayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'int32'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;127&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask_zero&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Bidirectional&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GRU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When the word is missing from GloVe, we set its word vector to all zeros (as described in the technical report).&lt;/p&gt;

&lt;p&gt;Following the notation of the paper, we denote the vector representation of the question by &lt;script type=&quot;math/tex&quot;&gt;u^Q&lt;/script&gt; and the representation of the passage by &lt;script type=&quot;math/tex&quot;&gt;u^P&lt;/script&gt; (&lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt; corresponds to the question and &lt;script type=&quot;math/tex&quot;&gt;P&lt;/script&gt; corresponds to the passage).&lt;/p&gt;

&lt;p&gt;The network takes the preprocessed question &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt; and the passage &lt;script type=&quot;math/tex&quot;&gt;P&lt;/script&gt;, applies masking on each one and then encodes them with 3 consecutive bidirectional GRU layers.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/YerevaNN/R-NET-in-Keras/blob/master/model.py#L81&quot;&gt;Code on GitHub&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Encode the passage P&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;uP&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Masking&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;uP&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Bidirectional&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GRU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                           &lt;span class=&quot;n&quot;&gt;return_sequences&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                           &lt;span class=&quot;n&quot;&gt;dropout&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropout_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;uP&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropout_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'uP'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Encode the question Q&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;uQ&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Masking&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;uQ&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Bidirectional&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GRU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                           &lt;span class=&quot;n&quot;&gt;return_sequences&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                           &lt;span class=&quot;n&quot;&gt;dropout&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropout_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uQ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;uQ&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropout_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'uQ'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uQ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After encoding the passage and the question we finally have their vector representations &lt;script type=&quot;math/tex&quot;&gt;u^P&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;u^Q&lt;/script&gt;. Now we can delve deeper in understanding the meaning of the passage having in mind the question.&lt;/p&gt;

&lt;h3 id=&quot;2-obtain-question-aware-representation-for-the-passage&quot;&gt;2. Obtain question aware representation for the passage&lt;/h3&gt;

&lt;p&gt;The next module computes another representation for the passage by taking into account the words inside the question sentence. We implement it using the following code:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/YerevaNN/R-NET-in-Keras/blob/master/model.py#L97&quot;&gt;Code on GitHub&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;vP&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;QuestionAttnGRU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
             &lt;span class=&quot;n&quot;&gt;return_sequences&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;uP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;uQ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;WQ_u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;WP_v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;WP_u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W_g1&lt;/span&gt;
             &lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/YerevaNN/R-NET-in-Keras/blob/master/layers/QuestionAttnGRU.py&quot;&gt;QuestionAttnGRU&lt;/a&gt; is a complex extension of a recurrent layer (extends WrappedGRU and overrides the step method by adding additional operations before passing the input to the GRU cell).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://rawgit.com/YerevaNN/yerevann.github.io/master/public/2017-08-22/QuestionAttnGRU.svg&quot; alt=&quot;QuestionAttnGRU&quot; title=&quot;Question Attention GRU&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The vectors of question aware representation of the passage are denoted by &lt;script type=&quot;math/tex&quot;&gt;v^P&lt;/script&gt;. As a reminder &lt;script type=&quot;math/tex&quot;&gt;u^P_t&lt;/script&gt; is the vector representation of the passage &lt;script type=&quot;math/tex&quot;&gt;P&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;u^Q&lt;/script&gt; is the matrix representation of the question &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt; (each row corresponds to a single word).&lt;/p&gt;

&lt;p&gt;In QuestionAttnGRU first we combine three things:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;the previous state of the GRU (&lt;script type=&quot;math/tex&quot;&gt;v^P_{t-1}&lt;/script&gt;)&lt;/li&gt;
  &lt;li&gt;matrix representation of the question (&lt;script type=&quot;math/tex&quot;&gt;u^Q&lt;/script&gt;)&lt;/li&gt;
  &lt;li&gt;vector representation of the passage (&lt;script type=&quot;math/tex&quot;&gt;u^P_{t}&lt;/script&gt;) at the &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;-th word.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We compute the dot product of each input with the corresponding weights, then sum-up all together after broadcasting them into the same shape. The outputs of dot(&lt;script type=&quot;math/tex&quot;&gt;u^P_{t}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;W^P_{u}&lt;/script&gt;) and dot(&lt;script type=&quot;math/tex&quot;&gt;v^P_{t-1}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;W^P_{v}&lt;/script&gt;) are vectors, while the output of dot(&lt;script type=&quot;math/tex&quot;&gt;u^Q&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;W^Q_{u}&lt;/script&gt;) is a matrix, therefore we broadcast (repeat several times) the vectors to match the shape of the matrix and then compute the sum of three matrices. Then we apply tanh activation on the result. The output of this operation is then multiplied (dot product) by a weight vector &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;, after which &lt;script type=&quot;math/tex&quot;&gt;softmax&lt;/script&gt; activation is applied. The output of the &lt;script type=&quot;math/tex&quot;&gt;softmax&lt;/script&gt; is a vector of non-negative numbers that represent the “importance” of each word in the question. This kind of vectors are often called &lt;em&gt;attention vectors&lt;/em&gt;. When computing the dot product of &lt;script type=&quot;math/tex&quot;&gt;u^Q&lt;/script&gt; (matrix representation of the question) and the attention vector, we obtain a single vector for the entire question which is a weighted average of question word vectors (weighted by the attention scores). The intuition behind this part is that we get a representation of the parts of the question that are relevant to the current word of the passage. This representation, denoted by &lt;script type=&quot;math/tex&quot;&gt;c_{t}&lt;/script&gt;, depends on the current word, the whole question and the previous state of the recurrent cell (formula 4 on page 3 of the &lt;a href=&quot;https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf&quot;&gt;report&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;These ideas seem to come from a paper by &lt;a href=&quot;https://arxiv.org/abs/1509.06664&quot;&gt;Rocktäschel et al.&lt;/a&gt; from Deepmind. The authors suggested to pass this &lt;script type=&quot;math/tex&quot;&gt;c_{t}&lt;/script&gt; vector as an input to the GRU cell. &lt;a href=&quot;https://arxiv.org/abs/1512.08849&quot;&gt;Wang and Jiang&lt;/a&gt; from Singapore Management University argued that passing &lt;script type=&quot;math/tex&quot;&gt;c_{t}&lt;/script&gt; is not enough, because we are losing information from the “original” input &lt;script type=&quot;math/tex&quot;&gt;u^P_{t}&lt;/script&gt;. So they suggested to concatenate &lt;script type=&quot;math/tex&quot;&gt;c_{t}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;u^P_{t}&lt;/script&gt; before passing it to the GRU cell.&lt;/p&gt;

&lt;p&gt;The authors of R-NET did one more step. They applied an additional gate to the concatenated vector &lt;script type=&quot;math/tex&quot;&gt;[c_{t}, u^P_{t}]&lt;/script&gt;. The gate is simply a dot product of some new weight matrix &lt;script type=&quot;math/tex&quot;&gt;W_{g}&lt;/script&gt; and the concatenated vector, passed through a sigmoid activation function. The output of the gate is a vector of non-negative numbers, which is then (element-wise) multiplied by the original concatenated vector (see formula 6 on page 4 of the &lt;a href=&quot;https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf&quot;&gt;report&lt;/a&gt;). The result of this multiplication is finally passed to the GRU cell as an input.&lt;/p&gt;

&lt;h3 id=&quot;3-apply-self-matching-attention-on-the-passage-to-get-its-final-representation&quot;&gt;3. Apply self-matching attention on the passage to get its final representation&lt;/h3&gt;

&lt;p&gt;Next, the authors suggest to add a self attention mechanism on the passage itself.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/YerevaNN/R-NET-in-Keras/blob/master/model.py#L105&quot;&gt;Code on GitHub&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;hP&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Bidirectional&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SelfAttnGRU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                               &lt;span class=&quot;n&quot;&gt;return_sequences&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;
                       &lt;span class=&quot;n&quot;&gt;vP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                       &lt;span class=&quot;n&quot;&gt;WP_v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;WPP_v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W_g2&lt;/span&gt;
                   &lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;hP&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropout_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'hP'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The output of the previous step (Question attention) is denoted by &lt;script type=&quot;math/tex&quot;&gt;v^P&lt;/script&gt;. It represents the encoding of the passage while taking into account the question. &lt;script type=&quot;math/tex&quot;&gt;v^P&lt;/script&gt; is passed as an input to the self-matching attention module (top input, left input). The authors argue that the vectors &lt;script type=&quot;math/tex&quot;&gt;v^P_{t}&lt;/script&gt; have very limited information about the context. &lt;a href=&quot;https://github.com/YerevaNN/R-NET-in-Keras/blob/master/layers/SelfAttnGRU.py&quot;&gt;Self-matching attention module&lt;/a&gt; attempts to augment the passage vectors by information from other relevant parts of the passage.&lt;/p&gt;

&lt;p&gt;The output of the self-matching GRU cell at time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; is denoted by &lt;script type=&quot;math/tex&quot;&gt;h^P_{t}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://rawgit.com/YerevaNN/yerevann.github.io/master/public/2017-08-22/SelfAttnGRU.svg&quot; alt=&quot;SelfAttnGRU&quot; title=&quot;Self-matching Attention GRU&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The implementation is very similar to the previous module. We compute dot products of weights &lt;script type=&quot;math/tex&quot;&gt;W^PP_{u}&lt;/script&gt; with the current word vector &lt;script type=&quot;math/tex&quot;&gt;v^P_{t}&lt;/script&gt;, and &lt;script type=&quot;math/tex&quot;&gt;W^P_{v}&lt;/script&gt; with the entire &lt;script type=&quot;math/tex&quot;&gt;v^P&lt;/script&gt; matrix, then add them up and apply &lt;script type=&quot;math/tex&quot;&gt;\tanh{}&lt;/script&gt; activation. Next, the result is multiplied by a weight-vector &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; and passed through &lt;script type=&quot;math/tex&quot;&gt;softmax&lt;/script&gt; activation, which produces an attention vector. The dot product of the attention vector and &lt;script type=&quot;math/tex&quot;&gt;v^P&lt;/script&gt; matrix, again denoted by &lt;script type=&quot;math/tex&quot;&gt;c_{t}&lt;/script&gt;, is the weighted average of all word vectors of the passage that are relevant to the current word &lt;script type=&quot;math/tex&quot;&gt;v^P_{t}&lt;/script&gt;. &lt;script type=&quot;math/tex&quot;&gt;c_{t}&lt;/script&gt; is then concatenated with &lt;script type=&quot;math/tex&quot;&gt;v^P_{t}&lt;/script&gt; itself. The concatenated vector is passed through a gate and is given to GRU cell as an input.&lt;/p&gt;

&lt;p&gt;The authors consider this step as their main contribution to the architecture.&lt;/p&gt;

&lt;p&gt;It is interesting to note that the authors write &lt;code class=&quot;highlighter-rouge&quot;&gt;BiRNN&lt;/code&gt; in Section 3.3 (Self-Matching Attention) and just &lt;code class=&quot;highlighter-rouge&quot;&gt;RNN&lt;/code&gt; in Section 3.2 (which describes question-aware passage representation). For that reason we used BiGRU in SelfAttnGRU and unidirectional GRU in QuestionAttnGRU. Later we discovered a sentence in Section 4.1 which suggests that we were not correct: &lt;code class=&quot;highlighter-rouge&quot;&gt;the gated attention-based recurrent network for question and passage matching is also encoded bidirectionally in our experiment&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;4-predict-the-interval-which-contains-the-answer-of-a-question&quot;&gt;4. Predict the interval which contains the answer of a question&lt;/h3&gt;

&lt;p&gt;Finally we’re ready to predict the interval of the passage which contains the answer of the question. To do this we use &lt;a href=&quot;https://github.com/YerevaNN/R-NET-in-Keras/blob/master/layers/QuestionPooling.py&quot;&gt;QuestionPooling layer&lt;/a&gt; followed by PointerGRU (&lt;a href=&quot;https://arxiv.org/abs/1506.03134&quot;&gt;Vinyals et al., Pointer networks, 2015&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/YerevaNN/R-NET-in-Keras/blob/master/model.py#L118&quot;&gt;Code on GitHub&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;rQ&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;QuestionPooling&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uQ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;WQ_u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;WQ_v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rQ&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropout_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'rQ'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rQ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PointerGRU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;return_sequences&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;initial_state_provided&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ps'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;fake_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;WP_h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Wa_h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rQ&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;answer_start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Slice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'answer_start '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;answer_end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Slice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'answer_end'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;QuestionPooling is the attention pooling of the whole question vector &lt;script type=&quot;math/tex&quot;&gt;u^Q&lt;/script&gt;. Its purpose is to create the first hidden state of PointerGRU. It is similar to the other attention-based modules, but has a strange description in the report. Formula 11 on page 5 includes a product of two tensors &lt;script type=&quot;math/tex&quot;&gt;W_v^Q&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;V_r^Q&lt;/script&gt;. Both these tensors are trainable parameters (as confirmed by Furu Wei, one of the coauthors of the technical report), and it is not clear why this dot product is not replaced by a single trainable vector.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;h^P&lt;/script&gt; is the output of the previous module and it contains the final representation of the passage. It is passed to this module as an input to obtain the final answer.&lt;/p&gt;

&lt;p&gt;In Section 4.2 of the &lt;a href=&quot;https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf&quot;&gt;technical report&lt;/a&gt; the authors write that after submitting their paper to ACL they made one more modification. They have added &lt;a href=&quot;https://github.com/YerevaNN/R-NET-in-Keras/blob/master/model.py#L114-L116&quot;&gt;another bidirectional GRU&lt;/a&gt; on top of &lt;script type=&quot;math/tex&quot;&gt;h^P&lt;/script&gt; before feeding it to PointerGRU.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://rawgit.com/YerevaNN/yerevann.github.io/master//public/2017-08-22/PointerGRU.svg&quot; alt=&quot;PointerGRU&quot; title=&quot;Pointer GRU&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/YerevaNN/R-NET-in-Keras/blob/master/layers/PointerGRU.py&quot;&gt;PointerGRU&lt;/a&gt; is a recurrent network that works for just two steps. The first step predicts the first word of the answer span, and the second step predicts the last word. Here is how it works. Both &lt;script type=&quot;math/tex&quot;&gt;h^P&lt;/script&gt; and the previous state of the PointerGRU cell are multiplied by their corresponding weights &lt;script type=&quot;math/tex&quot;&gt;W&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;W^a_{v}&lt;/script&gt;. Recall that the initial hidden state of the PointerGRU is the output of QuestionPooling. The products are then summed up and passed through &lt;script type=&quot;math/tex&quot;&gt;tanh&lt;/script&gt; activation. The result is multiplied by the weight vector &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;softmax&lt;/script&gt; activation is applied which outputs scores over &lt;script type=&quot;math/tex&quot;&gt;h^P&lt;/script&gt;. These scores, denoted by &lt;script type=&quot;math/tex&quot;&gt;a^t&lt;/script&gt; are probabilities over the words of the passage. Argmax of &lt;script type=&quot;math/tex&quot;&gt;a^1&lt;/script&gt; vector is the predicted starting point, and argmax of &lt;script type=&quot;math/tex&quot;&gt;a^2&lt;/script&gt; is the predicted final point of the answer (formula 9 on page 4 of the &lt;a href=&quot;https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf&quot;&gt;report&lt;/a&gt;). The hidden state of PointerGRU is determined based on the dot product of &lt;script type=&quot;math/tex&quot;&gt;h^P&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;a^t&lt;/script&gt;, which is passed as an input to a simple GRU cell (formula 10 on page 4 of the &lt;a href=&quot;https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf&quot;&gt;report&lt;/a&gt;). So, unlike all previous modules of R-NET, the &lt;em&gt;output&lt;/em&gt; of PointerGRU (the red diamond at the top-right corner of the chart) is different from its hidden state.&lt;/p&gt;

&lt;h2 id=&quot;implementation-details&quot;&gt;Implementation details&lt;/h2&gt;

&lt;p&gt;We use Theano backend for Keras. It was faster than TensorFlow in our experiments. Our experience shows that TensorFlow is usually faster for simple network architectures. Probably Theano’s optimization process is more efficient for complex extensions of recurrent networks.&lt;/p&gt;

&lt;h4 id=&quot;layers-with-masking-support&quot;&gt;Layers with masking support&lt;/h4&gt;

&lt;p&gt;One of the most important challenges in training recurrent networks is to handle different lengths of data points in a single batch. Keras has a &lt;a href=&quot;https://keras.io/layers/core/#masking&quot;&gt;Masking layer&lt;/a&gt; that handles the basic cases. We use it in the &lt;a href=&quot;https://github.com/YerevaNN/R-NET-in-Keras/blob/master/model.py#L81&quot;&gt;encoding layer&lt;/a&gt;. But R-NET has more complex scenarios for which we had to develop our own solutions. For example, in all attention pooling modules we use &lt;script type=&quot;math/tex&quot;&gt;softmax&lt;/script&gt; which is applied along “time” axis (e.g. over the words of the passage). We don’t want to have positive probabilities after the last word of the sentence. So we have implemented a &lt;a href=&quot;https://github.com/YerevaNN/R-NET-in-Keras/blob/master/layers/helpers.py#L7&quot;&gt;custom Softmax function&lt;/a&gt; which supports masking:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keepdims&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keepdims&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;floatx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;m&lt;/code&gt; is used for numerical stability. To support masking we &lt;a href=&quot;https://github.com/YerevaNN/R-NET-in-Keras/blob/master/layers/helpers.py#L15&quot;&gt;multiply&lt;/a&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;e&lt;/code&gt; by the mask. We also clip &lt;code class=&quot;highlighter-rouge&quot;&gt;s&lt;/code&gt; by a very small number, because in theory it is possible that all positive values of &lt;code class=&quot;highlighter-rouge&quot;&gt;e&lt;/code&gt; are outside the mask.&lt;/p&gt;

&lt;p&gt;Note that details like this are not described in the technical report. Probably these are considered as commonly known tricks. But sometimes the details of the masking process can have critical effects on the results (we know this from the work on &lt;a href=&quot;https://arxiv.org/abs/1703.07771&quot;&gt;medical time series&lt;/a&gt;).&lt;/p&gt;

&lt;h4 id=&quot;slice-layer&quot;&gt;Slice layer&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/YerevaNN/R-NET-in-Keras/blob/master/layers/Slice.py&quot;&gt;Slice layer&lt;/a&gt; is supposed to slice and return the input tensor at the given indices. It also supports masking. The slice layer in R-NET model is needed to extract the final answer (i.e. the &lt;code class=&quot;highlighter-rouge&quot;&gt;interval_start&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;interval_end&lt;/code&gt; numbers). The final output of the model is a tensor with shape &lt;code class=&quot;highlighter-rouge&quot;&gt;(batch x 2 x passage_length)&lt;/code&gt;. The first row contains probabilities for &lt;code class=&quot;highlighter-rouge&quot;&gt;answer_start&lt;/code&gt; and the second one for &lt;code class=&quot;highlighter-rouge&quot;&gt;answer_end&lt;/code&gt;, that’s why we need to &lt;a href=&quot;https://github.com/YerevaNN/R-NET-in-Keras/blob/master/model.py#L134-L135&quot;&gt;slice&lt;/a&gt; the rows first and then extract the required information. Obviously we could accomplish the task without creating a new layer, yet it wouldn’t be a “Kerasic” solution.&lt;/p&gt;

&lt;h4 id=&quot;generators&quot;&gt;Generators&lt;/h4&gt;

&lt;p&gt;Keras supports &lt;a href=&quot;https://github.com/YerevaNN/R-NET-in-Keras/blob/master/data.py#L46&quot;&gt;batch generators&lt;/a&gt; which are responsible for generating one batch per each iteration. One benefit of this approach is that the generator is working on a separate thread and is not waiting for the network to finish its training on the previous batch.&lt;/p&gt;

&lt;h4 id=&quot;bidirectional-grus&quot;&gt;Bidirectional GRUs&lt;/h4&gt;

&lt;p&gt;R-NET uses multiple bidirectional GRUs. The common way of implementing BiRNN is to take two copies of the same network (without sharing the weights) and then concatenate the hidden states to produce the output. One can take the sum of the vectors instead of concatenating them, but concatenation seems to be more popular (that’s the default version of &lt;a href=&quot;https://keras.io/layers/wrappers/&quot;&gt;Bidirectional layer&lt;/a&gt; in Keras).&lt;/p&gt;

&lt;h4 id=&quot;dropout&quot;&gt;Dropout&lt;/h4&gt;

&lt;p&gt;The report indicates that dropout is applied “between layers with a dropout rate of 0.2”. We have applied dropout &lt;a href=&quot;https://github.com/YerevaNN/R-NET-in-Keras/blob/master/model.py#L85&quot;&gt;before each of the three layers&lt;/a&gt; of BiGRUs of both encoders, at the &lt;a href=&quot;https://github.com/YerevaNN/R-NET-in-Keras/blob/master/model.py#L87&quot;&gt;outputs of both encoders&lt;/a&gt;, right &lt;a href=&quot;https://github.com/YerevaNN/R-NET-in-Keras/blob/master/model.py#L103&quot;&gt;after QuestionAttnGRU&lt;/a&gt;, &lt;a href=&quot;https://github.com/YerevaNN/R-NET-in-Keras/blob/master/model.py#L112&quot;&gt;after SelfAttnGRU&lt;/a&gt; and &lt;a href=&quot;https://github.com/YerevaNN/R-NET-in-Keras/blob/master/model.py#L119&quot;&gt;after QuestionPooling&lt;/a&gt; layer. We are not sure that this is exactly what the authors did.&lt;/p&gt;

&lt;p&gt;One more implementation detail is related to the way dropout is applied on the passage and question representation matrices. The rows of these matrices correspond to different words and the “vanilla” dropout will apply different masks on different words. These matrices are used as inputs to recurrent networks. But it is a common trick to apply the same mask at each “timestep”, i.e. each word. That’s how dropout is implemented in &lt;a href=&quot;https://github.com/fchollet/keras/blob/master/keras/layers/recurrent.py#L15&quot;&gt;recurrent layers in Keras&lt;/a&gt;. The report doesn’t discuss these details.&lt;/p&gt;

&lt;h4 id=&quot;weight-sharing&quot;&gt;Weight sharing&lt;/h4&gt;

&lt;p&gt;The report doesn’t explicitly describe which weights are shared. We have decided to share those weights that are represented by the same symbol in the report. Note that the authors use the same symbol (e.g. &lt;script type=&quot;math/tex&quot;&gt;c_{t}&lt;/script&gt;) for different variables (not weights) that obviously cannot be shared. But we hope that our assumption is true for weights. In particular, we share:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;W^Q_{u}&lt;/script&gt; matrix between &lt;code class=&quot;highlighter-rouge&quot;&gt;QuestionAttnGRU&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;QuestionPooling&lt;/code&gt; layers,&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;W^P_{v}&lt;/script&gt; matrix between &lt;code class=&quot;highlighter-rouge&quot;&gt;QuestionAttnGRU&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;SelfAttnGRU&lt;/code&gt; layers,&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; vector between all four instances (it is used right before applying softmax).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We didn’t share the weights of the “attention gates”: &lt;script type=&quot;math/tex&quot;&gt;W_{g}&lt;/script&gt;. The reason is that we have a mix of uni- and bidirectional GRUs that use this gate and require different dimensions.&lt;/p&gt;

&lt;h4 id=&quot;hyperparameters&quot;&gt;Hyperparameters&lt;/h4&gt;

&lt;p&gt;The authors of the report tell many details about hyperparameters. Hidden vector lengths are 75 for all layers. As we concatenate the hidden states of two GRUs in bidirectional, we effectively get 150 dimensional vectors. 75 is not an even number so it could not refer to the length of the concatenated vector :) &lt;a href=&quot;http://ruder.io/optimizing-gradient-descent/index.html#adadelta&quot;&gt;AdaDelta optimizer&lt;/a&gt; is used to train the network with learning rate=1, &lt;script type=&quot;math/tex&quot;&gt;\rho=0.95&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\varepsilon=1e^{-6}&lt;/script&gt;. Nothing is written about the size of batches, or the way batches are sampled. We used &lt;code class=&quot;highlighter-rouge&quot;&gt;batch_size=50&lt;/code&gt; in our experiments to fit in 4GB GPU memory.&lt;/p&gt;

&lt;p&gt;We couldn’t get good performance with &lt;code class=&quot;highlighter-rouge&quot;&gt;75&lt;/code&gt; hidden units. The models were quickly overfitting. We got our best results using &lt;code class=&quot;highlighter-rouge&quot;&gt;45&lt;/code&gt; dimensional hidden states.&lt;/p&gt;

&lt;h4 id=&quot;weight-initialization&quot;&gt;Weight initialization&lt;/h4&gt;

&lt;p&gt;The report doesn’t discuss weight initialization. We used default initialization schemes of Keras. In particular, Keras uses orthogonal initialization for recurrent connections of GRU, and uniform (&lt;a href=&quot;http://www.jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf&quot;&gt;Glorot, Bengio, 2010&lt;/a&gt;) initialization for the connections that come from the inputs. We used Glorot initialization for &lt;a href=&quot;https://github.com/YerevaNN/R-NET-in-Keras/blob/master/layers/SharedWeight.py#L12&quot;&gt;all shared weights&lt;/a&gt;. It is not obvious that this was the best solution.&lt;/p&gt;

&lt;h4 id=&quot;training&quot;&gt;Training&lt;/h4&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/YerevaNN/R-NET-in-Keras/blob/master/train.py&quot;&gt;training script&lt;/a&gt; is very simple. First we create the model:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hdim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hdim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;                                            &lt;span class=&quot;c&quot;&gt;# Defauls is 45&lt;/span&gt;
             &lt;span class=&quot;n&quot;&gt;dropout_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;                                 &lt;span class=&quot;c&quot;&gt;# Default is 0 (0.2 in the report)&lt;/span&gt;
             &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;                                                    &lt;span class=&quot;c&quot;&gt;# Size of passage&lt;/span&gt;
             &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;                                                    &lt;span class=&quot;c&quot;&gt;# Size of question&lt;/span&gt;
             &lt;span class=&quot;n&quot;&gt;char_level_embeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;char_level_embeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;          &lt;span class=&quot;c&quot;&gt;# Default is false&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It is possible to slightly speed up computations by fixing &lt;code class=&quot;highlighter-rouge&quot;&gt;M&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;N&lt;/code&gt;. It usually helps Theano’s compiler to further optimize the computational graph.&lt;/p&gt;

&lt;p&gt;We compile the model and fit it on the training set. Our training data is 90% of the original training set of SQuAD dataset. The other 10% is used as an internal validation dataset. We check the validation score after each epoch and save the current state of the model if it was better than the previous best one. The original &lt;em&gt;development set&lt;/em&gt; of SQuAD is used as a test set, we don’t do model selection based on that.&lt;/p&gt;

&lt;p&gt;We had an idea to form the batches in a way that passages inside each batch have almost the same number of words. That would allow to train a little bit faster (as there would be many batches with short sequences), but we didn’t use this trick yet. We took maximum 300 words from passages and 30 words from questions to avoid very long sequences.&lt;/p&gt;

&lt;p&gt;Each epoch took around 100 minutes on a GTX980 GPU. We got our best results after 31 epochs.&lt;/p&gt;

&lt;h2 id=&quot;results-and-comparison-with-r-net-technical-report&quot;&gt;Results and comparison with &lt;a href=&quot;https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf&quot;&gt;R-NET technical report&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;R-NET is currently (August 2017) the &lt;a href=&quot;https://rajpurkar.github.io/SQuAD-explorer/&quot;&gt;best model on Stanford QA&lt;/a&gt; benchmark among single models. SQuAD dataset uses two performance metrics: exact match (EM) and F1-score (F1). Human performance is estimated to be EM=82.3% and F1=91.2% on the test set.&lt;/p&gt;

&lt;p&gt;The report by Microsoft Research describes two versions of R-NET. The first one is called &lt;em&gt;R-NET (Wang et al., 2017)&lt;/em&gt; (which refers to a paper which is not yet available online) and reaches EM=71.3% and F1=79.7% on the test set. It is the model we described above without the additional biGRU between SelfAttnGRU and PointerGRU. The second version called &lt;em&gt;R-NET (March 2017)&lt;/em&gt; has the additional BiGRU and reaches EM=72.3% and F1=80.7%. The current best single model on SQuAD leaderboard has a higher score, which means R-NET development continued since the technical report was released. Ensemble models reach even higher scores.&lt;/p&gt;

&lt;p&gt;The best performance we got so far with our implementation is EM=57.52% and F1=67.42% on the development set. These results would put R-NET at the bottom of the SQuAD leaderboard. The model is available on &lt;a href=&quot;https://github.com/YerevaNN/R-NET-in-Keras&quot;&gt;GitHub&lt;/a&gt;. We want to emphasize that R-NET’s technical report is pretty good in terms of the reported details of the architecture compared to many other papers. Probably we misunderstood several important details or have bugs in the code. Any feedback will be appreciated.&lt;/p&gt;

&lt;h2 id=&quot;challenges-of-reproducibility&quot;&gt;Challenges of reproducibility&lt;/h2&gt;

&lt;p&gt;Recently, ICML 2017 hosted a special &lt;a href=&quot;https://sites.google.com/view/icml-reproducibility-workshop/home&quot;&gt;workshop&lt;/a&gt; devoted to the issues of reproducibility in machine learning. Hugo Larochelle shared the &lt;a href=&quot;https://drive.google.com/file/d/0B8lLzpxgRHNQZ0paZWQ0cTcxMlNYYnc0TnpHekMxMjVBckVR/view&quot;&gt;slides of his presentation&lt;/a&gt;, where he discussed many aspects of the problem. He argues that the research should be considered as reproducible if the code is open-sourced. On the other hand he suggests that the community should not require researchers to compare their new models with a related published result if the code for the latter is not available.&lt;/p&gt;

&lt;p&gt;As a radical solution he suggests to use platforms like &lt;a href=&quot;http://ai-on.org/&quot;&gt;AI-ON&lt;/a&gt;. AI-ON is open-sourcing not only the code, but the whole research process, including discussions and code experiments. We think about starting AI-ON projects just for reproducing the results of important papers that come without code.&lt;/p&gt;

&lt;p&gt;On the other hand, there are many simple tricks that can significantly improve reproducibility with little effort. For example, many papers report the number of parameters in the neural network. This number is a good checksum for other people. Another simple trick is to write the shapes of the tensors in the diagrams (just like we did in this post) or even in the text.&lt;/p&gt;

&lt;p&gt;The best open source model on SQuAD that we are aware of is the implementation of &lt;a href=&quot;https://arxiv.org/abs/1704.00051&quot;&gt;DrQA architecture&lt;/a&gt; released in Facebook’s &lt;a href=&quot;https://github.com/facebookresearch/ParlAI&quot;&gt;ParlAI repository&lt;/a&gt;. It &lt;a href=&quot;https://github.com/facebookresearch/ParlAI/issues/109&quot;&gt;reaches&lt;/a&gt; EM=66.4% and F1=76.5%. We will continue to play with our codebase and try to improve the results.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Interpreting neurons in an LSTM network</title>
   <link href="http://yerevann.github.io//2017/06/27/interpreting-neurons-in-an-LSTM-network/"/>
   <updated>2017-06-27T00:00:00+00:00</updated>
   <id>http://yerevann.github.io//2017/06/27/interpreting-neurons-in-an-LSTM-network</id>
   <content type="html">&lt;p&gt;By &lt;a href=&quot;https://github.com/TigranGalstyan&quot;&gt;Tigran Galstyan&lt;/a&gt; and
   &lt;a href=&quot;https://github.com/Hrant-Khachatrian&quot;&gt;Hrant Khachatrian&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A few months ago, we showed how effectively an LSTM network can perform text &lt;a href=&quot;http://yerevann.github.io/2016/09/09/automatic-transliteration-with-lstm/&quot;&gt;
transliteration&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For humans, transliteration is a relatively easy and interpretable task, so it’s a good task for interpreting what the network is doing, and whether it is similar to how humans approach the same task.&lt;/p&gt;

&lt;p&gt;In this post we’ll try to understand: What do individual neurons of the network actually learn?  How are they used to make decisions?&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 class=&quot;no_toc&quot; id=&quot;contents&quot;&gt;Contents&lt;/h2&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#transliteration&quot; id=&quot;markdown-toc-transliteration&quot;&gt;Transliteration&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#network-architecture&quot; id=&quot;markdown-toc-network-architecture&quot;&gt;Network architecture&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#analyzing-the-neurons&quot; id=&quot;markdown-toc-analyzing-the-neurons&quot;&gt;Analyzing the neurons&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#how-does-t-become-ծ&quot; id=&quot;markdown-toc-how-does-t-become-ծ&quot;&gt;How does “t” become “ծ”?&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#what-did-this-neuron-learn&quot; id=&quot;markdown-toc-what-did-this-neuron-learn&quot;&gt;What did this neuron learn?&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#visualizing-lstm-cells&quot; id=&quot;markdown-toc-visualizing-lstm-cells&quot;&gt;Visualizing LSTM cells&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#concluding-remarks&quot; id=&quot;markdown-toc-concluding-remarks&quot;&gt;Concluding remarks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;transliteration&quot;&gt;Transliteration&lt;/h2&gt;

&lt;p&gt;About half of the billions of internet users speak languages written in non-Latin alphabets, like Russian, Arabic, Chinese, Greek and Armenian.  Very often, they haphazardly use the Latin alphabet to write those languages.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Привет&lt;/code&gt;: &lt;code class=&quot;highlighter-rouge&quot;&gt;Privet&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;Privyet&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;Priwjet&lt;/code&gt;, …&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;كيف حالك&lt;/code&gt;: &lt;code class=&quot;highlighter-rouge&quot;&gt;kayf halk&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;keyf 7alek&lt;/code&gt;, …&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;Բարև Ձեզ&lt;/code&gt;: &lt;code class=&quot;highlighter-rouge&quot;&gt;Barev Dzez&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;Barew Dzez&lt;/code&gt;, …&lt;/p&gt;

&lt;p&gt;So a growing share of user-generated text content is in these “Latinized” or “romanized” formats that are difficult to parse, search or even identify.  Transliteration is the task of automatically converting this content into the native canonical format.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Aydpes aveli sirun e.&lt;/code&gt;: &lt;code class=&quot;highlighter-rouge&quot;&gt;Այդպես ավելի սիրուն է:&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;What makes this problem non-trivial?&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Different users romanize in different ways, as we saw above. 
For example, &lt;code class=&quot;highlighter-rouge&quot;&gt;v&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;w&lt;/code&gt; could be Armenian &lt;code class=&quot;highlighter-rouge&quot;&gt;վ&lt;/code&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Multiple letters can be romanized to the same Latin letter.
For example, &lt;code class=&quot;highlighter-rouge&quot;&gt;r&lt;/code&gt; could be Armenian &lt;code class=&quot;highlighter-rouge&quot;&gt;ր&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;ռ&lt;/code&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A single letter can be romanized to a combination of multiple Latin letters.
For example, &lt;code class=&quot;highlighter-rouge&quot;&gt;ch&lt;/code&gt; could be Cyrillic &lt;code class=&quot;highlighter-rouge&quot;&gt;ч&lt;/code&gt; or Armenian &lt;code class=&quot;highlighter-rouge&quot;&gt;չ&lt;/code&gt;, but &lt;code class=&quot;highlighter-rouge&quot;&gt;c&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;h&lt;/code&gt; by themselves are for other letters.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;English words and translingual Latin tokens like URLs occur in non-Latin text.
For example, the letters in &lt;code class=&quot;highlighter-rouge&quot;&gt;youtube.com&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;MSFT&lt;/code&gt; should not be changed.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Humans are great at resolving these ambiguities.  We showed that LSTMs can also learn to resolve all these ambiguities, at least for Armenian. For example, our model correctly transliterated &lt;code class=&quot;highlighter-rouge&quot;&gt;es sirum em Deep Learning&lt;/code&gt; into &lt;code class=&quot;highlighter-rouge&quot;&gt;ես սիրում եմ Deep Learning&lt;/code&gt; and not &lt;code class=&quot;highlighter-rouge&quot;&gt;ես սիրում եմ Դեեփ Լէարնինգ&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;network-architecture&quot;&gt;Network architecture&lt;/h2&gt;

&lt;p&gt;We took lots of Armenian text from Wikipedia and used &lt;a href=&quot;https://github.com/YerevaNN/translit-rnn/blob/master/languages/hy-AM/transliteration.json&quot;&gt;probabilistic rules&lt;/a&gt; to obtain romanized text. The rules are chosen in a way that they cover most of the romanization rules people use for Armenian.&lt;/p&gt;

&lt;p&gt;We encode Latin characters as one-hot vectors and apply character level bidirectional LSTM. At each time-step the network tries to guess the next character of the original Armenian sentence. Sometimes a single Armenian character is represented by multiple Latin letters, so it is very helpful to align the romanized and original texts before giving them to LSTM (otherwise we should use sequence-to-sequence networks, which are harder to train). Fortunately we can do the alignment, because the romanized version was generated by ourselves. For example, &lt;code class=&quot;highlighter-rouge&quot;&gt;dzi&lt;/code&gt; should be transliterated into &lt;code class=&quot;highlighter-rouge&quot;&gt;ձի&lt;/code&gt;, where &lt;code class=&quot;highlighter-rouge&quot;&gt;dz&lt;/code&gt; corresponds to &lt;code class=&quot;highlighter-rouge&quot;&gt;ձ&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;i&lt;/code&gt; to &lt;code class=&quot;highlighter-rouge&quot;&gt;ի&lt;/code&gt;. So we add a placeholder character in the Armenian version: &lt;code class=&quot;highlighter-rouge&quot;&gt;ձի&lt;/code&gt; becomes &lt;code class=&quot;highlighter-rouge&quot;&gt;ձ_ի&lt;/code&gt;, so that now &lt;code class=&quot;highlighter-rouge&quot;&gt;z&lt;/code&gt; should be transliterated into &lt;code class=&quot;highlighter-rouge&quot;&gt;_&lt;/code&gt;. After the inference we just remove &lt;code class=&quot;highlighter-rouge&quot;&gt;_&lt;/code&gt;s from the output string.&lt;/p&gt;

&lt;p&gt;Our network consists of two LSTMs (228 cells) going forward and backward on the Latin sequence. The outputs of the LSTMs are concatenated at each step (&lt;em&gt;concat layer&lt;/em&gt;), then a dense layer with 228 neurons is applied on top of it (&lt;em&gt;hidden layer&lt;/em&gt;), and another dense layer (&lt;em&gt;output layer&lt;/em&gt;) with softmax activations is used to get the output probabilities. We also concatenate the input vector to the hidden layer, so it has 300 neurons. This is a more simplified version of the network described in our &lt;a href=&quot;http://yerevann.github.io/2016/09/09/automatic-transliteration-with-lstm/#network-architecture&quot;&gt;previous post&lt;/a&gt; on this topic (the main difference is that we don’t use the second layer of biLSTM).&lt;/p&gt;

&lt;h2 id=&quot;analyzing-the-neurons&quot;&gt;Analyzing the neurons&lt;/h2&gt;

&lt;p&gt;We tried to answer the following questions:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;How does the network handle interesting cases with several possible outcomes (e.g. &lt;code class=&quot;highlighter-rouge&quot;&gt;r&lt;/code&gt; =&amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;ր&lt;/code&gt; vs &lt;code class=&quot;highlighter-rouge&quot;&gt;ռ&lt;/code&gt; etc.)?&lt;/li&gt;
  &lt;li&gt;What are the problems particular neurons are helping solve?&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;how-does-t-become-ծ&quot;&gt;How does “t” become “ծ”?&lt;/h3&gt;

&lt;p&gt;First, we fixed one particular character for the input and one for the output.
For example we are interested in how &lt;code class=&quot;highlighter-rouge&quot;&gt;t&lt;/code&gt; becomes &lt;code class=&quot;highlighter-rouge&quot;&gt;ծ&lt;/code&gt; (we know &lt;code class=&quot;highlighter-rouge&quot;&gt;t&lt;/code&gt; can become &lt;code class=&quot;highlighter-rouge&quot;&gt;տ&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;թ&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;ծ&lt;/code&gt;). We now that it usually happens when &lt;code class=&quot;highlighter-rouge&quot;&gt;t&lt;/code&gt; appears in a bigram &lt;code class=&quot;highlighter-rouge&quot;&gt;ts&lt;/code&gt;, which should be converted to &lt;code class=&quot;highlighter-rouge&quot;&gt;ծ_&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;For every neuron, we draw the histograms of its activations in cases where the correct output is &lt;code class=&quot;highlighter-rouge&quot;&gt;ծ&lt;/code&gt;, and where the correct output is &lt;em&gt;not&lt;/em&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;ծ&lt;/code&gt;. For most of the neurons these two histograms are pretty similar, but there are cases like this:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Input = &lt;code class=&quot;highlighter-rouge&quot;&gt;t&lt;/code&gt;, Output = &lt;code class=&quot;highlighter-rouge&quot;&gt;ծ&lt;/code&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Input = &lt;code class=&quot;highlighter-rouge&quot;&gt;t&lt;/code&gt;,  Output != &lt;code class=&quot;highlighter-rouge&quot;&gt;ծ&lt;/code&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;http://yerevann.github.io/public/2017-06-27/ts.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;http://yerevann.github.io/public/2017-06-27/chts.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;These histograms show that by looking at the activation of this particular neuron we can guess with high accuracy whether the output for &lt;code class=&quot;highlighter-rouge&quot;&gt;t&lt;/code&gt; is &lt;code class=&quot;highlighter-rouge&quot;&gt;ծ&lt;/code&gt;. To quantify the difference between the two histograms we used &lt;a href=&quot;https://en.wikipedia.org/wiki/Hellinger_distance&quot;&gt;Hellinger distance&lt;/a&gt; (we take the minimum and maximum values of neuron activations, split the range into 1000 bins and apply discrete Hellinger distance formula on two histograms). We calculated this distance for all neurons and visualized the most interesting ones in a single image:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://yerevann.github.io/public/2017-06-27/t-ծ.png&quot; alt=&quot;t=&amp;gt;ծ&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The color of a neuron indicates the distance between its two histograms (darker colors correspond to larger distances). The width of a line between two neurons indicate the mean of the value that the neuron on the lower end of the connection contributes to the neuron on the higher end. Orange and green lines correspond to positive and negative signals, respectively.&lt;/p&gt;

&lt;p&gt;The neurons at the top of the image are from the output layer, the neurons below the output layer are from the hidden layer (top 12 neurons in terms of the distance between histograms). Concat layer comes under the hidden layer. The neurons of the concat layer are split into two parts: the left half of the neurons are the outputs of the LSTM that goes forward on the input sequence and the right half contains the neurons from the LSTM that goes backwards. From each LSTM we display top 10 neurons in terms of the distance between histograms.&lt;/p&gt;

&lt;p&gt;In the case of &lt;code class=&quot;highlighter-rouge&quot;&gt;t&lt;/code&gt; =&amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;ծ&lt;/code&gt;, it is obvious that all top 12 neurons of the hidden layer pass positive signals to &lt;code class=&quot;highlighter-rouge&quot;&gt;ծ&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;ց&lt;/code&gt; (another Armenian character that is often romanized as &lt;code class=&quot;highlighter-rouge&quot;&gt;ts&lt;/code&gt;), and pass negative signals to &lt;code class=&quot;highlighter-rouge&quot;&gt;տ&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;թ&lt;/code&gt; and others.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://yerevann.github.io/public/2017-06-27/t-ծ-concat.png&quot; alt=&quot;t=&amp;gt;ծ - concat layer&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can also see that the outputs of the right-to-left LSTM are darker, which implies that these neurons “have more knowledge” about whether to predict &lt;code class=&quot;highlighter-rouge&quot;&gt;ծ&lt;/code&gt;. On the other hand, the lines between those neurons and the hidden layer are thicker, which means that they have more contribution in activating the top 12 neurons in the hidden layer. This is a very natural result, because we know that &lt;code class=&quot;highlighter-rouge&quot;&gt;t&lt;/code&gt; usually becomes &lt;code class=&quot;highlighter-rouge&quot;&gt;ծ&lt;/code&gt; when the &lt;em&gt;next&lt;/em&gt; symbol is &lt;code class=&quot;highlighter-rouge&quot;&gt;s&lt;/code&gt;, and only the right-to-left LSTM is aware of the next character.&lt;/p&gt;

&lt;p&gt;We did the same analysis for the neurons and gates inside the LSTMs. The results are visualized as six rows of neurons at the bottom of the image. In particular, it is interesting to note that the most “confident” neurons are the so called &lt;em&gt;cell inputs&lt;/em&gt;. Recall that cell inputs, as well as all the gates, depend on the input at the current step and the hidden state of the previous step (which is the hidden state at the &lt;em&gt;next&lt;/em&gt; character as we talk about the right-to-left LSTM), so all of them are “aware” of the next &lt;code class=&quot;highlighter-rouge&quot;&gt;s&lt;/code&gt;, but for some reason cell inputs are more confident than others.&lt;/p&gt;

&lt;p&gt;In the cases where &lt;code class=&quot;highlighter-rouge&quot;&gt;s&lt;/code&gt; should be transliterated into &lt;code class=&quot;highlighter-rouge&quot;&gt;_&lt;/code&gt; (the placeholder), the useful information is more likely to come from the LSTM that goes forward, as &lt;code class=&quot;highlighter-rouge&quot;&gt;s&lt;/code&gt; becomes &lt;code class=&quot;highlighter-rouge&quot;&gt;_&lt;/code&gt; mainly in case of &lt;code class=&quot;highlighter-rouge&quot;&gt;ts&lt;/code&gt; =&amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;ծ_&lt;/code&gt;. We see that in the next plot:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://yerevann.github.io/public/2017-06-27/s-_.png&quot; alt=&quot;s=&amp;gt;placeholder&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;what-did-this-neuron-learn&quot;&gt;What did this neuron learn?&lt;/h3&gt;

&lt;p&gt;In the second part of our analysis we tried to figure out in which ambiguous cases each of the neurons is most helpful. We took the set of Latin characters that can be transliterated into more than one Armenian letters. Then we removed the cases where one of the possible outcomes appears less than 300 times in our 5000 sample sentences, because our distance metric didn’t seem to work well with few samples. And we analyzed every fixed neuron for every possible input-output pair.&lt;/p&gt;

&lt;p&gt;For example, here is the analysis of the neuron #70 of the output layer of the left-to-right LSTM. We have seen in the previous visualization that it helps determining whether &lt;code class=&quot;highlighter-rouge&quot;&gt;s&lt;/code&gt; should be transliterated into &lt;code class=&quot;highlighter-rouge&quot;&gt;_&lt;/code&gt;. We see that the top input-output pairs for this neuron are the following:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Hellinger distance&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Latin character&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Armenian character&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.9482&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;s&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;_&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.8285&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;h&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;հ&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.8091&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;h&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;_&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.6125&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;o&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;օ&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;So this neuron is most helpful when predicting &lt;code class=&quot;highlighter-rouge&quot;&gt;_&lt;/code&gt; from &lt;code class=&quot;highlighter-rouge&quot;&gt;s&lt;/code&gt; (as we already knew), but it also helps to determine whether Latin &lt;code class=&quot;highlighter-rouge&quot;&gt;h&lt;/code&gt; should be transliterated as Armenian &lt;code class=&quot;highlighter-rouge&quot;&gt;հ&lt;/code&gt; or the placeholder &lt;code class=&quot;highlighter-rouge&quot;&gt;_&lt;/code&gt; (e.g. Armenian &lt;code class=&quot;highlighter-rouge&quot;&gt;չ&lt;/code&gt; is usually romanized as &lt;code class=&quot;highlighter-rouge&quot;&gt;ch&lt;/code&gt;, so &lt;code class=&quot;highlighter-rouge&quot;&gt;h&lt;/code&gt; sometimes becomes &lt;code class=&quot;highlighter-rouge&quot;&gt;_&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;We visualize Hellinger distances of the histograms of neuron activations when the input is &lt;code class=&quot;highlighter-rouge&quot;&gt;h&lt;/code&gt; and the output is &lt;code class=&quot;highlighter-rouge&quot;&gt;_&lt;/code&gt;, and see that the neuron #70 is among the top 10 neurons of the left-to-right LSTM for the &lt;code class=&quot;highlighter-rouge&quot;&gt;h&lt;/code&gt;=&amp;gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;_&lt;/code&gt; pair.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://yerevann.github.io/public/2017-06-27/h-_.png&quot; alt=&quot;h=&amp;gt;placeholder&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;visualizing-lstm-cells&quot;&gt;Visualizing LSTM cells&lt;/h2&gt;

&lt;p&gt;Inspired by &lt;a href=&quot;https://arxiv.org/abs/1506.02078&quot;&gt;this paper&lt;/a&gt; by Andrej Karpathy, Justin Johnson and Fei-Fei Li, we tried to find neurons or LSTM cells specialised in some language specific patterns in the sequences. In particular, we tried to find the neurons that react most to the suffix &lt;code class=&quot;highlighter-rouge&quot;&gt;թյուն&lt;/code&gt; (romanized as &lt;code class=&quot;highlighter-rouge&quot;&gt;tyun&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://yerevann.github.io/public/2017-06-27/utyun1.png&quot; alt=&quot;tyun&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The first row of this visualization is the output sequence. Rows below show the activations of the most interesting neurons:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Cell #6 in the LSTM that goes backwards,&lt;/li&gt;
  &lt;li&gt;Cell #147 in the LSTM that goes forward,&lt;/li&gt;
  &lt;li&gt;37th neuron in the hidden layer,&lt;/li&gt;
  &lt;li&gt;78th neuron in the concat layer.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;http://yerevann.github.io/public/2017-06-27/utyun2.png&quot; alt=&quot;tyun&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can see that Cell #6 is active on &lt;code class=&quot;highlighter-rouge&quot;&gt;tyun&lt;/code&gt;s and is not active on the other parts of the sequence. Cell #144 of the forward LSTM behaves the opposite way, it is active on everything except &lt;code class=&quot;highlighter-rouge&quot;&gt;tyun&lt;/code&gt;s.&lt;/p&gt;

&lt;p&gt;We know that &lt;code class=&quot;highlighter-rouge&quot;&gt;t&lt;/code&gt; in the suffix &lt;code class=&quot;highlighter-rouge&quot;&gt;tyun&lt;/code&gt; should always become &lt;code class=&quot;highlighter-rouge&quot;&gt;թ&lt;/code&gt; in Armenian, so we thought that if a neuron is active on &lt;code class=&quot;highlighter-rouge&quot;&gt;tyun&lt;/code&gt;s, it may help in determining whether the Latin &lt;code class=&quot;highlighter-rouge&quot;&gt;t&lt;/code&gt; should be transliterated as &lt;code class=&quot;highlighter-rouge&quot;&gt;թ&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;տ&lt;/code&gt;. So we visualized the most important neurons for the pair &lt;code class=&quot;highlighter-rouge&quot;&gt;t&lt;/code&gt; =&amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;թ&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://yerevann.github.io/public/2017-06-27/t-թ.png&quot; alt=&quot;t-&amp;gt;թ&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Indeed, Cell #147 in the forward LSTM is among the top 10.&lt;/p&gt;

&lt;h2 id=&quot;concluding-remarks&quot;&gt;Concluding remarks&lt;/h2&gt;

&lt;p&gt;Interpretability of neural networks remains an important challenge in machine learning. CNNs and LSTMs perform well for many learning tasks, but there are very few tools to understand the inner workings of these systems. Transliteration is a pretty good problem for analyzing the impact of particular neurons.&lt;/p&gt;

&lt;p&gt;Our experiments showed that too many neurons are involved in the “decision making” even for the simplest cases, but it is possible to identify a subset of neurons that have more influence than the rest. On the other hand, most neurons are involved in multiple decision making processes depending on the context. This is expected, since nothing in the loss functions we use when training neural nets forces the neurons to be independent and interpretable. Recently, there have been &lt;a href=&quot;https://arxiv.org/abs/1606.03657&quot;&gt;some attempts&lt;/a&gt; to apply information-theoretic regularization terms in order to obtain more interpretability. It would be interesting to test those ideas in the context of transliteration.&lt;/p&gt;

&lt;p&gt;We would like to thank Adam Mathias Bittlingmayer and Zara Alaverdyan for  helpful comments and discussions.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Announcing YerevaNN non-profit foundation</title>
   <link href="http://yerevann.github.io//2016/10/17/announcing-yerevann-non-profit-foundation/"/>
   <updated>2016-10-17T00:00:00+00:00</updated>
   <id>http://yerevann.github.io//2016/10/17/announcing-yerevann-non-profit-foundation</id>
   <content type="html">&lt;p&gt;Today we have officially registered YerevaNN scientific educational foundation, which aims to promote world-class AI research in Armenia and develop high quality educational programs in machine learning and related disciplines. The board members of the foundation are Gor Vardanyan, founder of FimeTech, Vazgen Hakobjanyan, cofounder of Teamable, and Rouben Meschian, founder of Arminova Technologies. Hrant Khachatrian is the director of the foundation.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;&lt;img src=&quot;http://yerevann.github.io/public/2016-10-17/cover.jpg&quot; alt=&quot;YerevaNN&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The core project of the foundation is to support an AI research lab based in Yerevan, Armenia. Inspired by &lt;a href=&quot;https://openai.com/about/&quot;&gt;OpenAI&lt;/a&gt;, the lab focuses on non-commercial machine learning research and is committed to publish all obtained results and release all the code on GitHub. The three initial members of YerevaNN lab, Tigran Galstyan, Karen Hambardzumyan and Hrayr Harutyunyan, currently work on projects ranging from generative models to natural language processing.&lt;/p&gt;

&lt;p&gt;Follow us on our &lt;a href=&quot;http://yerevann.github.io/&quot;&gt;blog&lt;/a&gt;, on &lt;a href=&quot;https://www.facebook.com/YerevaNNlab/&quot;&gt;Facebook&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/YerevaNN&quot;&gt;Twitter&lt;/a&gt; and &lt;a href=&quot;https://plus.google.com/110195306327238545309&quot;&gt;Google Plus&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Sentence representations and question answering (slides)</title>
   <link href="http://yerevann.github.io//2016/09/21/presentation-sentence-representations-and-question-answering/"/>
   <updated>2016-09-21T00:00:00+00:00</updated>
   <id>http://yerevann.github.io//2016/09/21/presentation-sentence-representations-and-question-answering</id>
   <content type="html">&lt;p&gt;The success of neural word embedding models like &lt;a href=&quot;https://en.wikipedia.org/wiki/Word2vec&quot;&gt;word2vec&lt;/a&gt; and &lt;a href=&quot;http://nlp.stanford.edu/projects/glove/&quot;&gt;GloVe&lt;/a&gt; motivated research on representing sentences in an n-dimensional space. &lt;a href=&quot;https://github.com/mike1808&quot;&gt;Michael Manukyan&lt;/a&gt; and &lt;a href=&quot;https://github.com/Harhro94&quot;&gt;Hrayr Harutyunyan&lt;/a&gt; reviewed several sentence representation algorithms and their applications in state-of-the-art &lt;a href=&quot;http://arxiv.org/abs/1608.07905&quot;&gt;automated question answering&lt;/a&gt; systems during a talk at the Armenian NLP meetup. The slides of the talk are below. Follow us on &lt;a href=&quot;https://www.slideshare.net/YerevaNN/&quot;&gt;SlideShare&lt;/a&gt; to get the latest slides from YerevaNN.&lt;/p&gt;

&lt;!--more--&gt;

&lt;iframe src=&quot;//www.slideshare.net/slideshow/embed_code/key/NkfvTBRSIjKEW0&quot; width=&quot;595&quot; height=&quot;485&quot; frameborder=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; scrolling=&quot;no&quot; style=&quot;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&quot; allowfullscreen=&quot;&quot;&gt; &lt;/iframe&gt;
&lt;div style=&quot;margin-bottom:5px&quot;&gt; &lt;strong&gt; &lt;a href=&quot;//www.slideshare.net/YerevaNN/sentence-representations-and-question-answering-yerevann&quot; title=&quot;Sentence representations and question answering (YerevaNN)&quot; target=&quot;_blank&quot;&gt;Sentence representations and question answering (YerevaNN)&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href=&quot;//www.slideshare.net/YerevaNN&quot; target=&quot;_blank&quot;&gt;YerevaNN&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Automatic transliteration with LSTM</title>
   <link href="http://yerevann.github.io//2016/09/09/automatic-transliteration-with-lstm/"/>
   <updated>2016-09-09T00:00:00+00:00</updated>
   <id>http://yerevann.github.io//2016/09/09/automatic-transliteration-with-lstm</id>
   <content type="html">&lt;p&gt;By &lt;a href=&quot;https://github.com/TigranGalstyan&quot;&gt;Tigran Galstyan&lt;/a&gt;, &lt;a href=&quot;https://github.com/Harhro94&quot;&gt;Hrayr Harutyunyan&lt;/a&gt; and &lt;a href=&quot;https://github.com/Hrant-Khachatrian&quot;&gt;Hrant Khachatrian&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Many languages have their own non-Latin alphabets but the web is full of content in those languages written in Latin letters, which makes it inaccessible to various NLP tools (e.g. automatic translation). Transliteration is the process of converting the romanized text back to the original writing system. In theory every language has a strict set of romanization rules, but in practice people do not follow the rules and most of the romanized content is hard to transliterate using rule based algorithms. We believe this problem is solvable using the state of the art NLP tools, and we demonstrate a high quality solution for Armenian based on recurrent neural networks. We invite everyone to &lt;a href=&quot;https://github.com/YerevaNN/translit-rnn&quot;&gt;adapt our system&lt;/a&gt; for more languages.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 class=&quot;no_toc&quot; id=&quot;contents&quot;&gt;Contents&lt;/h2&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#problem-description&quot; id=&quot;markdown-toc-problem-description&quot;&gt;Problem description&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#data-processing&quot; id=&quot;markdown-toc-data-processing&quot;&gt;Data processing&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#source-of-the-data&quot; id=&quot;markdown-toc-source-of-the-data&quot;&gt;Source of the data&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#romanization-rules&quot; id=&quot;markdown-toc-romanization-rules&quot;&gt;Romanization rules&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#geographic-dependency&quot; id=&quot;markdown-toc-geographic-dependency&quot;&gt;Geographic dependency&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#filtering-out-large-non-armenian-chunks&quot; id=&quot;markdown-toc-filtering-out-large-non-armenian-chunks&quot;&gt;Filtering out large non-Armenian chunks&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#network-architecture&quot; id=&quot;markdown-toc-network-architecture&quot;&gt;Network architecture&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#encoding-the-characters&quot; id=&quot;markdown-toc-encoding-the-characters&quot;&gt;Encoding the characters&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#aligning&quot; id=&quot;markdown-toc-aligning&quot;&gt;Aligning&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#bidirectional-lstm-with-residual-like-connections&quot; id=&quot;markdown-toc-bidirectional-lstm-with-residual-like-connections&quot;&gt;Bidirectional LSTM with residual-like connections&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#results&quot; id=&quot;markdown-toc-results&quot;&gt;Results&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#future-work&quot; id=&quot;markdown-toc-future-work&quot;&gt;Future work&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;problem-description&quot;&gt;Problem description&lt;/h2&gt;

&lt;p&gt;Since early 1990s computers became widespread in many countries, but the  operating systems did not fully support different alphabets out of the box. Most keyboards had only latin letters printed on them, and people started to invent romanization rules for their languages. Every language has its own story, and these stories are usually not known outside their own communities. In case of Armenian, &lt;a href=&quot;https://en.wikipedia.org/wiki/ArmSCII&quot;&gt;some solutions&lt;/a&gt; have been developed, but even those who knew how to write in Armenian characters, were not sure that the readers (r.g. the recipient of the email) would be able to read that.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;img src=&quot;http://yerevann.github.io/public/2016-09-09/armenian-unicode.jpg&quot; alt=&quot;Armenian alphabet in the Unicode space. Source: Wikipedia&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Armenian alphabet in the Unicode space. Source: &lt;a href=&quot;https://en.wikipedia.org/wiki/Armenian_alphabet#Character_encodings&quot;&gt;Wikipedia&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;In the Unicode era all major OSes started to support displaying &lt;a href=&quot;https://en.wikipedia.org/wiki/Armenian_alphabet&quot;&gt;Armenian characters&lt;/a&gt;. But the lack of keyboard layouts was still a problem. In late 2000s mobile internet penetration exploded in Armenia, and most of the early mobile phones did not support writing in Armenian. For example, iOS doesn’t include Armenian keyboard and started to officially support custom keyboards &lt;a href=&quot;http://www.theverge.com/2014/6/2/5773504/developers-already-at-work-on-alternate-ios-8-keyboards/in/6116530&quot;&gt;only in 2014&lt;/a&gt;! The result was that lots of people entered the web (mostly through social networks) without having access to Armenian letters. So everyone started to use some sort of romanization (obviously no one was aware that there are fixed standards for the &lt;a href=&quot;https://en.wikipedia.org/wiki/Romanization_of_Armenian&quot;&gt;romanization of Armenian&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Currently there are many attempts to fight romanized Armenian on forums and social networks. Armenian keyboard layouts are developed for every popular platform. But still lots of content is produced in non-Armenian letters (maybe only Facebook knows the exact scale of the problem), and such content remains inaccessible for search indexing, automated translation, text-to-speech, etc. Recently the problem started to flow outside the web, people use romanized Armenian on the streets.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;img src=&quot;http://yerevann.github.io/public/2016-09-09/translit-in-the-wild.jpg&quot; alt=&quot;Romanized Armenian on the street. Source: VKontakte social network&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Romanized Armenian on the street. Source: VKontakte social network&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;There are some online tools that correctly transliterate romanized Armenian if its written using strict rules. &lt;a href=&quot;https://hayeren.am/?p=convertor&quot;&gt;Hayeren.am&lt;/a&gt; is the most famous example. Facebook’s search box also recognizes some romanizations (but not all). But for many practical cases these tools do not give a reasonable output. The algorithm must be able to use the context to correctly predict the Armenian character.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;img src=&quot;http://yerevann.github.io/public/2016-09-09/facebook-translit.jpg&quot; alt=&quot;Facebook's search box recognizes some romanized Armenian&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Facebook’s search box recognizes some romanized Armenian. Note that the spelling suggestion is not for Armenian.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Finally, there are debates whether these tools actually help fighting the “translit” problem. Some argue that people will not be forced to use Armenian keyboard if there are very good tools to transliterate. We believe that the goal of making this content available for the NLP tools is extremely important, as no one will (and should) develop, say, language translation tools for romanized alphabets.&lt;/p&gt;

&lt;p&gt;Wikipedia has similar stories for &lt;a href=&quot;https://en.wikipedia.org/wiki/Greeklish&quot;&gt;Greek&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Fingilish&quot;&gt;Persian&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Translit&quot;&gt;Cyrillic&lt;/a&gt; alphabets. The problem exists for many writing systems and is mostly overlooked by the NLP community, although it’s definitely not the hardest problem in NLP. We hope that the solution we develop for Armenian might become helpful for other languages as well.&lt;/p&gt;

&lt;h2 id=&quot;data-processing&quot;&gt;Data processing&lt;/h2&gt;

&lt;p&gt;We are using a recurrent neural network that takes a sequence of characters (romanized Armenian) at its input and outputs a sequence of Armenian characters. In order to train such a system we take a lot of text in Armenian, romanize it using probabilistic rules and give them to the network.&lt;/p&gt;

&lt;h3 id=&quot;source-of-the-data&quot;&gt;Source of the data&lt;/h3&gt;

&lt;p&gt;We chose Armenian Wikipedia as the easiest available large corpus of Armenian text. The dumps are available &lt;a href=&quot;https://dumps.wikimedia.org/hywiki/&quot;&gt;here&lt;/a&gt;. These dumps are in a very complicated XML format, but they can be parsed by the &lt;a href=&quot;https://github.com/attardi/wikiextractor&quot;&gt;WikiExtractor tool&lt;/a&gt;. The details are in the &lt;a href=&quot;https://github.com/YerevaNN/translit-rnn&quot;&gt;Readme file&lt;/a&gt; of the repository we released today.&lt;/p&gt;

&lt;p&gt;The disadvantage of Wiki is that it doesn’t contain very diverse texts. For example, it doesn’t contain any dialogs or non formal speech (while social networks are full of them). On the other hand it’s very easy to parse and it’s quite large (356MB). We splitted this into training (284MB), validation (36MB) and test (36MB) sets, but then we understood that the overlap between training and validation sets can be very high. Finally we decided to use some &lt;a href=&quot;http://grapaharan.org/index.php/Պատը&quot;&gt;fiction text&lt;/a&gt; with lots of dialogs as a validation set.&lt;/p&gt;

&lt;h3 id=&quot;romanization-rules&quot;&gt;Romanization rules&lt;/h3&gt;

&lt;p&gt;To generate the input sequences for the network we need to romanize the texts. We use probabilistic rules, as different people prefer different romanizations. Armenian alphabet has 39 characters, while Latin has only 26. Some of the Armenian letters are romanized in a unique way, like &lt;code class=&quot;highlighter-rouge&quot;&gt;ա&lt;/code&gt;-&lt;code class=&quot;highlighter-rouge&quot;&gt;a&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;բ&lt;/code&gt;-&lt;code class=&quot;highlighter-rouge&quot;&gt;b&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;դ&lt;/code&gt;-&lt;code class=&quot;highlighter-rouge&quot;&gt;d&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;ի&lt;/code&gt;-&lt;code class=&quot;highlighter-rouge&quot;&gt;i&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;մ&lt;/code&gt;-&lt;code class=&quot;highlighter-rouge&quot;&gt;m&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;ն&lt;/code&gt;-&lt;code class=&quot;highlighter-rouge&quot;&gt;n&lt;/code&gt;. Some letters require a combination of two Latin letters: &lt;code class=&quot;highlighter-rouge&quot;&gt;շ&lt;/code&gt;-&lt;code class=&quot;highlighter-rouge&quot;&gt;sh&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;ժ&lt;/code&gt;-&lt;code class=&quot;highlighter-rouge&quot;&gt;zh&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;խ&lt;/code&gt;-&lt;code class=&quot;highlighter-rouge&quot;&gt;kh&lt;/code&gt;. The latter is also romanized to &lt;code class=&quot;highlighter-rouge&quot;&gt;gh&lt;/code&gt; or even &lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt; (because this one looks like Russian &lt;code class=&quot;highlighter-rouge&quot;&gt;х&lt;/code&gt; which is pronounced the same way as Armenian &lt;code class=&quot;highlighter-rouge&quot;&gt;խ&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;But the main obstacle is that the same Latin character can correspond to different Armenian letters. For example &lt;code class=&quot;highlighter-rouge&quot;&gt;c&lt;/code&gt; can come from both &lt;code class=&quot;highlighter-rouge&quot;&gt;ց&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;ծ&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;t&lt;/code&gt; can come from both &lt;code class=&quot;highlighter-rouge&quot;&gt;տ&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;թ&lt;/code&gt;, and so on. This is what the network has to learn to infer from the context.&lt;/p&gt;

&lt;p&gt;We have created a probabilistic mapping, so that each Armenian letter is romanized according to the given probabilities. For example, &lt;code class=&quot;highlighter-rouge&quot;&gt;ծ&lt;/code&gt; is replaced by &lt;code class=&quot;highlighter-rouge&quot;&gt;ts&lt;/code&gt; in 60% of cases, &lt;code class=&quot;highlighter-rouge&quot;&gt;c&lt;/code&gt; in 30% of cases, and &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;amp;&lt;/code&gt; in 10% of cases. The full set of rules are here and can be browsed &lt;a href=&quot;http://jsoneditoronline.org/?id=ef9f135c1a0b4f3ad4724f5fa628fb00&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;img src=&quot;http://yerevann.github.io/public/2016-09-09/hy-rules.jpg&quot; alt=&quot;Some of the romanization rules for Armenian&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Some of the romanization rules for Armenian&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;geographic-dependency&quot;&gt;Geographic dependency&lt;/h3&gt;

&lt;p&gt;The romanization rules vary a lot in different countries. For example, Armenian letter &lt;code class=&quot;highlighter-rouge&quot;&gt;շ&lt;/code&gt; is mostly romanized as &lt;code class=&quot;highlighter-rouge&quot;&gt;sh&lt;/code&gt;, but Armenians in Germany prefer &lt;code class=&quot;highlighter-rouge&quot;&gt;sch&lt;/code&gt;, Armenians in France sometimes use &lt;code class=&quot;highlighter-rouge&quot;&gt;ch&lt;/code&gt;, and Armenians in Russia use &lt;code class=&quot;highlighter-rouge&quot;&gt;w&lt;/code&gt; (because &lt;code class=&quot;highlighter-rouge&quot;&gt;w&lt;/code&gt; is visually similar to Russian &lt;code class=&quot;highlighter-rouge&quot;&gt;ш&lt;/code&gt; which sounds like &lt;code class=&quot;highlighter-rouge&quot;&gt;sh&lt;/code&gt;). There are many other similar differences that might require separate analysis.&lt;/p&gt;

&lt;p&gt;Finally, Armenian language has two branches: Eastern and Western Armenian. These branches have crucial differences in romanization rules. Here we focus only on the rules for Eastern Armenian and those that are commonly used in Armenia.&lt;/p&gt;

&lt;h3 id=&quot;filtering-out-large-non-armenian-chunks&quot;&gt;Filtering out large non-Armenian chunks&lt;/h3&gt;

&lt;p&gt;Wikidumps contain some large regions where there are no Armenian characters. We noticed that these regions were confusing the network. So now when generating a chunk to give to the system we drop the ones that do not contain at least 33% Armenian characters.&lt;/p&gt;

&lt;p&gt;This is a difficult decision, as one might want the system to recognize English words in the text and leave them without transliteration. For example, the word &lt;code class=&quot;highlighter-rouge&quot;&gt;You Tube&lt;/code&gt; should not be transliterated to Armenian. We hope that such small cases of English words/names will remain in the training set.&lt;/p&gt;

&lt;h2 id=&quot;network-architecture&quot;&gt;Network architecture&lt;/h2&gt;

&lt;p&gt;Our search for a good network architecture started from &lt;a href=&quot;https://github.com/Lasagne/Recipes/blob/master/examples/lstm_text_generation.py&quot;&gt;Lasagne implementation&lt;/a&gt; of &lt;a href=&quot;https://github.com/karpathy/char-rnn&quot;&gt;Karpathy’s popular char-rnn network&lt;/a&gt;. Char-rnn is a language model, it predicts the next character given the previous ones and is based on 2 layers of LSTMs going from left to right. The context from the right is also important in our case, so we replaced simple LSTMs with &lt;a href=&quot;http://www.cs.toronto.edu/~graves/asru_2013.pdf&quot;&gt;bidirectional LSTMs&lt;/a&gt; (introduced &lt;a href=&quot;ftp://ftp.idsia.ch/pub/juergen/nn_2005.pdf&quot;&gt;here&lt;/a&gt; back in 1995).&lt;/p&gt;

&lt;p&gt;We have also added a shortcut connection from the input to the output of the 2nd biLSTM layer. This should help to learn the “easy” transliteration rules on this short way and leave LSTMs for the complex stuff.&lt;/p&gt;

&lt;p&gt;Just like char-rnn, our network works on character level data and has no access to dictionaries.&lt;/p&gt;

&lt;h3 id=&quot;encoding-the-characters&quot;&gt;Encoding the characters&lt;/h3&gt;

&lt;p&gt;First we define the set of possible characters (“vocabularies”) for the input and the output. The input “vocabulary” contains all the characters that appear in the right hand sides of the romanization rules, the digits and some punctuation (that can provide useful context). Then a special program runs over the entire corpus, generates the romanized version, and every symbol outside the input vocabulary is replaced by some placeholder symbol (&lt;code class=&quot;highlighter-rouge&quot;&gt;#&lt;/code&gt;) in both original and romanized versions. The symbols that are left in the original version form the “output vocabulary”.&lt;/p&gt;

&lt;p&gt;All symbols are encoded as one-hot vectors and are passed to the network. In our case the input vectors are 72 dimensional and the output vectors are 152 dimensional.&lt;/p&gt;

&lt;h3 id=&quot;aligning&quot;&gt;Aligning&lt;/h3&gt;

&lt;p&gt;After some experiments we noticed that LSTMs are really struggling when the characters are not aligned in inputs and outputs. As one Armenian character can be replaced by 2 or 3 Latin characters, the input and output sequences usually have different lengths, and the network has to “remember” by how many characters the romanized sequence is ahead of the Armenian sequence in order to print the next character in the correct place. This turned to be extremely difficult, and we decided to explicitly align the Armenian sequence by &lt;a href=&quot;https://github.com/YerevaNN/translit-rnn/blob/master/utils.py#L227-L232&quot;&gt;adding some placeholder symbols&lt;/a&gt; after those characters that are romanized to multi-character Latin.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;img src=&quot;http://yerevann.github.io/public/2016-09-09/aligning.png&quot; alt=&quot;Character level alignment of Armenian text with the romanization&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Character level alignment of Armenian text with the romanization&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Also there is one exceptional case in Armenian: the Latin letter ‘u’ should be transliterated to 2 Armenian symbols: &lt;code class=&quot;highlighter-rouge&quot;&gt;ու&lt;/code&gt;. This is another source of misalignment. We &lt;a href=&quot;https://github.com/YerevaNN/translit-rnn/blob/master/utils.py#L160-L166&quot;&gt;explicitly replace&lt;/a&gt; all &lt;code class=&quot;highlighter-rouge&quot;&gt;ու&lt;/code&gt; pairs with some placeholder symbol to avoid the problem.&lt;/p&gt;

&lt;h3 id=&quot;bidirectional-lstm-with-residual-like-connections&quot;&gt;Bidirectional LSTM with residual-like connections&lt;/h3&gt;

&lt;p&gt;LSTM network expects a sequence of vectors at its input. In our case it is a sequence of one-hot vectors, and the sequence length is a hyperparameter. We used &lt;code class=&quot;highlighter-rouge&quot;&gt;--seq_len 30&lt;/code&gt; for the final model. This means that the network reads 30 characters in Armenian, transforms to Latin characters (it usually becomes a bit longer than 30), then crops up to the latest whitespace before the 30th symbol. The remaining cells are filled with another placeholder symbol. This ensures that the words are not split in the middle.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;img src=&quot;http://yerevann.github.io/public/2016-09-09/bilstm-network.png&quot; alt=&quot;Network architecture&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Network architecture. Green boxes encapsulate all the magic inside LSTM. Grey trapezoids denote dense connections. Dotted line is an identity connection without trainable parameters.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;These 30 one-hot vectors are passed to the first layer of bidirectional  LSTM. Basically it is a combination of two separate LSTMs, first one is passing over the sequence from left to right, and the other is passing from right to left. We use 1024 neurons in all LSTMs. Both LSTMs output some 1024-dimensional vectors at every position. These outputs are &lt;a href=&quot;https://github.com/YerevaNN/translit-rnn/blob/master/utils.py#L283&quot;&gt;concatenated&lt;/a&gt; into a 2048 dimensional vector and are passed through another dense layer that outputs a 1024 dimensional vector. That’s what we call one layer of a bidirectional LSTM. The number of such layers is another hyperparameter (&lt;code class=&quot;highlighter-rouge&quot;&gt;--depth&lt;/code&gt;). Our experiments showed that 2 layers learn better than 1 or 3 layers.&lt;/p&gt;

&lt;p&gt;At every position the output of the last bidirectional LSTM is &lt;a href=&quot;https://github.com/YerevaNN/translit-rnn/blob/master/utils.py#L292&quot;&gt;concatenated with the one-hot vector of the input&lt;/a&gt; forming a 1096 dimensional vector. Then it is densely connected to the final layer with 152 neurons on which softmax is applied. The total loss is the mean of the cross entropy losses of the current sequence.&lt;/p&gt;

&lt;p&gt;The concatenation of the input vector to the output of the LSTM is similar to the residual connections introduced in &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;&gt;deep residual networks&lt;/a&gt;. Some of the transliteration rules are very easy and deterministic, so they can be learned by a diagonal-like matrix between input and output vectors. For more complex rules the output of LSTMs will become important. One important difference from deep residual networks is that instead of adding the input vector to the output of LSTMs, we just concatenate them. Also, our residual connections do not help fighting the vanishing/exploding gradient problem, we have LSTM for that.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;We have trained this network using &lt;code class=&quot;highlighter-rouge&quot;&gt;adagrad&lt;/code&gt; algorithm with gradient clipping (learning rate was set to &lt;code class=&quot;highlighter-rouge&quot;&gt;0.01&lt;/code&gt; and was not modified). Training is not very stable and it’s very hard to wait until it overfits on our hardware (NVidia GTX 980). We use &lt;code class=&quot;highlighter-rouge&quot;&gt;--batch_size 350&lt;/code&gt; and it consumes more than 2GB of GPU memory.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python -u train.py --hdim 1024 --depth 2 --seq_len 30 --batch_size 350 &amp;amp;&amp;gt; log
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The model we got for Armenian was trained for 42 hours. Here are the plots of training and validation sets:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;img src=&quot;http://yerevann.github.io/public/2016-09-09/loss.png&quot; alt=&quot;Loss functions&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Loss functions. Green is the validation loss, blue is the training loss.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The loss quickly drops in the first quarter of the first epoch, then continues to slowly decrease. We stopped after 5.1 epochs. The &lt;a href=&quot;https://en.wikipedia.org/wiki/Levenshtein_distance&quot;&gt;Levenshtein distance&lt;/a&gt; between the original Armenian text and the output of the network on the validation test is 405 (the length is 36694). For example, hayeren.am’s converter output has more than 2500 edit distance.&lt;/p&gt;

&lt;p&gt;Here are some results.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Romanized snippet from Wikipedia (test set)&lt;/th&gt;
      &lt;th&gt;Transliteration by translit-rnn&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Belgiayi gyuxatntesutyuny Belgiayi tntesutyan jyuxeric mekn e։ Gyuxatntesutyany bnorosh e bardzr intyensivutyune, sakayn myec che nra der@ erkri tntesutyan mej։ Byelgian manr ev mijin agrarayin tntesutyunneri erkir e։ Gyuxatntyesutyan mej ogtagortsvox hoghataracutyan mot kese patkanum e 5-ic 20 ha unecox fermernerin, voronq masnagitacats yen qaxaknerin mterqner matakararelu gorcum, talis en apranqayin artadranqi himnakan zangvatse։&lt;/td&gt;
      &lt;td&gt;Բելգիայի գյուղատնտեսությունը Բելգիայի տնտեսության ճյուղերից մեկն է։ Գյուղատնտեսությանը բնորոշ է բարձր ինտենսիվությունը, սակայն մեծ չէ նրա դերը երկրի տնտեսության մեջ։ Բելգիան մանր և միջին ագրարային տնտեսությունների երկիր է։ Գյուղատնտեսության մեջ օգտագործվող հողատարածության մոտ կեսը պատկանում է 5-ից 20 հա ունեցող ֆերմերներին, որոնք մասնագիտացած են քաղաքներին մթերքներ մատակարարելու գործում, տալիս են ապրանքային արտադրանքի հիմնական զանգվածը։&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Edit distance between this output and the original text is 0. Next we try some legal text in Armenian:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Romanized snippet from Armenian constitution&lt;/th&gt;
      &lt;th&gt;Transliteration by translit-rnn&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Zhoghovurdn ir ishkhanutyunn irakanatsnum e azat yntrutyunneri, hanraqveneri, inchpyes naev Sahmanadrutyamb naghatesvac petakan ev teghakan inqnakaravarman marminnyeri u pashtonatar anzanc midjocov:&lt;/td&gt;
      &lt;td&gt;հողովուրդն իր իշխանությունն իրականացնում է ազատ ընտրությունների, հանրաքվեների, ինչպես նաև Սահմանադրությամբ նախատեսված պետական և տեղական ինքնակառավարման մարմինների ու պաշտոնատար անձանց միջոցով:&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;There is only one error here. The first word should start by &lt;code class=&quot;highlighter-rouge&quot;&gt;Ժ&lt;/code&gt; and not &lt;code class=&quot;highlighter-rouge&quot;&gt;հ&lt;/code&gt;. The possible reason for this is that the network doesn’t have a left-side context for that character.&lt;/p&gt;

&lt;p&gt;An interesting feature of this system is that it also tries to learn when the Latin letters should not be converted to Armenian. Next example comes from a random Facebook group:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Random post from a Facebook group&lt;/th&gt;
      &lt;th&gt;Transliteration by translit-rnn&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;aysor aravotyan jamy 10;40–11;00 ynkac hatvacum 47 hamari yertuxayini miji txa,vor qez pahecir txamardavari u vori hamar MERSI.,xndrum em ete kardas PM gri. p.s.anlurj, animast u antexi commentner chgreq,karevor e u lurj.&lt;/td&gt;
      &lt;td&gt;այսօր առավոտյան ժամը 10;40–11;00 ընկած հատվածում 47 համարի երթուղայինի միջի տղա,որ քեզ պահեցիր տղամարդավարի ու որի համար ՄԵՐSI.,խնդրում եմ եթե կարդաս ՊՄ գրի. p.s.անլուրջ, անիմաստ ու անտեղի ցոմմենտներ չգրեք,կարևոր է ու լուրջ.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;It is interesting that the sequence &lt;code class=&quot;highlighter-rouge&quot;&gt;p.s.&lt;/code&gt; is not transliterated. Also it decided to leave half of the letters of &lt;code class=&quot;highlighter-rouge&quot;&gt;MERSI&lt;/code&gt; in Latin which is probably because it’s written in all caps (Wikipedia doesn’t contain a lot of text in all caps, maybe except some abbreviations).  Also, the word &lt;code class=&quot;highlighter-rouge&quot;&gt;commentner&lt;/code&gt; is transliterated as &lt;code class=&quot;highlighter-rouge&quot;&gt;ցոմմենտներ&lt;/code&gt; (instead of &lt;code class=&quot;highlighter-rouge&quot;&gt;քոմենթներ&lt;/code&gt;), because it’s not really a romanized Armenian word, it just includes the English word &lt;code class=&quot;highlighter-rouge&quot;&gt;comment&lt;/code&gt; (and it definitely doesn’t appear in Wiki).&lt;/p&gt;

&lt;h2 id=&quot;future-work&quot;&gt;Future work&lt;/h2&gt;

&lt;p&gt;First we plan to understand what the system actually learned by visualizing its behavior on different cases. It is interesting to see how the residual connection performed and also if the network managed to discover some rules known from Armenian orthography.&lt;/p&gt;

&lt;p&gt;Next, we want to bring this tool to the web. We will have to make much smaller/faster model, translate it to Javascript, and probably wrap it in a Chrome extension.&lt;/p&gt;

&lt;p&gt;Finally, we would like to see this tool applied to more languages. We have released all the code in the &lt;a href=&quot;https://github.com/YerevaNN/translit-rnn&quot;&gt;translit-rnn repository&lt;/a&gt; and prepared instructions on how to add a new language. Basically a large corpus and probabilistic romanization rules are required.&lt;/p&gt;

&lt;p&gt;We would like to thank Adam Mathias Bittlingmayer for many valuable discussions.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Combining CNN and RNN for spoken language identification</title>
   <link href="http://yerevann.github.io//2016/06/26/combining-cnn-and-rnn-for-spoken-language-identification/"/>
   <updated>2016-06-26T00:00:00+00:00</updated>
   <id>http://yerevann.github.io//2016/06/26/combining-cnn-and-rnn-for-spoken-language-identification</id>
   <content type="html">&lt;p&gt;By &lt;a href=&quot;https://github.com/Harhro94&quot;&gt;Hrayr Harutyunyan&lt;/a&gt; and &lt;a href=&quot;https://github.com/Hrant-Khachatrian&quot;&gt;Hrant Khachatrian&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Last year Hrayr used &lt;a href=&quot;/2015/10/11/spoken-language-identification-with-deep-convolutional-networks/&quot;&gt;convolutional networks to identify spoken language&lt;/a&gt; from short audio recordings for a &lt;a href=&quot;https://community.topcoder.com/longcontest/?module=ViewProblemStatement&amp;amp;rd=16555&amp;amp;compid=49304&quot;&gt;TopCoder contest&lt;/a&gt; and got 95% accuracy. After the end of the contest we decided to try recurrent neural networks and their combinations with CNNs on the same task. The best combination allowed to reach 99.24% and an ensemble of 33 models reached 99.67%. This work became Hrayr’s bachelor’s thesis.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 class=&quot;no_toc&quot; id=&quot;contents&quot;&gt;Contents&lt;/h2&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#inputs-and-outputs&quot; id=&quot;markdown-toc-inputs-and-outputs&quot;&gt;Inputs and outputs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#network-architecture&quot; id=&quot;markdown-toc-network-architecture&quot;&gt;Network architecture&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#convolutional-networks-cnn&quot; id=&quot;markdown-toc-convolutional-networks-cnn&quot;&gt;Convolutional networks (CNN)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#recurrent-neural-networks-rnn&quot; id=&quot;markdown-toc-recurrent-neural-networks-rnn&quot;&gt;Recurrent neural networks (RNN)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#combinations-of-cnn-and-rnn&quot; id=&quot;markdown-toc-combinations-of-cnn-and-rnn&quot;&gt;Combinations of CNN and RNN&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#ensembling&quot; id=&quot;markdown-toc-ensembling&quot;&gt;Ensembling&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#final-remarks&quot; id=&quot;markdown-toc-final-remarks&quot;&gt;Final remarks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;inputs-and-outputs&quot;&gt;Inputs and outputs&lt;/h2&gt;
&lt;p&gt;As before, the inputs of the networks are spectrograms of speech recordings. It seems spectrograms are the standard way to represent audio for deep learning systems (see &lt;a href=&quot;http://arxiv.org/abs/1508.01211&quot;&gt;“Listen, Attend and Spell”&lt;/a&gt; and &lt;a href=&quot;http://arxiv.org/abs/1512.02595&quot;&gt;“Deep Speech 2: End-to-End Speech Recognition in
English and Mandarin”&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Some networks use up to 11khz frequencies (858 x 256 image) and others use up to 5.5khz frequencies (858 x 128 image). In general the networks which use up to 5.5khz frequencies perform a little bit better (probably because the higher frequencies do not contain much useful information and just make overfitting easier).&lt;/p&gt;

&lt;p&gt;The output layer of all networks is a fully connected softmax layer with 176 units.&lt;/p&gt;

&lt;p&gt;We didn’t augment the data using &lt;a href=&quot;/2015/10/11/spoken-language-identification-with-deep-convolutional-networks/#data-augmentation&quot;&gt;&lt;em&gt;vocal tract length augmentation&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;network-architecture&quot;&gt;Network architecture&lt;/h2&gt;

&lt;p&gt;We have tested several network architectures. First set of architectures are plain AlexNet-like convolutional networks. The second set contains no convolutions and interprets the columns of the spectrogram as a sequence of inputs to a recurrent network. The third set applies RNN on top of the features extracted by a convolutional network. All models are implemented in &lt;a href=&quot;http://deeplearning.net/software/theano/&quot;&gt;Theano&lt;/a&gt; and &lt;a href=&quot;http://lasagne.readthedocs.io/en/latest/&quot;&gt;Lasagne&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Almost all networks easily reach 100% accuracy on the training set. In the following tables we describe all architectures we tried and report accuracy on the validation set.&lt;/p&gt;

&lt;h3 id=&quot;convolutional-networks-cnn&quot;&gt;Convolutional networks (CNN)&lt;/h3&gt;

&lt;p&gt;The network consists of 6 blocks of 2D convolution, ReLU nonlinearity, 2D max pooling and batch normalization. We use 7x7 filters for the first convoluational layer, 5x5 for the second and 3x3 for the rest. Pooling size is always 3x3 with a stride 2.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1502.03167&quot;&gt;Batch normalization&lt;/a&gt; significantly increases the training speed (this fact is reported in lots of recent papers). Finally we use only 1 fully connected layer between the last pooling layer and the softmax layer, and apply 50% dropout on that.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Network&lt;/th&gt;
      &lt;th&gt;Accuracy&lt;/th&gt;
      &lt;th&gt;Notes&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/YerevaNN/Spoken-language-identification/blob/master/theano/networks/tc_net.py&quot;&gt;tc_net&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&amp;lt;80%&lt;/td&gt;
      &lt;td&gt;The difference between this network and the CNN descibed in the previous work is that this network has only one fully connected layer. We didn’t train this network much because of &lt;code class=&quot;highlighter-rouge&quot;&gt;ignore_border=False&lt;/code&gt;, which slows down the training&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/YerevaNN/Spoken-language-identification/blob/master/theano/networks/tc_net_mod.py&quot;&gt;tc_net_mod&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;97.14&lt;/td&gt;
      &lt;td&gt;This network is the same as &lt;code class=&quot;highlighter-rouge&quot;&gt;tc_net&lt;/code&gt; but instead of &lt;code class=&quot;highlighter-rouge&quot;&gt;ignore_border=False&lt;/code&gt;, we put &lt;code class=&quot;highlighter-rouge&quot;&gt;pad=2&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/YerevaNN/Spoken-language-identification/blob/master/theano/networks/tc_net_mod_5khz_small.py&quot;&gt;tc_net_mod_5khz_small&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;96.49&lt;/td&gt;
      &lt;td&gt;This network is a smaller copy of &lt;code class=&quot;highlighter-rouge&quot;&gt;tc_net_mod&lt;/code&gt; network and works with up to 5.5khz frequencies&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The Lasagne setting &lt;code class=&quot;highlighter-rouge&quot;&gt;ignore_border=False&lt;/code&gt; &lt;a href=&quot;http://lasagne.readthedocs.io/en/latest/modules/layers/pool.html#lasagne.layers.MaxPool2DLayer&quot;&gt;prevents&lt;/a&gt; Theano from using CuDNN. Setting it to &lt;code class=&quot;highlighter-rouge&quot;&gt;True&lt;/code&gt; significantly increased the speed.&lt;/p&gt;

&lt;p&gt;Here is the detailed description of the best network of this set:  &lt;a href=&quot;https://github.com/YerevaNN/Spoken-language-identification/blob/master/theano/networks/tc_net_mod.py&quot;&gt;tc_net_mod&lt;/a&gt;.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Nr&lt;/th&gt;
      &lt;th&gt;Type&lt;/th&gt;
      &lt;th&gt;Channels&lt;/th&gt;
      &lt;th&gt;Width&lt;/th&gt;
      &lt;th&gt;Height&lt;/th&gt;
      &lt;th&gt;Kernel size / stride&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;Input&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;858&lt;/td&gt;
      &lt;td&gt;256&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Conv&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;852&lt;/td&gt;
      &lt;td&gt;250&lt;/td&gt;
      &lt;td&gt;7x7 / 1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;ReLU&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;852&lt;/td&gt;
      &lt;td&gt;250&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;MaxPool&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;427&lt;/td&gt;
      &lt;td&gt;126&lt;/td&gt;
      &lt;td&gt;3x3 / 2, pad=2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;BatchNorm&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;427&lt;/td&gt;
      &lt;td&gt;126&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;Conv&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;423&lt;/td&gt;
      &lt;td&gt;122&lt;/td&gt;
      &lt;td&gt;5x5 / 1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;ReLU&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;423&lt;/td&gt;
      &lt;td&gt;122&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;MaxPool&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;213&lt;/td&gt;
      &lt;td&gt;62&lt;/td&gt;
      &lt;td&gt;3x3 / 2, pad=2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;BatchNorm&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;213&lt;/td&gt;
      &lt;td&gt;62&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;Conv&lt;/td&gt;
      &lt;td&gt;64&lt;/td&gt;
      &lt;td&gt;211&lt;/td&gt;
      &lt;td&gt;60&lt;/td&gt;
      &lt;td&gt;3x3 / 1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;ReLU&lt;/td&gt;
      &lt;td&gt;64&lt;/td&gt;
      &lt;td&gt;211&lt;/td&gt;
      &lt;td&gt;60&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;MaxPool&lt;/td&gt;
      &lt;td&gt;64&lt;/td&gt;
      &lt;td&gt;107&lt;/td&gt;
      &lt;td&gt;31&lt;/td&gt;
      &lt;td&gt;3x3 / 2, pad=2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;BatchNorm&lt;/td&gt;
      &lt;td&gt;64&lt;/td&gt;
      &lt;td&gt;107&lt;/td&gt;
      &lt;td&gt;31&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;Conv&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
      &lt;td&gt;105&lt;/td&gt;
      &lt;td&gt;29&lt;/td&gt;
      &lt;td&gt;3x3 / 1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;ReLU&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
      &lt;td&gt;105&lt;/td&gt;
      &lt;td&gt;29&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;MaxPool&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
      &lt;td&gt;54&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;3x3 / 2, pad=2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;BatchNorm&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
      &lt;td&gt;54&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;Conv&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
      &lt;td&gt;52&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;3x3 / 1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;ReLU&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
      &lt;td&gt;52&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;MaxPool&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;3x3 / 2, pad=2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;BatchNorm&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;Conv&lt;/td&gt;
      &lt;td&gt;256&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;3x3 / 1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;ReLU&lt;/td&gt;
      &lt;td&gt;256&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;MaxPool&lt;/td&gt;
      &lt;td&gt;256&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;3x3 / 2, pad=2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;BatchNorm&lt;/td&gt;
      &lt;td&gt;256&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;Fully connected&lt;/td&gt;
      &lt;td&gt;1024&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;ReLU&lt;/td&gt;
      &lt;td&gt;1024&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;BatchNorm&lt;/td&gt;
      &lt;td&gt;1024&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Dropout&lt;/td&gt;
      &lt;td&gt;1024&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;Fully connected&lt;/td&gt;
      &lt;td&gt;176&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Softmax Loss&lt;/td&gt;
      &lt;td&gt;176&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;During the training we accidentally discovered a &lt;a href=&quot;https://github.com/Theano/Theano/issues/4534&quot;&gt;bug in Theano&lt;/a&gt;, which was quickly fixed by Theano developers.&lt;/p&gt;

&lt;h3 id=&quot;recurrent-neural-networks-rnn&quot;&gt;Recurrent neural networks (RNN)&lt;/h3&gt;

&lt;p&gt;The spectrogram can be viewed as a sequence of column vectors that consist of 256 (or 128, if only &amp;lt;5.5KHz frequencies are used) numbers. We apply &lt;a href=&quot;https://en.wikipedia.org/wiki/Recurrent_neural_network&quot;&gt;recurrent networks&lt;/a&gt; with 500 &lt;a href=&quot;https://arxiv.org/abs/1412.3555&quot;&gt;GRU cells&lt;/a&gt; in each layer on these sequences.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/2016-06-26/rnn.png&quot; alt=&quot;GRU runs directly on the spectrogram&quot; title=&quot;GRU runs directly on the spectrogram&quot; /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Network&lt;/th&gt;
      &lt;th&gt;Accuracy&lt;/th&gt;
      &lt;th&gt;Notes&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/YerevaNN/Spoken-language-identification/blob/master/theano/networks/rnn.py&quot;&gt;rnn&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;93.27&lt;/td&gt;
      &lt;td&gt;One GRU layer on top ot the input layer&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/YerevaNN/Spoken-language-identification/blob/master/theano/networks/rnn_2layers.py&quot;&gt;rnn_2layers&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;95.66&lt;/td&gt;
      &lt;td&gt;Two GRU layers on top ot the input layer&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/YerevaNN/Spoken-language-identification/blob/master/theano/networks/rnn_2layers_5khz.py&quot;&gt;rnn_2layers_5khz&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;98.42&lt;/td&gt;
      &lt;td&gt;Two GRU layers on top ot the input layer, maximum frequency: 5.5khz&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The second layer of GRU cells improved the performance. Cropping out frequencies above 5.5KHz helped fight overfitting. We didn’t use dropout for RNNs.&lt;/p&gt;

&lt;p&gt;Both RNNs and CNNs were trained using &lt;a href=&quot;http://lasagne.readthedocs.io/en/latest/modules/updates.html#lasagne.updates.adadelta&quot;&gt;adadelta&lt;/a&gt; for a few epochs, then by &lt;a href=&quot;http://lasagne.readthedocs.io/en/latest/modules/updates.html#lasagne.updates.momentum&quot;&gt;SGD with momentum&lt;/a&gt; (0.003 or 0.0003) until overfitting. If SGD with momentum is applied from the very beginning, the convergence is very slow. Adadelta converges faster but usually doesn’t reach high validation accuracy.&lt;/p&gt;

&lt;h3 id=&quot;combinations-of-cnn-and-rnn&quot;&gt;Combinations of CNN and RNN&lt;/h3&gt;

&lt;p&gt;The general architecture of these combinations is a convolutional feature extractor applied on the input, then some recurrent network on top of the CNN’s output, then an optional fully connected layer on RNN’s output and finally a softmax layer.&lt;/p&gt;

&lt;p&gt;The output of the CNN is a set of several channels (also known as &lt;em&gt;feature maps&lt;/em&gt;). We can have separate GRUs acting on each channel (with or without weight sharing) as described in this picture:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/2016-06-26/cnn-multi-rnn.png&quot; alt=&quot;Multiple GRUs run on CNN output&quot; title=&quot;Multiple GRUs run on CNN output&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Another option is to interpret CNN’s output as a 3D-tensor and run a single GRU on 2D slices of that tensor:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/2016-06-26/cnn-one-rnn.png&quot; alt=&quot;Single GRU runs on CNN output&quot; title=&quot;Single GRU runs on CNN output&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The latter option has more parameters, but the information from different channels is mixed inside the GRU, and it seems to improve performance. This architecture is similar to the one described in &lt;a href=&quot;http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43455.pdf&quot;&gt;this paper&lt;/a&gt; on speech recognition, except that they also use some residual connections (“shortcuts”) from input to RNN and from CNN to fully connected layers. It is interesting to note that recently it was shown that similar architectures work well for &lt;a href=&quot;http://arxiv.org/abs/1602.00367&quot;&gt;text classification&lt;/a&gt;.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Network&lt;/th&gt;
      &lt;th&gt;Accuracy&lt;/th&gt;
      &lt;th&gt;Notes&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/YerevaNN/Spoken-language-identification/blob/master/theano/networks/tc_net_rnn.py&quot;&gt;tc_net_rnn&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;92.4&lt;/td&gt;
      &lt;td&gt;CNN consists of 3 convolutional blocks and outputs 32 channels of size 104x13. Each of these channels is fed to a separate GRU as a sequence of 104 vectors of size 13. The outputs of GRUs are combined and fed to a fully connected layer&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/YerevaNN/Spoken-language-identification/blob/master/theano/networks/tc_net_rnn_nodense.py&quot;&gt;tc_net_rnn_nodense&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;91.94&lt;/td&gt;
      &lt;td&gt;Same as above, except there is no fully connected layer on top of GRUs. Outputs of GRU are fed directly to the softmax layer&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/YerevaNN/Spoken-language-identification/blob/master/theano/networks/tc_net_rnn_shared.py&quot;&gt;tc_net_rnn_shared&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;96.96&lt;/td&gt;
      &lt;td&gt;Same as above, but the 32 GRUs share weights. This helped to fight overfitting&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/YerevaNN/Spoken-language-identification/blob/master/theano/networks/tc_net_rnn_shared_pad.py&quot;&gt;tc_net_rnn_shared_pad&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;98.11&lt;/td&gt;
      &lt;td&gt;4 convolutional blocks in CNN using &lt;code class=&quot;highlighter-rouge&quot;&gt;pad=2&lt;/code&gt; instead of &lt;code class=&quot;highlighter-rouge&quot;&gt;ignore_broder=False&lt;/code&gt; (which enabled CuDNN and the training became much faster). The output of CNN is a set of 32 channels of size 54x8. 32 GRUs are applied (one for each channel) with shared weights and there is no fully connected layer&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/YerevaNN/Spoken-language-identification/blob/master/theano/networks/tc_net_deeprnn_shared_pad.py&quot;&gt;tc_net_deeprnn_shared_pad&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;95.67&lt;/td&gt;
      &lt;td&gt;4 convolutional block as above, but 2-layer GRUs with shared weights are applied on CNN’s outputs. Overfitting became stronger because of this second layer&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/YerevaNN/Spoken-language-identification/blob/master/theano/networks/tc_net_shared_pad_augm.py&quot;&gt;tc_net_shared_pad_augm&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;98.68&lt;/td&gt;
      &lt;td&gt;Same as &lt;a href=&quot;https://github.com/YerevaNN/Spoken-language-identification/blob/master/theano/networks/tc_net_rnn_shared_pad.py&quot;&gt;tc_net_rnn_shared_pad&lt;/a&gt;, but the network randomly crops the input and takes 9s interval. The performance became a bit better due to this&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/YerevaNN/Spoken-language-identification/blob/master/theano/networks/tc_net_rnn_onernn.py&quot;&gt;tc_net_rnn_onernn&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;99.2&lt;/td&gt;
      &lt;td&gt;The outputs of a CNN with 4 convolutional blocks are grouped into a 32x54x8 3D-tensor and a single GRU runs on a sequence of 54 vectors of size 32*8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/YerevaNN/Spoken-language-identification/blob/master/theano/networks/tc_net_rnn_onernn_notimepool.py&quot;&gt;tc_net_rnn_onernn_notimepool&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;99.24&lt;/td&gt;
      &lt;td&gt;Same as above, but the stride along the time axis is set to 1 in every pooling layer. Because of this the CNN outputs 32 channels of size 852x8&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The second layer of GRU in this setup didn’t help due to the overfitting.&lt;/p&gt;

&lt;p&gt;It seems that subsampling in the time dimension is not a good idea. The information that is lost during subsampling can be better used by the RNN. In the &lt;a href=&quot;http://arxiv.org/abs/1602.00367v1&quot;&gt;paper on text classification&lt;/a&gt; by Yijun Xiao and
Kyunghyun Cho, the authors even suggest that maybe all pooling/subsampling layers can be replaced by recurrent layers. We didn’t experiment with this idea, but it looks very promising.&lt;/p&gt;

&lt;p&gt;These networks were trained using SGD with momentum only. The learning rate was set to 0.003 for around 10 epochs, then it was manually decreased to 0.001 and then to 0.0003. On average, it took 35 epochs to train these networks.&lt;/p&gt;

&lt;h1 id=&quot;ensembling&quot;&gt;Ensembling&lt;/h1&gt;

&lt;p&gt;The best single model had 99.24% accuracy on the validation set. We had 33 predictions by all these models (there were more than one predictions for some models, taken after different epochs) and we just summed up the predicted probabilities and got 99.67% accuracy. Surprisingly, our other attempts of ensembling (e.g. &lt;a href=&quot;http://www.scholarpedia.org/article/Ensemble_learning#Voting_based_methods&quot;&gt;majority voting&lt;/a&gt;, ensemble only on some subset of all models) didn’t give better results.&lt;/p&gt;

&lt;h1 id=&quot;final-remarks&quot;&gt;Final remarks&lt;/h1&gt;

&lt;p&gt;The number of hyperparameters in these CNN+RNN mixtures is huge. Because of the limited hardware we covered only a very small fraction of possible configurations.&lt;/p&gt;

&lt;p&gt;The organizers of the original contest &lt;a href=&quot;http://apps.topcoder.com/forums//?module=Thread&amp;amp;threadID=866217&amp;amp;start=0&amp;amp;mc=3&quot;&gt;did not publicly release&lt;/a&gt;
the dataset. Nevertheless we release the full source code &lt;a href=&quot;https://github.com/YerevaNN/Spoken-language-identification/tree/master/theano&quot;&gt;on GitHub&lt;/a&gt;. We couldn’t find many Theano/Lasagne implementations of CNN+RNN networks on GitHub, and we hope these scripts will partially fill that gap.&lt;/p&gt;

&lt;p&gt;This work was part of Hrayr’s bachelor’s thesis, which is available on &lt;a href=&quot;http://www.academia.edu/25722629/%D4%BD%D5%B8%D5%BD%D6%84%D5%AB%D6%81_%D5%AC%D5%A5%D5%A6%D5%BE%D5%AB_%D5%B3%D5%A1%D5%B6%D5%A1%D5%B9%D5%B8%D6%82%D5%B4_%D5%AD%D5%B8%D6%80%D5%A8_%D5%B8%D6%82%D5%BD%D5%B8%D6%82%D6%81%D5%B4%D5%A1%D5%B6_%D5%B4%D5%A5%D5%A9%D5%B8%D5%A4%D5%B6%D5%A5%D6%80%D5%B8%D5%BE&quot;&gt;academia.edu&lt;/a&gt; (the text is in Armenian).&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Playground for bAbI tasks</title>
   <link href="http://yerevann.github.io//2016/02/23/playground-for-babi-tasks/"/>
   <updated>2016-02-23T00:00:00+00:00</updated>
   <id>http://yerevann.github.io//2016/02/23/playground-for-babi-tasks</id>
   <content type="html">&lt;p&gt;Recently we have &lt;a href=&quot;/2016/02/05/implementing-dynamic-memory-networks/&quot;&gt;implemented&lt;/a&gt; Dynamic memory networks in Theano and trained it on Facebook’s bAbI tasks which are designed for testing basic reasoning abilities. Our implementation now solves 8 out of 20 bAbI tasks which is still behind state-of-the-art. Today we release a &lt;a href=&quot;http://yerevann.com/dmn-ui/&quot;&gt;web application&lt;/a&gt; for testing and comparing several network architectures and pretrained models.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 class=&quot;no_toc&quot; id=&quot;contents&quot;&gt;Contents&lt;/h2&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#attention-module&quot; id=&quot;markdown-toc-attention-module&quot;&gt;Attention module&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#architecture-extensions&quot; id=&quot;markdown-toc-architecture-extensions&quot;&gt;Architecture extensions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#results&quot; id=&quot;markdown-toc-results&quot;&gt;Results&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#visualizing-dynamic-memory-networks&quot; id=&quot;markdown-toc-visualizing-dynamic-memory-networks&quot;&gt;Visualizing Dynamic memory networks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#looking-for-feedback&quot; id=&quot;markdown-toc-looking-for-feedback&quot;&gt;Looking for feedback&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;attention-module&quot;&gt;Attention module&lt;/h2&gt;

&lt;p&gt;One of the key parts in the DMN architecture, as described in the &lt;a href=&quot;http://arxiv.org/abs/1506.07285&quot;&gt;original paper&lt;/a&gt;, is its attention system. DMN obtains internal representations of input sentences and question and passes these to the episodic memory module. Episodic memory passes over all the facts, generates &lt;em&gt;episodes&lt;/em&gt;, which are finally combined into a &lt;em&gt;memory&lt;/em&gt;. Each episode is created by looking at all input sentences according to some &lt;em&gt;attention&lt;/em&gt;. Attention system gives a score for each of the sentences, and if the score is low for some sentence, it will be ignored when constructing the episode.&lt;/p&gt;

&lt;p&gt;Attention system is a simple 2 layer neural network where input is a vector of features computed based on input sentence, question and current state of the memory. This vector of features is described in the paper as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/2016-02-23/attention-vector.png&quot; alt=&quot;attention module input&quot; title=&quot;attention module input&quot; /&gt;&lt;/p&gt;

&lt;p&gt;where &lt;code class=&quot;highlighter-rouge&quot;&gt;c&lt;/code&gt; is an input sentence, &lt;code class=&quot;highlighter-rouge&quot;&gt;q&lt;/code&gt; is the question, &lt;code class=&quot;highlighter-rouge&quot;&gt;m&lt;/code&gt; is the current state of the memory. We tried to stay as close to the original as possible in our first implementation, but probably we understood these expressions too literally. We implemented &lt;code class=&quot;highlighter-rouge&quot;&gt;|c-q|&lt;/code&gt; as an &lt;a href=&quot;https://github.com/YerevaNN/Dynamic-memory-networks-in-Theano/blob/master/dmn_basic.py#L217&quot;&gt;absolute value&lt;/a&gt; of a difference of two vectors, which caused lots of trouble, as Theano’s implementation of (the gradient of) &lt;code class=&quot;highlighter-rouge&quot;&gt;abs&lt;/code&gt; function gave &lt;code class=&quot;highlighter-rouge&quot;&gt;NaN&lt;/code&gt;s at random during training. Then, the terms &lt;code class=&quot;highlighter-rouge&quot;&gt;cWq&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;cWm&lt;/code&gt; actually produce &lt;a href=&quot;https://github.com/YerevaNN/Dynamic-memory-networks-in-Theano/blob/master/dmn_basic.py#L215&quot;&gt;just two numbers&lt;/a&gt;, and they do not affect anything in a large vector.&lt;/p&gt;

&lt;p&gt;Later we implemented another version called &lt;a href=&quot;https://github.com/YerevaNN/Dynamic-memory-networks-in-Theano/blob/master/dmn_smooth.py#L223&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;dmn_smooth&lt;/code&gt;&lt;/a&gt; which uses Euclidean distance between two vectors (instead of &lt;code class=&quot;highlighter-rouge&quot;&gt;abs&lt;/code&gt;). This version is much more stable and gives better results. It is interesting to note that this version trains faster on CPU than on our GPU (GTX 980). It could be because of our not so optimal code or some &lt;a href=&quot;https://github.com/Theano/Theano/issues/1168&quot;&gt;issue&lt;/a&gt; in Theano’s &lt;code class=&quot;highlighter-rouge&quot;&gt;scan&lt;/code&gt; function.&lt;/p&gt;

&lt;h2 id=&quot;architecture-extensions&quot;&gt;Architecture extensions&lt;/h2&gt;
&lt;p&gt;The only significant difference between our implementation and the original DMN, as we understand it, is the fixed number of episodes. In the paper the authors describe a stop condition, so that the network decides if it needs to compute more episodes. We did not implement it yet.&lt;/p&gt;

&lt;p&gt;Our implementations heavily overfit on many tasks. We tried several techniques to fight that, but with little luck. First, we have implemented a version of &lt;code class=&quot;highlighter-rouge&quot;&gt;dmn_smooth&lt;/code&gt; which supports &lt;a href=&quot;https://github.com/YerevaNN/Dynamic-memory-networks-in-Theano/blob/master/dmn_batch.py&quot;&gt;mini-batch training&lt;/a&gt;. Then we applied &lt;a href=&quot;https://en.wikipedia.org/wiki/Dropout_(neural_networks)&quot;&gt;dropout&lt;/a&gt; and &lt;a href=&quot;http://arxiv.org/abs/1502.03167&quot;&gt;batch normalization&lt;/a&gt; on top of the memory module (before passing to the answer module). All of these tricks help for some tasks for some hyperparameters, but still we could not beat the results obtained using simple &lt;code class=&quot;highlighter-rouge&quot;&gt;dmn_smooth&lt;/code&gt; trained without mini-batches.&lt;/p&gt;

&lt;p&gt;We plan to bring some ideas from the &lt;a href=&quot;http://arxiv.org/abs/1508.05508&quot;&gt;Neural Reasoner paper&lt;/a&gt;, especially the idea of recovering the input sentences based on the outputs of the input module.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;
&lt;p&gt;We train our implementations on bAbI tasks in a weakly supervised setting, as described in our &lt;a href=&quot;http://yerevann.github.io/2016/02/05/implementing-dynamic-memory-networks/#memory-networks&quot;&gt;previous post&lt;/a&gt;. Here we compare our results to &lt;a href=&quot;http://arxiv.org/abs/1410.3916&quot;&gt;End-to-end memory networks&lt;/a&gt; (MemN2N).&lt;/p&gt;

&lt;p&gt;So far our best results are obtained by training &lt;code class=&quot;highlighter-rouge&quot;&gt;dmn_smooth&lt;/code&gt; with 100 neurons for internal representations, 5 memory hops, using simple gradient descent for 11 epochs. We train jointly on all 20 bAbI tasks.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Task&lt;/th&gt;
      &lt;th&gt;MemN2N best version&lt;/th&gt;
      &lt;th&gt;Joint100 75.05%&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1. Single supporting fact&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;99.9%&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;100%&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2. Two supporting facts&lt;/td&gt;
      &lt;td&gt;81.2%&lt;/td&gt;
      &lt;td&gt;39.7%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3. Three supporting facts&lt;/td&gt;
      &lt;td&gt;68.3%&lt;/td&gt;
      &lt;td&gt;41.5%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4. Two argument relations&lt;/td&gt;
      &lt;td&gt;82.5%&lt;/td&gt;
      &lt;td&gt;75.5%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5. Three arguments relations&lt;/td&gt;
      &lt;td&gt;87.1%&lt;/td&gt;
      &lt;td&gt;50.1%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;6. Yes/no questions&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;98%&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;97.7%&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;7. Counting&lt;/td&gt;
      &lt;td&gt;89.9%&lt;/td&gt;
      &lt;td&gt;91.4%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;8. Lists/sets&lt;/td&gt;
      &lt;td&gt;93.9%&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;95.2%&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;9. Simple negation&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;98.5%&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;99%&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;10. Indefinite knowledge&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;97.4%&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;87.3%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;11. Basic coreference&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;96.7%&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;100%&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;12. Conjuction&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;100%&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;87%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;13. Compound coreference&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;99.5%&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;96.4%&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;14. Time reasoning&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;98%&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;73.1%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;15. Basic deduction&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;98.2%&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;53.9%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;16. Basic induction&lt;/td&gt;
      &lt;td&gt;49%&lt;/td&gt;
      &lt;td&gt;49.5%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;17. Positional reasoning&lt;/td&gt;
      &lt;td&gt;57.4%&lt;/td&gt;
      &lt;td&gt;59.3%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;18. Size reasoning&lt;/td&gt;
      &lt;td&gt;90.8%&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;98.3%&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;19. Path finding&lt;/td&gt;
      &lt;td&gt;9.4%&lt;/td&gt;
      &lt;td&gt;9%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;20. Agent’s motivations&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;99.8%&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;97.1%&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Average accuracy&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;84.775%&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;75.05%&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Solved tasks&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;10&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;8&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We solve (obtain &amp;gt;95% accuracy) 8 tasks. Our system outperforms MemN2N on some tasks, but on average stays behind by 10 percentage points. Experiments show that our networks do not manage to find connections between several sentences at once (tasks 2, 3 etc.). Task 19 (path finding) remains the most difficult one. It is actually the only task on which none of our implementations overfit. The authors of &lt;a href=&quot;http://arxiv.org/abs/1508.05508&quot;&gt;Neural Reasoner&lt;/a&gt; claim some success on that task when training on 10 000 examples. We use only 1000 samples per task for all experiments.&lt;/p&gt;

&lt;h2 id=&quot;visualizing-dynamic-memory-networks&quot;&gt;Visualizing Dynamic memory networks&lt;/h2&gt;

&lt;p&gt;We have created a web application / playground for Dynamic memory networks focused on bAbI tasks. It allows to choose a pretrained model and send custom input sentences and questions. The app shows the predicted answer and visualizes attention scores for each memory step.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;img src=&quot;/public/2016-02-23/dmn-ui.png&quot; alt=&quot;Playground for bAbI tasks&quot; title=&quot;Playground for bAbI tasks&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Web-based &lt;a href=&quot;http://yerevann.com/dmn-ui/&quot;&gt;playground for bAbI tasks&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;These visualizations show that the network does not significantly change its attention for different episodes, so it is very hard to correctly answer the questions from tasks 2 or 3.&lt;/p&gt;

&lt;p&gt;Web app is accessible at &lt;strong&gt;&lt;a href=&quot;http://yerevann.com/dmn-ui/&quot;&gt;http://yerevann.com/dmn-ui/&lt;/a&gt;&lt;/strong&gt;. Note that the vocabulary of bAbI tasks is quite limited, and our implementation of DMN cannot process out-of-vocabulary words. &lt;code class=&quot;highlighter-rouge&quot;&gt;Sample&lt;/code&gt; button is a good starting point, it gives a random sample from bAbI test set.&lt;/p&gt;

&lt;h2 id=&quot;looking-for-feedback&quot;&gt;Looking for feedback&lt;/h2&gt;

&lt;p&gt;Everything described in this post is available on Github. DMN implementations are &lt;a href=&quot;https://github.com/YerevaNN/Dynamic-memory-networks-in-Theano&quot;&gt;here&lt;/a&gt;, Flask-based restful server of the web app is in the &lt;a href=&quot;https://github.com/YerevaNN/Dynamic-memory-networks-in-Theano/tree/master/server&quot;&gt;/server/ folder&lt;/a&gt;, UI is in &lt;a href=&quot;https://github.com/YerevaNN/dmn-ui&quot;&gt;another repository&lt;/a&gt;. Feel free to fork, report issues, and please share your thoughts.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Implementing Dynamic memory networks</title>
   <link href="http://yerevann.github.io//2016/02/05/implementing-dynamic-memory-networks/"/>
   <updated>2016-02-05T00:00:00+00:00</updated>
   <id>http://yerevann.github.io//2016/02/05/implementing-dynamic-memory-networks</id>
   <content type="html">&lt;p&gt;The Allen Institute for Artificial Intelligence has organized a 4 month &lt;a href=&quot;https://www.kaggle.com/c/the-allen-ai-science-challenge&quot;&gt;contest&lt;/a&gt; in Kaggle on question answering. The aim is to create a system which can correctly answer the questions from the 8th grade science exams of US schools (biology, chemistry, physics etc.). DeepHack Lab organized a &lt;a href=&quot;http://qa.deephack.me/&quot;&gt;scientific school + hackathon&lt;/a&gt; devoted to this contest in Moscow. Our team decided to use this opportunity to explore the deep learning techniques on question answering (although they seem to be far behind traditional systems). We tried to implement Dynamic memory networks described &lt;a href=&quot;http://arxiv.org/abs/1506.07285&quot;&gt;in a paper by A. Kumar et al&lt;/a&gt;. Here we report some preliminary results. In the next blog post we will describe the techniques we used to get to top 5% in the contest.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 class=&quot;no_toc&quot; id=&quot;contents&quot;&gt;Contents&lt;/h2&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#babi-tasks&quot; id=&quot;markdown-toc-babi-tasks&quot;&gt;bAbI tasks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#memory-networks&quot; id=&quot;markdown-toc-memory-networks&quot;&gt;Memory networks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#dynamic-memory-networks&quot; id=&quot;markdown-toc-dynamic-memory-networks&quot;&gt;Dynamic memory networks&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#semantic-memory&quot; id=&quot;markdown-toc-semantic-memory&quot;&gt;Semantic memory&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#input-module&quot; id=&quot;markdown-toc-input-module&quot;&gt;Input module&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#episodic-memory&quot; id=&quot;markdown-toc-episodic-memory&quot;&gt;Episodic memory&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#initial-experiments&quot; id=&quot;markdown-toc-initial-experiments&quot;&gt;Initial experiments&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#next-steps&quot; id=&quot;markdown-toc-next-steps&quot;&gt;Next steps&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;babi-tasks&quot;&gt;bAbI tasks&lt;/h2&gt;

&lt;p&gt;The questions of this contest are quite hard, they not only require lots of knowledge in natural sciences, but also abilities to make inferences, generalize the concepts, apply the general ideas to the examples and so on. The methods based on deep learning do not seem to be mature enough to handle all of these difficulties. On the other hand these questions have 4 answer candidates. That’s why, as was noted by &lt;a href=&quot;https://www.youtube.com/watch?v=lM2-Mi-2egM&quot;&gt;Dr. Vorontsov&lt;/a&gt;, simple search engine indexed on lots of documents will perform better as a question answering system than any “intelligent” system.&lt;/p&gt;

&lt;p&gt;But there is already some work on creating question answering / reasoning systems using neural approaches. As another lecturer of the DeepHack event, &lt;a href=&quot;https://www.youtube.com/watch?v=gi4Zf59_IcU&quot;&gt;Tomas Mikolov&lt;/a&gt;, told us, we should start from easy, even synthetic questions and try to gradually increase the difficulty. This roadmap towards building intelligent question answering systems is described in &lt;a href=&quot;http://arxiv.org/abs/1502.05698&quot;&gt;a paper&lt;/a&gt; by Facebook researchers Weston, Bordes, Chopra, Rush, Merriënboer and Mikolov, where the authors introduce a benchmark of toy questions called &lt;a href=&quot;http://fb.ai/babi&quot;&gt;bAbI tasks&lt;/a&gt; which test several basic reasoning capabilities of a QA system.&lt;/p&gt;

&lt;p&gt;Questions in the bAbI dataset are grouped into 20 types, each of them has 1000 samples for training and another 1000 samples for testing. A system is said to have passed a given task, if it correctly answers at least 95% of the questions in the test set. There is also a version with 10K samples, but as Mikolov told during the lecture, deep learning is not necessarily about large datasets, and in this setting it is more interesting to see if the systems can learn answering questions by looking at a few training samples.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;img src=&quot;/public/2016-02-06/babi1.png&quot; alt=&quot;some of the bAbI tasks&quot; title=&quot;some of the bAbI tasks&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/public/2016-02-06/babi2.png&quot; alt=&quot;some of the bAbI tasks&quot; title=&quot;some of the bAbI tasks&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Some of the bAbI tasks. More examples can be found in the &lt;a href=&quot;http://arxiv.org/pdf/1502.05698v10.pdf&quot;&gt;paper&lt;/a&gt;.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;memory-networks&quot;&gt;Memory networks&lt;/h2&gt;

&lt;p&gt;bAbI tasks were first evaluated on an LSTM-based system, which achieve 50% performance on average and do not pass any task. Then the authors of the paper try &lt;a href=&quot;http://arxiv.org/abs/1410.3916&quot;&gt;Memory Networks&lt;/a&gt; by Weston et al. It is a recurrent network which has a long-term memory component where it can learn to write some data (the input sentences) and read them later.&lt;/p&gt;

&lt;p&gt;bAbI tasks include not only the answers to the questions but also the numbers of those sentences which help answer the question. This information is taken into account when training MemNN, they not only get the correct answers but also an information about which input sentences affect the answer. Under this so called &lt;em&gt;strongly supervised&lt;/em&gt; setting “plain” Memory networks pass 7 of the 20 tasks. Then the authors apply some modifications to them and pass 16 tasks.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;img src=&quot;/public/2016-02-06/memn2n.png&quot; alt=&quot;End-to-end memory networks&quot; title=&quot;End-to-end memory networks&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;The structure of MemN2N from the &lt;a href=&quot;http://arxiv.org/abs/1410.3916&quot;&gt;paper&lt;/a&gt;.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We are mostly interested in &lt;em&gt;weakly supervised&lt;/em&gt; setting, because the additional information on important sentences is not available in many real scenarios. This was investigated in a paper by Sukhbaatar, Szlam, Weston and Fergus (from New York University and Facebook AI Research) where they introduce &lt;a href=&quot;http://arxiv.org/abs/1503.08895&quot;&gt;End-to-end memory networks&lt;/a&gt; (MemN2N). They investigate many different configurations of these systems and the best version passes 9 tasks out of 20. Facebook’s MemN2N repository on GitHub lists &lt;a href=&quot;https://github.com/facebook/MemNN&quot;&gt;some implementations of MemN2N&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;dynamic-memory-networks&quot;&gt;Dynamic memory networks&lt;/h2&gt;

&lt;p&gt;Another advancement in the direction of memory networks was made by Kumar, Irsoy, Ondruska, Iyyer, Bradbury, Gulrajani and Socher from Metamind. By the way, Richard Socher is the author of &lt;a href=&quot;http://cs224d.stanford.edu/&quot;&gt;an excellent course on deep learning and NLP&lt;/a&gt; at Stanford, which helped us a lot to get into the topic. Their &lt;a href=&quot;http://arxiv.org/abs/1506.07285&quot;&gt;paper&lt;/a&gt; introduces a new system called Dynamic memory networks (DMN) which passes 18 bAbI tasks in the strongly supervised setting. The paper does not talk about weakly supervised setting, so we decided to implement DMN from scratch in &lt;a href=&quot;http://deeplearning.net/software/theano/&quot;&gt;Theano&lt;/a&gt;.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;img src=&quot;/public/2016-02-06/dmn-high-level.png&quot; alt=&quot;High-level structure of DMN&quot; title=&quot;High-level structure of DMN&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;High-level structure of DMN from the &lt;a href=&quot;http://arxiv.org/abs/1506.07285&quot;&gt;paper&lt;/a&gt;.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;semantic-memory&quot;&gt;Semantic memory&lt;/h3&gt;

&lt;p&gt;The input of the DMN is a sequence of word vectors of input sentences. We followed the paper and used pretrained &lt;a href=&quot;http://nlp.stanford.edu/projects/glove/&quot;&gt;GloVe vectors&lt;/a&gt; and added the dimensionality of word vectors to the list of hyperparamaters (controlled by the command line argument &lt;code class=&quot;highlighter-rouge&quot;&gt;--word_vector_size&lt;/code&gt;). DMN architecture treats these vectors as part of a so called &lt;em&gt;semantic memory&lt;/em&gt; (in contrast to the &lt;em&gt;episodic memory&lt;/em&gt;) which may contain other knowledge as well. Our implementation uses only word vectors and does &lt;em&gt;not&lt;/em&gt; fine tune them during the training, so we don’t consider it as a part of the neural network.&lt;/p&gt;

&lt;h3 id=&quot;input-module&quot;&gt;Input module&lt;/h3&gt;

&lt;p&gt;The first module of DMN is an &lt;em&gt;input module&lt;/em&gt; that is a &lt;a href=&quot;http://arxiv.org/abs/1412.3555&quot;&gt;gated recurrent unit&lt;/a&gt; (GRU) running on the sequence of word vectors. GRU is a recurrent unit with 2 gates that control when its content is updated and when its content is erased. The hidden state of the input module is meant to represent the input processed so far in a vector. Input module outputs its hidden states either after every word (&lt;code class=&quot;highlighter-rouge&quot;&gt;--input_mask word&lt;/code&gt;) or after every sentence (&lt;code class=&quot;highlighter-rouge&quot;&gt;--input_mask sentence&lt;/code&gt;). These outputs are called &lt;code class=&quot;highlighter-rouge&quot;&gt;facts&lt;/code&gt;.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;img src=&quot;/public/2016-02-06/gru.png&quot; alt=&quot;Formal definition of GRU&quot; title=&quot;Formal definition of GRU&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Formal definition of GRU. &lt;code class=&quot;highlighter-rouge&quot;&gt;z&lt;/code&gt; is the &lt;em&gt;update gate&lt;/em&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;r&lt;/code&gt; is the &lt;em&gt;reset gate&lt;/em&gt;. More details and images can be found &lt;a href=&quot;http://deeplearning4j.org/lstm.html&quot;&gt;here&lt;/a&gt;.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Then there is a &lt;em&gt;question module&lt;/em&gt; that processes the question word by word and outputs one vector at the end. This is done by using the same GRU as in the input module using the same weights.&lt;/p&gt;

&lt;h3 id=&quot;episodic-memory&quot;&gt;Episodic memory&lt;/h3&gt;

&lt;p&gt;The fact and question vectors extracted from the input enter the &lt;em&gt;episodic memory&lt;/em&gt; module. Episodic memory is basically a composition of two nested GRUs. The outer GRU generates the final memory vector working over a sequence of so called &lt;em&gt;episodes&lt;/em&gt;. This GRU state is initialized by the question vector. The inner GRU generates the episodes.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;img src=&quot;/public/2016-02-06/dmn-details.png&quot; alt=&quot;Details of DMN architecture&quot; title=&quot;Details of DMN architecture&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Details of DMN architecture from the &lt;a href=&quot;http://arxiv.org/abs/1506.07285&quot;&gt;paper&lt;/a&gt;.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The inner GRU generates the episodes by passing over the facts from the input module. But when updating its inner state, the GRU takes into account the output of some &lt;code class=&quot;highlighter-rouge&quot;&gt;attention function&lt;/code&gt; on the current fact. Attention function gives a score (between 0 and 1) to each of the fact, and GRU (softly) ignores the facts having low scores. Attention function is a simple 2 layer neural network depending on the question vector, current fact, and current state of the memory. After each full pass on all facts the inner GRU outputs an &lt;em&gt;episode&lt;/em&gt; which is fed into the outer GRU which on its turn updates the memory. Then because of the updated memory the attention may give different scores to the facts. So new episodes can be created. The number of steps of the outer GRU, that is the number of the episodes, can be determined dynamically, but we fix it to simplify the implementation. It is configured by &lt;code class=&quot;highlighter-rouge&quot;&gt;--memory_hops&lt;/code&gt; setting.&lt;/p&gt;

&lt;p&gt;All facts, episodes and memories are in the same n-dimensional space, which is controlled by the command line argument &lt;code class=&quot;highlighter-rouge&quot;&gt;--dim&lt;/code&gt;. Inner and outer GRUs share their weights.&lt;/p&gt;

&lt;p&gt;###&lt;/p&gt;

&lt;p&gt;The final state of the memory is being fed into the &lt;em&gt;answer module&lt;/em&gt;, which produces the answer. We have implemented two kinds of answer modules. First is a simple linear layer on top of the memory vector with softmax activation (&lt;code class=&quot;highlighter-rouge&quot;&gt;--answer_module feedforward&lt;/code&gt;). This is useful if each answer is just one word (like in the bAbI dataset). The second kind of answer module is another GRU that can produce multiple words (&lt;code class=&quot;highlighter-rouge&quot;&gt;--answer_module recurrent&lt;/code&gt;). Its implementation is half baked now, as we didn’t need it for bAbI.&lt;/p&gt;

&lt;p&gt;The whole system is end-to-end differentiable and is trained using stochastic gradient descent. We use &lt;a href=&quot;http://arxiv.org/abs/1212.5701&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;adadelta&lt;/code&gt;&lt;/a&gt; by default. More formulas and details of architecture can be found in the original paper. But the paper does not contain many implementation details, so we may have diverged from the original implementation.&lt;/p&gt;

&lt;h2 id=&quot;initial-experiments&quot;&gt;Initial experiments&lt;/h2&gt;

&lt;p&gt;We have tested this system on bAbI tasks with a few randomly selected hyperparameters. We initialized the word vectors by using 50-dimensional GloVe vectors trained on Wikipedia. Answer module is a simple feedforward classifier over the vocabulary (which is &lt;em&gt;very&lt;/em&gt; limited in bAbI tasks). Here are the results.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;img src=&quot;/public/2016-02-06/results.png&quot; alt=&quot;Results&quot; title=&quot;Results&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;First two columns are for strongly supervised systems &lt;a href=&quot;http://arxiv.org/abs/1410.3916&quot;&gt;MemNN&lt;/a&gt; and &lt;a href=&quot;http://arxiv.org/abs/1506.07285&quot;&gt;DMN&lt;/a&gt;. Third column is the best results of &lt;a href=&quot;http://arxiv.org/abs/1410.3916&quot;&gt;MemN2N&lt;/a&gt;. The last 3 columns are our results with different dimensions of the memory.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;First basic observation is that weakly supervised systems are generally worse than the strongly supervised ones. When compared to MemN2N, our system performs much worse on the tasks 2, 3 and 16. As a result we pass only 7 tasks out of 20. On the other hand, our results on tasks 5, 6, 8, 9, 10 and 18 are better than MemN2N. Surprisingly what we got on the 17th task is better than in strongly supervised systems!&lt;/p&gt;

&lt;p&gt;Our system converges very fast on some of the tasks (like the first one), overfits on many other tasks and does not converge on tasks 2, 3 and 19.&lt;/p&gt;

&lt;p&gt;19th task (path finding) is not solved by any of these systems. Wojciech Zaremba from OpenAI informed us &lt;a href=&quot;https://www.youtube.com/watch?v=ezE-13X0UoM&quot;&gt;during his lecture&lt;/a&gt; about one system which managed to solve it using 10K training samples. This remains a very interesting challenge for us. We need to carefully experiment with various parameters to reach some meaningful conclusions.&lt;/p&gt;

&lt;p&gt;We have tried to test on the full shuffled list of 20000 bAbI tasks. We couldn’t reach 60% average accuracy after 50 hours of training on an Amazon instance, while MemN2N authors report 87.6% accuracy.&lt;/p&gt;

&lt;p&gt;This implementation of DMN is available on &lt;a href=&quot;https://github.com/YerevaNN/Dynamic-memory-networks-in-Theano&quot;&gt;Github&lt;/a&gt;. We really need lots of feedback on this code.&lt;/p&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next steps&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;We need a good way to visualize the attention in the episodic memory. This will help us understand what is exactly going on inside the system. Many papers now include such visualizations on some examples.&lt;/li&gt;
  &lt;li&gt;Our model overfits on many of the tasks even with 25-dimensional memory. We briefly experimented with L2 regularization but it didn’t help much (&lt;code class=&quot;highlighter-rouge&quot;&gt;--l2&lt;/code&gt;).&lt;/li&gt;
  &lt;li&gt;Currently we are working on a slightly modified architecture which will be optimized for multiple choice questions. Basically it will include one more input module which will read the answer choices and will provide another input for the attention mechanism.&lt;/li&gt;
  &lt;li&gt;Then we will be able to evaluate our code on more complex QA datasets like &lt;a href=&quot;http://research.microsoft.com/en-us/um/redmond/projects/mctest/&quot;&gt;MCTest&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Training with batches is not properly implemented yet. There are several technical challenges related to the variable length of input sequences. It becomes much harder to keep in control because of this kind of &lt;a href=&quot;https://github.com/Theano/Theano/issues/1772&quot;&gt;bugs&lt;/a&gt; in Theano.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We would like to thank the organizers of DeepHack.Q&amp;amp;A for the really amazing atmosphere here in &lt;a href=&quot;https://mipt.ru/&quot;&gt;PhysTech&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Generating Constitution with recurrent neural networks</title>
   <link href="http://yerevann.github.io//2015/11/12/generating-constitution-with-recurrent-neural-networks/"/>
   <updated>2015-11-12T00:00:00+00:00</updated>
   <id>http://yerevann.github.io//2015/11/12/generating-constitution-with-recurrent-neural-networks</id>
   <content type="html">&lt;p&gt;By &lt;a href=&quot;https://github.com/hnhnarek&quot;&gt;Narek Hovsepyan&lt;/a&gt; and &lt;a href=&quot;https://github.com/Hrant-Khachatrian&quot;&gt;Hrant Khachatrian&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Few months ago &lt;a href=&quot;http://cs.stanford.edu/people/karpathy/&quot;&gt;Andrej Karpathy&lt;/a&gt; wrote a &lt;a href=&quot;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&quot;&gt;great blog post&lt;/a&gt; about recurrent neural networks. He explained how these networks work and implemented a character-level RNN language model which learns to generate Paul Graham essays, &lt;a href=&quot;http://cs.stanford.edu/people/karpathy/char-rnn/shakespear.txt&quot;&gt;Shakespeare works&lt;/a&gt;, &lt;a href=&quot;http://cs.stanford.edu/people/karpathy/char-rnn/wiki.txt&quot;&gt;Wikipedia articles&lt;/a&gt;, &lt;a href=&quot;http://cs.stanford.edu/people/jcjohns/fake-math/4.pdf&quot;&gt;LaTeX articles&lt;/a&gt; and even C++ code. He also released the code of the network on &lt;a href=&quot;https://github.com/karpathy/char-rnn&quot;&gt;Github&lt;/a&gt;. Lots of people did experiments, like generating &lt;a href=&quot;https://gist.github.com/nylki/1efbaa36635956d35bcc&quot;&gt;recipes&lt;/a&gt;, &lt;a href=&quot;http://cpury.github.io/learning-holiness/&quot;&gt;Bible&lt;/a&gt; or &lt;a href=&quot;https://soundcloud.com/seaandsailor/sets/char-rnn-composes-irish-folk-music&quot;&gt;Irish folk music&lt;/a&gt;. We decided to test it on some legal texts in Armenian.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 class=&quot;no_toc&quot; id=&quot;contents&quot;&gt;Contents&lt;/h2&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#character-level-rnn-language-model&quot; id=&quot;markdown-toc-character-level-rnn-language-model&quot;&gt;Character-level RNN language model&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#data&quot; id=&quot;markdown-toc-data&quot;&gt;Data&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#network-parameters&quot; id=&quot;markdown-toc-network-parameters&quot;&gt;Network parameters&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#analysis&quot; id=&quot;markdown-toc-analysis&quot;&gt;Analysis&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#generated-samples&quot; id=&quot;markdown-toc-generated-samples&quot;&gt;Generated samples&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#nanogenmo&quot; id=&quot;markdown-toc-nanogenmo&quot;&gt;NaNoGenMo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;character-level-rnn-language-model&quot;&gt;Character-level RNN language model&lt;/h2&gt;

&lt;p&gt;Andrej did a great job explaining how the recurrent networks learn and even visualized how they work on text input in &lt;a href=&quot;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&quot;&gt;his blog&lt;/a&gt;. The program, called &lt;code class=&quot;highlighter-rouge&quot;&gt;char-rnn&lt;/code&gt;, treats the input as a sequence of characters and has no prior knowledge about them. For example, it doesn’t know that the text is in English, that there are words and there are sentences, that the space character has a special meaning and so on. After some training it manages to figure out that some character combinations appear more often than the others, learns to predict English words, uses proper punctuation, and even understands that open parentheses must be closed. When trained on Wikipedia articles it can generate text in MediaWiki format without syntax errors, although the text has little or no meaning.&lt;/p&gt;

&lt;h2 id=&quot;data&quot;&gt;Data&lt;/h2&gt;

&lt;p&gt;We decided to test Karpathy’s RNN on Armenian text. Armenian language has a &lt;a href=&quot;https://en.wikipedia.org/wiki/Armenian_alphabet&quot;&gt;unique alphabet&lt;/a&gt;, and the characters are encoded in the Unicode space by the codes &lt;a href=&quot;http://www.unicode.org/charts/PDF/U0530.pdf&quot;&gt;U+0530 - U+058F&lt;/a&gt;. In UTF-8 these symbols use two bytes where the first byte is always &lt;code class=&quot;highlighter-rouge&quot;&gt;0xD4&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;0xD5&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;0xD6&lt;/code&gt;. So the neural net has to look at almost 2 times larger distances (when compared to English) in order to be able to learn the words. Also, the Armenian alphabet contains 39 letters, 50% more than Latin.&lt;/p&gt;

&lt;p&gt;Recently the main political topic in Armenia is the Constitutional reform. This helped us to choose the corpus for training. We took all three versions of the Constitution of Armenia (the &lt;a href=&quot;http://www.arlis.am/documentview.aspx?docID=1&quot;&gt;first version&lt;/a&gt; voted in 1995, the &lt;a href=&quot;http://www.arlis.am/documentview.aspx?docID=75780&quot;&gt;updated version&lt;/a&gt; of 2005, and the &lt;a href=&quot;http://moj.am/storage/uploads/Sahmanadrakan_1-15.docx&quot;&gt;new proposal&lt;/a&gt; which will be voted later this year) and concatenated them in a &lt;a href=&quot;https://github.com/YerevaNN/char-rnn-constitution/blob/master/data/input.txt&quot;&gt;single text file&lt;/a&gt;. The size of the corpus is just 440 KB, which is roughly 224 000 Unicode symbols (all non-Armenian symbols, including spaces and numbers use 1 byte). Andrej suggests to use at least 1MB data, so our corpus is very small. On the other hand the text is quite specific, the vocabulary is very small and the structure of the text is fairly simple.&lt;/p&gt;

&lt;p&gt;All articles are of the following form:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Հոդված 1. Հայաստանի Հանրապետությունը ինքնիշխան, ժողովրդավարական, սոցիալական, իրավական պետություն է:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The first word, &lt;code class=&quot;highlighter-rouge&quot;&gt;Հոդված&lt;/code&gt;, means “Article”. Sentences end with the symbol &lt;code class=&quot;highlighter-rouge&quot;&gt;:&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;network-parameters&quot;&gt;Network parameters&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;char-rnn&lt;/code&gt; works with basic recurrent neural networks, LSTM networks and GRU-RNNs. In our experiments we only used LSTM network with 2 layers. Actually we don’t really understand how LSTM networks work in details, but we hope to improve our understanding by watching the videos of Richard Socher’s excellent &lt;a href=&quot;http://cs224d.stanford.edu/index.html&quot;&gt;NLP course&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We trained the network for 50 epochs with the default learning rate parameters (base rate is &lt;code class=&quot;highlighter-rouge&quot;&gt;2e-3&lt;/code&gt;, which decays by a factor of &lt;code class=&quot;highlighter-rouge&quot;&gt;0.97&lt;/code&gt; after each &lt;code class=&quot;highlighter-rouge&quot;&gt;10&lt;/code&gt; epochs). We wanted to understand how the size of LSTM internal state (&lt;code class=&quot;highlighter-rouge&quot;&gt;rnn_size&lt;/code&gt;), &lt;a href=&quot;https://www.youtube.com/watch?v=UcKPdAM8cnI&quot;&gt;dropout&lt;/a&gt; and batch size affect the performance. We used &lt;a href=&quot;https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search&quot;&gt;grid search&lt;/a&gt; over the following values:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;rnn_size&lt;/code&gt;: &lt;code class=&quot;highlighter-rouge&quot;&gt;128&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;256&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;512&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;batch_size&lt;/code&gt;: &lt;code class=&quot;highlighter-rouge&quot;&gt;25&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;50&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;100&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;dropout&lt;/code&gt;: &lt;code class=&quot;highlighter-rouge&quot;&gt;0&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;0.2&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;0.4&lt;/code&gt; and at the end we tried &lt;code class=&quot;highlighter-rouge&quot;&gt;0.6&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After installing Lua, Torch and CUDA (as described on &lt;a href=&quot;https://github.com/karpathy/char-rnn#requirements&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;char-rnn&lt;/code&gt; page&lt;/a&gt;) we have moved our mini-corpus to &lt;code class=&quot;highlighter-rouge&quot;&gt;/data/input.txt&lt;/code&gt; and ran the &lt;a href=&quot;https://github.com/YerevaNN/char-rnn-constitution/blob/master/run.sh&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;run.sh&lt;/code&gt; file&lt;/a&gt;, which contains commands like this:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;th train.lua &lt;span class=&quot;nt&quot;&gt;-data_dir&lt;/span&gt; data/ &lt;span class=&quot;nt&quot;&gt;-batch_size&lt;/span&gt; 50 &lt;span class=&quot;nt&quot;&gt;-dropout&lt;/span&gt; 0.4 &lt;span class=&quot;nt&quot;&gt;-rnn_size&lt;/span&gt; 512 &lt;span class=&quot;nt&quot;&gt;-gpuid&lt;/span&gt; 0 &lt;span class=&quot;nt&quot;&gt;-savefile&lt;/span&gt; bs50s512d0.4 | tee log_bs50s512d0.4&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;File names encode the hyperparameters, and the output of &lt;code class=&quot;highlighter-rouge&quot;&gt;char-rnn&lt;/code&gt; is logged using &lt;a href=&quot;https://en.wikipedia.org/wiki/Tee_(command)&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;tee&lt;/code&gt; command&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;analysis&quot;&gt;Analysis&lt;/h2&gt;

&lt;p&gt;We have adapted &lt;a href=&quot;https://github.com/YerevaNN/Caffe-python-tools/blob/master/plot_loss.py&quot;&gt;this script&lt;/a&gt; written by Hrayr to plot the behavior of loss functions during the 50 epochs. The script, which runs on &lt;code class=&quot;highlighter-rouge&quot;&gt;char-rnn&lt;/code&gt; output is available on &lt;a href=&quot;https://github.com/YerevaNN/char-rnn-constitution/blob/master/plot_loss.py&quot;&gt;Github&lt;/a&gt;. These graphs show, for example, that we practically do not gain anything after 25 epochs.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;img src=&quot;/public/2015-11-11/plot_bs50s256all.png&quot; alt=&quot;Training and validation loss&quot; title=&quot;Training and validation loss&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Training (blue to aqua) and validation (red to green) loss over 50 epochs. RNN size was set to 256 and the batch size was 50. In particular, this graph shows that when no dropout is used, validation loss actually increases after 20 epochs. Plotted using &lt;a href=&quot;https://github.com/YerevaNN/char-rnn-constitution/blob/master/plot_loss.py&quot;&gt;this script&lt;/a&gt;.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Experiments showed that, unsuprisingly, training loss is better (after 50 epochs) when RNN size is increased and when dropout ratio is decreased. Under all configurations we got the lowest train losses using batch size 50 (compared to 25 and 100) and we don’t have explanation for this.&lt;/p&gt;

&lt;p&gt;For validation loss, we have the following tables.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Dropout&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;0&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;0.2&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;0.4&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;0.6&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Batch size&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;RNN Size&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;25&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
      &lt;td&gt;0.5060&lt;/td&gt;
      &lt;td&gt;0.4307&lt;/td&gt;
      &lt;td&gt;0.4813&lt;/td&gt;
      &lt;td&gt;0.5373&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;- &lt;/code&gt;256&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;- &lt;/code&gt;0.5322&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;- &lt;/code&gt;0.4185&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;- &lt;/code&gt;0.4021&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;- &lt;/code&gt;0.4261&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;- - &lt;/code&gt;512&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;- - &lt;/code&gt;0.5596&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;- - &lt;/code&gt;0.4495&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;- - &lt;/code&gt;0.4380&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;- - &lt;/code&gt;0.4126&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;50&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
      &lt;td&gt;0.4883&lt;/td&gt;
      &lt;td&gt;0.4452&lt;/td&gt;
      &lt;td&gt;0.4813&lt;/td&gt;
      &lt;td&gt;0.5373&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;- &lt;/code&gt;256&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;- &lt;/code&gt;0.5249&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;- &lt;/code&gt;0.3887&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;- &lt;/code&gt;0.3996&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;- &lt;/code&gt;0.4280&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;- - &lt;/code&gt;512&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;- - &lt;/code&gt;0.5340&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;- - &lt;/code&gt;0.4420&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;- - &lt;/code&gt;0.3997&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;- - &lt;/code&gt;0.3800&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;100&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
      &lt;td&gt;0.5341&lt;/td&gt;
      &lt;td&gt;0.5144&lt;/td&gt;
      &lt;td&gt;0.5454&lt;/td&gt;
      &lt;td&gt;0.6094&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;- &lt;/code&gt;256&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;- &lt;/code&gt;0.5660&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;- &lt;/code&gt;0.4464&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;- &lt;/code&gt;0.4500&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;- &lt;/code&gt;0.4723&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;- - &lt;/code&gt;512&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;- - &lt;/code&gt;0.6032&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;- - &lt;/code&gt;0.4804&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;- - &lt;/code&gt;0.4599&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;- - &lt;/code&gt;0.4399&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;When RNN size is only 128, we notice that the best performance is achieved when dropout is 20%. Larger dropout values do not allow the network to learn enough. When RNN size is increased to 256, the optimal dropout value is somewhere between 20% and 40%. For RNN size 512,  the best performance we observed used 60% dropout. We didn’t try to go any further.&lt;/p&gt;

&lt;p&gt;As for the batch sizes, we see the best performance on 25 if RNN size is only 128. For larger networks, batch size 50 performs better. Overall we obtained the lowest validation score, 0.38, using 60% dropout, 50 batch size and 512 RNN size.&lt;/p&gt;

&lt;h2 id=&quot;generated-samples&quot;&gt;Generated samples&lt;/h2&gt;

&lt;p&gt;When the trained models are ready, we can generate text samples by using &lt;code class=&quot;highlighter-rouge&quot;&gt;sample.lua&lt;/code&gt; script included in the repository. It accepts one important parameter called &lt;code class=&quot;highlighter-rouge&quot;&gt;temperature&lt;/code&gt; which determines how much the network can “fantasize”. Higher temperature gives more diversity but at a cost of making more mistakes, as Andrej explains in his blog post. The command looks like this&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;th sample.lua cv/lm_bs50s128d0_epoch50.00_0.4883.t7 &lt;span class=&quot;nt&quot;&gt;-length&lt;/span&gt; 3000 &lt;span class=&quot;nt&quot;&gt;-temperature&lt;/span&gt; 0.5 &lt;span class=&quot;nt&quot;&gt;-gpuid&lt;/span&gt; 0 &lt;span class=&quot;nt&quot;&gt;-primetext&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Հոդված&quot;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;primetext&lt;/code&gt; parameter allows to predefine the first characters of the generated sequence. Also it makes the output fully reproducible. Here is a snippet from &lt;code class=&quot;highlighter-rouge&quot;&gt;bs50s128d0&lt;/code&gt; model, which is available &lt;a href=&quot;https://github.com/YerevaNN/char-rnn-constitution/tree/master/models&quot;&gt;on Github&lt;/a&gt; (validation loss is 0.4883, sampled with 0.5 temperature).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Հոդված 111. Սահմանադրական դատարանի կազմավորումը, եթե այլ չեն &lt;em&gt;հասատատիրի&lt;/em&gt; &lt;em&gt;առնչամի&lt;/em&gt; կարելի սահմանափակվել միայն օրենքով, եթե դա անհրաժեշտ է հանցագործությունների իրավունք:
Յուրաքանչյուր ոք ունի Հայաստանի Հանրապետության քաղաքացիությունը որոշում է կայացնում դատավորին կազմավորման կարգը&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;There are 2 nonexistent words here (marked by italic), others are fine. The sentences have no meaning, some parts are quite unnatural, making them difficult to read.&lt;/p&gt;

&lt;p&gt;The network easily (even with 128 RNN size) learns to separate the articles by new line and starts them by the word &lt;code class=&quot;highlighter-rouge&quot;&gt;Հոդված&lt;/code&gt; followed by some number. But even the best one doesn’t manage to use increasing numbers for consecutive articles. Actually, very often the article number starts with &lt;code class=&quot;highlighter-rouge&quot;&gt;1&lt;/code&gt;, because more than one third of the articles in the corpus have numbers starting with &lt;code class=&quot;highlighter-rouge&quot;&gt;1&lt;/code&gt;. It also understands some basic punctuation. It correctly puts commas before the word &lt;code class=&quot;highlighter-rouge&quot;&gt;եթե&lt;/code&gt;, which is the Armenian word for “if”.&lt;/p&gt;

&lt;p&gt;With 256 RNN size and 40% dropout the result is much more readable.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Հոդված 14. Պատգամավոր կարող է դնել իր վստահության հարցը: Կառավարության անդամների լիազորությունները &lt;em&gt;համապատասխանական&lt;/em&gt; կազմակերպությունների կամ միավորման և գործունեության կարգը սահմանվում է օրենքով: &lt;br /&gt;
 Հոդված 107. Պատգամավորի լիազորությունները դադարեցնում է Սահմանադրությամբ և օրենքներով: Այդ իրավունքը կարող է սահմանափակվել միայն օրենքով: &lt;br /&gt;
 Հոդված 126. Հանրապետության նախագահի հրամանագրերը և կարգադրությունները կամ այլ պետությունը միասնական կառավարման մարմինների կողմից հանցագործության կատարման պահին գործող դատարանների նախագահների թեկնածությունների և առաջարկությամբ սահմանադրական դատարանի նախագահ:&lt;br /&gt;
  Հայաստանի Հանրապետության իրավունքը&lt;br /&gt;&lt;/p&gt;
  &lt;ol&gt;
    &lt;li&gt;Յուրաքանչյուր ոք ունի իր իրավունքների և ազատությունների պաշտպանության նպատակով:&lt;br /&gt;&lt;/li&gt;
    &lt;li&gt;Ազգային ժողովի նախագահի վերահսկողության կամ Սահմանադրության 190-րդ հոդվածի 1-ին կետով նախատեսված դեպքերում և կարգով ընդունված որոշումները սահմանվում են օրենքով:&lt;br /&gt;&lt;/li&gt;
    &lt;li&gt;Յուրաքանչյուր ոք ունի իր իրավունքների և ազատությունների սահմանափակումների հետ &lt;em&gt;չապահողական&lt;/em&gt; կամ այլ դեպքերում վարչապետի նախագահների նախնական հանձնաժողովներն ստեղծվում են Սահմանադրությամբ և օրենքներով: &lt;br /&gt;&lt;/li&gt;
    &lt;li&gt;Յուրաքանչյուր ոք ունի իր ազգային որոշումները սահմանվում են օրենքով:&lt;br /&gt;&lt;/li&gt;
    &lt;li&gt;Յուրաքանչյուր ոք ունի իր իրավունքների և ազատությունների պաշտպանության նպատակով:&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;Only 2 of the 140 words are nonexistent, but both are syntactically correct. For example there is no such word &lt;code class=&quot;highlighter-rouge&quot;&gt;չապահողական&lt;/code&gt; in Armenian, but &lt;code class=&quot;highlighter-rouge&quot;&gt;չ&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;ապա&lt;/code&gt; are prefixes, &lt;code class=&quot;highlighter-rouge&quot;&gt;հող&lt;/code&gt; means “soil” and &lt;code class=&quot;highlighter-rouge&quot;&gt;ական&lt;/code&gt; is a suffix. Sentences still do not have valid structure.&lt;/p&gt;

&lt;p&gt;The network learned that sometimes ordered lists appear in the articles, but couldn’t learn to properly enumerate the points. Sometimes it counts up to 2 only :) It would be interesting to see on what kind of corpora it will be able to count a bit more.&lt;/p&gt;

&lt;p&gt;Here is one more snippet using the best performing model &lt;code class=&quot;highlighter-rouge&quot;&gt;bs50s512d0.6&lt;/code&gt; (temperature is again 0.5).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Հոդված 21. Յուրաքանչյուր ոք ունի ազատ տեղաշարժվելու և բնակություն է կառավարության անդամներին:
 Հանրապետության Նախագահը պաշտոնն ստանձնում է Հանրապետության Նախագահը չի կարող զբաղվել ձեռնարկատիրական գործունեությամբ: &lt;br /&gt;
 Հոդված 50. Հանրապետության Նախագահը պաշտոնն ստանձնում է Հանրապետության Նախագահի պաշտոնը թափուր մնալու դեպքում Հանրապետության Նախագահի արտահերթ ընտրությունը կազմված է վարչապետի առաջարկությամբ վերահսկողությունը &lt;br /&gt;&lt;/p&gt;
  &lt;ol&gt;
    &lt;li&gt;Յուրաքանչյուր ոք ունի ազատ տեղաշարժվելու և բնակավայր ընտրելու իրավունք:&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;There are virtually no invalid words anymore (less than 0.5%, and most are one character typos). Sentences are better formed. Sometimes a sentence is composed of two exact copies of different sentences that actually occur in the corpus. For example the combination &lt;code class=&quot;highlighter-rouge&quot;&gt;Հանրապետության Նախագահը պաշտոնն ստանձնում է&lt;/code&gt; appears &lt;a href=&quot;https://github.com/YerevaNN/char-rnn-constitution/blob/master/data/input.txt#L130&quot;&gt;7 times&lt;/a&gt; in the corpus, and &lt;code class=&quot;highlighter-rouge&quot;&gt;Հանրապետության Նախագահը չի կարող զբաղվել ձեռնարկատիրական գործունեությամբ&lt;/code&gt; appears &lt;a href=&quot;https://github.com/YerevaNN/char-rnn-constitution/blob/master/data/input.txt#L1501&quot;&gt;once&lt;/a&gt;. So the generated samples are often boring. Although sometimes the combination of such two parts does have a meaning. The following &lt;a href=&quot;https://github.com/YerevaNN/char-rnn-constitution/blob/master/samples/sample_bs50s512d0.6t0.5.txt#L222&quot;&gt;article&lt;/a&gt; is a very good example, and doesn’t appear in the corpus.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Հոդված 151. Հանրապետության Նախագահի հրամանագրերը և կարգադրությունները կատարում է Ազգային ժողովի նախագահը:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;When the temperature is increased to 0.75, the samples become more interesting.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Հոդված 52. Հանրապետության Նախագահի լիազորությունները սահմանվում են Սահմանադրությամբ և սահմանադրական դատարանի դատավորների մեկ մտնում առաջին ատյանի դատարանները: &lt;br /&gt;
 Հոդված 107. Ազգային ժողովի լիազորությունների ժամկետը &lt;em&gt;կեղերով&lt;/em&gt; բացասական տեղեկատվության ազատության ենթարկելու հարց հարուցելու կամ այլ գործադիր իշխանության, տեղական ինքնակառավարման մարմինների անկախության մասին.&lt;br /&gt;
 7) եզրակացություն է տալիս իր լիազորությունների երաշխավորվում է միջազգային իրավունքի սկզբունքները և նախարարներից, ներկայացնում է Ազգային ժողովին եզրակացություններ ներկայացնելու համար:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Typos are a bit more common. An “ordered list” is generated here which starts with &lt;code class=&quot;highlighter-rouge&quot;&gt;7&lt;/code&gt; and has only one entry. Article numbers are not tied to &lt;code class=&quot;highlighter-rouge&quot;&gt;1&lt;/code&gt;s anymore. Higher temperatures produce more nonexistent words.&lt;/p&gt;

&lt;h2 id=&quot;nanogenmo&quot;&gt;NaNoGenMo&lt;/h2&gt;

&lt;p&gt;Since 1999 every November is declared a &lt;a href=&quot;http://nanowrimo.org/&quot;&gt;National Novel Writing Month&lt;/a&gt;, when people are encouraged to write a novel in one month. Since 2013, similar event is organized for algorithms. It’s called &lt;a href=&quot;https://github.com/dariusk/NaNoGenMo-2015&quot;&gt;National Novel Generating Month&lt;/a&gt;. The rules are very simple, each participant must share one generated novel (at least 50 000 words) and release the source code. &lt;a href=&quot;http://www.theverge.com/2014/11/25/7276157/nanogenmo-robot-author-novel&quot;&gt;The Verge&lt;/a&gt; wrote about last year’s results.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/in/armen-khachikyan-ba969218&quot;&gt;Armen Khachikyan&lt;/a&gt; told us about this, and we thought that we can take part in it with a long enough generated Constitution. Here is &lt;a href=&quot;https://github.com/dariusk/NaNoGenMo-2015/issues/154&quot;&gt;our entry&lt;/a&gt;. It was generated by the following command:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;th sample.lua cv/lm_bs50s512d0.6_epoch50.00_0.3800.t7 &lt;span class=&quot;nt&quot;&gt;-length&lt;/span&gt; 900000 &lt;span class=&quot;nt&quot;&gt;-temperature&lt;/span&gt; 0.5 &lt;span class=&quot;nt&quot;&gt;-gpuid&lt;/span&gt; 0 &lt;span class=&quot;nt&quot;&gt;-primetext&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Գ Լ ՈՒ Խ  1&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; sample_bs50s512d0.6t0.5.txt&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The model was generated by the following command:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;th train.lua &lt;span class=&quot;nt&quot;&gt;-data_dir&lt;/span&gt; data/ &lt;span class=&quot;nt&quot;&gt;-batch_size&lt;/span&gt; 50 &lt;span class=&quot;nt&quot;&gt;-dropout&lt;/span&gt; 0.6 &lt;span class=&quot;nt&quot;&gt;-rnn_size&lt;/span&gt; 512 &lt;span class=&quot;nt&quot;&gt;-gpuid&lt;/span&gt; 0 &lt;span class=&quot;nt&quot;&gt;-savefile&lt;/span&gt; bs50s512d0.6 | tee log_bs50s512d0.6 &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;All related files are in our &lt;a href=&quot;https://github.com/YerevaNN/char-rnn-constitution&quot;&gt;Github repository&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Spoken language identification with deep convolutional networks</title>
   <link href="http://yerevann.github.io//2015/10/11/spoken-language-identification-with-deep-convolutional-networks/"/>
   <updated>2015-10-11T00:00:00+00:00</updated>
   <id>http://yerevann.github.io//2015/10/11/spoken-language-identification-with-deep-convolutional-networks</id>
   <content type="html">&lt;p&gt;By &lt;a href=&quot;https://github.com/Harhro94&quot;&gt;Hrayr Harutyunyan&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Recently &lt;a href=&quot;https://topcoder.com/&quot;&gt;TopCoder&lt;/a&gt; announced a &lt;a href=&quot;https://community.topcoder.com/longcontest/?module=ViewProblemStatement&amp;amp;rd=16555&amp;amp;compid=49304&quot;&gt;contest&lt;/a&gt;
to identify the spoken language in audio recordings. I decided to test how well 
deep convolutional networks will perform on this kind of data. In short I managed to get 
around 95% accuracy and finished at the 10th place. This post reveals all the details.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 class=&quot;no_toc&quot; id=&quot;contents&quot;&gt;Contents&lt;/h2&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#dataset-and-scoring&quot; id=&quot;markdown-toc-dataset-and-scoring&quot;&gt;Dataset and scoring&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#preprocessing&quot; id=&quot;markdown-toc-preprocessing&quot;&gt;Preprocessing&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#network-architecture&quot; id=&quot;markdown-toc-network-architecture&quot;&gt;Network architecture&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#data-augmentation&quot; id=&quot;markdown-toc-data-augmentation&quot;&gt;Data augmentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#ensembling&quot; id=&quot;markdown-toc-ensembling&quot;&gt;Ensembling&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#what-we-learned-from-this-contest&quot; id=&quot;markdown-toc-what-we-learned-from-this-contest&quot;&gt;What we learned from this contest&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#unexplored-options&quot; id=&quot;markdown-toc-unexplored-options&quot;&gt;Unexplored options&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dataset-and-scoring&quot;&gt;Dataset and scoring&lt;/h2&gt;

&lt;p&gt;The recordings were in one of the 176 languages. Training set consisted of 66176 &lt;code class=&quot;highlighter-rouge&quot;&gt;mp3&lt;/code&gt; files, 
376 per language, from which I have separated 12320 recordings for validation 
(Python script is &lt;a href=&quot;https://github.com/YerevaNN/Spoken-language-identification-CNN/blob/master/choose_val_set.py&quot;&gt;available on GitHub&lt;/a&gt;). 
Test set consisted of 12320 &lt;code class=&quot;highlighter-rouge&quot;&gt;mp3&lt;/code&gt; files. All recordings had the same length (~10 sec) 
and seemed to be noise-free (at least all the samples that I have checked).&lt;/p&gt;

&lt;p&gt;Score was calculated the following way: for every &lt;code class=&quot;highlighter-rouge&quot;&gt;mp3&lt;/code&gt; top 3 guesses were uploaded in a CSV file. 
1000 points were given if the first guess is correct,
400 points if the second guess is correct and 160 points if the third guess is correct. 
During the contest the score was calculated only on 3520 recordings from the test set. 
After the contest the final score was calculated on the remaining 8800 recordings.&lt;/p&gt;

&lt;h2 id=&quot;preprocessing&quot;&gt;Preprocessing&lt;/h2&gt;

&lt;p&gt;I entered the contest just 14 days before the deadline, so didn’t have much time to investigate
audio specific techniques. But we had a deep convolutional network developed few months ago,
and it seemed to be a good idea to test a pure CNN on this problem. 
Some Google search revealed that the idea is not new. The earliest attempt I could find was a 
&lt;a href=&quot;http://research.microsoft.com/en-us/um/people/dongyu/nips2009/papers/montavon-paper.pdf&quot;&gt;paper by G. Montavon&lt;/a&gt;
presented in NIPS 2009 conference. The author used a network with 3 convolutional layers trained on 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Spectrogram&quot;&gt;spectrograms&lt;/a&gt; of audio recordings, and 
the output of convolutional/subsampling layers was given to a &lt;a href=&quot;https://en.wikipedia.org/wiki/Time_delay_neural_network&quot;&gt;time-delay neural network&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I found a &lt;a href=&quot;http://www.frank-zalkow.de/en/code-snippets/create-audio-spectrograms-with-python.html?ckattempt=1&quot;&gt;Python script&lt;/a&gt; 
which creates a spectrogram of a &lt;code class=&quot;highlighter-rouge&quot;&gt;wav&lt;/code&gt; file. I used &lt;a href=&quot;http://www.mpg123.de/index.shtml&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;mpg123&lt;/code&gt; library&lt;/a&gt; 
to convert &lt;code class=&quot;highlighter-rouge&quot;&gt;mp3&lt;/code&gt; files to &lt;code class=&quot;highlighter-rouge&quot;&gt;wav&lt;/code&gt; format.&lt;/p&gt;

&lt;p&gt;The preprocessing script is available on &lt;a href=&quot;https://github.com/YerevaNN/Spoken-language-identification-CNN/blob/master/augment_data.py&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;network-architecture&quot;&gt;Network architecture&lt;/h2&gt;

&lt;p&gt;I took the network architecture designed for the Kaggle’s &lt;a href=&quot;/2015/08/17/diabetic-retinopathy-detection-contest-what-we-did-wrong/&quot;&gt;diabetic retinopathy detection contest&lt;/a&gt;. 
It has 6 convolutional layers and 2 fully connected layers with 50% dropout. 
Activation function is always ReLU. Learning rates are set to be higher for
the first convolutional layers and lower for the top convolutional layers. 
The last fully connected layer has 176 neurons and is trained using a softmax loss.&lt;/p&gt;

&lt;p&gt;It is important to note that this network does not take into account the sequential characteristics
of the audio data. Although recurrent networks perform well on speech recognition tasks 
(one notable example is &lt;a href=&quot;http://arxiv.org/abs/1303.5778&quot;&gt;this paper&lt;/a&gt; 
by A. Graves, A. Mohamed and G. Hinton, cited by 272 papers according to the Google Scholar), 
I didn’t have time to learn how they work.&lt;/p&gt;

&lt;p&gt;I trained the CNN on &lt;a href=&quot;http://caffe.berkeleyvision.org&quot;&gt;Caffe&lt;/a&gt; with 32 images in a batch,
its description in Caffe prototxt format is available &lt;a href=&quot;https://github.com/YerevaNN/Spoken-language-identification-CNN/blob/master/prototxt/main_32r-2-64r-2-64r-2-128r-2-128r-2-256r-2-1024rd0.5-1024rd0.5_DLR.prototxt&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Nr&lt;/td&gt;
      &lt;td&gt;Type&lt;/td&gt;
      &lt;td&gt;Batches&lt;/td&gt;
      &lt;td&gt;Channels&lt;/td&gt;
      &lt;td&gt;Width&lt;/td&gt;
      &lt;td&gt;Height&lt;/td&gt;
      &lt;td&gt;Kernel size / stride&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;Input&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;858&lt;/td&gt;
      &lt;td&gt;256&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Conv&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;852&lt;/td&gt;
      &lt;td&gt;250&lt;/td&gt;
      &lt;td&gt;7x7 / 1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;ReLU&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;852&lt;/td&gt;
      &lt;td&gt;250&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;MaxPool&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;426&lt;/td&gt;
      &lt;td&gt;125&lt;/td&gt;
      &lt;td&gt;3x3 / 2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;Conv&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;64&lt;/td&gt;
      &lt;td&gt;422&lt;/td&gt;
      &lt;td&gt;121&lt;/td&gt;
      &lt;td&gt;5x5 / 1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;ReLU&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;64&lt;/td&gt;
      &lt;td&gt;422&lt;/td&gt;
      &lt;td&gt;121&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;MaxPool&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;64&lt;/td&gt;
      &lt;td&gt;211&lt;/td&gt;
      &lt;td&gt;60&lt;/td&gt;
      &lt;td&gt;3x3 / 2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;Conv&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;64&lt;/td&gt;
      &lt;td&gt;209&lt;/td&gt;
      &lt;td&gt;58&lt;/td&gt;
      &lt;td&gt;3x3 / 1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;ReLU&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;64&lt;/td&gt;
      &lt;td&gt;209&lt;/td&gt;
      &lt;td&gt;58&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;MaxPool&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;64&lt;/td&gt;
      &lt;td&gt;104&lt;/td&gt;
      &lt;td&gt;29&lt;/td&gt;
      &lt;td&gt;3x3 / 2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;Conv&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
      &lt;td&gt;102&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;3x3 / 1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;ReLU&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
      &lt;td&gt;102&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;MaxPool&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
      &lt;td&gt;51&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;3x3 / 2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;Conv&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
      &lt;td&gt;49&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;3x3 / 1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;ReLU&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
      &lt;td&gt;49&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;MaxPool&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;3x3 / 2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;Conv&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;256&lt;/td&gt;
      &lt;td&gt;22&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;3x3 / 1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;17&lt;/td&gt;
      &lt;td&gt;ReLU&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;256&lt;/td&gt;
      &lt;td&gt;22&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;18&lt;/td&gt;
      &lt;td&gt;MaxPool&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;256&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3x3 / 2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;19&lt;/td&gt;
      &lt;td&gt;Fully connected&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;1024&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;ReLU&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;1024&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;21&lt;/td&gt;
      &lt;td&gt;Dropout&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;1024&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;22&lt;/td&gt;
      &lt;td&gt;Fully connected&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;1024&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;23&lt;/td&gt;
      &lt;td&gt;ReLU&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;1024&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;Dropout&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;1024&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;Fully connected&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;176&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;26&lt;/td&gt;
      &lt;td&gt;Softmax Loss&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;176&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/Hrant-Khachatrian&quot;&gt;Hrant&lt;/a&gt; suggested to try the &lt;a href=&quot;http://arxiv.org/abs/1212.5701&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ADADELTA&lt;/code&gt; solver&lt;/a&gt;.
It is a method which dynamically calculates learning rate for every network parameter, and the 
training process is said to be independent of the initial choice of learning rate. Recently it
was &lt;a href=&quot;https://github.com/BVLC/caffe/pull/2782&quot;&gt;implemented in Caffe&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In practice, the base learning rate set in the Caffe solver did matter. At first I tried to use &lt;code class=&quot;highlighter-rouge&quot;&gt;1.0&lt;/code&gt; 
learning rate, and the network didn’t learn at all. Setting the base learning rate to &lt;code class=&quot;highlighter-rouge&quot;&gt;0.01&lt;/code&gt;
helped a lot and I trained the network for 90 000 iterations (more than 50 epochs). 
Then I switched to &lt;code class=&quot;highlighter-rouge&quot;&gt;0.001&lt;/code&gt; base learning rate for another 60 000
iterations. The solver is available &lt;a href=&quot;https://github.com/YerevaNN/Spoken-language-identification-CNN/blob/master/prototxt/solver.main.adadelta.prototxt&quot;&gt;here&lt;/a&gt;. 
Not sure why the base learning rate mattered so much at the early stages of the training.
One possible reason could be the large learning rate coefficients on the lower convolutional layers.
Both tricks (dynamically updating the learning rates in &lt;code class=&quot;highlighter-rouge&quot;&gt;ADADELTA&lt;/code&gt; and large learning rate coefficients)
aim to fight the gradient vanishing problem, and maybe their combination is not a very good idea. 
This should be carefully analysed.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;img src=&quot;/public/2015-10-11/no-augm-loss.jpg&quot; alt=&quot;Training (blue) and validation (red) loss&quot; title=&quot;Training (blue) and validation (red) loss&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Training (blue) and validation (red) loss over the 150 000 iterations on the non-augmented dataset. The sudden drop of training loss corresponds to the point when the base learning rate was changed from &lt;code class=&quot;highlighter-rouge&quot;&gt;0.01&lt;/code&gt; to &lt;code class=&quot;highlighter-rouge&quot;&gt;0.001&lt;/code&gt;. Plotted using &lt;a href=&quot;https://github.com/YerevaNN/Caffe-python-tools/blob/master/plot_loss.py&quot;&gt;this script&lt;/a&gt;.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The signs of overfitting were getting more and more visible and I stopped at 150 000 iterations. 
The softmax loss got to 0.43 and it corresponded to 3 180 000 score 
(out of 3 520 000 possible). Some ensembling with other models of the same network allowed to
get a bit higher score (3 220 000), but it was obvious that data augmentation is needed to overcome the 
overfitting problem.&lt;/p&gt;

&lt;h2 id=&quot;data-augmentation&quot;&gt;Data augmentation&lt;/h2&gt;

&lt;p&gt;The most important weakness of our team in the &lt;a href=&quot;/2015/08/17/diabetic-retinopathy-detection-contest-what-we-did-wrong/&quot;&gt;previous contest&lt;/a&gt;
was that we didn’t augment the dataset well enough. So I was looking for ways to augment the 
set of spectrograms. One obvious idea was to crop random, say, 9 second intervals of the recordings.
Hrant suggested another idea: to warp the frequency axis of the spectrogram. This process is known as
&lt;em&gt;vocal tract length perturbation&lt;/em&gt;, and is generally used for speaker normalization at least 
&lt;a href=&quot;http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;amp;arnumber=650310&amp;amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fiel4%2F89%2F14168%2F00650310&quot;&gt;since 1998&lt;/a&gt;.
In 2013 &lt;a href=&quot;https://www.cs.toronto.edu/~hinton/absps/perturb.pdf&quot;&gt;N. Jaitly and G. Hinton&lt;/a&gt;
used this technique to augment the audio dataset. I &lt;a href=&quot;https://github.com/YerevaNN/Spoken-language-identification-CNN/blob/master/augment_data.py#L32&quot;&gt;used this formula&lt;/a&gt;
to linearly scale the frequency bins during spectrogram generation:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;img src=&quot;/public/2015-10-11/frequency-warp-formula.png&quot; alt=&quot;Frequency warping formula&quot; title=&quot;Frequency warping formula&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Frequency warping formula from the &lt;a href=&quot;http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;amp;arnumber=650310&amp;amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fiel4%2F89%2F14168%2F00650310&quot;&gt;paper by L. Lee and R. Rose&lt;/a&gt;. α is the scaling factor. Following Jaitly and Hinton I &lt;a href=&quot;https://github.com/YerevaNN/Spoken-language-identification-CNN/blob/master/augment_data.py#L92&quot;&gt;chose it uniformly&lt;/a&gt; between 0.9 and 1.1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;I also &lt;a href=&quot;https://github.com/YerevaNN/Spoken-language-identification-CNN/blob/master/augment_data.py#L77&quot;&gt;randomly cropped&lt;/a&gt;
the spectrograms so they had &lt;code class=&quot;highlighter-rouge&quot;&gt;768x256&lt;/code&gt; size. Here are the results:&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/public/2015-10-11/spectrogram.jpg&quot; alt=&quot;Spectrogram without modifications&quot; title=&quot;Spectrogram without modifications&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Spectrogram of one of the recordings&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/public/2015-10-11/spectrogram-warped-cropped.jpg&quot; alt=&quot;Cropped spectrogram with warped frequency axis&quot; title=&quot;Cropped spectrogram with warped frequency axis&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Cropped spectrogram of the same recording with warped frequency axis&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;For each &lt;code class=&quot;highlighter-rouge&quot;&gt;mp3&lt;/code&gt; I have created 20 random spectrograms, but trained the network on 10 of them. 
It took more than 2 days to create the augmented dataset and convert it to LevelDB format (the format Caffe suggests). 
But training the network proved to be even harder. For 3 days I couldn’t significantly decrease
the train loss. After removing the dropout layers the loss started to decrease but it would take weeks 
to reach reasonable levels. Finally, Hrant suggested to try to reuse the weights of the 
model trained on the non-augmented dataset. The problem was that due to the cropping,
the image sizes in the two datasets were different. But it turned out that convolutional 
and pooling layers in Caffe &lt;a href=&quot;https://github.com/BVLC/caffe/issues/189#issuecomment-36754479&quot;&gt;work with images of variable sizes&lt;/a&gt;, 
only the fully connected layers couldn’t reuse the weights from the first model. 
So I just &lt;a href=&quot;https://github.com/YerevaNN/Spoken-language-identification-CNN/blob/master/prototxt/augm_32r-2-64r-2-64r-2-128r-2-128r-2-256r-2-1024r-1024r_DLR_nolrcoef.prototxt#L292&quot;&gt;renamed the FC layers&lt;/a&gt;
in the prototxt file and &lt;a href=&quot;http://caffe.berkeleyvision.org/tutorial/interfaces.html#command-line&quot;&gt;initialized&lt;/a&gt;
the network (convolution filters) by the weights of the first model:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;./build/tools/caffe train &lt;span class=&quot;nt&quot;&gt;--solver&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;solver.prototxt &lt;span class=&quot;nt&quot;&gt;--weights&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;models/main_32r-2-64r-2-64r-2-128r-2-128r-2-256r-2-1024rd0.5-1024rd0.5_DLR_72K-adadelta0.01_iter_153000.caffemodel&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This helped a lot. I used standard stochastic gradient descent (inverse decay learning rate policy)
with base learning rate &lt;code class=&quot;highlighter-rouge&quot;&gt;0.001&lt;/code&gt; for 36 000 iterations (less than 2 epochs), then increased 
the base learning rate to &lt;code class=&quot;highlighter-rouge&quot;&gt;0.01&lt;/code&gt; for another 48 000 iterations (due to the inverse decay policy
the rate decreased seemingly too much). 
These trainings were done without any regularization techniques,
weight decay or dropout layers, and there were clear signs of overfitting. I tried to add 50%
dropout layers on fully connected layers, but the training was extremely slow. To improve the 
speed I used 30% dropout, and trained the network for 120 000 more iterations using &lt;a href=&quot;https://github.com/YerevaNN/Spoken-language-identification-CNN/blob/master/prototxt/solver.augm.nolrcoef.prototxt&quot;&gt;this solver&lt;/a&gt;.
Softmax loss on the validation set reached 0.21 which corresponded to 3 390 000 score. 
The score was calculated by averaging softmax outputs over 20 spectrograms of each recording.&lt;/p&gt;

&lt;h2 id=&quot;ensembling&quot;&gt;Ensembling&lt;/h2&gt;

&lt;p&gt;30 hours before the deadline I had several models from the same network. And even simple
ensembling (just the sum of softmax activations of different models) performed better than
any individual model. Hrant suggested to use &lt;a href=&quot;https://github.com/dmlc/xgboost&quot;&gt;XGBoost&lt;/a&gt;, 
which is a fast implementation of &lt;a href=&quot;https://en.wikipedia.org/wiki/Gradient_boosting&quot;&gt;gradient boosting&lt;/a&gt; 
algorithm and is very popular among Kagglers. XGBoost has a good documentation and 
all parameters are &lt;a href=&quot;https://github.com/dmlc/xgboost/blob/master/doc/parameter.md&quot;&gt;well explained&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;To perform the ensembling I was creating a CSV file containing softmax activations 
(or the average of softmax activations among &lt;a href=&quot;https://github.com/YerevaNN/Spoken-language-identification-CNN/blob/master/ensembling/get_output_layers.py#L40&quot;&gt;20&lt;/a&gt; 
augmented versions of the same recording) using &lt;a href=&quot;https://github.com/YerevaNN/Spoken-language-identification-CNN/blob/master/ensembling/get_output_layers.py&quot;&gt;this script&lt;/a&gt;.
Then I was running XGBoost on these CSV files. The submission file (which was requested by TopCoder)
was generated using &lt;a href=&quot;https://github.com/YerevaNN/Spoken-language-identification-CNN/blob/master/make_submission.py&quot;&gt;this script&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I also tried to train a &lt;a href=&quot;https://github.com/YerevaNN/Spoken-language-identification-CNN/blob/master/ensembling/ensemble.theano.py&quot;&gt;simple neural network&lt;/a&gt;
with one hidden layer on the same CSV files. The results were significantly better than
with XGBoost.&lt;/p&gt;

&lt;p&gt;The best result was obtained by ensembling the following two models: snapshots of the last 
network (the one with 30% dropout) after 90 000 iterations and 105 000 iterations. Final 
score was 3 401 840 and it was the &lt;a href=&quot;http://community.topcoder.com/longcontest/stats/?module=ViewOverview&amp;amp;rd=16555&quot;&gt;10th result&lt;/a&gt;
of the contest.&lt;/p&gt;

&lt;h2 id=&quot;what-we-learned-from-this-contest&quot;&gt;What we learned from this contest&lt;/h2&gt;

&lt;p&gt;This was a quite interesting contest, although too short when compared with Kaggle’s contests.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Plain, &lt;a href=&quot;http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf&quot;&gt;AlexNet&lt;/a&gt;-like 
convolutional networks work quite well for fixed length audio recordings&lt;/li&gt;
  &lt;li&gt;Vocal tract length perturbation works well as an augmentation technique&lt;/li&gt;
  &lt;li&gt;Caffe supports sharing weights between convolutional networks having different input sizes&lt;/li&gt;
  &lt;li&gt;Single layer neural network sometimes performs better than XGBoost for ensembling (although 
I had just one day to test the both)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;unexplored-options&quot;&gt;Unexplored options&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;It is interesting to see if a network with 50% dropout layers will improve the accuracy&lt;/li&gt;
  &lt;li&gt;Maybe larger convolutional networks, like &lt;em&gt;OxfordNet&lt;/em&gt; will perform better. 
They require much more memory, and it was risky to play with them under a tough deadline&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cs.toronto.edu/~asamir/papers/icassp12_cnn.pdf&quot;&gt;Hybrid methods&lt;/a&gt;
combining CNN and Hidden Markov Models should work better&lt;/li&gt;
  &lt;li&gt;We believe it is possible to squeeze more from these models with better ensembling methods&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://apps.topcoder.com/forums/?module=Thread&amp;amp;threadID=866734&amp;amp;start=0&amp;amp;mc=4&quot;&gt;Other contestants report&lt;/a&gt;
better results based on careful mixing of the results of more traditional techniques, 
including &lt;a href=&quot;https://en.wikipedia.org/wiki/N-gram&quot;&gt;n-gram&lt;/a&gt;
and &lt;a href=&quot;https://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model&quot;&gt;Gaussian Mixture Models&lt;/a&gt;.
We believe the combination of these techniques with the deep models will provide very 
good results on this dataset&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One important issue is that the organizers of this contest &lt;a href=&quot;http://apps.topcoder.com/forums//?module=Thread&amp;amp;threadID=866217&amp;amp;start=0&amp;amp;mc=3&quot;&gt;do not allow&lt;/a&gt;
to use the dataset outside the contest. We hope this decision will be changed eventually.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Diabetic retinopathy detection contest. What we did wrong</title>
   <link href="http://yerevann.github.io//2015/08/17/diabetic-retinopathy-detection-contest-what-we-did-wrong/"/>
   <updated>2015-08-17T00:00:00+00:00</updated>
   <id>http://yerevann.github.io//2015/08/17/diabetic-retinopathy-detection-contest-what-we-did-wrong</id>
   <content type="html">&lt;p&gt;After watching the &lt;a href=&quot;https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&quot;&gt;awesome video course by Hugo Larochelle&lt;/a&gt; on neural nets (more on this in the &lt;a href=&quot;/2015/07/30/getting-started-with-neural-networks/&quot;&gt;previous post&lt;/a&gt;) we decided to test our knowledge on some computer vision contest. We looked at &lt;a href=&quot;https://www.kaggle.com/competitions&quot;&gt;Kaggle&lt;/a&gt; and the only active competition related to computer vision (except for the &lt;a href=&quot;https://www.kaggle.com/c/digit-recognizer&quot;&gt;digit recognizer contest&lt;/a&gt;, for which lots of perfect out-of-the-box solutions exist) was the &lt;a href=&quot;https://www.kaggle.com/c/diabetic-retinopathy-detection&quot;&gt;Diabetic retinopathy detection contest&lt;/a&gt;. This was probably quite hard to become our very first project, but nevertheless we decided to try. The team included &lt;a href=&quot;https://www.linkedin.com/in/mahnerak&quot;&gt;Karen&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/galstyantik&quot;&gt;Tigran&lt;/a&gt;, &lt;a href=&quot;https://github.com/Harhro94&quot;&gt;Hrayr&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/pub/narek-hovsepyan/86/b35/380&quot;&gt;Narek&lt;/a&gt; (1st to 3rd year bachelor students) and &lt;a href=&quot;https://github.com/Hrant-Khachatrian&quot;&gt;me&lt;/a&gt; (PhD student). Long story short, we finished at the &lt;a href=&quot;https://www.kaggle.com/c/diabetic-retinopathy-detection/leaderboard&quot;&gt;82nd place&lt;/a&gt; out of 661 participants, and in this post I will describe in details what we did and what mistakes we made. All required files are on these 2 &lt;a href=&quot;https://github.com/YerevaNN/Caffe-python-tools&quot;&gt;github&lt;/a&gt; &lt;a href=&quot;https://github.com/YerevaNN/Kaggle-diabetic-retinopathy-detection&quot;&gt;repositories&lt;/a&gt;. We hope this will be interesting for those who just start to play with neural networks. Also we hope to get feedback from experts and other participants.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 class=&quot;no_toc&quot; id=&quot;contents&quot;&gt;Contents&lt;/h2&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#the-contest&quot; id=&quot;markdown-toc-the-contest&quot;&gt;The contest&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#software-and-hardware&quot; id=&quot;markdown-toc-software-and-hardware&quot;&gt;Software and hardware&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#image-preprocessing&quot; id=&quot;markdown-toc-image-preprocessing&quot;&gt;Image preprocessing&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#data-augmentation&quot; id=&quot;markdown-toc-data-augmentation&quot;&gt;Data augmentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#choosing-training--validation-sets&quot; id=&quot;markdown-toc-choosing-training--validation-sets&quot;&gt;Choosing training / validation sets&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#convolutional-network-architecture&quot; id=&quot;markdown-toc-convolutional-network-architecture&quot;&gt;Convolutional network architecture&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#loss-function&quot; id=&quot;markdown-toc-loss-function&quot;&gt;Loss function&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#preparing-submissions&quot; id=&quot;markdown-toc-preparing-submissions&quot;&gt;Preparing submissions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#attempts-to-ensemble&quot; id=&quot;markdown-toc-attempts-to-ensemble&quot;&gt;Attempts to ensemble&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#more-on-this-contest&quot; id=&quot;markdown-toc-more-on-this-contest&quot;&gt;More on this contest&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#acknowledgements&quot; id=&quot;markdown-toc-acknowledgements&quot;&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-contest&quot;&gt;The contest&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Diabetic_retinopathy&quot;&gt;Diabetic retinopathy&lt;/a&gt; is a disease when the retina of the eye is damaged due to diabetes. It is one of the leading causes of blindness in the world. The contest’s aim was to see if computer programs can diagnose the disease automatically from the image of the retina. &lt;a href=&quot;https://www.kaggle.com/c/diabetic-retinopathy-detection/forums/t/15605/human-performance-on-the-competition-data-set&quot;&gt;It seems&lt;/a&gt; the winners slightly surpassed the performance of general ophthalmologists.&lt;/p&gt;

&lt;p&gt;Each eye of the patient can be in one of the 5 levels: from 0 to 4, where 0 corresponds to the healthy state and 4 is the most severe state. Different eyes of the same person can be at different levels (although some contestants managed to leverage the fact that two eyes are not completely independent). Contestants &lt;a href=&quot;https://www.kaggle.com/c/diabetic-retinopathy-detection/data&quot;&gt;were given&lt;/a&gt; 35126 JPEG images of retinas for training (32.5GB), 53576 images for testing (49.6GB) and a CSV file where level of the disease is written for the train images. The goal was to create another CSV file where disease levels are written for each of the test images. Contestants could submit maximum 5 CSV files per day for evaluation.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;img src=&quot;/public/2015-08-17/eye-0.jpeg&quot; alt=&quot;Healthy eye: level 0&quot; title=&quot;Healthy eye: level 0&quot; /&gt;&lt;/th&gt;
      &lt;th&gt;&lt;img src=&quot;/public/2015-08-17/eye-4.jpeg&quot; alt=&quot;Severe state: level 4&quot; title=&quot;Severe state: level 4&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Healthy eye: level 0&lt;/td&gt;
      &lt;td&gt;Severe state: level 4&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The score was evaluated using a metric called &lt;strong&gt;quadratic weighted kappa&lt;/strong&gt;. It is &lt;a href=&quot;https://www.kaggle.com/c/diabetic-retinopathy-detection/details/evaluation&quot;&gt;described&lt;/a&gt; as being an &lt;em&gt;agreement&lt;/em&gt; between two raters: the agreement between the scores assigned by human rater (which is unknown to contestants) and the predicted scores. If the agreement is random, the score is close 0 (sometimes it can even be negative). In case of a perfect agreement the score is 1. It is &lt;em&gt;quadratic&lt;/em&gt; in a sense that, for example, if you predict level 4 for a healthy eye, it is 16 times worse than if you predict level 1. Winners achieved a score &lt;a href=&quot;https://www.kaggle.com/c/diabetic-retinopathy-detection/leaderboard&quot;&gt;more than 0.84&lt;/a&gt;. Our best result was around 0.50.&lt;/p&gt;

&lt;h2 id=&quot;software-and-hardware&quot;&gt;Software and hardware&lt;/h2&gt;
&lt;p&gt;It was obvious that we were going to use a &lt;a href=&quot;https://www.youtube.com/watch?v=rxKrCa4bg1I&amp;amp;index=69&amp;amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&quot;&gt;convolutional neural network&lt;/a&gt; for predicting. Not only because of its &lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network#Applications&quot;&gt;awesome performance&lt;/a&gt; on many computer vision problems, including another Kaggle competition on &lt;a href=&quot;https://www.kaggle.com/c/datasciencebowl&quot;&gt;plankton classification&lt;/a&gt;, but also because it was the only technique we knew for image classification. We were aware of several libraries that implement convolutional networks, namely Python-based &lt;a href=&quot;http://deeplearning.net/software/theano/&quot;&gt;Theano&lt;/a&gt;, &lt;a href=&quot;http://caffe.berkeleyvision.org/&quot;&gt;Caffe&lt;/a&gt; written in C++, &lt;a href=&quot;https://github.com/dmlc/cxxnet&quot;&gt;cxxnet&lt;/a&gt; (developed by the &lt;a href=&quot;https://www.kaggle.com/c/datasciencebowl/forums/t/12887/brief-describe-method-and-cxxnet-v2/69545&quot;&gt;2nd  place winners&lt;/a&gt; of the plankton contest) and &lt;a href=&quot;https://github.com/torch/nn/&quot;&gt;Torch&lt;/a&gt;. We chose Caffe because it seemed to be the simplest one for beginners: it allows to define the neural network by a simple text file (like &lt;a href=&quot;https://github.com/BVLC/caffe/blob/master/examples/mnist/lenet.prototxt&quot;&gt;this&lt;/a&gt;) and train a network without writing a single line of code.&lt;/p&gt;

&lt;p&gt;We didn’t have a computer with CUDA-enabled GPU in the university, but our friends at &lt;a href=&quot;http://cyclopstudio.com/&quot;&gt;Cyclop Studio&lt;/a&gt; donated us an Intel Core i5 computer with 4GB RAM and &lt;a href=&quot;http://www.geforce.com/hardware/desktop-gpus/geforce-gtx-550ti/specifications&quot;&gt;NVidia GeForce GTX 550 TI&lt;/a&gt; card. 550 TI has a 1GB of memory which forced us to use very small batch sizes for the neural network. Later we switched to &lt;a href=&quot;http://www.geforce.com/hardware/desktop-gpus/geforce-gtx-980/specifications&quot;&gt;GeForce GTX 980&lt;/a&gt; with 4GB memory, which was completely fine for us.&lt;/p&gt;

&lt;p&gt;Karen and Tigran managed to &lt;a href=&quot;http://caffe.berkeleyvision.org/install_apt.html&quot;&gt;install Caffe on Ubuntu&lt;/a&gt; and make it work with CUDA, which was enough to start the training. Later Narek and Hrayr found out how to play with Caffe models &lt;a href=&quot;https://github.com/BVLC/caffe/tree/master/python/caffe&quot;&gt;using Python&lt;/a&gt;, so we can run our models on the test set. Karen has &lt;a href=&quot;https://docs.c9.io/docs/running-your-own-ssh-workspace&quot;&gt;connected Cloud9 to the server&lt;/a&gt;, and we could work remotely through a web interface.&lt;/p&gt;

&lt;h2 id=&quot;image-preprocessing&quot;&gt;Image preprocessing&lt;/h2&gt;
&lt;p&gt;Images from the training and test datasets have very different resolutions, aspect ratios, colors, are cropped in various ways, some are of very low quality, are out of focus etc. Neural networks require a fixed input size, so we had to resize / crop all of them to some fixed dimensions. Karen and Tigran looked at many sample images and decided that the optimal resolution which preserves the details required for classification is 512x512. We thought that in 256x256 we might lose the small details that differ healthy eye images from level 1 images. In fact, by the end of the competition we saw that our networks cannot differentiate between level 0 and 1 images even with 512x512, so probably we could safely work on 256x256 from the very beginning (which would be much faster to train). All preprocessing was done using &lt;a href=&quot;http://www.imagemagick.org/&quot;&gt;imagemagick&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We tried three methods to preprocess the images. First, as suggested by Karen and Tigran, we resized the images and then applied the so called &lt;em&gt;&lt;a href=&quot;http://www.imagemagick.org/Usage/transform/#charcoal&quot;&gt;charcoal&lt;/a&gt;&lt;/em&gt; effect which is basically an edge detector. This highlighted the signs of blood on the retina. One of the challenging problems throughout the contest was to define a naming convention for everything: databases of preprocessed images, convnet descriptions, models, CSV files etc. We used the prefix &lt;code class=&quot;highlighter-rouge&quot;&gt;edge&lt;/code&gt; for anything which was based on the images preprocessed this way. The best kappa score achieved on this dataset was 0.42.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;img src=&quot;/public/2015-08-17/eye-edge-0.jpg&quot; alt=&quot;`edge` level 0&quot; title=&quot;`edge` level 0&quot; /&gt;&lt;/th&gt;
      &lt;th&gt;&lt;img src=&quot;/public/2015-08-17/eye-edge-3.jpg&quot; alt=&quot;`edge` level 3&quot; title=&quot;`edge` level 3&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Preprocessed image &lt;em&gt;(edge)&lt;/em&gt; level 0&lt;/td&gt;
      &lt;td&gt;Preprocessed image &lt;em&gt;(edge)&lt;/em&gt; level 3&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;But later we noticed that this method makes the dirt on lens or other optical issues appear similar to a blood sign, and it really confused our neural networks. The following two images are of healthy eyes (level 0), but both were recognized by almost all our models as level 4.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;img src=&quot;/public/2015-08-17/orig-35297_left-0.jpeg&quot; alt=&quot;healthy eye&quot; title=&quot;healthy eye&quot; /&gt;&lt;/th&gt;
      &lt;th&gt;&lt;img src=&quot;/public/2015-08-17/edge-35297_left-0.jpeg&quot; alt=&quot;`edge`, recognized as level 4&quot; title=&quot;`edge`, recognized as level 4&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;img src=&quot;/public/2015-08-17/orig-44330_left-0.jpeg&quot; alt=&quot;healthy eye&quot; title=&quot;healthy eye&quot; /&gt;&lt;/th&gt;
      &lt;th&gt;&lt;img src=&quot;/public/2015-08-17/edge-44330_left-0.jpeg&quot; alt=&quot;`edge`, recognized as level 4&quot; title=&quot;`edge`, recognized as level 4&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Original images of healthy eyes&lt;/td&gt;
      &lt;td&gt;Preprocessed versions &lt;code class=&quot;highlighter-rouge&quot;&gt;edge&lt;/code&gt; recognized as level 4&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;So we decided to avoid using filters on the images, and leave all the work to the convolutional network: just resize and convert to one channel image (to save space and memory). We thought that the color information is not very important to detect the disease, although this could be one of our mistakes. Following the discussion at &lt;a href=&quot;https://www.kaggle.com/c/diabetic-retinopathy-detection/forums/t/13147/rgb-or-grayscale/69138&quot;&gt;Kaggle forums&lt;/a&gt; we decided to use the green channel only. We got our best results (kappa = 0.5) on this dataset. We used prefix &lt;code class=&quot;highlighter-rouge&quot;&gt;g&lt;/code&gt; for these images.&lt;/p&gt;

&lt;p&gt;Finally we tried to apply the &lt;a href=&quot;http://www.imagemagick.org/Usage/color_mods/#equalize&quot;&gt;&lt;em&gt;equalize&lt;/em&gt;&lt;/a&gt; filter on top of the green channel, which makes the histogram of the image uniform. The best kappa score we managed to get on the dataset preprocessed this way was only 0.4. We used prefix &lt;code class=&quot;highlighter-rouge&quot;&gt;ge&lt;/code&gt; for these images.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;img src=&quot;/public/2015-08-17/g-99_left-3.jpeg&quot; alt=&quot;Just the green channel: g&quot; title=&quot;Just the green channel: g&quot; /&gt;&lt;/th&gt;
      &lt;th&gt;&lt;img src=&quot;/public/2015-08-17/ge-99_left-3.jpeg&quot; alt=&quot;Histogram equalization on top of the green channel: ge&quot; title=&quot;Histogram equalization on top of the green channel: ge&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Just the green channel: &lt;code class=&quot;highlighter-rouge&quot;&gt;g&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;Histogram equalization on top of the green channel: &lt;code class=&quot;highlighter-rouge&quot;&gt;ge&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;data-augmentation&quot;&gt;Data augmentation&lt;/h2&gt;
&lt;p&gt;One of the problems of neural networks is that they are extremely powerful. They learn so well that they usually learn something that degrades their performance on other (previously unseen) data. One (made-up) example: the images in the training set are taken by different cameras and have different characteristics. If for some reason, say, the percentage of images of level 2 in dark images is higher than in general, the network may start to predict level 2 more often for dark images. We are not aware of any way to detect such “misleading” correlations by looking at neuron activations of convolution filters. But, fortunately, it is possible to train the network on one subset of data and test it on another, and if the performance on these subsets are different, then the network has learned something very specific to the training data, it has &lt;strong&gt;overfit&lt;/strong&gt; the training data, and we should try to avoid it.&lt;/p&gt;

&lt;p&gt;One of the solutions to this problem is to enlarge the dataset in order to minimize the chances of such correlations to happen. This is called &lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=Km1Q5VcSKAg&amp;amp;index=77&amp;amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&quot;&gt;data augmentation&lt;/a&gt;&lt;/em&gt;. The organizers of this contest explicitly &lt;a href=&quot;https://www.kaggle.com/c/diabetic-retinopathy-detection/rules&quot;&gt;forbid&lt;/a&gt; to use data outside the dataset they provided. But it’s obvious that if you take an image, zoom it, rotate it, flip it, change the brightness etc. the level of the disease will not be changed. So it is possible to apply these transformations to the images and obtain much larger and “more random” training dataset. One approach is to take all versions of all images into the training set, another approach is to randomly choose one transformation for each of the images. The mixture of these approaches helps to solve another problem which will be discussed in the next section.&lt;/p&gt;

&lt;p&gt;We applied very limited transformations only. For every image we created 4 samples: original, rotated by 180 degrees, and the vertical flipped versions of these two. This helped to avoid the problem, that some of the images in the dataset &lt;a href=&quot;https://www.kaggle.com/c/diabetic-retinopathy-detection/data&quot;&gt;were flipped&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We believe that we spent way too little time on data augmentation. All other contestants we have seen use much more sophisticated transformations. Probably this was our most important mistake.&lt;/p&gt;

&lt;h2 id=&quot;choosing-training--validation-sets&quot;&gt;Choosing training / validation sets&lt;/h2&gt;
&lt;p&gt;There are two reasons to train the networks only on a subset of the train dataset provided by Kaggle. First reason is to be able to compare different models. We need to choose the model which generalizes best to the unseen data, not the one which performs best on the data it has been trained on. So we train various models on some subset of the dataset (again called a &lt;em&gt;training set&lt;/em&gt;), then compare their performance on the other subset (called a &lt;em&gt;validation set&lt;/em&gt;) and pick the one which works better on the latter.&lt;/p&gt;

&lt;p&gt;The second reason is to detect overfitting while training. During the training we sometimes (in Caffe this is configured by the &lt;a href=&quot;http://caffe.berkeleyvision.org/tutorial/solver.html&quot;&gt;&lt;em&gt;test_interval&lt;/em&gt; parameter&lt;/a&gt;) run the network on the validation set and calculate the loss. When we see that the loss on the validation set does not decrease anymore, we know that overfitting happens. This is best illustrated in this &lt;a href=&quot;https://en.wikipedia.org/wiki/Overfitting#/media/File:Overfitting_svg.svg&quot;&gt;image from Wikipedia&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The distribution of images of different levels in the training set provided by Kaggle was very uneven. More than half of the images were of healthy eyes:&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Level&lt;/td&gt;
      &lt;td&gt;Number of images&lt;/td&gt;
      &lt;td&gt;Percentage&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;25810&lt;/td&gt;
      &lt;td&gt;73.48%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2443&lt;/td&gt;
      &lt;td&gt;6.95%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;5292&lt;/td&gt;
      &lt;td&gt;15.07%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;873&lt;/td&gt;
      &lt;td&gt;2.49%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;708&lt;/td&gt;
      &lt;td&gt;2.02%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Neural networks seem to be very sensitive to this kind of distributions. Our very first neural network (using softmax classification) was randomly giving labels 0 and 2 to almost all images (which brought a kappa score 0.138). So we had to make the classes more or less equal. Here we did couple of trivial mistakes.&lt;/p&gt;

&lt;p&gt;At first we augmented the dataset by creating lots of rotations (multiples of 30 degrees, 12 versions of each image) and created a dataset of around 100K images with equally distributed classes. So we took 36 times more versions of images of level 4 than of images of level 0. As we had only 12 versions of each image, we took every image 3 times. Finally, we separated the training and validation sets &lt;em&gt;after&lt;/em&gt; these augmentations. After training 88000 iterations (with batch size 2, we were still on GeForce 550 Ti) we had 0.55 kappa score on our validation set. But on Kaggle’s test set the score was only 0.23. So we had a terrible overfitting and didn’t detect it locally.&lt;/p&gt;

&lt;p&gt;The most important point here, as I understand it, is that the separation of training and validation sets should have been done &lt;em&gt;before&lt;/em&gt; the data augmentation. In our case we had different rotations of the same image in both sets, which didn’t allow us to detect overfitting.&lt;/p&gt;

&lt;p&gt;So later we took 7472 images (21%) as a validation set, and performed the data augmentation on the remaining 27654 images. Validation set had the same ratio of classes as the Kaggle’s test set. This is important for choosing the best model: validation set should be similar to the test set as much as possible.&lt;/p&gt;

&lt;p&gt;Also we decided to get rid off the rotations by multiples of 30 degrees, as the images were being distorted (we applied rotations &lt;em&gt;after&lt;/em&gt; resizing the images). Although, after the competition we saw that &lt;a href=&quot;http://jeffreydf.github.io/diabetic-retinopathy-detection/&quot;&gt;other contestants&lt;/a&gt; have used such rotations. So maybe this was another mistake.&lt;/p&gt;

&lt;p&gt;Then, it turned out that the idea of taking copies of the same image is terrible, because the network overfits the smaller classes (like level 3 and level 4) and it is hard to notice that just by looking at validation loss values, because the corresponding classes are very small in the validation set. We identified this problem by carefully visualizing neuron activations on training and validation sets (just 2 weeks before the competition deadline):&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;img src=&quot;/public/2015-08-17/3-4-overfit.png&quot; alt=&quot;Blue dots are from the training set, orange dots are from the validation set. x axis is the activation of a top layer neuron. y axis is the original label (0 to 4)&quot; title=&quot;Blue dots are from the training set, orange dots are from the validation set. x axis is the activation of a top layer neuron. y axis is the original label (0 to 4)&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Every dot corresponds to one image. Blue dots are from the training set, orange dots are from the validation set. &lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt; axis is the activation of a top layer neuron. &lt;code class=&quot;highlighter-rouge&quot;&gt;y&lt;/code&gt; axis is the original label (0 to 4). Basically there is no overfitting for the images of level 0, 1 or 2: the activations are very similar. But the overfitting of the images of level 3 and 4 is obvious. Training samples are concentrated around fixed values, while validation samples are spread widely&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Finally we decided to train a network to differentiate between two classes only: images of level 0 and 1 versus images of level 2, 3 and 4. The ratio of the images in these classes was 4:1. We augmented the training set only by vertical flipping and rotating by 180 degrees. We took all 4 versions of each image of the second class and we randomly took one of the 4 versions of each image of the first class. This way we ended up with a training set of two equal classes. This gave us our best kappa score 0.50.&lt;/p&gt;

&lt;p&gt;Later we wanted to train a classifier which would differentiate level 0 images from level 1 images only, but the networks we tried didn’t work at all. Another classifier we used to differentiate between level 2 and level 3 + level 4 images actually learned something, but we couldn’t increase the overall kappa score based on that.&lt;/p&gt;

&lt;p&gt;After preparing the list of files for the training and validation sets, we used a tool bundled with Caffe to create a &lt;a href=&quot;http://leveldb.org/&quot;&gt;LevelDB&lt;/a&gt; database from the directory of images. Caffe &lt;a href=&quot;http://caffe.berkeleyvision.org/tutorial/data.html&quot;&gt;prefers&lt;/a&gt; to read from LevelDB rather than from directory:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;./build/tools/convert_imageset &lt;span class=&quot;nt&quot;&gt;-backend&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;leveldb &lt;span class=&quot;nt&quot;&gt;-gray&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-shuffle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true &lt;/span&gt;data/train.g/ train.g.01v234.txt leveldb/train.g.01v234&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;gray&lt;/code&gt; is set to &lt;code class=&quot;highlighter-rouge&quot;&gt;true&lt;/code&gt; because we use single-channel images and &lt;code class=&quot;highlighter-rouge&quot;&gt;shuffle&lt;/code&gt; is required to properly shuffle the images before importing into the database.&lt;/p&gt;

&lt;h2 id=&quot;convolutional-network-architecture&quot;&gt;Convolutional network architecture&lt;/h2&gt;

&lt;p&gt;Our best performing &lt;a href=&quot;https://github.com/YerevaNN/Kaggle-diabetic-retinopathy-detection/blob/master/g_01v234_40r-2-40r-2-40r-2-40r-4-256rd0.5-256rd0.5.prototxt&quot;&gt;neural network architecture&lt;/a&gt; and corresponding &lt;a href=&quot;https://github.com/YerevaNN/Kaggle-diabetic-retinopathy-detection/blob/master/best-performing-solver.prototxt&quot;&gt;solver&lt;/a&gt; are on Github. &lt;code class=&quot;highlighter-rouge&quot;&gt;Batch size&lt;/code&gt; was always fixed to 20 (on GTX 980 card). We used a simple &lt;em&gt;stochastic gradient descent&lt;/em&gt; with 0.9 &lt;code class=&quot;highlighter-rouge&quot;&gt;momentum&lt;/code&gt; and didn’t touch learning rate policy at all (it didn’t decrease the rate significantly). We started at 0.001 &lt;code class=&quot;highlighter-rouge&quot;&gt;learning rate&lt;/code&gt;, and sometimes manually decreased it (but not in this particular case which brought the best kappa score). Also in this best performing case we started with 0 &lt;code class=&quot;highlighter-rouge&quot;&gt;weight decay&lt;/code&gt;, and after the first signs of overfitting (after 48K iterations, which is almost 20 epochs) increased it to 0.0015.&lt;/p&gt;

&lt;p&gt;Convolution was done similar to the “traditional” &lt;a href=&quot;http://caffe.berkeleyvision.org/gathered/examples/mnist.html&quot;&gt;LeNet architecture&lt;/a&gt; (developed by &lt;a href=&quot;http://yann.lecun.com/&quot;&gt;Yann LeCun&lt;/a&gt;, who invented the convolutional networks): one max pooling layer after every convolution layer, with fully connected layers at the end.&lt;/p&gt;

&lt;p&gt;Almost all other contestants used the other famous approach, with multiple consecutive convolutional layers with small kernels before a pooling layer. This was developed by &lt;a href=&quot;http://www.robots.ox.ac.uk/~vgg/research/very_deep/&quot;&gt;Karen Simonyan and Andrew Zisserman&lt;/a&gt; at Visual Geometry Group, University of Oxford (that’s why it is called &lt;em&gt;VGGNet&lt;/em&gt; or &lt;em&gt;OxfordNet&lt;/em&gt;) for the &lt;a href=&quot;http://www.image-net.org/challenges/LSVRC/2014/results#clsloc&quot;&gt;ImageNet 2014 contest&lt;/a&gt; where they took 1st and 2nd places for localization and classification tasks, respectively. Their approach was popularized by &lt;a href=&quot;http://cs231n.github.io/convolutional-networks/#case&quot;&gt;Andrej Karpathy&lt;/a&gt; and was successfully used in the &lt;a href=&quot;http://benanne.github.io/2015/03/17/plankton.html#architecture&quot;&gt;plankton classification contest&lt;/a&gt;. I have tried this approach once, but it required significantly more memory and time, so I quickly abandoned it.&lt;/p&gt;

&lt;p&gt;Here is the structure of our network:&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Nr&lt;/td&gt;
      &lt;td&gt;Type&lt;/td&gt;
      &lt;td&gt;Batches&lt;/td&gt;
      &lt;td&gt;Channels&lt;/td&gt;
      &lt;td&gt;Width&lt;/td&gt;
      &lt;td&gt;Height&lt;/td&gt;
      &lt;td&gt;Kernel size / stride&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;Input&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;512&lt;/td&gt;
      &lt;td&gt;512&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Conv&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;506&lt;/td&gt;
      &lt;td&gt;506&lt;/td&gt;
      &lt;td&gt;7x7 / 1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;ReLU&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;506&lt;/td&gt;
      &lt;td&gt;506&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;MaxPool&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;253&lt;/td&gt;
      &lt;td&gt;253&lt;/td&gt;
      &lt;td&gt;3x3 / 2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;Conv&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;249&lt;/td&gt;
      &lt;td&gt;249&lt;/td&gt;
      &lt;td&gt;5x5 / 1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;ReLU&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;249&lt;/td&gt;
      &lt;td&gt;249&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;MaxPool&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;124&lt;/td&gt;
      &lt;td&gt;124&lt;/td&gt;
      &lt;td&gt;3x3 / 2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;Conv&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;120&lt;/td&gt;
      &lt;td&gt;120&lt;/td&gt;
      &lt;td&gt;5x5 / 1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;ReLU&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;120&lt;/td&gt;
      &lt;td&gt;120&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;MaxPool&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;60&lt;/td&gt;
      &lt;td&gt;60&lt;/td&gt;
      &lt;td&gt;3x3 / 2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;Conv&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;56&lt;/td&gt;
      &lt;td&gt;56&lt;/td&gt;
      &lt;td&gt;5x5 / 1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;ReLU&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;56&lt;/td&gt;
      &lt;td&gt;56&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;MaxPool&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;4x4 / 4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;Fully connected&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;256&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;ReLU&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;256&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;Dropout&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;256&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;Fully connected&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;256&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;17&lt;/td&gt;
      &lt;td&gt;ReLU&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;256&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;18&lt;/td&gt;
      &lt;td&gt;Dropout&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;256&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;19&lt;/td&gt;
      &lt;td&gt;Fully connected&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;Euclidean Loss&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Some observations related to the network architecture:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Rectifier_(neural_networks)&quot;&gt;ReLU activations&lt;/a&gt; on all convolutional and fully connected layers helped a lot, kappa score increased by almost 0.1. It’s interesting to note that Christian Szegedy, one of the GoogLeNet developers (winner of the classification contest at ImageNet 2014), &lt;a href=&quot;https://www.youtube.com/watch?v=ySrj_G5gHWI&quot;&gt;expressed an opinion&lt;/a&gt; that the main reason for the deep learning revolution happening now is the ReLU function :)&lt;/li&gt;
  &lt;li&gt;2 fully connected layers (256 neurons each) at the end is better than one fully connected layer. Kappa was increased by almost 0.03&lt;/li&gt;
  &lt;li&gt;Number of filters in the convolutional layers are not very important. Difference between, say, 20 and 40 filters is very little&lt;/li&gt;
  &lt;li&gt;Dropout helps fight overfitting (we used 50% probability everywhere)&lt;/li&gt;
  &lt;li&gt;We didn’t notice any difference with Local response normalization layers&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Below are the 40 filters of the first convolutional layer of our best model (visualization code is adapted from &lt;a href=&quot;http://nbviewer.ipython.org/github/BVLC/caffe/blob/master/examples/00-classification.ipynb&quot;&gt;here&lt;/a&gt;). They don’t seem to be very meaningful:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/2015-08-17/convolutional-filters.png&quot; alt=&quot;Filters of the 1st convolutional layer&quot; title=&quot;Filters of the 1st convolutional layer&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I tried to use dropout on convolutional layers as well, but couldn’t make the network learn anything. The loss was quickly becoming &lt;code class=&quot;highlighter-rouge&quot;&gt;nan&lt;/code&gt;. Probably the learning rate should have been very different…&lt;/p&gt;

&lt;h2 id=&quot;loss-function&quot;&gt;Loss function&lt;/h2&gt;
&lt;p&gt;Submissions of this contest were evaluated by the metric called &lt;strong&gt;quadratic weighted kappa&lt;/strong&gt;. We found an &lt;a href=&quot;http://www.real-statistics.com/reliability/weighted-cohens-kappa/&quot;&gt;Excel code&lt;/a&gt; that implements it which helped us to get some intuition.&lt;/p&gt;

&lt;p&gt;At the beginning we started to use &lt;a href=&quot;http://caffe.berkeleyvision.org/doxygen/classcaffe_1_1SoftmaxWithLossLayer.html&quot;&gt;softmax loss&lt;/a&gt; on top of the 5 neurons of the final fully connected layer. Later we decided to use something that will take into account the fact that the order of the labels matters (0 and 1 are closer than 0 and 4). We left only one neuron in the last layer and tried to use &lt;a href=&quot;http://caffe.berkeleyvision.org/doxygen/classcaffe_1_1EuclideanLossLayer.html&quot;&gt;Euclidean loss&lt;/a&gt;. We even tried to “scale” the labels of the images in a way that will make it closer to being “quadratic”: we replaced the labels [0,1,2,3,4] with [0,2,3,4,6].&lt;/p&gt;

&lt;p&gt;Ideally we would like to have a loss function that implements the kappa metric. But we didn’t risk to implement a new layer in Caffe. &lt;a href=&quot;http://jeffreydf.github.io/diabetic-retinopathy-detection/#the-opening&quot;&gt;Jeffrey De Fauw&lt;/a&gt; has implemented some continuous approximation of kappa metric using Theano with a lot of success.&lt;/p&gt;

&lt;p&gt;When we switched to 0,1 vs 2,3,4 classification, I thought 2-neuron softmax would be better than Euclidean loss because of the second neuron: it might bring some information that could help to obtain better score. But after some tests I saw that the sum of the activations of the two softmax neurons tends to 1, so the second neuron does not bring new information. The rest of the training was done using Euclidean loss (although I am not sure if that was the best option).&lt;/p&gt;

&lt;p&gt;We logged the output of Caffe into a file, then plotted the graphs of training and validation losses using a &lt;a href=&quot;https://github.com/YerevaNN/Caffe-python-tools/blob/master/plot_loss.py&quot;&gt;Python script&lt;/a&gt; written by Hrayr:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;./build/tools/caffe train &lt;span class=&quot;nt&quot;&gt;-solver&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;solver.prototxt &amp;amp;&amp;gt; log_g_g_01v234_40r-2-40r-2-40r-2-40r-4-256rd0.5-256rd0.5-wd0-lr0.001.txt

python plot_loss.py log_g_01v234_40r-2-40r-2-40r-2-40r-4-256rd0.5-256rd0.5-wd0-lr0.001.txt&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The script allows to print multiple logs on the same image and uses &lt;code class=&quot;highlighter-rouge&quot;&gt;moving average&lt;/code&gt; to make the graph look smoother. It correctly aligns the graphs even if the log does not start from the first iteration (in case the training is resumed from a Caffe snapshot). For example, in the plot below &lt;code class=&quot;highlighter-rouge&quot;&gt;train 1&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;val 1&lt;/code&gt; correspond to the model described in the previous section with &lt;code class=&quot;highlighter-rouge&quot;&gt;weight decay=0&lt;/code&gt;; &lt;code class=&quot;highlighter-rouge&quot;&gt;train 2&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;val 2&lt;/code&gt; correspond to the model which started from the 48000th iteration of the previous model but used &lt;code class=&quot;highlighter-rouge&quot;&gt;weight decay=0.0015&lt;/code&gt;. The best kappa score was obtained on 81000th iteration of the second model. Then we observe overfitting.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/2015-08-17/log_g_01v234_40r-2-40r-2-40r-2-40r-4-256rd0.5-256rd0.5-wd0-lr0.001.txt.png&quot; alt=&quot;Training and validation losses for our best model&quot; title=&quot;Training and validation losses for our best model&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note that the validation loss is usually lower than the training loss. The reason is that the classes are equal in the training set and are far from being equal in the validation set. So the training and validation losses cannot be compared.&lt;/p&gt;

&lt;h2 id=&quot;preparing-submissions&quot;&gt;Preparing submissions&lt;/h2&gt;
&lt;p&gt;After training the models we used a &lt;a href=&quot;https://github.com/YerevaNN/Caffe-python-tools/blob/master/predict_regression.py&quot;&gt;Python script&lt;/a&gt; to make predictions for the images in validation set. It creates a CSV file with neuron activations. Then we imported this CSV into Wolfram Mathematica and played with it there.&lt;/p&gt;

&lt;p&gt;I use Mathematica mainly because of its nice visualizations. Here is one of them: the &lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt; axis is the activation of the single neuron of the last layer, and the graphs present the percentages of the images of each particular label that have &lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt; activation. Ideally the graphs corresponding to different labels should be clearly separable by vertical lines. Unfortunately that’s not the case, which visually explains why the kappa score is so low.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/2015-08-17/best-model-graphs.png&quot; alt=&quot;Percentage of images per given neuron activation&quot; title=&quot;Percentage of images per given neuron activation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In order to convert the neuron activations to predicted levels we need to determine 4 “threshold” numbers. These graphs show that it’s not obvious how to choose these 4 numbers in order to maximize the kappa score. So we take, say, 1000 random 4-tuples of numbers between minimum and maximum activations of the neuron, and calculate the kappa score for each of the tuples. Then we take the 4-tuple for which the kappa was maximal, and use these numbers as thresholds for the images in the test set.&lt;/p&gt;

&lt;p&gt;Note that we calculate the kappa scores for the validation set, although there is a risk to overfit the validation set. Ideally we should choose those thresholds which attain maximum kappa score on the train set. But, in practice, the thresholds that maximize the kappa score on validation set perform better on the test set, mainly because the network has already overfit the training set!&lt;/p&gt;

&lt;h2 id=&quot;attempts-to-ensemble&quot;&gt;Attempts to ensemble&lt;/h2&gt;
&lt;p&gt;Usually it is possible to improve the scores by merging several models. This is called &lt;a href=&quot;https://en.wikipedia.org/wiki/Ensemble_learning&quot;&gt;ensembling&lt;/a&gt;. For example, the 3rd place winners of this contest have merged the results of 9 convolutional networks.&lt;/p&gt;

&lt;p&gt;We developed couple of ways to merge the results from two networks, but they didn’t work well for us. They gave very small improvements (less than 0.01) only when both networks gave similar kappa scores. When one network was clearly stronger than the other one, the ensemble didn’t help at all. One of our ensemble methods was an extension of the “thresholding” method described in the previous section to 2 dimensions. We plot the images on a 2D plane in a way that each of the coordinates corresponds to a neuron activation of one model. Then we looked for random lines that split the plane in a way that maximizes the kappa score. We tried two methods of splitting the plane which are demonstrated below. Each blue dot corresponds to an image of label 0, orange dots correspond of images having label 4.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/public/2015-08-17/model-merge-diagonals.png&quot; alt=&quot;Ensemble of two networks, threshold lines are diagonal&quot; title=&quot;Ensemble of two networks, threshold lines are diagonal&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/public/2015-08-17/model-merge-lines.png&quot; alt=&quot;Ensemble of two networks, threshold curves are perpendicular lines&quot; title=&quot;Ensemble of two networks, threshold curves are perpendicular lines&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We didn’t try to merge more than 2 networks at once. Probably this was another mistake.&lt;/p&gt;

&lt;p&gt;The only method of ensembling that worked for us was to take an average over 4 rotated / flipped versions of the images. We also tried to take minimum, maximum and harmonic mean of the neuron activations. Minimum and maximum brought 0.01 improvement to the kappa score, while harmonic and arithmetic means brought 0.02 improvement. The best result we achieved used the arithmetic mean. Note that this required to have 4 versions of test images (which took 2 days to rotate / flip) and to run the network on all versions (which took another day).&lt;/p&gt;

&lt;p&gt;All these experiments can be replicated in Mathematica by using the script &lt;code class=&quot;highlighter-rouge&quot;&gt;main.nb&lt;/code&gt; and the required CSV files that are &lt;a href=&quot;https://github.com/YerevaNN/Kaggle-diabetic-retinopathy-detection/tree/master/mathematica&quot;&gt;available on Github&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Finally, note that Mathematica is the only non-free software used in the whole training process. We believe it is better to keep the ecosystem clean :) We will probably use &lt;a href=&quot;http://ipython.org/&quot;&gt;IPython&lt;/a&gt; next time.&lt;/p&gt;

&lt;h2 id=&quot;more-on-this-contest&quot;&gt;More on this contest&lt;/h2&gt;
&lt;p&gt;Many contestants have published their solutions. Here are the ones I could find. Please, let me know if I missed something. Most of the solution are heavily influenced by the winner method of the plankton classification contest.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1st place: &lt;a href=&quot;https://www.kaggle.com/c/diabetic-retinopathy-detection/forums/t/15801/competition-report-min-pooling-and-thank-you&quot;&gt;Min-Pooling&lt;/a&gt; used OpenCV to preprocess the images, augmented the dataset by scaling, skewing and rotating (and notably not by changing colors), trained several networks on his own &lt;a href=&quot;https://github.com/btgraham/SparseConvNet&quot;&gt;SparseConvNet&lt;/a&gt; library and used random forests to combine predictions from two eyes of the same person. Kappa = 0.84958&lt;/li&gt;
  &lt;li&gt;2nd place: &lt;a href=&quot;https://www.kaggle.com/c/diabetic-retinopathy-detection/forums/t/15807/team-o-o-competition-report-and-code&quot;&gt;o_O team&lt;/a&gt; used Theano, Lasagne, nolearn to train OxfordNet-like network on minimal preprocessed images. They have heavily augmented the dataset. They note the importance of using larger images to achieve high scores. Kappa = 0.84479&lt;/li&gt;
  &lt;li&gt;3rd place: &lt;a href=&quot;https://www.kaggle.com/c/diabetic-retinopathy-detection/forums/t/15845/3rd-place-solution-report&quot;&gt;Reformed Gamblers team&lt;/a&gt; combined results of 9 convolutional networks (OxfordNet-like and others) with leaky ReLU activations and non-trivial loss functions. They used Torch on multiple GPUs. Kappa = 0.83937&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Update:&lt;/strong&gt; 4th place: Julian and Daniel &lt;a href=&quot;http://blog.kaggle.com/2015/08/14/diabetic-retinopathy-winners-interview-4th-place-julian-daniel/&quot;&gt;gave an interview&lt;/a&gt; to Kaggle. They did extensive preprocessing and data augmentation, used CXXNet, PyLearn and Keras to train multiple OxfordNet-like networks. They highlight the importance of good parameter initialization.&lt;/li&gt;
  &lt;li&gt;5th place: &lt;a href=&quot;http://jeffreydf.github.io/diabetic-retinopathy-detection/&quot;&gt;Jeffrey De Fauw&lt;/a&gt; used Theano to train OxfordNet-like network with leaky ReLU activations on significantly augmented dataset. He has also implemented a smooth approximation of kappa metric and used it as a loss layer. Well written blog post. Kappa = 0.82899&lt;/li&gt;
  &lt;li&gt;20th place: &lt;a href=&quot;http://ilyakava.tumblr.com/post/125230881527/my-1st-kaggle-convnet-getting-to-3rd-percentile&quot;&gt;Ilya Kavalerov&lt;/a&gt;, again Theano, OxfordNet, good augmentation, non-obvious loss function. Interesting read. Kappa = 0.76523&lt;/li&gt;
  &lt;li&gt;46th place: &lt;a href=&quot;https://nikogamulin.github.io/2015/07/31/Diabetic-retinopathy-detection-with-convolutional-neural-network.html&quot;&gt;Niko Gamulin&lt;/a&gt; used Caffe on GTX 980 GPU (just like us) but OxfordNet architecture. Kappa = 0.63129&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After the contest we tried to use leaky ReLUs, something we just didn’t think of during the contest. The results are not promising. Here are the plots of the validation losses with negative slope values (&lt;code class=&quot;highlighter-rouge&quot;&gt;ns&lt;/code&gt;) 0, 0.01, 0.33 and 0.5 respectively:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/2015-08-17/leaky-ReLU.png&quot; alt=&quot;Validation losses using leaky ReLU activations&quot; title=&quot;Validation losses using leaky ReLU activations&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Finally, Hrayr suggested to use different learning rates for different convolutional layers (Caffe supports this by specifying multiplication constants per layer). He used larger coefficients (12) for the first layers than for the top layers. The full prototxt file is on &lt;a href=&quot;https://github.com/YerevaNN/Kaggle-diabetic-retinopathy-detection/blob/master/g_01v234_32r-2-64r-2-64r-2-128r-2-128r-2-256r-2-512rd0.5-256rd0.5_manual_learning_rates.prototxt&quot;&gt;Github&lt;/a&gt;. This network allowed to get up to 0.52 kappa score on the local validation set. We didn’t try to run it on test images, although in almost all cases our scores on private leaderboard were higher than the scores on local validation sets.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;We would like to express gratitude to Hugo Larochelle for his excellent video course on neural networks. After watching the videos we could easily understand almost all the terms in Caffe documentation.&lt;/p&gt;

&lt;p&gt;We would like to thank the organizers of the contest for a great competition and the contestants for helpful discussions in forums and published solutions. We learned a lot from this contest.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Getting started with neural networks</title>
   <link href="http://yerevann.github.io//2015/07/30/getting-started-with-neural-networks/"/>
   <updated>2015-07-30T00:00:00+00:00</updated>
   <id>http://yerevann.github.io//2015/07/30/getting-started-with-neural-networks</id>
   <content type="html">&lt;h2 id=&quot;who-we-are&quot;&gt;Who we are&lt;/h2&gt;

&lt;p&gt;We are a group of students from the department of &lt;a href=&quot;http://ysu.am/faculties/en/Informatics-and-Applied-Mathematics&quot;&gt;Informatics and Applied Mathematics&lt;/a&gt; at &lt;a href=&quot;http://ysu.am/main/en&quot;&gt;Yerevan State University&lt;/a&gt;. In 2014, inspired by successes of neural nets in various fields, especially by &lt;a href=&quot;http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/&quot;&gt;GoogLeNet’s&lt;/a&gt; excellent performance in ImageNet 2014, we decided to dive into the topic of neural networks. We study calculus, combinatorics, graph theory, algebra and many other topics in the university but we learn nothing about machine learning. Just a few students take some &lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/info&quot;&gt;ML courses&lt;/a&gt; from Coursera or elsewhere.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;choosing-a-video-course&quot;&gt;Choosing a video course&lt;/h2&gt;
&lt;p&gt;At the beginning of 2015 the &lt;a href=&quot;http://ysu.am/sss/en&quot;&gt;Student Scientific Society&lt;/a&gt; of the department initiated a project to study neural networks. We had to choose some video course on the internet, then watch and discuss the videos once per week in the university. We wanted a course that would cover everything from the very basics to convolutional networks and deep learning. We followed &lt;a href=&quot;http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html&quot;&gt;Yoshua Bengio’s&lt;/a&gt; advice given during his &lt;a href=&quot;http://www.reddit.com/r/MachineLearning/comments/1ysry1/ama_yoshua_bengio&quot;&gt;interview on Reddit&lt;/a&gt; and chose &lt;a href=&quot;https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&quot;&gt;this excellent class&lt;/a&gt; by &lt;a href=&quot;http://www.dmi.usherb.ca/~larocheh/index_en.html&quot;&gt;Hugo Larochelle&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Hugo’s lectures are really great. First two chapters teach the &lt;a href=&quot;https://www.youtube.com/watch?v=SGZ6BttHMPw&amp;amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;amp;index=1&quot;&gt;basic structure&lt;/a&gt; of neural networks and describe the &lt;a href=&quot;https://www.youtube.com/watch?v=5adNQvSlF50&amp;amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;amp;index=7&quot;&gt;backpropagation&lt;/a&gt; algorithm in details. We loved that he showed the derivation of the gradients of the loss function. Because of this, &lt;a href=&quot;https://github.com/Harhro94&quot;&gt;Hrayr&lt;/a&gt; managed to implement a simple multilayer neural net on his own. Next two chapters (which we skipped) talk about Conditional Random Fields. The fifth chapter introduces unsupervised learning with &lt;a href=&quot;https://www.youtube.com/watch?v=p4Vh_zMw-HQ&amp;amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;amp;index=36&quot;&gt;Restricted Boltzmann Machines&lt;/a&gt;. This was the hardest part for us, mainly because of our lack of knowledge in probabilistic graphical models. The sixth chapter on &lt;a href=&quot;https://www.youtube.com/watch?v=FzS3tMl4Nsc&amp;amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;amp;index=44&quot;&gt;autoencoders&lt;/a&gt; is our favorite: the magic of denoising autoencoders is very surprising. Then there are chapters on &lt;a href=&quot;https://www.youtube.com/watch?v=vXMpKYRhpmI&amp;amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;amp;index=51&quot;&gt;deep learning&lt;/a&gt;, another unsupervised learning technique called &lt;a href=&quot;https://www.youtube.com/watch?v=7a0_iEruGoM&amp;amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;amp;index=60&quot;&gt;sparse coding&lt;/a&gt; (which we also skipped due to time limits) and &lt;a href=&quot;https://www.youtube.com/watch?v=rxKrCa4bg1I&amp;amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;amp;index=69&quot;&gt;computer vision&lt;/a&gt; (with strong emphasis on convolutional networks). The last chapter is about &lt;a href=&quot;https://www.youtube.com/watch?v=OzZIOiMVUyM&amp;amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;amp;index=79&quot;&gt;natural language processing&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/2015-07-30/denoising-autoencoder-slide.png&quot; alt=&quot;Denoising autoencoders&quot; title=&quot;A slide on denoising autoencoders from Hugo Larochelle's video course&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The lectures contain lots of references to papers and demonstrations, the slides are full of visualizations and graphs, and, last but not least, Hugo kindly answers all questions posed in the comments of Youtube videos. After watching the chapter on convolutional networks we decided to apply what we learned on some computer vision contest. We looked at the list of active competitions on Kaggle and the only one related to computer vision was the &lt;a href=&quot;https://www.kaggle.com/c/diabetic-retinopathy-detection&quot;&gt;Diabetic retinopathy detection contest&lt;/a&gt;. It seemed to be very hard as a first project in neural nets, but we decided to try. We’ll describe our experience with this contest in the next post.&lt;/p&gt;
</content>
 </entry>
 

</feed>
