<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>EFAVDB</title><link href="https://efavdb.com/" rel="alternate"></link><link href="https://efavdb.com/feeds/all.atom.xml" rel="self"></link><id>https://efavdb.com/</id><updated>2022-08-07T00:00:00-07:00</updated><subtitle>Everybody's Favorite Data Blog</subtitle><entry><title>Spaced repetition can allow for infinite recall</title><link href="https://efavdb.com/memory%20recall" rel="alternate"></link><published>2022-08-07T00:00:00-07:00</published><updated>2022-08-07T00:00:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2022-08-07:/memory recall</id><summary type="html">&lt;p align="center"&gt;
         &lt;img src="images/jeopardy.jpg"&gt;
&lt;/p&gt;

&lt;p&gt;My friend Andrew is an advocate of the &amp;#8220;spaced repetition&amp;#8221; technique for
memorization of a great many facts [1].  The ideas behind this are&amp;nbsp;two-fold:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;When one first &amp;#8220;learns&amp;#8221; a new fact, it needs to be reviewed frequently in
  order to not forget it.  However, with each additional review, the …&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p align="center"&gt;
         &lt;img src="images/jeopardy.jpg"&gt;
&lt;/p&gt;

&lt;p&gt;My friend Andrew is an advocate of the &amp;#8220;spaced repetition&amp;#8221; technique for
memorization of a great many facts [1].  The ideas behind this are&amp;nbsp;two-fold:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;When one first &amp;#8220;learns&amp;#8221; a new fact, it needs to be reviewed frequently in
  order to not forget it.  However, with each additional review, the fact can
be retained longer before a refresher is needed to maintain it in&amp;nbsp;recall.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Because of this, one can maintain a large, growing body of facts in recall
  through daily review:  Each day, one need only review for ten minutes or so,
covering a small number of facts. The facts included should be sampled from the
full library in a way that prefers newer entries, but that also sprinkles in
older facts often enough so that none are ever forgotten.  Apps have been
written to intelligently take care of the sampling process for&amp;nbsp;us.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Taking this framework as correct motivates questioning exactly how far it can
be pushed:  &lt;em&gt;Would an infinitely-long-lived, but forgetful person be able to
recall an infinite number of facts using this method? &lt;/em&gt;  &lt;span class="math"&gt;\(\ldots\)&lt;/span&gt; Below, we
show that the answer is: &lt;em&gt;&lt;span class="caps"&gt;YES&lt;/span&gt;!&lt;/em&gt;&lt;/p&gt;
&lt;h5&gt;Proof:&lt;/h5&gt;
&lt;p&gt;We first posit that the number of days &lt;span class="math"&gt;\(T\)&lt;/span&gt; that a fact can be retained before
it needs to be reviewed grows as a power-law in &lt;span class="math"&gt;\(s\)&lt;/span&gt;, the number of times it&amp;#8217;s
been reviewed so&amp;nbsp;far,&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{1}\label{1}
T(s) \sim s^{\gamma},
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
with &lt;span class="math"&gt;\(\gamma &amp;gt; 0\)&lt;/span&gt;. With this assumption, if &lt;span class="math"&gt;\(N(t)\)&lt;/span&gt; facts are to be recalled
from &lt;span class="math"&gt;\(t\)&lt;/span&gt; days ago, one can show that the amount of work needed today to retain
these will go like (see appendix for a proof of this&amp;nbsp;line)&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{2}\label{2}
w(t) \sim \frac{N(t)}{t^{\gamma / (\gamma + 1)}}.
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
The total work needed today is then the sum of work needed for each past day&amp;#8217;s&amp;nbsp;facts,&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{3} \label{3}
W(total) = \int_1^{\infty} \frac{N(t)}{t^{\gamma / (\gamma + 1)}} dt.
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Now, each day we only have a finite amount of time to study.  However, the
above total work integral will diverge at large &lt;span class="math"&gt;\(t\)&lt;/span&gt; unless it decays faster
than &lt;span class="math"&gt;\(1/t\)&lt;/span&gt;.  To ensure this, we can limit the number of facts retained from
from &lt;span class="math"&gt;\(t\)&lt;/span&gt; days ago to go&amp;nbsp;as&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{4} \label{4}
N(t) \sim \frac{1}{t^{\epsilon}} \times \frac{1}{t^{1 / (\gamma + 1)}},
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; is some small, positive constant.  Plugging (\ref{4}) into
(\ref{3}) shows that we are guaranteed a finite required study time each day.
However, after &lt;span class="math"&gt;\(t\)&lt;/span&gt; days of study, the total number of facts retained scales&amp;nbsp;as&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
N_{total}(t) &amp;amp;\sim &amp;amp; \int_1^{t} N(t) dt \\
&amp;amp;\sim &amp;amp; \int_0^{t} \frac{1}{t^{1 / (\gamma + 1)}} \\
&amp;amp;\sim &amp;amp; t^{ \gamma / (\gamma + 1)}. \tag{5} \label{5}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Because we assume that &lt;span class="math"&gt;\(\gamma &amp;gt; 0\)&lt;/span&gt;, this grows without bound over time,
eventually allowing for an infinitely large&amp;nbsp;library.&lt;/p&gt;
&lt;p&gt;We conclude that &amp;#8212; though we can&amp;#8217;t remember a fixed number of facts from each
day in the past using spaced repetition &amp;#8212; we can ultimately recall an infinite
number of facts using this method.  To do this only requires that we gradually
curate our previously-introduced facts so that the scaling (\ref{4}) holds at
all&amp;nbsp;times.&lt;/p&gt;
&lt;h3&gt;Appendix: Proof of&amp;nbsp;(2)&lt;/h3&gt;
&lt;p&gt;Recall that we assume &lt;span class="math"&gt;\(N(s)\)&lt;/span&gt; facts have been reviewed exactly &lt;span class="math"&gt;\(s\)&lt;/span&gt; times.  On a
given day, the number of these that need to be reviewed then goes&amp;nbsp;like&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{A1}\label{A1}
W(s) \sim \frac{N(s)}{T(s)}.
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(T(s)\)&lt;/span&gt; is given in (\ref{1}).  This holds because each of the &lt;span class="math"&gt;\(N(s)\)&lt;/span&gt;
facts that have been studied &lt;span class="math"&gt;\(s\)&lt;/span&gt; times so far must be reviewed within &lt;span class="math"&gt;\(T(s)\)&lt;/span&gt;
days, or one will be forgotten.  During these &lt;span class="math"&gt;\(T(s)\)&lt;/span&gt; days, each will move to
having been reviewed &lt;span class="math"&gt;\(s+1\)&lt;/span&gt; times.&amp;nbsp;Therefore,&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{A2} \label{A2}
\frac{ds}{dt} &amp;amp;\sim &amp;amp; \frac{1}{T(s)}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Integrating this gives &lt;span class="math"&gt;\(s\)&lt;/span&gt; as a function of &lt;span class="math"&gt;\(t\)&lt;/span&gt;,&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{A3} \label{A3}
s \sim t^{1 / (\gamma + 1)}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;Plugging this last line and (1) into (A1), we get&amp;nbsp;(2).&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;[1] See Andrew&amp;#8217;s blog post on spaced repetition &lt;a
href="https://andrewjudson.com/spaced-repitition/2022/06/03/spaced-repitition.html"&gt;
here&lt;/a&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Case studies"></category></entry><entry><title>Counting the number of ways to make change for a trillion dollars</title><link href="https://efavdb.com/change" rel="alternate"></link><published>2021-11-22T09:24:00-08:00</published><updated>2021-11-22T09:24:00-08:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2021-11-22:/change</id><summary type="html">&lt;p&gt;There are four ways to make change for &lt;span class="math"&gt;\(N=10\)&lt;/span&gt; cents: &lt;span class="math"&gt;\(\{\)&lt;/span&gt;10 pennies; 1 nickel
and 5 pennies; 2 nickels; 1 dime&lt;span class="math"&gt;\(\}\)&lt;/span&gt;.  How many ways are there to make change
for one trillion dollars &amp;#8212; using just pennies, nickels, dimes, and quarters?
To answer this, we present here a hybrid dynamic …&lt;/p&gt;</summary><content type="html">&lt;p&gt;There are four ways to make change for &lt;span class="math"&gt;\(N=10\)&lt;/span&gt; cents: &lt;span class="math"&gt;\(\{\)&lt;/span&gt;10 pennies; 1 nickel
and 5 pennies; 2 nickels; 1 dime&lt;span class="math"&gt;\(\}\)&lt;/span&gt;.  How many ways are there to make change
for one trillion dollars &amp;#8212; using just pennies, nickels, dimes, and quarters?
To answer this, we present here a hybrid dynamic programming / analytic
strategy that allows us to count the number of ways &lt;span class="math"&gt;\(\mathbb{Q}(N)\)&lt;/span&gt; to make
change for any &lt;span class="math"&gt;\(N\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The result for one trillion&amp;nbsp;dollars?&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\mathbb{Q}(10^{14}) = 133333333333423333333333351000000000001\)&lt;/span&gt;.&lt;/p&gt;
&lt;h4&gt;Dynamic programming&amp;nbsp;solution&lt;/h4&gt;
&lt;p&gt;We begin by introducing a set of recursion relations that will directly allow
for a &amp;#8220;pure&amp;#8221; dynamic programming strategy for counting the number of ways to
make&amp;nbsp;change:&lt;/p&gt;
&lt;p&gt;Let, &lt;span class="math"&gt;\(\mathbb{Q}(N), \mathbb{D}(N), \mathbb{N}(N)\)&lt;/span&gt; and &lt;span class="math"&gt;\(\mathbb{P}(N)\)&lt;/span&gt; be the
number of ways to make change for &lt;span class="math"&gt;\(N\)&lt;/span&gt; cents allowing all four denominations,
allowing only up to dimes (i.e., excluding quarters), only up to nickels, and
finally only pennies, respectively.  Next, we note&amp;nbsp;that
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \nonumber
\mathbb{Q}(N) &amp;amp;=&amp;amp; \mathbb{D}(N) + \mathbb{D}(N-25) + \mathbb{D}(N - 50) +
\ldots \\
&amp;amp;\equiv &amp;amp; \mathbb{D}(N) + \mathbb{Q}(N-25). \tag{1} \label{q_iter}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Here, we have decomposed the number of ways to make change in terms of the
number of quarters included.  For example, the second term in the first line
above is the number of ways to make change when we have exactly &lt;span class="math"&gt;\(1\)&lt;/span&gt; quarter:
The count in this case is simply the number of ways we can make change for
&lt;span class="math"&gt;\(N-25\)&lt;/span&gt; cents, using only dimes, nickels, and pennies &amp;#8212; i.e.,
&lt;span class="math"&gt;\(\mathbb{D}(N-25)\)&lt;/span&gt;.&amp;nbsp;Similarly,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \nonumber
\mathbb{D}(N) &amp;amp;=&amp;amp; \mathbb{N}(N) +\mathbb{N}(N-10) + \mathbb{N}(N-20) + \ldots
\\
&amp;amp;\equiv &amp;amp; \mathbb{N}(N) + \mathbb{D}(N-10) \tag{2} \label{d_iter}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
and
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \nonumber
\mathbb{N}(N) &amp;amp;=&amp;amp; \mathbb{P}(N) + \mathbb{P}(N-5) + \mathbb{P}(N-10) + \ldots
\\
&amp;amp;\equiv &amp;amp; \mathbb{P}(N) + \mathbb{N}(N-5).  \tag{3}  \label{n_iter} 
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Finally, &lt;span class="math"&gt;\(\mathbb{P}(N) \equiv 1\)&lt;/span&gt; for all natural &lt;span class="math"&gt;\(N\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;It is straightforward to sum the above equations in reverse using dynamic
programming: We start by using the fact that &lt;span class="math"&gt;\(\mathbb{P}(N) = 1\)&lt;/span&gt; all natural
&lt;span class="math"&gt;\(N\)&lt;/span&gt;, then evaluate &lt;span class="math"&gt;\([\mathbb{N}(1), \ldots, \mathbb{N}(N)]\)&lt;/span&gt; on a computer using
the iterative equation (\ref{n_iter}).  Storing these values in memory, we can
then evaluate &lt;span class="math"&gt;\([\mathbb{D}(1), \ldots, \mathbb{D}(N)]\)&lt;/span&gt; using (\ref{d_iter}),
and then finally evaluate &lt;span class="math"&gt;\([\mathbb{Q}(1), \ldots, \mathbb{Q}(N)]\)&lt;/span&gt; using
(\ref{q_iter}).  A python program that carries out this strategy is given
below.  Using this we are able to evaluate that, e.g., the number of ways to
make change for one dollar is &lt;span class="math"&gt;\(\mathbb{Q}(100) = 242\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The program below is sufficient for &amp;#8220;practical&amp;#8221; &lt;span class="math"&gt;\(N\)&lt;/span&gt;, but we need to do better
to count the number of ways to make change for &lt;em&gt;one trillion dollars&lt;/em&gt;:
On my laptop, I can evaluate the result for one-hundred thousand dollars in
about &lt;span class="math"&gt;\(20\)&lt;/span&gt; seconds.  The runtime of our program scales linearly with &lt;span class="math"&gt;\(N\)&lt;/span&gt;
though, so it would take it about &lt;span class="math"&gt;\(2 \cdot 10^{7}\)&lt;/span&gt; seconds to get our target
result &amp;#8212; too&amp;nbsp;slow.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;ways_to_make_change&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;denominations_list&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;    This function calculates the number of ways to make change for N cents,&lt;/span&gt;
&lt;span class="sd"&gt;    using only the denominations included in the passed `denominations_list`.&lt;/span&gt;

&lt;span class="sd"&gt;    parameters&lt;/span&gt;
&lt;span class="sd"&gt;    ----------&lt;/span&gt;
&lt;span class="sd"&gt;    N: int&lt;/span&gt;
&lt;span class="sd"&gt;        The target number of cents we wish to make change for.&lt;/span&gt;

&lt;span class="sd"&gt;    denominations_list: list&lt;/span&gt;
&lt;span class="sd"&gt;        A list of denominations that we can use to make change.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;denom&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;denominations_list&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;counts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;denom&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;counts_lower_denoms&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;denom&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                &lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;counts_lower_denoms&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;denom&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;counts&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ways_to_make_change&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  

&lt;span class="c1"&gt;# output: 242&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h5&gt;Geometric interpretation, large &lt;span class="math"&gt;\(N\)&lt;/span&gt; limiting&amp;nbsp;form&lt;/h5&gt;
&lt;p&gt;&lt;img alt="![lattice]({static}/images/change_lattice.png)" src="https://efavdb.com/images/change_lattice.png"&gt;&lt;/p&gt;
&lt;p&gt;In this section, we briefly consider the geometry of our problem. The plot
above is a 3d visual of all the possible ways to make change for one dollar:
Each point here has an &lt;span class="math"&gt;\((x,y,z)\)&lt;/span&gt; position that encodes the number of nickels,
dimes, and quarters, respectively, in a particular change solution.  To get the
full solution for a particular lattice point, we calculate the sum &lt;span class="math"&gt;\(5 x + 10 y
+ 25 z\)&lt;/span&gt; &amp;#8212; subtracting this from &lt;span class="math"&gt;\(N=100\)&lt;/span&gt; gives the number of pennies that must
be included in its solution.  In this way, we see that we get a valid solution
at each non-negative lattice point whose coordinates give a sum that is not
larger than &lt;span class="math"&gt;\(100\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For general &lt;span class="math"&gt;\(N\)&lt;/span&gt;, we&amp;nbsp;have
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
\mathbb{Q}(N) = \text{# of lattice points: } 5 x + 10 y + 25 z \leq N 
\tag{4} \label{4}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt; 
where we are again counting all non-negative lattice points on a lattice spaced
along &lt;span class="math"&gt;\(x\)&lt;/span&gt; with distance &lt;span class="math"&gt;\(5\)&lt;/span&gt;,&amp;nbsp;etc.&lt;/p&gt;
&lt;p&gt;We note that the inequality (\ref{4}) defines a polytope, and that there is a
beautiful, pre-existing body of work &amp;#8212; based on the use of generating
functions &amp;#8212; that allows one to count the number of lattice points within an
arbitrary polytope &lt;span class="math"&gt;\([1]\)&lt;/span&gt;.  One can look up results from that line of work to
directly answer the change counting problem we are considering here.  However,
the hybrid dynamic programming / analytic approach that we present below &amp;#8212;
based on our recursion relations above &amp;#8212; is more tailored to our problem and
consequently easier to&amp;nbsp;derive.&lt;/p&gt;
&lt;p&gt;Before moving on, we point out that we can get a good approximation to the
number of lattice points at large &lt;span class="math"&gt;\(N\)&lt;/span&gt; by simply considering the volume of the
polytope:   The idea is to make use of the fact that the lattice points are
evenly spaced. To the find an approximation to the number that sit inside the
polytope then, we simply need to divide the polytope&amp;#8217;s volume (&lt;span class="math"&gt;\(\frac{1}{3!}
N^3\)&lt;/span&gt;) by the volume of each lattice cell (&lt;span class="math"&gt;\(5 \cdot 10 \cdot 25\)&lt;/span&gt;).  This gives&amp;nbsp;[2],
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
\mathbb{Q}(N) \sim \frac{1}{7500}N^3.
\tag{5} \label{5}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Eq. (\ref{5}) is the correct, leading behavior at large &lt;span class="math"&gt;\(N\)&lt;/span&gt;.  However, it&amp;#8217;s not
exact because of &amp;#8220;discreteness effects&amp;#8221; that occur at the boundary of our
polytope &amp;#8212; these determine whether specific, individual lattice points lie
inside the polytope or not.  These effects result in correction terms at
quadratic and lower powers of &lt;span class="math"&gt;\(N\)&lt;/span&gt;.  Our calculation below provides one method
for capturing the full set of&amp;nbsp;terms.&lt;/p&gt;
&lt;h5&gt;Exact&amp;nbsp;solution&lt;/h5&gt;
&lt;p&gt;To get the exact result for &lt;span class="math"&gt;\(\mathbb{Q}(N)\)&lt;/span&gt;, we will directly sum the
recursions relations (\ref{q_iter})-(\ref{n_iter}):  Recall from above, that we&amp;nbsp;have
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
\mathbb{P}(N) \equiv 1 \tag{6} \label{6}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
for any natural &lt;span class="math"&gt;\(N\)&lt;/span&gt;.  Plugging this into (\ref{n_iter}), we&amp;nbsp;obtain
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{7} \label{7}
\mathbb{N}(N) =  \lfloor \frac{N}{5} \rfloor + 1
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Notice that if we evaluate this at a series of points separated by &lt;span class="math"&gt;\(\Delta N =
5\)&lt;/span&gt;, we get back  what looks like a simple linear function of &lt;span class="math"&gt;\(N\)&lt;/span&gt; &amp;#8212; it
increases by &lt;span class="math"&gt;\(1\)&lt;/span&gt; with each increase of &lt;span class="math"&gt;\(5\)&lt;/span&gt; in &lt;span class="math"&gt;\(N\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Now, if we plug our last line into (\ref{d_iter}) we&amp;nbsp;obtain
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \nonumber
\mathbb{D}(N) &amp;amp;=&amp;amp; \left ( \lfloor \frac{N}{5} \rfloor + 1  \right) + \left(
\lfloor \frac{N  - 10 }{5} \rfloor  + 1\right) + \ldots \\ &amp;amp;=&amp;amp; \left ( \lfloor
\frac{N}{5}\rfloor  +  1 \right) + \left ( \lfloor \frac{N}{5}\rfloor  -  1
\right)  + \ldots \tag{8} \label{8}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
This is an arithmetic sequence that depends on &lt;span class="math"&gt;\(\lfloor \frac{N}{5} \rfloor\)&lt;/span&gt;.
Summing this series, we&amp;nbsp;obtain
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
\mathbb{D}(N) =
\begin{cases}
\frac{1}{4}  \left (\lfloor \frac{N}{5} \rfloor + 2 \right)^2, &amp;amp; \text{$\lfloor
\frac{N}{5} \rfloor$ even} \\
\frac{1}{4} \left (\lfloor \frac{N}{5}\rfloor  + 1 \right)\left (\lfloor
\frac{N}{5}\rfloor  + 3 \right ), &amp;amp;  \text{$\lfloor \frac{N}{5}\rfloor$ odd}
\tag{9} \label{9}
\end{cases}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Notice that in this case, if we evaluate the function at a series of points
each separated by &lt;span class="math"&gt;\(10\)&lt;/span&gt;, the result will again look like a simple, smooth
function &amp;#8212; this time a quadratic function of &lt;span class="math"&gt;\(N\)&lt;/span&gt;.  However, the particular
form of the quadratic depends on whether &lt;span class="math"&gt;\(\lfloor \frac{N}{5} \rfloor\)&lt;/span&gt; is even
or odd &amp;#8212; that is, the function skips back and forth between two different&amp;nbsp;quadratics.&lt;/p&gt;
&lt;p&gt;We are now at the point where we can solve for the full number of ways to make
change, &lt;span class="math"&gt;\(\mathbb{Q}(N)\)&lt;/span&gt;.  In principle, all we need to do is to plug
(\ref{9}) into (\ref{q_iter}), and sum as above.  This is straightforward
for a particular &lt;span class="math"&gt;\(N\)&lt;/span&gt;, but we&amp;#8217;d like a general formula.  If we work out a few
terms for any particular value of &lt;span class="math"&gt;\(N\)&lt;/span&gt;, we see that we will get alternating
terms:  perhaps starting with a term of the upper type from (\ref{9}), followed
by a term of the lower type, etc.  The sums of the upper form can be evaluated
analytically (e.g., by using the pyramid summation method), as can the sum of
the terms of the bottom form. For any starting point,  the result is a cubic in
&lt;span class="math"&gt;\(\lfloor \frac{N}{5} \rfloor\)&lt;/span&gt;.  However, a small bit of work shows that the
precise cubic form that applies depends on the value of &lt;span class="math"&gt;\(\lfloor \frac{N}{5}
\rfloor (\text{mod } 10)\)&lt;/span&gt;, so we have ten separate cubics to solve&amp;nbsp;for.&lt;/p&gt;
&lt;p&gt;Rather than laboriously evaluate the sums for each of the ten possible
remainders of &lt;span class="math"&gt;\(\lfloor \frac{N}{5} \rfloor\)&lt;/span&gt; modulo &lt;span class="math"&gt;\(10\)&lt;/span&gt;, we can simply carry
out polynomial fits to data that we get from our python implementation of the
dynamic programming approach.  For example, plugging the first four values of
&lt;span class="math"&gt;\(N\)&lt;/span&gt; that satisfy &lt;span class="math"&gt;\(\lfloor \frac{N}{5} \rfloor \equiv 0 (\text{mod } 10)\)&lt;/span&gt; into
our program, we&amp;nbsp;get
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \nonumber
\mathbb{Q}(1) &amp;amp;=&amp;amp; 1 \\ \nonumber
\mathbb{Q}(50) &amp;amp;=&amp;amp; 49 \\ \nonumber
\mathbb{Q}(100) &amp;amp;=&amp;amp; 242 \\
\mathbb{Q}(150) &amp;amp;=&amp;amp; 680
\tag{10} \label{10}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Fitting a cubic to this data &amp;#8212; which involves solving a system of linear
equations for the unknown coefficients of the polynomial &amp;#8212;&amp;nbsp;gives
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
\mathbb{Q}(N) = \frac{\lfloor \frac{N}{5}\rfloor ^3}{60}+\frac{9 \lfloor
\frac{N}{5}\rfloor ^2}{40}+\frac{53 \lfloor \frac{N}{5}\rfloor }{60}+1
\tag{11} \label{11}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Again, this holds when &lt;span class="math"&gt;\(\lfloor \frac{N}{5}\rfloor \equiv 0 (\text{mod } 10)\)&lt;/span&gt;.
This is the equation we used to get the result quoted for one trillion dollars
at the top of our post.  Notice that the leading term here matches what&amp;#8217;s
required by our asymptotic result,&amp;nbsp;(\ref{5}).&lt;/p&gt;
&lt;p&gt;It turns out that when we plug in data for each of the other nine possible
remainders, the cubic fits returned are all the same &amp;#8212; except for their
constant terms.  Presumably, this is because the coefficients of positive
powers of &lt;span class="math"&gt;\(N\)&lt;/span&gt; each correspond to some asymptotic, geometric effects that must
all agree when &lt;span class="math"&gt;\(N\)&lt;/span&gt; is large &amp;#8212; as we discussed above. At any rate, we have
tabulated the constant terms that apply for each possible remainder below, and
this completes our&amp;nbsp;solution.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;span class="math"&gt;\(\lfloor \frac{N}{5} \rfloor \mod(10)\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;constant term in &lt;span class="math"&gt;\(\mathbb{Q}(N)\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(0\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(1\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(1\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(\frac{7}{8}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(2\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(\frac{6}{5}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(3\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(\frac{7}{8}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(4\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(\frac{4}{5}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(5\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(\frac{7}{8}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(6\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(1\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(7\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(\frac{43}{40}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(8\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(1\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(9\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(\frac{27}{40}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;Acknowledgements&lt;/h4&gt;
&lt;p&gt;This work in this post was done in collaboration with my father, Steven&amp;nbsp;Landy.&lt;/p&gt;
&lt;h4&gt;References&lt;/h4&gt;
&lt;p&gt;[1] Beck, Matthias, and Sinai Robins. Computing the continuous discretely. Vol.
61. Berlin: Springer Science+ Business Media, &lt;span class="caps"&gt;LLC&lt;/span&gt;,&amp;nbsp;2007.&lt;/p&gt;
&lt;p&gt;[2] One way to derive this volume result is to integrate the recursion
relations (1)-(3), ignoring discreteness effects &amp;#8212; i.e., integrate assuming
that each of the variables &lt;span class="math"&gt;\(\mathbb{P}(N), \mathbb{N}(N), \mathbb{D}(N)\)&lt;/span&gt;, and
&lt;span class="math"&gt;\(\mathbb{Q}(N)\)&lt;/span&gt; are smooth functions of &lt;span class="math"&gt;\(N\)&lt;/span&gt;.  This will hold in the large &lt;span class="math"&gt;\(N\)&lt;/span&gt;
limit to leading order, so will give us the leading order&amp;nbsp;result.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="programming, Case studies"></category></entry><entry><title>Generalized Dollar Cost Averaging</title><link href="https://efavdb.com/generalized_dollar_cost_averaging" rel="alternate"></link><published>2021-03-20T00:00:00-07:00</published><updated>2021-03-20T00:00:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2021-03-20:/generalized_dollar_cost_averaging</id><summary type="html">&lt;p&gt;In this note, I consider a generalization of Dollar Cost Averaging &amp;#8212; a popular
investing strategy that involves gradually building up one&amp;#8217;s holding in a stock
over a pre-specified period of time.  The generalization I consider can
guarantee better prices paid per share &amp;#8212; relative to the standard approach &amp;#8212;
but its …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In this note, I consider a generalization of Dollar Cost Averaging &amp;#8212; a popular
investing strategy that involves gradually building up one&amp;#8217;s holding in a stock
over a pre-specified period of time.  The generalization I consider can
guarantee better prices paid per share &amp;#8212; relative to the standard approach &amp;#8212;
but its use comes at the cost of requiring one to have a flexible budget. The
idea essentially boils down to leaning in more heavily on low price&amp;nbsp;periods.&lt;/p&gt;
&lt;h3&gt;Model system and&amp;nbsp;results&lt;/h3&gt;
&lt;p&gt;In this post, we will assume a scenario where one wants to enter into a stock
over a set of &lt;span class="math"&gt;\(K\)&lt;/span&gt; equally spaced periods.  These periods could be separated by
minutes, hours, or days, etc.  The price of the stock at period &lt;span class="math"&gt;\(i\)&lt;/span&gt; will be
written as &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; and the number of shares purchased then will&amp;nbsp;be
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
N_i = A \frac{1}{p_i^{\gamma}}, \tag{1} \label{1}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt; 
where &lt;span class="math"&gt;\(A\)&lt;/span&gt; and &lt;span class="math"&gt;\(\gamma\)&lt;/span&gt; are constants that are set before beginning.  The total
number of shares purchased, total money spent, and price per share realized are
then given&amp;nbsp;by,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{2} \label{2}
N_{total} &amp;amp;=&amp;amp; \sum_{i=1}^K N_i \\
\tag{3} \label{3}
\text{total money spent} &amp;amp;=&amp;amp; \sum_{i=1}^K p_i N_i \\
\tag{4} \label{4}
\text{price per share} &amp;amp;=&amp;amp; \frac{\sum_{i=1}^K p_i N_i}{\sum_{i=1}^K N_i},
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
respectively.&lt;/p&gt;
&lt;p&gt;The green curve in the figure below records the price paid per share under our
strategy as a function of &lt;span class="math"&gt;\(\gamma\)&lt;/span&gt; for the example price series shown in black
(this has a hidden, separate x-axis).
&lt;p align="center"&gt;
&lt;img src="images/price_per_share.png" alt="price paid" style="width:100%"&gt;
&lt;/p&gt;
This figure illustrates a number of general points about our&amp;nbsp;strategy:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If &lt;span class="math"&gt;\(\gamma = 0\)&lt;/span&gt;, we purchase a fixed number of shares each day, &lt;span class="math"&gt;\(A\)&lt;/span&gt;.  The
  resulting price paid per share (\ref{4}) is then the arithmetic mean of the
period&amp;nbsp;prices.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If &lt;span class="math"&gt;\(\gamma =1\)&lt;/span&gt;, we spend an equal amount of money each day.  This is the
  standard dollar cost averaging method.  Plugging this choice into (\ref{4}),
one finds that the price paid per share in this case is the harmonic mean of
the prices over the&amp;nbsp;series.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The arithmetic-harmonic means inequality implies that the &lt;span class="math"&gt;\(\gamma=1\)&lt;/span&gt;
  necessarily beats the &lt;span class="math"&gt;\(\gamma=0\)&lt;/span&gt; approach when it comes to price per share
paid.  This can be understood as follows:  The &lt;span class="math"&gt;\(\gamma=1\)&lt;/span&gt; method results in
relatively more shares being purchased on low price periods, which  causes
these periods to be more heavily weighted in the relevant&amp;nbsp;average.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;More generally, we prove in an appendix below that increasing &lt;span class="math"&gt;\(\gamma\)&lt;/span&gt; always
  causes the price per share to go down.  This holds for all possible price
series &amp;#8212; including that shown in our figure above, where we see the green
curve is monotonically decreasing.  This result holds because increasing
&lt;span class="math"&gt;\(\gamma\)&lt;/span&gt; causes one lean in even harder on low price&amp;nbsp;periods. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;As &lt;span class="math"&gt;\(\gamma \to \infty\)&lt;/span&gt;, our purchase strategy is dominated by the lowest
price period, and this is the price we end up paying per share on&amp;nbsp;average.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;As &lt;span class="math"&gt;\(\gamma \to -\infty\)&lt;/span&gt;, our strategy is dominated by the highest price
  period, and this is the price we pay.  This limit is attractive when we want
to exit / sell a&amp;nbsp;holding.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The downside of our strategy &amp;#8212; relative to the standard dollar cost
  averaging method &amp;#8212; is that it requires a flexible budget.  However, for
&lt;span class="math"&gt;\(\gamma\)&lt;/span&gt; not too large, we can often get a meaningful reduction in price per
share without dramatically increasing the variance in amount spent: e.g., a one
percent drop in price per share might be possible if one is willing to have a
ten percent flexibility in one&amp;#8217;s budget.  We give numerical examples in the
next&amp;nbsp;section.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If the stock is drifting over the &lt;span class="math"&gt;\(K\)&lt;/span&gt; periods in question, all dollar cost
  averaging methods may give a higher price than simply purchasing everything
on the first period.  For this reason &amp;#8212; when a cheaper price per share is the
objective &amp;#8212; I suggest using these strategies over short time frames only &amp;#8212;
say, over a day or week.  That&amp;#8217;s because stock price variance typically
dominates drift over short&amp;nbsp;periods.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We now turn to a quick numerical study, and then conclude with our appendix
covering the proof of&amp;nbsp;monotonicity.&lt;/p&gt;
&lt;h3&gt;Numerical&amp;nbsp;study&lt;/h3&gt;
&lt;p&gt;In this section, we provide some helper python code that allows one to simulate
price series and check the results of running our strategy (\ref{1}) on top
of them.  The following two code blocks (1) generate log normal random walk
price series, and (2) carry out our&amp;nbsp;strategy.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;simulated_price_series&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;initial_price&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;drift&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;steps&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;    Returns a simulated price series asserting the log of the&lt;/span&gt;
&lt;span class="sd"&gt;    price takes a random walk with drift and variance, as&lt;/span&gt;
&lt;span class="sd"&gt;    passed. &lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;drifts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;steps&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;drift&lt;/span&gt;
    &lt;span class="n"&gt;random_factors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;steps&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;random_factors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;

    &lt;span class="n"&gt;returns&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;drifts&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;random_factors&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;initial_price&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;returns&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;simulated_series_and_purchases&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;gamma&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;initial_price&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;drift&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;steps&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;    Simulate price trajectory of length steps and then apply the&lt;/span&gt;
&lt;span class="sd"&gt;    generalized dollar cost averaging method -- this entails buying&lt;/span&gt;
&lt;span class="sd"&gt;    N_i shares at step i, with N_i ~ 1 / p_i ^ gamma.&lt;/span&gt;

&lt;span class="sd"&gt;    Return the total money spent, shares purchased, and average price&lt;/span&gt;
&lt;span class="sd"&gt;    per share.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;price_series&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;simulated_price_series&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;initial_price&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;drift&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;steps&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;shares_series&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;price_series&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;price_series&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="n"&gt;gamma&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;money_spent_series&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;price_series&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;shares_series&lt;/span&gt;

    &lt;span class="c1"&gt;# summary stats&lt;/span&gt;
    &lt;span class="n"&gt;shares&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shares_series&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;money_spent&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;money_spent_series&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;shares&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;money_spent&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;money_spent&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;shares&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;With these in place, I ran 40k simulation with the following parameters for
the price&amp;nbsp;series:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;initial_price&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;10.0&lt;/span&gt;
&lt;span class="n"&gt;steps&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;
&lt;span class="n"&gt;drift&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;
&lt;span class="n"&gt;std&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.05&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In the simulations, I considered &lt;span class="math"&gt;\(\gamma=5\)&lt;/span&gt;, &lt;span class="math"&gt;\(10\)&lt;/span&gt;, and &lt;span class="math"&gt;\(1\)&lt;/span&gt; &amp;#8212; the last being
the standard dollar cost averaging method.  Histograms of the resulting spends
are shown below for each case.  The titles give the mean and standard deviation
of the amount spent over the traces, and the legends give the mean prices per
share, averaging over traces.  We can see that we can get a fairly significant
improvement in mean price as we lift &lt;span class="math"&gt;\(\gamma\)&lt;/span&gt;, but doing this requires that
we occassionally spend a lot more&amp;nbsp;money.&lt;/p&gt;
&lt;p&gt;One can mitigate the very large spend events by capping the amount spent per
day.  For example, if one caps the daily spend at fifty percent above that on
the first day, the &lt;span class="math"&gt;\(\gamma=10\)&lt;/span&gt; case here gives a spend distribution of &lt;span class="math"&gt;\(10194
\pm 2069\)&lt;/span&gt;, and a mean price per share of &lt;span class="math"&gt;\(9.86\)&lt;/span&gt;.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="images/spend_distributions.png" alt="price paid" style="width:100%"&gt;
&lt;/p&gt;

&lt;h3&gt;Appendix: Average price per share&amp;nbsp;monotonicity&lt;/h3&gt;
&lt;p&gt;In this appendix, we give a proof that increasing &lt;span class="math"&gt;\(\gamma\)&lt;/span&gt; in (\ref{1}), always
results in a decreasing average price per share (more precisely, it can&amp;#8217;t
result in an&amp;nbsp;increase).&lt;/p&gt;
&lt;p&gt;To begin, we recall our definitions:  The number of shares purchased at &lt;span class="math"&gt;\(p_i\)&lt;/span&gt;
is assumed to&amp;nbsp;be
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{A1}
N_i = A p_i^{-\gamma}.
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
The total number of shares purchased is then obtained by summing&amp;nbsp;this
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{A2}
N_{total} = \sum_{i=1}^k N_i
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
The average price per share is&amp;nbsp;then
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{A3} \label{A3}
\overline{p} = \sum_{i=1}^k w_i p_i
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
where
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
w_j &amp;amp;\equiv &amp;amp; \frac{N_j}{N_{total}} \\
&amp;amp;=&amp;amp; \frac{p_j^{-\gamma}}{\sum_{i=1}^k p_i^{-\gamma}} \tag{A4}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
is the normalized weight applied to price &lt;span class="math"&gt;\(p_j\)&lt;/span&gt;.  Plugging this last line into
(\ref{A3}), we get a simple expression for the average price per&amp;nbsp;share
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{A5}\tag{A5}
\overline{p} = \frac{\langle p_i^{-\gamma + 1} \rangle}{\langle p_i^{-\gamma}
\rangle} 
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Here, we use brackets for equal weight averages over the price series.  This
formula shows that the mean paid price per share is related to the ratio of two
adjacent moments of the price series.  We will now argue that this is monotic
in &lt;span class="math"&gt;\(\gamma\)&lt;/span&gt; [Aside:  It looks like one might be able to provide an alternative
proof of our result from the ratio of moments expression above, combining that
with one of the Chebychev inequalities.  However, I haven&amp;#8217;t been able to get
that to work just&amp;nbsp;yet].&lt;/p&gt;
&lt;p&gt;To show that (\ref{A3}) is monotonic in &lt;span class="math"&gt;\(\gamma\)&lt;/span&gt;, we first note that the
relative weight applied to any two prices is given&amp;nbsp;by
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{A6}\tag{A6}
\frac{w_i}{w_j} = \left(\frac{p_i}{p_j}\right)^{-\gamma}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
If &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; is the minimum price in the series this is necessarily an increasing
function of &lt;span class="math"&gt;\(\gamma\)&lt;/span&gt; for any other &lt;span class="math"&gt;\(p_j\)&lt;/span&gt;.  In order for the weights to continue
to be normalized to one, this implies that the weight applied to the smallest
price must always increase as we increase &lt;span class="math"&gt;\(\gamma\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Next, we note that with the above observation, our result is trivially true
when &lt;span class="math"&gt;\(k=1\)&lt;/span&gt; and &lt;span class="math"&gt;\(k=2\)&lt;/span&gt;.  Suppose then that the result holds up to &lt;span class="math"&gt;\(k-1\)&lt;/span&gt; periods
and the consider the case at &lt;span class="math"&gt;\(k\)&lt;/span&gt;.  If we order the &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; values from largest to
smallest, we then&amp;nbsp;have
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{A7} \tag{A7}
\overline{p} = \left ( 1 - w_k \right) \sum_{i=1}^{k-1} \tilde{w}_i p_i+ w_k
p_k
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
The sum above is the weighted average we&amp;#8217;d realize if we only invested on the
first &lt;span class="math"&gt;\(k-1\)&lt;/span&gt; ordered prices, and the &lt;span class="math"&gt;\(\tilde{w}_i\)&lt;/span&gt; are the effective weights
we&amp;#8217;d apply in that case. By assumption, this inner sum is necessarily
non-increasing with &lt;span class="math"&gt;\(\gamma\)&lt;/span&gt;.   Therefore, if we let &lt;span class="math"&gt;\(w^{\prime}\)&lt;/span&gt; be the weight
applied at some &lt;span class="math"&gt;\(\gamma^{\prime} &amp;gt; \gamma\)&lt;/span&gt;, we&amp;nbsp;have
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
\overline{p}^{\prime} &amp;amp;=&amp;amp; \left ( 1 - w_k^{\prime} \right) \sum_{i=1}^{k-1}
\tilde{w}_i^{\prime} p_i + w_k^{\prime} p_k \\
&amp;amp;\leq &amp;amp;  \left ( 1 - w_k^{\prime} \right) \sum_{i=1}^{k-1} \tilde{w}_i p_i +
w_k^{\prime} p_k \label{A8} \tag{A8}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
But this last line is just like the &lt;span class="math"&gt;\(k=2\)&lt;/span&gt; case: We move from one weighted
average of two values (namely, &lt;span class="math"&gt;\(p_k\)&lt;/span&gt; and the weighted sum of the first &lt;span class="math"&gt;\(k-1\)&lt;/span&gt;
prices with weights fixed as at &lt;span class="math"&gt;\(\gamma\)&lt;/span&gt; &amp;#8212; which is necessarily not smaller
than &lt;span class="math"&gt;\(p_k\)&lt;/span&gt;) to another that applies more weight to the smaller of the two
values &amp;#8212; ultimately giving a smaller result.  Continuing with the last line,
then, we&amp;nbsp;have
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
\overline{p}^{\prime} &amp;amp;\leq&amp;amp; \left ( 1 - w_k^{\prime} \right) \sum_{i=1}^{k-1}
\tilde{w}_i p_i + w_k^{\prime} p_k \\
&amp;amp;\leq&amp;amp; \left ( 1 - w_k \right) \sum_{i=1}^{k-1} \tilde{w}_i p_i + w_k p_k \\
&amp;amp;\equiv&amp;amp; \overline{p}. \label{A9} \tag{A9}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt; 
This completes our&amp;nbsp;proof.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Finance"></category><category term="theory"></category><category term="statistics"></category></entry><entry><title>Physics-based proof of the duality theorem for linear programs</title><link href="https://efavdb.com/duality" rel="alternate"></link><published>2021-02-14T09:24:00-08:00</published><updated>2021-02-14T09:24:00-08:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2021-02-14:/duality</id><summary type="html">&lt;p&gt;Textbook proofs of the duality theorem often apply abstract arguments that
offer little tangible insight into the relationship between a linear program
and its dual. Here, we map the general linear program onto a simple mechanics
problem. In this context, the significance of the theorem is relatively&amp;nbsp;clear.&lt;/p&gt;
&lt;h4&gt;The general …&lt;/h4&gt;</summary><content type="html">&lt;p&gt;Textbook proofs of the duality theorem often apply abstract arguments that
offer little tangible insight into the relationship between a linear program
and its dual. Here, we map the general linear program onto a simple mechanics
problem. In this context, the significance of the theorem is relatively&amp;nbsp;clear.&lt;/p&gt;
&lt;h4&gt;The general linear program and its&amp;nbsp;dual&lt;/h4&gt;
&lt;p&gt;The goal of a linear program is to optimize a linear objective function subject
to some linear constraints.  By introducing slack and other auxiliary
variables, the general linear program can be expressed as&amp;nbsp;[1]
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}  \tag{1} \label{primal}
\text{max}_{\textbf{x} \in \mathbb{R}_n}  \textbf{c}^T \cdot \textbf{x},
\text{subject to: } A \cdot \textbf{x} \leq  \textbf{b}.
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;Here, the optimization is over &lt;span class="math"&gt;\(\textbf{x} \in \mathbb{R}_n\)&lt;/span&gt;, &lt;span class="math"&gt;\(A\)&lt;/span&gt; is a given &lt;span class="math"&gt;\(m
\times n\)&lt;/span&gt; real matrix, &lt;span class="math"&gt;\(\textbf{b}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textbf{c}\)&lt;/span&gt; are given real vectors,
and the inequality holds component-wise.  Any &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; that satisfies the
inequality constraints above is called a &lt;em&gt;feasible solution&lt;/em&gt; to
(\ref{primal}) and if a point can be found with finite components that
optimizes the objective it is called an &lt;em&gt;optimal solution&lt;/em&gt;.  Associated
with (\ref{primal}) is the dual linear&amp;nbsp;program,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \label{dual} \tag{2}
\text{min}_{\textbf{w} \in \mathbb{R}_m} \textbf{b}^T \cdot \textbf{w} ,
\text{subject to: } A^T \cdot \textbf{w} = \textbf{c}, \textbf{w} \geq
\textbf{0}.  \end{eqnarray}&lt;/div&gt;
&lt;p&gt;
The optimization here is over &lt;span class="math"&gt;\(\textbf{w} \in \mathbb{R}_m\)&lt;/span&gt;, and &lt;span class="math"&gt;\(A\)&lt;/span&gt;,
&lt;span class="math"&gt;\(\textbf{b}\)&lt;/span&gt;, and &lt;span class="math"&gt;\(\textbf{c}\)&lt;/span&gt; are the same variables present in
(\ref{primal}).  In this post, we apply some simple ideas from classical
mechanics to prove the following theorem, one of the central results connecting
the two linear programs&amp;nbsp;above:&lt;/p&gt;
&lt;h5&gt;Duality&amp;nbsp;theorem&lt;/h5&gt;
&lt;p&gt;If  (\ref{primal}) has an optimal solution at &lt;span class="math"&gt;\(\textbf{x}^*\)&lt;/span&gt;, then (\ref{dual})
will also have an optimal solution at some point &lt;span class="math"&gt;\(\textbf{w}^*\)&lt;/span&gt;, and these
points&amp;nbsp;satisfy
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{strong_law} \tag{3}
\textbf{c}^T \cdot \textbf{x}^*= \textbf{b}^T \cdot \textbf{w}^*.
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
That is, the optimal objectives of (\ref{primal}) and (\ref{dual})&amp;nbsp;agree.&lt;/p&gt;
&lt;h5&gt;&lt;em&gt;Proof:&lt;/em&gt;&lt;/h5&gt;
&lt;p&gt;To begin we assume that  &lt;span class="math"&gt;\(\textbf{x}^*\)&lt;/span&gt;  is an optimal solution to
(\ref{primal}).  We also assume for simplicity that (i) we have chosen a
coordinate system so that &lt;span class="math"&gt;\(\textbf{x} =  \textbf{0}\)&lt;/span&gt; is a feasible solution of
(\ref{primal}) and that (ii) each row &lt;span class="math"&gt;\(\hat{\textbf{A}}_i\)&lt;/span&gt; of &lt;span class="math"&gt;\(A\)&lt;/span&gt; has been
normalized to unit&amp;nbsp;length. &lt;/p&gt;
&lt;p&gt;Next, we introduce a physical system relevant to (\ref{primal}) and
(\ref{dual}):  We consider a mobile point particle that interacts with a set of
&lt;span class="math"&gt;\(m\)&lt;/span&gt; fixed walls, all sitting in &lt;span class="math"&gt;\(\mathbb{R}_n\)&lt;/span&gt;.   The particle&amp;#8217;s coordinate
&lt;span class="math"&gt;\(\textbf{x}_p\)&lt;/span&gt; is initially set to &lt;span class="math"&gt;\(\textbf{x}_p = \textbf{0}\)&lt;/span&gt;.  The fixed
&lt;span class="math"&gt;\(i\)&lt;/span&gt;-th wall sits at those points &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; that satisfy &lt;span class="math"&gt;\(\hat{\textbf{A}}_i
\cdot \textbf{x} = b_i\)&lt;/span&gt;.   We take the force on the particle from wall &lt;span class="math"&gt;\(i\)&lt;/span&gt; to
have two parts:  (i) a constant, long range force, &lt;span class="math"&gt;\(w_i \hat{\textbf{A}}_i\)&lt;/span&gt;,
normal to the wall, and (ii)  a  &amp;#8220;hard core&amp;#8221; force, &lt;span class="math"&gt;\(-n_i(\textbf{x}_p)
\hat{\textbf{A}}_i\)&lt;/span&gt;, also normal to the wall.  The hard core force makes the
wall impenetrable to the particle, but otherwise allows the particle to move
freely:   Its magnitude is zero when the particle does not touch the wall, but
on contact it scales up to whatever value is needed to prevent the particle
from passing through&amp;nbsp;it.&lt;/p&gt;
&lt;p&gt;To relate the physical system to our linear programs, we&amp;#8217;ll require
&lt;span class="math"&gt;\(\textbf{x}_p\)&lt;/span&gt; to be a feasible solution to (\ref{primal}) and the vector of
long range force magnitudes &lt;span class="math"&gt;\(\textbf{w}\)&lt;/span&gt; to be a feasible solution to
(\ref{dual}).  A point &lt;span class="math"&gt;\(\textbf{x}_p\)&lt;/span&gt; is a feasible solution to (\ref{primal})
if and only if the particle is within the interior space bounded by the set of
walls.  Further, the equality constraint of (\ref{dual}) is equivalent to the
condition that all feasible &lt;span class="math"&gt;\(\textbf{w}\)&lt;/span&gt; result in a total long range force
acting on the particle of &lt;span class="math"&gt;\(\sum_i  w_i \hat{\textbf{A}}_i = \textbf{c}\)&lt;/span&gt;.   The
non-negativity condition on feasible &lt;span class="math"&gt;\(\textbf{w}\)&lt;/span&gt; vectors in (\ref{dual})
further requires that the long range forces each be either attractive or zero.
A simple example of the sort of physical system we&amp;#8217;ve described here is shown
in Fig.&amp;nbsp;1a.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;figure class="image"&gt;
  &lt;img src="images/duality.png"&gt;
&lt;/figure&gt;
&lt;b&gt;Fig. 1:&lt;/b&gt; An example system:  (a) A particle at &lt;span class="math"&gt;\(\textbf{x}_p=\textbf{0}\)&lt;/span&gt;
interacts with three walls.  The total long range force on the particle is
&lt;span class="math"&gt;\(\sum_i w_i \hat{\textbf{A}}_i = \textbf{c}\)&lt;/span&gt;. (b) The particle sits at its long
range potential minimum, &lt;span class="math"&gt;\(\textbf{x}^*\)&lt;/span&gt;, with walls &lt;span class="math"&gt;\(2\)&lt;/span&gt; and &lt;span class="math"&gt;\(3\)&lt;/span&gt; &lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;binding&amp;#8221;.
The hard core normal forces from these walls must point inward and sum to
&lt;span class="math"&gt;\(-\textbf{c}\)&lt;/span&gt; &amp;#8212; otherwise, there would be a net force on the particle, and it
would continue to move, but that won&amp;#8217;t happen once it sits at its potential
minimum.  Wall &lt;span class="math"&gt;\(1\)&lt;/span&gt; is not binding and is now a distance &lt;span class="math"&gt;\(d_1(\textbf{x}^*)\)&lt;/span&gt;
away from the particle.  This results in a positive difference between the dual
and primal objectives (\ref{gap}), unless we can find a feasible &lt;span class="math"&gt;\(\textbf{w}\)&lt;/span&gt;
for which &lt;span class="math"&gt;\(w_1=0\)&lt;/span&gt;.  Setting &lt;span class="math"&gt;\(\textbf{w}^* = \textbf{n}(\textbf{x}^*)\)&lt;/span&gt; &amp;#8212;  the
vector of hard-core normal force magnitudes at &lt;span class="math"&gt;\(\textbf{x}^*\)&lt;/span&gt; &amp;#8212; provides such
a solution.  This is non-negative, zero for non-binding walls, and results in a
total long range force of &lt;span class="math"&gt;\(\textbf{c}\)&lt;/span&gt; &amp;#8212; a consequence of the point above that
the particle must remain at rest at its potential minimum.  This choice for
&lt;span class="math"&gt;\(\textbf{w}\)&lt;/span&gt; optimizes (\ref{dual}) and gives an objective matching that of
(\ref{primal}) at
&lt;span class="math"&gt;\(\textbf{x}^*\)&lt;/span&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;When we assert the conditions above, the potential associated with the long
range forces in our physical system ends up being related to the objective
functions of (\ref{primal}) and (\ref{dual}).  Up to a constant, the potential
of a force &lt;span class="math"&gt;\(\textbf{f}(\textbf{x})\)&lt;/span&gt; is defined to be&amp;nbsp;[2] 
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{potential} \tag{4}
U(\textbf{x}) \equiv -\int \textbf{f}(\textbf{x}) \cdot d\textbf{x}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
In our case, the long range force between the wall &lt;span class="math"&gt;\(i\)&lt;/span&gt; and the particle is a
constant and normal to the wall.  Its potential is therefore&amp;nbsp;simply
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{potential_i} \tag{5}
U_i(\textbf{x}_p) = w_i d_i(\textbf{x}_p),
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
where 
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{perp_distance} \tag{6}
d_{i}(\textbf{x}_p)  \equiv b_i - \hat{\textbf{A}}_i \cdot \textbf{x}_p.
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
 This is the perpendicular distance between the particle and the wall &lt;span class="math"&gt;\(i\)&lt;/span&gt;.  The
physical significance of (\ref{potential_i}) is that it is the total energy it
takes to separate the particle from wall &lt;span class="math"&gt;\(i\)&lt;/span&gt; by a distance &lt;span class="math"&gt;\(d_i\)&lt;/span&gt;, working
against the attractive force &lt;span class="math"&gt;\(w_i\)&lt;/span&gt;.  Notice that if we plug
(\ref{perp_distance}) into (\ref{potential_i}), the total long range potential
at &lt;span class="math"&gt;\(\textbf{x}_p\)&lt;/span&gt; can be written&amp;nbsp;as
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\nonumber
\sum_i w_i d_i(\textbf{x}_p)&amp;amp;=&amp;amp; \sum_i w_i  \left (b_i - \hat{\textbf{A}}_i
\cdot \textbf{x}_p  \right) 
\\ &amp;amp;=&amp;amp; \textbf{w}^T \cdot \textbf{b} - \textbf{c} \cdot \textbf{x}_p. \tag{7}
 \label{potential_as_objective_difference}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Here, we have used one of the feasibility conditions on &lt;span class="math"&gt;\(\textbf{w}\)&lt;/span&gt; to get the
last line.   The right side of (\ref{potential_as_objective_difference}) is the
difference between the dual and primal objectives.  This will be minimized at
the feasible point &lt;span class="math"&gt;\(\textbf{x}_p\)&lt;/span&gt; that maximizes &lt;span class="math"&gt;\(\textbf{c}\cdot \textbf{x}_p\)&lt;/span&gt;
&amp;#8212; the primal objective &amp;#8212; and at that feasible &lt;span class="math"&gt;\(\textbf{w}\)&lt;/span&gt; that minimizes
&lt;span class="math"&gt;\(\textbf{w}^T \cdot \textbf{b}\)&lt;/span&gt; &amp;#8212; the dual objective.  In other words, we see
that both our programs independently contribute to the common goal of
minimizing the particle&amp;#8217;s long range potential,
(\ref{potential_as_objective_difference}), subject to our system&amp;#8217;s&amp;nbsp;constraints.&lt;/p&gt;
&lt;p&gt;The last preperatory remark we must make relates to the fact that we have
assumed that &lt;span class="math"&gt;\(\textbf{x}^*\)&lt;/span&gt; is an optimal solution to (\ref{primal}) &amp;#8212; i.e.,
it is a point that is as far &amp;#8220;down&amp;#8221; in the &lt;span class="math"&gt;\(\textbf{c}\)&lt;/span&gt; direction as possible
within the feasible set.  This must mean that &lt;span class="math"&gt;\(\textbf{x}^*\)&lt;/span&gt; sits somewhere at
the boundary of the primal feasible set, with some of the constraints of
(\ref{primal}) binding &amp;#8212; i.e., satisfied as equalities.  Further, if we place
and release the particle gently at &lt;span class="math"&gt;\(\textbf{x}^*\)&lt;/span&gt;, it must stay at rest as it
can fall no further &amp;#8212; just as a particle acted on by gravity stays at rest
when it is placed in the bottom of a bucket. To stay at rest, there must be no
net force on the particle, which means that the hard core normal forces &lt;span class="math"&gt;\(-n_i
\hat{\textbf{A}}_i\)&lt;/span&gt; from the binding walls at &lt;span class="math"&gt;\(\textbf{x}^*\)&lt;/span&gt;  must sum to
exactly &lt;span class="math"&gt;\(-\textbf{c}\)&lt;/span&gt;, fully countering the constant long range force.   The
set of forces acting on the particle at &lt;span class="math"&gt;\(\textbf{x}^*\)&lt;/span&gt; is illustrated Fig. 1b
for our simple&amp;nbsp;example.&lt;/p&gt;
&lt;p&gt;To complete the argument, we note that the long range potential at
&lt;span class="math"&gt;\(\textbf{x}^*\)&lt;/span&gt; is given from (\ref{potential_as_objective_difference})&amp;nbsp;by
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \label{gap} \tag{8} 
\sum_i w_i d_i(\textbf{x}^*) &amp;amp;=&amp;amp; \textbf{w}^T \cdot \textbf{b} - \textbf{c}
\cdot \textbf{x}^*.
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
This is non-negative because  &lt;span class="math"&gt;\(\textbf{w} \geq \textbf{0}\)&lt;/span&gt; and
&lt;span class="math"&gt;\(d_i(\textbf{x}^*) &amp;gt;0\)&lt;/span&gt; for each non-binding wall.   It follows that the dual
objective is always bounded from below by the optimal primal objective.
Further, the gap between the two &amp;#8212; the left side of (\ref{gap}) &amp;#8212; can only be
zero if the long range interaction strengths are zero for each non-binding wall
at &lt;span class="math"&gt;\(\textbf{x}^*\)&lt;/span&gt; &amp;#8212; i.e., if the particle is not actually attracted to the
walls that are not binding at &lt;span class="math"&gt;\(\textbf{x}^*\)&lt;/span&gt;.   The vector
&lt;span class="math"&gt;\(\textbf{n}(\textbf{x}^*)\)&lt;/span&gt; of hard core normal force magnitudes at
&lt;span class="math"&gt;\(\textbf{x}^*\)&lt;/span&gt; provides such a solution for &lt;span class="math"&gt;\(\textbf{w}\)&lt;/span&gt;:  This is a feasible
&lt;span class="math"&gt;\(\textbf{w}\)&lt;/span&gt;  because it is non-negative and results in normal forces that sum
to &lt;span class="math"&gt;\(\textbf{c}\)&lt;/span&gt;.  Further, it is non-zero only for the binding constraints.  It
follows that &lt;span class="math"&gt;\(\textbf{w}^* = \textbf{n}(\textbf{x}^*)\)&lt;/span&gt; gives an optimal
solution to the dual, at which point its objective matches that of the optimal
primal solution.  This argument is summarized in the caption to Fig.&amp;nbsp;1.&lt;/p&gt;
&lt;h4&gt;References&lt;/h4&gt;
&lt;p&gt;[1] Matousek, J., and Ga ̈rtner, B. Understanding and using linear programming.
Springer&amp;nbsp;(2007)&lt;/p&gt;
&lt;p&gt;[2] Marion, Jerry B. Classical dynamics of particles and systems. Academic
Press,&amp;nbsp;(2013).&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="optimization"></category></entry><entry><title>To Flourish or to Perish</title><link href="https://efavdb.com/to-flourish-or-to-perish" rel="alternate"></link><published>2021-01-03T00:00:00-08:00</published><updated>2021-01-03T00:00:00-08:00</updated><author><name>Dustin McIntosh</name></author><id>tag:efavdb.com,2021-01-03:/to-flourish-or-to-perish</id><summary type="html">&lt;p&gt;In this post, I explore some basic math behind the World Wandering Dudes framework previously introduced in &lt;a href="https://www.efavdb.com/world-wandering-dudes"&gt;this post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To briefly reintroduce the system: imagine a field, a 2-D square lattice of &lt;span class="math"&gt;\(M\)&lt;/span&gt; sites with food distributed randomly at a density &lt;span class="math"&gt;\(\rho_{food}\)&lt;/span&gt;.  Creatures wander across the field taking &lt;span class="math"&gt;\(N …&lt;/span&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;In this post, I explore some basic math behind the World Wandering Dudes framework previously introduced in &lt;a href="https://www.efavdb.com/world-wandering-dudes"&gt;this post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To briefly reintroduce the system: imagine a field, a 2-D square lattice of &lt;span class="math"&gt;\(M\)&lt;/span&gt; sites with food distributed randomly at a density &lt;span class="math"&gt;\(\rho_{food}\)&lt;/span&gt;.  Creatures wander across the field taking &lt;span class="math"&gt;\(N\)&lt;/span&gt; steps per day via random walk, gathering any food they come across.  At the end of each day, if a creature has no food it dies, if a creature has exactly 1 food it survives, and if a creature has 2 or more food it survives and reproduces.  The food will then resprout at random&amp;nbsp;locations.&lt;/p&gt;
&lt;p&gt;For the purposes of this post, to make the modeling simpler, we introduce the following&amp;nbsp;properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;All food spoils at the end of each day before resprouting - creatures cannot store it and it does not accumulate on the&amp;nbsp;field.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;All creatures teleport to a random field position at the end of each day, preventing creatures from consistently competing for food with their&amp;nbsp;children.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Periodic boundary conditions on the field with &lt;span class="math"&gt;\(M \gg N\)&lt;/span&gt; such that creatures do not wander “around the world” within a single&amp;nbsp;day.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First, let’s imagine a single creature alone on the field. After taking &lt;span class="math"&gt;\(N\)&lt;/span&gt; steps, the creature will explore &lt;span class="math"&gt;\(N_{unique}\)&lt;/span&gt; unique sites on the lattice, where &lt;span class="math"&gt;\(\langle N_{unique} \rangle \sim N^\beta\)&lt;/span&gt;, with &lt;span class="math"&gt;\(\beta \approx 0.87\)&lt;/span&gt; (see &lt;a href="https://www.efavdb.com/random-walk-scaling"&gt;this post&lt;/a&gt; for more details). The number of food that this creature will gather by the end of the day is &lt;span class="math"&gt;\(x \sim \text{Binom}(N_{unique}, \rho_{food})\)&lt;/span&gt;. We’ll represent the probability the creature gets &lt;span class="math"&gt;\(x\)&lt;/span&gt; food&amp;nbsp;as&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{1} \label{binom}
p_x = {N_{unique} \choose x} {\rho_{food}}^x (1-\rho_{food})^{N_{unique}-x}. \\
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;For very small &lt;span class="math"&gt;\(N\)&lt;/span&gt;, &lt;span class="math"&gt;\(p_0\)&lt;/span&gt; is large and &lt;span class="math"&gt;\(p_{2+} = 1 - p_0 - p_1\)&lt;/span&gt; is small - the creature is likely to perish without reproducing.  The larger &lt;span class="math"&gt;\(N\)&lt;/span&gt;, the larger &lt;span class="math"&gt;\(N_{unique}\)&lt;/span&gt;, resulting in smaller &lt;span class="math"&gt;\(p_0\)&lt;/span&gt; and larger &lt;span class="math"&gt;\(p_{2+}\)&lt;/span&gt;. There is a specific &lt;span class="math"&gt;\(N\)&lt;/span&gt; for which &lt;span class="math"&gt;\(p_0 = p_{2+}\)&lt;/span&gt; - at this critical point &lt;span class="math"&gt;\(N_c\)&lt;/span&gt;, the creature is equally likely to reproduce and to die. For &lt;span class="math"&gt;\(\rho_{food} = 0.03\)&lt;/span&gt;, the relevant probabilities along with this critical point are plotted&amp;nbsp;below.&lt;/p&gt;
&lt;p align="center"&gt;
     &lt;img src="images/binomial_random_walk.png"&gt;
&lt;/p&gt;

&lt;p&gt;A couple of interesting&amp;nbsp;notes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;At &lt;span class="math"&gt;\(N_c\)&lt;/span&gt;, the creature is most likely to get exactly 1 food and barely survive, it dies or reproduces with equal probability &lt;span class="math"&gt;\(\approx 0.315 &amp;lt; 1/3\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="math"&gt;\(N_{unique, c} &amp;gt; 1/\rho_{food}\)&lt;/span&gt; - the creature must be, on average, gathering more than 1 food in order for it to not die&amp;nbsp;off.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can reproduce &lt;span class="math"&gt;\(N_c\)&lt;/span&gt; from the simulation framework as well.  We ran numerous simulations, holding &lt;span class="math"&gt;\(\rho_{food} = 0.03\)&lt;/span&gt; constant and varying &lt;span class="math"&gt;\(N\)&lt;/span&gt; for a few creatures (code &lt;a href="https://github.com/dustinmcintosh/world_wandering_dudes/blob/master/scripts/lifetime_study.py"&gt;here&lt;/a&gt;). For each &lt;span class="math"&gt;\(N\)&lt;/span&gt;, we plot the fraction of simulations that still had creatures after 200&amp;nbsp;days:&lt;/p&gt;
&lt;p align="center"&gt;
     &lt;img src="images/frac_survive.png"&gt;
&lt;/p&gt;

&lt;p&gt;For sufficiently large fields, the transition from long-term survival to population collapse is sharp at &lt;span class="math"&gt;\(N_c(\rho_{food} = 0.03) \approx 72\)&lt;/span&gt; as predicted by the binomial probabilities above, creatures that walk longer than this survive and multiply, creatures that walk shorter, die off.  Smaller fields show propensity for population collapse at even higher &lt;span class="math"&gt;\(N\)&lt;/span&gt; which can be attributed to inherent instability of relatively small populations (&lt;span class="math"&gt;\(n &amp;lt; 10\)&lt;/span&gt; or&amp;nbsp;so).&lt;/p&gt;
&lt;p&gt;We can extend this understanding to a population of &lt;span class="math"&gt;\(n\)&lt;/span&gt; creatures: Based on the rules outlined at the beginning of the post, the number of creatures on day &lt;span class="math"&gt;\(i+1\)&lt;/span&gt;, &lt;span class="math"&gt;\(n_{i+1}\)&lt;/span&gt;, will&amp;nbsp;be:&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{2} \label{dt}
n_{i+1} = n_{i} - n_{i}(x=0) + n_{i}(x \geq 2), \\
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(n_{i}(x=j)\)&lt;/span&gt; is the number of creatures getting &lt;span class="math"&gt;\(j\)&lt;/span&gt; food in the previous day (creatures tomorrow = creatures today - deaths today + births today).  In equilibrium, &lt;span class="math"&gt;\(\langle n_{i+1} \rangle = \langle n_i \rangle\)&lt;/span&gt;, implying &lt;span class="math"&gt;\(\langle n_{i}(x=0) \rangle = \langle n_{i}(x&amp;gt;=2) \rangle\)&lt;/span&gt; (births=deaths).  With well-separated creatures, we’ll&amp;nbsp;find&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
\langle n_{i}(x=0) \rangle &amp;amp;=&amp;amp; \langle n_{i} \rangle p_0 \\
\langle n_{i}(x \geq 2) \rangle &amp;amp;=&amp;amp; \langle n_{i} \rangle p_{2+}. \tag{3} \label{birthdeath}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;Again, equilibrium is only achieved at &lt;span class="math"&gt;\(p_0 = p_{2+}\)&lt;/span&gt; as discussed&amp;nbsp;above.&lt;/p&gt;
&lt;p&gt;However, if the creatures are thriving, they will multiply to the point that they are no longer in isolation and start to “steal” each other&amp;#8217;s food.  This will then curb the population’s growth. To explore this, for simulations with creatures that survived their first 200 days, we look at the average number of creatures on the field from day 100 to 200 (to isolate the equilibrium condition).  We plot &lt;span class="math"&gt;\(\langle n \rangle / \rho_{food} M\)&lt;/span&gt; (average creatures as a fraction of the daily food sprout&amp;nbsp;rate):&lt;/p&gt;
&lt;p align="center"&gt;
     &lt;img src="images/equil_n.png"&gt;
&lt;/p&gt;

&lt;p&gt;The trend generally makes sense, creatures with longer walks gather more food and can maintain larger populations. Near the critical point &lt;span class="math"&gt;\(N_c\)&lt;/span&gt;, the stable populations are very low relative to the amount of food on the field, but they are stable (for sufficiently large fields).  In this case, interactions between creatures are rare occurrences, but there are enough to keep the population from growing at the very slow rate that it would if all its creatures were in isolation (&lt;span class="math"&gt;\(p_{2+}\)&lt;/span&gt; is only marginally larger here than &lt;span class="math"&gt;\(p_0\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;The remainder of this post is a discussion of the theoretical line in the figure&amp;nbsp;above.&lt;/p&gt;
&lt;p&gt;In order to approximate the impact of creature food “stealing” on the equilibrium population level, we must return to equations (\ref{birthdeath}) and add a term to account for these interactions to each.  Some creatures that would have otherwise reproduced will lose this ability due to displaced food.  Likewise, some creatures that would have barely survived will die.  As an approximation, we’ll only model the impact on creatures that would have gotten exactly 1 (or 2) food as these creatures are the most at risk of losing the ability to survive (or reproduce) as a result of food displacement.  The probability of the average creature having an interaction with one of the other creatures scales as &lt;span class="math"&gt;\(    n_{i} (\langle N_{unique} \rangle/M)\)&lt;/span&gt; (the number of other creatures times their relative footprint on the field), So, we can modify equations (\ref{birthdeath}), reducing births and supplementing&amp;nbsp;deaths:&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
\langle n_{i}(x=0) \rangle &amp;amp;=&amp;amp; \langle n_{i} \rangle  (p_0 + p_1 \frac{A \langle N_{unique} \rangle}{M} \langle n_{i} \rangle) \\
\langle n_{i}(x \geq 2) \rangle &amp;amp;=&amp;amp; \langle n_{i} \rangle (p_{2+} - p_2 \frac{A \langle N_{unique} \rangle}{M} \langle n_{i} \rangle) , \tag{4} \label{birthdeathint}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(A\)&lt;/span&gt; is some constant of proportionality having to do with some of the geometry of the system and the fraction of food displaced as a result of the average&amp;nbsp;interaction.&lt;/p&gt;
&lt;p&gt;Again, equilibrium will be reached when the two quantities in equations (\ref{birthdeathint}) are equal, which leads to a unique solution for the average number of creatures at&amp;nbsp;equilibrium:&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{5} \label{equilN}
\frac{\langle n_{i} \rangle}{\rho_{food} M} = \frac{1}{A \rho_{food}}\frac{p_{2+}-p_0}{p_1 + p_2}.
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;The line in the figure above is for &lt;span class="math"&gt;\(A = 3/4\)&lt;/span&gt;, which was a fit by eye. Not shown here, but we have confirmed that this prediction (with &lt;span class="math"&gt;\(A = 3/4\)&lt;/span&gt;) works reasonbly well for other values of &lt;span class="math"&gt;\(\rho_{food}\)&lt;/span&gt; as&amp;nbsp;well.&lt;/p&gt;
&lt;p&gt;This discussed model has some&amp;nbsp;limitations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;It considers only creatures with exactly 1(2) food to be at risk of dying (failing to reproduce) as a result of interactions, while actually creatures that get 3+ food without interactions have a chance to have enough stolen to not reproduce or even to not even&amp;nbsp;survive.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It fails to consider 3+ -body interactions (a creature with a trajectory overlapping with 2 or more other&amp;nbsp;creatures).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It neglects the distribution of &lt;span class="math"&gt;\(N_{unique}\)&lt;/span&gt;, essentially mapping &lt;span class="math"&gt;\(N_{unique} \rightarrow \langle N_{unique} \rangle\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The model is thus not correct, but simple and captures the core of the problem. It reminds me vaguely of the Flory approximation in polymer physics (see discussion &lt;a href="https://ethz.ch/content/dam/ethz/special-interest/mavt/process-engineering/macro-dam/documents/NetworkGels_Lecture6.pdf"&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="World Wandering Dudes"></category><category term="python"></category><category term="statistics"></category><category term="world wandering dudes"></category></entry><entry><title>Pricing dividend stocks</title><link href="https://efavdb.com/pricing-dividend-stocks" rel="alternate"></link><published>2020-12-20T00:00:00-08:00</published><updated>2020-12-20T00:00:00-08:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2020-12-20:/pricing-dividend-stocks</id><summary type="html">&lt;p&gt;We review how one can price a dividend-bearing stock by simply discounting its dividend stream using the value suggested by the Capital Asset Pricing Model (&lt;span class="caps"&gt;CAPM&lt;/span&gt;).  As an example, we consider the price of &lt;span class="caps"&gt;AT&lt;/span&gt;&amp;amp;T common stock.  The model result matches the current market price quite well.  Varying inputs …&lt;/p&gt;</summary><content type="html">&lt;p&gt;We review how one can price a dividend-bearing stock by simply discounting its dividend stream using the value suggested by the Capital Asset Pricing Model (&lt;span class="caps"&gt;CAPM&lt;/span&gt;).  As an example, we consider the price of &lt;span class="caps"&gt;AT&lt;/span&gt;&amp;amp;T common stock.  The model result matches the current market price quite well.  Varying inputs to the model also allows us to explore how its price might adjust to changes in company&amp;nbsp;performance.&lt;/p&gt;
&lt;h2&gt;Discounted revenue&amp;nbsp;equations&lt;/h2&gt;
&lt;p&gt;In this note, we&amp;#8217;ll consider the value of a stock that generates a discrete set of dividends, with dividend &lt;span class="math"&gt;\(D_t\)&lt;/span&gt; paid out at time &lt;span class="math"&gt;\(t\)&lt;/span&gt;, for &lt;span class="math"&gt;\(t \in \{1, 2, \ldots\}\)&lt;/span&gt;.  To estimate a fair price &lt;span class="math"&gt;\(V_0\)&lt;/span&gt; for the stock today (assumed to be just after the most recent dividend payout at &lt;span class="math"&gt;\(t=0\)&lt;/span&gt;), we&amp;#8217;ll consider how the value evolves over time.  If we purchase and hold the stock now, then at period &lt;span class="math"&gt;\(1\)&lt;/span&gt;, we&amp;#8217;ll still have the stock but also a dividend.  The total expected fair value at that time will then sum&amp;nbsp;to
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{1} \label{ev_step}
\text{expected value at period 1} \equiv E(V_1 + D_1).
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Of course, because there is some risk in our investment, the expected value above may not be realized.  To take this into account &amp;#8212; as well as the fact that inflation and other concerns cause us to prefer money now over money later &amp;#8212; we will assert that we are only willing to pay a fraction &lt;span class="math"&gt;\(d\)&lt;/span&gt; of (\ref{ev_step}) above to acquire and hold the stock now.  That is, we will define the fair price now&amp;nbsp;as
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{discount} \tag{2}
V_0 = d \times E(V_1 + D_1).
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Note that at this point, we still don&amp;#8217;t know the values of &lt;span class="math"&gt;\(V_0\)&lt;/span&gt; or &lt;span class="math"&gt;\(V_1\)&lt;/span&gt; in the above equation.  However, we can make progress if we assume that the same &lt;span class="math"&gt;\(d\)&lt;/span&gt; factor can be applied between any two periods.  In this case, we also have for general &lt;span class="math"&gt;\(i\)&lt;/span&gt;,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{3}
E(V_i) = d \times  E(V_{i+1} + D_{i+1}).
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
If we plug this into (\ref{discount}) we&amp;nbsp;obtain
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{4} \label{value}
V_0 = \sum_{i=1}^{\infty} d^i E(D_i)
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
This is an expression for the current price that depends only on the future dividends &amp;#8212; quantities that we can attempt to forecast via extrapolations of recent dividend values, or through a thorough analysis of the company&amp;#8217;s prospects.  To simplify our result further, we&amp;#8217;ll assume a steady growth for the dividends at rate &lt;span class="math"&gt;\(g\)&lt;/span&gt;, so&amp;nbsp;that
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \label{div_growth} \tag{5}
E(D_{t}) = D_0 g^i
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Plugging this form into (\ref{value}), we&amp;nbsp;get
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
V_0 &amp;amp;=&amp;amp; \sum_{i=1}^{\infty} d^i D_0 g^i \\
&amp;amp;=&amp;amp; D_0 \frac{dg}{1 - dg} \tag{6} \label{value_geometric}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Formula (\ref{value_geometric}) is the result that we will use below to approximately price dividend-bearing stocks.  Note that it is a function of only three quantities:  &lt;span class="math"&gt;\(D_0\)&lt;/span&gt; &amp;#8212; the most recent dividend value, &lt;span class="math"&gt;\(g\)&lt;/span&gt; &amp;#8212; the assumed growth rate of the dividends, and &lt;span class="math"&gt;\(d\)&lt;/span&gt; &amp;#8212; the &amp;#8220;discount rate&amp;#8221;, which in principle could vary person to person.  To price a stock, one need only plug in values for these three quantities.  To set &lt;span class="math"&gt;\(D_0\)&lt;/span&gt;, we can simply look up the value of the most recent dividend. To set &lt;span class="math"&gt;\(g\)&lt;/span&gt;, we can forecast the business prospects of the company.  To set &lt;span class="math"&gt;\(d\)&lt;/span&gt;, we should pick a value that reflects both how risk-averse we are and also how much risk the company in question carries:  &lt;span class="math"&gt;\(d\)&lt;/span&gt; should be set smaller for more risk averse individuals and for companies carrying more&amp;nbsp;risk.&lt;/p&gt;
&lt;p&gt;In the next two sections, we &lt;em&gt;(i)&lt;/em&gt; show how one can use the Capital Asset Pricing Model (&lt;span class="caps"&gt;CAPM&lt;/span&gt;) to estimate the &amp;#8220;market&amp;#8221; value of &lt;span class="math"&gt;\(d\)&lt;/span&gt;, and &lt;em&gt;(ii)&lt;/em&gt; use our results to estimate a fair market price for &lt;span class="caps"&gt;AT&lt;/span&gt;&amp;amp;T&amp;#8217;s&amp;nbsp;stock.&lt;/p&gt;
&lt;h2&gt;&lt;span class="caps"&gt;CAPM&lt;/span&gt; discount&amp;nbsp;rate&lt;/h2&gt;
&lt;p&gt;The &lt;span class="caps"&gt;CAPM&lt;/span&gt; provides a method for estimating the return one should expect from a stock.  Combining this with our analysis above, we can get an estimate for discount rate &lt;span class="math"&gt;\(d\)&lt;/span&gt; the market applies to a stock.  Plugging this into (\ref{value_geometric}) then gives a self-consistent estimate for the stock&amp;#8217;s fair market price.  To make the connection, we first note that (\ref{discount}) implies that the expected return on holding the stock for one period is given&amp;nbsp;by
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
E(r_{stock}) &amp;amp;\equiv &amp;amp; \log \left ( \frac{E(V_{1} + D_{1})}{V_0}\right)\\ 
&amp;amp;=&amp;amp;  \log (1/d)
\tag{8}\label{grow_at_d_inv}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Next, we quote the &lt;span class="caps"&gt;CAPM&lt;/span&gt;&amp;#8217;s estimate for the return of a stock&lt;span class="math"&gt;\(^1\)&lt;/span&gt; 
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{9} \label{capm}
E(r_{stock, capm}) = r_{risk free} + \beta \times \left (r_{market} - r_{risk free} \right).
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Here, &lt;span class="math"&gt;\(r_{risk free}\)&lt;/span&gt; is typically taken as the rate of growth of short-term treasury bonds, &lt;span class="math"&gt;\(r_{market}\)&lt;/span&gt; is the mean growth rate of the market as a whole &amp;#8212; often the historical growth rate of the S&amp;amp;P500 is used here, and &lt;span class="math"&gt;\(\beta\)&lt;/span&gt; is the slope in a fit of the stock&amp;#8217;s past total returns to those of the market.  Tests of this prediction have been carried out, and the model has been found to be fairly accurate.  We can therefore approximate the market&amp;#8217;s expectation for the return on the stock to be that given by &lt;span class="caps"&gt;CAPM&lt;/span&gt;.  Equating &lt;span class="math"&gt;\(E(r_{stock})\)&lt;/span&gt; and &lt;span class="math"&gt;\(E(r_{stock, capm})\)&lt;/span&gt; from the last two lines gives the &lt;span class="caps"&gt;CAPM&lt;/span&gt; approximation for the market discount&amp;nbsp;rate,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
d_{market} &amp;amp;\approx&amp;amp; \exp \left (-E(r_{stock, capm}) \right) \\
&amp;amp;=&amp;amp; \exp \left ( - r_{risk free} - \beta \times \left (r_{market} - r_{risk free} \right) \right)
 \tag{10} \label{market_d}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
These equations give us a reasonable method for setting &lt;span class="math"&gt;\(d\)&lt;/span&gt;, enabling us to estimate what the market as a whole should think is a fair price for a given stock.  We turn now to our application, the pricing of &lt;span class="caps"&gt;AT&lt;/span&gt;&amp;amp;T&amp;#8217;s common&amp;nbsp;stock.&lt;/p&gt;
&lt;h1&gt;Application: &lt;span class="caps"&gt;AT&lt;/span&gt;&amp;amp;T (ticker&amp;nbsp;T)&lt;/h1&gt;
&lt;p&gt;We are now ready to consider the fair market value of &lt;span class="caps"&gt;AT&lt;/span&gt;&amp;amp;T&amp;#8217;s common stock.  Recall that we need three quantities to price the stock, &lt;span class="math"&gt;\(D_0\)&lt;/span&gt;, &lt;span class="math"&gt;\(d\)&lt;/span&gt;, and &lt;span class="math"&gt;\(g\)&lt;/span&gt;. We consider each of these&amp;nbsp;below.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span class="caps"&gt;THE&lt;/span&gt; &lt;span class="caps"&gt;MOST&lt;/span&gt; &lt;span class="caps"&gt;RECENT&lt;/span&gt; &lt;span class="caps"&gt;DIVIDEND&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;A quick online search for the dividend history of &lt;span class="caps"&gt;AT&lt;/span&gt;&amp;amp;T&amp;#8217;s shows that a &lt;span class="math"&gt;\(52\)&lt;/span&gt; cent dividend was payed in October 2020 &amp;#8212; &lt;a href="https://www.nasdaq.com/market-activity/stocks/t/dividend-history"&gt;link&lt;/a&gt;.&amp;nbsp;Therefore,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
D_0 = 0.52 \tag{10}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
This company pays out dividends&amp;nbsp;quarterly.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span class="caps"&gt;THE&lt;/span&gt; &lt;span class="caps"&gt;DIVIDEND&lt;/span&gt; &lt;span class="caps"&gt;GROWTH&lt;/span&gt; &lt;span class="caps"&gt;RATE&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The link above also shows that the dividend paid by &lt;span class="caps"&gt;AT&lt;/span&gt;&amp;amp;T in 2015 was &lt;span class="math"&gt;\(47\)&lt;/span&gt; cents per quarter.  The historical quarterly growth rate of the dividend over this period has therefore&amp;nbsp;been
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
g_{historical} &amp;amp;=&amp;amp; (52 / 47)^{1 / 20} \\
&amp;amp;\approx &amp;amp; 1.005 \tag{11}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Here, the exponent was one over the number of quarters considered between October 2015 and October 2020 &amp;#8212; 20&amp;nbsp;quarters.&lt;/p&gt;
&lt;p&gt;It is important to note that the historical growth rate is not necessarily that which we can expect going forward.  In fact, &lt;span class="caps"&gt;AT&lt;/span&gt;&amp;amp;T has already declared a dividend for January 2021 of &lt;span class="math"&gt;\(52\)&lt;/span&gt; cents, the same value as was distributed each quarter in 2020.  In each of the past five years, the company raised dividends with each new year.  The upcoming dividend therefore signals a change in growth rate &amp;#8212; at least in the short-term.  If a value near &lt;span class="math"&gt;\(g\approx 1.000\)&lt;/span&gt; better describes the behavior of &lt;span class="caps"&gt;AT&lt;/span&gt;&amp;amp;T in the near term, this will significantly affect its stock price.  We estimate prices for a set of &lt;span class="math"&gt;\(g\)&lt;/span&gt; values below to highlight this&amp;nbsp;dependence.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span class="caps"&gt;THE&lt;/span&gt; &lt;span class="caps"&gt;MARKET&lt;/span&gt; &lt;span class="caps"&gt;DISCOUNT&lt;/span&gt; &lt;span class="caps"&gt;RATE&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;To evaluate the &lt;span class="caps"&gt;CAPM&lt;/span&gt; market discount rate, we need to know a stock&amp;#8217;s beta.  This can be obtained through a regression or it can simply be looked up online.   Doing this for &lt;span class="caps"&gt;AT&lt;/span&gt;&amp;amp;T we see a current value quoted near &lt;span class="math"&gt;\(0.7\)&lt;/span&gt;.  However, over the last five years the value has varied, going as low as &lt;span class="math"&gt;\(0.3\)&lt;/span&gt; or so in 2016 &amp;#8212; &lt;a href="https://www.zacks.com/stock/chart/T/fundamental/beta"&gt;link&lt;/a&gt;.  Since the value has fluctuated over time, we&amp;#8217;ll quote values below for a few possible forward looking betas, &lt;span class="math"&gt;\(\{0.3, 0.5, 0.7, 0.9\}\)&lt;/span&gt;.  To convert these to discount rates, we used the &lt;code&gt;capm_d_quarterly&lt;/code&gt; method below &amp;#8212; python code that implements (\ref{market_d}).  You can see that we assume a risk-free rate of &lt;span class="math"&gt;\(0.07\%\)&lt;/span&gt;, and a market growth rate of &lt;span class="math"&gt;\(9.8\%\)&lt;/span&gt; &amp;#8212; these are the current short-term treasury rates and historical average S&amp;amp;P500 rates, according to an online search.  The discount rates that result for our chosen betas&amp;nbsp;are &lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
d_{beta = 0.3} &amp;amp;=&amp;amp; 0.993 \\
d_{beta = 0.5}  &amp;amp;=&amp;amp; 0.988 \\
d_{beta = 0.7}  &amp;amp;=&amp;amp; 0.983 \\
d_{beta = 0.9}  &amp;amp;=&amp;amp; 0.978 
 \tag{12} \label{discounting_macys}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;em&gt;&lt;span class="caps"&gt;FINAL&lt;/span&gt; &lt;span class="caps"&gt;CAPM&lt;/span&gt; &lt;span class="caps"&gt;PRICES&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We now have everything we need to estimate a fair market price for &lt;span class="caps"&gt;AT&lt;/span&gt;&amp;amp;T.  We simply need to plug the values for &lt;span class="math"&gt;\(D_0\)&lt;/span&gt;, &lt;span class="math"&gt;\(g\)&lt;/span&gt;, and &lt;span class="math"&gt;\(d\)&lt;/span&gt; we have obtained above into the pricing equation, (\ref{value_geometric}).  Our code snippet below takes care of this for us and the values that it provides are printed&amp;nbsp;underneath.&lt;/p&gt;
&lt;p&gt;Reviewing the output, we see a large range of possible price values as we vary our degrees of freedom &amp;#8212; the values range from &lt;span class="math"&gt;\(18\)&lt;/span&gt; to over &lt;span class="math"&gt;\(200\)&lt;/span&gt;! (The NaN value in the table corresponds to a situation where the growth rate is larger than the discount rate, which is not possible long term).  These results indicate that there is strong sensitivity in the price of a stock to its growth rate and beta.  E.g., we note that if we plug in today&amp;#8217;s values of &lt;span class="math"&gt;\(\beta = 0.7\)&lt;/span&gt; and &lt;span class="math"&gt;\(g = 1.000\)&lt;/span&gt; (marked with an * in the table below), we get a value of &lt;span class="math"&gt;\(29.39\)&lt;/span&gt; dollars  &amp;#8212;  amazingly close to the last traded value of &lt;span class="math"&gt;\(29.40\)&lt;/span&gt; on 12/18 (not bad!).  However, we note that if the company were able to quickly return to the old steady growth rate of &lt;span class="math"&gt;\(1.005\)&lt;/span&gt;, we&amp;#8217;d move to the cell just right of the current value, giving a price of &lt;span class="math"&gt;\(41\)&lt;/span&gt; dollars &amp;#8212; an impressive&amp;nbsp;gain.&lt;/p&gt;
&lt;p&gt;Should we buy the stock?  That&amp;#8217;s for you to decide &amp;#8212; hopefully the results here have helped provide a mental framework useful for thinking through that&amp;nbsp;question.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;

&lt;span class="n"&gt;R_RISK_FREE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0007&lt;/span&gt;
&lt;span class="n"&gt;R_MARKET&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.098&lt;/span&gt;
&lt;span class="n"&gt;D_0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.51&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;capm_d_quarterly&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;beta&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;r_risk_free&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;R_RISK_FREE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;r_market&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;R_MARKET&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;    Return the discount rate from CAPM, on a quaterly basis.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;r_annual&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;r_risk_free&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;beta&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r_market&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;r_risk_free&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;r_quarterly&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;r_annual&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mf"&gt;4.0&lt;/span&gt;
    &lt;span class="n"&gt;d_quarterly&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;r_quarterly&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;d_quarterly&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;capm_price&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;beta&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D_0&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;    Price of stock given its beta, dividend growth rate, and most recent dividend in dollars.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;capm_d_quarterly&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;beta&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;D_0&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;beta&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.99&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.995&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.005&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.01&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;results&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;
            &lt;span class="s1"&gt;&amp;#39;beta&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;beta&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s1"&gt;&amp;#39;g&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s1"&gt;&amp;#39;price&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;capm_price&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;beta&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;beta&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D_0&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;D_0&lt;/span&gt;&lt;span class="p"&gt;)})&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;results&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pivot_table&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;price&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;beta&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;g&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# # output&lt;/span&gt;
&lt;span class="c1"&gt;# g         0.990      0.995      1.000       1.005       1.010&lt;/span&gt;
&lt;span class="c1"&gt;# beta                                                         &lt;/span&gt;
&lt;span class="c1"&gt;# 0.3   28.850623  40.594413  67.995569  204.979922         NaN&lt;/span&gt;
&lt;span class="c1"&gt;# 0.5   22.526179  29.140479  41.082910   69.133459  213.387273&lt;/span&gt;
&lt;span class="c1"&gt;# 0.7   18.459807  22.703362  29.392585*  41.497605   70.069095&lt;/span&gt;
&lt;span class="c1"&gt;# 0.9   15.625393  18.579209  22.856848   29.605283   41.834554&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Footnotes&lt;/h1&gt;
&lt;p&gt;[1]  For a simple derivation of &lt;span class="caps"&gt;CAPM&lt;/span&gt;&amp;#8217;s main results, see &lt;a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=364760"&gt;Derman&lt;/a&gt;.  As a caveat, we note that one of the primary assumptions of &lt;span class="caps"&gt;CAPM&lt;/span&gt; is that market prices are set by rational market&amp;nbsp;participants.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Finance"></category></entry><entry><title>Utility engines</title><link href="https://efavdb.com/utility-engines" rel="alternate"></link><published>2020-09-13T09:24:00-07:00</published><updated>2020-09-13T09:24:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2020-09-13:/utility-engines</id><summary type="html">&lt;p&gt;A person&amp;#8217;s happiness does not depend only on their current lot in life, but
also on the rate of change of their lot.  This is because a person&amp;#8217;s prior
history informs their expectations.  Here, we build a model that highlights
this emotional &amp;#8220;path-dependence&amp;#8221; quality of utility.  Interestingly, we …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A person&amp;#8217;s happiness does not depend only on their current lot in life, but
also on the rate of change of their lot.  This is because a person&amp;#8217;s prior
history informs their expectations.  Here, we build a model that highlights
this emotional &amp;#8220;path-dependence&amp;#8221; quality of utility.  Interestingly, we find
that it can be gamed: One can increase net happiness via a process of gradual
increased deprivation, followed by sudden jolts in increased consumption, as
shown in the cartoon below &amp;#8212; this is the best approach.  In particular, it
beats the steady consumption rate&amp;nbsp;strategy.&lt;/p&gt;
&lt;p align="center"&gt;
         &lt;img src="images/engine.png"&gt;
&lt;/p&gt;

&lt;h3&gt;The utility function&amp;nbsp;model&lt;/h3&gt;
&lt;p&gt;In this post, we assume that the utility realized by a person over a time &lt;span class="math"&gt;\(T\)&lt;/span&gt;
is given&amp;nbsp;by
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{1} \label{1}  U(t) = \int_0^T \left (a \vert
x^{\prime}(t) \vert + b \vert x^{\prime}(t) \vert^2 \right)
\text{sign}(x^{\prime}(t)) dt.
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Here, &lt;span class="math"&gt;\(x(t)\)&lt;/span&gt; is a measure of consumption (one&amp;#8217;s &amp;#8220;lot in life&amp;#8221;) at time &lt;span class="math"&gt;\(t\)&lt;/span&gt;.
Our model here is a natural Taylor expansion, relevant for small changes in
&lt;span class="math"&gt;\(x(t)\)&lt;/span&gt;.  It is positive when consumption is going up and negative when
consumption is going down. We&amp;#8217;ll be interested to learn whether varying &lt;span class="math"&gt;\(x(t)\)&lt;/span&gt;
subject to a fixed average constraint can result in increased net happiness,
relative to the steady state consumption solution.  The answer is yes, and this
can be understood qualitatively by considering the two terms&amp;nbsp;above:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;First term,  &lt;span class="math"&gt;\(a \vert x^{\prime}(t) \vert \text{sign}(x^{\prime}(t))\)&lt;/span&gt;:  This
  term is proportional to the rate of change of consumption.
  We assume that &lt;span class="math"&gt;\(a &amp;gt; 0\)&lt;/span&gt;, so that as we start to consume less, it is negative,
as we go back up it is positive.  We will be interested in cycles that repeat
here, so will assume that our &lt;span class="math"&gt;\(x(t)\)&lt;/span&gt; is periodic with period &lt;span class="math"&gt;\(T\)&lt;/span&gt;.   In this
case, the linear term &amp;#8212; while possibly acutely felt at each moment &amp;#8212; will
integrate to an average of zero over the long&amp;nbsp;term.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Second term, &lt;span class="math"&gt;\(b \vert x^{\prime}(t) \vert^2 \text{sign}(x^{\prime}(t))\)&lt;/span&gt;: This
  term is non-linear.  It is very weak for small rates of change but kicks in
strongly whenever we have an abrupt change.  We again assume that &lt;span class="math"&gt;\(b&amp;gt;0\)&lt;/span&gt; so that
big drops in consumption are very painful,&amp;nbsp;etc.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With the above comments in place, we can now see how our figure gives a net
gain in utility:  On average, only the quadratic term matters and this will
effectively only contribute during sudden jumps.  The declines in our figure
are gradual, and so contribute only weakly in this term.  However, the
increases are sudden and each give a significant utility &amp;#8220;fix&amp;#8221; as a&amp;nbsp;consequence.&lt;/p&gt;
&lt;p&gt;For those interested, we walk through the simple mathematics needed to exactly
optimize our utility function in an appendix.  Concluding comments on the
practical application of these ideas are covered&amp;nbsp;next.&lt;/p&gt;
&lt;h3&gt;Practical&amp;nbsp;considerations&lt;/h3&gt;
&lt;p&gt;A few&amp;nbsp;comments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Many people treat themselves on occasion &amp;#8212; with chocolates, vacations, etc.
  &amp;#8212; perhaps empirically realizing that varying things improves their long
  term happiness.  It is interesting to consider the possibility of optimizing
  this effect, which we do with our toy model here:  In this model, we do not
  want to live in a steady state salted with occasional treats:  Instead, we
  want the saw-tooth shape of consumption shown in our&amp;nbsp;figure.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A sad fact of life is that progress tends to be gradual, while set backs tend
  to occur suddenly &amp;#8212; e.g., stocks tend to move in patterns like this.  This
  is the worst way things could go, according to our&amp;nbsp;model.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;True, human utility functions are certainly more complex than what we have
  considered&amp;nbsp;here.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It is interesting to contrast models of utility with conservative physical
  systems, where the energy of a state is not path dependent, but depends only
on the current state.  Path dependence means that two identical people in the
same current situation can have very different valuations of their lot in&amp;nbsp;life.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The appendix below discusses the mathematical optimization of&amp;nbsp;(\ref{1}).&lt;/p&gt;
&lt;h3&gt;Appendix &amp;#8212; optimizing&amp;nbsp;(\ref{1})&lt;/h3&gt;
&lt;p&gt;For simplicity, we consider a path that goes down from &lt;span class="math"&gt;\(t=0\)&lt;/span&gt; to &lt;span class="math"&gt;\(t_0\)&lt;/span&gt; &amp;#8212; making
its way down by &lt;span class="math"&gt;\(\Delta x\)&lt;/span&gt;, then goes back up to where it started from &lt;span class="math"&gt;\(t_0\)&lt;/span&gt; to
&lt;span class="math"&gt;\(T\)&lt;/span&gt;.  It is easy to see that the first term integrates to zero in this case,
provided we start and end at the same value of &lt;span class="math"&gt;\(x\)&lt;/span&gt;.  Now, consider the second
term.  On the way down, we&amp;nbsp;have
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} 
\int_0^{t_0} \vert x^{\prime} \vert^2 dt &amp;amp;\equiv &amp;amp; t_0 \langle \vert x^{\prime}
\vert^2 \rangle_{t_0}
\\ &amp;amp;\geq &amp;amp; t_0 \langle \vert x^{\prime} \vert \rangle^2_{t_0}
\\
&amp;amp;=&amp;amp; t_0 \left( \frac{\Delta x}{t_0} \right)^2
\tag{2}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
The inequality here is equivalent to the statement that the variance of the
rate of change of our consumption is positive.  We get equality &amp;#8212; and minimal
loss from the quadratic term on the way down &amp;#8212; if the slope is constant
throughout.  That is, we want a linear drop in &lt;span class="math"&gt;\(x(t)\)&lt;/span&gt; from &lt;span class="math"&gt;\(0\)&lt;/span&gt; to &lt;span class="math"&gt;\(t_0\)&lt;/span&gt;.  With
this choice, we&amp;nbsp;get
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
\int_0^{t_0} \vert x^{\prime} \vert^2 dt = \frac{\Delta x^2}{t_0}.  \tag{3}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
On the way back up, we&amp;#8217;d like to max out the inequality analogous to above.
This is achieved by having the recovery occur as quickly as possible, say over
a window of time &lt;span class="math"&gt;\(t_{r}\)&lt;/span&gt; with &lt;span class="math"&gt;\(r\)&lt;/span&gt; standing for recovery.  We can decrease our
loss by increasing &lt;span class="math"&gt;\(t_0\)&lt;/span&gt; up to &lt;span class="math"&gt;\(t_0 \to T\)&lt;/span&gt;.  In this case, our integral of
the quadratic over all time goes&amp;nbsp;to
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
\text{gain} = b \Delta x^2 \left (\frac{1}{t_r} - \frac{1}{T} \right)
\tag{4}.
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
This gives the optimal lift possible &amp;#8212; that realized by the saw-tooth approach
shown in our first figure&amp;nbsp;above. &lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Case studies"></category></entry><entry><title>2 + 1 = 4, by quinoa</title><link href="https://efavdb.com/quinoa%20packing" rel="alternate"></link><published>2020-07-03T00:00:00-07:00</published><updated>2020-07-03T00:00:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2020-07-03:/quinoa packing</id><summary type="html">&lt;p align="center"&gt;
         &lt;img src="images/quinoa.jpg"&gt;
&lt;/p&gt;

&lt;p&gt;I was struck the other day by the following:  The cooking instructions on my
Bob&amp;#8217;s tri-colored quinoa package said to combine 2 cups of water with 1 cup of
dried quinoa, which would ultimately create 4 cups of cooked quinoa.  See image&amp;nbsp;above.&lt;/p&gt;
&lt;p&gt;My first reaction was to believe …&lt;/p&gt;</summary><content type="html">&lt;p align="center"&gt;
         &lt;img src="images/quinoa.jpg"&gt;
&lt;/p&gt;

&lt;p&gt;I was struck the other day by the following:  The cooking instructions on my
Bob&amp;#8217;s tri-colored quinoa package said to combine 2 cups of water with 1 cup of
dried quinoa, which would ultimately create 4 cups of cooked quinoa.  See image&amp;nbsp;above.&lt;/p&gt;
&lt;p&gt;My first reaction was to believe that some error had been made.  However, I
then realized that the explanation was packing:  When one packs spheres or
other awkward solid geometric shapes into a container, they cannot fill the
space completely.  Little pockets of air sit between the spheres.  A quick
google search for the packing fraction of spheres gives a value of &lt;span class="math"&gt;\(0.75\)&lt;/span&gt; for a
crystalline structure and about &lt;span class="math"&gt;\(0.64\)&lt;/span&gt; for random packings &amp;#8212; apparently a
universal&amp;nbsp;law.&lt;/p&gt;
&lt;p&gt;We can get a similar number out from my quinoa instructions: Suppose that
before the quinoa is cooked, the water fills its volume completely.  However,
after cooking, the water is absorbed into the quinoa and forced to share its
packing fraction.  The quinoa stays at the same packing fraction before and
after cooking, so the water must be responsible for the volume growth.  This
implies it went from 2 cups to 3,&amp;nbsp;or
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{1} \label{1}
2 = \rho \times 3,
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(\rho\)&lt;/span&gt; is the packing fraction of the quinoa &amp;#8220;spheres&amp;#8221;. We conclude that
the packing fraction is &lt;span class="math"&gt;\(\rho = 2/3\)&lt;/span&gt;, very close to the googled value of &lt;span class="math"&gt;\(\rho
= 0.64\)&lt;/span&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Cooking math"></category></entry><entry><title>Long term credit assignment with temporal reward transport</title><link href="https://efavdb.com/ltca" rel="alternate"></link><published>2020-06-29T12:00:00-07:00</published><updated>2020-06-29T12:00:00-07:00</updated><author><name>Cathy Yeh</name></author><id>tag:efavdb.com,2020-06-29:/ltca</id><summary type="html">&lt;p&gt;[&lt;span class="caps"&gt;TOC&lt;/span&gt;]&lt;/p&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;Standard reinforcement learning algorithms struggle with poor sample efficiency in the presence of sparse rewards with long temporal delays between action and effect. To address the long term credit assignment problem, we build on the work of [1] to use “temporal reward transport” (&lt;span class="caps"&gt;TRT&lt;/span&gt;) to augment the immediate …&lt;/p&gt;</summary><content type="html">&lt;p&gt;[&lt;span class="caps"&gt;TOC&lt;/span&gt;]&lt;/p&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;Standard reinforcement learning algorithms struggle with poor sample efficiency in the presence of sparse rewards with long temporal delays between action and effect. To address the long term credit assignment problem, we build on the work of [1] to use “temporal reward transport” (&lt;span class="caps"&gt;TRT&lt;/span&gt;) to augment the immediate rewards of significant state-action pairs with rewards from the distant future using an attention mechanism to identify candidates for &lt;span class="caps"&gt;TRT&lt;/span&gt;. A series of gridworld experiments show clear improvements in learning when &lt;span class="caps"&gt;TRT&lt;/span&gt; is used in conjunction with a standard advantage actor critic&amp;nbsp;algorithm.&lt;/p&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Episodic reinforcement learning (&lt;span class="caps"&gt;RL&lt;/span&gt;) models the interaction of an agent with an environment as a Markov Decision Process with a finite number of time steps &lt;span class="math"&gt;\(T\)&lt;/span&gt;.  The environment dynamics &lt;span class="math"&gt;\(p(s’,r|s, a)\)&lt;/span&gt; are modeled as a joint probability distribution over the next state &lt;span class="math"&gt;\(s'\)&lt;/span&gt; and reward &lt;span class="math"&gt;\(r\)&lt;/span&gt; picked up along the way given the previous state &lt;span class="math"&gt;\(s\)&lt;/span&gt; and action &lt;span class="math"&gt;\(a\)&lt;/span&gt;.  In general, the agent does not have access to an exact model of the&amp;nbsp;environment.&lt;/p&gt;
&lt;p&gt;The agent&amp;#8217;s goal is to maximize its cumulative rewards, the discounted returns &lt;span class="math"&gt;\(G_t\)&lt;/span&gt;,&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{return} \tag{1}
G_t := R_{t+1} + \gamma R_{t+2} + … = \sum_{k=0}^T \gamma^k R_{t+k+1}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(0 \leq \gamma \leq 1\)&lt;/span&gt;, and &lt;span class="math"&gt;\(R_{t}\)&lt;/span&gt; is the reward at time &lt;span class="math"&gt;\(t\)&lt;/span&gt;.  In episodic &lt;span class="caps"&gt;RL&lt;/span&gt;, the discount factor &lt;span class="math"&gt;\(\gamma\)&lt;/span&gt; is often used to account for uncertainty in the future, to favor rewards now vs. later, and as a variance reduction technique, e.g. in policy gradient methods [2,&amp;nbsp;3].&lt;/p&gt;
&lt;p&gt;Using a discount factor &lt;span class="math"&gt;\(\gamma &amp;lt; 1\)&lt;/span&gt; introduces a timescale by exponentially suppressing rewards in the future by &lt;span class="math"&gt;\(\exp(-n/\tau_{\gamma})\)&lt;/span&gt;.  The number of timesteps it takes for a reward to decay by &lt;span class="math"&gt;\(1/e\)&lt;/span&gt; is &lt;span class="math"&gt;\(\tau_{\gamma} = 1/(1-\gamma)\)&lt;/span&gt;, in units of timesteps, which follows from solving for &lt;span class="math"&gt;\(n\)&lt;/span&gt; after setting the left and right sides of (\ref{discount-timescale}) to be&amp;nbsp;equal&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\label{discount-timescale}\tag{2}
\gamma ^ n \approx \frac{1}{e} = \lim_{n \rightarrow \infty} \left(1 - \frac{1}{n} \right)^n
\end{align}&lt;/div&gt;
&lt;hr&gt;
&lt;p&gt;The state value function &lt;span class="math"&gt;\(v_{\pi}(s)\)&lt;/span&gt; is the expected return when starting in state &lt;span class="math"&gt;\(s\)&lt;/span&gt;, following policy &lt;span class="math"&gt;\(\pi(a|s) := p(a|s)\)&lt;/span&gt;, a function of the current&amp;nbsp;state.&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{state-value} \tag{3}
v_{\pi}(s) = \mathbb{E}_{\pi}[G_t | S_t = s]
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;Policy gradient algorithms improve the policy by using gradient ascent along the gradient of the value&amp;nbsp;function.&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{policy-gradient} \tag{4} 
\nabla_{\theta} v_\pi(s_0) = \mathbb{E}_{\tau \sim \pi}\left[\sum_{t=0}^{T-1} \nabla_{\theta} \log \pi_{\theta}(A_t | S_t) \mathcal{R}(\tau)\right],
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\tau \sim \pi\)&lt;/span&gt; describes the agent&amp;#8217;s trajectory following policy &lt;span class="math"&gt;\(\pi\)&lt;/span&gt; beginning from state &lt;span class="math"&gt;\(s_0\)&lt;/span&gt;, and &lt;span class="math"&gt;\(\mathcal{R}(\tau)\)&lt;/span&gt; is a function of the rewards obtained along the trajectory.  In practice, policy gradients approximate the expected value in (\ref{policy-gradient}) by sampling, which results in very high variance estimates of the&amp;nbsp;gradient.&lt;/p&gt;
&lt;p&gt;Common techniques to reduce the variance of the estimated policy gradient include&amp;nbsp;[2]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;only assigning credit for rewards (the &amp;#8220;rewards-to-go&amp;#8221;) accumulated after a particular action was taken instead of crediting the action for all rewards from the&amp;nbsp;trajectory.&lt;/li&gt;
&lt;li&gt;subtracting a baseline from the rewards weight that is independent of action.  Oftentimes, this baseline is the value function in&amp;nbsp;(\ref{state-value}).&lt;/li&gt;
&lt;li&gt;using a large batch&amp;nbsp;size.&lt;/li&gt;
&lt;li&gt;using the value function (\ref{state-value}) to bootstrap the returns some number of steps into the future instead of using the full raw discounted return, giving rise to a class of algorithms called actor critics that learn a policy and value function in parallel.  For example, one-step bootstrapping would approximate the discounted returns in (\ref{return})&amp;nbsp;as&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{bootstrap} \tag{5}
G_t = R_{t+1} + \gamma R_{t+2} +  \gamma^2 R_{t+3} + \ldots \approx R_{t+1} + \gamma V(S_{t+1}),
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(V(S_{t+1})\)&lt;/span&gt; is the estimate of the value of state &lt;span class="math"&gt;\(S_{t+1}\)&lt;/span&gt;&amp;nbsp;(\ref{state-value}).&lt;/p&gt;
&lt;p&gt;All of these techniques typically make use of discounting, so an action receives little credit for rewards that happen more than &lt;span class="math"&gt;\(\tau_{\gamma}\)&lt;/span&gt; timesteps in the future, making it challenging for standard reinforcement learning algorithms to learn effective policies in situations where action and effect are separated by long temporal&amp;nbsp;delays.&lt;/p&gt;
&lt;h1&gt;Results&lt;/h1&gt;
&lt;h2&gt;Temporal reward&amp;nbsp;transport&lt;/h2&gt;
&lt;p&gt;We use temporal reward transport (or &lt;span class="caps"&gt;TRT&lt;/span&gt;), inspired directly by the Temporal Value Transport algorithm from [1], to mitigate the loss of signal from discounting by splicing temporally delayed future rewards to the immediate rewards following an action that the &lt;span class="caps"&gt;TRT&lt;/span&gt; algorithm determines should receive&amp;nbsp;credit.&lt;/p&gt;
&lt;p&gt;To assign credit to a specific observation-action pair, we use an attention layer in a neural network binary classifier.  The classifier predicts whether the undiscounted returns for an episode are below or above a certain threshold.  If a particular observation and its associated action are highly attended to for the classification problem, then that triggers the splicing of future rewards in the episode to that particular observation-action&amp;nbsp;pair.&lt;/p&gt;
&lt;p&gt;Model training is divided into two&amp;nbsp;parts:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Experience collection using the current policy in an advantage actor critic (&lt;span class="caps"&gt;A2C&lt;/span&gt;)&amp;nbsp;model.&lt;/li&gt;
&lt;li&gt;Parameter updates for the &lt;span class="caps"&gt;A2C&lt;/span&gt; model and binary&amp;nbsp;classifier.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class="caps"&gt;TRT&lt;/span&gt; happens between step 1 and 2; it plays no role in experience collection, but modifies the collected rewards through the splicing mechanism, thereby affecting the advantage and, consequently, the policy gradient in&amp;nbsp;(\ref{policy-gradient}).&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \notag
R_t \rightarrow R_t + [\text{distal rewards} (t' &amp;gt; t + \tau_\gamma)]
\end{eqnarray}&lt;/div&gt;
&lt;h2&gt;Environment for&amp;nbsp;experiments&lt;/h2&gt;
&lt;p&gt;We created a &lt;a href="https://github.com/openai/gym"&gt;gym&lt;/a&gt; &lt;a href="https://github.com/maximecb/gym-minigrid"&gt;gridworld&lt;/a&gt; environment to specifically study long term credit assignment.  The environment is a simplified version of the 3-d DeepMind Lab experiments laid out in [1].  As in [1], we structure the environment to comprise three phases.  In the first phase, the agent must take an action that yields no immediate reward.  In the second phase, the agent engages with distractions that yield immediate rewards.  In the final phase, the agent can acquire a distal reward, depending on the action it took in phase&amp;nbsp;1.&lt;/p&gt;
&lt;p&gt;Concretely, the gridworld environment consists&amp;nbsp;of:&lt;/p&gt;
&lt;p&gt;(1) Empty grid with key: agent can pick up the key but receives no immediate reward for picking it&amp;nbsp;up.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="images/ltca_gridworld_p1.png" alt="Phase 1" style="width:190px;"&gt;
&lt;/p&gt;

&lt;p&gt;(2) Distractor phase: Agent engages with distractors, gifts that yield immediate&amp;nbsp;rewards.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="images/ltca_gridworld_p2.png" alt="Phase 1" style="width:190px;"&gt;
&lt;/p&gt;

&lt;p&gt;(3) Delayed reward phase: Agent should move to a green goal grid cell.  If the agent is carrying the key when it reaches the goal, it is rewarded extra&amp;nbsp;points.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="images/ltca_gridworld_p3.png" alt="Phase 1" style="width:190px;"&gt;
&lt;/p&gt;

&lt;p&gt;The agent remains in each phase for a fixed period of time, regardless of how quickly it finishes the intermediate task, and then teleports to the next phase.  At the end of each episode, the environment resets with a different random seed that randomizes the placement of the key in phase 1 and distractor objects in phase&amp;nbsp;2.&lt;/p&gt;
&lt;h2&gt;Experiments&lt;/h2&gt;
&lt;p&gt;In all experiments, we fix the time spent in phase 1 and phase 3, the number of distractor gifts in phase 2, as well as the distal reward in phase 3.  In phase 3, the agent receives 5 points for reaching the goal without a key and 20 points for reaching the goal carrying a key (with a small penalty proportional to step count to encourage moving quickly to the&amp;nbsp;goal).&lt;/p&gt;
&lt;p&gt;Our evaluation metric for each experiment is the the distal reward obtained in phase 3, which focuses on whether the agent learns to pick up the key in phase 1 in order to acquire the distal reward, although we verify that the agent is also learning to open the gifts in phase 2 by plotting the overall returns (see &amp;#8220;Data and code availability&amp;#8221;&amp;nbsp;section).&lt;/p&gt;
&lt;p&gt;Each experiment varies a particular parameter in the second phase, namely, the time delay, distractor reward size, and distractor reward variance, and compares the performance of the baseline &lt;span class="caps"&gt;A2C&lt;/span&gt; algorithm with &lt;span class="caps"&gt;A2C&lt;/span&gt; supplemented with &lt;span class="caps"&gt;TRT&lt;/span&gt; (&lt;span class="caps"&gt;A2C&lt;/span&gt;+&lt;span class="caps"&gt;TRT&lt;/span&gt;).&lt;/p&gt;
&lt;h3&gt;Time delay in distractor&amp;nbsp;phase&lt;/h3&gt;
&lt;p&gt;We vary the time spent in the distractor phase, &lt;span class="math"&gt;\(T_2\)&lt;/span&gt;, as a multiple of the discount factor timescale.  We used a discount factor of &lt;span class="math"&gt;\(\gamma=0.99\)&lt;/span&gt;, which corresponds to a timescale of ~100 steps according to (\ref{discount-timescale}).  We ran experiments for &lt;span class="math"&gt;\(T_2 = (0, 0.5, 1, 2) * \tau_{\gamma}\)&lt;/span&gt;.  The distractor reward is 3 points per&amp;nbsp;gift.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="images/ltca_time_delay_expt_plots.png" alt="P2 time delay expt" style="width:800px;"&gt;
&lt;/p&gt;

&lt;p&gt;&lt;small&gt;Fig 1. Returns in phase 3 for time delays in phase 2 of 0.5&lt;span class="math"&gt;\(\tau_{\gamma}\)&lt;/span&gt;, &lt;span class="math"&gt;\(\tau_{\gamma}\)&lt;/span&gt;, and 2&lt;span class="math"&gt;\(\tau_{\gamma}\)&lt;/span&gt;.
&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;As the environment becomes more challenging from left to right with increasing time delay, we see that &lt;span class="caps"&gt;A2C&lt;/span&gt; plateaus around 5 points in phase 3, corresponding to reaching the goal without the key, whereas &lt;span class="caps"&gt;A2C&lt;/span&gt;+&lt;span class="caps"&gt;TRT&lt;/span&gt; increasingly learns to pick up the key over the training&amp;nbsp;period.&lt;/p&gt;
&lt;h3&gt;Distractor reward&amp;nbsp;size&lt;/h3&gt;
&lt;p&gt;We vary the size of the distractor rewards, 4 gifts for the agent to toggle open, in phase 2.  We run experiments for a reward of 0, 1, 5, and 8, resulting in maximum possible rewards in phase 2 of 0, 4, 20, and&amp;nbsp;32.&lt;/p&gt;
&lt;p&gt;In comparison, the maximum possible reward in phase 3 is&amp;nbsp;20.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="images/ltca_reward_expt_plots.png" alt="P2 reward size expt" style="width:800px;"&gt;
&lt;/p&gt;

&lt;p&gt;&lt;small&gt;Fig 2. Returns in phase 3 for distractor rewards of size 0, 5, and 8.
&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;Like the time delay experiments, we see that &lt;span class="caps"&gt;A2C&lt;/span&gt;+&lt;span class="caps"&gt;TRT&lt;/span&gt; shows progress learning to pick up the key, whereas &lt;span class="caps"&gt;A2C&lt;/span&gt; does not over the training period with increasing distractor&amp;nbsp;sizes.&lt;/p&gt;
&lt;h3&gt;Distractor reward&amp;nbsp;variance&lt;/h3&gt;
&lt;p&gt;We fix the mean reward size of the gifts in phase 2 at 5, but change the variance of the rewards by drawing each reward from a uniform distribution centered at 5, with minimum and maximum ranges of [5, 5], [3, 7], and [0, 10], corresponding to variances of 0, 1.33, and 8.33,&amp;nbsp;respectively.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="images/ltca_reward_var_expt_plots.png" alt="P2 reward variance expt" style="width:800px;"&gt;
&lt;/p&gt;

&lt;p&gt;&lt;small&gt;Fig 3. Returns in phase 3 for distractor reward variance of size 0, 1.33, and 8.33.
&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;The signal-to-noise ratio of the policy gradient, defined as the ratio of the squared magnitude of the expected gradient to the variance of the gradient estimate, was shown to be approximately inversely proportional to the variance of the distractor rewards in phase 2 in [1] for &lt;span class="math"&gt;\(\gamma = 1\)&lt;/span&gt;.  The poor performance of &lt;span class="caps"&gt;A2C&lt;/span&gt; in the highest variance (low signal-to-noise ratio) case is consistent with this observation, with a small standard deviation in performance around the plateau value of 5 compared to the experiments on time delay and distractor reward&amp;nbsp;size.&lt;/p&gt;
&lt;h1&gt;Discussion&lt;/h1&gt;
&lt;p&gt;Like temporal value transport introduced in [1], &lt;span class="caps"&gt;TRT&lt;/span&gt; is a heuristic.  Nevertheless, coupling this heuristic with &lt;span class="caps"&gt;A2C&lt;/span&gt; has been shown to improve performance on several tasks characterized by delayed rewards that are a challenge for standard deep &lt;span class="caps"&gt;RL&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Our contribution is a simplified, modular implementation of core ideas in [1], namely, splicing additional rewards from the distant future to state-action pairs identified as significant through a self-attention mechanism.  Unlike [1], we implement the self-attention mechanism in a completely separate model and splice the rewards-to-go instead of an estimated value.  In addition to the modularity that comes splitting out the attention mechanism for &lt;span class="caps"&gt;TRT&lt;/span&gt; into a separate model, another advantage of decoupling the models is that we can increase the learning rate of the classifier without destabilizing the learning of the main actor critic model if the classification problem is comparatively&amp;nbsp;easy.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Related&amp;nbsp;work&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Other works also draw on the idea of using hindsight to reduce the variance estimates of the policy gradient, and hence increase sample efficiency.  &amp;#8220;Hindsight credit assignment&amp;#8221; proposed in [7] similarly learns discriminative models in hindsight that give rise to a modified form of the value function, evaluated using tabular models in a few toy environments (not focused specifically on the long term credit assignment problem).  &lt;span class="caps"&gt;RUDDER&lt;/span&gt; [8] is more similar in spirit to [1] and &lt;span class="caps"&gt;TRT&lt;/span&gt; in the focus on redistributing rewards to significant state-action pairs, but identified using saliency analysis on an &lt;span class="caps"&gt;LSTM&lt;/span&gt; instead of an attention&amp;nbsp;mechanism.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Future&amp;nbsp;work&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The robustness of the &lt;span class="caps"&gt;TRT&lt;/span&gt; algorithm should be further assessed on a wider variety of environments, including e.g. Atari Bowling, which is another environment with a delayed reward task used for evaluations by [1] and [8].  It remains to be seen whether the attention mechanism and &lt;span class="caps"&gt;TRT&lt;/span&gt; can handle more complex scenarios, in particular scenarios where a sequence of actions must be taken.  Just as it is difficult to extract interpretable features from a linear model in the presence of multicollinearity, it is possible that the attention-based classifier may encounter similar problems identifying important state-action pairs when a sequence of actions is required, as our model has no mechanism for causal&amp;nbsp;reasoning.&lt;/p&gt;
&lt;p&gt;Although our experiments only evaluated &lt;span class="caps"&gt;TRT&lt;/span&gt; on &lt;span class="caps"&gt;A2C&lt;/span&gt;, coupling it with any policy gradient method based on sampling action space should yield similar benefits, which could be straightforwardly tested with our modular&amp;nbsp;implementation.&lt;/p&gt;
&lt;p&gt;A benefit of using self-attention is the temporal granularity over an &lt;span class="caps"&gt;LSTM&lt;/span&gt;.  However, a downside is that our approach relies on having the full context of the episode for the attention mechanism ([1] similarly relies on full episodes), in contrast to other methods that can handle commonly used truncation windows with a bootstrapped final value for non-terminal states.  Holding full episodes in memory can become untenable for very long episodes, but we have not yet worked out a way to handle this situation in the current&amp;nbsp;setup.&lt;/p&gt;
&lt;p&gt;Our first pass implementation transported the raw rewards-to-go instead of the value estimate used in [1], but it is unclear whether transporting the rewards-to-go (essentially re-introducing a portion of the undiscounted Monte Carlo returns) for a subset of important state-action pairs provides a strong enough signal to outweigh the advantages of using a boostrapped estimate intended for variance reduction; the answer may depend on the particular task/environment and is of course contingent on the quality of the value&amp;nbsp;estimate.&lt;/p&gt;
&lt;p&gt;The classifier model itself has a lot of room for experimentation.  The idea of using a classifier was motivated by a wish to easily extract state-action pairs with high returns from the attention layer, although we have yet to explore whether this provides a clear benefit over a regression model like&amp;nbsp;[1].&lt;/p&gt;
&lt;p&gt;The binary classifier is trained to predict whether the rewards-to-go of each subsequence of an episode exceeds a moving average of maximum returns.  On the one hand, this is less intuitive than only making the prediction for undiscounted returns of the full episode and introduces highly non-iid inputs for the classifier, which can make make training less stable.  On the other hand, one can interpret the current format as a form of data augmentation that results in more instances of the positive class (high return episodes) that benefits the&amp;nbsp;classifier.&lt;/p&gt;
&lt;p&gt;If the classifier were modified to only make a single prediction per episode, it may be necessary to create a buffer of recent experiences to shift the distribution of data towards more positive samples for the classifier to draw from in addition to episodes generated from the most recent policy (with the untested assumption that the classifier would be less sensitive to training on such off-policy data than the main actor critic model while benefiting from the higher incidence of the positive&amp;nbsp;class).&lt;/p&gt;
&lt;p&gt;Finally, the &lt;span class="caps"&gt;TRT&lt;/span&gt; algorithm introduces additional hyperparameters that could benefit from additional tuning, including the constant factor multiplying the transported rewards and the attention score threshold to trigger &lt;span class="caps"&gt;TRT&lt;/span&gt;.&lt;/p&gt;
&lt;h1&gt;Methods&lt;/h1&gt;
&lt;h2&gt;Environment&lt;/h2&gt;
&lt;p&gt;The agent receives a partial observation of the environment, the 7x7 grid in front of it, with each grid cell encoding 3 input values, resulting in 7x7x3 values total (not&amp;nbsp;pixels).&lt;/p&gt;
&lt;p&gt;The gridworld environment supports 7 actions: left, right, forward, pickup, drop, toggle,&amp;nbsp;done.&lt;/p&gt;
&lt;p&gt;The environment consists of three&amp;nbsp;phases:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Phase 1 &amp;#8220;key&amp;#8221;: 6x6 grid cells, time spent = 30&amp;nbsp;steps&lt;/li&gt;
&lt;li&gt;Phase 2 &amp;#8220;gifts&amp;#8221;: 10x10 grid cells, time spent = 50 steps (except for the time delay experiment, which varies the time&amp;nbsp;spent)&lt;/li&gt;
&lt;li&gt;Phase 3 &amp;#8220;goal&amp;#8221;: 7x7 grid cells, time spent =&amp;nbsp;70.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If the agent picks up the key in phase 1, it is initialized carrying the key in phase 3, but not phase 2.  The carrying state is visible to the agent in phase 1 and 3.  Except for the time delay experiment, each episode is 150&amp;nbsp;timesteps.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Distractor rewards in phase&amp;nbsp;2:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;4 distractor objects, gifts that the agent can toggle open, that yield immediate&amp;nbsp;rewards&lt;/li&gt;
&lt;li&gt;Each opened gift yields a mean reward of 3 points (except in the reward size&amp;nbsp;experiment)&lt;/li&gt;
&lt;li&gt;Gift rewards have a variance of 0 (except in the reward variance&amp;nbsp;experiment)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Distal rewards in phase&amp;nbsp;3:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;5 points for reaching the goal without a key and 20 points for reaching the goal carrying a key.  There is a small penalty of &lt;code&gt;-0.9 * step_count / max_steps=70&lt;/code&gt; to encourage moving quickly to the goal.  For convenience of parallelizing experience collection of complete episodes, the time in the final phase is fixed, even if the agent finishes the task of navigating to the green goal earlier.  Furthermore, for convenience of tracking rewards acquired in the final phase, the agent only receives the reward for task completion in the last step of the final phase, even though this last reward reflects the time and state in which the agent initially reached the&amp;nbsp;goal.&lt;/p&gt;
&lt;p&gt;Note, unlike the Reconstructive Memory Agent in [1], our agent does not have the ability to encode and reconstruct memories, and our environment is not set up to test for that&amp;nbsp;ability.&lt;/p&gt;
&lt;h2&gt;Agent&amp;nbsp;model&lt;/h2&gt;
&lt;p&gt;The agent&amp;#8217;s model is an actor critic consisting of 3 components: an image encoder convolutional net (&lt;span class="caps"&gt;CNN&lt;/span&gt;), an recurrent neural net layer providing memory, and dual heads outputting the policy and value.  We used an open-sourced model that has been extensively tested for gym-minigrid environments from&amp;nbsp;[4].&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="images/ltca_a2c_model.png" alt="A2C model" style="width:300px;"&gt;
&lt;/p&gt;

&lt;p&gt;&lt;small&gt;Fig 4. &lt;span class="caps"&gt;A2C&lt;/span&gt; model with three convolutional layers, &lt;span class="caps"&gt;LSTM&lt;/span&gt;, and dual policy and value function heads.
&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;The image encoder consists of three convolutional layers interleaved with rectified linear (ReLU) activation functions.  A max pooling layer also immediately precedes the second convolutional&amp;nbsp;layer.&lt;/p&gt;
&lt;p&gt;The encoded image is followed by a single Long Short Term Memory (&lt;span class="caps"&gt;LSTM&lt;/span&gt;)&amp;nbsp;layer.&lt;/p&gt;
&lt;p&gt;The &lt;span class="caps"&gt;LSTM&lt;/span&gt; outputs a hidden state which feeds into the dual heads of the actor critic.  Both heads consist of two fully connected linear layers sandwiching a tanh activation layer.  The output of the actor, the policy, is the same size as the action space in the environment.  The output of the critic is a scalar corresponding to the estimated&amp;nbsp;value.&lt;/p&gt;
&lt;h2&gt;Binary classifier with&amp;nbsp;self-attention&lt;/h2&gt;
&lt;p&gt;The inputs to the binary classifier are the sequence of image embeddings output by the actor critic model&amp;#8217;s &lt;span class="caps"&gt;CNN&lt;/span&gt; (not the hidden state of the &lt;span class="caps"&gt;LSTM&lt;/span&gt;) and one-hot encoded actions taken in that&amp;nbsp;state.&lt;/p&gt;
&lt;p&gt;The action passes through a linear layer with 32 hidden units before concatenation with the image&amp;nbsp;embedding.&lt;/p&gt;
&lt;p&gt;Next, the concatenated vector &lt;span class="math"&gt;\(\mathbf{x}_i\)&lt;/span&gt; undergoes three separate linear transformations, playing the role of &amp;#8220;query&amp;#8221;, &amp;#8220;key&amp;#8221; and &amp;#8220;value&amp;#8221; (see [6] for an excellent explanation upon which we based our implementation of attention).  Each transformation projects the vector to a space of size equal to the length of the&amp;nbsp;episode.&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{key-query} \tag{6}
\mathbf{q}_i &amp;amp;=&amp;amp; \mathbf{W}_q \mathbf{x}_i \\
\mathbf{k}_i &amp;amp;=&amp;amp; \mathbf{W}_k \mathbf{x}_i \\
\mathbf{v}_i &amp;amp;=&amp;amp; \mathbf{W}_v \mathbf{x}_i \\
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;The self-attention layer outputs a weighted average over the value vectors, where the weight is not a parameter of the neural net, but the dot product of the query and key&amp;nbsp;vectors.&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{self-attention} \tag{7}
w'_{ij} &amp;amp;=&amp;amp; \mathbf{q}_i^\top \mathbf{k}_j \\
w_{ij} &amp;amp;=&amp;amp; \text{softmax}(w'_{ij}) \\
\mathbf{y_i} &amp;amp;=&amp;amp; \sum_j w_{ij} \mathbf{v_j}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;The dot product in (\ref{self-attention}) is between embeddings of different frames in an episode.  We apply masking to the weight matrix before the softmax in (\ref{self-attention}) to ensure that observations from different episodes do not pay attention to each other, in addition to future masking (observations can only attend past observations in the same&amp;nbsp;episode).&lt;/p&gt;
&lt;p&gt;The output of the attention layer then passes through a fully connected layer with 64 hidden units, followed by a ReLU activation, and the final output is a scalar, the logit predicting whether the rewards-to-go from a given observation are below or above a&amp;nbsp;threshold.&lt;/p&gt;
&lt;p&gt;The threshold itself is a moving average over the maximum undiscounted returns seen across network updates, where the averaging window is a hyperparameter that should balance updating the threshold in response to higher returns due to an improving policy (in general, increasing, although monotonicity is not enforced) with not increasing so quickly such that there are too few episodes in the positive (high returns) class in a given batch of collected&amp;nbsp;experiences.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="images/ltca_classifier_model.png" alt="Classifier model" style="width:300px;"&gt;
&lt;/p&gt;

&lt;p&gt;&lt;small&gt;Fig 5. Binary classifier model with attention, accepting sequences as input.
&lt;/small&gt;&lt;/p&gt;
&lt;h2&gt;Temporal reward&amp;nbsp;transport&lt;/h2&gt;
&lt;p&gt;After collecting a batch of experiences by following the &lt;span class="caps"&gt;A2C&lt;/span&gt; model&amp;#8217;s policy, we calculate the attention scores &lt;span class="math"&gt;\(w_{ij}\)&lt;/span&gt; from (\ref{self-attention}) using observations from the full episode as&amp;nbsp;context.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="images/ltca_attention_single_episode.png" alt="Attention scores single episode" style="width:400px;"&gt;
&lt;/p&gt;

&lt;p&gt;&lt;small&gt;Fig 6. Attention scores for a single episode with future masking (of the upper right triangle).  The bright vertical stripes correspond to two highly attended state-action pairs.
&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;We calculate the importance, defined as the average weight of observation &lt;span class="math"&gt;\(i\)&lt;/span&gt;, ignoring masked regions in the attention matrix&amp;nbsp;as&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{importance} \tag{8}
\text{importance}_i = \frac{1}{T - i} \sum_{j \geq i}^{T} w_{ij}
\end{eqnarray}&lt;/div&gt;
&lt;p align="center"&gt;
&lt;img src="images/ltca_importances.png" alt="Importances for a batch of frames" style="width:600px;"&gt;
&lt;/p&gt;

&lt;p&gt;&lt;small&gt;Fig 7. Importances for a batch of collected experiences (16 processes x 600 frames = 9600 frames), with frame or step number on the horizontal axis and process number on the vertical axis.
&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;Observations with an importance score above a threshold (between 0 and 1) hyperparameter are eligible for &lt;span class="caps"&gt;TRT&lt;/span&gt;.  After identifying the candidates for &lt;span class="caps"&gt;TRT&lt;/span&gt;, we add the distal rewards-to-go, weighted by the importance and another hyperparameter for tuning the impact of the &lt;span class="caps"&gt;TRT&lt;/span&gt; rewards &lt;span class="math"&gt;\(\alpha_{TRT}\)&lt;/span&gt; to the original reward &lt;span class="math"&gt;\(r_i\)&lt;/span&gt; obtained during experience&amp;nbsp;collection:&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{trt} \tag{9}
r_i &amp;amp;\rightarrow&amp;amp; r_i + \text{TRT-reward}_i \\
\text{TRT-reward}_i &amp;amp;\equiv&amp;amp; \alpha_{TRT} * \text{importance}_i * \text{rewards-to-go}_i
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;We define the distal rewards-to-go in (\ref{trt}) as the total undiscounted returns from observation &lt;span class="math"&gt;\(i\)&lt;/span&gt;, excluding rewards accumulated in an immediate time window of size equal to the discount factor timescale &lt;span class="math"&gt;\(\tau_\gamma\)&lt;/span&gt; defined in (\ref{discount-timescale}).  This temporal exclusion zone helps prevent overcounting&amp;nbsp;rewards.&lt;/p&gt;
&lt;p&gt;We calculate the advantage after &lt;span class="caps"&gt;TRT&lt;/span&gt; using the generalized advantage estimation algorithm &lt;span class="caps"&gt;GAE&lt;/span&gt;-&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; [3] with &lt;span class="math"&gt;\(\lambda=0.95\)&lt;/span&gt;, which, analogous to &lt;span class="caps"&gt;TD&lt;/span&gt;-&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; [2], calculates the advantage from an exponentially weighted average over 1- to n-step bootstrapped estimates of the &lt;span class="math"&gt;\(Q\)&lt;/span&gt; value.  One of the benefits of using &lt;span class="caps"&gt;GAE&lt;/span&gt;-&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; is the spillover effect that enables the &lt;span class="caps"&gt;TRT&lt;/span&gt;-reinforced rewards to directly affect neighboring state-action pairs in addition to the triggering state-action&amp;nbsp;pair.&lt;/p&gt;
&lt;h2&gt;Training&lt;/h2&gt;
&lt;p&gt;For experience collection, we used 16 parallel processes, with 600 frames collected per process for a batch size of 9600 frames between parameter&amp;nbsp;updates.&lt;/p&gt;
&lt;p&gt;The &lt;span class="caps"&gt;A2C&lt;/span&gt; model loss per time step&amp;nbsp;is&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \label{a2c-loss} \tag{10}
\mathcal{L}_{A2C} \equiv \mathcal{L}_{policy} + \alpha_{value} \mathcal{L}_{value} - \alpha_{entropy} \text{entropy},
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \notag
\mathcal{L}_{policy} &amp;amp;=&amp;amp; - \log p(a_t | o_t, h_t), \\
\mathcal{L}_{value} &amp;amp;=&amp;amp; \left\Vert \hat{V}(o_t, h_t) - R_t \right\Vert^2,
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;and &lt;span class="math"&gt;\(o_t\)&lt;/span&gt; and &lt;span class="math"&gt;\(h_t\)&lt;/span&gt; are the observation and hidden state from the &lt;span class="caps"&gt;LSTM&lt;/span&gt; at time &lt;span class="math"&gt;\(t\)&lt;/span&gt;, respectively.  We accumulate the losses defined in (\ref{a2c-loss}) by iterating over batches of consecutive time steps equal to the size of the &lt;span class="caps"&gt;LSTM&lt;/span&gt; memory of 10, i.e. truncated backpropagation in time for 10&amp;nbsp;timesteps.&lt;/p&gt;
&lt;p&gt;The classifier with attention has a &lt;a href="https://pytorch.org/docs/stable/nn.html#bcewithlogitsloss"&gt;binary cross entropy loss&lt;/a&gt;, where the contribution to the loss from positive examples is weighted by a factor of&amp;nbsp;2.&lt;/p&gt;
&lt;p&gt;We clip both gradient norms according to a hyperparameter &lt;span class="math"&gt;\(\text{max_grad_norm}=0.5\)&lt;/span&gt;, and we optimize both models using RMSprop with learning rates of 0.01, RMSprop &lt;span class="math"&gt;\(\alpha=0.99\)&lt;/span&gt;, and RMSprop &lt;span class="math"&gt;\(\epsilon=1e^{-8}\)&lt;/span&gt;.&lt;/p&gt;
&lt;h1&gt;Data and code&amp;nbsp;availability&lt;/h1&gt;
&lt;h2&gt;Environment&lt;/h2&gt;
&lt;p&gt;Code for the components of the 3 phase environment is in our &lt;a href="https://github.com/frangipane/gym-minigrid"&gt;fork&lt;/a&gt; of &lt;a href="https://github.com/maximecb/gym-minigrid"&gt;gym-minigrid&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The base environment for running the experiments is defined in &lt;a href="https://github.com/frangipane/rl-credit/blob/master/rl_credit/examples/environment.py"&gt;https://github.com/frangipane/rl-credit/&lt;/a&gt;.  Each experiment script subclasses that base environment, varying some parameter in the distractor&amp;nbsp;phase.&lt;/p&gt;
&lt;h2&gt;Experiments&lt;/h2&gt;
&lt;p&gt;The parameters and results of the experiments are documented in the following publicly available reports on Weights and&amp;nbsp;Biases:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://app.wandb.ai/frangipane/distractor_time_delays/reports/Distractor-Gift-time-delay--VmlldzoxMjYyNzY"&gt;Distractor phase time&amp;nbsp;delays&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://app.wandb.ai/frangipane/distractor_reward_size/reports/Distractor-gift-reward-size--VmlldzoxMjcxMTI"&gt;Distractor phase reward&amp;nbsp;size&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://app.wandb.ai/frangipane/distractor_reward_variance/reports/Distractor-gift-variance--VmlldzoxMjgzNTc"&gt;Distractor phase variance of&amp;nbsp;rewards&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Code for running the experiments is at &lt;a href="https://github.com/frangipane/rl-credit"&gt;https://github.com/frangipane/rl-credit&lt;/a&gt; in the examples/&amp;nbsp;submodule.&lt;/p&gt;
&lt;h1&gt;Acknowledgements&lt;/h1&gt;
&lt;p&gt;Thank you to OpenAI, my OpenAI mentor J. Tworek, Microsoft for the cloud computing credits, Square for supporting my participation in the program, and my 2020 cohort of Scholars: A. Carrera, P. Mishkin, K. Ndousse, J. Orbay, A. Power (especially for the tip about future masking in transformers), and K.&amp;nbsp;Slama.&lt;/p&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;p&gt;[1] Hung C, Lillicrap T, Abramson J, et al. 2019. &lt;a href="https://www.nature.com/articles/s41467-019-13073-w"&gt;Optimizing agent behavior over long time scales by transporting value&lt;/a&gt;. Nat Commun 10,&amp;nbsp;5223.&lt;/p&gt;
&lt;p&gt;[2] Sutton R, Barto A.  2018.  &lt;a href="http://incompleteideas.net/book/RLbook2018.pdf"&gt;Reinforcement Learning: An Introduction (2nd Edition)&lt;/a&gt;.  Cambridge (&lt;span class="caps"&gt;MA&lt;/span&gt;): &lt;span class="caps"&gt;MIT&lt;/span&gt;&amp;nbsp;Press.&lt;/p&gt;
&lt;p&gt;[3] Schulman J, Moritz P, Levine S, et al. 2016. &lt;a href="https://arxiv.org/abs/1506.02438"&gt;High-Dimensional Continuous Control Using Generalized Advantage Estimation&lt;/a&gt;. &lt;span class="caps"&gt;ICLR&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;[4] Willems L.  &lt;a href="https://github.com/lcswillems/rl-starter-files"&gt;&lt;span class="caps"&gt;RL&lt;/span&gt; Starter Files&lt;/a&gt; and &lt;a href="https://github.com/lcswillems/torch-ac"&gt;Torch &lt;span class="caps"&gt;AC&lt;/span&gt;&lt;/a&gt;.&amp;nbsp;GitHub.&lt;/p&gt;
&lt;p&gt;[5] Chevalier-Boisvert M, Willems L, Pal S.  2018.  &lt;a href="https://github.com/maximecb/gym-minigrid"&gt;Minimalistic Gridworld Environment for OpenAI Gym&lt;/a&gt;.&amp;nbsp;GitHub.&lt;/p&gt;
&lt;p&gt;[6] Bloem P.  2019.  &lt;a href="http://www.peterbloem.nl/blog/transformers"&gt;Transformers from Scratch&lt;/a&gt; [blog].  [accessed 2020 May 1].&amp;nbsp;http://www.peterbloem.nl/blog/transformers.&lt;/p&gt;
&lt;p&gt;[7] Harutyunyan A, Dabney W, Mesnard T. 2019.  &lt;a href="http://papers.nips.cc/paper/9413-hindsight-credit-assignment.pdf"&gt;Hindsight Credit Assignment&lt;/a&gt;. Advances in Neural Information Processing Systems 32:&amp;nbsp;12488&amp;#8212;12497.&lt;/p&gt;
&lt;p&gt;[8] Arjona-Medina J, Gillhofer M, Widrich M, et al. 2019.  &lt;a href="https://papers.nips.cc/paper/9509-rudder-return-decomposition-for-delayed-rewards.pdf"&gt;&lt;span class="caps"&gt;RUDDER&lt;/span&gt;: Return Decomposition for Delayed Rewards&lt;/a&gt;.  Advances in Neural Information Processing Systems 32:&amp;nbsp;13566&amp;#8212;13577.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Machine learning"></category><category term="machine learning"></category><category term="reinforcement learning"></category><category term="OpenAI"></category></entry><entry><title>Visualizing an actor critic algorithm in real time</title><link href="https://efavdb.com/visualize-actor-critic" rel="alternate"></link><published>2020-05-07T12:00:00-07:00</published><updated>2020-05-07T12:00:00-07:00</updated><author><name>Cathy Yeh</name></author><id>tag:efavdb.com,2020-05-07:/visualize-actor-critic</id><summary type="html">&lt;p&gt;Deep reinforcement learning algorithms can be hard to debug, so it helps to visualize as much as possible in the absence of a stack trace [1].  How do we know if the learned policy and value functions make sense?  Seeing these quantities plotted in real time as an agent is …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Deep reinforcement learning algorithms can be hard to debug, so it helps to visualize as much as possible in the absence of a stack trace [1].  How do we know if the learned policy and value functions make sense?  Seeing these quantities plotted in real time as an agent is interacting with an environment can help us answer that&amp;nbsp;question.&lt;/p&gt;
&lt;p&gt;Here’s an example of an agent wandering around a custom &lt;a href="https://github.com/frangipane/gym-minigrid"&gt;gridworld&lt;/a&gt; environment.  When the agent executes the &lt;code&gt;toggle&lt;/code&gt; action in front of an unopened red gift, it receives a reward of 1 point, and the gift turns&amp;nbsp;grey/inactive.&lt;/p&gt;
&lt;iframe width="640" height="360" src="https://www.youtube.com/embed/M3PMwPFRoc8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;The model is an actor critic, a type of policy gradient algorithm (for a nice introduction, see Jonathan’s &lt;a href="https://efavdb.com/battleship"&gt;battleship&lt;/a&gt; post or [2]) that uses a neural network to parametrize its policy and value&amp;nbsp;functions.&lt;/p&gt;
&lt;p&gt;This agent barely &amp;#8220;meets expectations&amp;#8221; &amp;#8212; notably getting stuck at an opened gift between frames 5-35 &amp;#8212; but the values and policy mostly make sense.  For example, we tend to see spikes in value when the agent is immediately in front of an unopened gift while the policy simultaneously outputs a much higher probability of taking the appropriate &lt;code&gt;toggle&lt;/code&gt; action in front of the unopened gift.  (We&amp;#8217;d achieve better performance by incorporating some memory into the model in the form of an &lt;a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/"&gt;&lt;span class="caps"&gt;LSTM&lt;/span&gt;&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;We’re sharing a little helper code to generate the matplotlib plots of the value and policy functions that are shown in the&amp;nbsp;video.&lt;/p&gt;
&lt;script src="https://gist.github.com/frangipane/4adca6481bf55f2260ff215c5686851b.js"&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Comments&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Training of the model is not included.  You&amp;#8217;ll need to load a trained actor critic model, along with access to its policy and value functions for plotting.  Here, the trained model has been loaded into &lt;code&gt;agent&lt;/code&gt; with a &lt;code&gt;get_action&lt;/code&gt; method that returns the &lt;code&gt;action&lt;/code&gt; to take, along with a numpy array of &lt;code&gt;policy&lt;/code&gt; probabilities and a scalar &lt;code&gt;value&lt;/code&gt; for the observation at the current time&amp;nbsp;step.&lt;/li&gt;
&lt;li&gt;The minigridworld environment conforms to the OpenAI gym &lt;span class="caps"&gt;API&lt;/span&gt;, and the &lt;code&gt;for&lt;/code&gt; loop is a standard implementation for interacting with the&amp;nbsp;environment.&lt;/li&gt;
&lt;li&gt;The gridworld environment already has a built in method for rendering the environment in iteractive mode &lt;code&gt;env.render('human')&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Matplotlib&amp;#8217;s &lt;code&gt;autoscale_view&lt;/code&gt; and &lt;code&gt;relim&lt;/code&gt; functions are used to make updates to the figures at each step.  In particular, this allows us to show what appears to be a sliding window over time of the value function line plot.  When running the script, the plots pop up as three separate&amp;nbsp;figures.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;p&gt;[1] Berkeley Deep &lt;span class="caps"&gt;RL&lt;/span&gt; bootcamp - Core Lecture 6 Nuts and Bolts of Deep &lt;span class="caps"&gt;RL&lt;/span&gt; Experimentation &amp;#8212; John Schulman (&lt;a href="https://youtu.be/8EcdaCk9KaQ"&gt;video&lt;/a&gt; | &lt;a href="https://drive.google.com/open?id=0BxXI_RttTZAhc2ZsblNvUHhGZDA"&gt;slides&lt;/a&gt;) - great advice on the debugging process, things to&amp;nbsp;plot&lt;/p&gt;
&lt;p&gt;[2] OpenAI Spinning Up: &lt;a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html"&gt;Intro to policy&amp;nbsp;optimization&lt;/a&gt;&lt;/p&gt;</content><category term="Programming"></category><category term="programming"></category><category term="tools"></category><category term="python"></category><category term="reinforcement learning"></category></entry><entry><title>2-D random walks are special</title><link href="https://efavdb.com/random-walk-scaling" rel="alternate"></link><published>2020-04-10T00:00:00-07:00</published><updated>2020-04-10T00:00:00-07:00</updated><author><name>Dustin McIntosh</name></author><id>tag:efavdb.com,2020-04-10:/random-walk-scaling</id><summary type="html">&lt;p&gt;Here, we examine the statistics behind discrete random walks on square lattices in &lt;span class="math"&gt;\(M\)&lt;/span&gt; dimensions, with focus on two metrics (see figure below for an example in 2-D): 1. &lt;span class="math"&gt;\(R\)&lt;/span&gt;, the final distance traveled from origin (measured by the Euclidean norm) and 2. &lt;span class="math"&gt;\(N_{unique}\)&lt;/span&gt;, the number of unique locations …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Here, we examine the statistics behind discrete random walks on square lattices in &lt;span class="math"&gt;\(M\)&lt;/span&gt; dimensions, with focus on two metrics (see figure below for an example in 2-D): 1. &lt;span class="math"&gt;\(R\)&lt;/span&gt;, the final distance traveled from origin (measured by the Euclidean norm) and 2. &lt;span class="math"&gt;\(N_{unique}\)&lt;/span&gt;, the number of unique locations visited on the&amp;nbsp;lattice.&lt;/p&gt;
&lt;p align="center"&gt;
     &lt;img src="images/example_rw.png"&gt;
&lt;/p&gt;

&lt;p&gt;We envision a single random walker on an &lt;span class="math"&gt;\(M\)&lt;/span&gt;-D lattice and allow it to wander randomly throughout the lattice, taking &lt;span class="math"&gt;\(N\)&lt;/span&gt; steps. We’ll examine how the distributions of &lt;span class="math"&gt;\(R\)&lt;/span&gt; and &lt;span class="math"&gt;\(N_{unique}\)&lt;/span&gt; vary with &lt;span class="math"&gt;\(M\)&lt;/span&gt; and &lt;span class="math"&gt;\(N\)&lt;/span&gt;; we&amp;#8217;ll show that their averages, &lt;span class="math"&gt;\(\langle R \rangle\)&lt;/span&gt; and &lt;span class="math"&gt;\(\langle N_{unique} \rangle\)&lt;/span&gt;, and their standard deviations, &lt;span class="math"&gt;\(\sigma_R\)&lt;/span&gt; and &lt;span class="math"&gt;\(\sigma_{N_{unique}}\)&lt;/span&gt;, scale as power laws with &lt;span class="math"&gt;\(N\)&lt;/span&gt;. The dependence of the exponents and scaling factors on &lt;span class="math"&gt;\(M\)&lt;/span&gt; is interesting and can be only partially reconciled with&amp;nbsp;theory.&lt;/p&gt;
&lt;p&gt;A simple simulation of random walks is easy to write in python for arbitrary dimensions (see &lt;a
href="https://colab.research.google.com/drive/13GYlaTvO-Wu_3ep_Pa0mRZo-CYelDFmf"&gt;this colab notebook&lt;/a&gt;, &lt;a href="https://github.com/dustinmcintosh/random-walks"&gt;github&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Here’s a look at the distribution of our two metrics for &lt;span class="math"&gt;\(N = 1000\)&lt;/span&gt; for a few different&amp;nbsp;dimensionalities:&lt;/p&gt;
&lt;p align="center"&gt;
     &lt;img src="images/unique_locations_visited_1000.png"&gt;
&lt;/p&gt;

&lt;p&gt;Let’s make some high-level sense of these&amp;nbsp;results:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\langle R \rangle\)&lt;/span&gt; depends only weakly on &lt;span class="math"&gt;\(M\)&lt;/span&gt; while &lt;span class="math"&gt;\(\langle N_{unique} \rangle\)&lt;/span&gt; clearly increases with &lt;span class="math"&gt;\(M\)&lt;/span&gt;. Both of these results make sense: In 1-D, the walker always has an equal chance to step further away from the origin or closer to it.  It also always has at least a 50% chance of backtracking to a position it has already visited. As you add dimensions, it becomes less likely to step immediately closer or further from the origin and more likely to wander in an orthogonal direction, increasing the distance from the origin by roughly the same amount independent of &lt;em&gt;which&lt;/em&gt; orthogonal direction, while also visiting completely new parts of the&amp;nbsp;lattice.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In 1-D, if you take an even number of steps, &lt;span class="math"&gt;\(R\)&lt;/span&gt; is always an even integer, so the distribution of &lt;span class="math"&gt;\(R\)&lt;/span&gt; appears “stripe-y” above. &lt;span class="math"&gt;\(R(M=1)\)&lt;/span&gt; can be understood as something like a &lt;a href="https://en.wikipedia.org/wiki/Folded_normal_distribution"&gt;folded normal distribution&lt;/a&gt; for large &lt;span class="math"&gt;\(N\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;As &lt;span class="math"&gt;\(M\)&lt;/span&gt; gets larger, the distribution of &lt;span class="math"&gt;\(R\)&lt;/span&gt; tightens up. We are essentially taking &lt;span class="math"&gt;\(M\)&lt;/span&gt; 1-D random walks, each with &lt;span class="math"&gt;\(\sim N/M\)&lt;/span&gt; steps and adding the results in quadrature. The result of this is that our walker is less likely to be very close to the origin and simultaneously less likely to wander too far afield as it is more likely to have traveled a little bit in many orthogonal&amp;nbsp;dimensions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The width of the &lt;span class="math"&gt;\(N_{unique}\)&lt;/span&gt; distribution, &lt;span class="math"&gt;\(\sigma_{N_{unique}}\)&lt;/span&gt;, depends in an interesting way on &lt;span class="math"&gt;\(M\)&lt;/span&gt;, increasing from 1-D to 2-D and thereafter decreasing. This seems because the distribution is centered most distant from the extremes of 0 or &lt;span class="math"&gt;\(N\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s take a look at the means of our two metrics as a function of &lt;span class="math"&gt;\(N\)&lt;/span&gt; for various &lt;span class="math"&gt;\(M\)&lt;/span&gt; (apologies for using a gif here, but it helps with 1. keeping this post concise and 2. comparing the plots as &lt;span class="math"&gt;\(M\)&lt;/span&gt; changes incrementally.  If you prefer static images, I’ve included them at &lt;a href="https://github.com/dustinmcintosh/random-walks"&gt;github&lt;/a&gt;):&lt;/p&gt;
&lt;p align="center"&gt;
     &lt;img src="images/R_and_Nu_vs_n.gif"&gt;
&lt;/p&gt;

&lt;p&gt;Note, this is a &lt;a href="https://en.wikipedia.org/wiki/Log%E2%80%93log_plot"&gt;log-log plot&lt;/a&gt;; power laws show up as straight lines.  One of the first things you’ll notice is that the variation in dependence on &lt;span class="math"&gt;\(N\)&lt;/span&gt; across different &lt;span class="math"&gt;\(M\)&lt;/span&gt; is fairly banal for &lt;span class="math"&gt;\(R\)&lt;/span&gt;, but much more interesting for &lt;span class="math"&gt;\(N_{unique}\)&lt;/span&gt;.  In fact, it is a well-known result (see, e.g., &lt;a href="https://math.stackexchange.com/questions/103142/expected-value-of-random-walk"&gt;here&lt;/a&gt;) that &lt;span class="math"&gt;\(\langle R \rangle \sim N^\alpha\)&lt;/span&gt; with &lt;span class="math"&gt;\(\alpha = 0.5\)&lt;/span&gt; for all &lt;span class="math"&gt;\(M\)&lt;/span&gt;.  &lt;span class="math"&gt;\(\langle N_{unique} \rangle\)&lt;/span&gt;, on the other hand, seems similar to &lt;span class="math"&gt;\(\langle R \rangle\)&lt;/span&gt; in 1-D, but the dependence on &lt;span class="math"&gt;\(N\)&lt;/span&gt; increases dramatically thereafter and approaches &lt;span class="math"&gt;\(\langle N_{unique} \rangle \approx N^\beta\)&lt;/span&gt; with &lt;span class="math"&gt;\(\beta=1\)&lt;/span&gt; at higher dimensions.  If you look closely, you’ll note that 2-D is particularly special (more on that&amp;nbsp;shortly).&lt;/p&gt;
&lt;p&gt;We can also look at the widths (standard deviations) of the two distributions (again, you’ll note that 2-D is particularly&amp;nbsp;interesting):&lt;/p&gt;
&lt;p align="center"&gt;
     &lt;img src="images/std_vs_N.gif"&gt;
&lt;/p&gt;

&lt;p&gt;First, note that all 4 of the quantities describing the distributions of &lt;span class="math"&gt;\(R\)&lt;/span&gt; and &lt;span class="math"&gt;\(N_{unique}\)&lt;/span&gt; plotted above are well-described by power laws. Thus, we can extract eight parameters to describe them (four exponents and four scaling factors) as a function of &lt;span class="math"&gt;\(M\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
\langle R \rangle &amp;amp;\approx&amp;amp; R_0 * N^\alpha \\
\langle N_{unique} \rangle &amp;amp;\approx&amp;amp; N_0 * N^\beta \\
\sigma_R &amp;amp;\approx&amp;amp; \sigma_{R,0} * N^\gamma \\
\sigma_{N_{unique}} &amp;amp;\approx&amp;amp; \sigma_{N_{unique},0} * N^\delta \\
\end{eqnarray}&lt;/div&gt;
&lt;p align="center"&gt;
     &lt;img src="images/exponents_scaling_vs_M.png"&gt;
&lt;/p&gt;

&lt;p&gt;I’ve plotted all the theoretical results that I could find for these parameters as lines&amp;nbsp;above:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;As mentioned, &lt;span class="math"&gt;\(\alpha = 0.5\)&lt;/span&gt; for all &lt;span class="math"&gt;\(M\)&lt;/span&gt;. (blue line top&amp;nbsp;plot)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="math"&gt;\(R_0\)&lt;/span&gt; gives &lt;span class="math"&gt;\(\langle R \rangle\)&lt;/span&gt; its weak dependence on &lt;span class="math"&gt;\(M\)&lt;/span&gt;, varying according to an elegant ratio of Gamma functions as described &lt;a href="https://math.stackexchange.com/questions/103142/expected-value-of-random-walk"&gt;here&lt;/a&gt;.  (blue dash-dot line bottom&amp;nbsp;plot)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It has been shown that &lt;span class="math"&gt;\(\beta = 0.5\)&lt;/span&gt; for &lt;span class="math"&gt;\(M=1\)&lt;/span&gt; and &lt;span class="math"&gt;\(\beta = 1\)&lt;/span&gt; for all &lt;span class="math"&gt;\(M \geq 3\)&lt;/span&gt;.  (orange dash and line in the top plot)  I found these theoretical results in this &lt;a href="https://www.osti.gov/servlets/purl/4637387"&gt;wonderful paper&lt;/a&gt;, which you should really glance at, if not read, just to see an interesting piece of history - it’s by George H. Vineyard in 1963 (no so long ago for such a fundamental math problem!) at Brookhaven National Lab for the &lt;span class="caps"&gt;US&lt;/span&gt; Atomic Energy Commission. Written on a typewriter, here is a sample equation, complete with hand-written scribbles to indicate the vectors and a clearly corrected typo on the first&amp;nbsp;cosine:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p align="center"&gt;
     &lt;img src="images/Vineyard_pic.png"&gt;
&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;In the same paper, Vineyard derives that &lt;span class="math"&gt;\(N_{unique, 0} = \sqrt{8/\pi}\)&lt;/span&gt; in 1-D and &lt;span class="math"&gt;\(N_{unique, 0} \approx 0.659462670\)&lt;/span&gt; (yes with all those sig. figs.) in 3-D as derived from evaluating Watson’s Integrals (which, I take it, is what the integrals in the image above are called). (orange dashes in bottom&amp;nbsp;plot)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The most interesting thing about all this, to me, is that there is no known theoretical result for &lt;span class="math"&gt;\(\beta\)&lt;/span&gt; in 2-D. From our data, we get &lt;span class="math"&gt;\(\beta = 0.87 \pm 0.02\)&lt;/span&gt;.  An interesting phenomenological argument for &lt;span class="math"&gt;\(\beta\)&lt;/span&gt;’s dependence on &lt;span class="math"&gt;\(M\)&lt;/span&gt;: The random walker spends most of it’s time roaming around within a distance of &lt;span class="math"&gt;\(\langle R \rangle \sim N^{\alpha}\)&lt;/span&gt; from the origin (where &lt;span class="math"&gt;\(\alpha=1/2\)&lt;/span&gt;, independent of &lt;span class="math"&gt;\(M\)&lt;/span&gt;), as that is where it is going to end up.  In 1-D, there are only &lt;span class="math"&gt;\(\sim 2 N^{1/2}\)&lt;/span&gt; sites within this distance, so the number of unique sites visited scales with &lt;span class="math"&gt;\(N^{1/2}\)&lt;/span&gt;. In 3-D, there are &lt;span class="math"&gt;\(\sim \frac{4}{3} \pi N^{3/2}\)&lt;/span&gt; sites in the sphere of radius &lt;span class="math"&gt;\(N^{1/2}\)&lt;/span&gt;, which is &lt;span class="math"&gt;\(\gg N\)&lt;/span&gt;, so the walker explores &lt;span class="math"&gt;\(\sim N\)&lt;/span&gt; sites (as it can visit no more than it walks), similar for larger &lt;span class="math"&gt;\(M\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;This is what makes 2-D random walks special&lt;/strong&gt;: the number of sites within &lt;span class="math"&gt;\(N^{1/2}\)&lt;/span&gt; distance of the origin is &lt;span class="math"&gt;\(\sim N\)&lt;/span&gt;, scaling as the number of steps taken - so, the scaling arguments above no longer apply, leaving us somewhere between &lt;span class="math"&gt;\(1/2 &amp;lt; \beta &amp;lt; 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A few final notes on the exponents and scaling factors that I don’t understand in terms of&amp;nbsp;theory:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We don’t get exactly &lt;span class="math"&gt;\(\beta=1\)&lt;/span&gt; from the data in 3-D above, but that may be because some of our &lt;span class="math"&gt;\(N\)&lt;/span&gt; are not quite large&amp;nbsp;enough.&lt;/li&gt;
&lt;li&gt;I have not seen any theory for &lt;span class="math"&gt;\(\sigma_{R,0}\)&lt;/span&gt;, &lt;span class="math"&gt;\(\sigma_{N_{unique},0}\)&lt;/span&gt;, &lt;span class="math"&gt;\(\gamma\)&lt;/span&gt;, or &lt;span class="math"&gt;\(\delta\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\gamma\)&lt;/span&gt; appears to also be independent of &lt;span class="math"&gt;\(M\)&lt;/span&gt; at 0.5. Surely this isn&amp;#8217;t hard to&amp;nbsp;derive?&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\delta\)&lt;/span&gt; increases dramatically from 1-D to 2-D (again, 2-D appears the most interesting case) and slowly settles back down to 0.5 at large &lt;span class="math"&gt;\(M\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\sigma_{N_{unique},0}\)&lt;/span&gt; has a strange effect for &lt;span class="math"&gt;\(M=2\)&lt;/span&gt; also, dipping before going back up and then finally decaying towards zero for &lt;span class="math"&gt;\(M&amp;gt;4\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;For large &lt;span class="math"&gt;\(M\)&lt;/span&gt;, the data indicate that &lt;span class="math"&gt;\(\sigma_{R,0} \approx \sigma_{N_{unique},0}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\gamma \approx \delta \approx 0.5\)&lt;/span&gt;, indicating that &lt;span class="math"&gt;\(\sigma_{R} \approx \sigma_{N_{unique}}\)&lt;/span&gt;.  Further, not shown here, but in &lt;a href="https://github.com/dustinmcintosh/random-walks/blob/master/figures/scaling_of_std_dev_vs_m.png"&gt;github&lt;/a&gt;,  &lt;span class="math"&gt;\(\sigma_{R,0} \approx \sigma_{N_{unique},0} \sim \sqrt{1/M}\)&lt;/span&gt;. Or, in total, &lt;span class="math"&gt;\(\sigma_{R} \approx \sigma_{N_{unique}} \sim \sqrt{N/M}\)&lt;/span&gt; for large &lt;span class="math"&gt;\(M\)&lt;/span&gt; and &lt;span class="math"&gt;\(N\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I really enjoyed working on this post from end-to-end: writing the random walker class, finding a great python jackknife function from astropy (see the &lt;a href="https://colab.research.google.com/drive/13GYlaTvO-Wu_3ep_Pa0mRZo-CYelDFmf"&gt;colab notebook&lt;/a&gt;), hacking my way through matplotlib, discovering the amazing Vineyard paper, and just generally exploring this incredibly rich problem that is so easy to state yet difficult to theoretically solve.  I’ll also note that the 2-D results are of particular interest to me given their applicability to the &lt;a href="https://www.efavdb.com/world-wandering-dudes"&gt;World Wandering Dudes&lt;/a&gt; project that I have been working&amp;nbsp;on.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Statistics"></category><category term="statistics"></category><category term="python"></category></entry><entry><title>Q-learning and DQN</title><link href="https://efavdb.com/dqn" rel="alternate"></link><published>2020-04-06T00:00:00-07:00</published><updated>2020-04-06T00:00:00-07:00</updated><author><name>Cathy Yeh</name></author><id>tag:efavdb.com,2020-04-06:/dqn</id><summary type="html">&lt;p&gt;[&lt;span class="caps"&gt;TOC&lt;/span&gt;]&lt;/p&gt;
&lt;p&gt;Q-learning is a reinforcement learning (&lt;span class="caps"&gt;RL&lt;/span&gt;) algorithm that is the basis for deep Q networks (&lt;span class="caps"&gt;DQN&lt;/span&gt;), the algorithm by Google DeepMind that achieved human-level performance for a range of Atari games and kicked off the deep &lt;span class="caps"&gt;RL&lt;/span&gt; revolution starting in&amp;nbsp;2013-2015.&lt;/p&gt;
&lt;p&gt;We begin with some historical context, then provide …&lt;/p&gt;</summary><content type="html">&lt;p&gt;[&lt;span class="caps"&gt;TOC&lt;/span&gt;]&lt;/p&gt;
&lt;p&gt;Q-learning is a reinforcement learning (&lt;span class="caps"&gt;RL&lt;/span&gt;) algorithm that is the basis for deep Q networks (&lt;span class="caps"&gt;DQN&lt;/span&gt;), the algorithm by Google DeepMind that achieved human-level performance for a range of Atari games and kicked off the deep &lt;span class="caps"&gt;RL&lt;/span&gt; revolution starting in&amp;nbsp;2013-2015.&lt;/p&gt;
&lt;p&gt;We begin with some historical context, then provide an overview of value function methods / Q-learning, and conclude with a discussion of &lt;span class="caps"&gt;DQN&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If you want to skip straight to code, the implementation of &lt;span class="caps"&gt;DQN&lt;/span&gt; that we used to train the agent playing Atari Breakout below is available &lt;a href="https://github.com/frangipane/reinforcement-learning/tree/master/DQN"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="images/atari_breakout.gif" alt="Atari Breakout" style="width:250px;"&gt;
&lt;/p&gt;

&lt;p&gt;If you watch the video long enough, you&amp;#8217;ll see the agent has learned a strategy that favors breaking bricks at the edges so the ball &amp;#8220;breaks out&amp;#8221; to the upper side, resulting in a cascade of&amp;nbsp;points.&lt;/p&gt;
&lt;h2&gt;Historical&amp;nbsp;context&lt;/h2&gt;
&lt;p&gt;The theories that underpin today’s reinforcement learning algorithms were developed decades ago.  For example, Watkins developed Q-learning, a value function method, in &lt;a href="http://www.cs.rhul.ac.uk/~chrisw/thesis.html"&gt;1989&lt;/a&gt;, and Williams proposed the &lt;span class="caps"&gt;REINFORCE&lt;/span&gt; policy gradient method in &lt;a href="https://link.springer.com/content/pdf/10.1007%2FBF00992696.pdf"&gt;1987&lt;/a&gt;. So why the recent surge of interest in deep &lt;span class="caps"&gt;RL&lt;/span&gt;?&lt;/p&gt;
&lt;h3&gt;Representational power from Neural&amp;nbsp;Networks&lt;/h3&gt;
&lt;p&gt;Until 2013, most applications of &lt;span class="caps"&gt;RL&lt;/span&gt; relied on hand engineered inputs for value function and policy representations, which drastically limited the scope of applicability to the real world.  Mnih et. al [1] made use of advances in computational power and neural network (&lt;span class="caps"&gt;NN&lt;/span&gt;) architectures to use a deep &lt;span class="caps"&gt;NN&lt;/span&gt; for &lt;em&gt;value function approximation&lt;/em&gt;, showing that NNs can learn a useful representation from raw pixel inputs in Atari&amp;nbsp;games.&lt;/p&gt;
&lt;h3&gt;Variations on a theme: vanilla &lt;span class="caps"&gt;RL&lt;/span&gt; algorithms don’t work well&amp;nbsp;out-of-the-box&lt;/h3&gt;
&lt;p&gt;The basic &lt;span class="caps"&gt;RL&lt;/span&gt; algorithms that were developed decades ago do not work well in practice without modifications.  For example, &lt;span class="caps"&gt;REINFORCE&lt;/span&gt; relies on Monte Carlo estimates of the performance gradient; such estimates of the performance gradient are high variance, resulting in unstable or impractically slow learning (poor sample efficiency).  The original Q-learning algorithm also suffers from instability due to correlated sequential training data and parameter updates affecting both the estimator and target, creating a “moving target” and hence&amp;nbsp;divergences.&lt;/p&gt;
&lt;p&gt;We can think of these original &lt;span class="caps"&gt;RL&lt;/span&gt; algorithms as the Wright Brothers plane.
&lt;p align="center"&gt;
&lt;img src="images/wright_brothers_plane.png" alt="Wright brothers plane" style="width:500px;"&gt;
&lt;/p&gt;&lt;/p&gt;
&lt;p&gt;The foundational shape is there and recognizable in newer models.  However, the enhancements of newer algorithms aren&amp;#8217;t just bells and whistles &amp;#8212; they have enabled the move from toy problems into more functional&amp;nbsp;territory.&lt;/p&gt;
&lt;h2&gt;Q-learning&lt;/h2&gt;
&lt;h3&gt;Background&lt;/h3&gt;
&lt;p&gt;&lt;span class="caps"&gt;RL&lt;/span&gt; models the sequential decision-making problem as a Markov Decision Process (&lt;span class="caps"&gt;MDP&lt;/span&gt;): transitions from state to state involve both environment dynamics and an agent whose actions affect both the probability of transitioning to the next state and the reward&amp;nbsp;received.&lt;/p&gt;
&lt;p&gt;The goal is to find a policy, a mapping from state to actions, that will maximize the agent’s expected returns, i.e. their cumulative future&amp;nbsp;rewards.&lt;/p&gt;
&lt;p&gt;Q-learning is an algorithm for learning the eponymous &lt;span class="math"&gt;\(Q(s,a)\)&lt;/span&gt; action-value function, defined as the expected returns for each state-action &lt;span class="math"&gt;\((s,a)\)&lt;/span&gt; pair, corresponding to following the optimal&amp;nbsp;policy.&lt;/p&gt;
&lt;h3&gt;Goal: solve the Bellman optimality&amp;nbsp;equation&lt;/h3&gt;
&lt;p&gt;Recall that &lt;span class="math"&gt;\(q_*\)&lt;/span&gt; is described by a self-consistent, recursive relation, the Bellman optimality equation, that falls out from the Markov property [6, 7] of&amp;nbsp;MDPs&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{action-value-bellman-optimality} \tag{1}
q_*(s, a) &amp;amp;=&amp;amp; \mathbb{E}_{\pi*} [R_{t+1} + \gamma \max_{a'} q_*(S_{t+1}', a') | S_t = s, A_t = a] \\
          &amp;amp;=&amp;amp; \sum_{s', r} p(s', r | s, a) [r + \gamma \max_{a'} q_*(s', a') ]
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(0 \leq \gamma \leq 1\)&lt;/span&gt; is the &lt;em&gt;discount rate&lt;/em&gt; which characterizes how much we weight rewards now vs. later, &lt;span class="math"&gt;\(R_{t+1}\)&lt;/span&gt; is the reward at timestep &lt;span class="math"&gt;\(t+1\)&lt;/span&gt;, and &lt;span class="math"&gt;\(p(s', r | s, a)\)&lt;/span&gt; is the environment transition&amp;nbsp;dynamics.&lt;/p&gt;
&lt;p&gt;Our &lt;a href="https://efavdb.com/intro-rl-toy-example.html"&gt;introduction to &lt;span class="caps"&gt;RL&lt;/span&gt;&lt;/a&gt; provides more background on the Bellman equations in case (\ref{action-value-bellman-optimality}) looks&amp;nbsp;unfamiliar.&lt;/p&gt;
&lt;h3&gt;The Q-learning approach to solving the Bellman&amp;nbsp;equation&lt;/h3&gt;
&lt;p&gt;We use capitalized &lt;span class="math"&gt;\(Q\)&lt;/span&gt; to denote an estimate and lowercase &lt;span class="math"&gt;\(q\)&lt;/span&gt; to denote the real action-value function.  The Q-learning algorithm makes the following&amp;nbsp;update:&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{q-learning} \tag{2}
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t)]
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;The quantity in square brackets in (\ref{q-learning}) is exactly 0 for the optimal action-value, &lt;span class="math"&gt;\(q*\)&lt;/span&gt;, based on (\ref{action-value-bellman-optimality}).  We can think of it as an error term, “the Bellman error”, that describes how far off the target quantity &lt;span class="math"&gt;\(R_{t+1} + \gamma \max_a Q(S_{t+1}, a)\)&lt;/span&gt; is from our current estimate &lt;span class="math"&gt;\(Q(S_t, A_t)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The goal with Q-learning is to iteratively calculate (\ref{q-learning}), updating our estimate of &lt;span class="math"&gt;\(Q\)&lt;/span&gt; to reduce the Bellman error, until we have converged on a&amp;nbsp;solution.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q-learning makes two&amp;nbsp;approximations:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I. It replaces the expectation value in (\ref{action-value-bellman-optimality}) with sampled estimates, similar to Monte Carlo estimates.  Unlike the dynamic programming approach we described in an earlier &lt;a href="https://efavdb.com/dp-in-rl.html"&gt;post&lt;/a&gt;, sampling is necessary since we don&amp;#8217;t have access to the model of the environment, i.e. the environment transition&amp;nbsp;dynamics.&lt;/p&gt;
&lt;p&gt;&lt;span class="caps"&gt;II&lt;/span&gt;. It replaces the target &lt;span class="math"&gt;\(R_{t+1} + \max_a \gamma q_*(s’,a’)\)&lt;/span&gt; in (\ref{action-value-bellman-optimality}), which contains the true action-value function &lt;span class="math"&gt;\(q_*\)&lt;/span&gt;, with the one-step temporal difference, &lt;span class="caps"&gt;TD&lt;/span&gt;(0), target &lt;span class="math"&gt;\(R_{t+1} + \gamma \max_a Q(S_{t+1}, a)\)&lt;/span&gt;.  The &lt;span class="caps"&gt;TD&lt;/span&gt;(0) target is an example of &lt;em&gt;bootstrapping&lt;/em&gt; because it makes use of the current estimate of the action-value function, instead of, say the cumulative rewards from an entire episode, which would be a Monte Carlo target.  Temporal difference methods reduce variance that comes from sampling a single trajectory like Monte Carlo at the cost of introducing bias from using an approximate function in the target for&amp;nbsp;updates.&lt;/p&gt;
&lt;p&gt;Figure 8.11 of [7] nicely summarizes the types of approximations and their limits in the following&amp;nbsp;diagram:&lt;/p&gt;
&lt;p&gt;&lt;img alt="backup approximations" src="https://efavdb.com/images/backup_limits_diagram_sutton_barto.png"&gt;&lt;/p&gt;
&lt;h2&gt;Deep Q-Networks (&lt;span class="caps"&gt;DQN&lt;/span&gt;)&lt;/h2&gt;
&lt;h3&gt;Key contributions to&amp;nbsp;Q-learning&lt;/h3&gt;
&lt;p&gt;The &lt;span class="caps"&gt;DQN&lt;/span&gt; authors made two key enhancements to the original Q-learning algorithm to actually make it&amp;nbsp;work:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Experience replay buffer&lt;/strong&gt;: to reduce the instability caused by training on highly correlated sequential data, store samples (transition tuples &lt;span class="math"&gt;\((s, a, s’, r)\)&lt;/span&gt;) in an “experience replay buffer”.  Cut down correlations by randomly sampling the buffer for minibatches of training data.  The idea of experience replay was introduced by &lt;a href="http://www.incompleteideas.net/lin-92.pdf"&gt;Lin in 1992&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Freeze the target network&lt;/strong&gt;: to address the instability caused by chasing a moving target, freeze the target network and only update it periodically with the latest parameters from the trained&amp;nbsp;estimator.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These modifications enabled [1] to successfully train a deep Q-network, an action-value function approximated by a convolutional neural net, on the high dimensional visual inputs of a variety of Atari&amp;nbsp;games.&lt;/p&gt;
&lt;p&gt;The authors also employed a number of tweaks / data preprocessing on top of the aforementioned key enhancements.  One preprocessing trick of note was the concatenation of the four most recent frames as input into the Q-network in order to provide some sense of velocity or trajectory, e.g. the trajectory of a ball in games such as Pong or Breakout.  This preprocessing decision helps uphold the assumption that the problem is a Markov Decision Process, which underlies the Bellman optimality equations and Q-learning algorithms; otherwise, the assumption is violated if the agent only observes some fraction of the state of the environment, turning the problem into a &lt;a href="https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process"&gt;partially observable &lt;span class="caps"&gt;MDP&lt;/span&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;span class="caps"&gt;DQN&lt;/span&gt; implementation in&amp;nbsp;code&lt;/h3&gt;
&lt;p&gt;We’ve implemented &lt;span class="caps"&gt;DQN&lt;/span&gt; &lt;a href="https://github.com/frangipane/reinforcement-learning/blob/master/DQN/dqn.py"&gt;here&lt;/a&gt;, tested for (1) the &lt;a href="https://gym.openai.com/envs/CartPole-v1/"&gt;Cartpole&lt;/a&gt; toy problem, which uses a multilayer perceptron &lt;code&gt;MLPCritic&lt;/code&gt; as the Q-function approximator for non-visual input data, and (2) Atari Breakout, which uses a convolutional neural network &lt;code&gt;CNNCritic&lt;/code&gt; as the Q-function approximator for the (visual) Atari pixel&amp;nbsp;data.&lt;/p&gt;
&lt;p&gt;The Cartpole problem is trainable on the average modern laptop &lt;span class="caps"&gt;CPU&lt;/span&gt;, but we recommend using a beefier setup with GPUs and lots of memory to do Q-learning on Atari.  Thanks to the OpenAI Scholars program and Microsoft, we were able to train &lt;span class="caps"&gt;DQN&lt;/span&gt; on Breakout using an Azure &lt;a href="https://docs.microsoft.com/en-us/azure/virtual-machines/nc-series"&gt;Standard_NC24&lt;/a&gt; consisting of 224 GiB &lt;span class="caps"&gt;RAM&lt;/span&gt; and 2 K80&amp;nbsp;GPUs.&lt;/p&gt;
&lt;p&gt;The values from the &lt;span class="math"&gt;\(Q\)&lt;/span&gt; estimator and frozen target network are fed into the Huber loss that is used to update the parameters of the Q-function in this code&amp;nbsp;snippet:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;compute_loss_q&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;o2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;obs&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;act&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;rew&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;obs2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;done&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

        &lt;span class="c1"&gt;# Pick out q-values associated with / indexed by the action that was taken&lt;/span&gt;
        &lt;span class="c1"&gt;# for that observation&lt;/span&gt;
        &lt;span class="n"&gt;q&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gather&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ac&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;long&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

        &lt;span class="c1"&gt;# Bellman backup for Q function&lt;/span&gt;
        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;no_grad&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
                &lt;span class="c1"&gt;# Targets come from frozen target Q-network&lt;/span&gt;
                &lt;span class="n"&gt;q_target&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;target_q_network&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;o2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;
                &lt;span class="n"&gt;backup&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;gamma&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;q_target&lt;/span&gt;

        &lt;span class="n"&gt;loss_q&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;smooth_l1_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;backup&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;loss_q&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The experience replay buffer was taken from OpenAI’s Spinning Up in &lt;span class="caps"&gt;RL&lt;/span&gt; [6] code tutorials for the&amp;nbsp;problem:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;ReplayBuffer&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;    A simple FIFO experience replay buffer for DDPG agents.&lt;/span&gt;

&lt;span class="sd"&gt;    Copied from: https://github.com/openai/spinningup/blob/master/spinup/algos/pytorch/ddpg/ddpg.py#L11,&lt;/span&gt;
&lt;span class="sd"&gt;    modified action buffer for discrete action space.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;obs_dim&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;act_dim&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="o"&gt;...&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;store&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;obs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;act&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rew&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;next_obs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;obs_buf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ptr&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;obs&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;obs2_buf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ptr&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;next_obs&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;act_buf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ptr&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;act&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rew_buf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ptr&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;rew&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;done_buf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ptr&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ptr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ptr&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_size&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sample_batch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;idxs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;choice&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;batch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;obs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;obs_buf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;idxs&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                     &lt;span class="n"&gt;obs2&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;obs2_buf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;idxs&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                     &lt;span class="n"&gt;act&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;act_buf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;idxs&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                     &lt;span class="n"&gt;rew&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rew_buf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;idxs&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                     &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;done_buf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;idxs&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;act&amp;#39;&lt;/span&gt;
                &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
                &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;()}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Finally, we used OpenAI’s baselines &lt;a href="https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py"&gt;Atari wrappers&lt;/a&gt; to handle the rather involved data preprocessing&amp;nbsp;steps.&lt;/p&gt;
&lt;p&gt;You can see logs and plots like this plot of the mean raw returns per step in the environment for the Atari &lt;span class="caps"&gt;DQN&lt;/span&gt; training run in our &lt;a href="https://app.wandb.ai/frangipane/dqn/runs/30fhfv6y?workspace=user-frangipane"&gt;wandb dashboard&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="training curve" src="https://efavdb.com/images/atari_training_returns.png"&gt;&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;From a pedagogical point of view, Q-learning is a good study for someone getting off the ground with &lt;span class="caps"&gt;RL&lt;/span&gt; since it pulls together many core &lt;span class="caps"&gt;RL&lt;/span&gt; concepts,&amp;nbsp;namely:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Model the sequential decision making process as an &lt;strong&gt;&lt;span class="caps"&gt;MDP&lt;/span&gt;&lt;/strong&gt; where environment dynamics are&amp;nbsp;unknown.&lt;/li&gt;
&lt;li&gt;Frame the problem as finding &lt;strong&gt;action-value functions&lt;/strong&gt; that satisfy the Bellman&amp;nbsp;equations.&lt;/li&gt;
&lt;li&gt;Iteratively solve the Bellman equations using &lt;strong&gt;bootstrapped  estimates&lt;/strong&gt; from samples of an agent’s interactions with an&amp;nbsp;environment.&lt;/li&gt;
&lt;li&gt;Use neural networks to &lt;strong&gt;approximate value functions&lt;/strong&gt; to handle the more realistic situation of an observation space being too high-dimensional to be stored in&amp;nbsp;table.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class="caps"&gt;DQN&lt;/span&gt; on top of vanilla Q-learning itself is noteworthy because the modifications &amp;#8212; experience replay and frozen target networks &amp;#8212; are what make Q-learning actually work, demonstrating that the devil is in the&amp;nbsp;details.&lt;/p&gt;
&lt;p&gt;Furthermore, the &lt;span class="caps"&gt;DQN&lt;/span&gt; tricks have been incorporated in many other &lt;span class="caps"&gt;RL&lt;/span&gt; algorithms, e.g. see [6] for more examples.  The tricks aren’t necessarily “pretty”, but they come from understanding/intuition about shortcomings of the basic&amp;nbsp;algorithms.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Papers&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[1] Mnih et al 2015 - &lt;a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#the-optimal-q-function-and-the-optimal-action"&gt;Human-level control through deep reinforcement&amp;nbsp;learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Video&amp;nbsp;lectures&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[2] David Silver - &lt;span class="caps"&gt;RL&lt;/span&gt; lecture 6 Value Function Approximation (&lt;a href="https://www.youtube.com/watch?v=UoPei5o4fps"&gt;video&lt;/a&gt;, &lt;a href="https://www.davidsilver.uk/wp-content/uploads/2020/03/FA.pdf"&gt;slides&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;[3] Sergey Levine’s lecture (&lt;span class="caps"&gt;CS285&lt;/span&gt;) on value function methods (&lt;a href="https://www.youtube.com/watch?v=doR5bMe-Wic&amp;amp;list=PLkFD6_40KJIwhWJpGazJ9VSj9CFMkb79A&amp;amp;index=8&amp;amp;t=129s"&gt;video&lt;/a&gt;, &lt;a href="http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-7.pdf"&gt;slides&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;[4] Sergey Levine’s lecture (&lt;span class="caps"&gt;CS285&lt;/span&gt;) on deep &lt;span class="caps"&gt;RL&lt;/span&gt; with Q-functions (&lt;a href="https://www.youtube.com/watch?v=7Lwf-BoIu3M&amp;amp;list=PLkFD6_40KJIwhWJpGazJ9VSj9CFMkb79A&amp;amp;index=9&amp;amp;t=0s"&gt;video&lt;/a&gt;, &lt;a href="http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-8.pdf"&gt;slides&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;[5] Vlad Mnih - Berkeley Deep &lt;span class="caps"&gt;RL&lt;/span&gt; Bootcamp 2017 - Core Lecture 3 &lt;span class="caps"&gt;DQN&lt;/span&gt; + Variants (&lt;a href="https://www.youtube.com/watch?v=fevMOp5TDQs"&gt;video&lt;/a&gt;, &lt;a href="https://drive.google.com/open?id=0BxXI_RttTZAhVUhpbDhiSUFFNjg"&gt;slides&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Books /&amp;nbsp;tutorials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[6] OpenAI - Spinning Up: &lt;a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#the-optimal-q-function-and-the-optimal-action"&gt;The Optimal Q-Function and the Optimal&amp;nbsp;Action&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[7] Sutton and Barto - &lt;a href="http://incompleteideas.net/book/RLbook2018.pdf"&gt;Reinforcement Learning: An Introduction (2nd Edition)&lt;/a&gt;, section 6.5 “Q-learning: Off-policy &lt;span class="caps"&gt;TD&lt;/span&gt; Control”, section 16.5 “Human-level Video Game&amp;nbsp;Play”&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Machine learning"></category><category term="machine learning"></category><category term="reinforcement learning"></category><category term="OpenAI"></category></entry><entry><title>Sample pooling to reduce needed disease screening test counts</title><link href="https://efavdb.com/pooling" rel="alternate"></link><published>2020-03-29T00:00:00-07:00</published><updated>2020-03-29T00:00:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2020-03-29:/pooling</id><summary type="html">&lt;p&gt;Pooling of test samples can be used to reduce the mean number of test counts
required to determine who in a set of subjects carries a disease. E.g., if the
blood samples of a set of office workers are combined and tested, and the test
comes back negative, then …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Pooling of test samples can be used to reduce the mean number of test counts
required to determine who in a set of subjects carries a disease. E.g., if the
blood samples of a set of office workers are combined and tested, and the test
comes back negative, then the full office can be ruled out as disease carriers
using just a single test (whereas the naive approach would require testing each
separately).  However, if the test comes back positive, then a refined search
through the workers must be carried out to decide which have the disease and
which do&amp;nbsp;not.&lt;/p&gt;
&lt;p&gt;Here, we consider two methods for refined search when a group is flagged
positive, and provide python code that can be used to find the optimal pooling
strategy.  This depends on the frequency of disease within the testing
population, &lt;span class="math"&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Impact summary of pooling&amp;nbsp;concept: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If &lt;span class="math"&gt;\(p = O(1)\)&lt;/span&gt;, so that many people have the illness, pooling doesn&amp;#8217;t&amp;nbsp;help. &lt;/li&gt;
&lt;li&gt;If &lt;span class="math"&gt;\(p = 0.1\)&lt;/span&gt;, perhaps typical of people being screened with symptoms, we can
   reduce the test count needed by about &lt;span class="math"&gt;\(\sim 0.6\)&lt;/span&gt; using pooling, and the two refined
search methods we consider perform similarly&amp;nbsp;here.&lt;/li&gt;
&lt;li&gt;If &lt;span class="math"&gt;\(p = 0.001\)&lt;/span&gt;, so that positive cases are rare &amp;#8212; perhaps useful for
   screening an office of workers expected to be healthy, then we can cut the
mean test count by a factor of &lt;span class="math"&gt;\(50\)&lt;/span&gt;, and the bisection method for refined search performs best here (details&amp;nbsp;below).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Code for this analysis can be found at our github (&lt;a href="https://github.com/EFavDB/pooling/blob/master/pooling_samples.ipynb"&gt;link&lt;/a&gt;).&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;&lt;span class="caps"&gt;COVID19&lt;/span&gt; background, strategies considered&amp;nbsp;here&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;The idea of pooling is an old one, but I happened on the idea when an article
was posted about it to the statistics subreddit this past week (&lt;a
href="https://www.reddit.com/r/statistics/comments/fl3dlw/q_if_you_could_test_batches_of_64_samples_for/"&gt;link&lt;/a&gt;).
There the question was posed what the optimal pooling count would be,
motivating this&amp;nbsp;post.&lt;/p&gt;
&lt;p&gt;I imagine pooling may be useful for &lt;span class="caps"&gt;COVID19&lt;/span&gt; under two conditions:  (1)
situations where testing capactity is the limiting factor (as opposed to speed
of diagnosis, say), and (2) Situations where a great many people need to be
screened and it is unlikely that any of them have it &amp;#8212; e.g., daily tests
within a large office&amp;nbsp;buiding.&lt;/p&gt;
&lt;p&gt;We consider two pooling methods here:  (1) A simple method where if the test
on the group comes back positive, we immediately screen each individual.  (2) A
bisection method, where if a group comes back positive, we split it in two and
run the test on each subgroup, repeating from there recursively.  E.g., in a
group of size 16 with one positive, the recursive approach generates the following
set of test subsets (see notebook on our github linked above for&amp;nbsp;code)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;seq&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;generate_random_seq&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;test_counts_needed&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_count&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seq&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here, the 13th individual had the disease, and the bisection method required a
total of 9 tests (one for each row above) to determine the full set of diagnoses.  Note that
9 is less than 16, the number needed when we screen everyone from the&amp;nbsp;start.&lt;/p&gt;
&lt;p&gt;Our purpose is to provide code and equations that can be used to select from these two
methods should anyone want to apply this idea.  Caveat:  We currently ignore
any possibility of error in the tests.  This may make the approach invalid for
some or all of the current covid19 tests.  Error rates should be studied next
where&amp;nbsp;appropriate.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Model and&amp;nbsp;results&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;We posit that we have a pool&amp;nbsp;of
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
N = 2^{\mathbb{K}} \tag{1} \label{count_pop}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt; 
people to be tested.  In the first round, we pool all their samples and test the
group.  If the group comes back positive, we then run one of the refined methods to
figure out which people exactly have the illness.  Each person is supposed to have a probability &lt;span class="math"&gt;\(p\)&lt;/span&gt; of having the disease.
Below, we ask how to set &lt;span class="math"&gt;\(\mathbb{K}\)&lt;/span&gt; &amp;#8212; which determines the pooling size &amp;#8212;
so as to minimize the mean number of tests needed divided by &lt;span class="math"&gt;\(N\)&lt;/span&gt;, which can be
considered the pooling reduction&amp;nbsp;factor.&lt;/p&gt;
&lt;p&gt;The mean number of tests needed from the simple strategy&amp;nbsp;is
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{2} \label{simple_result}
\overline{N}_{simple} = (1 - p)^N\times 1 + \left [1 - (1-p)^N \right] \times (1 + N)
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
The mean number needed in the bisection strategy&amp;nbsp;is
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{3} \label{bisection_result}
\overline{N}_{bisection} = 1 +  2 \sum_{k=0}^{\mathbb{K}} 2^k \left (1 - (1 -p)^{2^{\mathbb{K}-k}} \right)
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
The proof of (\ref{simple_result}) is straightforward and we give an argument for
(\ref{bisection_result}) in an appendix. A cell of our notebook checks this
and confirms its&amp;nbsp;accuracy.&lt;/p&gt;
&lt;p&gt;Using the above results, our code produces plots of the mean number of tests
needed to screen a population vs &lt;span class="math"&gt;\(\mathbb{K}\)&lt;/span&gt;.  This then finds the optimal
number for each type.  The plots below give the results for the three &lt;span class="math"&gt;\(p\)&lt;/span&gt; values
noted in the&amp;nbsp;abstract.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Case 1: &lt;span class="math"&gt;\(p = 0.5\)&lt;/span&gt;, large fraction of disease carriers.  Main result: The
pooling strategies both cause the mean number of tests to be larger than if
we just screened each individual from the start (seen here because the y-axis
values are always bigger than 1).  The approach is not useful here.
&lt;img alt="![parameter study]({static}/images/pooling_05.png)" src="https://efavdb.com/images/pooling_05.png"&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Case 2: &lt;span class="math"&gt;\(p = 0.1\)&lt;/span&gt;, modest fraction of disease carriers.  Main result: The two
methods both give comparable benefits.  It is optimal to pool using
&lt;span class="math"&gt;\(\mathbb{K}=2\)&lt;/span&gt;, which gives groups of &lt;span class="math"&gt;\(N = 4\)&lt;/span&gt; patients. This cuts the number of
needed tests by a factor of &lt;span class="math"&gt;\(0.6\)&lt;/span&gt;.
&lt;img alt="![parameter study]({static}/images/pooling_01.png)" src="https://efavdb.com/images/pooling_01.png"&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Case 3: &lt;span class="math"&gt;\(p = 0.001\)&lt;/span&gt;, small fraction of disease carriers.  Main result:
Bisection wins, the optimal &lt;span class="math"&gt;\(\mathbb{K} = 9\)&lt;/span&gt; here, which gives a pooling
group of size &lt;span class="math"&gt;\(512\)&lt;/span&gt;.  We cut the test count needed by a factor of &lt;span class="math"&gt;\(50\)&lt;/span&gt;.  Note:
We also show here a histogram showing the number of tests needed when we run a
simulated system like this.  We see that we often only need one test, and there
is another peak around &lt;span class="math"&gt;\(20\)&lt;/span&gt; tests, with a long tail after that. 
&lt;img alt="![parameter study]({static}/images/pooling_01.png)" src="https://efavdb.com/images/pooling_0001.png"&gt;
&lt;img alt="![parameter study]({static}/images/pooling_hist.png)" src="https://efavdb.com/images/pooling_hist.png"&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The code to generate the optimal &lt;span class="math"&gt;\(\mathbb{K}\)&lt;/span&gt; plots above is given below.  This
can be used to generate generalized plots like those above for any &lt;span class="math"&gt;\(p\)&lt;/span&gt;. The
histogram plot is contained in our github repo, linked in our abstract.  Our
appendix&amp;nbsp;follows.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;pylab&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;

&lt;span class="n"&gt;K&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="n"&gt;P_POSITIVE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.05&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;theory_bisection&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;P_POSITIVE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;theory_simple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;P_POSITIVE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;
    &lt;span class="n"&gt;p0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;
    &lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;p0&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;p0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Bisection: fraction of full testing: &lt;/span&gt;&lt;span class="si"&gt;%2.2f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;theory_bisection&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Simple: fraction of full testing: &lt;/span&gt;&lt;span class="si"&gt;%2.2f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;theory_simple&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;theory_bisection&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;span class="n"&gt;min_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;o--&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;bisection (min = &lt;/span&gt;&lt;span class="si"&gt;%2.2f&lt;/span&gt;&lt;span class="s1"&gt;)&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;min_index&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;min_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;min_index&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;ro&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;theory_simple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;span class="n"&gt;min_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;o--&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;simple (min = &lt;/span&gt;&lt;span class="si"&gt;%2.2f&lt;/span&gt;&lt;span class="s1"&gt;)&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;min_index&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;min_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;min_index&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;go&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Test count reduction vs log_2 pooling size, p = &lt;/span&gt;&lt;span class="si"&gt;%0.3f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;log_2 pooling size&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;mean tests / pooling size&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;&lt;strong&gt;Appendix: Derivation of&amp;nbsp;(\ref{bisection_result})&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Consider a binary tree with the root node being the initial test. Each node
has two children that correspond to the tests of the two subgroups for a given
test.  We must test these if the parent is positive.  Level &lt;span class="math"&gt;\(0\)&lt;/span&gt; is the initial
test and &lt;span class="math"&gt;\(k\)&lt;/span&gt; rows down we call the level &lt;span class="math"&gt;\(k\)&lt;/span&gt; of tests.  There are total of &lt;span class="math"&gt;\(2^k\)&lt;/span&gt;
posible tests to run at this level, and there are a total of &lt;span class="math"&gt;\(\mathbb{K}\)&lt;/span&gt;&amp;nbsp;levels.&lt;/p&gt;
&lt;p&gt;The number of tests that need to be run at level &lt;span class="math"&gt;\(k\)&lt;/span&gt; is set by the number of
positive tests at level &lt;span class="math"&gt;\(k-1\)&lt;/span&gt;.  We&amp;nbsp;have
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
\text{Number of tests} = 1 + \sum_{k=0}^{\mathbb{K} - 1} \text{number positive level k}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Averaging this equation&amp;nbsp;gives
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
\overline{\text{Number of tests}} &amp;amp;=&amp;amp; 1 + \sum_{k=0}^{\mathbb{K} - 1} 2^k \times prob(\text{test at level k positive}) \\
&amp;amp;=&amp;amp; 1 +  \sum_{k=0}^{\mathbb{K} - 1} 2^k \times [ 1- (1 - p)^{2^{\mathbb{K} - k}}]. 
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
The inner factor here is the probability that a given test of the size being
considered comes back positive &amp;#8212; this has &lt;span class="math"&gt;\(N / 2^k = 2^{\mathbb{K} - k}\)&lt;/span&gt; people
in it.  This is the result shown above in&amp;nbsp;(\ref{bisection_result}).&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Statistics"></category></entry><entry><title>Dynamic programming in reinforcement learning</title><link href="https://efavdb.com/reinforcement-learning-dynamic-programming" rel="alternate"></link><published>2020-03-28T12:00:00-07:00</published><updated>2020-03-28T12:00:00-07:00</updated><author><name>Cathy Yeh</name></author><id>tag:efavdb.com,2020-03-28:/reinforcement-learning-dynamic-programming</id><summary type="html">&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;We discuss how to use dynamic programming (&lt;span class="caps"&gt;DP&lt;/span&gt;) to solve reinforcement learning (&lt;span class="caps"&gt;RL&lt;/span&gt;) problems where we have a perfect model of the environment.  &lt;span class="caps"&gt;DP&lt;/span&gt; is a general approach to solving problems by breaking them into subproblems that can be solved separately, cached, then combined to solve the overall&amp;nbsp;problem …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;We discuss how to use dynamic programming (&lt;span class="caps"&gt;DP&lt;/span&gt;) to solve reinforcement learning (&lt;span class="caps"&gt;RL&lt;/span&gt;) problems where we have a perfect model of the environment.  &lt;span class="caps"&gt;DP&lt;/span&gt; is a general approach to solving problems by breaking them into subproblems that can be solved separately, cached, then combined to solve the overall&amp;nbsp;problem.&lt;/p&gt;
&lt;p&gt;We’ll use a toy model, taken from [1], of a student transitioning between five states in college, which we also used in our &lt;a href="https://efavdb.com/intro-rl-toy-example.html"&gt;introduction&lt;/a&gt; to &lt;span class="caps"&gt;RL&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img alt="student MDP" src="https://efavdb.com/images/student_mdp.png"&gt;&lt;/p&gt;
&lt;p&gt;The model (dynamics) of the environment describe the probabilities of receiving a reward &lt;span class="math"&gt;\(r\)&lt;/span&gt; in the next state &lt;span class="math"&gt;\(s'\)&lt;/span&gt; given the current state &lt;span class="math"&gt;\(s\)&lt;/span&gt; and action &lt;span class="math"&gt;\(a\)&lt;/span&gt; taken, &lt;span class="math"&gt;\(p(s’, r | s, a)\)&lt;/span&gt;.  We can read these dynamics off the diagram of the student Markov Decision Process (&lt;span class="caps"&gt;MDP&lt;/span&gt;), for&amp;nbsp;example:&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(p(s'=\text{CLASS2}, r=-2 | s=\text{CLASS1}, a=\text{study}) =&amp;nbsp;1.0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(p(s'=\text{CLASS2}, r=1 | s=\text{CLASS3}, a=\text{pub}) =&amp;nbsp;0.4\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If you&amp;#8217;d like to jump straight to code, see this &lt;a href="https://github.com/frangipane/reinforcement-learning/blob/master/02-dynamic-programming/student_MDP_dynamic_programming_solutions.ipynb"&gt;jupyter notebook&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;The role of value functions in &lt;span class="caps"&gt;RL&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;The agent’s (student’s) policy maps states to actions, &lt;span class="math"&gt;\(\pi(a|s) := p(a|s)\)&lt;/span&gt;. 
The goal is to find the optimal policy &lt;span class="math"&gt;\(\pi_*\)&lt;/span&gt; that will maximize the expected cumulative rewards, the discounted return &lt;span class="math"&gt;\(G_t\)&lt;/span&gt;, in each state &lt;span class="math"&gt;\(s\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The value functions, &lt;span class="math"&gt;\(v_{\pi}(s)\)&lt;/span&gt; and &lt;span class="math"&gt;\(q_{\pi}(s, a)\)&lt;/span&gt;, in MDPs formalize this&amp;nbsp;goal.&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
v_{\pi}(s) &amp;amp;=&amp;amp; \mathbb{E}_{\pi}[G_t | S_t = s] \\
q_{\pi}(s, a) &amp;amp;=&amp;amp; \mathbb{E}_{\pi}[G_t | S_t = s, A_t = a]
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;We want to be able to calculate the value function for an arbitrary policy, i.e. &lt;em&gt;prediction&lt;/em&gt;, as well as use the value functions to find an optimal policy, i.e. the &lt;em&gt;control&lt;/em&gt;&amp;nbsp;problem.&lt;/p&gt;
&lt;h2&gt;Policy&amp;nbsp;evaluation&lt;/h2&gt;
&lt;p&gt;Policy evaluation deals with the problem of calculating the value function for some arbitrary policy.  In our introduction to &lt;span class="caps"&gt;RL&lt;/span&gt; &lt;a href="https://efavdb.com/intro-rl-toy-example.html"&gt;post&lt;/a&gt;, we showed that the value functions obey self-consistent, recursive relations, that make them amenable to &lt;span class="caps"&gt;DP&lt;/span&gt; approaches given a model of the&amp;nbsp;environment.&lt;/p&gt;
&lt;p&gt;These recursive relations are the Bellman expectation equations, which write the value of each state in terms of an average over the values of its successor / neighboring states, along with the expected reward along the&amp;nbsp;way.&lt;/p&gt;
&lt;p&gt;The Bellman expectation equation for &lt;span class="math"&gt;\(v_{\pi}(s)\)&lt;/span&gt;&amp;nbsp;is&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{state-value-bellman} \tag{1}
v_{\pi}(s) = \sum_{a} \pi(a|s) \sum_{s’, r} p(s’, r | s, a) [r + \gamma v_{\pi}(s’) ],
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\gamma\)&lt;/span&gt; is the discount factor &lt;span class="math"&gt;\(0 \leq \gamma \leq 1\)&lt;/span&gt; that weights the importance of future vs. current returns. &lt;strong&gt;&lt;span class="caps"&gt;DP&lt;/span&gt; turns (\ref{state-value-bellman}) into an update rule&lt;/strong&gt; (\ref{policy-evaluation}), &lt;span class="math"&gt;\(\{v_k(s’)\} \rightarrow v_{k+1}(s)\)&lt;/span&gt;, which iteratively converges towards the solution, &lt;span class="math"&gt;\(v_\pi(s)\)&lt;/span&gt;, for&amp;nbsp;(\ref{state-value-bellman}):&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{policy-evaluation} \tag{2}
v_{k+1}(s) = \sum_{a} \pi(a|s) \sum_{s’, r} p(s’, r | s, a) [r + \gamma v_k(s’) ]
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;Applying policy evaluation to our student model for an agent with a random policy, we arrive at the following state value function (see &lt;a href="https://github.com/frangipane/reinforcement-learning/blob/master/02-dynamic-programming/student_MDP_dynamic_programming_solutions.ipynb"&gt;jupyter notebook&lt;/a&gt; for&amp;nbsp;implementation):&lt;/p&gt;
&lt;p&gt;&lt;img alt="student MDP value function random policy" src="https://efavdb.com/images/student_mdp_values_random_policy.png"&gt;&lt;/p&gt;
&lt;h2&gt;Finding the optimal value functions and&amp;nbsp;policy&lt;/h2&gt;
&lt;h3&gt;Policy&amp;nbsp;iteration&lt;/h3&gt;
&lt;p&gt;We can evaluate the value functions for a given policy by turning the Bellman expectation equation (\ref{state-value-bellman}) into an update equation with the iterative policy evaluation&amp;nbsp;algorithm.&lt;/p&gt;
&lt;p&gt;But how do we use value functions to achieve our end goal of finding an optimal policy that corresponds to the optimal value&amp;nbsp;functions?&lt;/p&gt;
&lt;p&gt;Imagine we know the value function for a policy.  If taking the greedy action, corresponding to taking &lt;span class="math"&gt;\(\text{arg} \max_a q_{\pi}(s,a)\)&lt;/span&gt;, from any state in that policy is not consistent with that policy, or, equivalently, &lt;span class="math"&gt;\(\max_a q_{\pi}(s,a) &amp;gt; v_\pi(s)\)&lt;/span&gt;, then the policy is not optimal since we can improve the policy by taking the greedy action in that state and then onwards following the original&amp;nbsp;policy.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;policy iteration&lt;/em&gt; algorithm involves taking turns calculating the value function for a policy (policy evaluation) and improving on the policy (policy improvement) by taking the greedy action in each state for that value function until converging to &lt;span class="math"&gt;\(\pi_*\)&lt;/span&gt; and &lt;span class="math"&gt;\(v_*\)&lt;/span&gt; (see [2] for pseudocode for policy&amp;nbsp;iteration).&lt;/p&gt;
&lt;h3&gt;Value&amp;nbsp;iteration&lt;/h3&gt;
&lt;p&gt;Unlike policy iteration, the value iteration algorithm does not require complete convergence of policy evaluation before policy improvement, and, in fact, makes use of just a single iteration of policy evaluation.  Just as policy evaluation could be viewed as turning the Bellman expectation equation into an update, value iteration turns the Bellman optimality equation into an&amp;nbsp;update.&lt;/p&gt;
&lt;p&gt;In our previous &lt;a href="https://efavdb.com/intro-rl-toy-example.html"&gt;post&lt;/a&gt; introducing &lt;span class="caps"&gt;RL&lt;/span&gt; using the student example, we saw that the optimal value functions are the solutions to the Bellman optimality equation, e.g. for the optimal state-value&amp;nbsp;function:&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{state-value-bellman-optimality} \tag{3}
v_*(s) &amp;amp;=&amp;amp; \max_a q_{\pi*}(s, a) \\
    &amp;amp;=&amp;amp; \max_a \mathbb{E} [R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a] \\
    &amp;amp;=&amp;amp; \max_a \sum_{s’, r} p(s’, r | s, a) [r + \gamma v_*(s’) ]
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;As a &lt;span class="caps"&gt;DP&lt;/span&gt; update equation, (\ref{state-value-bellman-optimality})&amp;nbsp;becomes:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{value-iteration} \tag{4}
v_{k+1}(s) = \max_a \sum_{s’, r} p(s’, r | s, a) [r + \gamma v_k(s’) ]
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;Value iteration combines (truncated) policy evaluation with policy improvement in a single step; the state-value functions are updated with the averages of the value functions of the neighbor states that can occur from a greedy action, i.e. the action that maximizes the right hand side of&amp;nbsp;(\ref{value-iteration}).&lt;/p&gt;
&lt;p&gt;Applying value iteration to our student model, we arrive at the following optimal state value function, with the optimal policy delineated by red arrows (see &lt;a href="https://github.com/frangipane/reinforcement-learning/blob/master/02-dynamic-programming/student_MDP_dynamic_programming_solutions.ipynb"&gt;jupyter notebook&lt;/a&gt;):&lt;/p&gt;
&lt;p&gt;&lt;img alt="student MDP optimal policy and value function" src="https://efavdb.com/images/student_mdp_optimal_policy.png"&gt;&lt;/p&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;We’ve discussed how to solve for (a) the value functions of an arbitrary policy, (b) the optimal value functions and optimal policy.  Solving for (a) involves turning the Bellman expectation equations into an update, whereas (b) involves turning the Bellman optimality equations into an update.  These algorithms are guaranteed to converge (see [1] for notes on how the contraction mapping theorem guarantees&amp;nbsp;convergence).&lt;/p&gt;
&lt;p&gt;You can see the application of both policy evaluation and value iteration to the student model problem in this &lt;a href="https://github.com/frangipane/reinforcement-learning/blob/master/02-dynamic-programming/student_MDP_dynamic_programming_solutions.ipynb"&gt;jupyter notebook&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a name="References"&gt;References&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;[1] David Silver&amp;#8217;s &lt;span class="caps"&gt;RL&lt;/span&gt; Course Lecture 3 - Planning by Dynamic Programming (&lt;a href="https://www.youtube.com/watch?v=Nd1-UUMVfz4"&gt;video&lt;/a&gt;,
  &lt;a href="https://www.davidsilver.uk/wp-content/uploads/2020/03/DP.pdf"&gt;slides&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;[2] Sutton and Barto -
  &lt;a href="http://incompleteideas.net/book/RLbook2018.pdf"&gt;Reinforcement Learning: An Introduction&lt;/a&gt; - Chapter 4: Dynamic&amp;nbsp;Programming&lt;/p&gt;
&lt;p&gt;[3] Denny Britz’s &lt;a href="https://github.com/dennybritz/reinforcement-learning/tree/master/DP"&gt;notes&lt;/a&gt; on &lt;span class="caps"&gt;RL&lt;/span&gt; and &lt;span class="caps"&gt;DP&lt;/span&gt;, including crisp implementations in code of policy evaluation, policy iteration, and value iteration for the gridworld example discussed in&amp;nbsp;[2].&lt;/p&gt;
&lt;p&gt;[4] Deep &lt;span class="caps"&gt;RL&lt;/span&gt; Bootcamp Lecture 1: Motivation + Overview + Exact Solution Methods, by Pieter Abbeel (&lt;a href="https://www.youtube.com/watch?v=qaMdN6LS9rA"&gt;video&lt;/a&gt;, &lt;a href="https://drive.google.com/open?id=0BxXI_RttTZAhVXBlMUVkQ1BVVDQ"&gt;slides&lt;/a&gt;) - a very compressed&amp;nbsp;intro.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Machine learning"></category><category term="reinforcement learning"></category><category term="machine learning"></category></entry><entry><title>Introduction to reinforcement learning by example</title><link href="https://efavdb.com/intro-rl-toy-example" rel="alternate"></link><published>2020-03-11T12:00:00-07:00</published><updated>2020-03-11T12:00:00-07:00</updated><author><name>Cathy Yeh</name></author><id>tag:efavdb.com,2020-03-11:/intro-rl-toy-example</id><summary type="html">&lt;p&gt;We take a top-down approach to introducing reinforcement learning (&lt;span class="caps"&gt;RL&lt;/span&gt;) by starting with a toy example: a student going through college.  In order to frame the problem from the &lt;span class="caps"&gt;RL&lt;/span&gt; point-of-view, we&amp;#8217;ll walk through the following&amp;nbsp;steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Setting up a model of the problem&lt;/strong&gt; as a Markov Decision Process …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;We take a top-down approach to introducing reinforcement learning (&lt;span class="caps"&gt;RL&lt;/span&gt;) by starting with a toy example: a student going through college.  In order to frame the problem from the &lt;span class="caps"&gt;RL&lt;/span&gt; point-of-view, we&amp;#8217;ll walk through the following&amp;nbsp;steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Setting up a model of the problem&lt;/strong&gt; as a Markov Decision Process, the framework that underpins the &lt;span class="caps"&gt;RL&lt;/span&gt; approach to sequential decision-making&amp;nbsp;problems&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deciding on an objective&lt;/strong&gt;: maximize&amp;nbsp;rewards&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Writing down an equation whose solution is our objective&lt;/strong&gt;: Bellman&amp;nbsp;equations&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;David Silver walks through this example in his &lt;a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html"&gt;lecture notes&lt;/a&gt; on &lt;span class="caps"&gt;RL&lt;/span&gt;, but as far as we can tell, does not provide code, so we&amp;#8217;re sharing our implementation,&amp;nbsp;comprising:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the student&amp;#8217;s college &lt;a href="https://github.com/frangipane/reinforcement-learning/blob/master/02-dynamic-programming/discrete_limit_env.py"&gt;environment&lt;/a&gt; using the OpenAI gym&amp;nbsp;package.&lt;/li&gt;
&lt;li&gt;a &lt;a href="https://github.com/frangipane/reinforcement-learning/blob/master/02-dynamic-programming/student_MDP.ipynb"&gt;jupyter notebook&lt;/a&gt; sampling from the&amp;nbsp;model&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Student in toy&amp;nbsp;college&lt;/h2&gt;
&lt;p&gt;We model the student as an agent in a college environment who can move between five states: &lt;span class="caps"&gt;CLASS&lt;/span&gt; 1, 2, 3, the &lt;span class="caps"&gt;FACEBOOK&lt;/span&gt; state, and &lt;span class="caps"&gt;SLEEP&lt;/span&gt; state.  The states are represented by the four circles and square.  The &lt;span class="caps"&gt;SLEEP&lt;/span&gt; state &amp;#8212; the square with no outward bound arrows &amp;#8212; is a terminal state, i.e. once a student reaches that state, her journey is&amp;nbsp;finished.&lt;/p&gt;
&lt;p&gt;&lt;img alt="student MDP" src="https://efavdb.com/images/student_mdp.png"&gt;&lt;/p&gt;
&lt;p&gt;Actions that a student can take in her current state are labeled in red (facebook/quit/study/sleep/pub) and influence which state she’ll find herself in&amp;nbsp;next.&lt;/p&gt;
&lt;p&gt;In this model, most state transitions are deterministic functions of the action in the current state, e.g. if she decides to study in &lt;span class="caps"&gt;CLASS&lt;/span&gt; 1, then she’ll definitely advance to &lt;span class="caps"&gt;CLASS&lt;/span&gt; 2.  The single non-deterministic state transition is if she goes pubbing while in &lt;span class="caps"&gt;CLASS&lt;/span&gt; 3, where the pubbing action is indicated by a solid dot; she can end up in &lt;span class="caps"&gt;CLASS&lt;/span&gt; 1, 2 or back in 3 with probability 0.2, 0.4, or 0.4, respectively, depending on how reckless the pubbing&amp;nbsp;was.&lt;/p&gt;
&lt;p&gt;The model also specifies the reward &lt;span class="math"&gt;\(R\)&lt;/span&gt; associated with acting in one state and ending up in the next.  In this example, the dynamics &lt;span class="math"&gt;\(p(s’,r|s,a)\)&lt;/span&gt;, are given to us, i.e. we have a full model of the environment, and, hopefully, the rewards have been designed to capture the actual end goal of the&amp;nbsp;student.&lt;/p&gt;
&lt;h2&gt;Markov Decision&amp;nbsp;Process&lt;/h2&gt;
&lt;p&gt;Formally, we’ve modeled the student’s college experience as a finite Markov Decision Process (&lt;span class="caps"&gt;MDP&lt;/span&gt;).  The dynamics are Markov because the probability of ending up in the next state depends only on the current state and action, not on any history leading up to the current state.  The Markov property is integral to the simplification of the equations that describe the model, which we&amp;#8217;ll see in a&amp;nbsp;bit.&lt;/p&gt;
&lt;p&gt;The components of an &lt;span class="caps"&gt;MDP&lt;/span&gt;&amp;nbsp;are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(S\)&lt;/span&gt; - the set of possible&amp;nbsp;states&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(R\)&lt;/span&gt; - the set of (scalar)&amp;nbsp;rewards&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(A\)&lt;/span&gt; - the set of possible actions in each&amp;nbsp;state&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The dynamics of the system are described by the probabilities of receiving a reward in the next state given the current state and action taken, &lt;span class="math"&gt;\(p(s’,r|s,a)\)&lt;/span&gt;.  In this example, the &lt;span class="caps"&gt;MDP&lt;/span&gt; is finite because there are a finite number of states, rewards, and&amp;nbsp;actions.&lt;/p&gt;
&lt;p&gt;The student’s agency in this environment comes from how she decides to act in each state.  The mapping of a state to actions is the &lt;strong&gt;policy&lt;/strong&gt;, &lt;span class="math"&gt;\(\pi(a|s) := p(a|s)\)&lt;/span&gt;, and can be a deterministic or stochastic function of her&amp;nbsp;state.&lt;/p&gt;
&lt;p&gt;Suppose we have an indifferent student who always chooses actions randomly.  We can sample from the &lt;span class="caps"&gt;MDP&lt;/span&gt; to get some example trajectories the student might experience with this policy.  In the sample trajectories below, the states are enclosed in parentheses &lt;code&gt;(STATE)&lt;/code&gt;, and actions enclosed in square brackets &lt;code&gt;[action]&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sample trajectories&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;CLASS1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="c1"&gt;--[facebook]--&amp;gt;(FACEBOOK)--[facebook]--&amp;gt;(FACEBOOK)--[facebook]--&amp;gt;(FACEBOOK)--[facebook]--&amp;gt;(FACEBOOK)--[quit]--&amp;gt;(CLASS1)--[facebook]--&amp;gt;(FACEBOOK)--[quit]--&amp;gt;(CLASS1)--[study]--&amp;gt;(CLASS2)--[sleep]--&amp;gt;(SLEEP)&lt;/span&gt;

&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;FACEBOOK&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="c1"&gt;--[quit]--&amp;gt;(CLASS1)--[study]--&amp;gt;(CLASS2)--[study]--&amp;gt;(CLASS3)--[study]--&amp;gt;(SLEEP)&lt;/span&gt;

&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;SLEEP&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;CLASS1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="c1"&gt;--[facebook]--&amp;gt;(FACEBOOK)--[quit]--&amp;gt;(CLASS1)--[study]--&amp;gt;(CLASS2)--[sleep]--&amp;gt;(SLEEP)&lt;/span&gt;

&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;FACEBOOK&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="c1"&gt;--[facebook]--&amp;gt;(FACEBOOK)--[facebook]--&amp;gt;(FACEBOOK)--[facebook]--&amp;gt;(FACEBOOK)--[facebook]--&amp;gt;(FACEBOOK)--[quit]--&amp;gt;(CLASS1)--[facebook]--&amp;gt;(FACEBOOK)--[quit]--&amp;gt;(CLASS1)--[study]--&amp;gt;(CLASS2)--[study]--&amp;gt;(CLASS3)--[pub]--&amp;gt;(CLASS2)--[study]--&amp;gt;(CLASS3)--[study]--&amp;gt;(SLEEP)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Rewards following a random policy&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;Under this random policy, what total reward would the student expect when starting from any of the states?  We can estimate the expected rewards by summing up the rewards per trajectory and plotting the distributions of total rewards per starting&amp;nbsp;state:&lt;/p&gt;
&lt;p&gt;&lt;img alt="histogram of sampled returns" src="https://efavdb.com/images/intro_rl_histogram_sampled_returns.png"&gt;&lt;/p&gt;
&lt;h2&gt;Maximizing rewards: discounted return and value&amp;nbsp;functions&lt;/h2&gt;
&lt;p&gt;We’ve just seen how we can estimate rewards starting from each state given a random policy.  Next, we’ll formalize our goal in terms of maximizing&amp;nbsp;returns.&lt;/p&gt;
&lt;h3&gt;Returns&lt;/h3&gt;
&lt;p&gt;We simply summed the rewards from the sample trajectories above, but the quantity we often want to maximize in practice is the &lt;strong&gt;discounted return &lt;span class="math"&gt;\(G_t\)&lt;/span&gt;&lt;/strong&gt;, which is a sum of the weighted&amp;nbsp;rewards:&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{return} \tag{1}
G_t := R_{t+1} + \gamma R_{t+2} + … = \sum_{k=0}^\infty \gamma^k R_{t+k+1}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(0 \leq \gamma \leq 1\)&lt;/span&gt;.  &lt;span class="math"&gt;\(\gamma\)&lt;/span&gt; is the &lt;em&gt;discount rate&lt;/em&gt; which characterizes how much we weight rewards now vs. later.  Discounting is mathematically useful for avoiding infinite returns in MDPs without a terminal state and allows us to account for uncertainty in the future when we don’t have a perfect model of the&amp;nbsp;environment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Aside&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The discount factor introduces a time scale since it says that we don&amp;#8217;t care about rewards that are far in the future.  The half-life (actually, the &lt;span class="math"&gt;\(1/e\)&lt;/span&gt; life) of a reward in units of time steps is &lt;span class="math"&gt;\(1/(1-\gamma)\)&lt;/span&gt;, which comes from a definition of &lt;span class="math"&gt;\(1/e\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
\frac{1}{e} = \lim_{n \rightarrow \infty} \left(1 - \frac{1}{n} \right)^n
\end{align}&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\gamma = 0.99\)&lt;/span&gt; is often used in practice, which corresponds to a half-life of 100 timesteps since &lt;span class="math"&gt;\(0.99^{100} = (1 - 1/100)^{100} \approx 1/e\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;Value&amp;nbsp;functions&lt;/h3&gt;
&lt;p&gt;Earlier, we were able to estimate the expected undiscounted returns starting from each state by sampling from the &lt;span class="caps"&gt;MDP&lt;/span&gt; under a random policy.  Value functions formalize this notion of the &amp;#8220;goodness&amp;#8221; of being in a&amp;nbsp;state.&lt;/p&gt;
&lt;h4&gt;State value function &lt;span class="math"&gt;\(v\)&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;The &lt;strong&gt;state value function&lt;/strong&gt; &lt;span class="math"&gt;\(v_{\pi}(s)\)&lt;/span&gt; is the expected return when starting in state &lt;span class="math"&gt;\(s\)&lt;/span&gt;, following policy &lt;span class="math"&gt;\(\pi\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{state-value} \tag{2}
v_{\pi}(s) = \mathbb{E}_{\pi}[G_t | S_t = s]
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;The state value function can be written as a recursive relationship, the Bellman expectation equation, expressing the value of a state in terms of the values of its neighors by making use of the Markov&amp;nbsp;property.&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{state-value-bellman} \tag{3}
v_{\pi}(s) &amp;amp;=&amp;amp; \mathbb{E}_{\pi}[G_t | S_t = s] \\
       &amp;amp;=&amp;amp; \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+2} | S_t = s] \\
       &amp;amp;=&amp;amp; \sum_{a} \pi(a|s) \sum_{s’, r} p(s’, r | s, a) [r + \gamma v_{\pi}(s’) ]
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;This equation expresses the value of a state as an average over the discounted value of its neighbor / successor states, plus the expected reward transitioning from &lt;span class="math"&gt;\(s\)&lt;/span&gt; to &lt;span class="math"&gt;\(s’\)&lt;/span&gt;, and &lt;span class="math"&gt;\(v_{\pi}\)&lt;/span&gt; is the unique&lt;a href="#unique"&gt;*&lt;/a&gt; solution.  The distribution of rewards depends on the student’s policy since her actions influence her future&amp;nbsp;rewards.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note on terminology&lt;/em&gt;:
Policy &lt;em&gt;evaluation&lt;/em&gt; uses the Bellman expectation equation to solve for the value function given a policy &lt;span class="math"&gt;\(\pi\)&lt;/span&gt; and environment dynamics &lt;span class="math"&gt;\(p(s’, r | s, a)\)&lt;/span&gt;.  This is different from policy iteration and value iteration, which are concerned with finding an optimal&amp;nbsp;policy.&lt;/p&gt;
&lt;p&gt;We can solve the Bellman equation for the value function as an alternative to the sampling we did earlier for the student toy example.  Since the problem has a small number of states and actions, and we have full knowledge of the environment, an exact solution is feasible by directly solving the system of linear equations or iteratively using dynamic programming.  Here is the solution to (\ref{state-value-bellman}) for &lt;span class="math"&gt;\(v\)&lt;/span&gt; under a random policy in the student example (compare to the sample means in the histogram of&amp;nbsp;returns):&lt;/p&gt;
&lt;p&gt;&lt;img alt="student MDP value function random policy" src="https://efavdb.com/images/student_mdp_values_random_policy.png"&gt;&lt;/p&gt;
&lt;p&gt;We can verify that the solution is self-consistent by spot checking the value of a state in terms of the values of its neighboring states according to the Bellman equation, e.g. the &lt;span class="caps"&gt;CLASS1&lt;/span&gt; state with &lt;span class="math"&gt;\(v_{\pi}(\text{CLASS1}) = -1.3\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
v_{\pi}(\text{CLASS1}) = 0.5 [-2 + 2.7] + 0.5 [-1 + -2.3] = -1.3
$$&lt;/div&gt;
&lt;h4&gt;Action value function &lt;span class="math"&gt;\(q\)&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;Another value function is the action value function &lt;span class="math"&gt;\(q_{\pi}(s, a)\)&lt;/span&gt;, which is the expected return from a state &lt;span class="math"&gt;\(s\)&lt;/span&gt; if we follow a policy &lt;span class="math"&gt;\(\pi\)&lt;/span&gt; after taking an action &lt;span class="math"&gt;\(a\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{action-value} \tag{4}
q_{\pi}(s, a) := \mathbb{E}_{\pi} [ G_t | S_t = s, A = a ]
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;We can also write &lt;span class="math"&gt;\(v\)&lt;/span&gt; and &lt;span class="math"&gt;\(q\)&lt;/span&gt; in terms of each other.  For example, the state value function can be viewed as an average over the action value functions for that state, weighted by the probability of taking each action, &lt;span class="math"&gt;\(\pi\)&lt;/span&gt;, from that&amp;nbsp;state:&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{state-value-one-step-backup} \tag{5}
v_{\pi}(s) = \sum_{a} \pi(a|s) q_{\pi}(s, a)
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;Rewriting &lt;span class="math"&gt;\(v\)&lt;/span&gt; in terms of &lt;span class="math"&gt;\(q\)&lt;/span&gt; in (\ref{state-value-one-step-backup}) is useful later for thinking about the &amp;#8220;advantage&amp;#8221;, &lt;span class="math"&gt;\(A(s,a)\)&lt;/span&gt;, of taking an action in a state, namely how much better is an action in that state than the&amp;nbsp;average?&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
A(s,a) \equiv q(s,a) - v(s)
\end{align}&lt;/div&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Why &lt;span class="math"&gt;\(q\)&lt;/span&gt; in addition to &lt;span class="math"&gt;\(v\)&lt;/span&gt;?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Looking ahead, we almost never have access to the environment dynamics in real world problems, but solving for &lt;span class="math"&gt;\(q\)&lt;/span&gt; instead of &lt;span class="math"&gt;\(v\)&lt;/span&gt; lets us get around this problem; we can figure out the best action to take in a state solely using &lt;span class="math"&gt;\(q\)&lt;/span&gt; (we further expand on this in our &lt;a href="#optimalq"&gt;discussion&lt;/a&gt; below on the Bellman optimality equation for &lt;span class="math"&gt;\(q_*\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A concrete example of using &lt;span class="math"&gt;\(q\)&lt;/span&gt; is provided in our &lt;a href="https://efavdb.com/multiarmed-bandits"&gt;post&lt;/a&gt; on multiarmed bandits (an example of a simple single-state &lt;span class="caps"&gt;MDP&lt;/span&gt;), which discusses agents/algorithms that don&amp;#8217;t have access to the true environment dynamics.  The strategy amounts to estimating the action value function of the slot machine and using those estimates to inform which slot machine arms to pull in order to maximize&amp;nbsp;rewards.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Optimal value and&amp;nbsp;policy&lt;/h2&gt;
&lt;p&gt;The crux of the &lt;span class="caps"&gt;RL&lt;/span&gt; problem is finding a policy that maximizes the expected return.  A policy &lt;span class="math"&gt;\(\pi\)&lt;/span&gt; is defined to be better than another policy &lt;span class="math"&gt;\(\pi’\)&lt;/span&gt; if &lt;span class="math"&gt;\(v_{\pi}(s) &amp;gt; v_{\pi’}(s)\)&lt;/span&gt; for all states.  We are guaranteed&lt;a href="#unique"&gt;*&lt;/a&gt; an optimal state value function &lt;span class="math"&gt;\(v_*\)&lt;/span&gt; which corresponds to one or more optimal policies &lt;span class="math"&gt;\(\pi*\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Recall that the value function for an arbitrary policy can be written in terms of an average over the action values for that state (\ref{state-value-one-step-backup}).  In contrast, the optimal value function &lt;span class="math"&gt;\(v_*\)&lt;/span&gt; must be consistent with following a policy that selects the action that maximizes the action value functions from a state, i.e. taking a &lt;span class="math"&gt;\(\max\)&lt;/span&gt; (\ref{state-value-bellman-optimality}) instead of an average (\ref{state-value-one-step-backup}) over &lt;span class="math"&gt;\(q\)&lt;/span&gt;, leading to the &lt;strong&gt;Bellman optimality equation&lt;/strong&gt; for &lt;span class="math"&gt;\(v_*\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{state-value-bellman-optimality} \tag{6}
v_*(s) &amp;amp;=&amp;amp; \max_a q_{\pi*}(s, a) \\
    &amp;amp;=&amp;amp; \max_a \mathbb{E}_{\pi*} [R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a] \\
    &amp;amp;=&amp;amp; \max_a \sum_{s’, r} p(s’, r | s, a) [r + \gamma v_*(s’) ]
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;The optimal policy immediately follows: take the action in a state that maximizes the right hand side of (\ref{state-value-bellman-optimality}).  The &lt;a href="https://en.wikipedia.org/wiki/Bellman_equation#Bellman's_Principle_of_Optimality"&gt;principle of optimality&lt;/a&gt;, which applies to the Bellman optimality equation, means that this greedy policy actually corresponds to the optimal policy!  Note: Unlike the Bellman expectation equations, the Bellman optimality equations are a nonlinear system of equations due to taking the&amp;nbsp;max.&lt;/p&gt;
&lt;p&gt;The Bellman optimality equation for the action value function &lt;span class="math"&gt;\(q_*(s,a)\)&lt;/span&gt;&lt;a name="optimalq"&gt;&lt;/a&gt;&amp;nbsp;is:&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{action-value-bellman-optimality} \tag{7}
q_*(s, a) &amp;amp;=&amp;amp; \mathbb{E}_{\pi*} [R_{t+1} + \gamma \max_{a'} q_*(S_{t+1}', a') | S_t = s, A_t = a] \\
          &amp;amp;=&amp;amp; \sum_{s', r} p(s', r | s, a) [r + \gamma \max_{a'} q_*(s', a') ]
\end{eqnarray}&lt;/div&gt;
&lt;hr&gt;
&lt;p&gt;Looking ahead: In practice, without a knowledge of the environment dynamics, &lt;span class="caps"&gt;RL&lt;/span&gt; algorithms based on solving value functions can approximate the expectation in (\ref{action-value-bellman-optimality}) by sampling, i.e. interacting with the environment, and iteratively selecting the action that corresponds to maximizing &lt;span class="math"&gt;\(q\)&lt;/span&gt; in each state that the agent lands in along its trajectory, which is possible since the maximum occurs &lt;strong&gt;inside&lt;/strong&gt; the summation in (\ref{action-value-bellman-optimality}).   In contrast, this sampling approach doesn&amp;#8217;t work for (\ref{state-value-bellman-optimality}) because of the maximum &lt;strong&gt;outside&lt;/strong&gt; the summation in&amp;#8230;that&amp;#8217;s why action value functions are so useful when we lack a model of the&amp;nbsp;environment!&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Here is the optimal state value function and policy for the student example, which we solve for in a later&amp;nbsp;post:&lt;/p&gt;
&lt;p&gt;&lt;img alt="student MDP optimal value function" src="https://efavdb.com/images/student_mdp_optimal_values.png"&gt;&lt;/p&gt;
&lt;p&gt;Comparing the values per state under the optimal policy vs the random policy, the value in every state under the optimal policy exceeds the value under the random&amp;nbsp;policy.&lt;/p&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;We’ve discussed how the problem of sequential decision making can be framed as an &lt;span class="caps"&gt;MDP&lt;/span&gt; using the student toy &lt;span class="caps"&gt;MDP&lt;/span&gt; as an example.  The goal in &lt;span class="caps"&gt;RL&lt;/span&gt; is to figure out a policy &amp;#8212; what actions to take in each state &amp;#8212; that maximizes our&amp;nbsp;returns.&lt;/p&gt;
&lt;p&gt;MDPs provide a framework for approaching the problem by defining the value of each state, the value functions, and using the value functions to define what a “best policy” means.  The value functions are unique solutions to the Bellman equations, and the &lt;span class="caps"&gt;MDP&lt;/span&gt; is “solved” when we know the optimal value&amp;nbsp;function.&lt;/p&gt;
&lt;p&gt;Much of reinforcement learning centers around trying to solve these equations under different conditions, e.g. unknown environment dynamics and large &amp;#8212; possibly continuous &amp;#8212; states and/or action spaces that require approximations to the value&amp;nbsp;functions.&lt;/p&gt;
&lt;p&gt;We&amp;#8217;ll discuss how we arrived at the solutions for this toy problem in a future&amp;nbsp;post!&lt;/p&gt;
&lt;h3&gt;Example&amp;nbsp;code&lt;/h3&gt;
&lt;p&gt;Code for sampling from the student environment under a random policy in order to generate the trajectories and histograms of returns is available in this &lt;a href="https://github.com/frangipane/reinforcement-learning/blob/master/02-dynamic-programming/student_MDP.ipynb"&gt;jupyter notebook&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://github.com/frangipane/reinforcement-learning/blob/master/02-dynamic-programming/discrete_limit_env.py"&gt;code&lt;/a&gt; for the student environment creates an environment with an &lt;span class="caps"&gt;API&lt;/span&gt; that is compatible with OpenAI gym &amp;#8212; specifically, it is derived from the &lt;code&gt;gym.envs.toy_text.DiscreteEnv&lt;/code&gt; environment.&lt;/p&gt;
&lt;p&gt;&lt;a name="unique"&gt;&lt;em&gt;&lt;/a&gt;The uniqueness of the solution to the Bellman equations for finite MDPs is stated without proof in Ref [2], but Ref [1] motivates it briefly via the &lt;/em&gt;contraction mapping&amp;nbsp;theorem*.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;[1] David Silver&amp;#8217;s &lt;span class="caps"&gt;RL&lt;/span&gt; Course Lecture 2 - (&lt;a href="https://www.youtube.com/watch?v=lfHX2hHRMVQ"&gt;video&lt;/a&gt;,
  &lt;a href="https://www.davidsilver.uk/wp-content/uploads/2020/03/MDP.pdf"&gt;slides&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;[2] Sutton and Barto -
  &lt;a href="http://incompleteideas.net/book/RLbook2018.pdf"&gt;Reinforcement Learning: An Introduction&lt;/a&gt; - Chapter 3: Finite Markov Decision&amp;nbsp;Processes&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Machine learning"></category><category term="reinforcement learning"></category><category term="machine learning"></category><category term="OpenAI"></category></entry><entry><title>A Framework for Studying Population Dynamics</title><link href="https://efavdb.com/world-wandering-dudes" rel="alternate"></link><published>2020-03-08T00:00:00-08:00</published><updated>2020-03-08T00:00:00-08:00</updated><author><name>Dustin McIntosh</name></author><id>tag:efavdb.com,2020-03-08:/world-wandering-dudes</id><summary type="html">&lt;p&gt;In this post, I want to briefly introduce a new side project for the blog with applications to understanding population dynamics, natural selection, game theory, and probably&amp;nbsp;more.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/dustinmcintosh/world_wandering_dudes"&gt;World Wandering Dudes&lt;/a&gt; is a simulation framework in which you initiate a “world” which consists of a “field” and a set of …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In this post, I want to briefly introduce a new side project for the blog with applications to understanding population dynamics, natural selection, game theory, and probably&amp;nbsp;more.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/dustinmcintosh/world_wandering_dudes"&gt;World Wandering Dudes&lt;/a&gt; is a simulation framework in which you initiate a “world” which consists of a “field” and a set of “creatures” (dudes). The field has food on it. Each day, the creatures run around gathering the food which they need to survive and&amp;nbsp;reproduce.&lt;/p&gt;
&lt;h3&gt;Example&lt;/h3&gt;
&lt;p&gt;Here’s an example of a few days passing in a world where food randomly sprouts each day, never spoiling, initiated with a single creature (particular note: after day 1 passes and there are two creatures, one of them doesn&amp;#8217;t store enough food to reproduce at the end of the day):
&lt;img alt="" src="https://efavdb.com/images/the_first_days.gif"&gt;&lt;/p&gt;
&lt;p&gt;Taking a snapshot of the world at the end of each day for the first 20 or so days you can see the creatures take over the full field before coming to some general equilibrium state.
&lt;img alt="" src="https://efavdb.com/images/each_day.gif"&gt;&lt;/p&gt;
&lt;h3&gt;How the world&amp;nbsp;works&lt;/h3&gt;
&lt;p&gt;A. Each day consists of a number of discrete time steps.  During each time step, the creatures move around the field randomly, if they find food they grab it from the field and store&amp;nbsp;it.&lt;/p&gt;
&lt;p&gt;B. At the end of the day, a few things&amp;nbsp;happen:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Each creature must eat some food.  If they don’t have enough stored, they&amp;nbsp;die.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If they have enough food after eating, they may also reproduce.  Offspring may have mutated properties (e.g., they may move a little faster each time step - speedy creatures - or they may eat less food - efficient&amp;nbsp;creatures)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The food may spoil throughout the world (or not) and new food may sprout on the&amp;nbsp;field.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Examining the historical&amp;nbsp;record&lt;/h3&gt;
&lt;p&gt;You can also look at this historical recordfor the field and examine some metrics including total creature count, birth/death rate, the mutation composition of the creatures, amount of stored food, amount of ungathered food, and more:
&lt;img alt="" src="https://efavdb.com/images/example_history.png"&gt;
Some phenomenological notes on this particular case (more details on the math behind some of this in future&amp;nbsp;posts):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The dynamics of the world are stochastic. For example, sometimes the first creature doesn’t find any food and dies&amp;nbsp;immediately.&lt;/li&gt;
&lt;li&gt;The population initially grows roughly exponentially as food becomes plentiful across the&amp;nbsp;map.&lt;/li&gt;
&lt;li&gt;With the accumulated food on the field from the initial low-population days, the creatures grow in numbers beyond a sustainable population and a period of starvation and population culling follows.&amp;nbsp;:(&lt;/li&gt;
&lt;li&gt;The population reaches an equilibrium at which the number of creatures is nearly the same as the amount of food sprouted each day (it’s not exactly&amp;nbsp;equal!).&lt;/li&gt;
&lt;li&gt;At equilibrium, the rate at which creatures are being born is equal to the rate at which they die (on average) and both appear to be about a third of the total population (it’s not a&amp;nbsp;third!).&lt;/li&gt;
&lt;li&gt;As mentioned above, upon reproduction the creatures will mutate and the fitter creatures may take over the world. In this particular case, efficient creatures come about first and quickly take over the population. The world can actually sustain a higher population of efficient vs normal/speedy creatures, so the total population increases accordingly. Shortly thereafter, a few speedy creatures start to show up and they, slowly, take over the world, out-competing the efficient creatures and slowly suppressing the overall&amp;nbsp;population.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;More to come on extensions of this project and understanding the math behind it in the&amp;nbsp;future.&lt;/p&gt;
&lt;h3&gt;Check it out&amp;nbsp;yourself&lt;/h3&gt;
&lt;p&gt;The github repository is &lt;a href="https://github.com/dustinmcintosh/world_wandering_dudes"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You’ll need a bunch of the usual python packages for data&amp;nbsp;science.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git clone https://github.com/dustinmcintosh/world_wandering_dudes

&lt;span class="nb"&gt;cd&lt;/span&gt; world_wandering_dudes
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Update the directory for saving figures in &lt;code&gt;SET_ME.py&lt;/code&gt; if you&amp;#8217;d like to store them somewhere&amp;nbsp;special.&lt;/p&gt;
&lt;p&gt;Run the sample&amp;nbsp;code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;python scripts/basic_simulation.py
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;You can recycle the same world again&amp;nbsp;using:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;python scripts/basic_simulation.py -wp my_world.pkl
&lt;/pre&gt;&lt;/div&gt;</content><category term="World Wandering Dudes"></category><category term="python"></category><category term="statistics"></category><category term="world wandering dudes"></category></entry><entry><title>Multiarmed bandits in the context of reinforcement learning</title><link href="https://efavdb.com/multiarmed-bandits" rel="alternate"></link><published>2020-02-25T12:00:00-08:00</published><updated>2020-02-25T12:00:00-08:00</updated><author><name>Cathy Yeh</name></author><id>tag:efavdb.com,2020-02-25:/multiarmed-bandits</id><summary type="html">&lt;p&gt;&lt;a href="http://incompleteideas.net/book/RLbook2018.pdf"&gt;Reinforcement Learning: An Introduction&lt;/a&gt; by Sutton and Barto[1] is a book that is universally recommended to beginners in their &lt;span class="caps"&gt;RL&lt;/span&gt; studies.  The first chapter is an extended text-heavy introduction. The second chapter deals with multiarmed bandits, i.e. slot machines with multiple arms, and is the subject of today …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="http://incompleteideas.net/book/RLbook2018.pdf"&gt;Reinforcement Learning: An Introduction&lt;/a&gt; by Sutton and Barto[1] is a book that is universally recommended to beginners in their &lt;span class="caps"&gt;RL&lt;/span&gt; studies.  The first chapter is an extended text-heavy introduction. The second chapter deals with multiarmed bandits, i.e. slot machines with multiple arms, and is the subject of today&amp;#8217;s&amp;nbsp;post.&lt;/p&gt;
&lt;p&gt;Before getting into the &lt;em&gt;what&lt;/em&gt; and &lt;em&gt;how&lt;/em&gt; of bandits, I&amp;#8217;d like to address the &lt;strong&gt;why&lt;/strong&gt;, since the &amp;#8220;why&amp;#8221; can guard against getting lost in the details / not seeing the forest for the&amp;nbsp;trees.&lt;/p&gt;
&lt;h1&gt;Why discuss multiarmed&amp;nbsp;bandits?&lt;/h1&gt;
&lt;p&gt;&lt;span class="caps"&gt;RL&lt;/span&gt; treats the problem of trying to achieve a goal in an environment where an agent is &lt;em&gt;not&lt;/em&gt; instructed about which actions to take to achieve that goal, in contrast to supervised learning problems.  Learning the best actions to take is a complicated problem, since the best actions depend on what state an agent is in, e.g. an agent trying to get to a goalpost east of its current location as quickly as possible may find that moving east is a generally good policy, but not if there is a fire-breathing dragon in the way, in which case, it might make sense to move up or down to navigate around the&amp;nbsp;obstacle.&lt;/p&gt;
&lt;p&gt;Multiarmed bandits are simpler problem: a single state system.  No matter which action an agent takes, i.e. which slot machine arm the agent pulls, the agent ends up back in the same state; the distribution of rewards as a consequence of the agent&amp;#8217;s action remains the same, assuming a stationary distribution of rewards, and actions have no effect on subsequent states or rewards.  This simple case study is useful for building intuition and introducing &lt;span class="caps"&gt;RL&lt;/span&gt; concepts that will be expanded on in later chapters of&amp;nbsp;[1].&lt;/p&gt;
&lt;h1&gt;Key &lt;span class="caps"&gt;RL&lt;/span&gt; concepts introduced by the multiarmed bandit&amp;nbsp;problem&lt;/h1&gt;
&lt;h2&gt;The nature of the&amp;nbsp;problem&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Agent has a goal&lt;/strong&gt;: In &lt;span class="caps"&gt;RL&lt;/span&gt; and multiarmed bandit problems, we want to figure out the strategy, or &amp;#8220;policy&amp;#8221; in &lt;span class="caps"&gt;RL&lt;/span&gt; lingo, that will maximize our rewards.  For the simple bandit problem, this goal is equivalent to maximizing the reward &amp;#8212; literally, money! &amp;#8212; for each arm&amp;nbsp;pull.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Unlike supervised learning, no ground truth is supplied&lt;/strong&gt;: Each slot has a different distribution of rewards, but the agent playing the machine does not know that distribution.  Instead, the agent has to try different actions and evaluate how good the actions are.  The goodness of an action is straightforwardly determined by its immediate reward in the bandit&amp;nbsp;case.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Exploration vs. exploitation&lt;/strong&gt;:  Based on a few trials, one arm may appear to yield the highest rewards, but the agent may decide to try others occasionally to improve its estimates of the rewards, an example of balancing exploration and exploitation.  The various algorithms handle exploration vs. exploitation differently, but this example introduces one method that is simple but widely-used in practice: the epsilon-greedy algorithm, which takes greedy actions most of the time (exploits) but takes random actions (explores) a fraction epsilon of the&amp;nbsp;time.&lt;/p&gt;
&lt;h3&gt;Different approaches to learning a&amp;nbsp;policy&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;model-free&lt;/strong&gt;:  All the strategies discussed in [1] for solving the bandit problem are &amp;#8220;model-free&amp;#8221; strategies.  In real world applications, a model of the world is rarely available, and the agent has to figure out how to act based on sampled experience, and the same applies to the bandit case; even though bandits are a simpler single state system (we don&amp;#8217;t have to model transitions from state to state), an agent still does not know the model that generates the probability of a reward &lt;span class="math"&gt;\(r\)&lt;/span&gt; given an action &lt;span class="math"&gt;\(a\)&lt;/span&gt;, &lt;span class="math"&gt;\(P(r|a)\)&lt;/span&gt; and has to figure that out from trial and&amp;nbsp;error.&lt;/p&gt;
&lt;p&gt;There &lt;em&gt;are&lt;/em&gt; model-based algorithms that attempt to model the environment&amp;#8217;s transition dynamics from data, but many popular algorithms today are model-free because of the difficulty of modeling those&amp;nbsp;dynamics.&lt;/p&gt;
&lt;h4&gt;Learning&amp;nbsp;action-values&lt;/h4&gt;
&lt;p&gt;The bandit problem introduces the idea of estimating the expected value associated with each action, namely the &lt;em&gt;action-value function&lt;/em&gt; in &lt;span class="caps"&gt;RL&lt;/span&gt; terms.  The concept is very intuitive &amp;#8212; as an agent pulls on different bandit arms, it will accumulate rewards associated with each arm.  A simple way to estimate the expected value per arm is just to average the rewards generated by pulling on each slot.  The policy that follows is then implicit, namely, take the action / pull on the arm with the highest estimated&amp;nbsp;action-value!&lt;/p&gt;
&lt;p&gt;Historically, &lt;span class="caps"&gt;RL&lt;/span&gt; formalism has dealt with estimating value functions and using them to figure out a policy, which includes the Q-Learning (&amp;#8220;Q&amp;#8221; stands for action-value!) approach we mentioned in our earlier &lt;a href="https://efavdb.com/openai-scholars-intro"&gt;post&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Learning policies&amp;nbsp;directly&lt;/h4&gt;
&lt;p&gt;[1] also use the bandit problem to introduce a type of algorithm that approaches the problem, not indirectly by learning a value function and deriving the policy from those value functions, but by parameterizing the policy directly and learning the parameters that optimize the rewards.  This class of algorithm is a &amp;#8220;policy gradient method&amp;#8221; and is very popular today for its nice convergence properties.  After the foreshadowing in the bandit problem, policy gradients only reappear very late in [1] &amp;#8212; chapter&amp;nbsp;13!&lt;/p&gt;
&lt;p&gt;We now provide code for&amp;nbsp;concreteness.&lt;/p&gt;
&lt;h1&gt;Ground truth is hidden in our multiarmed&amp;nbsp;bandit&lt;/h1&gt;
&lt;p&gt;The &lt;code&gt;Bandit&lt;/code&gt; class initializes a multiarmed bandit. The distribution of rewards per arm follows a Gaussian distribution with some mean dollar&amp;nbsp;amount.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Bandit&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;N-armed bandit with stationary distribution of rewards per arm.&lt;/span&gt;
&lt;span class="sd"&gt;    Each arm (action) is identified by an integer.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_arms&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_arms&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n_arms&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;
        &lt;span class="c1"&gt;# a dict of the mean action_value per arm, w/ each action_value sampled from a Gaussian&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;action_values&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_arms&lt;/span&gt;&lt;span class="p"&gt;))}&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;actions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;action_values&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;  &lt;span class="c1"&gt;# arms of the bandit&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__call__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Get reward from bandit for action&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;action_values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Implementation detail: the means per arm, stored in &lt;code&gt;self.action_values&lt;/code&gt;, are drawn from a Gaussian distribution upon&amp;nbsp;initialization).&lt;/p&gt;
&lt;p&gt;The agent doesn&amp;#8217;t know the true mean rewards per arm &amp;#8212; it only sees a sample reward when he takes the action of pulling on a particular bandit arm (&lt;code&gt;__call__&lt;/code&gt;).&lt;/p&gt;
&lt;h1&gt;Action, reward, update&amp;nbsp;strategy&lt;/h1&gt;
&lt;p&gt;For every action the agent takes, it gets a reward.  With each additional interaction with the bandit, the agent has a new data point it can use to update its strategy (whether indirectly, via an updated action-value estimate, or directly in the policy&amp;nbsp;gradient).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;BaseBanditAlgo&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ABC&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Base class for algorithms to maximize the rewards &lt;/span&gt;
&lt;span class="sd"&gt;    for the multiarmed bandit problem&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bandit&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Bandit&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bandit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bandit&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;timestep&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;

    &lt;span class="nd"&gt;@abstractmethod&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_select_action&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;pass&lt;/span&gt;

    &lt;span class="nd"&gt;@abstractmethod&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_update_for_action_and_reward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
         &lt;span class="k"&gt;pass&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;action&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_select_action&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bandit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_update_for_action_and_reward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__call__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_timesteps&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_timesteps&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;timestep&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Two types of strategies: value based and policy&amp;nbsp;based&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;value based - agents try to directly estimate the value of
   each action (and whose policies, i.e. probability of selecting an
   action, are therefore implicit, since the agent will want to choose
   the action that has the highest&amp;nbsp;value)&lt;/li&gt;
&lt;li&gt;policy based - agents don&amp;#8217;t try to directly estimate the value
   of an action and instead directly store the policy, i.e. the
   probability of taking each&amp;nbsp;action.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;An example of a &lt;strong&gt;value based&lt;/strong&gt; strategy / action-value method for the
bandit problem is the &lt;code&gt;EpsilonGreedy&lt;/code&gt; approach, which selects the
action associated with the highest estimated action-value with probability &lt;span class="math"&gt;\(1-\epsilon\)&lt;/span&gt;, but chooses a random arm
a fraction &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; of the time as part of its exploration&amp;nbsp;strategy.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;EpsilonGreedy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BaseEstimateActionValueAlgo&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Greedy algorithm that explores/samples from the non-greedy action some fraction, &lt;/span&gt;
&lt;span class="sd"&gt;    epsilon, of the time.&lt;/span&gt;

&lt;span class="sd"&gt;    - For a basic greedy algorithm, set epsilon = 0.&lt;/span&gt;
&lt;span class="sd"&gt;    - For optimistic intialization, set q_init &amp;gt; mu, the mean of the Gaussian from&lt;/span&gt;
&lt;span class="sd"&gt;      which the real values per bandit arm are sampled (default is 0).&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bandit&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Bandit&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bandit&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;epsilon&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_select_action&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="c1"&gt;# take random action&lt;/span&gt;
            &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;choice&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bandit&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;actions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="c1"&gt;# take greedy action&lt;/span&gt;
            &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;est_action_values&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;est_action_values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;(See end of post for additional action-value&amp;nbsp;methods.)&lt;/p&gt;
&lt;p&gt;An example of a &lt;strong&gt;policy based&lt;/strong&gt; strategy is the &lt;code&gt;GradientBandit&lt;/code&gt;
method, which stores its policy, the probability per action in
&lt;code&gt;self.preferences&lt;/code&gt;.  It learns these preferences by doing stochastic
gradient ascent along the preferences in the gradient of the expected
reward in &lt;code&gt;_update_for_action_and_reward&lt;/code&gt; (see [1] for&amp;nbsp;derivation).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;GradientBandit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BaseBanditAlgo&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Algorithm that does not try to estimate action values directly and, instead, tries to learn&lt;/span&gt;
&lt;span class="sd"&gt;    a preference for each action (equivalent to stochastic gradient ascent along gradient in expected&lt;/span&gt;
&lt;span class="sd"&gt;    reward over preferences).&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bandit&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Bandit&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bandit&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;  &lt;span class="c1"&gt;# step-size&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reward_baseline_avg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;preferences&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;action&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;bandit&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;actions&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_calc_probs_from_preferences&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_calc_probs_from_preferences&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Probabilities per action follow a Boltzmann distribution over the preferences &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class="n"&gt;exp_preferences_for_action&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;preferences&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;()}&lt;/span&gt;
        &lt;span class="n"&gt;partition_fxn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;exp_preferences_for_action&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;probabilities_for_action&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;OrderedDict&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;partition_fxn&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; 
                                                     &lt;span class="n"&gt;exp_preferences_for_action&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;()})&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_select_action&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;choice&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;probabilities_for_action&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;()),&lt;/span&gt; 
                                &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;probabilities_for_action&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;()))&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_update_for_action_and_reward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Update preferences&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class="n"&gt;reward_diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reward_baseline_avg&lt;/span&gt;

        &lt;span class="c1"&gt;# can we combine these updates into single expression using kronecker delta?&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;preferences&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;reward_diff&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;probabilities_for_action&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bandit&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;actions&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="k"&gt;continue&lt;/span&gt;
            &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;preferences&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;reward_diff&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;probabilities_for_action&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reward_baseline_avg&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;timestep&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;reward_diff&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_calc_probs_from_preferences&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Extra: Total rewards for different bandit&amp;nbsp;algorithms&lt;/h1&gt;
&lt;p&gt;We have discussed a bunch of different bandit algorithms, but haven&amp;#8217;t seen what rewards they yield in&amp;nbsp;practice!&lt;/p&gt;
&lt;p&gt;In this
&lt;a href="https://github.com/frangipane/reinforcement-learning/blob/master/00-Introduction/multiarmed_bandits.ipynb"&gt;Jupyter notebook&lt;/a&gt;,
we run the algorithms through a range of values for their parameters
to compare their cumulative rewards across 1000 timesteps (also
averaged across many trials of different bandits to smooth things
out).  In the end, we arrive at a plot of the parameter study, that
reproduces Figure 2.6 in&amp;nbsp;[1].&lt;/p&gt;
&lt;p&gt;&lt;img alt="![parameter study]({static}/images/reproduce_multiarmed_bandit_parameter_study.png)" src="https://efavdb.com/images/reproduce_multiarmed_bandit_parameter_study.png"&gt;&lt;/p&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;p&gt;[1] Sutton and Barto - &lt;a href="http://incompleteideas.net/book/RLbook2018.pdf"&gt;Reinforcement Learning: An Introduction (2nd&amp;nbsp;Edition)&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Machine learning"></category><category term="reinforcement learning"></category><category term="machine learning"></category></entry><entry><title>Introduction to OpenAI Scholars 2020</title><link href="https://efavdb.com/openai-scholars-intro" rel="alternate"></link><published>2020-02-14T09:00:00-08:00</published><updated>2020-02-14T09:00:00-08:00</updated><author><name>Cathy Yeh</name></author><id>tag:efavdb.com,2020-02-14:/openai-scholars-intro</id><summary type="html">&lt;p&gt;Two weeks ago, I started at the &lt;a href="https://openai.com/blog/openai-scholars-spring-2020/"&gt;OpenAI Scholars&lt;/a&gt; program, which provides the opportunity to study and work full time on a project in an area of deep learning over 4 months.  I’m having a blast!  It’s been a joy focusing 100% on learning and challenging myself in …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Two weeks ago, I started at the &lt;a href="https://openai.com/blog/openai-scholars-spring-2020/"&gt;OpenAI Scholars&lt;/a&gt; program, which provides the opportunity to study and work full time on a project in an area of deep learning over 4 months.  I’m having a blast!  It’s been a joy focusing 100% on learning and challenging myself in an atmosphere full of friendly intellectual energy and&amp;nbsp;drive.&lt;/p&gt;
&lt;p&gt;My mentor is Jerry Tworek, an OpenAI research scientist who works on reinforcement learning (&lt;span class="caps"&gt;RL&lt;/span&gt;) in robotics, and I’ve also chosen to focus on &lt;span class="caps"&gt;RL&lt;/span&gt; during the program.  I constructed a &lt;a href="https://docs.google.com/document/d/1MlM5bxMqqiUIig5I6Y28fegvbqokjuvS2llVd2dIIRE/edit?usp=sharing"&gt;syllabus&lt;/a&gt; that will definitely evolve over time, but I’ll try to keep it up-to-date to serve as a useful record for myself and a guide for others who might be interested in a similar course of&amp;nbsp;study.&lt;/p&gt;
&lt;p&gt;Some casual notes from the last two&amp;nbsp;weeks:&lt;/p&gt;
&lt;p&gt;(1) There are manifold benefits to working on a topic that is in my mentor’s area of expertise.  For example, I’ve already benefited from Jerry’s intuition around hyperparameter tuning and debugging &lt;span class="caps"&gt;RL&lt;/span&gt;-specific problems, as well as his guidance on major concepts I should focus on in my first month, namely, model-free &lt;span class="caps"&gt;RL&lt;/span&gt; divided broadly into Q-Learning and Policy&amp;nbsp;Gradients.&lt;/p&gt;
&lt;p&gt;(2) &lt;strong&gt;Weights &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; Biases&lt;/strong&gt; at &lt;a href="wandb.com"&gt;wandb.com&lt;/a&gt; is a fantastic free tool for tracking machine learning experiments that many people use at OpenAI.  It was staggeringly simple to integrate wandb with my training script &amp;#8212; both for local runs and in the cloud!  Just ~4 extra lines of code, and logged metrics automagically appear in my wandb dashboard, with auto-generated plots grouped by experiment name, saved artifacts,&amp;nbsp;etc.&lt;/p&gt;
&lt;p&gt;Here&amp;#8217;s an example of a &lt;a href="https://app.wandb.ai/frangipane/dqn?workspace=user-frangipane"&gt;dashboard&lt;/a&gt; tracking experiments for my first attempt at implementing a deep &lt;span class="caps"&gt;RL&lt;/span&gt; algorithm from scratch (&lt;span class="caps"&gt;DQN&lt;/span&gt;, or Deep Q learning).  The script that is generating the experiments is still a work in progress, but you can see how few lines were required to integrate with wandb &lt;a href="https://github.com/frangipane/reinforcement-learning/blob/master/DQN/dqn.py"&gt;here&lt;/a&gt;.  Stay tuned for a blog post about &lt;span class="caps"&gt;DQN&lt;/span&gt; itself in the&amp;nbsp;future!&lt;/p&gt;
&lt;p&gt;(3) I&amp;#8217;ve found it very helpful to parallelize reading Sutton and Barto&amp;#8217;s &lt;a href="http://incompleteideas.net/book/RLbook2018.pdf"&gt;Reinforcement Learning: An Introduction&lt;/a&gt;, &lt;em&gt;the&lt;/em&gt; classic text on &lt;span class="caps"&gt;RL&lt;/span&gt;, with watching David Silver&amp;#8217;s pedagogical online &lt;a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html"&gt;lectures&lt;/a&gt;.  Silver&amp;#8217;s lectures follow the book closely for the first few chapters, then start condensing several chapters per lecture beginning around lecture 4 or 5 &amp;#8212; helpful since I&amp;#8217;m aiming to ramp up on &lt;span class="caps"&gt;RL&lt;/span&gt; over a short period of time!  Silver also supplements with insightful explanations and material that aren&amp;#8217;t covered in the book, e.g. insights about the convergence properties of some &lt;span class="caps"&gt;RL&lt;/span&gt;&amp;nbsp;algorithms.&lt;/p&gt;
&lt;p&gt;Note, Silver contributed to the work on Deep Q Learning applied to Atari that generated a lot of interest in deep &lt;span class="caps"&gt;RL&lt;/span&gt; beginning in 2013, leading to a &lt;a href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf"&gt;publication&lt;/a&gt; in Nature in 2015, so his lecture 6 on Value Function Approximation (&lt;a href="https://www.youtube.com/watch?v=UoPei5o4fps"&gt;video&lt;/a&gt;, &lt;a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/FA.pdf"&gt;slides&lt;/a&gt;) is a perfect accompaniment to reading the&amp;nbsp;paper.&lt;/p&gt;</content><category term="Misc"></category><category term="OpenAI"></category><category term="reinforcement learning"></category><category term="machine learning"></category><category term="deep learning"></category></entry><entry><title>Universal limiting mean return of CPPI investment portfolios</title><link href="https://efavdb.com/universal-mean-return-formula-of-the-cppi-investment-strategy" rel="alternate"></link><published>2019-12-09T16:14:00-08:00</published><updated>2019-12-09T16:14:00-08:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2019-12-09:/universal-mean-return-formula-of-the-cppi-investment-strategy</id><summary type="html">&lt;p&gt;&lt;span class="caps"&gt;CPPI&lt;/span&gt;* is a risk management tactic that can be applied to
any investment portfolio. The approach entails banking a percentage of
profits whenever a new all time high wealth is achieved, thereby
ensuring that a portfolio&amp;#8217;s drawdown never goes below some maximum
percentage. Here, I review &lt;span class="caps"&gt;CPPI&lt;/span&gt; and then …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;span class="caps"&gt;CPPI&lt;/span&gt;* is a risk management tactic that can be applied to
any investment portfolio. The approach entails banking a percentage of
profits whenever a new all time high wealth is achieved, thereby
ensuring that a portfolio&amp;#8217;s drawdown never goes below some maximum
percentage. Here, I review &lt;span class="caps"&gt;CPPI&lt;/span&gt; and then consider the mean growth rate
of a &lt;span class="caps"&gt;CPPI&lt;/span&gt; portfolio. I find that in a certain, common limit, this mean
growth is given by a universal formula,
(\ref{cppi_asymptotic_growth}) below. This universal result does not
depend in detail on the statistics of the investment in question, but
instead only on its mean return and standard deviation. I illustrate
the formula&amp;#8217;s accuracy with a simulation in&amp;nbsp;python.&lt;/p&gt;
&lt;p&gt;*&lt;span class="caps"&gt;CPPI&lt;/span&gt; = &amp;#8220;Constant Proportion Portfolio&amp;nbsp;Insurance&amp;#8221;&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The drawdown of an investment portfolio at a given date is equal to
the amount of money lost relative to its maximum held capital up to
that date. This is illustrated in the figure at right &amp;#8212; a portfolio
that once held $100 now holds only $90, so the drawdown is currently
$10. &lt;a href="https://efavdb.com/wp-content/uploads/2019/12/dd.png"&gt;&lt;img alt="dd" src="https://efavdb.com/wp-content/uploads/2019/12/dd.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="caps"&gt;CPPI&lt;/span&gt; is a method that can be applied to guarantee that the maximum
fractional drawdown is never more than some predetermined value &amp;#8212; the
idea is to simply squirrel away an appropriate portion of earnings
whenever we hit a new maximum account value, and only risk what&amp;#8217;s left
over from that point on. For example, to cap the max loss at 50%, one
should only risk 50% of the initial capital and then continue to bank
50% of any additional earnings whenever a new all time high is&amp;nbsp;reached.&lt;/p&gt;
&lt;p&gt;According to Wikipedia, the first person to study &lt;span class="caps"&gt;CPPI&lt;/span&gt; was Perold, who derived the statistical properties of a &lt;span class="caps"&gt;CPPI&lt;/span&gt; portfolio&amp;#8217;s value at time &lt;span class="math"&gt;\(t\)&lt;/span&gt;, assuming the underlying stochastic investment follows a Wiener process. I was introduced to the &lt;span class="caps"&gt;CPPI&lt;/span&gt; concept by the book &amp;#8220;Algorithmic Trading&amp;#8221; by Ernest Chan. This book implicitly poses the question of what the mean return is for general, discrete investment strategies. Here, I show that a universal formula applies in this case, valid at low to modest leverages and small unit investment Sharpe ratios &amp;#8212; this result is given in equation (\ref{cppi_asymptotic_growth})&amp;nbsp;below.&lt;/p&gt;
&lt;p&gt;The post proceeds as follows: In the next section, I define some notation and then write down the limiting result. In the following section, I give a numerical example in python. Finally, an appendix contains a derivation of the main result. This makes use of the universal limiting drawdown distribution result from my &lt;a href="http://efavdb.github.io/universal-drawdown-statistics-in-investing"&gt;prior post&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;span class="caps"&gt;CPPI&lt;/span&gt; formulation and universal mean growth&amp;nbsp;rate&lt;/h2&gt;
&lt;p&gt;In this section, I review the &lt;span class="caps"&gt;CPPI&lt;/span&gt; strategy and give the limiting mean return result. Consider a portfolio that at time &lt;span class="math"&gt;\(t\)&lt;/span&gt; has&amp;nbsp;value,
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}\tag{1}
W_t = S_t + \Gamma_t
\end{align}
$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(W\)&lt;/span&gt; is our total wealth, &lt;span class="math"&gt;\(S\)&lt;/span&gt; is the banked (safe) portion, and &lt;span class="math"&gt;\(\Gamma\)&lt;/span&gt; is the portion we are willing to bet or gamble. The savings is set so that each time we reach a new all time high wealth, &lt;span class="math"&gt;\(S\)&lt;/span&gt; is adjusted to be equal to a fraction &lt;span class="math"&gt;\(\Pi\)&lt;/span&gt; of the net wealth. When this is done, the value of &lt;span class="math"&gt;\(\Gamma\)&lt;/span&gt; must also be adjusted downwards &amp;#8212; some of the investment money is moved to savings. Before adjustment, the result of a bet moves &lt;span class="math"&gt;\(\Gamma\)&lt;/span&gt;&amp;nbsp;to
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\label{gamble_eom_cppi} \tag{2}
\tilde{\Gamma}_{t+1} \equiv \left (1 + f (g_{t} - 1)\right) \Gamma_t.
\end{align}&lt;/div&gt;
&lt;p&gt;
Here, the tilde at left indicates this is the value before any reallocation is applied &amp;#8212; in case we have reached a new high, &lt;span class="math"&gt;\(g_t\)&lt;/span&gt; is a stochastic investment outcome variable, and &lt;span class="math"&gt;\(f\)&lt;/span&gt; is our &amp;#8220;leverage&amp;#8221; &amp;#8212; a constant that encodes how heavily we bet on the game. I will assume all &lt;span class="math"&gt;\(g_i\)&lt;/span&gt; are independent random variables that are identically distributed with distribution &lt;span class="math"&gt;\(p(g)\)&lt;/span&gt;. I will assume that &lt;span class="math"&gt;\(f\)&lt;/span&gt; is a fixed value throughout time, and will&amp;nbsp;write
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align} \tag{3} \label{3}
f = \frac{1}{\phi} \frac{ \langle g -1 \rangle }{ \text{var}(g)}
\end{align}&lt;/div&gt;
&lt;p&gt;
This re-parameterization helps to make the math work out more nicely below. It is also motivated by the Kelly formula, which specifies the gambling leverage that maximizes wealth at long times (here, setting &lt;span class="math"&gt;\(\phi \to 1\)&lt;/span&gt; gives the Kelly&amp;nbsp;exposure).&lt;/p&gt;
&lt;p&gt;The equations above define the &lt;span class="caps"&gt;CPPI&lt;/span&gt; scheme. The main result of this post is that in the limit where the leverage is not too high, and the stochastic &lt;span class="math"&gt;\(g\)&lt;/span&gt; has small Sharpe ratio (mean return over standard deviation), the mean log wealth at time &lt;span class="math"&gt;\(t\)&lt;/span&gt;&amp;nbsp;satisfies
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align} \label{cppi_asymptotic_growth} \tag{4}
\frac{1}{t}\langle \log W_{t} - \log W_{0} \rangle \sim \frac{ \langle g -1 \rangle^2 }{ \text{var}(g)} (1-\Pi) \frac{(2 \phi -1)}{2 \phi ^2}.
\end{align}&lt;/div&gt;
&lt;p&gt;
This result can be used to estimate the mean growth of a portfolio to which &lt;span class="caps"&gt;CPPI&lt;/span&gt; is applied. Before illustrating its accuracy via a simulation below, I highlight a few points about the&amp;nbsp;result:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The fact that (\ref{cppi_asymptotic_growth}) is universal makes it very practical to apply to a real world investment: Rather than having to estimate the full distribution for &lt;span class="math"&gt;\(g\)&lt;/span&gt;, we need only estimate its mean and variance to get an estimate for the portfolio&amp;#8217;s long-time mean&amp;nbsp;return.&lt;/li&gt;
&lt;li&gt;The mean return (\ref{cppi_asymptotic_growth}) is equivalent to that found by Perold for Gaussian processes &amp;#8212; as expected, since the result is&amp;nbsp;&amp;#8220;universal&amp;#8221;.&lt;/li&gt;
&lt;li&gt;The maximum return at fixed &lt;span class="math"&gt;\(\Pi\)&lt;/span&gt; is again obtained at &lt;span class="math"&gt;\(\phi = 1\)&lt;/span&gt;, the Kelly&amp;nbsp;maximum.&lt;/li&gt;
&lt;li&gt;If &lt;span class="math"&gt;\(\phi = 1/2\)&lt;/span&gt;, we are at twice Kelly exposure and the mean return is zero &amp;#8212; this a well known result. At &lt;span class="math"&gt;\(\phi &amp;lt; 1/2\)&lt;/span&gt;, we are above twice Kelly and the return is&amp;nbsp;negative.&lt;/li&gt;
&lt;li&gt;The mean return is reduced by a factor &lt;span class="math"&gt;\((1-\Pi)\)&lt;/span&gt; &amp;#8212; the fraction of new high wealths we are exposing to loss. It is interesting that the result is not suppressed faster than this as we hold out more wealth, given that we have lower exposure after a loss than we would&amp;nbsp;otherwise.&lt;/li&gt;
&lt;li&gt;One can ask what &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; gives us the same mean return using &lt;span class="caps"&gt;CPPI&lt;/span&gt; as we would obtain at full exposure using &lt;span class="math"&gt;\(\phi_0\)&lt;/span&gt;. E.g., consider the case of &lt;span class="math"&gt;\(\Pi = 1/2\)&lt;/span&gt;, which corresponds to insuring that half of our wealth is protected from loss. Equating the mean gains of these two gives
    &lt;div class="math"&gt;\begin{align}
    \frac{1}{2} \frac{(2 \phi -1)}{2 \phi ^2} = \frac{(2 (2)-1)}{2 (2)^2} = \frac{3}{8}.
    \end{align}&lt;/div&gt;
    The two roots for &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; are plotted versus &lt;span class="math"&gt;\(\phi_0\)&lt;/span&gt; below. &lt;a href="https://efavdb.com/wp-content/uploads/2019/12/solution_half_safe.png"&gt;&lt;img alt="solution_half_safe" src="https://efavdb.com/wp-content/uploads/2019/12/solution_half_safe.png"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Notice that we can&amp;#8217;t find solutions for all &lt;span class="math"&gt;\(\phi_0\)&lt;/span&gt; &amp;#8212; it&amp;#8217;s not possible to match the mean return for high leverages at full exposure when we force protection of some of our&amp;nbsp;assets.&lt;/p&gt;
&lt;p&gt;We now turn to a simulation&amp;nbsp;example.&lt;/p&gt;
&lt;h2&gt;Python &lt;span class="caps"&gt;CPPI&lt;/span&gt;&amp;nbsp;simulation&lt;/h2&gt;
&lt;p&gt;In the code below, we consider a system where in each step we either &amp;#8220;win&amp;#8221; or &amp;#8220;lose&amp;#8221;: If we win, the money we risk grows by a factor of &lt;span class="math"&gt;\(f \times 1.02\)&lt;/span&gt;, and if we lose, it goes down by a factor of &lt;span class="math"&gt;\(f \times 1 / 1.02\)&lt;/span&gt;. We take the probability of winning to be &lt;span class="math"&gt;\(0.65\)&lt;/span&gt;. This game has a Sharpe ratio of &lt;span class="math"&gt;\(0.32\)&lt;/span&gt;, small enough that our approximation should work well. The code below carries out a simulated repeated investment game over 100 trials &amp;#8212; we hope that it is clear what is happening at each&amp;nbsp;step.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="c1"&gt;# investment definitions -- a random walk&lt;/span&gt;
&lt;span class="n"&gt;LIFT_ON_WIN&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.02&lt;/span&gt;
&lt;span class="n"&gt;LIFT_ON_LOSS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;LIFT_ON_WIN&lt;/span&gt;

&lt;span class="n"&gt;P_WIN&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.65&lt;/span&gt;
&lt;span class="n"&gt;P_LOSS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;P_WIN&lt;/span&gt;

&lt;span class="n"&gt;g_minus_1_bar&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;P_WIN&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;LIFT_ON_WIN&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;P_LOSS&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;LIFT_ON_LOSS&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;var_g&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;P_WIN&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;LIFT_ON_WIN&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;P_LOSS&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;LIFT_ON_LOSS&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;g_minus_1_bar&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="n"&gt;full_kelly&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;g_minus_1_bar&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;var_g&lt;/span&gt;

&lt;span class="n"&gt;sharpe&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;g_minus_1_bar&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;var_g&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Sharpe ratio of g unit bet: &lt;/span&gt;&lt;span class="si"&gt;%.4f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;sharpe&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;simulate_once&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;steps&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;initial_value&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;full_kelly&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;phi&lt;/span&gt;

    &lt;span class="n"&gt;current_nav&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;initial_value&lt;/span&gt;
    &lt;span class="n"&gt;current_max&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;current_nav&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;steps&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# update current max&lt;/span&gt;
        &lt;span class="n"&gt;current_max&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;current_nav&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;current_max&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# calculate current effective nav&lt;/span&gt;
        &lt;span class="n"&gt;current_drawdown&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;current_max&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;current_nav&lt;/span&gt;
        &lt;span class="n"&gt;gamma&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;current_max&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;current_drawdown&lt;/span&gt;

        &lt;span class="c1"&gt;# play round of investment game&lt;/span&gt;
        &lt;span class="n"&gt;dice_roll&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;win&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dice_roll&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;P_WIN&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;win&lt;/span&gt;
        &lt;span class="n"&gt;g&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LIFT_ON_WIN&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;win&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;LIFT_ON_LOSS&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;
        &lt;span class="n"&gt;nav_change&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gamma&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;g&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# update wealth&lt;/span&gt;
        &lt;span class="n"&gt;current_nav&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;nav_change&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;current_nav&lt;/span&gt;

&lt;span class="n"&gt;end_results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;

&lt;span class="c1"&gt;# kelly and cppi properties&lt;/span&gt;
&lt;span class="n"&gt;PHI&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;10.0&lt;/span&gt;
&lt;span class="n"&gt;PI&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;75&lt;/span&gt;
&lt;span class="n"&gt;STEPS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;
&lt;span class="n"&gt;TRIALS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;

&lt;span class="c1"&gt;# simulation loop&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;trial&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;TRIALS&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="n"&gt;end_nav&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;simulate_once&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;PHI&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;PI&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;steps&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;STEPS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;end_results&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;end_nav&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;theory&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;g_minus_1_bar&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;var_g&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;PI&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;PHI&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;PHI&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Experiment: &lt;/span&gt;&lt;span class="si"&gt;%2.5f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;end_results&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;STEPS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Theory: &lt;/span&gt;&lt;span class="si"&gt;%2.5f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;theory&lt;/span&gt;

&lt;span class="c1"&gt;# OUTPUT:&lt;/span&gt;
&lt;span class="c1"&gt;# Sharpe ratio of g unit bet: 0.3249&lt;/span&gt;
&lt;span class="c1"&gt;# Experiment: 0.00251&lt;/span&gt;
&lt;span class="c1"&gt;# Theory: 0.00251&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The last lines above show the output of our print statements. In particular, the last two lines show the mean growth rate observed over the 100 trials and the theoretical value (\ref{cppi_asymptotic_growth}) &amp;#8212; these agree to three decimal&amp;nbsp;places.&lt;/p&gt;
&lt;p&gt;Using a loop over &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; values, I used the code above to obtain the plot below of mean returns vs &lt;span class="math"&gt;\(\phi\)&lt;/span&gt;. This shows that the limiting result works quite well over most &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; &amp;#8212; though there is some systematic, modest discrepancy at small &lt;span class="math"&gt;\(\phi\)&lt;/span&gt;. This is expected as the quadratic expansion for log wealth used below starts to break down at high exposures. Nevertheless, the fit is qualitatively quite good at all &lt;span class="math"&gt;\(\phi\)&lt;/span&gt;. This suggests that our result (\ref{cppi_asymptotic_growth}) can be used for quick mean return forecasts for most practical, applied cases of &lt;span class="caps"&gt;CPPI&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2019/12/mean_cppi_growth_rate.png"&gt;&lt;img alt="mean_cppi_growth_rate" src="https://efavdb.com/wp-content/uploads/2019/12/mean_cppi_growth_rate.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Appendix: Derivation of mean&amp;nbsp;return&lt;/h2&gt;
&lt;p&gt;We give a rough sketch here of a proof of (\ref{cppi_asymptotic_growth}). Our aim is to quickly get the universal form in a relatively clean way. Note that these results may be new, or perhaps well known to finance theorists &amp;#8212; I&amp;#8217;m not&amp;nbsp;sure.&lt;/p&gt;
&lt;p&gt;To begin let us&amp;nbsp;define
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
\Gamma^*_t = \max_{t^{\prime} \leq t} \Gamma_t.
\end{align}&lt;/div&gt;
&lt;p&gt;
This is the maximum &lt;span class="math"&gt;\(\Gamma\)&lt;/span&gt; seen to date at time &lt;span class="math"&gt;\(t\)&lt;/span&gt;. Necessarily, this is the value of &lt;span class="math"&gt;\(\Gamma\)&lt;/span&gt; as of the most recent all time high preceeding &lt;span class="math"&gt;\(t\)&lt;/span&gt;. If &lt;span class="math"&gt;\(\Gamma_t &amp;lt; \Gamma^*_t\)&lt;/span&gt;, we say that we are in drawdown by value &lt;span class="math"&gt;\(\Gamma^*_t - \Gamma_t\)&lt;/span&gt;. At all times, we&amp;nbsp;have
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\nonumber
S_t &amp;amp;= \frac{\Pi}{1 - \Pi} \Gamma^*_t \\
&amp;amp;\equiv \rho \Gamma^*_t.
\end{align}&lt;/div&gt;
&lt;p&gt;
This result holds because the saved portion is &lt;span class="math"&gt;\(\Pi\)&lt;/span&gt; times the net wealth when we reach a new high and &lt;span class="math"&gt;\(\Gamma^*\)&lt;/span&gt; is what&amp;#8217;s left over, &lt;span class="math"&gt;\((1 - \Pi)\)&lt;/span&gt; times the net wealth at that&amp;nbsp;time.&lt;/p&gt;
&lt;p&gt;From the above definitions, our net wealth after a step is given&amp;nbsp;by
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\nonumber
W_{t+1} &amp;amp;= W_{t} \left ( 1 + f(g_t -1) \frac{\Gamma_t}{W_{t}} \right ) \\ \nonumber
&amp;amp;= W_{t} \left ( 1 + f(g_t -1) \frac{\Gamma_t}{S_t + \Gamma_t} \right ) \\
&amp;amp;= W_{t} \left ( 1 + f(g_t -1) \frac{\frac{\Gamma_t}{\Gamma^*_t}}{\rho+ \frac{\Gamma_t}{\Gamma^*_t}} \right )
\end{align}&lt;/div&gt;
&lt;p&gt;
Iterating and taking the logarithm we&amp;nbsp;obtain
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\nonumber \tag{A1} \label{A1}
\log W_{t} &amp;amp;= \log W_{0} + \sum_{i=0}^{t-1} \log \left ( 1 + f(g_i -1) \frac{\frac{\Gamma_i}{\Gamma^*_i}}{\rho+ \frac{\Gamma_i}{\Gamma^*_i}} \right ) \\
&amp;amp;\approx \log W_{0} + \sum_{i=0}^{t-1} f (g_i -1) \frac{\frac{\Gamma_i}{\Gamma^*_i}}{\rho+ \frac{\Gamma_i}{\Gamma^*_i}} - \frac{f^2}{2} \left ((g_i -1) \frac{\frac{\Gamma_i}{\Gamma^*_i}}{\rho+ \frac{\Gamma_i}{\Gamma^*_i}}\right)^2 + \ldots
\end{align}&lt;/div&gt;
&lt;p&gt;
The series expansion in the second line can be shown to converge quickly provided we have selected a leverage &lt;span class="math"&gt;\(f\)&lt;/span&gt; that always results in a small percentage change in our net wealth each step. Note that it is the breakdown of this expansion that causes the slight divergence at low &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; in our last plot&amp;nbsp;above.&lt;/p&gt;
&lt;p&gt;Our aim is to evaluate the average of the last equation. The first key point needed to do this is to note that at step &lt;span class="math"&gt;\(i\)&lt;/span&gt;, we have &lt;span class="math"&gt;\(g_i\)&lt;/span&gt; and &lt;span class="math"&gt;\(\Gamma_i / \Gamma_i^*\)&lt;/span&gt; independent (the outcome of the unit bet doesn&amp;#8217;t depend on how much we have to wager at this time). This allows us to factor the averages above into one over &lt;span class="math"&gt;\(g\)&lt;/span&gt; and over the &lt;span class="math"&gt;\(\Gamma_i / \Gamma_i^*\)&lt;/span&gt; distribution. The former is relatively easy to write down if we assume some properties for &lt;span class="math"&gt;\(f\)&lt;/span&gt; and &lt;span class="math"&gt;\(g\)&lt;/span&gt;. To proceed on the latter, we note&amp;nbsp;that
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\nonumber
\log \frac{\Gamma_i}{\Gamma^*_i} = \log \Gamma_i - \log \Gamma^*_i
\end{align}&lt;/div&gt;
&lt;p&gt;
is the drawdown of a random walk, with steps defined by (\ref{gamble_eom_cppi}). We have argued in our last post that the tail of the distribution of this drawdown distribution is such&amp;nbsp;that
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\nonumber
p( \log \Gamma_i - \log \Gamma^*_i = -k) \propto \exp(\alpha k).
\end{align}&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; is given in that post as an implicit function of the statistics of &lt;span class="math"&gt;\(g\)&lt;/span&gt;. This tail form will hold almost everywhere when the Sharpe ratio is small. We will assume this&amp;nbsp;here.&lt;/p&gt;
&lt;p&gt;Using the change of variables rule, we&amp;nbsp;get
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\nonumber
p\left(\frac{\Gamma_i}{\Gamma^*_i} = k\right) \sim \begin{cases}
\alpha k^{\alpha - 1} &amp;amp; \text{if } x \in (0, 1) \\
0 &amp;amp; \text{else}
\end{cases}
\end{align}&lt;/div&gt;
&lt;p&gt;
Again, this is an approximation that assumes we spend relatively little time within one jump from the current all time high &amp;#8212; a result that will hold in the small Sharpe ratio limit. With this result, we&amp;nbsp;obtain
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\nonumber \tag{A2} \label{A2}
\left \langle \frac{\frac{\Gamma_i}{\Gamma^*_i}}{\rho+ \frac{\Gamma_i}{\Gamma^*_i}} \right \rangle &amp;amp;\equiv \int_0^1 \frac{x}{\rho + x} \alpha x^{\alpha - 1} dx \\
&amp;amp;= \frac{\alpha \, _2F_1\left(1,\alpha +1;\alpha +2;-\frac{1}{\rho }\right)}{\rho (\alpha +1) }
\end{align}&lt;/div&gt;
&lt;p&gt;
Here, &lt;span class="math"&gt;\(_2F_1\)&lt;/span&gt; is the hypergeometric function.&amp;nbsp;Similarly,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\nonumber \tag{A3} \label{A3}
\left \langle \left( \frac{\frac{\Gamma_i}{\Gamma^*_i}}{\rho+ \frac{\Gamma_i}{\Gamma^*_i}} \right)^2 \right \rangle &amp;amp;\equiv \int_0^1 \left( \frac{x}{\rho + x} \right)^2 \alpha x^{\alpha - 1} dx \\
&amp;amp;= \frac{\alpha \left(\frac{\rho }{\rho +1}-\frac{(\alpha +1) \, _2F_1\left(1,\alpha
+2;\alpha +3;-\frac{1}{\rho }\right)}{\alpha +2}\right)}{\rho ^2}
\end{align}&lt;/div&gt;
&lt;p&gt;
Note that both of the last two lines go to one as &lt;span class="math"&gt;\(\rho \to 0\)&lt;/span&gt;, the limit where we invest our entire net worth and protect nothing. In this case, the growth equations are just those for a fully invested account. If you plug the last two results into the second line of (\ref{A1}), you get an expression for the mean&amp;nbsp;return.&lt;/p&gt;
&lt;p&gt;To get the above results, we assumed a small Sharpe ratio. Therefore, to simplify things, we can use the value of &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; that we derived in our last post that is valid in this limit. This was given by &lt;span class="math"&gt;\(\alpha \sim 2 \mu / \sigma^2\)&lt;/span&gt;, where &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; and &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; and mean and standard deviation of the random walk. We now evaluate these to get an expression for &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; in terms of the statistics of &lt;span class="math"&gt;\(g\)&lt;/span&gt;. First, we note that the mean drift of &lt;span class="math"&gt;\(\log \Gamma\)&lt;/span&gt; is given&amp;nbsp;by
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\nonumber
\mu &amp;amp;\equiv \langle \log(1 + f (g-1)) \rangle \ \nonumber
&amp;amp;\sim f \langle g -1 \rangle - \frac{f^2}{2} \langle (g-1)^2 \rangle \\
&amp;amp;\sim f \langle g -1 \rangle - \frac{f^2}{2} \text{var}(g).
\end{align}&lt;/div&gt;
&lt;p&gt;
The last line follows from the assumption that the Sharpe ratio for &lt;span class="math"&gt;\(g -1\)&lt;/span&gt; is small, so&amp;nbsp;that
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\nonumber
\langle (g-1)^2 \rangle &amp;amp;= \left ( \langle (g-1)^2 \rangle - \langle (g-1) \rangle^2 \right) + \langle (g-1) \rangle^2 \\
&amp;amp;= \text{var}(g) \left ( 1 + \frac{ \langle (g-1) \rangle^2}{\text{var}(g)} \right ).
\end{align}&lt;/div&gt;
&lt;p&gt;
Similarly, one can show&amp;nbsp;that
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\nonumber
\sigma^2 &amp;amp;\equiv \text{var} \log(1 + f (g-1)) \\
&amp;amp;\sim f^2 \text{var}(g).
\end{align}&lt;/div&gt;
&lt;p&gt;
This&amp;nbsp;gives
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\nonumber
\alpha &amp;amp;\sim 2 \frac{\mu}{\sigma^2} \\ \nonumber
&amp;amp;\sim 2 \frac{ f \langle g -1 \rangle - \frac{f^2}{2} \text{var}(g)}{ f^2 \text{var}(g)} \\
&amp;amp;= \frac{2}{f} \frac{ \langle g -1 \rangle }{ \text{var}(g)} - 1
\end{align}&lt;/div&gt;
&lt;p&gt;
The second term in the first line can be neglected because we require the change in value to be a small fraction of our net wealth. We anticipate applying the algorithm to values of &lt;span class="math"&gt;\(f\)&lt;/span&gt; of order &lt;span class="math"&gt;\(f \sim O( \frac{ \langle g -1 \rangle }{ \text{var}(g)})\)&lt;/span&gt;, so the above is &lt;span class="math"&gt;\(O(1)\)&lt;/span&gt;. If we plug these results into the limiting form for &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; and use (\ref{3}) for &lt;span class="math"&gt;\(f\)&lt;/span&gt;, we&amp;nbsp;get
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align} \tag{A4} \label{A4}
\alpha \sim 2 \phi -1.
\end{align}&lt;/div&gt;
&lt;p&gt;
We won&amp;#8217;t show the details, but if you plug (\ref{A2}), (\ref{A3}), and (\ref{A4}) into (\ref{A1}), this gives (\ref{cppi_asymptotic_growth}). To get there, you need to show that a big collapse of terms occurs in the two hypergeometric functions. This collapse can be derived using the series expansion for &lt;span class="math"&gt;\(_2F_1\)&lt;/span&gt; in about one page. The collapse of terms only occurs in the small Sharpe ratio limit, where &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; is given as above. We note that for some walks, an exponential form holds everywhere. In this case, the more general expression using &lt;span class="math"&gt;\(_2F_1\)&lt;/span&gt; applies even at high Sharpe ratios &amp;#8212; though we still require &lt;span class="math"&gt;\(f\)&lt;/span&gt;&amp;nbsp;small.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Finance"></category></entry><entry><title>Universal drawdown statistics in investing</title><link href="https://efavdb.com/universal-drawdown-statistics-in-investing" rel="alternate"></link><published>2019-12-05T15:46:00-08:00</published><updated>2019-12-05T15:46:00-08:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2019-12-05:/universal-drawdown-statistics-in-investing</id><summary type="html">&lt;p&gt;We consider the equilibrium drawdown distribution for a biased random walk &amp;#8212; in the context of a repeated investment game, the drawdown at a given time is how much has been lost relative to the maximum capital held up to that time. We show that in the tail, this is exponential …&lt;/p&gt;</summary><content type="html">&lt;p&gt;We consider the equilibrium drawdown distribution for a biased random walk &amp;#8212; in the context of a repeated investment game, the drawdown at a given time is how much has been lost relative to the maximum capital held up to that time. We show that in the tail, this is exponential. Further, when mean drift is small, this has an exponent that is universal in form, depending only on the mean and standard deviation of the step distribution. We give simulation examples in python consistent with the&amp;nbsp;results.&lt;/p&gt;
&lt;h2&gt;Introduction and main&amp;nbsp;results&lt;/h2&gt;
&lt;p&gt;In this post, we consider a topic of high interest to investors and gamblers alike &amp;#8212; the statistics of drawdown. This is the amount of money the investor has lost relative to their maximum held capital to date. &lt;a href="https://efavdb.com/wp-content/uploads/2019/12/dd.png"&gt;&lt;img alt="dd" src="https://efavdb.com/wp-content/uploads/2019/12/dd.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For example, if an investor once held $100, but now holds only $90, his drawdown is currently $10. We will provide some results that characterize how unlikely it is for the investor to have a large drawdown of $\(k\), given knowledge of the statistics of his&amp;nbsp;bets.&lt;/p&gt;
&lt;p&gt;We will take as our model system a biased random walk. The probability that at step &lt;span class="math"&gt;\(t\)&lt;/span&gt; the investment goes from &lt;span class="math"&gt;\(k^{\prime}\)&lt;/span&gt; to &lt;span class="math"&gt;\(k\)&lt;/span&gt; will be taken to be independent of time and given&amp;nbsp;by
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{1} \label{step_distribution}
p(k^{\prime} \to k) = \tau(k - k^{\prime}).
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
We will assume that this has a positive bias &lt;span class="math"&gt;\(\mu\)&lt;/span&gt;, so that on average the investor makes money. With this assumption, we show below that for &lt;span class="math"&gt;\(\vert k \vert\)&lt;/span&gt; more than a few step sizes, the drawdown distribution has an exponential&amp;nbsp;form,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{2} \label{exponential}
p(k) \propto \exp\left( - \alpha \vert k \vert \right)
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
where the decay constant &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;&amp;nbsp;satisfies
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{3} \label{dd_decay_eqn}
1 = \int_{-\infty}^{\infty} \exp\left( \alpha j \right) \tau(-j) dj.
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
The form (\ref{exponential}) holds for general distributions and (\ref{dd_decay_eqn}) provides the formula for obtaining &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; in this case. However, in the limit where the mean drift &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; in &lt;span class="math"&gt;\(\tau\)&lt;/span&gt; is small relative to its standard deviation, &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;, we show that the solution to (\ref{dd_decay_eqn}) has a universal form,&amp;nbsp;giving
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{4} \label{exponential_universal}
p(k) \propto \exp\left( - 2 \frac{\mu}{\sigma^2} \vert k \vert \right).
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Because it is difficult to find very high drift investments, this simple form should hold for most real world investments (under the assumption of a Markov process). It can be used to give one a sense of how much time they can expect to sit at a particular drawdown, given estimates for &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; and &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The results (\ref{exponential} - \ref{exponential_universal}) are the main results of this post. These may be new, but could also be well-known to finance theorists &amp;#8212; we are not sure. We illustrate their accuracy in the following section using a numerical example, and provide derivations in an&amp;nbsp;appendix.&lt;/p&gt;
&lt;h2&gt;Numerical examples in&amp;nbsp;python&lt;/h2&gt;
&lt;p&gt;Here, we will consider two different kinds of random walk &amp;#8212; one where the steps are always the same size, but there is bias in the forward direction, and the other where the steps are taken from a Gaussian or normal distribution. The code below carries out a simulated investing scenario over one million&amp;nbsp;steps.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;binary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;    Return either mu - 1 or mu + 1 with equal probability.&lt;/span&gt;
&lt;span class="sd"&gt;    Note unit std.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;choice&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;normal_random_step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;    Return a random unit normal with unit std.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;

&lt;span class="c1"&gt;# CONSTANTS&lt;/span&gt;
&lt;span class="n"&gt;TIME_STEPS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;
&lt;span class="n"&gt;MU&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;

&lt;span class="c1"&gt;# BINARY WALK&lt;/span&gt;
&lt;span class="n"&gt;position&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="n"&gt;max_position_to_date&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="n"&gt;drawdowns_binary&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;TIME_STEPS&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;position&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;STEP_FUNC&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;MU&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;max_position_to_date&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_position_to_date&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;position&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;drawdowns_binary&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_position_to_date&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;position&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# GAUSSIAN / NORMAL WALK&lt;/span&gt;
&lt;span class="n"&gt;STEP_FUNC&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;normal_random_step&lt;/span&gt;
&lt;span class="n"&gt;position&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="n"&gt;max_position_to_date&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="n"&gt;drawdowns_normal&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;TIME_STEPS&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;position&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;STEP_FUNC&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;MU&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;max_position_to_date&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_position_to_date&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;position&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;drawdowns_normal&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_position_to_date&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;position&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2019/12/dd_normal.png"&gt;&lt;img alt="dd_normal" src="https://efavdb.com/wp-content/uploads/2019/12/dd_normal.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You can see in the code that we have a loop over steps. At each step, we append to a list of observed drawdown values. A plot of the histogram of these values for the Normal case at &lt;span class="math"&gt;\(\mu = 0.1\)&lt;/span&gt; is shown at&amp;nbsp;right.&lt;/p&gt;
&lt;p&gt;To check whether our theoretical forms are accurate, it is useful to plot the cumulative distribution functions vs the theoretical forms &amp;#8212; the latter will again be exponential with the same &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; values as the probability distribution functions. It turns out that the exponent &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; that solves (\ref{dd_decay_eqn}) is always given by the universal form for a Gaussian. However, for the binary walker, we need to solve for this numerically in general. The following code snippet does&amp;nbsp;this.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy.optimize&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;fsolve&lt;/span&gt;

&lt;span class="c1"&gt;# Solving numerically for binary case.&lt;/span&gt;
&lt;span class="n"&gt;binary_alpha_func&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;MU&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cosh&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;alpha_initial_guess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;
&lt;span class="n"&gt;alpha_solution&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fsolve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;binary_alpha_func&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha_initial_guess&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;A plot of the function above and the solution when &lt;span class="math"&gt;\(\mu = 0.85\)&lt;/span&gt; is shown below. Note that there is always an unphysical solution at &lt;span class="math"&gt;\(\alpha =0\)&lt;/span&gt; &amp;#8212; this should be&amp;nbsp;ignored.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2019/12/binary_sol.png"&gt;&lt;img alt="binary_sol" src="https://efavdb.com/wp-content/uploads/2019/12/binary_sol.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Using the above results, I have plotted the empirical cdfs versus &lt;span class="math"&gt;\(k\)&lt;/span&gt; for both walk distributions. The values are shown below for &lt;span class="math"&gt;\(\mu = 0.1\)&lt;/span&gt; (left) and &lt;span class="math"&gt;\(\mu = 0.85\)&lt;/span&gt; (right). The slopes of the theoretical and numerical results are what should be compared as these give the value of &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;. Note that &lt;span class="math"&gt;\(\mu = 0.1\)&lt;/span&gt; is a small drift relative to the standard deviation (&lt;span class="math"&gt;\(\sigma = 1\)&lt;/span&gt;, here), but &lt;span class="math"&gt;\(\mu = 0.85\)&lt;/span&gt; is not. This is why at left the universal form gives us a good fit to the decay rates for both systems, but at right we need our numerical solution to (\ref{dd_decay_eqn}) to get the binary decay&amp;nbsp;rate.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2019/12/results.png"&gt;&lt;img alt="results" src="https://efavdb.com/wp-content/uploads/2019/12/results.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In conclusion, we have found that the exponential form of drawdown works quite well in these examples, with the theoretical results (\ref{exponential} - \ref{exponential_universal}) providing methods for identifying the exponents. In particular, the plot at left above illustrates the universality of form (\ref{exponential_universal}) &amp;#8212; it holds for all walks, provided we are in the small bias&amp;nbsp;limit.&lt;/p&gt;
&lt;h2&gt;Appendix:&amp;nbsp;Derivations&lt;/h2&gt;
&lt;p&gt;To derive the exponential form, we consider an integral equation for the drawdown probability &lt;span class="math"&gt;\(p\)&lt;/span&gt;. At equilibrium, we&amp;nbsp;have
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{A1}
p(k) = \int_{-\infty}^{0} p(k^{\prime}) T(k^{\prime}, k) dk^{\prime}.
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(T\)&lt;/span&gt; is the transition function for the drawdown process. In the tail, we can ignore the boundary at zero and this goes&amp;nbsp;to
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{A2}
p(k) = \int_{-\infty}^{\infty} p(k^{\prime}) \tau(k - k^{\prime}) dk^{\prime},
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
where we have taken the upper limit to infinity, assuming that the transition function has a finite length so that this is acceptable. We can solve this by positing an exponential solution of&amp;nbsp;form
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{A3}
p(k) \equiv A \exp\left(\alpha k \right).
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Plugging this into the above&amp;nbsp;gives
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \nonumber
A \exp\left(\alpha k \right) &amp;amp;=&amp;amp; \int_{-\infty}^{\infty} A \exp\left(\alpha k^{\prime} \right) \tau(k - k^{\prime}) dk^{\prime}\ \tag{A4}
&amp;amp;=&amp;amp; A \exp\left(\alpha k \right) \int_{-\infty}^{\infty} \exp\left( \alpha j \right) \tau(-j) dj
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Simplifying this gives&amp;nbsp;(\ref{exponential}).&lt;/p&gt;
&lt;p&gt;Now, to get the universal form, we make use of the cumulant expansion,&amp;nbsp;writing
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \nonumber
1 &amp;amp;=&amp;amp; \int_{-\infty}^{\infty} \exp\left( \alpha j \right) \tau(-j) dj \
&amp;amp;\equiv &amp;amp; \exp \left ( - \mu \alpha + \sigma^2 \frac{\alpha^2}{2} + \ldots \right) \tag{A5}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Provided the expansion converges quickly, we&amp;nbsp;obtain
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
- \mu \alpha + \sigma^2 \frac{\alpha^2}{2} + \ldots = 0 \tag{A6}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
giving
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \label{cppi_alpha_asymptotic} \tag{A7}
\alpha \sim 2 \frac{\mu}{\sigma^2}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
With this solution, the &lt;span class="math"&gt;\(k\)&lt;/span&gt;-th term in the cumulant expansion goes&amp;nbsp;like
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{A8}
\frac{2^k}{k!} \left( \frac{\mu}{\sigma^2} \right)^k O(\overline{x^k}) \sim \frac{2^k}{k!} \left( \frac{\mu}{\sigma} \right)^k
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
assuming the the jumps are constrained over some length scale proportional to &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;. We see that provided the drift to standard deviation is small, the series converges quickly and our approximation is universally good. Unless you&amp;#8217;re cursed with an unusually large drift ratio for a given move, this form should work&amp;nbsp;well.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Finance"></category></entry><entry><title>TimeMarker class for python</title><link href="https://efavdb.com/timemarker-class-for-python" rel="alternate"></link><published>2019-09-14T22:31:00-07:00</published><updated>2019-09-14T22:31:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2019-09-14:/timemarker-class-for-python</id><summary type="html">&lt;p&gt;We give a simple class for marking the time at different points in a code block and then printing out the time gaps between adjacent marked points. This is useful for identifying slow spots in&amp;nbsp;code.&lt;/p&gt;
&lt;h2&gt;The TimeMarker&amp;nbsp;class&lt;/h2&gt;
&lt;p&gt;In the past, whenever I needed to speed up a block …&lt;/p&gt;</summary><content type="html">&lt;p&gt;We give a simple class for marking the time at different points in a code block and then printing out the time gaps between adjacent marked points. This is useful for identifying slow spots in&amp;nbsp;code.&lt;/p&gt;
&lt;h2&gt;The TimeMarker&amp;nbsp;class&lt;/h2&gt;
&lt;p&gt;In the past, whenever I needed to speed up a block of python code, the first thing I would do was import the time package, then manually insert a set of lines of the form &lt;code&gt;t1 = time.time()&lt;/code&gt;, &lt;code&gt;t2 = time.time()&lt;/code&gt;, etc. Then at the end, &lt;code&gt;print t2 - t1, t3 -t2, ...&lt;/code&gt;, etc. This works reasonably well, but I found it annoying and time consuming to have to save each time point to a different variable name. In particular, this prevented quick copy and paste of the time marker line. I finally thought to fix it this evening: Behold the &lt;code&gt;TimeMarker&lt;/code&gt; class, which solves this problem for&amp;nbsp;me:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;time&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;TimeMarker&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;  
    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;markers&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;mark&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;  
    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;markers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;print_markers&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;  
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;pair&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;markers&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;markers&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]):&lt;/span&gt;  
    &lt;span class="nb"&gt;print&lt;/span&gt; &lt;span class="n"&gt;pair&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;pair&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here is a simple code&amp;nbsp;example:  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;tm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TimeMarker&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;tm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mark&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  
&lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  
&lt;span class="n"&gt;tm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mark&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  
&lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  
&lt;span class="n"&gt;tm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mark&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;tm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;print_markers&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  
&lt;span class="c1"&gt;# (output...)  &lt;/span&gt;
&lt;span class="c1"&gt;# 7.9870223999e-05  &lt;/span&gt;
&lt;span class="c1"&gt;# 0.0279731750488  &lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The key here is that I can quickly paste the &lt;code&gt;tm.mark()&lt;/code&gt; repeatedly throughout my code and quickly check where the slow part&amp;nbsp;sits.&lt;/p&gt;</content><category term="Programming"></category><category term="programming"></category><category term="python"></category><category term="tools"></category></entry><entry><title>Backpropagation in neural networks</title><link href="https://efavdb.com/backpropagation-in-neural-networks" rel="alternate"></link><published>2019-07-18T23:35:00-07:00</published><updated>2019-07-18T23:35:00-07:00</updated><author><name>Cathy Yeh</name></author><id>tag:efavdb.com,2019-07-18:/backpropagation-in-neural-networks</id><summary type="html">&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;We give a short introduction to neural networks and the backpropagation algorithm for training neural networks. Our overview is brief because we assume familiarity with partial derivatives, the chain rule, and matrix&amp;nbsp;multiplication.&lt;/p&gt;
&lt;p&gt;We also hope this post will be a quick reference for those already familiar with the …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;We give a short introduction to neural networks and the backpropagation algorithm for training neural networks. Our overview is brief because we assume familiarity with partial derivatives, the chain rule, and matrix&amp;nbsp;multiplication.&lt;/p&gt;
&lt;p&gt;We also hope this post will be a quick reference for those already familiar with the notation used by Andrew Ng in his course on &lt;a href="https://www.coursera.org/learn/neural-networks-deep-learning/"&gt;&amp;#8220;Neural Networks and Deep Learning&amp;#8221;&lt;/a&gt;, the first in the deeplearning.ai series on Coursera. That course provides but doesn&amp;#8217;t derive the vectorized form of the backpropagation equations, so we hope to fill in that small gap while using the same&amp;nbsp;notation.&lt;/p&gt;
&lt;h2&gt;Introduction: neural&amp;nbsp;networks&lt;/h2&gt;
&lt;h3&gt;A single neuron acting on a single training&amp;nbsp;example&lt;/h3&gt;
&lt;p&gt;&lt;img alt="single neuron" src="https://efavdb.com/wp-content/uploads/2019/07/single_neuron-e1563431237482.png"&gt;&lt;/p&gt;
&lt;p&gt;The basic building block of a neural network is the composition of a nonlinear function (like a &lt;a href="https://en.wikipedia.org/wiki/Sigmoid_function"&gt;sigmoid&lt;/a&gt;, &lt;a href="http://mathworld.wolfram.com/HyperbolicTangent.html"&gt;tanh&lt;/a&gt;, or &lt;a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)"&gt;ReLU&lt;/a&gt;) &lt;span class="math"&gt;\(g(z)\)&lt;/span&gt;&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \nonumber
a^{[l]} = g(z^{[l]})
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;with a linear function acting on a (multidimensional) input, &lt;span class="math"&gt;\(a\)&lt;/span&gt;.
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \nonumber
z^{[l]} = w^{[l]T} a^{[l-1]} + b^{[l]}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;These building blocks, i.e. &amp;#8220;nodes&amp;#8221; or &amp;#8220;neurons&amp;#8221; of the neural network, are arranged in layers, with the layer denoted by superscript square brackets, e.g. &lt;span class="math"&gt;\([l]\)&lt;/span&gt; for the &lt;span class="math"&gt;\(l\)&lt;/span&gt;th layer. &lt;span class="math"&gt;\(n_l\)&lt;/span&gt; denotes the number of neurons in layer &lt;span class="math"&gt;\(l\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;Forward&amp;nbsp;propagation&lt;/h3&gt;
&lt;p&gt;Forward propagation is the computation of the multiple linear and nonlinear transformations of the neural network on the input data. We can rewrite the above equations in vectorized form to handle multiple training examples and neurons per layer&amp;nbsp;as&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{1} \label{1}
A^{[l]} = g(Z^{[l]})
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;with a linear function acting on a (multidimensional) input, &lt;span class="math"&gt;\(A\)&lt;/span&gt;.
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{2} \label{2}
Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;The outputs or activations, &lt;span class="math"&gt;\(A^{[l-1]}\)&lt;/span&gt;, of the previous layer serve as inputs for the linear functions, &lt;span class="math"&gt;\(z^{[l]}\)&lt;/span&gt;. If &lt;span class="math"&gt;\(n_l\)&lt;/span&gt; denotes the number of neurons in layer &lt;span class="math"&gt;\(l\)&lt;/span&gt;, and &lt;span class="math"&gt;\(m\)&lt;/span&gt; denotes the number of training examples in one (mini)batch pass through the neural network, then the dimensions of these matrices&amp;nbsp;are:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Variable&lt;/th&gt;
&lt;th&gt;Dimensions&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(A^{[l]}\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;(&lt;span class="math"&gt;\(n_l\)&lt;/span&gt;, &lt;span class="math"&gt;\(m\)&lt;/span&gt;)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(Z^{[l]}\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;(&lt;span class="math"&gt;\(n_l\)&lt;/span&gt;, &lt;span class="math"&gt;\(m\)&lt;/span&gt;)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(W^{[l]}\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;(&lt;span class="math"&gt;\(n_l\)&lt;/span&gt;, &lt;span class="math"&gt;\(n_{l-1}\)&lt;/span&gt;)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^{[l]}\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;(&lt;span class="math"&gt;\(n_l\)&lt;/span&gt;, 1)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;For example, this neural network consists of only a single hidden layer with 3 neurons in layer&amp;nbsp;1.&lt;/p&gt;
&lt;p&gt;&lt;img alt="neural network" src="https://efavdb.com/wp-content/uploads/2019/07/2layer_nn-e1563432145388.png"&gt;&lt;/p&gt;
&lt;p&gt;The matrix &lt;span class="math"&gt;\(W^{[1]}\)&lt;/span&gt; has dimensions (3, 2) because there are 3 neurons in layer 1 and 2 inputs from the previous layer (in this example, the inputs are the raw data, &lt;span class="math"&gt;\(\vec{x} = (x_1, x_2)\)&lt;/span&gt;). Each row of &lt;span class="math"&gt;\(W^{[1]}\)&lt;/span&gt; corresponds to a vector of weights for a neuron in layer&amp;nbsp;1.&lt;/p&gt;
&lt;p&gt;&lt;img alt="weights matrix" src="https://efavdb.com/wp-content/uploads/2016/06/weights_matrix-e1563432287786.png"&gt;&lt;/p&gt;
&lt;p&gt;The final output of the neural network is a prediction in the last layer &lt;span class="math"&gt;\(L\)&lt;/span&gt;, and the closeness of the prediction &lt;span class="math"&gt;\(A^{[L](i)}\)&lt;/span&gt; to the true label &lt;span class="math"&gt;\(y^{(i)}\)&lt;/span&gt; for training example &lt;span class="math"&gt;\(i\)&lt;/span&gt; is quantified by a loss function &lt;span class="math"&gt;\(\mathcal{L}(y^{(i)}, A^{[L](i)})\)&lt;/span&gt;, where superscript &lt;span class="math"&gt;\((i)\)&lt;/span&gt; denotes the &lt;span class="math"&gt;\(i\)&lt;/span&gt;th training example. For classification, the typical choice for &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; is the &lt;a href="https://en.wikipedia.org/wiki/Cross_entropy"&gt;cross-entropy loss&lt;/a&gt; (log&amp;nbsp;loss).&lt;/p&gt;
&lt;p&gt;The cost &lt;span class="math"&gt;\(J\)&lt;/span&gt; is the average loss over all &lt;span class="math"&gt;\(m\)&lt;/span&gt; training examples in the&amp;nbsp;dataset.&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{3} \label{3}
J = \frac{1}{m} \sum_{i=1}^m \mathcal{L}(y^{(i)}, A^{[L](i)})
\end{eqnarray}&lt;/div&gt;
&lt;h3&gt;Minimizing the cost with gradient&amp;nbsp;descent&lt;/h3&gt;
&lt;p&gt;The task of training a neural network is to find the set of parameters &lt;span class="math"&gt;\(W\)&lt;/span&gt; and &lt;span class="math"&gt;\(b\)&lt;/span&gt; (with different &lt;span class="math"&gt;\(W\)&lt;/span&gt; and &lt;span class="math"&gt;\(b\)&lt;/span&gt; for different nodes in the network) that will give us the best predictions, i.e. minimize the cost&amp;nbsp;(\ref{3}).&lt;/p&gt;
&lt;p&gt;Gradient descent is the workhorse that we employ for this optimization problem. We randomly initialize the parameters &lt;span class="math"&gt;\(W\)&lt;/span&gt; and &lt;span class="math"&gt;\(b\)&lt;/span&gt; for each node, then iteratively update the parameters by moving them in the direction that is opposite to the gradient of the&amp;nbsp;cost.&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \nonumber
W_\text{new} &amp;amp;=&amp;amp; W_\text{previous} - \alpha \frac{\partial J}{\partial W} \\
b_\text{new} &amp;amp;=&amp;amp; b_\text{previous} - \alpha \frac{\partial J}{\partial b}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; is the learning rate, a hyperparameter that needs to be tuned during the training process. The gradient of the cost is calculated by the backpropagation&amp;nbsp;algorithm.&lt;/p&gt;
&lt;h2&gt;Backpropagation&amp;nbsp;equations&lt;/h2&gt;
&lt;p&gt;These are the vectorized backpropagation (&lt;span class="caps"&gt;BP&lt;/span&gt;) equations which we wish to&amp;nbsp;derive:&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \nonumber
dW^{[l]} &amp;amp;\equiv&amp;amp; \frac{\partial J}{\partial W^{[l]}} = \frac{1}{m} dZ^{[l]}A^{[l-1]T} \tag{BP1} \label{BP1} \\
db^{[l]} &amp;amp;\equiv&amp;amp; \frac{\partial J}{\partial b^{[l]}} = \frac{1}{m} \sum_{i=1}^m dZ^{[l](i)} \tag{BP2} \label{BP2} \\
dA^{[l-1]} &amp;amp;\equiv&amp;amp; \frac{\partial \mathcal{L}}{\partial A^{[l-1]}} = W^{[l]T}dZ^{[l]} \tag{BP3} \label{BP3} \\
dZ^{[l]} &amp;amp;\equiv&amp;amp; \frac{\partial \mathcal{L}}{\partial Z^{[l]}} = dA^{[l]} * g'(Z^{[l]}) \tag{BP4} \label{BP4}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
The &lt;span class="math"&gt;\(*\)&lt;/span&gt; in the last line denotes element-wise&amp;nbsp;multiplication.&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(W\)&lt;/span&gt; and &lt;span class="math"&gt;\(b\)&lt;/span&gt; are the parameters we want to learn (update), but the &lt;span class="caps"&gt;BP&lt;/span&gt; equations include two additional expressions for the partial derivative of the loss in terms of linear and nonlinear activations per training example since they are intermediate terms that appear in the calculation of &lt;span class="math"&gt;\(dW\)&lt;/span&gt; and &lt;span class="math"&gt;\(db\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;Chain&amp;nbsp;rule&lt;/h3&gt;
&lt;p&gt;We&amp;#8217;ll need the chain rule for &lt;a href="https://en.wikipedia.org/wiki/Total_derivative"&gt;total derivatives&lt;/a&gt;, which describes how the change in a function &lt;span class="math"&gt;\(f\)&lt;/span&gt; with respect to a variable &lt;span class="math"&gt;\(x\)&lt;/span&gt; can be calculated as a sum over the contributions from intermediate functions &lt;span class="math"&gt;\(u_i\)&lt;/span&gt; that depend on &lt;span class="math"&gt;\(x\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \nonumber
\frac{\partial f(u_1, u_2, ..., u_k)}{\partial x} = \sum_{i}^k \frac{\partial f}{\partial u_i} \frac{\partial u_i}{\partial x}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
where the &lt;span class="math"&gt;\(u_i\)&lt;/span&gt; are functions of &lt;span class="math"&gt;\(x\)&lt;/span&gt;. This expression reduces to the single variable chain rule when only one &lt;span class="math"&gt;\(u_i\)&lt;/span&gt; is a function of &lt;span class="math"&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The gradients for every node can be calculated in a single backward pass through the network, starting with the last layer and working backwards, towards the input layer. As we work backwards, we cache the values of &lt;span class="math"&gt;\(dZ\)&lt;/span&gt; and &lt;span class="math"&gt;\(dA\)&lt;/span&gt; from previous calculations, which are then used to compute the derivative for variables that are further upstream in the computation graph. The dependency of the derivatives of upstream variables on downstream variables, i.e. cached derivatives, is manifested in the &lt;span class="math"&gt;\(\frac{\partial f}{\partial u_i}\)&lt;/span&gt; term in the chain rule. (Backpropagation is a dynamic programming&amp;nbsp;algorithm!)&lt;/p&gt;
&lt;h3&gt;The chain rule applied to&amp;nbsp;backpropagation&lt;/h3&gt;
&lt;p&gt;In this section, we apply the chain rule to derive the vectorized form of equations &lt;span class="caps"&gt;BP&lt;/span&gt;(1-4). Without loss of generality, we&amp;#8217;ll index an element of the matrix or vector on the left hand side of &lt;span class="caps"&gt;BP&lt;/span&gt;(1-4); the notation for applying the chain rule is therefore straightforward because the derivatives are just with respect to&amp;nbsp;scalars.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;BP1&lt;/span&gt;&lt;/strong&gt;
The partial derivative of the cost with respect to the &lt;span class="math"&gt;\(s\)&lt;/span&gt;th component (corresponding to the &lt;span class="math"&gt;\(s\)&lt;/span&gt;th input) of &lt;span class="math"&gt;\(\vec{w}\)&lt;/span&gt; in the &lt;span class="math"&gt;\(r\)&lt;/span&gt;th node in layer &lt;span class="math"&gt;\(l\)&lt;/span&gt;&amp;nbsp;is:&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
dW^{[l]}_{rs} &amp;amp;\equiv&amp;amp; \frac{\partial J}{\partial W^{[l]}_{rs}} \\
&amp;amp;=&amp;amp; \frac{1}{m} \sum_{i}^m \frac{\partial \mathcal{L}}{\partial W^{[l]}_{rs}} \\
&amp;amp;=&amp;amp; \frac{1}{m} \sum_{i}^m \frac{\partial \mathcal{L}}{\partial z^{[l]}_{ri}} \frac{\partial z^{[l]}_{ri}}{\partial W^{[l]}_{rs}} \tag{4} \label{4}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
The last line is due to the chain&amp;nbsp;rule.&lt;/p&gt;
&lt;p&gt;The first term in (\ref{4}) is &lt;span class="math"&gt;\(dZ^{[l]}_{ri}\)&lt;/span&gt; by definition (\ref{&lt;span class="caps"&gt;BP4&lt;/span&gt;}). We can simplify the second term of (\ref{4}) using the definition of the linear function (\ref{2}), which we rewrite below explicitly for the &lt;span class="math"&gt;\(i\)&lt;/span&gt;th training example in the &lt;span class="math"&gt;\(r\)&lt;/span&gt;th node in the &lt;span class="math"&gt;\(l\)&lt;/span&gt;th layer in order to be able to more easily keep track of indices when we take derivatives of the linear&amp;nbsp;function:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{5} \label{5}
Z^{[l]}_{ri} = \sum_j^{n_{l-1}} W^{[l]}_{rj} A^{[l-1]}_{ji} + b^{[l]}_r
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(n_{l-1}\)&lt;/span&gt; denotes the number of nodes in layer &lt;span class="math"&gt;\(l-1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Therefore,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
dW^{[l]}_{rs} &amp;amp;=&amp;amp; \frac{1}{m} \sum_{i}^m dZ^{[l]}_{ri} A^{[l-1]}_{si} \\
&amp;amp;=&amp;amp; \frac{1}{m} \sum_{i}^m dZ^{[l]}_{ri} A^{[l-1]T}_{is} \\
&amp;amp;=&amp;amp; \frac{1}{m} \left( dZ^{[l]} A^{[l-1]T} \right)_{rs}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;BP2&lt;/span&gt;&lt;/strong&gt;
The partial derivative of the cost with respect to &lt;span class="math"&gt;\(b\)&lt;/span&gt; in the &lt;span class="math"&gt;\(r\)&lt;/span&gt;th node in layer &lt;span class="math"&gt;\(l\)&lt;/span&gt;&amp;nbsp;is:&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
db^{[l]}_r &amp;amp;\equiv&amp;amp; \frac{\partial J}{\partial b^{[l]}_r} \\
&amp;amp;=&amp;amp; \frac{1}{m} \sum_{i}^m \frac{\partial \mathcal{L}}{\partial b^{[l]}_r} \\
&amp;amp;=&amp;amp; \frac{1}{m} \sum_{i}^m \frac{\partial \mathcal{L}}{\partial z^{[l]}_{ri}} \frac{\partial z^{[l]}_{ri}}{\partial b^{[l]}_r} \tag{6} \label{6} \\
&amp;amp;=&amp;amp; \frac{1}{m} \sum_{i}^m dZ^{[l]}_{ri}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
(\ref{6}) is due to the chain rule. The first term in (\ref{6}) is &lt;span class="math"&gt;\(dZ^{[l]}_{ri}\)&lt;/span&gt; by definition (\ref{&lt;span class="caps"&gt;BP4&lt;/span&gt;}). The second term of (\ref{6}) simplifies to &lt;span class="math"&gt;\(\partial z^{[l]}_{ri} / \partial b^{[l]}_r = 1\)&lt;/span&gt; from&amp;nbsp;(\ref{5}).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;BP3&lt;/span&gt;&lt;/strong&gt;
The partial derivative of the loss for the &lt;span class="math"&gt;\(i\)&lt;/span&gt;th example with respect to the nonlinear activation in the &lt;span class="math"&gt;\(r\)&lt;/span&gt;th node in layer &lt;span class="math"&gt;\(l-1\)&lt;/span&gt;&amp;nbsp;is:&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
dA^{[l-1]}_{ri} &amp;amp;\equiv&amp;amp; \frac{\partial \mathcal{L}}{\partial A^{[l-1]}_{ri}} \\
&amp;amp;=&amp;amp; \sum_{k=1}^{n_l} \frac{\partial \mathcal{L}}{\partial Z^{[l]}_{ki}} \frac{\partial Z^{[l]}_{ki}}{\partial A^{[l-1]}_{ri}} \tag{7} \label{7} \\
&amp;amp;=&amp;amp; \sum_{k=1}^{n_l} dZ^{[l]}_{ki} W^{[l]}_{kr} \tag{8} \label{8} \\
&amp;amp;=&amp;amp; \sum_{k=1}^{n_l} W^{[l]T}_{rk} dZ^{[l]}_{ki} \\
&amp;amp;=&amp;amp; \left( W^{[l]T} dZ^{[l]} \right)_{ri}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
The application of the chain rule (\ref{7}) includes a sum over the nodes in layer &lt;span class="math"&gt;\(l\)&lt;/span&gt; whose linear functions take &lt;span class="math"&gt;\(A^{[l-1]}_{ri}\)&lt;/span&gt; as an input, assuming the nodes between layers &lt;span class="math"&gt;\(l-1\)&lt;/span&gt; and &lt;span class="math"&gt;\(l\)&lt;/span&gt; are fully-connected. The first term in (\ref{8}) is by definition &lt;span class="math"&gt;\(dZ\)&lt;/span&gt; (\ref{&lt;span class="caps"&gt;BP4&lt;/span&gt;}); from (\ref{5}), the second term in (\ref{8}) evaluates to &lt;span class="math"&gt;\(\partial Z^{[l]}_{ki} / \partial A^{[l-1]}_{ri} = W^{[l]}_{kr}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;BP4&lt;/span&gt;&lt;/strong&gt;
The partial derivative of the loss for the &lt;span class="math"&gt;\(i\)&lt;/span&gt;th example with respect to the linear activation in the &lt;span class="math"&gt;\(r\)&lt;/span&gt;th node in layer &lt;span class="math"&gt;\(l\)&lt;/span&gt;&amp;nbsp;is:&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
dZ^{[l]}_{ri} &amp;amp;\equiv&amp;amp; \frac{\partial \mathcal{L}}{\partial Z^{[l]}_{ri}} \\
&amp;amp;=&amp;amp; \frac{\partial \mathcal{L}}{\partial A^{[l]}_{ri}} \frac{\partial A^{[l]}_{ri}}{\partial Z^{[l]}_{ri}} \\
&amp;amp;=&amp;amp; dA^{[l]}_{ri} * g'(Z^{[l]}_{ri})
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
The second line is by the application of the chain rule (single variable since only a single nonlinear activation depends on directly on &lt;span class="math"&gt;\(Z^{[l]}_{ri}\)&lt;/span&gt;). &lt;span class="math"&gt;\(g'(Z)\)&lt;/span&gt; is the derivative of the nonlinear activation function with respect to its input, which depends on the nonlinear activation function that is assigned to that particular node, e.g. sigmoid vs. tanh vs.&amp;nbsp;ReLU.&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;Backpropagation efficiently executes gradient descent for updating the parameters of a neural network by ordering and caching the calculations of the gradient of the cost with respect to the parameters in the nodes. This post is a little heavy on notation since the focus is on deriving the vectorized formulas for backpropagation, but we hope it complements the lectures in Week 3 of Andrew Ng&amp;#8217;s &lt;a href="https://www.coursera.org/learn/neural-networks-deep-learning/"&gt;&amp;#8220;Neural Networks and Deep Learning&amp;#8221;&lt;/a&gt; course as well as the excellent, but even more notation-heavy, resources on matrix calculus for backpropagation that are linked&amp;nbsp;below.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;More resources on vectorized backpropagation&lt;/strong&gt;
&lt;a href="https://explained.ai/matrix-calculus/index.html"&gt;The matrix calculus you need for deep learning&lt;/a&gt; - from explained.ai
&lt;a href="http://neuralnetworksanddeeplearning.com/chap2.html"&gt;How the backpropagation algorithm works&lt;/a&gt; - Chapter 2 of the Neural Networks and Deep Learning free online&amp;nbsp;text&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Theory"></category></entry><entry><title>An orientational integral</title><link href="https://efavdb.com/an-orientational-integral" rel="alternate"></link><published>2019-07-02T07:04:00-07:00</published><updated>2019-07-02T07:04:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2019-07-02:/an-orientational-integral</id><summary type="html">&lt;p&gt;We evaluate an integral having to do with vector averages over all
orientations in an n-dimensional&amp;nbsp;space.&lt;/p&gt;
&lt;h2&gt;Problem&amp;nbsp;definition&lt;/h2&gt;
&lt;p&gt;Let &lt;span class="math"&gt;\(\hat{v}\)&lt;/span&gt; be a unit vector in &lt;span class="math"&gt;\(n\)&lt;/span&gt;-dimensions and consider the orientation average&amp;nbsp;of
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{1} \label{1}
J \equiv \langle \hat{v} \cdot \vec{a}_1 …&lt;/div&gt;</summary><content type="html">&lt;p&gt;We evaluate an integral having to do with vector averages over all
orientations in an n-dimensional&amp;nbsp;space.&lt;/p&gt;
&lt;h2&gt;Problem&amp;nbsp;definition&lt;/h2&gt;
&lt;p&gt;Let &lt;span class="math"&gt;\(\hat{v}\)&lt;/span&gt; be a unit vector in &lt;span class="math"&gt;\(n\)&lt;/span&gt;-dimensions and consider the orientation average&amp;nbsp;of
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{1} \label{1}
J \equiv \langle \hat{v} \cdot \vec{a}_1 \hat{v} \cdot \vec{a}_2 \ldots \hat{v} \cdot \vec{a}_k \rangle
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(\vec{a}_1, \ldots, \vec{a}_k\)&lt;/span&gt; are some given fixed vectors. For example, if all &lt;span class="math"&gt;\(\vec{a}_i\)&lt;/span&gt; are equal to &lt;span class="math"&gt;\(\hat{x}\)&lt;/span&gt;, we want the orientation average of &lt;span class="math"&gt;\(v_x^k\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2&gt;Solution&lt;/h2&gt;
&lt;p&gt;We&amp;#8217;ll evaluate our integral using parameter differentiation of the multivariate Gaussian integral.&amp;nbsp;Let
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \nonumber
I &amp;amp;=&amp;amp; \frac{1}{(2 \pi)^{n/2}} \int e^{- \frac{\vert \vec{v} \vert^2}{2} + \sum_{i=1}^k \alpha_i \vec{v} \cdot \vec{a}_i} d^nv \\ \tag{2} \label{2}
&amp;amp;=&amp;amp; \exp \left [- \frac{1}{2} \vert \sum_{i=1}^k \alpha_i \vec{a}_i \vert^2 \right]
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
The expression in the second line follows from completing the square in the exponent in the first &amp;#8212; for review, see our post on the normal distribution, &lt;a href="http://efavdb.github.io/normal-distributions"&gt;here&lt;/a&gt;. Now, we consider a particular derivative of &lt;span class="math"&gt;\(I\)&lt;/span&gt; with respect to the &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; parameters. From the first line of (\ref{2}), we&amp;nbsp;have
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{3} \label{3}
\partial_{\alpha_1}\ldots \partial_{\alpha_k}I \vert_{\vec{\alpha}=0} &amp;amp;=&amp;amp; \frac{1}{(2 \pi)^{n/2}} \int e^{- \frac{\vert \vec{v} \vert^2}{2}} \prod_{i=1}^k \vec{v} \cdot \vec{a}_i d^n v \\
&amp;amp;\equiv &amp;amp; \frac{1}{(2 \pi)^{n/2}} \int_0^{\infty} e^{- \frac{\vert \vec{v} \vert^2}{2}} v^{n + k -1} dv \int \prod_{i=1}^k \hat{v} \cdot \vec{a}_i d \Omega_v \\
&amp;amp;=&amp;amp; \frac{2^{k/2 - 1}}{\pi^{n/2}} \Gamma(\frac{n+k}{2}) \times \int \prod_{i=1}^k \hat{v} \cdot \vec{a}_i d \Omega_v
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
The second factor above is almost our desired orientation average &lt;span class="math"&gt;\(J\)&lt;/span&gt; &amp;#8212; the only thing it&amp;#8217;s missing is the normalization, which we can get by evaluating this integral without any &lt;span class="math"&gt;\(\vec{a}\)&lt;/span&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt;s.&lt;/p&gt;
&lt;p&gt;Next, we evaluate the parameter derivative considered above in a second way, using the second line of (\ref{2}). This&amp;nbsp;gives,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{4} \label{4}
\partial_{\alpha_1}\ldots \partial_{\alpha_k}I \vert_{\vec{\alpha}=0} &amp;amp;=&amp;amp; \partial_{\alpha_1}\ldots \partial_{\alpha_k} \exp \left [- \frac{1}{2} \vert \sum_{i=1}^k \alpha_i \vec{a}_i \vert^2 \right] \vert_{\vec{\alpha}=0} \\
&amp;amp;=&amp;amp; \sum_{\text{pairings}} (\vec{a}_{i_1} \cdot \vec{a}_{i_2}) (\vec{a}_{i_3} \cdot \vec{a}_{i_4})\ldots (\vec{a}_{i_{k-1}} \cdot \vec{a}_{i_k})
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
The sum here is over all possible, unique pairings of the indices. You can see this is correct by carrying out the differentiation one parameter at a&amp;nbsp;time.&lt;/p&gt;
&lt;p&gt;To complete the calculation, we equate (\ref{3}) and (\ref{4}). This&amp;nbsp;gives
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{5}\label{5}
\int \prod_{i=1}^k \hat{v} \cdot \vec{a}_i d \Omega_v = \frac{\pi^{n/2}} {2^{k/2 - 1}\Gamma(\frac{n+k}{2})}\sum_{\text{pairings}} (\vec{a}_{i_1} \cdot \vec{a}_{i_2}) (\vec{a}_{i_3} \cdot \vec{a}_{i_4})\ldots (\vec{a}_{i_{k-1}} \cdot \vec{a}_{i_k})
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Again, to get the desired average, we need to divide the above by the normalization factor. This is given by the value of the integral (\ref{5}) when &lt;span class="math"&gt;\(k = 0\)&lt;/span&gt;. This&amp;nbsp;gives,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{6}\label{6}
J = \frac{1}{2^{k/2}}\frac{\Gamma(n/2)}{\Gamma(\frac{n+k}{2})} \sum_{\text{pairings}} (\vec{a}_{i_1} \cdot \vec{a}_{i_2}) (\vec{a}_{i_3} \cdot \vec{a}_{i_4})\ldots (\vec{a}_{i_{k-1}} \cdot \vec{a}_{i_k})
\end{eqnarray}&lt;/div&gt;
&lt;h2&gt;Example&lt;/h2&gt;
&lt;p&gt;Consider the case where &lt;span class="math"&gt;\(k=2\)&lt;/span&gt; and &lt;span class="math"&gt;\(\vec{a}_1 = \vec{a}_2 = \hat{x}\)&lt;/span&gt;. In this case, we note that the average of &lt;span class="math"&gt;\(\hat{v}_x^2\)&lt;/span&gt; is equal to the average along any other orientation. This means we&amp;nbsp;have
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\nonumber \tag{7} \label{7}
\langle \hat{v}_x^2 \rangle &amp;amp;=&amp;amp; \frac{1}{n} \sum_{i=1}^n \langle \hat{v}_x^2 + \hat{v}_y^2 + \ldots \rangle \\
&amp;amp;=&amp;amp; \frac{1}{n}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
We get this same result from our more general formula: Plugging in &lt;span class="math"&gt;\(k=2\)&lt;/span&gt; and &lt;span class="math"&gt;\(\vec{a}_1 = \vec{a}_2 = \hat{x}\)&lt;/span&gt; into (\ref{6}), we&amp;nbsp;obtain
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\nonumber \tag{8} \label{8}
\langle \hat{v}_x^2 \rangle &amp;amp;=&amp;amp; \frac{1}{2}\frac{\Gamma(n/2)}{\Gamma(\frac{n}{2} + 1)} \\
&amp;amp;=&amp;amp; \frac{1}{n}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
The two results&amp;nbsp;agree.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Theory"></category></entry><entry><title>Compounding benefits of tax protected accounts</title><link href="https://efavdb.com/compounding-benefits-of-tax-protected-accounts" rel="alternate"></link><published>2019-06-28T09:51:00-07:00</published><updated>2019-06-28T09:51:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2019-06-28:/compounding-benefits-of-tax-protected-accounts</id><summary type="html">&lt;p&gt;Here, we highlight one of the most important benefits of tax protected accounts (eg Traditional and Roth IRAs and 401ks). Specifically, we review the fact that not having to pay taxes on any investment growth that occurs while the money is held in the account results in compounding / exponential growth …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Here, we highlight one of the most important benefits of tax protected accounts (eg Traditional and Roth IRAs and 401ks). Specifically, we review the fact that not having to pay taxes on any investment growth that occurs while the money is held in the account results in compounding / exponential growth with a larger exponent than would be obtained in a traditional&amp;nbsp;account.&lt;/p&gt;
&lt;h2&gt;The growth&amp;nbsp;equations&lt;/h2&gt;
&lt;p&gt;Here, we consider three types of investment account: A standard bank account without tax protection, a traditional tax protected account, and a Roth tax protected account. We&amp;#8217;ll consider an idealized situation where we earn regular income of &lt;span class="math"&gt;\(D_0^{\prime}\)&lt;/span&gt; at time &lt;span class="math"&gt;\(0\)&lt;/span&gt; and then place this wealth (taxed, as appropriate for each case) into an investment that always returns a growth factor of &lt;span class="math"&gt;\(g\)&lt;/span&gt;. For simplicity, we&amp;#8217;ll assume that our tax rate never changes and is given by &lt;span class="math"&gt;\(t\)&lt;/span&gt;. In the next three sections, we calculate expressions for the final wealth at time &lt;span class="math"&gt;\(T\)&lt;/span&gt; that results from each account. Following that, we compare the&amp;nbsp;results.&lt;/p&gt;
&lt;h3&gt;Standard&amp;nbsp;account&lt;/h3&gt;
&lt;p&gt;In the standard account, the initial income must be taxed before it can be invested. Again, we define &lt;span class="math"&gt;\(t\)&lt;/span&gt; as the tax rate per year, so that the money left after tax at the start is&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{1} \label{1}  
D_0 = D_0^{\prime} (1 - t).  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
We place this money into an idealized investment that always returns a growth of &lt;span class="math"&gt;\(g\)&lt;/span&gt;. Therefore, after one year, the net wealth before tax is&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{2} \label{2}  
D_1^{\prime} = D_0 (1 + g).  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
The portion &lt;span class="math"&gt;\(D_0 g\)&lt;/span&gt; is new income that must be taxed, so after tax we have&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{3} \label{3}  
D_1 = D_0 + D_0 g (1 - t) = D_0[1 + g(1-t)].  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
If we iterate this expression up to time &lt;span class="math"&gt;\(T\)&lt;/span&gt;, we obtain&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\nonumber  
D_T &amp;amp;=&amp;amp; D_0[1 + g(1-t)]^T \  
&amp;amp;\equiv &amp;amp; D_0^{\prime} (1 - t)[1 + g(1-t)]^T \tag{4} \label{4}  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
This is our equation for the final, post-tax wealth obtained from the standard&amp;nbsp;account.&lt;/p&gt;
&lt;h3&gt;Traditional tax protected&amp;nbsp;account&lt;/h3&gt;
&lt;p&gt;In the traditional account, we do not need to pay tax at time &lt;span class="math"&gt;\(0\)&lt;/span&gt; on our initial &lt;span class="math"&gt;\(D_0^{\prime}\)&lt;/span&gt; dollars. Instead, this wealth is immediately put into our growth investment for &lt;span class="math"&gt;\(T\)&lt;/span&gt; years. This gives a pretax wealth at time &lt;span class="math"&gt;\(T\)&lt;/span&gt; of&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{5} \label{5}  
D_T^{\prime} = D_0 [1 + g]^T.  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
However, when this money is taken out at time &lt;span class="math"&gt;\(T\)&lt;/span&gt; it must be taxed. This gives&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{6} \label{6}  
D_T = D_0^{\prime} (1-t) [1 + g]^T.  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
This is the equation that describes the net wealth generated by the traditional tax protected&amp;nbsp;account.&lt;/p&gt;
&lt;h3&gt;Roth tax protected&amp;nbsp;account&lt;/h3&gt;
&lt;p&gt;In the Roth account, we do pay taxes on the initial &lt;span class="math"&gt;\(D_0^{\prime}\)&lt;/span&gt; at time &lt;span class="math"&gt;\(0\)&lt;/span&gt;. However, once this is done, we never need to pay taxes again, even when taking the money out at expiration. Therefore, the net wealth at time &lt;span class="math"&gt;\(T\)&lt;/span&gt; is&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{7} \label{7}  
D_T = D_0^{\prime} (1-t) (1 + g)^T  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Notice that this expression is identical to that for the traditional tax protected&amp;nbsp;account.&lt;/p&gt;
&lt;h2&gt;Comparison&lt;/h2&gt;
&lt;p&gt;Now that we have derived expressions for the final wealth in the three types of account, we can easily compare them. First, note that (\ref{4}), (\ref{6}), and (\ref{7}) all share the common factor of &lt;span class="math"&gt;\(D_0^{\prime} (1-t) \equiv D_0\)&lt;/span&gt;, which can be considered the initial post-tax wealth. This means that the only difference between the standard and tax protected accounts is the effective growth rate: The growth rate term for the standard account is&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}  
\text{growth factor (standard account)} = [1 + g(1-t)]^T \tag{8} \label{8}  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
while that for the two tax protected accounts is&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}  
\text{growth factor (tax protected)}=[1 + g]^T \tag{9}\label{9}  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
These two factors may look similar, but they represent exponential growth with different exponents. Consequently, for large &lt;span class="math"&gt;\(T\)&lt;/span&gt;, the growth from (\ref{9}) can be much larger than that from (\ref{8}). To illustrate this point, we tabulate the two functions assuming &lt;span class="math"&gt;\(7\)&lt;/span&gt; percent growth for &lt;span class="math"&gt;\(30\)&lt;/span&gt; years at a few representative tax rates below. Notice that the growth rates are similar when that tax rates are lower &amp;#8212; which makes sense because taxation does not have much of an effect in this limit. However, the tax protected account has a much larger value in the opposite limit &amp;#8212; 7.61 vs 2.81 for the standard&amp;nbsp;account!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;standard&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;  
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;tax_protected&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;  
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;

&lt;span class="n"&gt;taxes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  
&lt;span class="n"&gt;standard_values&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;standard&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.07&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;taxes&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  
&lt;span class="n"&gt;protected_values&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;tax_protected&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.07&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;taxes&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# output:  &lt;/span&gt;
&lt;span class="n"&gt;TAX&lt;/span&gt; &lt;span class="n"&gt;RATE&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;  
&lt;span class="n"&gt;STANDARD&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;6.25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;5.13&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;4.20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;3.44&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.81&lt;/span&gt;  
&lt;span class="n"&gt;PROTECTED&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;7.61&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;7.61&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;7.61&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;7.61&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;7.61&lt;/span&gt;  
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Final&amp;nbsp;comments&lt;/h2&gt;
&lt;p&gt;We&amp;#8217;ve considered an idealized situation here in order to highlight the important point that tax protected accounts enjoy much larger compounding / exponential growth rates than do standard accounts. This can have a very big effect when taxation is high. However, it&amp;#8217;s important to point out that there are other important characteristics not highlighted by our simplified model system. One important case is that the benefits of the traditional and roth accounts can differ if one&amp;#8217;s tax rate changes over time. If interested, you should look into this&amp;nbsp;elsewhere.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Finance"></category></entry><entry><title>Utility functions and immigration</title><link href="https://efavdb.com/utility-functions-and-immigration" rel="alternate"></link><published>2019-06-21T09:24:00-07:00</published><updated>2019-06-21T09:24:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2019-06-21:/utility-functions-and-immigration</id><summary type="html">&lt;p&gt;We consider how the &lt;span class="caps"&gt;GDP&lt;/span&gt; or utility output of a city depends on the number of people living within it. From this, we derive some interesting consequences that can inform both government and individual attitudes towards&amp;nbsp;newcomers.&lt;/p&gt;
&lt;p&gt;Edit 9/2022: The model here can&amp;#8217;t be complete because it doesn …&lt;/p&gt;</summary><content type="html">&lt;p&gt;We consider how the &lt;span class="caps"&gt;GDP&lt;/span&gt; or utility output of a city depends on the number of people living within it. From this, we derive some interesting consequences that can inform both government and individual attitudes towards&amp;nbsp;newcomers.&lt;/p&gt;
&lt;p&gt;Edit 9/2022: The model here can&amp;#8217;t be complete because it doesn&amp;#8217;t take into account
city capacity.  The true utility functions should be shaped like &lt;span class="math"&gt;\(S\)&lt;/span&gt;-curves.
Nevertheless, the arguments here can provide some insight into &amp;#8220;growth&amp;#8221;-phase
dynamics and&amp;nbsp;preferences.&lt;/p&gt;
&lt;h3&gt;The utility function and benefit per&amp;nbsp;person&lt;/h3&gt;
&lt;p&gt;In this post, we will consider an idealized town whose net output &lt;span class="math"&gt;\(U\)&lt;/span&gt; (the &lt;span class="caps"&gt;GDP&lt;/span&gt;) scales as a power law with the number of people &lt;span class="math"&gt;\(N\)&lt;/span&gt; living within it. That is, we&amp;#8217;ll assume,&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{1} \label{1}  
U(N) = a N^{\gamma}.  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
We&amp;#8217;ll assume that the average benefit captured per person is their share of this utility,&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{2} \label{2}  
BPP(N) = U(N) / N = a N^{\gamma -1}.  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
What can we say about the above &lt;span class="math"&gt;\(a\)&lt;/span&gt; and &lt;span class="math"&gt;\(\gamma\)&lt;/span&gt;? Well, we must have &lt;span class="math"&gt;\(a&amp;gt; 0\)&lt;/span&gt; if the society is productive. Further, because we know that cities allow for more complex economies as the number of occupants grow, we must have &lt;span class="math"&gt;\(\gamma &amp;gt; 1\)&lt;/span&gt;. These are the only assumptions we will make here. Below, we&amp;#8217;ll see that these assumptions imply some interesting&amp;nbsp;consequences.&lt;/p&gt;
&lt;h3&gt;Marginal&amp;nbsp;benefits&lt;/h3&gt;
&lt;p&gt;When a new person immigrates to a city, its &lt;span class="math"&gt;\(N\)&lt;/span&gt; value goes up by one. Here, we consider how the utility and benefit per person changes when this occurs. The increase in net utility is simply&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{3} \label{3}  
\partial_N U(N) = a \gamma N^{\gamma -1}.  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Notice that because we have &lt;span class="math"&gt;\(\gamma &amp;gt; 1\)&lt;/span&gt;, (\ref{3}) is a function that increases with &lt;span class="math"&gt;\(N\)&lt;/span&gt;. That is, cities with larger populations benefit more (as a collective) per immigrant newcomer than those cities with smaller &lt;span class="math"&gt;\(N\)&lt;/span&gt; would. This implies that the governments of large cities should be more enthusiastic about welcoming of newcomers than those of smaller&amp;nbsp;cities.&lt;/p&gt;
&lt;p&gt;Now consider the marginal benefit per person when one new person moves to this city. This is simply&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{4} \label{4}  
\partial_N BPP(N) = a (\gamma - 1) N^{\gamma -2}.  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Notice that this is different from the form (\ref{3}) that describes the marginal increase in total city utility. In particular, while (\ref{4}) is positive, it is not necessarily increasing with &lt;span class="math"&gt;\(N\)&lt;/span&gt;: If &lt;span class="math"&gt;\(\gamma &amp;lt; 2\)&lt;/span&gt;, (\ref{4}) decreases with &lt;span class="math"&gt;\(N\)&lt;/span&gt;. Cities having &lt;span class="math"&gt;\(\gamma\)&lt;/span&gt; values like this are such that the net new wealth captured per existing citizen &amp;#8212; thanks to each new immigrant &amp;#8212; quickly decays to zero. The consequence is that city governments and existing citizens can have a conflict of interest when it comes to&amp;nbsp;immigration.&lt;/p&gt;
&lt;h3&gt;Equilibration&lt;/h3&gt;
&lt;p&gt;In a local population that has freedom of movement, we can expect the migration of people to push the benefit per person to be equal across cities. In cases like this, we should then have&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{5} \label{5}  
a_i N^{\gamma_i -1} \approx a_j N^{\gamma_j -1},  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
for each city &lt;span class="math"&gt;\(i\)&lt;/span&gt; and &lt;span class="math"&gt;\(j\)&lt;/span&gt; for which there is low mutual migration costs. We point out that this is not the same result required to maximize the net, global output. This latter score is likely that which an authoritarian government might try to maximize. To maximize net utility, we need to have the marginal utility per city equal across cities, which means&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{6} \label{6}  
\partial_N U_i(N) = \partial_N U_j(N)  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
or,&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{7} \label{7}  
a_i \gamma_i N^{\gamma_i -1} = a_j \gamma_j N^{\gamma_j -1}.  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
We see that (\ref{5}) and (\ref{7}) differ in that there are &lt;span class="math"&gt;\(\gamma\)&lt;/span&gt; factors in (\ref{7}) that are not present in (\ref{5}). This implies that as long as the &lt;span class="math"&gt;\(\gamma\)&lt;/span&gt; values differ across cities, there will be a conflict of interest between the migrants and the&amp;nbsp;government.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Case studies"></category></entry><entry><title>The speed of traffic</title><link href="https://efavdb.com/the-speed-of-traffic" rel="alternate"></link><published>2019-06-14T09:21:00-07:00</published><updated>2019-06-14T09:21:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2019-06-14:/the-speed-of-traffic</id><summary type="html">&lt;p&gt;We use a simple argument to estimate the speed of traffic on a highway as a function of the density of cars. The idea is to simply calculate the maximum speed that traffic could go without supporting a growing traffic&amp;nbsp;jam.&lt;/p&gt;
&lt;h3&gt;Jam dissipation&amp;nbsp;argument&lt;/h3&gt;
&lt;p&gt;To estimate the speed of traffic …&lt;/p&gt;</summary><content type="html">&lt;p&gt;We use a simple argument to estimate the speed of traffic on a highway as a function of the density of cars. The idea is to simply calculate the maximum speed that traffic could go without supporting a growing traffic&amp;nbsp;jam.&lt;/p&gt;
&lt;h3&gt;Jam dissipation&amp;nbsp;argument&lt;/h3&gt;
&lt;p&gt;To estimate the speed of traffic as a function of density, we&amp;#8217;ll calculate an upper bound and argue that actual traffic speeds must be described by an equation similar to that obtained. To derive our upper bound, we&amp;#8217;ll consider what happens when a small traffic jam forms. If the speed of cars is such that the rate of exit from the jam is larger than the rate at which new cars enter the jam, then the jam will dissipate. On the other hand, if this doesn&amp;#8217;t hold, the jam will grow, causing the speed to drop until a speed is obtained that allows the jam to dissipate. This sets the bound. Although we consider a jam to make the argument simple, what we really have in mind is any other sort of modest slow-down that may&amp;nbsp;occur.&lt;/p&gt;
&lt;p&gt;To begin, we introduce some definitions. (1) Let &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; be the density of cars in units of &lt;span class="math"&gt;\([cars / mile]\)&lt;/span&gt;. (2) Next we consider the rate of exit from a jam: Note that when traffic is stopped, a car cannot move until the car in front of it does. Because a human is driving the car, there is a slight delay between the time that one car moves and the car behind it moves. Let &lt;span class="math"&gt;\(T\)&lt;/span&gt; be this delay time in &lt;span class="math"&gt;\([hours]\)&lt;/span&gt;. (3) Let &lt;span class="math"&gt;\(v\)&lt;/span&gt; be the speed of traffic outside the jam in units of &lt;span class="math"&gt;\([miles / hour]\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;With the above definitions, we now consider the rate at which cars exit a jam. This is the number of cars that can exit the jam per hour, which is simply&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{1} \label{1}  
r_{out} = \frac{1}{T}.  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Next, the rate at which cars enter the jam is given by&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{2} \label{2}  
r_{in} = \lambda v.  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Requiring that &lt;span class="math"&gt;\(r_{out} &amp;gt; r_{in}\)&lt;/span&gt; we get&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \label{3} \tag{3}  
v &amp;lt; \frac{1}{\lambda T}.  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
This is our bound and estimate for the speed of traffic. We note that this form for &lt;span class="math"&gt;\(v\)&lt;/span&gt; follows from dimensional analysis, so the actual rate of traffic must have the same algebraic form as our upper bound (\ref{3}) &amp;#8212; it can differ by a constant factor in front, but should have the same &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; and &lt;span class="math"&gt;\(T\)&lt;/span&gt;&amp;nbsp;dependence.&lt;/p&gt;
&lt;h3&gt;Plugging in&amp;nbsp;numbers&lt;/h3&gt;
&lt;p&gt;I estimate &lt;span class="math"&gt;\(T\)&lt;/span&gt;, the delay time between car movements to be about one second, which in hours is&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{4} \label{4}  
T \approx 0.00028\ [hour].  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Next for &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;, note that a typical car is about 10 feet long and a mile is around 5000 feet, so the maximum for &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; is around &lt;span class="math"&gt;\( \lambda \lesssim 500 [cars / mile]\)&lt;/span&gt;. Consider a case where there is a car every 10 car lengths or so. In this case, the density will go down from the maximum by a factor of 10, or&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{5} \label{5}  
\lambda \approx 50 \ [cars / mile].  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Plugging (\ref{4}) and (\ref{5}) into (\ref{3}), we obtain&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}  \tag{6}
v \lesssim \frac{1}{0.00028 * 50} \approx 70\ [mile / hour],  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
quite close to our typical highway traffic speeds (and speed&amp;nbsp;limits).&lt;/p&gt;
&lt;h3&gt;Final&amp;nbsp;comments&lt;/h3&gt;
&lt;p&gt;The above bound clearly depends on what values you plug in &amp;#8212; I picked numbers that seemed reasonable, but admit I adjusted them a bit till I got the final number I wanted for &lt;span class="math"&gt;\(v\)&lt;/span&gt;. Anecdotally, I&amp;#8217;ve found the result to work well at other densities: For example, when traffic is slow on the highway near my house, if I see that there is a car every 5 car lengths, the speed tends to be about &lt;span class="math"&gt;\(30 [miles / hour]\)&lt;/span&gt; &amp;#8212; so scaling rule seems to work. The last thing I should note is that wikipedia has an article outlining some of the extensive research literature that&amp;#8217;s been done on traffic flows &amp;#8212; you can see that &lt;a href="https://en.wikipedia.org/wiki/Traffic_flow"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Case studies"></category><category term="theory"></category><category term="traffic"></category></entry><entry><title>Linear compression in python: PCA vs unsupervised feature selection</title><link href="https://efavdb.com/unsupervised-feature-selection-in-python-with-linselect" rel="alternate"></link><published>2018-08-11T07:30:00-07:00</published><updated>2018-08-11T07:30:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2018-08-11:/unsupervised-feature-selection-in-python-with-linselect</id><summary type="html">&lt;p&gt;We illustrate the application of two linear compression algorithms in python: Principal component analysis (&lt;span class="caps"&gt;PCA&lt;/span&gt;) and least-squares feature selection. Both can be used to compress a passed array, and they both work by stripping out redundant columns from the array. The two differ in that &lt;span class="caps"&gt;PCA&lt;/span&gt; operates in a particular …&lt;/p&gt;</summary><content type="html">&lt;p&gt;We illustrate the application of two linear compression algorithms in python: Principal component analysis (&lt;span class="caps"&gt;PCA&lt;/span&gt;) and least-squares feature selection. Both can be used to compress a passed array, and they both work by stripping out redundant columns from the array. The two differ in that &lt;span class="caps"&gt;PCA&lt;/span&gt; operates in a particular rotated frame, while the feature selection solution operates directly on the original columns. As we illustrate below, &lt;span class="caps"&gt;PCA&lt;/span&gt; always gives a stronger compression. However, the feature selection solution is often comparably strong, and its output has the benefit of being relatively easy to interpret &amp;#8212; a virtue that is important for many&amp;nbsp;applications.&lt;/p&gt;
&lt;p&gt;We use our python package &lt;code&gt;linselect&lt;/code&gt; to carry out efficient feature selection-based compression below &amp;#8212; this is available on pypi (&lt;code&gt;pip install linselect&lt;/code&gt;) and &lt;a href="https://github.com/EFavDB/linselect"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Linear compression&amp;nbsp;algorithms&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2018/06/simple_line.jpg"&gt;&lt;img alt="simple_line" src="https://efavdb.com/wp-content/uploads/2018/06/simple_line.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;To compress a data array having &lt;span class="math"&gt;\(n\)&lt;/span&gt; columns, linear compression algorithms begin by fitting a &lt;span class="math"&gt;\(k\)&lt;/span&gt;-dimensional line, or &lt;em&gt;hyperplane&lt;/em&gt;, to the data (with &lt;span class="math"&gt;\(k &amp;lt; n\)&lt;/span&gt;). Any point in the hyperplane can be uniquely identified using a basis of &lt;span class="math"&gt;\(k\)&lt;/span&gt; components. Marking down each point&amp;#8217;s projected location in the hyperplane using these components then gives a &lt;span class="math"&gt;\(k\)&lt;/span&gt;-column, compressed representation of the data. This idea is illustrated in Fig. 1 at right, where a line is fit to some two-component data. Projecting the points onto the line and then marking down how far along the line each projected point sits, we obtain a one-column compression. Carrying out this process can be useful if storage space is at a premium or if any operations need to be applied to the array (usually operations will run much faster on the compressed format). Further, compressed data is often easier to interpret and visualize, thanks to its reduced&amp;nbsp;dimension.&lt;/p&gt;
&lt;p&gt;In this post, we consider two automated linear compression algorithms: principal component analysis (&lt;span class="caps"&gt;PCA&lt;/span&gt;) and least-squares unsupervised feature selection. These differ because they are obtained from different hyperplane fitting strategies: The &lt;span class="caps"&gt;PCA&lt;/span&gt; approach is obtained from the &lt;span class="math"&gt;\(k\)&lt;/span&gt;-dimensional hyperplane fit that minimizes the data&amp;#8217;s total squared-projection error. In general, the independent variables of this fit &amp;#8212; i.e., the &lt;span class="math"&gt;\(k\)&lt;/span&gt; components specifying locations in the fit plane &amp;#8212; end up being some linear combinations of the original &lt;span class="math"&gt;\(x_i\)&lt;/span&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt;s. In contrast, the feature selection strategy intelligently picks a subset of the original array columns as predictors and then applies the usual least-squares fit to the others for compression [1]. These approaches are illustrated in the left and right panels of Fig. 2 below. The two fit lines there look very similar, but the encodings returned by these strategies differ qualitatively: The 1-d compression returned by &lt;span class="caps"&gt;PCA&lt;/span&gt; is how far along the &lt;span class="math"&gt;\(PCA_1\)&lt;/span&gt; direction a point sits (this is some linear combination of &lt;span class="math"&gt;\(x_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(x_2\)&lt;/span&gt; &amp;#8212; see figure), while the feature selection solution simply returns each point&amp;#8217;s &lt;span class="math"&gt;\(x_1\)&lt;/span&gt; value. One of our goals here is to explain why this difference can favor the feature selection approach in certain&amp;nbsp;applications.&lt;/p&gt;
&lt;p&gt;Our post proceeds as follows: In the next section, we consider two representative applications in python: (1) The compression of a data set of tech-sector stock price quotes, and (2) the visualization of some economic summary statistics on the G20 nations. Working through these applications, we are able to familiarize ourselves with the output of the two algorithms, and also through contrast to highlight their relative virtues. The discussion section summarizes what we learn. Finally, a short appendix covers some of the formal mathematics of compression. There, we prove that linear compression-decompression operators are always&amp;nbsp;projections.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2018/06/pca_vs_linselect.jpg"&gt;&lt;img alt="pca_vs_linselect" src="https://efavdb.com/wp-content/uploads/2018/06/pca_vs_linselect.jpg"&gt;&lt;/a&gt;
&lt;strong&gt;Fig. 2&lt;/strong&gt;. A cartoon illustrating the projection that results when applying &lt;span class="caps"&gt;PCA&lt;/span&gt; (left) and unsupervised feature selection &amp;#8212; via &lt;code&gt;linselect&lt;/code&gt; (right): The original 2-d big dots are replaced by their small dot, effectively-1-d approximations &amp;#8212; a&amp;nbsp;projection.&lt;/p&gt;
&lt;h2&gt;Applications&lt;/h2&gt;
&lt;p&gt;Both data sets explored below are available on our Github, &lt;a href="https://github.com/EFavDB/linselect_demos"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Stock&amp;nbsp;prices&lt;/h3&gt;
&lt;h4&gt;Loading and compressing the&amp;nbsp;data&lt;/h4&gt;
&lt;p&gt;In this section, we apply our algorithms to a prepared data set of one year&amp;#8217;s worth of daily percentage price lifts on 50 individual tech stocks [2]. We expect these stocks to each be governed by a common set of market forces, motivating the idea that a substantial compression might be possible. This is true, and the compressed arrays that result may be more efficiently operated on, as noted above. In addition, we&amp;#8217;ll see below that we can learn something about the full data set by examining the compression&amp;nbsp;outputs.&lt;/p&gt;
&lt;p&gt;The code below loads our data, smooths it over a running 30 day window (to remove idiosyncratic noise that is not of much interest), prints out the first three rows, compresses the data using our two methods, and then finally prints out the first five &lt;span class="caps"&gt;PCA&lt;/span&gt; components and the top five selected&amp;nbsp;stocks.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.decomposition&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;PCA&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;StandardScaler&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;linselect&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;FwdSelect&lt;/span&gt;

&lt;span class="c1"&gt;# CONSTANTS&lt;/span&gt;
&lt;span class="n"&gt;KEEP&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="c1"&gt;# compression dimension&lt;/span&gt;
&lt;span class="n"&gt;WINDOW_SIZE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt; &lt;span class="c1"&gt;# smoothing window size&lt;/span&gt;

&lt;span class="c1"&gt;# LOAD AND SMOOTH THE DATA&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;stocks.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rolling&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;WINDOW_SIZE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iloc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;WINDOW_SIZE&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iloc&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;TICKERS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iloc&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iloc&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;

&lt;span class="c1"&gt;# PCA COMPRESSION&lt;/span&gt;
&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;StandardScalar&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;pca&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;PCA&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_components&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;KEEP&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;pca&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;X_compressed_pca&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pca&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# FEATURE SELECTION COMPRESSION&lt;/span&gt;
&lt;span class="n"&gt;selector&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;FwdSelect&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;selector&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;X_compressed_linselect&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;selector&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ordered_features&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;KEEP&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;

&lt;span class="c1"&gt;# PRINT OUT FIRST FIVE PCA COMPONENTs, TOP FIVE STOCKS&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt; &lt;span class="n"&gt;pca&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;components_&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;KEEP&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt; &lt;span class="n"&gt;TICKERS&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;selector&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ordered_features&lt;/span&gt;&lt;span class="p"&gt;][:&lt;/span&gt;&lt;span class="n"&gt;KEEP&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The output of the above print&amp;nbsp;statements:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# The first three rows of the data frame:&lt;/span&gt;
&lt;span class="n"&gt;date&lt;/span&gt; &lt;span class="n"&gt;AAPL&lt;/span&gt; &lt;span class="n"&gt;ADBE&lt;/span&gt; &lt;span class="n"&gt;ADP&lt;/span&gt; &lt;span class="n"&gt;ADSK&lt;/span&gt; &lt;span class="n"&gt;AMAT&lt;/span&gt; &lt;span class="n"&gt;AMZN&lt;/span&gt; \
&lt;span class="mi"&gt;30&lt;/span&gt; &lt;span class="mi"&gt;2017&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;05&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;31&lt;/span&gt; &lt;span class="mf"&gt;0.002821&lt;/span&gt; &lt;span class="mf"&gt;0.002994&lt;/span&gt; &lt;span class="mf"&gt;0.000248&lt;/span&gt; &lt;span class="mf"&gt;0.009001&lt;/span&gt; &lt;span class="mf"&gt;0.006451&lt;/span&gt; &lt;span class="mf"&gt;0.003237&lt;/span&gt;
&lt;span class="mi"&gt;31&lt;/span&gt; &lt;span class="mi"&gt;2017&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;06&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;01&lt;/span&gt; &lt;span class="mf"&gt;0.003035&lt;/span&gt; &lt;span class="mf"&gt;0.002776&lt;/span&gt; &lt;span class="mf"&gt;0.000522&lt;/span&gt; &lt;span class="mf"&gt;0.008790&lt;/span&gt; &lt;span class="mf"&gt;0.005487&lt;/span&gt; &lt;span class="mf"&gt;0.003450&lt;/span&gt;
&lt;span class="mi"&gt;32&lt;/span&gt; &lt;span class="mi"&gt;2017&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;06&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;02&lt;/span&gt; &lt;span class="mf"&gt;0.003112&lt;/span&gt; &lt;span class="mf"&gt;0.002964&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.000560&lt;/span&gt; &lt;span class="mf"&gt;0.008573&lt;/span&gt; &lt;span class="mf"&gt;0.005523&lt;/span&gt; &lt;span class="mf"&gt;0.003705&lt;/span&gt;

&lt;span class="n"&gt;ASML&lt;/span&gt; &lt;span class="n"&gt;ATVI&lt;/span&gt; &lt;span class="n"&gt;AVGO&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="n"&gt;TSLA&lt;/span&gt; &lt;span class="n"&gt;TSM&lt;/span&gt; \
&lt;span class="mi"&gt;30&lt;/span&gt; &lt;span class="mf"&gt;0.000755&lt;/span&gt; &lt;span class="mf"&gt;0.005933&lt;/span&gt; &lt;span class="mf"&gt;0.003988&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.001419&lt;/span&gt; &lt;span class="mf"&gt;0.004500&lt;/span&gt; &lt;span class="mf"&gt;0.003590&lt;/span&gt;
&lt;span class="mi"&gt;31&lt;/span&gt; &lt;span class="mf"&gt;0.002174&lt;/span&gt; &lt;span class="mf"&gt;0.006369&lt;/span&gt; &lt;span class="mf"&gt;0.003225&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.001125&lt;/span&gt; &lt;span class="mf"&gt;0.003852&lt;/span&gt; &lt;span class="mf"&gt;0.004279&lt;/span&gt;
&lt;span class="mi"&gt;32&lt;/span&gt; &lt;span class="mf"&gt;0.001566&lt;/span&gt; &lt;span class="mf"&gt;0.006014&lt;/span&gt; &lt;span class="mf"&gt;0.005343&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.001216&lt;/span&gt; &lt;span class="mf"&gt;0.004130&lt;/span&gt; &lt;span class="mf"&gt;0.004358&lt;/span&gt;

&lt;span class="n"&gt;TWTR&lt;/span&gt; &lt;span class="n"&gt;TXN&lt;/span&gt; &lt;span class="n"&gt;VMW&lt;/span&gt; &lt;span class="n"&gt;VZ&lt;/span&gt; &lt;span class="n"&gt;WDAY&lt;/span&gt; &lt;span class="n"&gt;WDC&lt;/span&gt; &lt;span class="n"&gt;ZNGA&lt;/span&gt;
&lt;span class="mi"&gt;30&lt;/span&gt; &lt;span class="mf"&gt;0.008292&lt;/span&gt; &lt;span class="mf"&gt;0.001467&lt;/span&gt; &lt;span class="mf"&gt;0.001984&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.001741&lt;/span&gt; &lt;span class="mf"&gt;0.006103&lt;/span&gt; &lt;span class="mf"&gt;0.002916&lt;/span&gt; &lt;span class="mf"&gt;0.007811&lt;/span&gt;
&lt;span class="mi"&gt;31&lt;/span&gt; &lt;span class="mf"&gt;0.008443&lt;/span&gt; &lt;span class="mf"&gt;0.001164&lt;/span&gt; &lt;span class="mf"&gt;0.002026&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.001644&lt;/span&gt; &lt;span class="mf"&gt;0.006303&lt;/span&gt; &lt;span class="mf"&gt;0.003510&lt;/span&gt; &lt;span class="mf"&gt;0.008379&lt;/span&gt;
&lt;span class="mi"&gt;32&lt;/span&gt; &lt;span class="mf"&gt;0.007796&lt;/span&gt; &lt;span class="mf"&gt;0.000637&lt;/span&gt; &lt;span class="mf"&gt;0.001310&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.001333&lt;/span&gt; &lt;span class="mf"&gt;0.006721&lt;/span&gt; &lt;span class="mf"&gt;0.002836&lt;/span&gt; &lt;span class="mf"&gt;0.008844&lt;/span&gt;

&lt;span class="c1"&gt;# PCA top components:&lt;/span&gt;
&lt;span class="p"&gt;[[&lt;/span&gt; &lt;span class="mf"&gt;0.10548148&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.20601986&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.0126039&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.20139121&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.11739195&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.02536787&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.2044143&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.08462741&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.03251305&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.10796197&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.00463919&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.17564998&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.08678107&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1931497&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.16850867&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.16260134&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.0174396&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.01174769&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.11617622&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.01036602&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;

&lt;span class="c1"&gt;# Feature selector output:&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;WDAY&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;PYPL&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;AMZN&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;LRCX&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;HPQ&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Lines 22 and 27 in the first code block above are the two compressed versions of the original data array, line 16. For each row, the first compression stores the amplitude of that date&amp;#8217;s stock changes along each of the first five &lt;span class="caps"&gt;PCA&lt;/span&gt; components (printed below line 17 of second code block), while the second compression is simply equal to the five columns of the original array corresponding to the stocks picked out by the selector (printed below line 24 of the second code&amp;nbsp;block).&lt;/p&gt;
&lt;h4&gt;Exploring the&amp;nbsp;encodings&lt;/h4&gt;
&lt;p&gt;Working with the compressed arrays obtained above provides some immediate operational benefits: Manipulations of the compressed arrays can be carried out more quickly and they require less memory for storage. Here, we review how valuable insight can also obtained from our compressions &amp;#8212; via study of the compression&amp;nbsp;components.&lt;/p&gt;
&lt;p&gt;First, we consider the &lt;span class="caps"&gt;PCA&lt;/span&gt; components. It turns out that these components are the eigenvectors of the correlation matrix of our data set (&lt;span class="math"&gt;\(X^T \cdot X\)&lt;/span&gt;) &amp;#8212; that is, they are the collective, fluctuation modes present in the data set (for those who have studied classical mechanics, you can imagine the system as one where the different stocks are masses that are connected by springs, and these eigenvectors are the modes of the system). Using this fact, one can show that the components evolve in an uncorrelated manner. Further, one can show that projecting the data set down onto the top &lt;span class="math"&gt;\(k\)&lt;/span&gt; modes gives the minimum squared projection error of all possible &lt;span class="math"&gt;\(k\)&lt;/span&gt;-component projections. The first component then describes the largest amplitude fluctuation pattern exhibited in the data. From line 18 above, this is &lt;span class="math"&gt;\([ 0.105, 0.206, -0.012, 0.201, ... ]\)&lt;/span&gt;. These coefficients tell us that when the first stock (&lt;span class="caps"&gt;AAPL&lt;/span&gt;) goes up by some amount, the second (&lt;span class="caps"&gt;ADBE&lt;/span&gt;) typically goes up by about twice as much (this follows from fact that 0.206 is about twice as big as 0.105), etc. This isn&amp;#8217;t the full story of course, because each day&amp;#8217;s movements are a superposition (sum) of the amplitudes along each of &lt;span class="caps"&gt;PCA&lt;/span&gt; components. Including more of these components in a compression allows one to capture more of the detailed correlation patterns exhibited in the data. However, each additional &lt;span class="caps"&gt;PCA&lt;/span&gt; component provides progressively less value as one moves down the ranking &amp;#8212; it is this fact that allows a good compression to be obtained using only a minority of these&amp;nbsp;modes.&lt;/p&gt;
&lt;p&gt;Whereas the &lt;span class="caps"&gt;PCA&lt;/span&gt; components directly encode the collective, correlated fluctuations exhibited in our data, the feature selection solution attempts to identify a minimally-redundant subset of the original array&amp;#8217;s columns &amp;#8212; one that is representative of the full set. This strategy is best understood in the limit where the original columns fall into a set of discreet clusters (in our example, we might expect the businesses operating in a particular sub-sector to fall into a single cluster). In such cases, a good compression is obtained by selecting one representative column from each cluster: Once the representatives are selected, each of the other members of a given cluster can be approximately reconstructed using its selected representative as a predictor. In the above, we see that our automated feature selector has worked well, in that the companies selected (&amp;#8216;&lt;span class="caps"&gt;WDAY&lt;/span&gt;&amp;#8217;, &amp;#8216;&lt;span class="caps"&gt;PYPL&lt;/span&gt;&amp;#8217;, &amp;#8216;&lt;span class="caps"&gt;AMZN&lt;/span&gt;&amp;#8217;, &amp;#8216;&lt;span class="caps"&gt;LRCX&lt;/span&gt;&amp;#8217;, and &amp;#8216;&lt;span class="caps"&gt;HPQ&lt;/span&gt;&amp;#8217;) each operate in a different part of the tech landscape [3]. In general, we can expect the feature selector to attempt to mimic the &lt;span class="caps"&gt;PCA&lt;/span&gt; approach, in that it will seek columns that fluctuate in a nearly orthogonal manner. However, whereas the &lt;span class="caps"&gt;PCA&lt;/span&gt; components highlight which columns fluctuate together, the feature selector attempts to throw out all but one of the columns that fluctuate together &amp;#8212; a sort-of dual&amp;nbsp;approach.&lt;/p&gt;
&lt;h4&gt;Compression&amp;nbsp;strength&lt;/h4&gt;
&lt;p&gt;To decide how many compression components are needed for a given application, one need only consider the variance explained as a function of the compression dimension &amp;#8212; this is equal to one minus the average squared error of the projections that result from the compressions (see footnote [4] for a visualization of the error that results from compression here). In the two python packages we&amp;#8217;re using, one can access these values as&amp;nbsp;follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;print&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cumsum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pca&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;explained_variance_ratio_&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.223&lt;/span&gt; &lt;span class="mf"&gt;0.367&lt;/span&gt; &lt;span class="mf"&gt;0.493&lt;/span&gt; &lt;span class="mf"&gt;0.598&lt;/span&gt; &lt;span class="mf"&gt;0.696&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;print&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;var&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mf"&gt;50.0&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;var&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;selector&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ordered_cods&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;KEEP&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.169&lt;/span&gt; &lt;span class="mf"&gt;0.316&lt;/span&gt; &lt;span class="mf"&gt;0.428&lt;/span&gt; &lt;span class="mf"&gt;0.530&lt;/span&gt; &lt;span class="mf"&gt;0.612&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The printed lines above show that both algorithms capture more than &lt;span class="math"&gt;\(50%\)&lt;/span&gt; of the variance exhibited in the data using only 4 of the 50 stocks. The &lt;span class="caps"&gt;PCA&lt;/span&gt; compressions are stronger in each dimension because &lt;span class="caps"&gt;PCA&lt;/span&gt; is unconstrained &amp;#8212; it can use any linear combination of the initial features for compression components, whereas the feature selector is constrained to use a subset of the original&amp;nbsp;features.&lt;/p&gt;
&lt;p&gt;A plot of the values above across all compression dimensions is shown in Fig. 3 below. Looking at this plot, we see an elbow somewhere between &lt;span class="math"&gt;\(5\)&lt;/span&gt; and &lt;span class="math"&gt;\(10\)&lt;/span&gt; retained components. This implies that our &lt;span class="math"&gt;\(50\)&lt;/span&gt;-dimensional data set mostly lies within a subspace of dimension &lt;span class="math"&gt;\(k \in (5, 10)\)&lt;/span&gt;. Using any &lt;span class="math"&gt;\(k\)&lt;/span&gt; in that interval will provide a decent compression, and a satisfying large dimensional reduction &amp;#8212; a typical result of applying these algorithms to large, raw data sets. Again, this is useful because it allows one to stop tracking redundant columns that offers little incremental&amp;nbsp;value.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2018/06/cod_stocks.png"&gt;&lt;img alt="cod_stocks" src="https://efavdb.com/wp-content/uploads/2018/06/cod_stocks.png"&gt;&lt;/a&gt; &lt;strong&gt;Fig. 3&lt;/strong&gt;. Plots of the compression strength (coefficient of determination or &lt;span class="math"&gt;\(r^2\)&lt;/span&gt;) for our two compression algorithms versus compression dimension. We see two things: (1) &lt;span class="caps"&gt;PCA&lt;/span&gt; gives a slightly stronger compression at each dimension, and (2) The full data set spans 50 dimensions, but the elbow in the plots suggests the data largely sits in a subspace having dimension between 5 to&amp;nbsp;10.&lt;/p&gt;
&lt;h3&gt;G20 economic summary&amp;nbsp;stats&lt;/h3&gt;
&lt;h4&gt;Loading and compressing the&amp;nbsp;data&lt;/h4&gt;
&lt;p&gt;In this section, we explore economic summary statistics on the 19 individual countries belonging to the G20 [5]. We scraped this data from data.un.org &amp;#8212; for example, the link used for the United States can be found &lt;a href="http://data.un.org/en/iso/us.html"&gt;here&lt;/a&gt;. Our aim here will be to illustrate how compression algorithms can be used to aid in the visualization of a data set: Plotting the rows of a data set allows one to quickly get a sense for the relationship between them (here, the different G20 countries). Because we cannot plot in more than two or three dimensions, compression is a necessary first step in this&amp;nbsp;process.&lt;/p&gt;
&lt;p&gt;A sample row from our data set is given below &amp;#8212; the values for&amp;nbsp;Argentina.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;GDP growth rate(annual %, const. 2005 prices) 2.40
GDP per capita(current US$) 14564
Economy: Agriculture(% of GVA) 6
Economy: Industry(% of GVA) 27.8
Economy: Services and other activity(% of GVA) 66.2
Employment: Agriculture(% of employed) 2
Employment: Industry(% of employed) 24.8
Employment: Services(% of employed) 73.1
Unemployment(% of labour force) 6.5
CPI: Consumer Price Index(2000=100) 332
Agricultural production index(2004-2006=100) 119
Food production index(2004-2006=100) 119
International trade: Exports(million US$) / GPV 0.091
International trade: Imports(million US$) / GPV 0.088
Balance of payments, current account / GPV -0.025
Labour force participation(female) pop. %) 48.6
Labour force participation(male) pop. %) 74.4
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Comparing each of the 19 countries across these 17 fields would be a complicated task. However, by considering a plot like Fig. 3 for this data set, we learned that many of these fields are highly correlated (plot not shown). This means that we can indeed get a reasonable, approximate understanding of the relationship between these economies by compressing down to two dimensions and plotting the result. The code to obtain these compressions&amp;nbsp;follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;linselect&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;FwdSelect&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.decomposition&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;PCA&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;StandardScaler&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;

&lt;span class="c1"&gt;# LOADING THE DATA&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;g20.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index_col&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;
&lt;span class="n"&gt;countries&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;

&lt;span class="c1"&gt;# FEATURE SELECTION&lt;/span&gt;
&lt;span class="n"&gt;selector&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;FwdSelect&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;selector&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;x1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;selector&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ordered_features&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;

&lt;span class="c1"&gt;# PRINCIPAL COMPONENT ANALYSIS&lt;/span&gt;
&lt;span class="n"&gt;pca&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;PCA&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;StandardScaler&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;x2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pca&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The plots of the &lt;span class="math"&gt;\((x_1, y_1)\)&lt;/span&gt; and &lt;span class="math"&gt;\((x_2, y_2)\)&lt;/span&gt; compressions obtained above are given in Fig.&amp;nbsp;4.&lt;/p&gt;
&lt;h4&gt;Visualizing and interpreting the compressed&amp;nbsp;data&lt;/h4&gt;
&lt;p&gt;The first thing to note about Fig. 4 is that the geometries of the upper (feature selection) and lower (&lt;span class="caps"&gt;PCA&lt;/span&gt;) plots are very similar &amp;#8212; the neighbors of each country are the same in the two plots. As we know from our discussion above, the first two &lt;span class="caps"&gt;PCA&lt;/span&gt; components must give a stronger compressed representation of the data than is obtained from the feature selection solution. However, given that similar country relationships are suggested by the two plots, the upper, feature selection view might be preferred. &lt;em&gt;This is because its axes retain their original meaning and are relatively easy to interpret&lt;/em&gt;: The y-axis is a measure of the relative scale of international trade within each of the individual economies and the x-axis is a measure of the internal makeup of the&amp;nbsp;economies.&lt;/p&gt;
&lt;p&gt;Examining the upper, feature selection plot of Fig. 4, a number of interesting insights can be found. One timely observation: International trade exports are a lower percentage of &lt;span class="caps"&gt;GDP&lt;/span&gt; for the &lt;span class="caps"&gt;US&lt;/span&gt; than for any other country considered (for imports, it is third, just after Argentina and Brazil). This observation might be related to the &lt;span class="caps"&gt;US&lt;/span&gt; administration&amp;#8217;s recent willingness to engage in trading tariff increases with other countries. Nations in the same quadrant include Great Britain (gb), Japan (jp), and Australia (au) &amp;#8212; each relatively industrialized and geographically isolated nations. In the opposite limits, we have Germany (de) and India (in). The former is relatively industrial and not isolated, while the latter&amp;#8217;s economy weights agriculture relatively&amp;nbsp;highly.&lt;/p&gt;
&lt;h4&gt;Summary&lt;/h4&gt;
&lt;p&gt;In this section, we illustrated a general analysis method that allows one to quickly gain insight into a data set: Visual study of the compressed data via a plot. Using this approach, we first found here that the G20 nations are best differentiated economically by considering how important international trade is to their economies and also the makeup of their economies (agricultural or other) &amp;#8212; i.e., these are the two features that best explain the full data set of 17 columns that we started with. Plotting the data across these two variables and considering the commonalities of neighboring countries, we were able to identify some natural hypotheses influencing the individual economies. Specifically, geography appears to inform at least one of their key characteristics: more isolated countries often trade less. This is an interesting insight, and one that is quickly arrived at through the compression / plotting&amp;nbsp;strategy.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2018/06/pca_linselect_g20.jpg"&gt;&lt;img alt="pca_linselect_g20" src="https://efavdb.com/wp-content/uploads/2018/06/pca_linselect_g20.jpg"&gt;&lt;/a&gt;
&lt;strong&gt;Fig. 4&lt;/strong&gt;. Plots of the compressed economic summary statistics on the G20 nations, taken from data.un.org: &lt;code&gt;linselect&lt;/code&gt; unsupervised feature selection (upper) and &lt;span class="caps"&gt;PCA&lt;/span&gt;&amp;nbsp;(lower).&lt;/p&gt;
&lt;h2&gt;Discussion&lt;/h2&gt;
&lt;p&gt;In this post, we have seen that carrying out compressions on a data set can provide insight into the original data. By examining the &lt;span class="caps"&gt;PCA&lt;/span&gt; components, we gain access to the collective fluctuations present within the data. The feature selection solution returns a minimal subset of the original features that captures the broad stroke information contained in the original full set &amp;#8212; in cases where clusters are present, the minimal set contains a representative from each. Both methods allow one to determine the effective dimension of a given data set &amp;#8212; when applied to raw data sets, this is often much lower than the apparent dimension due to heavy&amp;nbsp;redundancy.&lt;/p&gt;
&lt;p&gt;In general, compressing a data set down into lower dimensions will make the data easier to interpret. We saw in this in the second, G20 economic example above, where a feature set was originally provided that had many columns. Compressing this down into two-dimensions quickly gave us a sense of the relationships between the different economies. The &lt;span class="caps"&gt;PCA&lt;/span&gt; and feature selection solutions gave similar plots there, but the feature selection solution had the extra benefit of providing easily interpreted&amp;nbsp;axes.&lt;/p&gt;
&lt;p&gt;When one’s goal is to use compression for operational efficiency gains, the appropriate dimension can be identified by plotting the variance explained versus compression dimension. Because &lt;span class="caps"&gt;PCA&lt;/span&gt; is unconstrained, it will give a stronger compression at any dimension. However, the feature selection approach has its own operational advantages: Once a representative subset of features has been identified, one can often simply stop tracking the others. Doing this can result in a huge cost savings for large data pipelines. A similar savings is not possible for &lt;span class="caps"&gt;PCA&lt;/span&gt;, because evaluation of the &lt;span class="caps"&gt;PCA&lt;/span&gt; components requires one to first evaluate each of the original feature / column values for a given data point. A similar consideration is also important in some applications: For example, when developing a stock portfolio, transaction costs may make it prohibitively expensive to purchase all of the stocks present in a given sector. By purchasing only a representative subset, a minimal portfolio can be constructed without incurring a substantial transaction cost&amp;nbsp;burden.&lt;/p&gt;
&lt;p&gt;In summary, the two compression methods we have considered here are very similar, but subtly different. Appreciating these differences allows one to choose the best approach for a given&amp;nbsp;application.&lt;/p&gt;
&lt;h2&gt;Appendix: Compression as&amp;nbsp;projection&lt;/h2&gt;
&lt;p&gt;We can see that the composite linear compression-decompression operator is a projection operator as follows: If &lt;span class="math"&gt;\(X\)&lt;/span&gt; is our data array, the general equations describing compression and decompression&amp;nbsp;are,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
\label{A1} \tag{A1}
X_{compressed} &amp;amp;=&amp;amp; X \cdot M_{compression} \\
\label{A2} \tag{A2}
X_{approx} &amp;amp;=&amp;amp; X_{compressed} \cdot M_{decompression}.
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Here, &lt;span class="math"&gt;\(M_{compression}\)&lt;/span&gt; is an &lt;span class="math"&gt;\(n \times k\)&lt;/span&gt; matrix and &lt;span class="math"&gt;\(M_{decompression}\)&lt;/span&gt; is a &lt;span class="math"&gt;\(k \times n\)&lt;/span&gt; matrix. The squared error of the approximation&amp;nbsp;is,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
\Lambda &amp;amp;=&amp;amp; \sum_{i,j} \left (X_{ij} - X_{approx, ij}\right)^2 \\
&amp;amp;=&amp;amp; \sum_j \Vert X_j - X_{compressed} \cdot M_{decompression, j} \Vert^2. \label{A3} \tag{A3}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
This second line here shows that we can minimize the entire squared error by minimizing each of the column squared errors independently. Further, each of the column level minimizations is equivalent to a least-squares linear regression problem: We treat the column vector &lt;span class="math"&gt;\(M_{compressions, j}\)&lt;/span&gt; as an unknown coefficient vector, and attempt to set these so that the squared error of the fit to &lt;span class="math"&gt;\(X_j\)&lt;/span&gt; &amp;#8212; using the columns of &lt;span class="math"&gt;\(X_{compressed}\)&lt;/span&gt; as features &amp;#8212; is minimized. We&amp;#8217;ve worked out the least-squares linear fit solution in &lt;a href="http://efavdb.github.io/linear-regression"&gt;another post&lt;/a&gt; (it&amp;#8217;s also a well-known result). Plugging this result in, we get the optimal &lt;span class="math"&gt;\(M_{decompression}\)&lt;/span&gt;,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \label{A4}
M_{decompression}^* &amp;amp;=&amp;amp; \left ( X_{compressed}^T X_{compressed} \right)^{-1} X_{compressed}^T X \tag{A4}
\\
&amp;amp;=&amp;amp; \left ( M_{compression}^T X^T X M_{compression} \right)^{-1} M_{compression}^T X^T X.
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
To obtain the second line here, we have used (\ref{A1}), the definition of &lt;span class="math"&gt;\(X_{compressed}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;What happens if we try to compress our approximate matrix a second time? Nothing: The matrix product &lt;span class="math"&gt;\(M_{compression} M_{decompression}^*\)&lt;/span&gt; is a projection operator. That is, it satisfies the&amp;nbsp;condition
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
(M_{compression} M_{decompression}^*)^2 = M_{compression} M_{decompression}^*. \label{A5} \tag{A5}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
This result is easy enough to confirm using (\ref{A4}). What (\ref{A5}) means geometrically is that our compression operator projects a point in &lt;span class="math"&gt;\(n\)&lt;/span&gt;-dimensional space onto a subspace of dimension &lt;span class="math"&gt;\(k\)&lt;/span&gt;. Once a point sits in this subspace, hitting the point with the composite operator has no effect, as the new point already sits in the projected subspace. This is consistent with our 2-d cartoon depicting the effect of &lt;span class="caps"&gt;PCA&lt;/span&gt; and &lt;code&gt;linselect&lt;/code&gt;, above. However, this is also true for general choices of &lt;span class="math"&gt;\(M_{compression}\)&lt;/span&gt;, provided we use the optimal &lt;span class="math"&gt;\(M_{decompression}\)&lt;/span&gt; associated with&amp;nbsp;it.&lt;/p&gt;
&lt;h2&gt;Footnotes&lt;/h2&gt;
&lt;p&gt;[1] For a discussion on how &lt;span class="caps"&gt;PCA&lt;/span&gt; selects its &lt;span class="math"&gt;\(k\)&lt;/span&gt; components, see our prior &lt;a href="http://efavdb.github.io/principal-component-analysis"&gt;post&lt;/a&gt; on the topic. To identify good feature subsets, &lt;code&gt;linselect&lt;/code&gt; uses the stepwise selection strategy. This is described in its &lt;a href="https://github.com/EFavDB/linselect"&gt;readme&lt;/a&gt;. Here, we simply use the forward selection approach, but &lt;code&gt;linselect&lt;/code&gt; supports fairly general stepwise search&amp;nbsp;protocols.&lt;/p&gt;
&lt;p&gt;[2] The tickers included are: &lt;span class="caps"&gt;AAPL&lt;/span&gt;, &lt;span class="caps"&gt;ADBE&lt;/span&gt;, &lt;span class="caps"&gt;ADP&lt;/span&gt;, &lt;span class="caps"&gt;ADSK&lt;/span&gt;, &lt;span class="caps"&gt;AMAT&lt;/span&gt;, &lt;span class="caps"&gt;AMZN&lt;/span&gt;, &lt;span class="caps"&gt;ASML&lt;/span&gt;, &lt;span class="caps"&gt;ATVI&lt;/span&gt;, &lt;span class="caps"&gt;AVGO&lt;/span&gt;, &lt;span class="caps"&gt;BABA&lt;/span&gt;, &lt;span class="caps"&gt;BIDU&lt;/span&gt;, &lt;span class="caps"&gt;CRM&lt;/span&gt;, &lt;span class="caps"&gt;CSCO&lt;/span&gt;, &lt;span class="caps"&gt;CTSH&lt;/span&gt;, &lt;span class="caps"&gt;EA&lt;/span&gt;, &lt;span class="caps"&gt;FB&lt;/span&gt;, &lt;span class="caps"&gt;GOOG&lt;/span&gt;, &lt;span class="caps"&gt;GPRO&lt;/span&gt;, &lt;span class="caps"&gt;HPE&lt;/span&gt;, &lt;span class="caps"&gt;HPQ&lt;/span&gt;, &lt;span class="caps"&gt;IBM&lt;/span&gt;, &lt;span class="caps"&gt;INFY&lt;/span&gt;, &lt;span class="caps"&gt;INTC&lt;/span&gt;, &lt;span class="caps"&gt;INTU&lt;/span&gt;, &lt;span class="caps"&gt;ITW&lt;/span&gt;, &lt;span class="caps"&gt;LRCX&lt;/span&gt;, &lt;span class="caps"&gt;MSFT&lt;/span&gt;, &lt;span class="caps"&gt;NFLX&lt;/span&gt;, &lt;span class="caps"&gt;NOK&lt;/span&gt;, &lt;span class="caps"&gt;NVDA&lt;/span&gt;, &lt;span class="caps"&gt;NXPI&lt;/span&gt;, &lt;span class="caps"&gt;OMC&lt;/span&gt;, &lt;span class="caps"&gt;ORCL&lt;/span&gt;, &lt;span class="caps"&gt;PANW&lt;/span&gt;, &lt;span class="caps"&gt;PYPL&lt;/span&gt;, &lt;span class="caps"&gt;QCOM&lt;/span&gt;, &lt;span class="caps"&gt;SAP&lt;/span&gt;, &lt;span class="caps"&gt;SNAP&lt;/span&gt;, &lt;span class="caps"&gt;SQ&lt;/span&gt;, &lt;span class="caps"&gt;SYMC&lt;/span&gt;, T, &lt;span class="caps"&gt;TSLA&lt;/span&gt;, &lt;span class="caps"&gt;TSM&lt;/span&gt;, &lt;span class="caps"&gt;TWTR&lt;/span&gt;, &lt;span class="caps"&gt;TXN&lt;/span&gt;, &lt;span class="caps"&gt;VMW&lt;/span&gt;, &lt;span class="caps"&gt;VZ&lt;/span&gt;, &lt;span class="caps"&gt;WDAY&lt;/span&gt;, &lt;span class="caps"&gt;WDC&lt;/span&gt;, and &lt;span class="caps"&gt;ZNGA&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;[3] Workday (&lt;span class="caps"&gt;WDAY&lt;/span&gt;) is a SaaS company that offers a product to businesses, Paypal (&lt;span class="caps"&gt;PYPL&lt;/span&gt;) is a company that provides payments infrastructure supporting e-commerce, Amazon (&lt;span class="caps"&gt;AMZN&lt;/span&gt;) is an e-commerce company, Lam Research (&lt;span class="caps"&gt;LRCX&lt;/span&gt;) makes chips, and Hewlett-Packard (&lt;span class="caps"&gt;HPQ&lt;/span&gt;) makes computers. Each of these are representatives of a different&amp;nbsp;sub-sector.&lt;/p&gt;
&lt;p&gt;[4] We can also get a sense of the compression error by plotting the compressed traces for one of the stocks. &lt;a href="https://efavdb.com/wp-content/uploads/2018/06/sq.png"&gt;&lt;img alt="sq" src="https://efavdb.com/wp-content/uploads/2018/06/sq.png"&gt;&lt;/a&gt; The plot at right does this for Square inc. The ups and downs of &lt;span class="caps"&gt;SQ&lt;/span&gt; are largely captured by both methods. However, some refined details are lost in the compressions. Similar accuracy levels are seen for each of the other stocks in the full set (not shown&amp;nbsp;here).&lt;/p&gt;
&lt;p&gt;[5] The missing twentieth member of the G20 is the &lt;span class="caps"&gt;EU&lt;/span&gt;. We don&amp;#8217;t consider the &lt;span class="caps"&gt;EU&lt;/span&gt; here simply because the site we scraped from does not have a page dedicated to&amp;nbsp;it.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="linselect"></category><category term="linselect"></category><category term="methods"></category><category term="python"></category></entry><entry><title>linselect demo: a tech sector stock analysis</title><link href="https://efavdb.com/linselect-demo" rel="alternate"></link><published>2018-05-31T14:17:00-07:00</published><updated>2018-05-31T14:17:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2018-05-31:/linselect-demo</id><summary type="html">&lt;p&gt;This is a tutorial post relating to our python feature selection package, &lt;code&gt;linselect&lt;/code&gt;. The package allows one to easily identify minimal, informative feature subsets within a given data&amp;nbsp;set.&lt;/p&gt;
&lt;p&gt;Here, we demonstrate &lt;code&gt;linselect&lt;/code&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt;s basic &lt;span class="caps"&gt;API&lt;/span&gt; by exploring the relationship between the daily percentage lifts of 50 tech stocks over …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This is a tutorial post relating to our python feature selection package, &lt;code&gt;linselect&lt;/code&gt;. The package allows one to easily identify minimal, informative feature subsets within a given data&amp;nbsp;set.&lt;/p&gt;
&lt;p&gt;Here, we demonstrate &lt;code&gt;linselect&lt;/code&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt;s basic &lt;span class="caps"&gt;API&lt;/span&gt; by exploring the relationship between the daily percentage lifts of 50 tech stocks over one trading year. We will be interested in identifying minimal stock subsets that can be used to predict the lifts of the&amp;nbsp;others.&lt;/p&gt;
&lt;p&gt;This is a demonstration walkthrough, with commentary and interpretation throughout. See the package docs folder for docstrings that succinctly detail the &lt;span class="caps"&gt;API&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Contents:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Load the data and examine some stock&amp;nbsp;traces&lt;/li&gt;
&lt;li&gt;FwdSelect, RevSelect; supervised, single&amp;nbsp;target&lt;/li&gt;
&lt;li&gt;FwdSelect, RevSelect; supervised, multiple&amp;nbsp;targets&lt;/li&gt;
&lt;li&gt;FwdSelect, RevSelect;&amp;nbsp;unsupervised&lt;/li&gt;
&lt;li&gt;GenSelect&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The data and a Jupyter notebook containing the code for this demo are available on our github, &lt;a href="https://github.com/EFavDB/linselect_demos"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;linselect&lt;/code&gt; package can be found on our github, &lt;a href="https://github.com/efavdb/linselect"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;1 - Load the data and examine some stock&amp;nbsp;traces&lt;/h2&gt;
&lt;p&gt;In this tutorial, we will explore using &lt;code&gt;linselect&lt;/code&gt; to carry out various feature selection tasks on a prepared data set of daily percentage lifts for 50 of the largest tech stocks. This covers data from 2017-04-18 to 2018-04-13. In this section, we load the data and take a look at a couple of the stock traces that we will be&amp;nbsp;studying.&lt;/p&gt;
&lt;h3&gt;Load&amp;nbsp;data&lt;/h3&gt;
&lt;p&gt;The code snippet below loads the data and shows a small&amp;nbsp;sample.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# load packages  &lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;linselect&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;FwdSelect&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;RevSelect&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;GenSelect&lt;/span&gt;  
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;  
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;  
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;

&lt;span class="c1"&gt;# load the data, print out a sample  &lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;stocks.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="nb"&gt;print&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iloc&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  
&lt;span class="nb"&gt;print&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;

&lt;span class="c1"&gt;# date AAPL ADBE ADP ADSK  &lt;/span&gt;
&lt;span class="c1"&gt;# 0 2017-04-18 -0.004442 -0.001385 0.000687 0.004884  &lt;/span&gt;
&lt;span class="c1"&gt;# 1 2017-04-19 -0.003683 0.003158 0.001374 0.017591  &lt;/span&gt;
&lt;span class="c1"&gt;# 2 2017-04-20 0.012511 0.009215 0.009503 0.005459  &lt;/span&gt;
&lt;span class="c1"&gt;# (248, 51)  &lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The last line here shows that there were 248 trading days in the range&amp;nbsp;considered.&lt;/p&gt;
&lt;h3&gt;Plot some stock&amp;nbsp;traces&lt;/h3&gt;
&lt;p&gt;The plot below shows Apple&amp;#8217;s and Google&amp;#8217;s daily lifts on top of each other, over our full date range (the code for the plot can be found in our notebook). Visually, it&amp;#8217;s clear that the two are highly correlated &amp;#8212; when one goes up or down, the other tends to as well. This suggests that it should be possible to get a good fit to any one of the stocks using the changes in each of the other&amp;nbsp;stocks.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2018/05/apple_google.jpg"&gt;&lt;img alt="apple_google" src="https://efavdb.com/wp-content/uploads/2018/05/apple_google.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In general, a stock&amp;#8217;s daily price change should be a function of the market at large, the behavior of its market segment(s) and sub-segment(s), and some idiosyncratic behavior special to the company in question. Given this intuition, it seems reasonable to expect one to be able to fit a given stock given the lifts from just a small subset of the other stocks &amp;#8212; stocks representative of the sectors relevant to the stock in question. Adding multiple stocks from each segment shouldn&amp;#8217;t provide much additional value since these should be redundant. We&amp;#8217;ll confirm this intuition below and use &lt;code&gt;linselect&lt;/code&gt; to identify these optimal&amp;nbsp;subsets.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lesson&lt;/strong&gt;: The fluctuations of related stocks are often highly correlated. Below, we will be using &lt;code&gt;linselect&lt;/code&gt; to find minimal subsets of the 50 stocks that we can use to develop good linear fits to one, multiple, or all of the&amp;nbsp;others.&lt;/p&gt;
&lt;h2&gt;2 - FwdSelect and RevSelect; supervised, single&amp;nbsp;target&lt;/h2&gt;
&lt;p&gt;Goal: Demonstrate how to identify subsets of the stocks that can be used to fit a given target stock&amp;nbsp;well.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First we carry out a &lt;code&gt;FwdSelect&lt;/code&gt; fit to identify good&amp;nbsp;choices.&lt;/li&gt;
&lt;li&gt;Next, we compare the &lt;code&gt;FwdSelect&lt;/code&gt; and &lt;code&gt;RevSelect&lt;/code&gt; results&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Forward selection applied to &lt;span class="caps"&gt;AAPL&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;The code snippet below uses our forward selection class, &lt;code&gt;FwdSelect&lt;/code&gt; to seek the best feature subsets to fit &lt;span class="caps"&gt;AAPL&lt;/span&gt;&amp;#8217;s&amp;nbsp;performance.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Define X, y variables  &lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_feature_tickers&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;targets&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;  
    &lt;span class="n"&gt;all_tickers&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iloc&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;  
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;all_tickers&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;targets&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;TARGET_TICKERS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;AAPL&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  
&lt;span class="n"&gt;FEATURE_TICKERS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;get_feature_tickers&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;TARGET_TICKERS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;FEATURE_TICKERS&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;  
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;TARGET_TICKERS&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;

&lt;span class="c1"&gt;# Forward step-wise selection  &lt;/span&gt;
&lt;span class="n"&gt;selector&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;FwdSelect&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  
&lt;span class="n"&gt;selector&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Print out main results of selection process (ordered feature indices, CODs)  &lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt; &lt;span class="n"&gt;selector&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ordered_features&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  
&lt;span class="nb"&gt;print&lt;/span&gt; &lt;span class="n"&gt;selector&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ordered_cods&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# [25, 7, 41]  &lt;/span&gt;
&lt;span class="c1"&gt;# [0.43813848, 0.54534304, 0.58577418]  &lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The last two lines above print out the main outputs of &lt;code&gt;FwdSelect&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;ordered_features&lt;/code&gt; list provides the indices of the features, ranked by the algorithm. The first index shown provides the best possible single feature fit to &lt;span class="caps"&gt;AAPL&lt;/span&gt;, the second index provides the next best addition, etc. Note that we can get the tickers corresponding to these indices using:&lt;br&gt;
&lt;code&gt;python  
    print [FEATURE_TICKERS[i] for i in selector.ordered_features[:3]]  
    # ['MSFT' 'AVGO' 'TSM']&lt;/code&gt;&lt;br&gt;
    A little thought plus a Google search rationalizes why these might be the top three predictors for &lt;span class="caps"&gt;AAPL&lt;/span&gt;: First, Microsoft is probably a good representative of the large-scale tech sector, and second the latter two companies work closely with Apple. &lt;span class="caps"&gt;AVGO&lt;/span&gt; (Qualcomm) made Apple&amp;#8217;s modem chips until very recently, while &lt;span class="caps"&gt;TSM&lt;/span&gt; (Taiwan semi-conductor) makes the processors for iphones and ipads &amp;#8212; and may perhaps soon also provide the CPUs for all Apple computers. Apparently, we can predict &lt;span class="caps"&gt;APPL&lt;/span&gt; performance using only a combination of (a) a read on the tech sector at large, plus (b) a bit of idiosyncratic information also present in &lt;span class="caps"&gt;APPL&lt;/span&gt;&amp;#8217;s partner&amp;nbsp;stocks.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;ordered_cods&lt;/code&gt; list records the coefficient of determination (&lt;span class="caps"&gt;COD&lt;/span&gt; or R^2) of the fits in question &amp;#8212; the first number gives the &lt;span class="caps"&gt;COD&lt;/span&gt; obtained with just &lt;span class="caps"&gt;MSFT&lt;/span&gt;, the second with &lt;span class="caps"&gt;MSFT&lt;/span&gt; and &lt;span class="caps"&gt;AVGO&lt;/span&gt;,&amp;nbsp;etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A plot of the values in &lt;code&gt;ordered_cods&lt;/code&gt; versus feature count is given below. Here, we have labeled the x-axis with the tickers corresponding to the elements of our &lt;code&gt;selector.ordered_features&lt;/code&gt;. We see that the top three features almost fit &lt;span class="caps"&gt;AAPL&lt;/span&gt;&amp;#8217;s performance as well as the full&amp;nbsp;set!&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2018/05/apple.png"&gt;&lt;img alt="apple" src="https://efavdb.com/wp-content/uploads/2018/05/apple.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lesson&lt;/strong&gt;: We can often use &lt;code&gt;linselect&lt;/code&gt; to significantly reduce the dimension of a given feature set, with minimal cost in performance. This can be used to compress a data set and can also improve our understanding of the problem&amp;nbsp;considered.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lesson&lt;/strong&gt;: To get a feel for the effective number of useful features we have at hand, we can plot the output &lt;code&gt;ordered_cods&lt;/code&gt; versus feature&amp;nbsp;count.&lt;/p&gt;
&lt;h3&gt;Compare forward and reverse selection applied to &lt;span class="caps"&gt;TSLA&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;The code snippet below applies both &lt;code&gt;FwdSelect&lt;/code&gt; and &lt;code&gt;RevSelect&lt;/code&gt; to seek minimal subsets that fit Tesla&amp;#8217;s daily lifts well. The outputs are plotted below this. This shows that &lt;code&gt;FwdSelect&lt;/code&gt; performs slightly better when two or fewer features are included here, but that &lt;code&gt;RevSelect&lt;/code&gt; finds better subsets after&amp;nbsp;that.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lesson&lt;/strong&gt;: In general, we expect forward selection to work better when looking for small subsets and reverse selection to perform better at large&amp;nbsp;subsets.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Define X, y variables  &lt;/span&gt;
&lt;span class="n"&gt;TARGET_TICKERS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;TSLA&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  
&lt;span class="n"&gt;FEATURE_TICKERS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;get_feature_tickers&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;TARGET_TICKERS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;FEATURE_TICKERS&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;  
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;TARGET_TICKERS&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;

&lt;span class="c1"&gt;# Forward step-wise selection  &lt;/span&gt;
&lt;span class="n"&gt;selector&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;FwdSelect&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  
&lt;span class="n"&gt;selector&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Reverse step-wise selection  &lt;/span&gt;
&lt;span class="n"&gt;selector2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;RevSelect&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  
&lt;span class="n"&gt;selector2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2018/05/rev2.jpg"&gt;&lt;img alt="rev2" src="https://efavdb.com/wp-content/uploads/2018/05/rev2.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;3 - FwdSelect and RevSelect; supervised, multiple&amp;nbsp;targets&lt;/h2&gt;
&lt;p&gt;In the code below, we seek feature subsets that perform well when fitting multiple targets&amp;nbsp;simultaneously.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lesson&lt;/strong&gt;: &lt;code&gt;linselect&lt;/code&gt; can be used to find minimal feature subsets useful for fitting multiple targets. The optimal, &amp;#8220;perfect score&amp;#8221; &lt;span class="caps"&gt;COD&lt;/span&gt; in this case is equal to number of targets (three in our&amp;nbsp;example).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Define X, y variables  &lt;/span&gt;
&lt;span class="n"&gt;TARGET_TICKERS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;TSLA&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;ADP&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;NFLX&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  
&lt;span class="n"&gt;FEATURE_TICKERS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;get_feature_tickers&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;TARGET_TICKERS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;FEATURE_TICKERS&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;  
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;TARGET_TICKERS&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;

&lt;span class="c1"&gt;# Forward step-wise selection  &lt;/span&gt;
&lt;span class="n"&gt;selector&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;FwdSelect&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  
&lt;span class="n"&gt;selector&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Reverse step-wise selection  &lt;/span&gt;
&lt;span class="n"&gt;selector2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;RevSelect&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  
&lt;span class="n"&gt;selector2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2018/05/multiple.jpg"&gt;&lt;img alt="multiple" src="https://efavdb.com/wp-content/uploads/2018/05/multiple.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;4 - FwdSelect and RevSelect;&amp;nbsp;unsupervised&lt;/h2&gt;
&lt;p&gt;Here, we seek those features that give us a best fit to / linear representation of the whole set. This goal is analogous to that addressed by &lt;span class="caps"&gt;PCA&lt;/span&gt;, but is a feature selection variant: Whereas &lt;span class="caps"&gt;PCA&lt;/span&gt; returns a set of linear combinations of the original features, the approach here will return a subset of the original features. This has the benefit of leaving one with a feature subset that is&amp;nbsp;interpretable.&lt;/p&gt;
&lt;p&gt;(Note: See [1] for more examples like this. There, I show that if you try to fit smoothed versions of the stock performances, very good, small subsets can be found. Without smoothing, noise obscures this&amp;nbsp;point).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lesson&lt;/strong&gt;: Unsupervised selection seeks to find those features that best describe the full data set &amp;#8212; a feature selection analog of &lt;span class="caps"&gt;PCA&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lesson&lt;/strong&gt;: Again, a perfect &lt;span class="caps"&gt;COD&lt;/span&gt; score is equal to the number of targets. In the unsupervised case, this is also the number of features (50 in our&amp;nbsp;example).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Set X equal to full data set.  &lt;/span&gt;
&lt;span class="n"&gt;ALL_TICKERS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iloc&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;ALL_TICKERS&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;

&lt;span class="c1"&gt;# Stepwise regressions  &lt;/span&gt;
&lt;span class="n"&gt;selector&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;FwdSelect&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  
&lt;span class="n"&gt;selector&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;selector2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;RevSelect&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  
&lt;span class="n"&gt;selector2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2018/05/unsupervised.jpg"&gt;&lt;img alt="unsupervised" src="https://efavdb.com/wp-content/uploads/2018/05/unsupervised.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;5 -&amp;nbsp;GenSelect&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;GenSelect&lt;/code&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt;s &lt;span class="caps"&gt;API&lt;/span&gt; is designed to expose the full flexibility of the efficient linear stepwise algorithm. Because of this, its &lt;span class="caps"&gt;API&lt;/span&gt; is somewhat more complex than that of &lt;code&gt;FwdSelect&lt;/code&gt; and &lt;code&gt;RevSelect&lt;/code&gt;. Here, our aim is to quickly demo this &lt;span class="caps"&gt;API&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The Essential&amp;nbsp;ingredients:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We pass only a single data matrix &lt;code&gt;X&lt;/code&gt;, and must specify which columns are the predictors and which are&amp;nbsp;targets.&lt;/li&gt;
&lt;li&gt;Because we might sweep up and down, we cannot define an &lt;code&gt;ordered_features&lt;/code&gt; list as in &lt;code&gt;FwdSelect&lt;/code&gt; and &lt;code&gt;RevSelect&lt;/code&gt; (the best subset of size three now may not contain the features in the best subset of size two). Instead, &lt;code&gt;GenSelect&lt;/code&gt; maintains a dictionary &lt;code&gt;best_results&lt;/code&gt; that stores information on the best results seen so far for each possible feature count. The keys of this dictionary correspond to the possible feature set sizes. The values are also dictionaries, each having two keys: &lt;code&gt;s&lt;/code&gt; and &lt;code&gt;cod&lt;/code&gt;. These specify the best feature subset seen so far with size equal to the outer key, and the corresponding &lt;span class="caps"&gt;COD&lt;/span&gt;,&amp;nbsp;respectively.&lt;/li&gt;
&lt;li&gt;We can move back and forth, adding features to or removing them from the predictor set. We can specify the search protocol for doing&amp;nbsp;this.&lt;/li&gt;
&lt;li&gt;We can reposition our search to any predictor set location and continue the search from&amp;nbsp;there.&lt;/li&gt;
&lt;li&gt;We can access the costs of each possible move from our current location, without&amp;nbsp;stepping.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If an &lt;span class="math"&gt;\(m \times n\)&lt;/span&gt; data matrix &lt;code&gt;X&lt;/code&gt; is passed to &lt;code&gt;GenSelect&lt;/code&gt;, three Boolean arrays define the state of the&amp;nbsp;search.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;s&lt;/code&gt; &amp;#8212; This array specifies which of the columns are currently being used as&amp;nbsp;predictors.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;targets&lt;/code&gt; &amp;#8212; This specifies which of the columns are the target&amp;nbsp;variables.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mobile&lt;/code&gt; &amp;#8212; This specifies which of the columns are locked into or out of our fit &amp;#8212; those that are not mobile are marked &lt;code&gt;False&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note: We usually want the targets to not be mobile &amp;#8212; though this is not the case in unsupervised applications. One might sometimes also want to lock certain features into the predictor set, and the &lt;code&gt;mobile&lt;/code&gt; parameter can be used to accomplish&amp;nbsp;this.&lt;/p&gt;
&lt;h3&gt;Use GenSelect to carry out a forward sweep for &lt;span class="caps"&gt;TSLA&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;The code below carries out a single forward sweep for &lt;span class="caps"&gt;TSLA&lt;/span&gt;. Note that the &lt;code&gt;protocol&lt;/code&gt; argument of &lt;code&gt;search&lt;/code&gt; is set to &lt;code&gt;(1, 0)&lt;/code&gt;, which gives a forward search (see docstrings). For this reason, our results match those of &lt;code&gt;FwdSelect&lt;/code&gt; at this&amp;nbsp;point.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lesson&lt;/strong&gt;: Setting up a basic &lt;code&gt;GenSelect&lt;/code&gt; call requires defining a few input&amp;nbsp;parameters.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Define X  &lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;ALL_TICKERS&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;

&lt;span class="c1"&gt;# Define targets and mobile Boolean arrays  &lt;/span&gt;
&lt;span class="n"&gt;TARGET_TICKERS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;TSLA&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  
&lt;span class="n"&gt;FEATURE_TICKERS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;get_feature_tickers&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;TARGET_TICKERS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;targets&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;in1d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ALL_TICKERS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;TARGET_TICKERS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;mobile&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;in1d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ALL_TICKERS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;FEATURE_TICKERS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Set up search with an initial \`position\`. Then search.  &lt;/span&gt;
&lt;span class="n"&gt;selector&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GenSelect&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  
&lt;span class="n"&gt;selector&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;position&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mobile&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;mobile&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;targets&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;targets&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;selector&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;search&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;protocol&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;steps&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# Review best 3 feature set found  &lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ALL_TICKERS&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="n"&gt;selector&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_results&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;s&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt; &lt;span class="n"&gt;selector&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_results&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;cod&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# [&amp;#39;ATVI&amp;#39; &amp;#39;AVGO&amp;#39; &amp;#39;CTSH&amp;#39;] 0.225758  &lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Continue the search&amp;nbsp;above&lt;/h3&gt;
&lt;p&gt;A &lt;code&gt;GenSelect&lt;/code&gt; instance always retains a summary of the best results it has seen so far. This means that we can continue a search where we left off after a &lt;code&gt;search&lt;/code&gt; call completes. Below, we reposition our search and sweep back and forth to better explore a particular region. Note that this slightly improves our&amp;nbsp;result.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lesson&lt;/strong&gt;: We can carry out general search protocols using &lt;code&gt;GenSelect&lt;/code&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt;s &lt;code&gt;position&lt;/code&gt; and &lt;code&gt;search&lt;/code&gt; methods.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Reposition back to the best fit of size 3 seen above.  &lt;/span&gt;
&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;selector&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_results&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;s&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  
&lt;span class="n"&gt;selector&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;position&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Now sweep back and forth around there a few times.  &lt;/span&gt;
&lt;span class="n"&gt;STEPS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;  
&lt;span class="n"&gt;SWEEPS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;

&lt;span class="n"&gt;selector&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;search&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;protocol&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;steps&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;STEPS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;selector&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;search&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;protocol&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;STEPS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;STEPS&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;steps&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;SWEEPS&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;STEPS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Review best results found now with exactly N_RETAINED features (different from first pass in cell above?)  &lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ALL_TICKERS&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="n"&gt;selector&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_results&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;s&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt; &lt;span class="n"&gt;selector&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_results&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;cod&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# [&amp;#39;AMZN&amp;#39; &amp;#39;NVDA&amp;#39; &amp;#39;ZNGA&amp;#39;] 0.229958  &lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Compare to forward and reverse search&amp;nbsp;results&lt;/h3&gt;
&lt;p&gt;Below, we compare the &lt;span class="caps"&gt;COD&lt;/span&gt; values of our three&amp;nbsp;classes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lesson&lt;/strong&gt;: &lt;code&gt;GenSelect&lt;/code&gt; can be used to do a more thorough search than &lt;code&gt;FwdSelect&lt;/code&gt; and &lt;code&gt;RevSelect&lt;/code&gt;, and so can sometimes find better feature&amp;nbsp;subsets.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Get the best COD values seen for each feature set size from GenSelect search  &lt;/span&gt;
&lt;span class="n"&gt;gen_select_cods&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;  
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;  
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;selector&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_results&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  
        &lt;span class="k"&gt;break&lt;/span&gt;  
    &lt;span class="n"&gt;gen_select_cods&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;selector&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_results&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;cod&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# Plot cod versus feature set size.  &lt;/span&gt;
&lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gen_select_cods&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;GenSelect&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# FwdSelect again to get corresponding results.  &lt;/span&gt;
&lt;span class="n"&gt;selector2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;FwdSelect&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  
&lt;span class="n"&gt;selector2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;mobile&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;targets&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;selector2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ordered_cods&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;--&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;FwdSelect&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# RevSelect again to get corresponding results.  &lt;/span&gt;
&lt;span class="n"&gt;selector3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;RevSelect&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  
&lt;span class="n"&gt;selector3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;mobile&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;targets&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;selector3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ordered_cods&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;-.&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;RevSelect&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Coefficient of Determination (COD or R^2) for &lt;/span&gt;&lt;span class="si"&gt;{target}&lt;/span&gt;&lt;span class="s1"&gt; vs features retained&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;  
&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;, &amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;TARGET_TICKERS&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;  
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2018/05/comparison.jpg"&gt;&lt;img alt="comparison" src="https://efavdb.com/wp-content/uploads/2018/05/comparison.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Examine the cost of removing a feature from the predictor&amp;nbsp;set&lt;/h3&gt;
&lt;p&gt;Below, we reposition to the best feature set of size 10 seen so far. We then apply the method &lt;code&gt;reverse_cods&lt;/code&gt; to expose the cost of removing each of these individuals from the predictor set at this point. Were we to take a reverse step, the feature with the least cost would be the one taken (looks like &lt;span class="caps"&gt;FB&lt;/span&gt; from the&amp;nbsp;plot).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lesson&lt;/strong&gt;: We can easily access the costs associated with removing individual features from our current location. We can also access the &lt;span class="caps"&gt;COD&lt;/span&gt; gains associated with adding in new features by calling the &lt;code&gt;forward_cods&lt;/code&gt; method.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Reposition  &lt;/span&gt;
&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;selector&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_results&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;s&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  
&lt;span class="n"&gt;selector&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;position&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Get costs to remove a feature (see also \`forward_cods\` method)  &lt;/span&gt;
&lt;span class="n"&gt;costs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;selector&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reverse_cods&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  
&lt;span class="n"&gt;TICKERS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ALL_TICKERS&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="n"&gt;selector&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_results&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;s&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;

&lt;span class="c1"&gt;# Plot costs to remove each feature given current position  &lt;/span&gt;
&lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;costs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xticks&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;TICKERS&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;rotation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;90&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_xticklabels&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;TICKERS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2018/05/cost.jpg"&gt;&lt;img alt="cost" src="https://efavdb.com/wp-content/uploads/2018/05/cost.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Final&amp;nbsp;comments&lt;/h2&gt;
&lt;p&gt;In this tutorial, we&amp;#8217;ve illustrated many of the basic &lt;span class="caps"&gt;API&lt;/span&gt; calls available in &lt;code&gt;linselect&lt;/code&gt;. In a future tutorial post, we plan to illustrate some interesting use cases of some of these &lt;span class="caps"&gt;API&lt;/span&gt; calls &amp;#8212; e.g., how to use &lt;code&gt;GenSelect&lt;/code&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt;s arguments to explore the value of supplemental features, added to an already existing data&amp;nbsp;set.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;[1] J. Landy. Stepwise regression for unsupervised learning, 2017. &lt;a href="https://arxiv.org/abs/1706.03265"&gt;arxiv.1706.03265&lt;/a&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="linselect"></category><category term="linselect"></category></entry><entry><title>Making AI Interpretable with Generative Adversarial Networks</title><link href="https://efavdb.com/gans" rel="alternate"></link><published>2018-04-05T09:43:00-07:00</published><updated>2018-04-05T09:43:00-07:00</updated><author><name>Damien RJ</name></author><id>tag:efavdb.com,2018-04-05:/gans</id><summary type="html">&lt;p&gt;It has been quite awhile since I have posted, largely because soon after I started my job at Square I had a child! I hope to have some newer blog post soon. But along those lines I want to share a &lt;a href="https://medium.com/square-corner-blog/making-ai-interpretable-with-generative-adversarial-networks-766abc953edf"&gt;blog post&lt;/a&gt; I did with a coworker (&lt;a href="https://www.linkedin.com/in/juan-hernandez-025a5532/"&gt;Juan Hernandez …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;It has been quite awhile since I have posted, largely because soon after I started my job at Square I had a child! I hope to have some newer blog post soon. But along those lines I want to share a &lt;a href="https://medium.com/square-corner-blog/making-ai-interpretable-with-generative-adversarial-networks-766abc953edf"&gt;blog post&lt;/a&gt; I did with a coworker (&lt;a href="https://www.linkedin.com/in/juan-hernandez-025a5532/"&gt;Juan Hernandez&lt;/a&gt;) for Square that gives a taste of some of the cool data science work we have been up to. This post covers work we did to create a framework for making models interpretable.&lt;br&gt;
&lt;a href="https://medium.com/square-corner-blog/making-ai-interpretable-with-generative-adversarial-networks-766abc953edf"&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/800/1*lhYEmrsW9kqgB8nfIb9GJQ.png"&gt;&lt;/a&gt;&lt;/p&gt;</content><category term="Misc"></category></entry><entry><title>Integration method to map model scores to conversion rates from example data</title><link href="https://efavdb.com/integration-method-to-map-model-scores-to-conversion-rates-from-example-data" rel="alternate"></link><published>2018-03-03T17:53:00-08:00</published><updated>2018-03-03T17:53:00-08:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2018-03-03:/integration-method-to-map-model-scores-to-conversion-rates-from-example-data</id><summary type="html">&lt;p&gt;This note addresses the typical applied problem of estimating from data how a target &amp;#8220;conversion rate&amp;#8221; function varies with some available scalar score function &amp;#8212; e.g., estimating conversion rates from some marketing campaign as a function of a targeting model score. The idea centers around estimating the integral of the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This note addresses the typical applied problem of estimating from data how a target &amp;#8220;conversion rate&amp;#8221; function varies with some available scalar score function &amp;#8212; e.g., estimating conversion rates from some marketing campaign as a function of a targeting model score. The idea centers around estimating the integral of the rate function; differentiating this gives the rate function. The method is a variation on a standard technique for estimating pdfs via fits to empirical&amp;nbsp;cdfs.&lt;/p&gt;
&lt;h3&gt;Problem definition and naive binning&amp;nbsp;solution&lt;/h3&gt;
&lt;p&gt;Here, we are interested in estimating a rate function, &lt;span class="math"&gt;\(p \equiv p(x)\)&lt;/span&gt;, representing the probability of some &amp;#8220;conversion&amp;#8221; event as a function of &lt;span class="math"&gt;\(x\)&lt;/span&gt;, some scalar model score. To do this, we assume we have access to a finite set of score-outcome data of the form &lt;span class="math"&gt;\(\{(x_i, n_i), i= 1, \ldots ,k\}\)&lt;/span&gt;. Here, &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; is the score for example &lt;span class="math"&gt;\(i\)&lt;/span&gt; and &lt;span class="math"&gt;\(n_i \in \{0,1\}\)&lt;/span&gt; is its conversion&amp;nbsp;indicator.&lt;/p&gt;
&lt;p&gt;There are a number of standard methods for estimating rate functions. For example, if the score &lt;span class="math"&gt;\(x\)&lt;/span&gt; is a prior estimate for the conversion rate, a trivial mapping &lt;span class="math"&gt;\(p(x) = x\)&lt;/span&gt; may work. This won&amp;#8217;t work if the score function in question is not an estimate for &lt;span class="math"&gt;\(p\)&lt;/span&gt;. A more general approach is to bin together example data points that have similar scores: The observed conversion rate within each bin can then be used as an estimate for the true conversion rate in the bin&amp;#8217;s score range. An example output of this approach is shown in Fig. 1. Another option is to create a moving average, analogous to the binned&amp;nbsp;solution.&lt;/p&gt;
&lt;p&gt;The simple binning approach introduces two inefficiencies: (1) Binning coarsens a data set, resulting in a loss of information. (2) The data in one bin does not affect the data in the other bins, precluding exploitation of any global smoothness constraints that could be placed on &lt;span class="math"&gt;\(p\)&lt;/span&gt; as a function of &lt;span class="math"&gt;\(x\)&lt;/span&gt;. The running average approach is also subject to these issues. The method we discuss below alleviates both&amp;nbsp;inefficiencies.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2018/03/image17.png"&gt;&lt;img alt="" src="https://efavdb.com/wp-content/uploads/2018/03/image17.png"&gt;&lt;/a&gt;&lt;br&gt;
Fig. 1. Binned probability estimate approach: All data with scores in a given range are grouped together, and the outcomes from those data points are used to estimate the conversion rate in each bin. Here, the x-axis represents score range, data was grouped into six bins, and mean and standard deviation of the outcome probabilities were estimated from the observed outcomes within each&amp;nbsp;bin.&lt;/p&gt;
&lt;h3&gt;Efficient estimates by&amp;nbsp;integration&lt;/h3&gt;
&lt;p&gt;It can be difficult to directly fit a rate function p(x) using score-outcome data because data of this type does not lie on a continuous curve (the y-values alternate between 0 and 1, depending on the outcome for each example). However, if we consider the empirical integral of the available data, we obtain a smooth, increasing function that is much easier to&amp;nbsp;fit.&lt;/p&gt;
&lt;p&gt;To evaluate the empirical integral, we assume the samples are first sorted by &lt;span class="math"&gt;\(x\)&lt;/span&gt; and define&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$ \tag{1} \label{1}  
\delta x_i \equiv x_i - x_{i-1}.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Next, the empirical integral is taken as&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$ \tag{2} \label{2}  
\hat{J}(x_j) \equiv \sum_{i=0}^{j} n_i \delta x_i,  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
which approximates the integral&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{3} \label{3}  
J(x) \equiv \int_{x_0}^{x_j} p(x) dx.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
We can think of (\ref{3}) as the number of expected conversions given density-&lt;span class="math"&gt;\(1\)&lt;/span&gt; sampling over the &lt;span class="math"&gt;\(x\)&lt;/span&gt; range noted. Taking a fit to the &lt;span class="math"&gt;\(\{(x_i, \hat{J}(x_i))\}\)&lt;/span&gt; values gives a smooth estimate for (\ref{3}). Differentiating with respect to &lt;span class="math"&gt;\(x\)&lt;/span&gt; the gives an estimate for &lt;span class="math"&gt;\(p(x)\)&lt;/span&gt;. Fig. 2 illustrates the approach. Here, I fit the available data to a quadratic, capturing the growth in &lt;span class="math"&gt;\(p\)&lt;/span&gt; with &lt;span class="math"&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The example in Fig. 2 has no error bar shown. One way to obtain error bars would be to work with a particular fit form. The uncertainty in the fit coefficients could then be used to estimate uncertainties in the values at each&amp;nbsp;point.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2018/03/image16-1.png"&gt;&lt;img alt="image16" src="https://efavdb.com/wp-content/uploads/2018/03/image16-1.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Fig. 2. (Left) A plot of the empirical integral of the data used to generate Fig. 1 is in blue. A quadratic fit is shown in red. (Right) The derivative of the red fit function at left is shown, an estimate for the rate function in question, &lt;span class="math"&gt;\(p\equiv p(x)\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;Example python&amp;nbsp;code&lt;/h3&gt;
&lt;p&gt;The code snippet below carries out the procedure described above on a simple example. One example output is shown in Fig. 3 at the bottom of the section. Running the code multiple times gives one a sense of the error that is present in the predictions. In practical applications, this can&amp;#8217;t be done so carrying out the error analysis procedure suggested above should be done to get a better sense of the error&amp;nbsp;involved.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;pylab&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;  
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;  
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy.optimize&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;curve_fit&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;p_given_x&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;  
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;outcome_given_p&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;  
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;binomial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Generate some random data  &lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  
&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;p_given_x&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;outcome_given_p&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Calculate delta x, get weighted outcomes  &lt;/span&gt;
&lt;span class="n"&gt;delta_x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  
&lt;span class="n"&gt;weighted_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;delta_x&lt;/span&gt;

&lt;span class="c1"&gt;# Integrate and fit  &lt;/span&gt;
&lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cumsum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weighted_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;fit_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;  
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;

&lt;span class="n"&gt;popt&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pcov&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;curve_fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fit_func&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;j_fit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fit_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;popt&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Finally, differentiate and compare to actual p  &lt;/span&gt;
&lt;span class="n"&gt;p_fit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;j_fit&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;j_fit&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;delta_x&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# Plots  &lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;*&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;empirical integral&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;j_fit&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fit to integral&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;p_fit&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;g&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fit to p versus x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;k--&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;actual p versus x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2018/03/example_fit.png"&gt;&lt;img alt="example_fit" src="https://efavdb.com/wp-content/uploads/2018/03/example_fit.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Fig. 3. The result of one run of the algorithm on a data set where &lt;span class="math"&gt;\(p(x) \equiv x^2\)&lt;/span&gt;, given 200 random samples of &lt;span class="math"&gt;\(x \in (0, 1)\)&lt;/span&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Methods, Statistics, Theory"></category></entry><entry><title>Gaussian Processes</title><link href="https://efavdb.com/gaussian-processes" rel="alternate"></link><published>2017-11-25T09:53:00-08:00</published><updated>2017-11-25T09:53:00-08:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2017-11-25:/gaussian-processes</id><summary type="html">&lt;p&gt;We review the math and code needed to fit a Gaussian Process (&lt;span class="caps"&gt;GP&lt;/span&gt;) regressor to data. We conclude with a demo of a popular application, fast function minimization through &lt;span class="caps"&gt;GP&lt;/span&gt;-guided search. The gif below illustrates this approach in action &amp;#8212; the red points are samples from the hidden red curve …&lt;/p&gt;</summary><content type="html">&lt;p&gt;We review the math and code needed to fit a Gaussian Process (&lt;span class="caps"&gt;GP&lt;/span&gt;) regressor to data. We conclude with a demo of a popular application, fast function minimization through &lt;span class="caps"&gt;GP&lt;/span&gt;-guided search. The gif below illustrates this approach in action &amp;#8212; the red points are samples from the hidden red curve. Using these samples, we attempt to leverage GPs to find the curve&amp;#8217;s minimum as fast as&amp;nbsp;possible.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2017/11/full_search.gif"&gt;&lt;img alt="full_search" src="https://efavdb.com/wp-content/uploads/2017/11/full_search.gif"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Appendices contain quick reviews on (i) the &lt;span class="caps"&gt;GP&lt;/span&gt; regressor posterior derivation, (ii) SKLearn&amp;#8217;s &lt;span class="caps"&gt;GP&lt;/span&gt; implementation, and (iii) &lt;span class="caps"&gt;GP&lt;/span&gt;&amp;nbsp;classifiers.&lt;/p&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Gaussian Processes (GPs) provide a tool for treating the following general problem: A function &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt; is sampled at &lt;span class="math"&gt;\(n\)&lt;/span&gt; points, resulting in a set of noisy&lt;span class="math"&gt;\(^1\)&lt;/span&gt; function measurements, &lt;span class="math"&gt;\(\{f(x_i) = y_i \pm \sigma_i, i = 1, \ldots, n\}\)&lt;/span&gt;. Given these available samples, can we estimate the probability that &lt;span class="math"&gt;\(f = \hat{f}\)&lt;/span&gt;, where &lt;span class="math"&gt;\(\hat{f}\)&lt;/span&gt; is some candidate&amp;nbsp;function?&lt;/p&gt;
&lt;p&gt;To decompose and isolate the ambiguity associated with the above challenge, we begin by applying Bayes&amp;#8217;s&amp;nbsp;rule,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \label{Bayes} \tag{1}
p(\hat{f} \vert \{y\}) = \frac{p(\{y\} \vert \hat{f} ) p(\hat{f})}{p(\{y\}) }.
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
The quantity at left above is shorthand for the probability we seek &amp;#8212; the probability that &lt;span class="math"&gt;\(f = \hat{f}\)&lt;/span&gt;, given our knowledge of the sampled function values &lt;span class="math"&gt;\(\{y\}\)&lt;/span&gt;. To evaluate this, one can define and then evaluate the quantities at right. Defining the first in the numerator requires some assumption about the source of error in our measurement process. The second function in the numerator is the prior &amp;#8212; it is here where the greatest assumptions must be taken. For example, we&amp;#8217;ll see below that the prior effectively dictates the probability of a given smoothness for the &lt;span class="math"&gt;\(f\)&lt;/span&gt; function in&amp;nbsp;question.&lt;/p&gt;
&lt;p&gt;In the &lt;span class="caps"&gt;GP&lt;/span&gt; approach, both quantities in the numerator at right above are taken to be multivariate Normals / Gaussians. The specific parameters of this Gaussian can be selected to ensure that the resulting fit is good &amp;#8212; but the Normality requirement is essential for the mathematics to work out. Taking this approach, we can write down the posterior analytically, which then allows for some useful applications. For example, we used this approach to obtain the curves shown in the top figure of this post &amp;#8212; these were obtained through random sampling from the posterior of a fitted &lt;span class="caps"&gt;GP&lt;/span&gt;, pinned to equal measured values at the two pinched points shown. Posterior samples are useful for visualization and also for taking Monte Carlo&amp;nbsp;averages.&lt;/p&gt;
&lt;p&gt;In this post, we (i) review the math needed to calculate the posterior above, (ii) discuss numerical evaluations and fit some example data using GPs, and (iii) review how a fitted &lt;span class="caps"&gt;GP&lt;/span&gt; can help to quickly minimize a cost function &amp;#8212; eg a machine learning cross-validation score. Appendices cover the derivation of the &lt;span class="caps"&gt;GP&lt;/span&gt; regressor posterior, SKLearn&amp;#8217;s &lt;span class="caps"&gt;GP&lt;/span&gt; implementation, and &lt;span class="caps"&gt;GP&lt;/span&gt;&amp;nbsp;Classifiers.&lt;/p&gt;
&lt;p&gt;Our minimal python class SimpleGP used below is available on our GitHub, &lt;a href="https://github.com/EFavDB/gaussian_processes"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Note: To understand the mathematical details covered in this post, one should be familiar with multivariate normal distributions &amp;#8212; these are reviewed in our prior post, &lt;a href="http://efavdb.github.io/normal-distributions"&gt;here&lt;/a&gt;. These details can be skipped by those primarily interested in&amp;nbsp;applications.&lt;/p&gt;
&lt;h3&gt;Analytic evaluation of the&amp;nbsp;posterior&lt;/h3&gt;
&lt;p&gt;To evaluate the left side of (\ref{Bayes}), we will evaluate the right. Only the terms in the numerator need to be considered, because the denominator does not depend on &lt;span class="math"&gt;\(\hat{f}\)&lt;/span&gt;. This means that the denominator must equate to a normalization factor, common to all candidate functions. In this section, we will first write down the assumed forms for the two terms in the numerator and then consider the posterior that&amp;nbsp;results.&lt;/p&gt;
&lt;p&gt;The first assumption that we will make is that if the true function is &lt;span class="math"&gt;\(\hat{f}\)&lt;/span&gt;, then our &lt;span class="math"&gt;\(y\)&lt;/span&gt;-measurements are independent and Gaussian-distributed about &lt;span class="math"&gt;\(\hat{f}(x)\)&lt;/span&gt;. This assumption implies that the first term on the right of (\ref{Bayes})&amp;nbsp;is
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{2} \label{prob}
p(\{y\} \vert \hat{f} ) \equiv \prod_{i=1}^n \frac{1}{\sqrt{2 \pi \sigma_i^2}} \exp \left ( - \frac{(y_i - \hat{f}(x_i) )^2}{2 \sigma_i^2} \right).
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
The &lt;span class="math"&gt;\(y_i\)&lt;/span&gt; above are the actual measurements made at our sample points, and the &lt;span class="math"&gt;\(\sigma_i^2\)&lt;/span&gt; are their variance&amp;nbsp;uncertainties.&lt;/p&gt;
&lt;p&gt;The second thing we must do is assume a form for &lt;span class="math"&gt;\(p(\hat{f})\)&lt;/span&gt;, our prior. We restrict attention to a set of points &lt;span class="math"&gt;\(\{x_i: i = 1, \ldots, N\}\)&lt;/span&gt;, where the first &lt;span class="math"&gt;\(n\)&lt;/span&gt; points are the points that have been sampled, and the remaining &lt;span class="math"&gt;\((N-n)\)&lt;/span&gt; are test points at other locations &amp;#8212; points where we would like to estimate the joint statistics&lt;span class="math"&gt;\(^2\)&lt;/span&gt; of &lt;span class="math"&gt;\(f\)&lt;/span&gt;. To progress, we simply assume a multi-variate Normal distribution for &lt;span class="math"&gt;\(f\)&lt;/span&gt; at these points, governed by a covariance matrix &lt;span class="math"&gt;\(\Sigma\)&lt;/span&gt;. This&amp;nbsp;gives
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \label{prior} \tag{3}
&amp;amp;&amp;amp;p(f(x_1), \ldots, f(x_N) ) \sim \
&amp;amp;&amp;amp; \frac{1}{\sqrt{ (2 \pi)^{N} \vert \Sigma \vert }} \exp \left ( - \frac{1}{2} \sum_{ij=1}^N f_i \Sigma^{-1}_{ij} f_j \right).
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Here, we have introduced the shorthand, &lt;span class="math"&gt;\(f_i \equiv f(x_i)\)&lt;/span&gt;. Notice that we have implicitly assumed that the mean of our normal distribution is zero above. This is done for simplicity: If a non-zero mean is appropriate, this can be added in to the analysis, or subtracted from the underlying &lt;span class="math"&gt;\(f\)&lt;/span&gt; to obtain a new one with zero&amp;nbsp;mean.&lt;/p&gt;
&lt;p&gt;The particular form of &lt;span class="math"&gt;\(\Sigma\)&lt;/span&gt; is where all of the modeler&amp;#8217;s insight and ingenuity must be placed when working with GPs. Researchers who know their topic very well can assert well-motivated, complex priors &amp;#8212; often taking the form of a sum of terms, each capturing some physically-relevant contribution to the statistics of their problem at hand. In this post, we&amp;#8217;ll assume the simple&amp;nbsp;form
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{4} \label{covariance}
\Sigma_{ij} \equiv \sigma^2 \exp \left( - \frac{(x_i - x_j)^2}{2 l^2}\right).
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Notice that with this assumed form, if &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; and &lt;span class="math"&gt;\(x_j\)&lt;/span&gt; are close together, the exponential will be nearly equal to one. This ensures that nearby points are highly correlated, forcing all high-probability functions to be smooth. The rate at which (\ref{covariance}) dies down as two test points move away from each another is controlled by the length-scale parameter &lt;span class="math"&gt;\(l.\)&lt;/span&gt; If this is large (small), the curve will be smooth over a long (short) distance. We illustrate these points in the next section, and also explain how an appropriate length scale can be inferred from the sample data at hand in the section after&amp;nbsp;that.&lt;/p&gt;
&lt;p&gt;Now, if we combine (\ref{prob}) and (\ref{prior}) and plug this into (\ref{Bayes}), we obtain an expression for the posterior, &lt;span class="math"&gt;\(p(f \vert \{y\})\)&lt;/span&gt;. This function is an exponential whose argument is a quadratic in the &lt;span class="math"&gt;\(f_i\)&lt;/span&gt;. In other words, like the prior, the posterior is a multi-variate normal. With a little work, one can derive explicit expressions for the mean and covariance of this distribution: Using block notation, with &lt;span class="math"&gt;\(0\)&lt;/span&gt; corresponding to the sample points and &lt;span class="math"&gt;\(1\)&lt;/span&gt; to the test points, the marginal distribution at the test points&amp;nbsp;is
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{5} \label{posterior}
&amp;amp;&amp;amp; p(\textbf{f}_1 \vert \{y\}) =\
&amp;amp;&amp;amp; N\left ( \Sigma_{10} \frac{1}{\sigma^2 I_{00} + \Sigma_{00}} \cdot \textbf{y}, \Sigma_{11} - \Sigma_{10} \frac{1}{\sigma^2 I_{00} + \Sigma_{00}} \Sigma_{01} \right).
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Here,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{6} \label{sigma_mat}
\sigma^2 I_{00} \equiv
\left( \begin{array}{cccc}
\sigma_1^2 &amp;amp; 0 &amp;amp; \ldots &amp;amp;0 \\
0 &amp;amp; \sigma_2^2 &amp;amp; \ldots &amp;amp;0 \\
\ldots &amp;amp; &amp;amp; &amp;amp; \\
0 &amp;amp; 0 &amp;amp; \ldots &amp;amp; \sigma_n^2
\end{array} \right),
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
and &lt;span class="math"&gt;\(\textbf{y}\)&lt;/span&gt; is the length-&lt;span class="math"&gt;\(n\)&lt;/span&gt; vector of&amp;nbsp;measurements,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{7} \label{y_vec}
\textbf{y}^T \equiv (y_1, \ldots, y_n).
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;Equation (\ref{posterior}) is one of the main results for Gaussian Process regressors &amp;#8212; this result is all one needs to evaluate the posterior. Notice that the mean at all points is linear in the sampled values &lt;span class="math"&gt;\(\textbf{y}\)&lt;/span&gt; and that the variance at each point is reduced near the measured values. Those interested in a careful derivation of this result can consult our appendix &amp;#8212; we actually provide two derivations there. However, in the remainder of the body of the post, we will simply explore applications of this&amp;nbsp;formula.&lt;/p&gt;
&lt;h3&gt;Numerical evaluations of the&amp;nbsp;posterior&lt;/h3&gt;
&lt;p&gt;In this section, we will demonstrate how two typical applications of (\ref{posterior}) can be carried out: (i) Evaluation of the mean and standard deviation of the posterior distribution at a test point &lt;span class="math"&gt;\(x\)&lt;/span&gt;, and (ii) Sampling functions &lt;span class="math"&gt;\(\hat{f}\)&lt;/span&gt; directly from the posterior. The former is useful in that it can be used to obtain confidence intervals for &lt;span class="math"&gt;\(f\)&lt;/span&gt; at all locations, and the latter is useful both for visualization and also for obtaining general Monte Carlo averages over the posterior. Both concepts are illustrated in the header image for this post: In this picture, we fit a &lt;span class="caps"&gt;GP&lt;/span&gt; to a one-d function that had been measured at two locations. The blue shaded region represents a one-sigma confidence interval for the function value at each location, and the colored curves are posterior&amp;nbsp;samples.&lt;/p&gt;
&lt;p&gt;The code for our &lt;code&gt;SimpleGP&lt;/code&gt; fitter class is available on our &lt;a href="https://github.com/EFavDB/gaussian_processes"&gt;GitHub&lt;/a&gt;. We&amp;#8217;ll explain a bit how this works below, but those interested in the details should examine the code &amp;#8212; it&amp;#8217;s a short script and should be largely&amp;nbsp;self-explanatory.&lt;/p&gt;
&lt;h4&gt;Intervals&lt;/h4&gt;
&lt;p&gt;The code snippet below initializes our &lt;code&gt;SimpleGP&lt;/code&gt; class, defines some sample locations, values, and uncertainties, then evaluates the mean and standard deviation of the posterior at a set of test points. Briefly, this carried out as follows: The &lt;code&gt;fit&lt;/code&gt; method evaluates the inverse matrix &lt;span class="math"&gt;\(\left [ \sigma^2 I_{00} + \Sigma_{00} \right]^{-1}\)&lt;/span&gt; that appears in (\ref{posterior}) and saves the result for later use &amp;#8212; this allows us to avoid reevaluation of this inverse at each test point. Next, (\ref{posterior}) is evaluated once for each test point through the call to the &lt;code&gt;interval&lt;/code&gt; method.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Initialize fitter -- set covariance parameters&lt;/span&gt;
&lt;span class="n"&gt;WIDTH_SCALE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
&lt;span class="n"&gt;LENGTH_SCALE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SimpleGP&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;WIDTH_SCALE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;LENGTH_SCALE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;noise&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Insert observed sample data here, fit&lt;/span&gt;
&lt;span class="n"&gt;sample_x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.5&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;sample_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;sample_s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.25&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sample_y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sample_s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Get the mean and std at each point in x_test&lt;/span&gt;
&lt;span class="n"&gt;test_x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;05&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;means&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;interval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In the above, &lt;code&gt;WIDTH_SCALE&lt;/code&gt; and &lt;code&gt;LENGTH_SCALE&lt;/code&gt; are needed to specify the covariance matrix (\ref{covariance}). The former corresponds to &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; and the latter to &lt;span class="math"&gt;\(l\)&lt;/span&gt; in that equation. Increasing &lt;code&gt;WIDTH_SCALE&lt;/code&gt; corresponds to asserting less certainty as to the magnitude of unknown function and increasing &lt;code&gt;LENGTH_SCALE&lt;/code&gt; corresponds to increasing how smooth we expect the function to be. The figure below illustrates these points: Here, the blue intervals were obtained by setting &lt;code&gt;WIDTH_SCALE = LENGTH_SCALE  = 1&lt;/code&gt; and the orange intervals were obtained by setting &lt;code&gt;WIDTH_SCALE = 0.5&lt;/code&gt; and &lt;code&gt;LENGTH_SCALE  = 2&lt;/code&gt;. The result is that the orange posterior estimate is tighter and smoother than the blue posterior. In both plots, the solid curve is a plot of the mean of the posterior distribution, and the vertical bars are one sigma confidence&amp;nbsp;intervals.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2017/11/intervals.jpg"&gt;&lt;img alt="intervals" src="https://efavdb.com/wp-content/uploads/2017/11/intervals.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;Posterior&amp;nbsp;samples&lt;/h4&gt;
&lt;p&gt;To sample actual functions from the posterior, we will simply evaluate the mean and covariance matrix in (\ref{posterior}) again, this time passing in the multiple test point locations at which we would like to know the resulting sampled functions. Once we have the mean and covariance matrix of the posterior at these test points, we can pull samples from (\ref{posterior}) using an external library for multivariate normal sampling &amp;#8212; for this purpose, we used the python package numpy. The last step in the code snippet below carries out these&amp;nbsp;steps.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Insert observed sample data here.&lt;/span&gt;
&lt;span class="n"&gt;sample_x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;3.0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;sample_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;sample_s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# Initialize fitter -- set covariance parameters&lt;/span&gt;
&lt;span class="n"&gt;WIDTH_SCALE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
&lt;span class="n"&gt;LENGTH_SCALE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SimpleGP&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;WIDTH_SCALE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;LENGTH_SCALE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;noise&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sample_y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sample_s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Get the mean and std at each point in test_x&lt;/span&gt;
&lt;span class="n"&gt;test_x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;05&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;means&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;interval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Sample here&lt;/span&gt;
&lt;span class="n"&gt;SAMPLES&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;
&lt;span class="n"&gt;samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;SAMPLES&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Notice that in lines 2-4 here, we&amp;#8217;ve added in a few additional function sample locations (for fun). The resulting intervals and posterior samples are shown in the figure below. Notice that near the sampled points, the posterior is fairly well localized. However, on the left side of the plot, the posterior approaches the prior once we have moved a distance &lt;span class="math"&gt;\(\geq 1\)&lt;/span&gt;, the length scale chosen for the covariance matrix&amp;nbsp;(\ref{covariance}).&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2017/11/samples.jpg"&gt;&lt;img alt="samples" src="https://efavdb.com/wp-content/uploads/2017/11/samples.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Selecting the covariance&amp;nbsp;hyper-parameters&lt;/h3&gt;
&lt;p&gt;In the above, we demonstrated that the length scale of our covariance form dramatically affects the posterior &amp;#8212; the shape of the intervals and also of the samples from the posterior. Appropriately setting these parameters is a general problem that can make working with GPs a challenge. Here, we describe two methods that can be used to intelligently set such hyper-parameters, given some sampled&amp;nbsp;data.&lt;/p&gt;
&lt;h4&gt;Cross-validation&lt;/h4&gt;
&lt;p&gt;A standard method for setting hyper-parameters is to make use of a cross-validation scheme. This entails splitting the available sample data into a training set and a test set. One fits the &lt;span class="caps"&gt;GP&lt;/span&gt; to the training set using one set of hyper-parameters, then evaluates the accuracy of the model on the held out test set. One then repeats this process across many hyper-parameter choices, and selects that set which resulted in the best test set&amp;nbsp;performance.&lt;/p&gt;
&lt;h4&gt;Marginal Likelihood&amp;nbsp;Maximization&lt;/h4&gt;
&lt;p&gt;Often, one is interested in applying GPs in limits where evaluation of samples is expensive. This means that one often works with GPs in limits where only a small number of samples are available. In cases like this, the optimal hyper-parameters can vary quickly as the number of training points is increased. This means that the optimal selections obtained from a cross-validation schema may be far from the optimal set that applies when one trains on the full sample set&lt;span class="math"&gt;\(^3\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;An alternative general approach for setting the hyper-parameters is to maximize the marginal likelihood. That is, we try to maximize the likelihood of seeing the samples we have seen &amp;#8212; optimizing over the choice of available hyper-parameters. Formally, the marginal likelihood is evaluated by integrating out the unknown &lt;span class="math"&gt;\(\hat{f}^4\)&lt;/span&gt;,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{8}
p(\{y\} \vert \Sigma) \equiv \int p(\{y\} \vert f) p(f \vert \Sigma) df.
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Carrying out the integral directly can be done just as we have evaluated the posterior distribution in our appendix. However, a faster method is to note that after integrating out the &lt;span class="math"&gt;\(f\)&lt;/span&gt;, the &lt;span class="math"&gt;\(y\)&lt;/span&gt; values must be normally distributed&amp;nbsp;as
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{9}
p(\{y\} \vert \Sigma) \sim N(0, \Sigma + \sigma^2 I_{00}),
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(\sigma^2 I_{00}\)&lt;/span&gt; is defined as in (\ref{sigma_mat}). This&amp;nbsp;gives
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{10} \label{marginallikelihood}
\log p(\{y\}) \sim - \log \vert \Sigma + \sigma^2 I_{00} \vert - \textbf{y} \cdot ( \Sigma + \sigma^2 I_{00} )^{-1} \cdot \textbf{y}.
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
The two terms above compete: The second term is reduced by finding the covariance matrix that maximizes the exponent. Maximizing this alone would tend to result in an overfitting of the data. However, this term is counteracted by the first, which is the normalization for a Gaussian integral. This term becomes larger given short decay lengths and low diagonal variances. It acts as regularization term that suppresses overly complex&amp;nbsp;fits.&lt;/p&gt;
&lt;p&gt;In practice, to maximize (\ref{marginallikelihood}), one typically makes use of gradient descent, using analytical expressions for the gradient. This is the approach taken by SKLearn. Being able to optimize the hyper-parameters of a &lt;span class="caps"&gt;GP&lt;/span&gt; is one of this model&amp;#8217;s virtures. Unfortunately, (\ref{marginallikelihood}) is not guaranteed to be convex and multiple local minima often exist. To obtain a good minimum, one can attempt to initialize at some well-motivated point. Alternatively, one can reinitialize the gradient descent repeatedly at random points, finally selecting the best option at the&amp;nbsp;end.&lt;/p&gt;
&lt;h3&gt;Function minimum search and machine&amp;nbsp;learning&lt;/h3&gt;
&lt;p&gt;We&amp;#8217;re now ready to introduce one of the popular application of GPs: fast, guided function minimum search. In this problem, one is able to iteratively obtain noisy samples of a function, and the aim is to identify as quickly as possible the global minimum of the function. Gradient descent could be applied in cases like this, but this approach generally requires repeated sampling if the function is not convex. To reduce the number of steps / samples required, one can attempt to apply a more general, explore-exploit type strategy &amp;#8212; one balancing the desire to optimize about the current best known minimum with the goal of seeking out new local minima that are potentially even better. &lt;span class="caps"&gt;GP&lt;/span&gt; posteriors provide a natural starting point for developing such&amp;nbsp;strategies.&lt;/p&gt;
&lt;p&gt;The idea behind the &lt;span class="caps"&gt;GP&lt;/span&gt;-guided search approach is to develop a score function on top of the &lt;span class="caps"&gt;GP&lt;/span&gt; posterior. This score function should be chosen to encode some opinion of the value of searching a given point &amp;#8212; preferably one that takes an explore-exploit flavor. Once each point is scored, the point with the largest (or smallest, as appropriate) score is sampled. The process is then repeated iteratively until one is satisfied. Many score functions are possible. We discuss four possible choices below, then give an&amp;nbsp;example.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Gaussian Lower Confidence Bound (&lt;span class="caps"&gt;GLCB&lt;/span&gt;)&lt;/strong&gt;.
    The &lt;span class="caps"&gt;GLCB&lt;/span&gt; scores each point &lt;span class="math"&gt;\(x\)&lt;/span&gt; as
    &lt;div class="math"&gt;\begin{eqnarray}\tag{11}
    s_{\kappa}(x) = \mu(x) - \kappa \sigma(x).
    \end{eqnarray}&lt;/div&gt;
    Here, &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; and &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; are the &lt;span class="caps"&gt;GP&lt;/span&gt; posterior estimates for the mean and standard deviation for the function at &lt;span class="math"&gt;\(x\)&lt;/span&gt; and &lt;span class="math"&gt;\(\kappa\)&lt;/span&gt; is a control parameter. Notice that the first &lt;span class="math"&gt;\(\mu(x)\)&lt;/span&gt; term encourages exploitation around the best known local minimum. Similarly, the second &lt;span class="math"&gt;\(\kappa \sigma\)&lt;/span&gt; term encourages exploration &amp;#8212; search at points where the &lt;span class="caps"&gt;GP&lt;/span&gt; is currently most unsure of the true function&amp;nbsp;value.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gaussian Probability of Improvement (&lt;span class="caps"&gt;GPI&lt;/span&gt;)&lt;/strong&gt;.
    If the smallest value seen so far is &lt;span class="math"&gt;\(y\)&lt;/span&gt;, we can score each point using the probability that the true function value at that point is less than &lt;span class="math"&gt;\(y\)&lt;/span&gt;. That is, we can write
    &lt;div class="math"&gt;\begin{eqnarray}\tag{12}
    s(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \int_{-\infty}^y e^{-(v - \mu)^2 / (2 \sigma^2)} dv.
    \end{eqnarray}&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gaussian Expected Improvement (&lt;span class="caps"&gt;EI&lt;/span&gt;)&lt;/strong&gt;.
    A popular variant of the above is the so-called expected improvement.
    This is defined as
    &lt;div class="math"&gt;\begin{eqnarray} \tag{13}
    s(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \int_{-\infty}^y e^{-(v - \mu)^2 / (2 \sigma^2)} (y - v) dv.
    \end{eqnarray}&lt;/div&gt;
    This score function tends to encourage more exploration than the probability of improvement, since it values uncertainty more&amp;nbsp;highly.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Probability is minimum&lt;/strong&gt;.
    A final score function of interest is simply the probability that the point in question is the minimum. One way to obtain this score is to sample from the posterior many times. For each sample, we mark its global minimum, then take a majority vote for where to sample&amp;nbsp;next.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The gif at the top of this page (copied below) illustrates an actual &lt;span class="caps"&gt;GP&lt;/span&gt;-guided search, carried out in python using the package skopt&lt;span class="math"&gt;\(^5\)&lt;/span&gt;. The red curve at left is the (hidden) curve &lt;span class="math"&gt;\(f\)&lt;/span&gt; whose global minimum is being sought. The red points are the samples that have been obtained so far, and the green shaded curve is the &lt;span class="caps"&gt;GP&lt;/span&gt; posterior confidence interval for each point &amp;#8212; this gradually improves as more samples are obtained. At right is the Expected Improvement (&lt;span class="caps"&gt;EI&lt;/span&gt;) score function at each point that results from analysis on top of the &lt;span class="caps"&gt;GP&lt;/span&gt; posterior &amp;#8212; the score function used to guide search in this example. The process is initialized with five random samples, followed by guided search. Notice that as the process evolves, the first few samples focus on exploitation of known local minima. However, after a handful of iterations, the diminishing returns of continuing to sample these locations loses out to the desire to explore the middle points &amp;#8212; where the actual global minimum sits and is&amp;nbsp;found.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2017/11/full_search.gif"&gt;&lt;img alt="full_search" src="https://efavdb.com/wp-content/uploads/2017/11/full_search.gif"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Discussion&lt;/h3&gt;
&lt;p&gt;In this post we&amp;#8217;ve overviewed much of the math of GPs: The math needed to get to the posterior, how to sample from the posterior, and finally how to make practical use of the&amp;nbsp;posterior.&lt;/p&gt;
&lt;p&gt;In principle, GPs represent a powerful tool that can be used to fit any function. In practice, the challenge in wielding this tool seems to sit mainly with selection of appropriate hyper-parameters &amp;#8212; the search for appropriate parameters often gets stuck in local minima, causing fits to go off the rails. Nevertheless, when done correctly, application of GPs can provide some valuable performance gains &amp;#8212; and they are always fun to&amp;nbsp;visualize.&lt;/p&gt;
&lt;p&gt;Some additional topics relating to GPs are contained in our appendices. For those interested in even more detail, we can recommend the free online text by Rasmussen and Williams&lt;span class="math"&gt;\(^6\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;Appendix A: Derivation of&amp;nbsp;posterior&lt;/h3&gt;
&lt;p&gt;In this appendix, we present two methods to derive the posterior&amp;nbsp;(\ref{posterior}).&lt;/p&gt;
&lt;h4&gt;Method&amp;nbsp;1&lt;/h4&gt;
&lt;p&gt;We will begin by completing the square. Combining (\ref{prob}) and (\ref{prior}), a little algebra&amp;nbsp;gives
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align} \tag{A1} \label{square_complete}
 p(f_1, \ldots, f_N \vert \{y\})  &amp;amp;\sim \exp \left (-\sum_{i=1}^n \frac{(y_i - f_i)^2}{2 \sigma^2_i} - \frac{1}{2} \sum_{ij=1}^N f_i \Sigma^{-1}_{ij} f_j \right) \\
&amp;amp;\sim N\left ( \frac{1}{\Sigma^{-1} + \frac{1}{\sigma^2} I } \cdot \frac{1}{\sigma^2} I \cdot \textbf{y}, \frac{1}{\Sigma^{-1} + \frac{1}{\sigma^2} I } \right).
\end{align}&lt;/div&gt;
&lt;p&gt;
Here, &lt;span class="math"&gt;\(\frac{1}{\sigma^2} I\)&lt;/span&gt; is defined as in (\ref{sigma_mat}), but has zeros in all rows outside of the sample set. To obtain the expression (\ref{posterior}), we must identify the block structure of the inverse matrix that appears&amp;nbsp;above.&lt;/p&gt;
&lt;p&gt;To start, we&amp;nbsp;write
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align} \tag{A2} \label{matrix_to_invert}
\frac{1}{\Sigma^{-1} + \frac{1}{\sigma^2}I } &amp;amp;= \Sigma \frac{1}{I + \frac{1}{\sigma^2}I \Sigma} \\
&amp;amp;= \Sigma \left( \begin{matrix}
I_{00} + \frac{1}{\sigma^2}I_{00} \Sigma_{00} &amp;amp; \frac{1}{\sigma^2}I_{00} \Sigma_{01}\\
0 &amp;amp; I_{11}
\end{matrix} \right)^{-1},
\end{align}&lt;/div&gt;
&lt;p&gt;
where we are using block notation. To evaluate the inverse that appears above, we will make use of the block matrix inversion&amp;nbsp;formula,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
&amp;amp;\left( \begin{matrix}
A &amp;amp; B\\
C &amp;amp; D
\end{matrix} \right)^{-1} = \\
&amp;amp;\left( \begin{matrix}
(A - B D^{-1} C)^{-1} &amp;amp; - (A - B D^{-1} C)^{-1} B D^{-1} \\
-D^{-1} C (A - B D^{-1} C)^{-1} &amp;amp; D^{-1} + D^{-1} C (A - B D^{-1} C) B D^{-1}
\end{matrix} \right).
\end{align}&lt;/div&gt;
&lt;p&gt;
The matrix (\ref{matrix_to_invert}) has blocks &lt;span class="math"&gt;\(C = 0\)&lt;/span&gt; and &lt;span class="math"&gt;\(D=I\)&lt;/span&gt;, which simplifies the above significantly. Plugging in, we&amp;nbsp;obtain
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align} \label{shifted_cov} \tag{A3}
\frac{1}{\Sigma^{-1} + \frac{1}{\sigma^2}I } =
\Sigma \left( \begin{matrix}
\frac{1}{I_{00} + \frac{1}{\sigma^2}I \Sigma_{00}} &amp;amp; - \frac{1}{I_{00} + \frac{1}{\sigma^2}I \Sigma_{00}} \Sigma_{01}\\
0 &amp;amp; I_{11}
\end{matrix} \right)
\end{align}&lt;/div&gt;
&lt;p&gt;
With this result and (\ref{square_complete}), we can read off the mean of the test set&amp;nbsp;as
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align} \tag{A4} \label{mean_test}
&amp;amp; \left [ [ \Sigma^{-1} + \frac{1}{\sigma^2} I_{00} ]^{-1} \cdot \frac{1}{\sigma^2} I_{00} \cdot \textbf{y} \right ]_1 \\
&amp;amp;= \Sigma_{10} \frac{1}{I_{00} + \frac{1}{\sigma^2}I_{00} \Sigma_{00}} \frac{1}{\sigma^2} I_{00} \cdot \textbf{y} \\
&amp;amp;= \Sigma_{10} \frac{1}{\sigma^2 I_{00} + \Sigma_{00}} \cdot \textbf{y},
\end{align}&lt;/div&gt;
&lt;p&gt;
where we have multiplied the numerator and denominator by the inverse of &lt;span class="math"&gt;\(\frac{1}{\sigma^2}I_{00}\)&lt;/span&gt; in the second line. Similarly, the covariance of the test set is given by the lower right block of (\ref{shifted_cov}). This&amp;nbsp;is,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\tag{A5} \label{covariance_test}
\Sigma_{11} - \Sigma_{10} \cdot \frac{1}{\sigma^2 I_{00} + \Sigma_{00}} \cdot \Sigma_{01}.
\end{align}&lt;/div&gt;
&lt;p&gt;
The results (\ref{mean_test}) and (\ref{covariance_test}) give&amp;nbsp;(\ref{posterior}).&lt;/p&gt;
&lt;h4&gt;Method&amp;nbsp;2&lt;/h4&gt;
&lt;p&gt;In this second method, we consider the joint distribution of a set of test points &lt;span class="math"&gt;\(\textbf{f}_1\)&lt;/span&gt; and the set of observed samples &lt;span class="math"&gt;\(\textbf{f}_0\)&lt;/span&gt;. Again, we assume that the function density has mean zero. The joint probability density for the two is&amp;nbsp;then
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\tag{A6}
p(\textbf{f}_0, \textbf{f}_1) \sim N \left (
\left ( \begin{matrix}
0 \\
0
\end{matrix} \right),
\left ( \begin{matrix}{cc}
\Sigma_{0,0} &amp;amp; \Sigma_{0,1} \\
\Sigma_{1,0} &amp;amp; \Sigma_{11}
\end{matrix} \right )
\right )
\end{align}&lt;/div&gt;
&lt;p&gt;
Now, we use the&amp;nbsp;result
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align} \tag{A7}
p( \textbf{f}_1 \vert \textbf{f}_0) &amp;amp;=&amp;amp; \frac{p( \textbf{f}_0, \textbf{f}_1)}{p( \textbf{f}_0)}.
\end{align}&lt;/div&gt;
&lt;p&gt;
The last two expressions are all that are needed to derive (\ref{posterior}). The main challenge involves completing the square, and this can be done with the block matrix inversion formula, as in the previous&amp;nbsp;derivation.&lt;/p&gt;
&lt;h3&gt;Appendix B: SKLearn implementation and other&amp;nbsp;kernels&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2017/11/sklearn.jpg"&gt;&lt;img alt="sklearn" src="https://efavdb.com/wp-content/uploads/2017/11/sklearn.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;SKLearn provides contains the &lt;code&gt;GaussianProcessRegressor&lt;/code&gt; class. This allows one to carry out fits and sampling in any dimension &amp;#8212; i.e., it is more general than our minimal class in that it can fit feature vectors in more than one dimension. In addition, the &lt;code&gt;fit&lt;/code&gt; method of the SKLearn class attempts to find an optimal set of hyper-parameters for a given set of data. This is done through maximization of the marginal likelihood, as described above. Here, we provide some basic notes on this class and the built in kernels that one can use to define the covariance matrix &lt;span class="math"&gt;\(\Sigma\)&lt;/span&gt; in (\ref{prior}). We also include a simple code snippet illustrating&amp;nbsp;calls.&lt;/p&gt;
&lt;h4&gt;Pre-defined&amp;nbsp;Kernels&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Radial-basis function (&lt;span class="caps"&gt;RBF&lt;/span&gt;): This is the default &amp;#8212; equivalent to our (\ref{covariance}). The &lt;span class="caps"&gt;RBF&lt;/span&gt; is characterized by a scale parameter, &lt;span class="math"&gt;\(l\)&lt;/span&gt;. In more than one dimension, this can be a vector, allowing for anisotropic correlation&amp;nbsp;lengths.&lt;/li&gt;
&lt;li&gt;White kernel : The White Kernel is used for noise estimation &amp;#8212; docs suggest useful for estimating the global noise level, but not&amp;nbsp;pointwise.&lt;/li&gt;
&lt;li&gt;Matern: This is a generalized exponential decay, where the exponents is a powerlaw in separation distance. Special limits include the &lt;span class="caps"&gt;RBF&lt;/span&gt; and also an absolute distance exponential decay. Some special parameter choices allow for existence of single or double&amp;nbsp;derivatives.&lt;/li&gt;
&lt;li&gt;Rational quadratic: This is &lt;span class="math"&gt;\((1 + (d / l)^2)^{\alpha}\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Exp-Sine-Squared: This allows one to model periodic functions. This is just like the &lt;span class="caps"&gt;RBF&lt;/span&gt;, but the distance that gets plugged in is the sine of the actual distance. A periodicity parameter exists, as well as a &amp;#8220;variance&amp;#8221;
    &amp;#8212; the scale of the Gaussian&amp;nbsp;suppression.&lt;/li&gt;
&lt;li&gt;Dot product kernel : This takes form &lt;span class="math"&gt;\(1 + x_i \cdot x_j\)&lt;/span&gt;. It&amp;#8217;s not stationary, in the sense that the result changes if a constant translation is added in. They state that you get this result from linear regression analysis if you place &lt;span class="math"&gt;\(N(0,1)\)&lt;/span&gt; priors on the&amp;nbsp;coefficients.&lt;/li&gt;
&lt;li&gt;Kernels as objects : The kernels are objects, but support binary operations between them to create more complicated kernels, eg addition, multiplication, and exponentiation (latter simply raises initial kernel to a power). They all support analytic gradient evaluation. You can access all of the parameters in a kernel that you define via some helper functions &amp;#8212; eg, &lt;code&gt;kernel.get_params()&lt;/code&gt;. &lt;code&gt;kernel.hyperparameters&lt;/code&gt; is a list of all the&amp;nbsp;hyper-parameters.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Parameters&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;n_restarts_optimizer&lt;/code&gt;: This is the number of times to restart the fit &amp;#8212; useful for exploration of multiple local minima. The default is&amp;nbsp;zero.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;alpha&lt;/code&gt;: This optional argument allows one to pass in uncertainties for each&amp;nbsp;measurement.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;normalize_y&lt;/code&gt;: This is used to indicate that the mean of the &lt;span class="math"&gt;\(y\)&lt;/span&gt;-values we&amp;#8217;re looking for is not necessarily&amp;nbsp;zero.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Example&amp;nbsp;call&lt;/h4&gt;
&lt;p&gt;The code snippet below carries out a simple fit. The result is the plot shown at the top of this&amp;nbsp;section.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.gaussian_process.kernels&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;RBF&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ConstantKernel&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.gaussian_process&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;GaussianProcessRegressor&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="c1"&gt;# Build a model&lt;/span&gt;
&lt;span class="n"&gt;kernel&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1e-3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1e3&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;RBF&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;gp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GaussianProcessRegressor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_restarts_optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Some data&lt;/span&gt;
&lt;span class="n"&gt;xobs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;1.5&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="n"&gt;yobs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# Fit the model to the data (optimize hyper parameters)&lt;/span&gt;
&lt;span class="n"&gt;gp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xobs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;yobs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Plot points and predictions&lt;/span&gt;
&lt;span class="n"&gt;x_set&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;x_set&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;x_set&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;means&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sigmas&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_set&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;return_std&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;errorbar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_set&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;means&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;yerr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;sigmas&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_set&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;means&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;g&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;linewidth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;colors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;g&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;b&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;k&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;colors&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;y_set&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample_y&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_set&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_set&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_set&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;--&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;More details on the sklearn implementation can be found &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Appendix C: &lt;span class="caps"&gt;GP&lt;/span&gt;&amp;nbsp;Classifiers&lt;/h3&gt;
&lt;p&gt;Here, we describe how GPs are often used to fit binary classification data &amp;#8212; data where the response variable &lt;span class="math"&gt;\(y\)&lt;/span&gt; can take on values of either &lt;span class="math"&gt;\(0\)&lt;/span&gt; or &lt;span class="math"&gt;\(1\)&lt;/span&gt;. The mathematics for &lt;span class="caps"&gt;GP&lt;/span&gt; Classifiers does not work out as cleanly as it does for &lt;span class="caps"&gt;GP&lt;/span&gt; Regressors. The reason is that the &lt;span class="math"&gt;\(0 / 1\)&lt;/span&gt; response is not Gaussian-distributed, which means that the posterior is not either. To make use of the program, one approximates the posterior as normal, via the Laplace&amp;nbsp;approximation.&lt;/p&gt;
&lt;p&gt;The starting point is to write down a form for the probability of seeing a given &lt;span class="math"&gt;\(y\)&lt;/span&gt; value at &lt;span class="math"&gt;\(x\)&lt;/span&gt;. This, ones takes as the&amp;nbsp;form,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align} \tag{A8} \label{classifier}
p(y \vert f(x)) = \frac{1}{1 + \exp\left (- y \times f(x)\right)}.
\end{align}&lt;/div&gt;
&lt;p&gt;
This form is a natural non-linear generalization of logistic regression &amp;#8212; see our post on this topic, &lt;a href="http://efavdb.github.io/logistic-regression"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To proceed, the prior for &lt;span class="math"&gt;\(f\)&lt;/span&gt; is taken to once again have form (\ref{prior}). Using this and (\ref{classifier}), we obtain the posterior for &lt;span class="math"&gt;\(f\)&lt;/span&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
p(f \vert y) &amp;amp;\sim \frac{1}{1 + \exp\left (- y \times f(x)\right)} \exp \left ( - \frac{1}{2} \sum_{ij=1}^N f_i \Sigma^{-1}_{ij} f_j \right) \\
&amp;amp;\approx N(\mu, \Sigma^{\prime}) \tag{A9}
\end{align}&lt;/div&gt;
&lt;p&gt;
Here, the last line is the Laplace / Normal approximation to the line above it. Using this form, one can easily obtain confidence intervals and samples from the approximate posterior, as was done for&amp;nbsp;regressors.&lt;/p&gt;
&lt;h3&gt;Footnotes&lt;/h3&gt;
&lt;p&gt;[1] The size of the &lt;span class="math"&gt;\(\sigma_i\)&lt;/span&gt; determines how precisely we know the function value at each of the &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; points sampled &amp;#8212; if they are all &lt;span class="math"&gt;\(0\)&lt;/span&gt;, we know the function exactly at these points, but not anywhere&amp;nbsp;else.&lt;/p&gt;
&lt;p&gt;[2] One might wonder whether introducing more points to the analysis would change the posterior statistics for the original &lt;span class="math"&gt;\(N\)&lt;/span&gt; points in question. It turns out that this is not the case for GPs: If one is interested only in the joint-statistics of these &lt;span class="math"&gt;\(N\)&lt;/span&gt; points, all others integrate out. For example, consider the goal of identifying the posterior distribution of &lt;span class="math"&gt;\(f\)&lt;/span&gt; at only a single test point &lt;span class="math"&gt;\(x\)&lt;/span&gt;. In this case, the posterior for the &lt;span class="math"&gt;\(N = n+1\)&lt;/span&gt; points follows from Bayes&amp;#8217;s&amp;nbsp;rule,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align} \tag{f1}
p(f(x_1), \ldots, f(x_n), f(x_{n+1}) \vert \{y\}) = \frac{p(\{y\} \vert f) p(f)}{p(\{y\})}.
\end{align}&lt;/div&gt;
&lt;p&gt;
Now, by assumption, &lt;span class="math"&gt;\(p(\{y\} \vert f)\)&lt;/span&gt; depends only on &lt;span class="math"&gt;\(f(x_1),\ldots, f(x_n)\)&lt;/span&gt; &amp;#8212; the values of &lt;span class="math"&gt;\(f\)&lt;/span&gt; where &lt;span class="math"&gt;\(y\)&lt;/span&gt; was sampled. Integrating over all points except the sample set and test point &lt;span class="math"&gt;\(x\)&lt;/span&gt;&amp;nbsp;gives
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align} \tag{f2}
&amp;amp;p(f(x_1), \ldots, f(x_{n+1}) \vert \{y\}) =\\
&amp;amp; \frac{p(\{y\} \vert f(x_1),\ldots,f(x_n))}{p(\{y\})} \int p(f) \prod_{i \not \in \{x_1, \ldots, x_N\}} df_i
\end{align}&lt;/div&gt;
&lt;p&gt;
The result of the integral above is a Normal distribution &amp;#8212; one with covariance given by the original covariance function evaluated only at the points &lt;span class="math"&gt;\(\{x_1, \ldots, x_{N} \}\)&lt;/span&gt;. This fact is proven in our post on Normal distributions &amp;#8212; see equation (22) of that post, &lt;a href="http://efavdb.github.io/normal-distributions"&gt;here&lt;/a&gt;. The result implies that we can get the correct sampling statistics on any set of test points, simply by analyzing these alongside the sampled points. This fact is what allows us to tractably treat the formally-infinite number of degrees of freedom associated with&amp;nbsp;GPs.&lt;/p&gt;
&lt;p&gt;[3] We have a prior post illustrating this point &amp;#8212; see &lt;a href="http://efavdb.github.io/model-selection"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;[4] The marginal likelihood is equal to the denominator of (\ref{Bayes}), which we previously&amp;nbsp;ignored.&lt;/p&gt;
&lt;p&gt;[5] We made this gif through adapting the skopt tutorial code, &lt;a href="https://scikit-optimize.github.io/notebooks/bayesian-optimization.html"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;[6] For the free text by Rasmussen and Williams, see &lt;a href="http://www.gaussianprocess.org/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Methods"></category><category term="methods"></category><category term="programming"></category><category term="statistics"></category><category term="theory"></category></entry><entry><title>Martingales</title><link href="https://efavdb.com/martingales" rel="alternate"></link><published>2017-10-20T07:44:00-07:00</published><updated>2017-10-20T07:44:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2017-10-20:/martingales</id><summary type="html">&lt;p&gt;Here, I give a quick review of the concept of a Martingale. A Martingale is a sequence of random variables satisfying a specific expectation conservation law. If one can identify a Martingale relating to some other sequence of random variables, its use can sometimes make quick work of certain expectation …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Here, I give a quick review of the concept of a Martingale. A Martingale is a sequence of random variables satisfying a specific expectation conservation law. If one can identify a Martingale relating to some other sequence of random variables, its use can sometimes make quick work of certain expectation value&amp;nbsp;evaluations.&lt;/p&gt;
&lt;p&gt;This note is adapted from Chapter 2 of Stochastic Calculus and Financial Applications, by&amp;nbsp;Steele.&lt;/p&gt;
&lt;h3&gt;Definition&lt;/h3&gt;
&lt;p&gt;Often in random processes, one is interested in characterizing a sequence of random variables &lt;span class="math"&gt;\(\{X_i\}\)&lt;/span&gt;. The example we will keep in mind is a set of variables &lt;span class="math"&gt;\(X_i \in \{-1, 1\}\)&lt;/span&gt; corresponding to the steps of an unbiased random walk in one-dimension. A Martingale process &lt;span class="math"&gt;\(M_i = f(X_1, X_2, \ldots X_i)\)&lt;/span&gt; is a derived random variable on top of the &lt;span class="math"&gt;\(X_i\)&lt;/span&gt; variables satisfying the following conservation&amp;nbsp;law
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align} \tag{1}
E(M_i | X_1, \ldots X_{i-1}) = M_{i-1}.
\end{align}&lt;/div&gt;
&lt;p&gt;
For example, in the unbiased random walk example, if we take &lt;span class="math"&gt;\(S_n = \sum_{i=1}^n X_i\)&lt;/span&gt;, then &lt;span class="math"&gt;\(E(S_n) = S_{n-1}\)&lt;/span&gt;, so &lt;span class="math"&gt;\(S_n\)&lt;/span&gt; is a Martingale. If we can develop or identify a Martingale for a given &lt;span class="math"&gt;\(\{X_i\}\)&lt;/span&gt; process, it can often help us to quickly evaluate certain expectation values relating to the underlying process. Three useful Martingales&amp;nbsp;follow.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Again, the sum &lt;span class="math"&gt;\(S_n = \sum_{i=1}^n X_i\)&lt;/span&gt; is a Martingale, provided &lt;span class="math"&gt;\(E(X_i) = 0\)&lt;/span&gt; for all &lt;span class="math"&gt;\(i\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;The expression &lt;span class="math"&gt;\(S_n^2 - n \sigma^2\)&lt;/span&gt; is a Martingale, provided &lt;span class="math"&gt;\(E(X_i) = 0\)&lt;/span&gt; and &lt;span class="math"&gt;\(E(X_i^2) = \sigma^2\)&lt;/span&gt; for all &lt;span class="math"&gt;\(i\)&lt;/span&gt;. Proof: &lt;div class="math"&gt;\begin{align} \tag{2}
    E(S_n^2 | X_1, \ldots X_{n-1}) &amp;amp;= \sigma^2 + 2 E(X_n) S_{n-1} + S_{n-1}^2 - n \sigma^2\\
    &amp;amp;= S_{n-1}^2 - (n-1) \sigma^2.
    \end{align}&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;The product &lt;span class="math"&gt;\(P_n = \prod_{i=1}^n X_i\)&lt;/span&gt; is a Martingale, provided &lt;span class="math"&gt;\(E(X_i) = 1\)&lt;/span&gt; for all &lt;span class="math"&gt;\(i\)&lt;/span&gt;. One example of interest is
    &lt;div class="math"&gt;\begin{align} \tag{3}
    P_n = \frac{\exp \left ( \lambda \sum_{i=1}^n X_i\right)}{E(\exp \left ( \lambda X \right))^n}.
    \end{align}&lt;/div&gt;
    Here, &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; is a free tuning parameter. If we choose a &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; such that &lt;span class="math"&gt;\(E(\exp(\lambda X)) = 1\)&lt;/span&gt; for our process, we can get a particularly simple&amp;nbsp;form.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Stopped&amp;nbsp;processes&lt;/h3&gt;
&lt;p&gt;In some games, we may want to setup rules that say we will stop the game at time &lt;span class="math"&gt;\(\tau\)&lt;/span&gt; if some condition is met at index &lt;span class="math"&gt;\(\tau\)&lt;/span&gt;. For example, we may stop a random walk (initialized at zero) if the walker gets to either position &lt;span class="math"&gt;\(A\)&lt;/span&gt; or &lt;span class="math"&gt;\(-B\)&lt;/span&gt; (wins &lt;span class="math"&gt;\(A\)&lt;/span&gt; or loses &lt;span class="math"&gt;\(B\)&lt;/span&gt;). This motivates defining the stopped Martingale&amp;nbsp;as,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
M_{n \wedge \tau} = \begin{cases}
M_n &amp;amp;\text{if } \tau \geq n \\
M_{\tau} &amp;amp;\text{else}. \tag{4}
\end{cases}
\end{align}&lt;/div&gt;
&lt;p&gt;
Here, we prove that if &lt;span class="math"&gt;\(M_n\)&lt;/span&gt; is a Martingale, then so is $M_{n \wedge \tau} $. This is useful because it will tell us that the stopped Martingale has the same conservation law as the unstopped&amp;nbsp;version.&lt;/p&gt;
&lt;p&gt;First, we note that if &lt;span class="math"&gt;\(A_i \equiv f_2(X_1, \ldots X_{i-1})\)&lt;/span&gt; is some function of the observations so far, then the transformed&amp;nbsp;process
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align} \tag{5}
\tilde{M}_n \equiv M_0 + \sum_{i=1}^n A_i (M_i - M_{i-1})
\end{align}&lt;/div&gt;
&lt;p&gt;
is also a Martingale.&amp;nbsp;Proof:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align} \tag{6}
E(\tilde{M}_n | X_1, \ldots X_{n-1}) = A_n \left ( E(M_n) - M_{n-1} \right) + \tilde{M}_{n-1} = \tilde{M}_{n-1}.
\end{align}&lt;/div&gt;
&lt;p&gt;With this result we can prove the stopped Martingale is also a Martingale. We can do that by writing &lt;span class="math"&gt;\(A_i = 1(\tau \geq i)\)&lt;/span&gt; &amp;#8212; where &lt;span class="math"&gt;\(1\)&lt;/span&gt; is the indicator function. Plugging this into the above, we get the transformed&amp;nbsp;Martingale,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align} \nonumber \tag{7}
\tilde{M}_n &amp;amp;= M_0 + \sum_{i=1}^n 1(\tau \geq i) (M_i - M_{i-1}) \\
&amp;amp;= \begin{cases}
M_n &amp;amp; \text{if } \tau \geq n \
M_{\tau} &amp;amp; \text{else}.
\end{cases}
\end{align}&lt;/div&gt;
&lt;p&gt;
This is the stopped Martingale &amp;#8212; indeed a Martingale, by the&amp;nbsp;above.&lt;/p&gt;
&lt;h3&gt;Example&amp;nbsp;applications&lt;/h3&gt;
&lt;h3&gt;&lt;/h3&gt;
&lt;h4&gt;Problem&amp;nbsp;1&lt;/h4&gt;
&lt;p&gt;Consider an unbiased random walker that takes steps of size &lt;span class="math"&gt;\(1\)&lt;/span&gt;. If we stop the walk as soon as he reaches either &lt;span class="math"&gt;\(A\)&lt;/span&gt; or &lt;span class="math"&gt;\(-B\)&lt;/span&gt;, what is the probability that he is at &lt;span class="math"&gt;\(A\)&lt;/span&gt; when the game&amp;nbsp;stops?&lt;/p&gt;
&lt;p&gt;Solution: Let &lt;span class="math"&gt;\(\tau\)&lt;/span&gt; be the stopping time and let &lt;span class="math"&gt;\(S_n = \sum_{i=1}^n X_i\)&lt;/span&gt; be the walker&amp;#8217;s position at time &lt;span class="math"&gt;\(n\)&lt;/span&gt;. We know that &lt;span class="math"&gt;\(S_n\)&lt;/span&gt; is a Martingale. By the above, so then is &lt;span class="math"&gt;\(S_{n \wedge \tau}\)&lt;/span&gt;, the stopped process Martingale. By the Martingale&amp;nbsp;property
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align} \tag{8}
E(S_{n \wedge \tau}) = E(S_{i \wedge \tau})
\end{align}&lt;/div&gt;
&lt;p&gt;
for all &lt;span class="math"&gt;\(i\)&lt;/span&gt;. In particular, plugging in &lt;span class="math"&gt;\(i = 0\)&lt;/span&gt; gives &lt;span class="math"&gt;\(E(S_{n \wedge \tau}) = 0\)&lt;/span&gt;. If we take &lt;span class="math"&gt;\(n \to \infty\)&lt;/span&gt;,&amp;nbsp;then
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align} \tag{9}
\lim_{n \to \infty} E(S_{n \wedge \tau}) \to E(S_{\tau}) = 0.
\end{align}&lt;/div&gt;
&lt;p&gt;
But we also&amp;nbsp;have
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align} \tag{10}
E(S_{\tau}) = P(A) * A - (1 - P(A)) B.
\end{align}&lt;/div&gt;
&lt;p&gt;
Equating (9) and (10)&amp;nbsp;gives
&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation} \tag{11}
P(A) = \frac{B}{A + B}
\end{equation}&lt;/div&gt;
&lt;h5&gt;Problem&amp;nbsp;2&lt;/h5&gt;
&lt;p&gt;In the game above, what is the expected stopping time? Solution: Use the stopped version of the Martingale &lt;span class="math"&gt;\(S_n^2 - n \sigma^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;h5&gt;Problem&amp;nbsp;3&lt;/h5&gt;
&lt;p&gt;In a biased version of the random walk game, what is the probability of stopping at &lt;span class="math"&gt;\(A\)&lt;/span&gt;? Solution: Use the stopped Martingale of form &lt;span class="math"&gt;\(P_n = \frac{\exp \left ( \lambda \sum_{i=1}^n X_i\right)}{E(\exp \left ( \lambda X \right))^n}\)&lt;/span&gt;, with &lt;span class="math"&gt;\(\exp[\lambda] = q/p\)&lt;/span&gt;, where &lt;span class="math"&gt;\(p = 1-q\)&lt;/span&gt; is the probability of step to the&amp;nbsp;right.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Finance"></category><category term="theory"></category></entry><entry><title>Logistic Regression</title><link href="https://efavdb.com/logistic-regression" rel="alternate"></link><published>2017-07-29T19:10:00-07:00</published><updated>2017-07-29T19:10:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2017-07-29:/logistic-regression</id><summary type="html">&lt;p&gt;We review binary logistic regression. In particular, we derive a) the equations needed to fit the algorithm via gradient descent, b) the maximum likelihood fit&amp;#8217;s asymptotic coefficient covariance matrix, and c) expressions for model test point class membership probability confidence intervals. We also provide python code implementing a minimal …&lt;/p&gt;</summary><content type="html">&lt;p&gt;We review binary logistic regression. In particular, we derive a) the equations needed to fit the algorithm via gradient descent, b) the maximum likelihood fit&amp;#8217;s asymptotic coefficient covariance matrix, and c) expressions for model test point class membership probability confidence intervals. We also provide python code implementing a minimal &amp;#8220;LogisticRegressionWithError&amp;#8221; class whose &amp;#8220;predict_proba&amp;#8221; method returns prediction confidence intervals alongside its point&amp;nbsp;estimates.&lt;/p&gt;
&lt;p&gt;Our python code can be downloaded from our github page, &lt;a href="https://github.com/EFavDB/logistic-regression-with-error"&gt;here&lt;/a&gt;. Its use requires the jupyter, numpy, sklearn, and matplotlib&amp;nbsp;packages.&lt;/p&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;The logistic regression model is a linear classification model that can be used to fit binary data &amp;#8212; data where the label one wishes to predict can take on one of two values &amp;#8212; e.g., &lt;span class="math"&gt;\(0\)&lt;/span&gt; or &lt;span class="math"&gt;\(1\)&lt;/span&gt;. Its linear form makes it a convenient choice of model for fits that are required to be interpretable. Another of its virtues is that it can &amp;#8212; with relative ease &amp;#8212; be set up to return both point estimates and also confidence intervals for test point class membership probabilities. The availability of confidence intervals allows one to flag test points where the model prediction is not precise, which can be useful for some applications &amp;#8212; eg fraud&amp;nbsp;detection.&lt;/p&gt;
&lt;p&gt;In this note, we derive the expressions needed to fit the logistic model to a training data set. We assume the training data consists of a set of &lt;span class="math"&gt;\(n\)&lt;/span&gt; feature vector- label pairs, &lt;span class="math"&gt;\(\{(\vec{x}_i, y_i)\)&lt;/span&gt;, for &lt;span class="math"&gt;\(i = 1, 2, \ldots, n\}\)&lt;/span&gt;, where the feature vectors &lt;span class="math"&gt;\(\vec{x}_i\)&lt;/span&gt; belong to some &lt;span class="math"&gt;\(m\)&lt;/span&gt;-dimensional space and the labels are binary, &lt;span class="math"&gt;\(y_i \in \{0, 1\}.\)&lt;/span&gt; The logistic model states that the probability of belonging to class &lt;span class="math"&gt;\(1\)&lt;/span&gt; is given&amp;nbsp;by
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{1} \label{model1}
p(y=1 \vert \vec{x}) \equiv \frac{1}{1 + e^{- \vec{\beta} \cdot \vec{x} } },
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt; is a coefficient vector characterizing the model. Note that with this choice of sign in the exponent, predictor vectors &lt;span class="math"&gt;\(\vec{x}\)&lt;/span&gt; having a large, positive component along &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt; will be predicted to have a large probability of being in class &lt;span class="math"&gt;\(1\)&lt;/span&gt;. The probability of class &lt;span class="math"&gt;\(0\)&lt;/span&gt; is given by the&amp;nbsp;complement,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{2} \label{model2}
p(y=0 \vert \vec{x}) \equiv 1 - p(y=1 \vert \vec{x}) = \frac{1}{1 + e^{ \vec{\beta} \cdot \vec{x} } }.
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
The latter equality above follows from simplifying algebra, after plugging in (\ref{model1}) for &lt;span class="math"&gt;\(p(y=1 \vert&amp;nbsp;\vec{x}).\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To fit the Logistic model to a training set &amp;#8212; i.e., to find a good choice for the fit parameter vector &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt; &amp;#8212; we consider here only the maximum-likelihood solution. This is that &lt;span class="math"&gt;\(\vec{\beta}^*\)&lt;/span&gt; that maximizes the conditional probability of observing the training data. The essential results we review below are 1) a proof that the maximum likelihood solution can be found by gradient descent, and 2) a derivation for the asymptotic covariance matrix of &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt;. This latter result provides the basis for returning point estimate confidence&amp;nbsp;intervals.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2017/07/errorbar.png"&gt;&lt;img alt="errorbar" src="https://efavdb.com/wp-content/uploads/2017/07/errorbar.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;On our GitHub &lt;a href="https://github.com/EFavDB/logistic-regression-with-error"&gt;page&lt;/a&gt;, we provide a Jupyter notebook that contains some minimal code extending the SKLearn LogisticRegression class. This extension makes use of the results presented here and allows for class probability confidence intervals to be returned for individual test points. In the notebook, we apply the algorithm to the SKLearn Iris dataset. The figure at right illustrates the output of the algorithm along a particular cut through the Iris data set parameter space. The y-axis represents the probability of a given test point belong to Iris class &lt;span class="math"&gt;\(1\)&lt;/span&gt;. The error bars in the plot provide insight that is completely missed when considering the point estimates only. For example, notice that the error bars are quite large for each of the far right points, despite the fact that the point estimates there are each near &lt;span class="math"&gt;\(1\)&lt;/span&gt;. Without the error bars, the high probability of these point estimates might easily be misinterpreted as implying high model&amp;nbsp;confidence.&lt;/p&gt;
&lt;p&gt;Our derivations below rely on some prerequisites: Properties of covariance matrices, the multivariate Cramer-Rao theorem, and properties of maximum likelihood estimators. These concepts are covered in two of our prior posts [&lt;span class="math"&gt;\(1\)&lt;/span&gt;, &lt;span class="math"&gt;\(2\)&lt;/span&gt;].&lt;/p&gt;
&lt;h3&gt;Optimization by gradient&amp;nbsp;descent&lt;/h3&gt;
&lt;p&gt;In this section, we derive expressions for the gradient of the negative-log likelihood loss function and also demonstrate that this loss is everywhere convex. The latter result is important because it implies that gradient descent can be used to find the maximum likelihood&amp;nbsp;solution.&lt;/p&gt;
&lt;p&gt;Again, to fit the logistic model to a training set, our aim is to find &amp;#8212; and also to set the parameter vector to &amp;#8212; the maximum likelihood value. Assuming the training set samples are independent, the likelihood of observing the training set labels is given&amp;nbsp;by
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
L &amp;amp;\equiv&amp;amp; \prod_i p(y_i \vert \vec{x}_i) \\
&amp;amp;=&amp;amp; \prod_{i: y_i = 1} \frac{1}{1 + e^{-\vec{\beta} \cdot \vec{x}_i}} \prod_{i: y_i = 0} \frac{1}{1 + e^{\vec{\beta} \cdot \vec{x}_i}}.
\tag{3} \label{likelihood}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Maximizing this is equivalent to minimizing its negative logarithm &amp;#8212; a cost function that is somewhat easier to work&amp;nbsp;with,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
J &amp;amp;\equiv&amp;amp; -\log L \\
&amp;amp;=&amp;amp; \sum_{\{i: y_i = 1 \}} \log \left (1 + e^{- \vec{\beta} \cdot \vec{x}_i } \right ) + \sum_{\{i: y_i = 0 \}} \log \left (1 + e^{\vec{\beta} \cdot \vec{x}_i } \right ).
\tag{4} \label{costfunction}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
The maximum-likelihood solution, &lt;span class="math"&gt;\(\vec{\beta}^*\)&lt;/span&gt;, is that coefficient vector that minimizes the above. Note that &lt;span class="math"&gt;\(\vec{\beta}^*\)&lt;/span&gt; will be a function of the random sample, and so will itself be a random variable &amp;#8212; characterized by a distribution having some mean value, covariance, etc. Given enough samples, a theorem on maximum-likelihood asymptotics (Cramer-Rao) guarantees that this distribution will be unbiased &amp;#8212; i.e., it will have mean value given by the correct parameter values &amp;#8212; and will also be of minimal covariance [&lt;span class="math"&gt;\(1\)&lt;/span&gt;]. This theorem is one of the main results motivating use of the maximum-likelihood&amp;nbsp;solution.&lt;/p&gt;
&lt;p&gt;Because &lt;span class="math"&gt;\(J\)&lt;/span&gt; is convex (demonstrated below), the logistic regression maximum-likelihood solution can always be found by gradient descent. That is, one need only iteratively update &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt; in the direction of the negative &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt;-gradient of &lt;span class="math"&gt;\(J\)&lt;/span&gt;, which&amp;nbsp;is
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
- \nabla_{\vec{\beta}} J &amp;amp;=&amp;amp; \sum_{\{i: y_i = 1 \}}\vec{x}_i \frac{ e^{- \vec{\beta} \cdot \vec{x}_i } }{1 + e^{- \vec{\beta} \cdot \vec{x}_i }}
- \sum_{\{i: y_i = 0 \}} \vec{x}_i \frac{ e^{\vec{\beta} \cdot \vec{x}_i }}{1 + e^{\vec{\beta} \cdot \vec{x}_i } } \\
&amp;amp;\equiv&amp;amp; \sum_{\{i: y_i = 1 \}}\vec{x}_i p(y=0 \vert \vec{x}_i)
-\sum_{\{i: y_i = 0 \}} \vec{x}_i p(y= 1 \vert \vec{x}_i). \tag{5} \label{gradient}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Notice that the terms that contribute the most here are those that are most strongly misclassified &amp;#8212; i.e., those where the model&amp;#8217;s predicted probability for the observed class is very low. For example, a point with true label &lt;span class="math"&gt;\(y=1\)&lt;/span&gt; but large model &lt;span class="math"&gt;\(p(y=0 \vert \vec{x})\)&lt;/span&gt; will contribute a significant push on &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt; in the direction of &lt;span class="math"&gt;\(\vec{x}\)&lt;/span&gt; &amp;#8212; so that the model will be more likely to predict &lt;span class="math"&gt;\(y=1\)&lt;/span&gt; at this point going forward. Notice that the contribution of a term above is also proportional to the length of its feature vector &amp;#8212; training points further from the origin have a stronger impact on the optimization process than those near the origin (at fixed classification&amp;nbsp;difficulty).&lt;/p&gt;
&lt;p&gt;The Hessian (second partial derivative) matrix of the cost function follows from taking a second gradient of the above. With a little algebra, one can show that this has &lt;span class="math"&gt;\(i-j\)&lt;/span&gt; component given&amp;nbsp;by,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
H(J)_{ij} &amp;amp;\equiv&amp;amp; -\partial_{\beta_j} \partial_{\beta_i} \log L \\
&amp;amp;=&amp;amp; \sum_k x_{k; i} x_{k; j} p(y= 0 \vert \vec{x}_k) p(y= 1 \vert \vec{x}_k). \tag{6} \label{Hessian}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
We can prove that this is positive semi-definite using the fact that a matrix &lt;span class="math"&gt;\(M\)&lt;/span&gt; is necessarily positive semi-definite if &lt;span class="math"&gt;\(\vec{s}^T \cdot M \cdot \vec{s} \geq 0\)&lt;/span&gt; for all real &lt;span class="math"&gt;\(\vec{s}\)&lt;/span&gt; [&lt;span class="math"&gt;\(2\)&lt;/span&gt;]. Dotting our Hessian above on both sides by an arbitrary vector &lt;span class="math"&gt;\(\vec{s}\)&lt;/span&gt;, we&amp;nbsp;obtain
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
\vec{s}^T \cdot H \cdot \vec{s} &amp;amp;\equiv&amp;amp; \sum_k \sum_{ij} s_i x_{k; i} x_{k; j} s_j p(y= 0 \vert \vec{x}_k) p(y= 1 \vert \vec{x}_k) \\
&amp;amp;=&amp;amp; \sum_k \vert \vec{s} \cdot \vec{x}_k \vert^2 p(y= 0 \vert \vec{x}_k) p(y= 1 \vert \vec{x}_k) \geq 0.
\tag{7} \label{convex}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
The last form follows from the fact that both &lt;span class="math"&gt;\(p(y= 0 \vert \vec{x}_k)\)&lt;/span&gt; and &lt;span class="math"&gt;\(p(y= 1 \vert \vec{x}_k)\)&lt;/span&gt; are non-negative. This holds for any &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt; and any &lt;span class="math"&gt;\(\vec{s}\)&lt;/span&gt;, which implies that our Hessian is everywhere positive semi-definite. Because of this, convex optimization strategies &amp;#8212; e.g., gradient descent &amp;#8212; can always be applied to find the global maximum-likelihood&amp;nbsp;solution.&lt;/p&gt;
&lt;h3&gt;Coefficient uncertainty and significance&amp;nbsp;tests&lt;/h3&gt;
&lt;p&gt;The solution &lt;span class="math"&gt;\(\vec{\beta}^*\)&lt;/span&gt; that minimizes &lt;span class="math"&gt;\(J\)&lt;/span&gt; &amp;#8212; which can be found by gradient descent &amp;#8212; is a maximum likelihood estimate. In the asymptotic limit of a large number of samples, maximum-likelihood parameter estimates satisfy the Cramer-Rao lower bound [&lt;span class="math"&gt;\(2\)&lt;/span&gt;]. That is, the parameter covariance matrix satisfies [&lt;span class="math"&gt;\(3\)&lt;/span&gt;],
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
\text{cov}(\vec{\beta}^*, \vec{\beta}^*) &amp;amp;\sim&amp;amp; H(J)^{-1} \\
&amp;amp;\approx&amp;amp; \frac{1}{\sum_k \vec{x}_{k} \vec{x}_{k}^T p(y= 0 \vert \vec{x}_k) p(y= 1 \vert \vec{x}_k)}.
\tag{8} \label{covariance}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Notice that the covariance matrix will be small if the denominator above is large. Along a given direction, this requires that the training set contains samples over a wide range of values in that direction (we discuss this at some length in the analogous section of our post on Linear Regression [&lt;span class="math"&gt;\(4\)&lt;/span&gt;]). For a term to contribute in the denominator, the model must also have some confusion about its values: If there are no difficult-to-classify training examples, this means that there are no examples near the decision boundary. When this occurs, there will necessarily be a lot of flexibility in where the decision boundary is placed, resulting in large parameter&amp;nbsp;variances.&lt;/p&gt;
&lt;p&gt;Although the form above only holds in the asymptotic limit, we can always use it to approximate the true covariance matrix &amp;#8212; keeping in mind that the accuracy of the approximation will degrade when working with small training sets. For example, using (\ref{covariance}), the asymptotic variance for a single parameter can be approximated&amp;nbsp;by
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
\tag{9} \label{single_cov}
\sigma^2_{\beta^*_i} = \text{cov}(\vec{\beta}^*, \vec{\beta}^*)_{ii}.
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
In the asymptotic limit, the maximum-likelihood parameters will be Normally-distributed [&lt;span class="math"&gt;\(1\)&lt;/span&gt;], so we can provide confidence intervals for the parameters&amp;nbsp;as
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
\tag{10} \label{parameter_interval}
\beta_i \in \left ( \beta^*_i - z \sigma_{\beta^*_i}, \beta_i^* + z \sigma_{\beta^*_i} \right),
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
where the value of &lt;span class="math"&gt;\(z\)&lt;/span&gt; sets the size of the interval. For example, choosing &lt;span class="math"&gt;\(z = 2\)&lt;/span&gt; gives an interval construction procedure that will cover the true value approximately &lt;span class="math"&gt;\(95%\)&lt;/span&gt; of the time &amp;#8212; a result of Normal statistics [&lt;span class="math"&gt;\(5\)&lt;/span&gt;]. Checking which intervals do not cross zero provides a method for identifying which features contribute significantly to a given&amp;nbsp;fit.&lt;/p&gt;
&lt;h3&gt;Prediction confidence&amp;nbsp;intervals&lt;/h3&gt;
&lt;p&gt;The probability of class &lt;span class="math"&gt;\(1\)&lt;/span&gt; for a test point &lt;span class="math"&gt;\(\vec{x}\)&lt;/span&gt; is given by (\ref{model1}). Notice that this depends on &lt;span class="math"&gt;\(\vec{x}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt; only through the dot product &lt;span class="math"&gt;\(\vec{x} \cdot \vec{\beta}\)&lt;/span&gt;. At fixed &lt;span class="math"&gt;\(\vec{x}\)&lt;/span&gt;, the variance (uncertainty) in this dot product follows from the coefficient covariance matrix above: We have [&lt;span class="math"&gt;\(2\)&lt;/span&gt;],
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
\tag{11} \label{logit_var}
\sigma^2_{\vec{x} \cdot \vec{\beta}} \equiv \vec{x}^T \cdot \text{cov}(\vec{\beta}^*, \vec{\beta}^*) \cdot \vec{x}.
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
With this result, we can obtain an expression for the confidence interval for the dot product, or equivalently a confidence interval for the class probability. For example, the asymptotic interval for class &lt;span class="math"&gt;\(1\)&lt;/span&gt; probability is given&amp;nbsp;by
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
\tag{12} \label{prob_interval}
p(y=1 \vert \vec{x}) \in \left ( \frac{1}{1 + e^{- \vec{x} \cdot \vec{\beta}^* + z \sigma_{\vec{x} \cdot \vec{\beta}^*}}}, \frac{1}{1 + e^{- \vec{x} \cdot \vec{\beta}^* - z \sigma_{\vec{x} \cdot \vec{\beta}^*}}} \right),
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(z\)&lt;/span&gt; again sets the size of the interval as above (&lt;span class="math"&gt;\(z=2\)&lt;/span&gt; gives a &lt;span class="math"&gt;\(95%\)&lt;/span&gt; confidence interval, etc. [&lt;span class="math"&gt;\(5\)&lt;/span&gt;]), and &lt;span class="math"&gt;\(\sigma_{\vec{x} \cdot \vec{\beta}^*}\)&lt;/span&gt; is obtained from (\ref{covariance}) and&amp;nbsp;(\ref{logit_var}).&lt;/p&gt;
&lt;p&gt;The results (\ref{covariance}), (\ref{logit_var}), and (\ref{prob_interval}) are used in our Jupyter notebook. There we provide code for a minimal Logistic Regression class implementation that returns both point estimates and prediction confidence intervals for each test point. We used this code to generate the plot shown in the post introduction. Again, the code can be downloaded &lt;a href="https://github.com/EFavDB/logistic-regression-with-error"&gt;here&lt;/a&gt; if you are interested in trying it&amp;nbsp;out.&lt;/p&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;In this note, we have 1) reviewed how to fit a logistic regression model to a binary data set for classification purposes, and 2) have derived the expressions needed to return class membership probability confidence intervals for test&amp;nbsp;points.&lt;/p&gt;
&lt;p&gt;Confidence intervals are typically not available for many out-of-the-box machine learning models, despite the fact that intervals can often provide significant utility. The fact that logistic regression allows for meaningful error bars to be returned with relative ease is therefore a notable, advantageous&amp;nbsp;property.&lt;/p&gt;
&lt;h3&gt;Footnotes&lt;/h3&gt;
&lt;p&gt;[&lt;span class="math"&gt;\(1\)&lt;/span&gt;] Our notes on the maximum-likelihood estimators can be found &lt;a href="http://efavdb.github.io/maximum-likelihood-asymptotics"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;[&lt;span class="math"&gt;\(2\)&lt;/span&gt;] Our notes on covariance matrices and the multivariate Cramer-Rao theorem can be found &lt;a href="http://efavdb.github.io/multivariate-cramer-rao-bound"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;[&lt;span class="math"&gt;\(3\)&lt;/span&gt;] The Cramer-Rao identity [&lt;span class="math"&gt;\(2\)&lt;/span&gt;] states that covariance matrix of the maximum-likelihood estimators approaches the Hessian matrix of the log-likelihood, evaluated at their true values. Here, we approximate this by evaluating the Hessian at the maximum-likelihood point&amp;nbsp;estimate.&lt;/p&gt;
&lt;p&gt;[&lt;span class="math"&gt;\(4\)&lt;/span&gt;] Our notes on linear regression can be found &lt;a href="http://efavdb.github.io/linear-regression"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;[&lt;span class="math"&gt;\(5\)&lt;/span&gt;] Our notes on Normal distributions can be found &lt;a href="http://efavdb.github.io/normal-distributions"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Methods, Statistics, Theory"></category></entry><entry><title>Normal Distributions</title><link href="https://efavdb.com/normal-distributions" rel="alternate"></link><published>2017-05-13T21:48:00-07:00</published><updated>2017-05-13T21:48:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2017-05-13:/normal-distributions</id><summary type="html">&lt;p&gt;I review &amp;#8212; and provide derivations for &amp;#8212; some basic properties of Normal distributions. Topics currently covered: (i) Their normalization, (ii) Samples from a univariate Normal, (iii) Multivariate Normal distributions, (iv) Central limit&amp;nbsp;theorem.&lt;/p&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2017/05/carl-f-gauss-4.jpg"&gt;&lt;img alt="carl-f-gauss-4" src="https://efavdb.com/wp-content/uploads/2017/05/carl-f-gauss-4.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This post contains a running list of properties (with derivations) relating to Normal (Gaussian) distributions. Normal distributions …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I review &amp;#8212; and provide derivations for &amp;#8212; some basic properties of Normal distributions. Topics currently covered: (i) Their normalization, (ii) Samples from a univariate Normal, (iii) Multivariate Normal distributions, (iv) Central limit&amp;nbsp;theorem.&lt;/p&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2017/05/carl-f-gauss-4.jpg"&gt;&lt;img alt="carl-f-gauss-4" src="https://efavdb.com/wp-content/uploads/2017/05/carl-f-gauss-4.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This post contains a running list of properties (with derivations) relating to Normal (Gaussian) distributions. Normal distributions are important for two principal reasons: Their significance a la the central limit theorem and their appearance in saddle point approximations to more general integrals. As usual, the results here assume familiarity with calculus and linear&amp;nbsp;algebra.&lt;/p&gt;
&lt;p&gt;Pictured at right is an image of Gauss &amp;#8212; &amp;#8220;Few, but&amp;nbsp;ripe.&amp;#8221;&lt;/p&gt;
&lt;h3&gt;Normalization&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Consider the integral
    &lt;div class="math"&gt;\begin{align} \tag{1}
    I = \int_{-\infty}^{\infty} e^{-x^2} dx.
    \end{align}&lt;/div&gt;
    To evaluate, consider the value of &lt;span class="math"&gt;\(I^2\)&lt;/span&gt;. This is
    &lt;div class="math"&gt;\begin{align}\tag{2}
    I^2 &amp;amp;= \int_{-\infty}^{\infty} e^{-x^2} dx \int_{-\infty}^{\infty} e^{-y^2} dy \\
    &amp;amp;= \int_0^{\infty} e^{-r^2} 2 \pi r dr = -\pi e^{-r^2} \vert_0^{\infty} = \pi.
    \end{align}&lt;/div&gt;
    Here, I have used the usual trick of transforming the integral over the plane to one over polar &lt;span class="math"&gt;\((r, \theta)\)&lt;/span&gt; coordinates. The result above gives the normalization for the Normal&amp;nbsp;distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Samples from a univariate&amp;nbsp;normal&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Suppose &lt;span class="math"&gt;\(N\)&lt;/span&gt; independent samples are taken from a Normal distribution. The sample mean is defined as &lt;span class="math"&gt;\(\hat{\mu} = \frac{1}{N}\sum x_i\)&lt;/span&gt; and the sample variance as &lt;span class="math"&gt;\(\hat{S}^2 \equiv \frac{1}{N-1} \sum (x_i - \hat{\mu})^2\)&lt;/span&gt;. These two statistics are independent. Further, the former is Normal distributed with variance &lt;span class="math"&gt;\(\sigma^2/N\)&lt;/span&gt; and the latter is proportional to a &lt;span class="math"&gt;\(\chi_{N-1}^2.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Proof:&lt;/em&gt; Let the sample be &lt;span class="math"&gt;\(\textbf{x} = (x_1, x_2, \ldots, x_N)\)&lt;/span&gt;. Then the mean can be written as &lt;span class="math"&gt;\(\textbf{x} \cdot \textbf{1}/N\)&lt;/span&gt;, the projection of &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; along &lt;span class="math"&gt;\(\textbf{1}/N\)&lt;/span&gt;. Similarly, the sample variance can be expressed as the squared length of &lt;span class="math"&gt;\(\textbf{x} - (\textbf{x} \cdot \textbf{1} / N)\textbf{1} = \textbf{x} - (\textbf{x} \cdot \textbf{1} / \sqrt{N})\textbf{1}/\sqrt{N}\)&lt;/span&gt;, which is the squared length of &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; projected into the space orthogonal to &lt;span class="math"&gt;\(\textbf{1}\)&lt;/span&gt;. The independence of the &lt;span class="math"&gt;\(\{x_i\}\)&lt;/span&gt; implies that these two variables are themselves independent, the former Normal and the latter &lt;span class="math"&gt;\(\chi^2_{N-1}.\)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The result above implies that the weight for sample &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; can be written as
    &lt;div class="math"&gt;\begin{align} \tag{3}
    p(\textbf{x} \vert \mu, \sigma^2) = \frac{1}{(2 \pi \sigma^2)^{N/2}} e^{\left (N (\hat{\mu} - \mu)^2 + (N-1)S^2\right)/(2 \sigma^2) }.
    \end{align}&lt;/div&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Aside on sample variance: Given independent samples from any distribution, dividing by &lt;span class="math"&gt;\(N-1\)&lt;/span&gt; gives an unbiased estimate for the population variance. However, if the samples are not independent (eg, direct trace from &lt;span class="caps"&gt;MCMC&lt;/span&gt;), this factor is not appropriate: We have
    &lt;div class="math"&gt;\begin{align} \nonumber
    (N-1)E(S^2) &amp;amp;= E(\sum (x_i - \overline{x})^2) \\
    &amp;amp;= E(\sum (x_i - \mu)^2 - N ( \overline{x} - \mu)^2 ) \\ \tag{4}
    &amp;amp;= N [\sigma^2 - \text{var}(\overline{x})] \label{sample_var}
    \end{align}&lt;/div&gt;
    If the samples are independent, the above gives &lt;span class="math"&gt;\((N-1) \sigma^2\)&lt;/span&gt;. However, if the samples are all the same, &lt;span class="math"&gt;\(\text{var}(\overline{x}) = \sigma^2\)&lt;/span&gt;, giving &lt;span class="math"&gt;\(S^2=0\)&lt;/span&gt;. In general, the relationship between the samples determines whether &lt;span class="math"&gt;\(S^2\)&lt;/span&gt; is biased or&amp;nbsp;not.&lt;/li&gt;
&lt;li&gt;From the results above, the quantity
    &lt;div class="math"&gt;\begin{align} \label{t-var} \tag{5}
    (\hat{\mu}- \mu)/(S/\sqrt(N))
    \end{align}&lt;/div&gt;
    is the ratio of two independent variables &amp;#8212; the numerator a Normal and the denominator the square root of an independent &lt;span class="math"&gt;\(\chi^2_{N-1}\)&lt;/span&gt; variable. This quantity follows a universal distribution called the &lt;span class="math"&gt;\(t\)&lt;/span&gt;-distribution. One can write down closed-form expressions for the &lt;span class="math"&gt;\(t\)&lt;/span&gt;. For example, when &lt;span class="math"&gt;\(N=2\)&lt;/span&gt;, you get a Cauchy variable: the ratio of one Normal over the absolute value of another, independent Normal (see above). In general, &lt;span class="math"&gt;\(t\)&lt;/span&gt;-distributions have power law tails. A key point is that we cannot evaluate (\ref{t-var}) numerically if we do not know &lt;span class="math"&gt;\(\mu\)&lt;/span&gt;. Nevertheless, we can use the known distribution of the above to specify its likely range. Using this, we can then construct a confidence interval for &lt;span class="math"&gt;\(\mu\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Consider now a situation where you have two separate Normal distributions. To compare their variances you can take samples from the two and then construct the quantity
    &lt;div class="math"&gt;\begin{align}\label{f-var} \tag{6}
    \frac{S_x / \sigma_x}{ S_y/ \sigma_y}.
    \end{align}&lt;/div&gt;
    This is the ratio of two independent &lt;span class="math"&gt;\(\chi^2\)&lt;/span&gt; variables, resulting in what is referred to as an &lt;span class="math"&gt;\(F\)&lt;/span&gt;-distributed variable. Like (\ref{t-var}), we often cannot evaluate (\ref{f-var}) numerically. Instead, we use a tabulated cdf of the &lt;span class="math"&gt;\(F\)&lt;/span&gt;-distribution to derive confidence intervals for the ratio of the two underlying variances. Aside: The &lt;span class="math"&gt;\(F\)&lt;/span&gt;-distribution arises in the analysis of both &lt;span class="caps"&gt;ANOVA&lt;/span&gt; and linear regression. Note also that the square of a &lt;span class="math"&gt;\(t\)&lt;/span&gt;-distributed variable (Normal over the square root of a &lt;span class="math"&gt;\(\chi^2\)&lt;/span&gt; variable) is &lt;span class="math"&gt;\(F\)&lt;/span&gt;-distributed.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Multivariate&amp;nbsp;Normals&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Consider a set of jointly-distributed variables &lt;span class="math"&gt;\(x\)&lt;/span&gt; having normal distribution
    &lt;div class="math"&gt;\begin{align} \tag{7}
    p(x) = \sqrt{\frac{ \text{det}(M)} {2 \pi}} \exp \left [- \frac{1}{2} x^T \cdot M \cdot x \right ],
    \end{align}&lt;/div&gt;
    with &lt;span class="math"&gt;\(M\)&lt;/span&gt; a real, symmetric matrix. The correlation of two components is given by
    &lt;div class="math"&gt;\begin{align}\tag{8}
    \langle x_i x_j \rangle = M^{-1}_{ij}.
    \end{align}&lt;/div&gt;
&lt;em&gt;Proof:&lt;/em&gt; Let
    &lt;div class="math"&gt;\begin{align}\tag{9}
    I = \int dx \exp \left [- \frac{1}{2} x^T \cdot M \cdot x \right ].
    \end{align}&lt;/div&gt;
    Then,
    &lt;div class="math"&gt;\begin{align}\tag{10}
    \partial_{M_{ij}} \log I = -\frac{1}{2} \langle x_i x_j \rangle.
    \end{align}&lt;/div&gt;
    We can also evaluate this using the normalization of the integral as
    &lt;div class="math"&gt;\begin{align} \nonumber
    \partial_{M_{ij}} \log I &amp;amp;= - \frac{1}{2} \sum_{\alpha} \frac{1}{\lambda_{\alpha}} \partial_{M_{ij}} \lambda_{\alpha} \\ \nonumber
    &amp;amp;= - \frac{1}{2} \sum_{\alpha} \frac{1}{\lambda_{\alpha}} v_{\alpha i } v_{\alpha j} \\
    &amp;amp;= - \frac{1}{2} M^{-1}_{ij}. \tag{11}
    \end{align}&lt;/div&gt;
    Here, I&amp;#8217;ve used the result &lt;span class="math"&gt;\( \partial_{M_{ij}} \lambda_{\alpha} = v_{\alpha i } v_{\alpha j}\)&lt;/span&gt;. I give a proof of this next. The last line follows by expressing &lt;span class="math"&gt;\(M\)&lt;/span&gt; in terms of its eigenbasis. Comparing the last two lines above gives the&amp;nbsp;result.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Consider a matrix &lt;span class="math"&gt;\(M\)&lt;/span&gt; having eigenvalues &lt;span class="math"&gt;\(\{\lambda_{\alpha}\}\)&lt;/span&gt;. The first derivative of &lt;span class="math"&gt;\(\lambda_{\alpha}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(M_{ij}\)&lt;/span&gt; is given by &lt;span class="math"&gt;\(v_{\alpha, i} v_{\alpha, j}\)&lt;/span&gt;, where &lt;span class="math"&gt;\(v_{\alpha}\)&lt;/span&gt; is the unit eigenvector corresponding to the eigenvalue &lt;span class="math"&gt;\(\lambda_{\alpha}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Proof:&lt;/em&gt; The eigenvalue in question is given by
&lt;div class="math"&gt;\begin{align} \tag{12}
\lambda_{\alpha} = \sum_{ij} v_{\alpha i} M_{ij} v_{\alpha j}.
\end{align}&lt;/div&gt;
If we differentiate with respect to &lt;span class="math"&gt;\(M_{ab}\)&lt;/span&gt;, say, we obtain
&lt;div class="math"&gt;\begin{align} \nonumber
\partial_{M_{ab}} \lambda_{\alpha} &amp;amp;= \sum_{ij} \delta_{ia} \delta_{jb} v_{\alpha i} v_{\alpha j} + 2 v_{\alpha i} M_{ij} \partial_{M_{ab}} v_{\alpha j} \\
&amp;amp;= v_{\alpha a} v_{\alpha b} + 2 \lambda_{\alpha} v_{\alpha } \cdot \partial_{M_{ab}} v_{\alpha }
\tag{13}.
\end{align}&lt;/div&gt;
The last term above must be zero since the length of &lt;span class="math"&gt;\(v_{\alpha }\)&lt;/span&gt; is fixed at &lt;span class="math"&gt;\(1\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The conditional distribution. Let &lt;span class="math"&gt;\(x\)&lt;/span&gt; be a vector of jointly distributed variables of mean zero and covariance matrix &lt;span class="math"&gt;\(\Sigma\)&lt;/span&gt;. If we segment the variables into two sets, &lt;span class="math"&gt;\(x_0\)&lt;/span&gt; and &lt;span class="math"&gt;\(x_1\)&lt;/span&gt;, the distribution of &lt;span class="math"&gt;\(x_1\)&lt;/span&gt; at fixed &lt;span class="math"&gt;\(x_0\)&lt;/span&gt; is also normal. Here, we find the mean and covariance. We have
    &lt;div class="math"&gt;\begin{align} \label{multivargaucond} \tag{14}
    p(x) = \mathcal{N} e^{-\frac{1}{2} x_0^T \Sigma^{-1}_{00} x_0} e^{ -\frac{1}{2} \left \{ x_1^T \Sigma^{-1}_{11} x_1 + 2 x_1^T \Sigma^{-1}_{10} x_0 \right \} }
    \end{align}&lt;/div&gt;
    Here, &lt;span class="math"&gt;\(\Sigma^{-1}_{ij}\)&lt;/span&gt; refers to the &lt;span class="math"&gt;\(i-j\)&lt;/span&gt; block of the inverse. To complete the square, we write
    &lt;div class="math"&gt;\begin{align} \tag{15}
    x_1^T \Sigma^{-1}_{11} x_1 + 2 x_1^T \Sigma^{-1}_{10} x_0 + c = (x_1^T + a) \Sigma^{-1}_{11} ( x_1 + a).
    \end{align}&lt;/div&gt;
    Comparing both sides, we find
    &lt;div class="math"&gt;\begin{align} \tag{16}
    x_1^T \Sigma^{-1}_{10} x_0 = x_1^T \Sigma^{-1}_{11} a
    \end{align}&lt;/div&gt;
    This holds for any value of &lt;span class="math"&gt;\(x_1^T\)&lt;/span&gt;, so we must have
    &lt;div class="math"&gt;\begin{align}\tag{17}
    a = \left( \Sigma^{-1}_{11} \right)^{-1} \Sigma^{-1}_{10} x_0 .
    \end{align}&lt;/div&gt;
    Plugging the last few results into (\ref{multivargaucond}), we obtain
    &lt;div class="math"&gt;\begin{align} \nonumber
    p(x) = \mathcal{N} e^{-\frac{1}{2} x_0^T \left( \Sigma^{-1}_{00} -
    \Sigma^{-1}_{01} \left( \Sigma^{-1}_{11} \right)^{-1} \Sigma^{-1}_{10} \right) x_0}\times  \\
    e^{ -\frac{1}{2} \left (x_1 + \left( \Sigma^{-1}_{11} \right)^{-1} \Sigma^{-1}_{10} x_0 \right) \Sigma^{-1}_{11} \left (x_1 + \left( \Sigma^{-1}_{11} \right)^{-1} \Sigma^{-1}_{10} x_0 \right) } \tag{18} \label{multivargaucondfix}
    \end{align}&lt;/div&gt;
    This shows that &lt;span class="math"&gt;\(x_0\)&lt;/span&gt; and &lt;span class="math"&gt;\(x_1 + \left( \Sigma^{-1}_{11} \right)^{-1} \Sigma^{-1}_{10} x_0\)&lt;/span&gt; are independent. This formula also shows that the average value of &lt;span class="math"&gt;\(x_1\)&lt;/span&gt; shifts at fixed &lt;span class="math"&gt;\(x_0\)&lt;/span&gt;,
    &lt;div class="math"&gt;\begin{align}\tag{19}
    \langle x_1 \rangle = \langle x_1 \rangle_0 - \left( \Sigma^{-1}_{11} \right)^{-1} \Sigma^{-1}_{10} x_0.
    \end{align}&lt;/div&gt;
    With some work, we can rewrite this as
    &lt;div class="math"&gt;\begin{align} \tag{20}
    \langle x_1 \rangle = \langle x_1 \rangle_0 + \Sigma_{10} \frac{1}{\Sigma_{00}}x_0.
    \end{align}&lt;/div&gt;
    There are two ways to prove this equivalent form holds. One is to make use of the expression for the inverse of a block matrix. The second is to note that the above is simply the linear response to a shift in &lt;span class="math"&gt;\(x_0\)&lt;/span&gt; &amp;#8212; see post on linear&amp;nbsp;regression.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;If we integrate over &lt;span class="math"&gt;\(x_1\)&lt;/span&gt; in (\ref{multivargaucondfix}), we obtain the distribution for &lt;span class="math"&gt;\(x_0\)&lt;/span&gt;. This is
    &lt;div class="math"&gt;\begin{align} \tag{21}
    p(x_0) = \mathcal{N} e^ {-\frac{1}{2} x_0^T \left( \Sigma^{-1}_{00} -
    \Sigma^{-1}_{01} \left( \Sigma^{-1}_{11} \right)^{-1} \Sigma^{-1}_{10} \right) x_0}
    \end{align}&lt;/div&gt;
    The block-diagonal inverse theorem can be used to show that this is equivalent to
    &lt;div class="math"&gt;\begin{align} \tag{22}
    p(x_0) = \mathcal{N} e^{ -\frac{1}{2} x_0^T \left( \Sigma_{00} \right)^{-1} x_0}
    \end{align}&lt;/div&gt;
    Another way to see this is correct is to make use of the fact that the coefficient matrix in the normal is the inverse of the correlation matrix. We know that after integrating out the values of &lt;span class="math"&gt;\(x_1\)&lt;/span&gt;, we remain normal, and the covariance matrix will simply be given by that for &lt;span class="math"&gt;\(x_0\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;The covariance of the &lt;span class="caps"&gt;CDF&lt;/span&gt; transform in multivariate case &amp;#8212; a result needed for fitting Gaussian Copulas to data: Let &lt;span class="math"&gt;\(x_1, x_2\)&lt;/span&gt; be jointly distributed Normal variables with covariance matrix
    &lt;div class="math"&gt;\begin{align}
    C = \left( \begin{array}{cc}
    1 &amp;amp; \rho \\
    \rho &amp;amp; 1
    \end{array} \right)
    \end{align}&lt;/div&gt;
    The &lt;span class="caps"&gt;CDF&lt;/span&gt; transform of &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; is defined as
    &lt;div class="math"&gt;\begin{align}
    X_i \equiv \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{x_i} \exp\left( -\frac{\tilde{x}_i^2}{2} \right)d\tilde{x}_i.
    \end{align}&lt;/div&gt;
    Here, we&amp;#8217;ll calculate the covariance of &lt;span class="math"&gt;\(X_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(X_2\)&lt;/span&gt;. Up to a constant that does not depend on &lt;span class="math"&gt;\(\rho\)&lt;/span&gt;, this is given by the integral
    &lt;div class="math"&gt;\begin{align}
    J \equiv &amp;amp;\frac{1}{\sqrt{(2 \pi)^2 \text{det} C}} \int d\vec{x} e^{-\frac{1}{2} \vec{x} \cdot C^{-1} \cdot \vec{x}} \times \\
    &amp;amp;\frac{1}{2 \pi} \int_{-\infty}^{x_1}\int_{-\infty}^{x_2} e^{-\frac{\tilde{x}_1^2}{2} -\frac{\tilde{x}_2^2}{2}}d\tilde{x}_1 d\tilde{x}_2.
    \end{align}&lt;/div&gt;
    To progress, we first write
    &lt;div class="math"&gt;\begin{align}
    \exp\left( -\frac{\tilde{x}_i^2}{2} \right ) = \frac{1}{\sqrt{2\pi }}\int \exp \left (- \frac{1}{2} k_i^2 + i k \tilde{x}_i \right )
    \end{align}&lt;/div&gt;
    We will substitute this equation into the prior line and then integrate over the &lt;span class="math"&gt;\(\tilde{x}_i\)&lt;/span&gt; using the result
    &lt;div class="math"&gt;\begin{align}
    \int_{-\infty}^{x_i} \exp \left ( i k \tilde{x}_i \right ) d \tilde{x}_i = \frac{e^{i k_i x_i}}{i k_i}.
    \end{align}&lt;/div&gt;
    This gives
    &lt;div class="math"&gt;\begin{align}
    J = &amp;amp;\frac{-1}{(2 \pi)^3 \sqrt{\text{det} C} } \int_{k_1} \int_{k_2} \frac{e^{-\frac{1}{2} (k_1^2 + k_2^2)}}{k_1 k_2} \times \\
    &amp;amp;\int d\vec{x} e^{-\frac{1}{2} \vec{x} \cdot C^{-1} \cdot \vec{x} + i \vec{k} \cdot \vec{x}}
    \end{align}&lt;/div&gt;
    The integral on &lt;span class="math"&gt;\(\vec{x}\)&lt;/span&gt; can now be carried out by completing the square. This gives
    &lt;div class="math"&gt;\begin{align}
    J = \frac{1}{(2 \pi)^2} \int_{k_1} \int_{k_2} \frac{1}{k_1 k_2}
    \exp\left( -\frac{1}{2} \vec{k} \cdot (C + I) \cdot \vec{k} \right)
    \end{align}&lt;/div&gt;
    We now differentiate with respect to &lt;span class="math"&gt;\(\rho\)&lt;/span&gt; to get rid of the &lt;span class="math"&gt;\(k_1 k_2\)&lt;/span&gt; in the denominator. This gives
    &lt;div class="math"&gt;\begin{align} \nonumber
    \partial_{\rho} J &amp;amp;= \frac{1}{(2 \pi)^2} \int_{k_1} \int_{k_2}
    \exp\left( -\frac{1}{2} \vec{k} \cdot (C + I) \cdot \vec{k} \right) \\ \nonumber
    &amp;amp;= \frac{1}{2 \pi } \frac{1}{\sqrt{\text{det}(C + I)}} \\
    &amp;amp;= \frac{1}{4 \pi } \frac{1}{\sqrt{1 - \frac{\rho^2}{4}}}.
    \end{align}&lt;/div&gt;
    The last step is to integrate with respect to &lt;span class="math"&gt;\(\rho\)&lt;/span&gt;, but we will now switch back to the original goal of calculating the covariance of the two &lt;span class="caps"&gt;CDF&lt;/span&gt; transforms, &lt;span class="math"&gt;\(P\)&lt;/span&gt;, rather than &lt;span class="math"&gt;\(J\)&lt;/span&gt; itself. At &lt;span class="math"&gt;\(\rho = 0\)&lt;/span&gt;, we must have &lt;span class="math"&gt;\(P(\rho=0) = 0\)&lt;/span&gt;, since the transforms will also be uncorrelated in this limit. This gives
    &lt;div class="math"&gt;\begin{align} \nonumber
    P &amp;amp;= \int_0^{\rho} \frac{1}{4 \pi } \frac{1}{\sqrt{1 - \frac{\rho^2}{4}}} d \rho \\
    &amp;amp;= \frac{1}{2 \pi } \sin^{-1} \left( \frac{\rho}{2} \right). \tag{23}
    \end{align}&lt;/div&gt;
    Using a similar calculation, we find that the diagonal terms of the &lt;span class="caps"&gt;CDF&lt;/span&gt; covariance matrix are &lt;span class="math"&gt;\(1/12\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Central Limit&amp;nbsp;Theorem&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Let &lt;span class="math"&gt;\(x_1, x_2, \ldots, x_N\)&lt;/span&gt; be &lt;span class="caps"&gt;IID&lt;/span&gt; random variables with an mgf that exists near &lt;span class="math"&gt;\(0\)&lt;/span&gt;. Let &lt;span class="math"&gt;\(E(x_i) = \mu\)&lt;/span&gt; and &lt;span class="math"&gt;\(\text{var}(x_i) = \sigma^2\)&lt;/span&gt;. Then the variable &lt;span class="math"&gt;\(\frac{\overline{x} - \mu}{\sigma / \sqrt{N}}\)&lt;/span&gt; approaches standard normal as &lt;span class="math"&gt;\(N \to \infty\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Proof:&lt;/em&gt; Let &lt;span class="math"&gt;\(y_i =\frac{x_i - \mu}{\sigma}\)&lt;/span&gt;. Then,
&lt;div class="math"&gt;\begin{align}\tag{24}
\tilde{y} \equiv \frac{\overline{x} - \mu}{\sigma / \sqrt{N}} = \frac{1}{\sqrt{N}} \sum_i y_i.
\end{align}&lt;/div&gt;
Using the fact that the mgf of a sum of independent variables is given by the product of their mgfs, the quantity at left is
&lt;div class="math"&gt;\begin{align} \tag{25}
m_{\tilde{y}}(t) = \left [ m_{y}\left (\frac{t}{\sqrt{N}} \right) \right]^n.
\end{align}&lt;/div&gt;
We now expand the term in brackets using a Taylor series, obtaining
&lt;div class="math"&gt;\begin{align} \tag{26}
m_{\tilde{y}}(t) &amp;amp;= \left [1 + \frac{t^2}{2 N } + O\left (\frac{t^3}{ N^{3/2}} \right) \right]^N \\ &amp;amp;\to \exp\left ( \frac{t^2}{2} \right),
\end{align}&lt;/div&gt;
where the latter form is the fixed &lt;span class="math"&gt;\(t\)&lt;/span&gt; limit as &lt;span class="math"&gt;\(N \to \infty\)&lt;/span&gt;. This is the mgf for a &lt;span class="math"&gt;\(N(0,1)\)&lt;/span&gt; variable, proving the&amp;nbsp;result.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;One can get a sense of the accuracy of the normal approximation at fixed &lt;span class="math"&gt;\(N\)&lt;/span&gt; through consideration of higher moments. For example, if we have an even distribution with mgf &lt;span class="math"&gt;\(1 + x^2 /2 + (1 + \kappa^{\prime}) x^4 / 8 + \ldots\)&lt;/span&gt;. Then the mgf for the scaled average above will be
    &lt;div class="math"&gt;\begin{align}\nonumber
    m_{\tilde{y}} &amp;amp;= \left [1 + \frac{t^2}{2 N } + \frac{(1 + \kappa^{\prime}) t^4}{8 N^2 } + \ldots \right]^N \\
    &amp;amp;= 1 + \frac{t^2}{2} + \left (1 + \frac{\kappa^{\prime}}{ N } \right) \frac{t^4}{8} + \ldots \tag{27}
    \end{align}&lt;/div&gt;
    This shows that the deviation in the kurtosis away from its &lt;span class="math"&gt;\(N(0,1)\)&lt;/span&gt; value decays like &lt;span class="math"&gt;\(1/N\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Statistics"></category><category term="theory"></category><category term="statistics"></category></entry><entry><title>Model AUC depends on test set difficulty</title><link href="https://efavdb.com/model-auc-depends-on-test-set-difficulty" rel="alternate"></link><published>2017-03-18T22:36:00-07:00</published><updated>2017-03-18T22:36:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2017-03-18:/model-auc-depends-on-test-set-difficulty</id><summary type="html">&lt;p&gt;The &lt;span class="caps"&gt;AUC&lt;/span&gt; score is a popular summary statistic that is often used to communicate the performance of a classifier. However, we illustrate here that this score depends not only on the quality of the model in question, but also on the difficulty of the test set considered: If samples are …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The &lt;span class="caps"&gt;AUC&lt;/span&gt; score is a popular summary statistic that is often used to communicate the performance of a classifier. However, we illustrate here that this score depends not only on the quality of the model in question, but also on the difficulty of the test set considered: If samples are added to a test set that are easily classified, the &lt;span class="caps"&gt;AUC&lt;/span&gt; will go up &amp;#8212; even if the model studied has not improved. In general, this behavior implies that isolated, single &lt;span class="caps"&gt;AUC&lt;/span&gt; scores cannot be used to meaningfully qualify a model&amp;#8217;s performance. Instead, the &lt;span class="caps"&gt;AUC&lt;/span&gt; should be considered a score that is primarily useful for comparing and ranking multiple models &amp;#8212; each at a common test set&amp;nbsp;difficulty.&lt;/p&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;An important challenge associated with building good classification algorithms centers around their optimization: If an adjustment is made to an algorithm, we need a score that will enable us to decide whether or not the change made was an improvement. Many scores are available for this purpose. A sort-of all-purpose score that is quite popular for characterizing binary classifiers is the model &lt;span class="caps"&gt;AUC&lt;/span&gt; score (defined&amp;nbsp;below).&lt;/p&gt;
&lt;p&gt;The purpose of this post is to illustrate a subtlety associated with the &lt;span class="caps"&gt;AUC&lt;/span&gt; that is not always appreciated: The score depends strongly on the difficulty of the test set used to measure model performance. In particular, if any soft-balls are added to a test set that are easily classified (i.e., are far from any decision boundary), the &lt;span class="caps"&gt;AUC&lt;/span&gt; will increase. This increase does not imply a model improvement. Two key take-aways&amp;nbsp;follow:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;span class="caps"&gt;AUC&lt;/span&gt; is an inappropriate score for comparing models validated on test sets having differing sampling distributions. Therefore, comparing the AUCs of models trained on samples having differing distributions requires care: The training sets can have different distributions, but the test sets must&amp;nbsp;not.&lt;/li&gt;
&lt;li&gt;A single &lt;span class="caps"&gt;AUC&lt;/span&gt; measure cannot typically be used to meaningfully communicate the quality of a single model (though single model &lt;span class="caps"&gt;AUC&lt;/span&gt; scores are often&amp;nbsp;reported!)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The primary utility of the &lt;span class="caps"&gt;AUC&lt;/span&gt; is that it allows one to compare multiple models at fixed test set difficulty: If a model change results in an increase in the &lt;span class="caps"&gt;AUC&lt;/span&gt; at fixed test set distribution, it can often be considered an&amp;nbsp;improvement.&lt;/p&gt;
&lt;p&gt;We review the definition of the &lt;span class="caps"&gt;AUC&lt;/span&gt; below and then demonstrate the issues alluded to&amp;nbsp;above.&lt;/p&gt;
&lt;h3&gt;The &lt;span class="caps"&gt;AUC&lt;/span&gt; score,&amp;nbsp;reviewed&lt;/h3&gt;
&lt;p&gt;Here, we quickly review the definition of the &lt;span class="caps"&gt;AUC&lt;/span&gt;. This is a score that can be used to quantify the accuracy of a binary classification algorithm on a given test set &lt;span class="math"&gt;\(\mathcal{S}\)&lt;/span&gt;. The test set consists of a set of feature vector-label pairs of the&amp;nbsp;form
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{1}
\mathcal{S} = \{(\textbf{x}_i, y_i) \}.
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;Here, &lt;span class="math"&gt;\(\textbf{x}_i\)&lt;/span&gt; is the set of features, or predictor variables, for example &lt;span class="math"&gt;\(i\)&lt;/span&gt; and &lt;span class="math"&gt;\(y_i \in \{0,1 \}\)&lt;/span&gt; is the label for example &lt;span class="math"&gt;\(i\)&lt;/span&gt;. A classifier function &lt;span class="math"&gt;\(\hat{p}_1(\textbf{x})\)&lt;/span&gt; is one that attempts to guess the value of &lt;span class="math"&gt;\(y_i\)&lt;/span&gt; given only the feature vector &lt;span class="math"&gt;\(\textbf{x}_i\)&lt;/span&gt;. In particular, the output of the function &lt;span class="math"&gt;\(\hat{p}_1(\textbf{x}_i)\)&lt;/span&gt; is an estimate for the probability that the label &lt;span class="math"&gt;\(y_i\)&lt;/span&gt; is equal to &lt;span class="math"&gt;\(1\)&lt;/span&gt;. If the algorithm is confident that the class is &lt;span class="math"&gt;\(1\)&lt;/span&gt; (&lt;span class="math"&gt;\(0\)&lt;/span&gt;), the probability returned will be large&amp;nbsp;(small).&lt;/p&gt;
&lt;p&gt;To characterize model performance, we can set a threshold value of &lt;span class="math"&gt;\(p^*\)&lt;/span&gt; and mark all examples in the test set with &lt;span class="math"&gt;\(\hat{p}(\textbf{x}_i) &amp;gt; p^*\)&lt;/span&gt; as being candidates for class one. The fraction of the truly positive examples in &lt;span class="math"&gt;\(\mathcal{S}\)&lt;/span&gt; marked in this way is referred to as the true-positive rate (&lt;span class="caps"&gt;TPR&lt;/span&gt;) at threshold &lt;span class="math"&gt;\(p^*\)&lt;/span&gt;. Similarly, the fraction of negative examples in &lt;span class="math"&gt;\(\mathcal{S}\)&lt;/span&gt; marked is referred to as the false-positive rate (&lt;span class="caps"&gt;FPR&lt;/span&gt;) at threshold &lt;span class="math"&gt;\(p^*\)&lt;/span&gt;. Plotting the &lt;span class="caps"&gt;TPR&lt;/span&gt; against the &lt;span class="caps"&gt;FPR&lt;/span&gt; across all thresholds gives the model&amp;#8217;s so-called receiver operating characteristic (&lt;span class="caps"&gt;ROC&lt;/span&gt;) curve. A hypothetical example is shown at right in blue. The dashed line is just the &lt;span class="math"&gt;\(y=x\)&lt;/span&gt; line, which corresponds to the &lt;span class="caps"&gt;ROC&lt;/span&gt; curve of a random classifier (one returning a uniform random &lt;span class="math"&gt;\(p\)&lt;/span&gt; value each&amp;nbsp;time).&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2017/03/example.png"&gt;&lt;img alt="example" src="https://efavdb.com/wp-content/uploads/2017/03/example.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Notice that if the threshold is set to &lt;span class="math"&gt;\(p^* = 1\)&lt;/span&gt;, no positive or negative examples will typically be marked as candidates, as this would require one-hundred percent confidence of class &lt;span class="math"&gt;\(1\)&lt;/span&gt;. This means that we can expect an &lt;span class="caps"&gt;ROC&lt;/span&gt; curve to always go through the point &lt;span class="math"&gt;\((0,0)\)&lt;/span&gt;. Similarly, with &lt;span class="math"&gt;\(p^*\)&lt;/span&gt; set to &lt;span class="math"&gt;\(0\)&lt;/span&gt;, all examples should be marked as candidates for class &lt;span class="math"&gt;\(1\)&lt;/span&gt; &amp;#8212; and so an &lt;span class="caps"&gt;ROC&lt;/span&gt; curve should also always go through the point &lt;span class="math"&gt;\((1,1)\)&lt;/span&gt;. In between, we hope to see a curve that increases in the &lt;span class="caps"&gt;TPR&lt;/span&gt; direction more quickly than in the &lt;span class="caps"&gt;FPR&lt;/span&gt; direction &amp;#8212; since this would imply that the examples the model is most confident about tend to actually be class &lt;span class="math"&gt;\(1\)&lt;/span&gt; examples. In general, the larger the Area Under the (&lt;span class="caps"&gt;ROC&lt;/span&gt;) Curve &amp;#8212; again, blue at right &amp;#8212; the better. We call this area the &amp;#8220;&lt;span class="caps"&gt;AUC&lt;/span&gt; score for the model&amp;#8221; &amp;#8212; the topic of this&amp;nbsp;post.&lt;/p&gt;
&lt;h3&gt;&lt;span class="caps"&gt;AUC&lt;/span&gt; sensitivity to test set&amp;nbsp;difficulty&lt;/h3&gt;
&lt;p&gt;To illustrate the sensitivity of the &lt;span class="caps"&gt;AUC&lt;/span&gt; score to test set difficulty, we now consider a toy classification problem: In particular, we consider a set of unit-variance normal distributions, each having a different mean &lt;span class="math"&gt;\(\mu_i\)&lt;/span&gt;. From each distribution, we will take a single sample &lt;span class="math"&gt;\(x_i\)&lt;/span&gt;. From this, we will attempt to estimate whether or not the corresponding mean satisfies &lt;span class="math"&gt;\(\mu_i &amp;gt; 0\)&lt;/span&gt;. That is, our training set will take the form &lt;span class="math"&gt;\(\mathcal{S} = \{(x_i, \mu_i)\}\)&lt;/span&gt;, where &lt;span class="math"&gt;\(x_i \sim N(\mu_i, 1)\)&lt;/span&gt;. For different &lt;span class="math"&gt;\(\mathcal{S}\)&lt;/span&gt;, we will study the &lt;span class="caps"&gt;AUC&lt;/span&gt; of the classifier&amp;nbsp;function,&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \label{classifier} \tag{2}
\hat{p}(x) = \frac{1}{2} (1 + \text{tanh}(x))
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
A plot of this function is shown below. You can see that if any test sample &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; is far to the right (left) of &lt;span class="math"&gt;\(x=0\)&lt;/span&gt;, the model will classify the sample as positive (negative) with high certainty. At intermediate values near the boundary, the estimated probability of being in the positive class lifts in a reasonable&amp;nbsp;way.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2017/03/classifier-2.png"&gt;&lt;img alt="classifier" src="https://efavdb.com/wp-content/uploads/2017/03/classifier-2.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Notice that if a test example has a mean very close to zero, it will be difficult to classify that example as positive or negative. This is because both positive and negative &lt;span class="math"&gt;\(x\)&lt;/span&gt; samples are equally likely in this case. This means that the model cannot do much better than a random guess for such &lt;span class="math"&gt;\(\mu\)&lt;/span&gt;. On the other hand, if an example &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; is selected that is very far from the origin, a single sample &lt;span class="math"&gt;\(x\)&lt;/span&gt; from &lt;span class="math"&gt;\(N(\mu, 1)\)&lt;/span&gt; will be sufficient to make a very good guess as to whether &lt;span class="math"&gt;\(\mu &amp;gt; 0\)&lt;/span&gt;. Such examples are hard to get wrong,&amp;nbsp;soft-balls.&lt;/p&gt;
&lt;p&gt;The impact of adding soft-balls to the test set on the &lt;span class="caps"&gt;AUC&lt;/span&gt; for model (\ref{classifier}) can be studied by changing the sampling distribution of &lt;span class="math"&gt;\(\mathcal{S}\)&lt;/span&gt;. The following python snippet takes samples &lt;span class="math"&gt;\(\mu_i\)&lt;/span&gt; from three distributions &amp;#8212; one tight about &lt;span class="math"&gt;\(0\)&lt;/span&gt; (resulting in a very difficult test set), one that is very wide containing many soft-balls that are easily classified, and one that is intermediate. The &lt;span class="caps"&gt;ROC&lt;/span&gt; curves that result from these three cases are shown following the code. The three curves are very different, with the &lt;span class="caps"&gt;AUC&lt;/span&gt; of the soft-ball set very large and that of the tight set close to that of the random classifier. Yet, in each case the model considered was the same &amp;#8212; (\ref{classifier}). How could the &lt;span class="caps"&gt;AUC&lt;/span&gt; have&amp;nbsp;improved?!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;metrics&lt;/span&gt;

&lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ax2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;3.5&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;SAMPLES&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;
&lt;span class="n"&gt;means_std&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;means_std&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;001&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
    &lt;span class="n"&gt;means&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;means_std&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;SAMPLES&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;x_set&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;means&lt;/span&gt;
    &lt;span class="n"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;x_set&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;fpr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tpr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;thresholds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;roc_curve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;means&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fpr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tpr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;means_std&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fpr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fpr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;k--&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;means&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;means&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;*&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;means_std&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ax1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;lower right&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shadow&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ax2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;lower right&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shadow&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ax1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;TPR versus FPR -- The ROC curve&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ax2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Means sampled for each case&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2017/03/Examples.png"&gt;&lt;img alt="Examples" src="https://efavdb.com/wp-content/uploads/2017/03/Examples.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The explanation for the differing &lt;span class="caps"&gt;AUC&lt;/span&gt; values above is clear: Consider, for example, the effect of adding soft-ball negatives to &lt;span class="math"&gt;\(\mathcal{S}\)&lt;/span&gt;. In this case, the model (\ref{classifier}) will be able to correctly identify almost all true positive examples at a much higher threshold than that where it begins to mis-classify the introduced negative softballs. This means that the &lt;span class="caps"&gt;ROC&lt;/span&gt; curve will now hit a &lt;span class="caps"&gt;TPR&lt;/span&gt; value of &lt;span class="math"&gt;\(1\)&lt;/span&gt; well-before the &lt;span class="caps"&gt;FPR&lt;/span&gt; does (which requires all negatives &amp;#8212; including the soft-balls to be mis-classified). Similarly, if many soft-ball positives are added in, these will be easily identified as such well-before any negative examples are mis-classified. This again results in a raising of the &lt;span class="caps"&gt;ROC&lt;/span&gt; curve, and an increase in &lt;span class="caps"&gt;AUC&lt;/span&gt; &amp;#8212; all without any improvement in the actual model quality, which we have held&amp;nbsp;fixed.&lt;/p&gt;
&lt;h3&gt;Discussion&lt;/h3&gt;
&lt;p&gt;The toy example considered above illustrates the general point the &lt;span class="caps"&gt;AUC&lt;/span&gt; of a model is really a function of both the model and the test set it is being applied to. Keeping this in mind will help to prevent incorrect interpretations of the &lt;span class="caps"&gt;AUC&lt;/span&gt;. A special case to watch out for in practice is the situation where the &lt;span class="caps"&gt;AUC&lt;/span&gt; changes upon adjustment of the training and testing protocol applied (which can result, for example, from changes to how training examples are collected for the model). If you see such a change occur in your work, be careful to consider whether or not it is possible that the difficulty of the test set has changed in the process. If so, the change in the &lt;span class="caps"&gt;AUC&lt;/span&gt; may not indicate a change in model&amp;nbsp;quality.&lt;/p&gt;
&lt;p&gt;Because the &lt;span class="caps"&gt;AUC&lt;/span&gt; score of a model can depend highly on the difficulty of the test set, reporting this score alone will generally not provide much insight into the accuracy of the model &amp;#8212; which really depends only on performance near the true decision boundary and not on soft-ball performance. Because of this, it may be a good practice to always report &lt;span class="caps"&gt;AUC&lt;/span&gt; scores for optimized models next to those of some fixed baseline model. Comparing the differences of the two &lt;span class="caps"&gt;AUC&lt;/span&gt; scores provides an approximate method for removing the effect of test set difficulty. If you come across an isolated, high &lt;span class="caps"&gt;AUC&lt;/span&gt; score in the wild, remember that this does not imply a good&amp;nbsp;model!&lt;/p&gt;
&lt;p&gt;A special situation exists where reporting an isolated &lt;span class="caps"&gt;AUC&lt;/span&gt; score for a single model can provide value: The case where the test set employed shares the same distribution as that of the application set (the space where the model will be employed). In this case, performance within the test set directly relates to expected performance during application. However, applying the &lt;span class="caps"&gt;AUC&lt;/span&gt; to situations such as this is not always useful. For example, if the positive class sits within only a small subset of feature space, samples taken from much of the rest of the space will be &amp;#8220;soft-balls&amp;#8221; &amp;#8212; examples easily classified as not being in the positive class. Measuring the &lt;span class="caps"&gt;AUC&lt;/span&gt; on test sets over the full feature space in this context will always result in &lt;span class="caps"&gt;AUC&lt;/span&gt; values near one &amp;#8212; leaving it difficult to register improvements in the model near the decision boundary through measurement of the &lt;span class="caps"&gt;AUC&lt;/span&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Methods, Theory"></category></entry><entry><title>Simple python to LaTeX parser</title><link href="https://efavdb.com/simple-python-to-latex-parser" rel="alternate"></link><published>2016-11-18T12:59:00-08:00</published><updated>2016-11-18T12:59:00-08:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2016-11-18:/simple-python-to-latex-parser</id><summary type="html">&lt;p&gt;We demo a script that converts python numerical commands to LaTeX format. A notebook available on our GitHub page will take this and pretty print the&amp;nbsp;result.&lt;/p&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Here, we provide a simple script that accepts numerical python commands in string format and converts them into LaTeX markup. An example …&lt;/p&gt;</summary><content type="html">&lt;p&gt;We demo a script that converts python numerical commands to LaTeX format. A notebook available on our GitHub page will take this and pretty print the&amp;nbsp;result.&lt;/p&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Here, we provide a simple script that accepts numerical python commands in string format and converts them into LaTeX markup. An example input / output&amp;nbsp;follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;f(x_123, 2) / (2 + 3/(1 + z(np.sqrt((x + 3)/3)))) + np.sqrt(2 ** w) * np.tanh(2 * math.pi* x)&amp;#39;&lt;/span&gt;  
&lt;span class="nb"&gt;print&lt;/span&gt; &lt;span class="n"&gt;command_to_latex&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;## output:  &lt;/span&gt;
\&lt;span class="n"&gt;frac&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt; \&lt;span class="n"&gt;left&lt;/span&gt; &lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="mi"&gt;123&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; \&lt;span class="n"&gt;right&lt;/span&gt; &lt;span class="p"&gt;)}{&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; \&lt;span class="n"&gt;frac&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;}{&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt; \&lt;span class="n"&gt;left&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; \&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;\&lt;span class="n"&gt;frac&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;}{&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;}}&lt;/span&gt; \&lt;span class="n"&gt;right&lt;/span&gt; &lt;span class="p"&gt;)}}&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; \&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;{{&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="o"&gt;^&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;}}&lt;/span&gt; \&lt;span class="n"&gt;cdot&lt;/span&gt; \&lt;span class="n"&gt;tanh&lt;/span&gt; \&lt;span class="n"&gt;left&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; \&lt;span class="n"&gt;cdot&lt;/span&gt; \&lt;span class="n"&gt;pi&lt;/span&gt; \&lt;span class="n"&gt;cdot&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; \&lt;span class="n"&gt;right&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;  
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If the output shown here is plugged into a LaTeX editor, we get the following&amp;nbsp;result:&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{1}  
\frac{f \left ({x}_{123} , 2 \right )}{2 + \frac{3}{1 + z \left ( \sqrt{\frac{x + 3}{3}} \right )}} + \sqrt{{2}^{w}} \cdot \tanh \left (2 \cdot \pi \cdot x \right )  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Our Jupyter &lt;a href="https://github.com/EFavDB/python_command_to_latex"&gt;notebook&lt;/a&gt; automatically pretty prints to this&amp;nbsp;form.&lt;/p&gt;
&lt;p&gt;We provide the script here as it may be useful for two sorts of applications: 1) facilitating write-ups of completed projects, and 2) visualizing typed-up formulas to aid checks of their accuracy. The latter is particularly helpful for lengthy commands, which are often hard to read in python&amp;nbsp;format.&lt;/p&gt;
&lt;p&gt;We note that the python package sympy also provides a simple command-to-latex parser. However, I have had trouble getting it to output results if any functions appear that have not been defined &amp;#8212; we illustrate this issue in the&amp;nbsp;notebook.&lt;/p&gt;
&lt;p&gt;As usual, our code can be downloaded from our github page &lt;a href="https://github.com/EFavDB/python_command_to_latex"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Code&lt;/h3&gt;
&lt;p&gt;The main code segment follows. The method command_to_latex recursively computes the LaTeX for any combinations of variables grouped together via parentheses. The base case occurs when there are no parentheses left, at which point the method parse_simple_eqn is called, which converts simple commands to LaTeX. The results are then recombined within the recursive method. Additional replacements can be easily added in the appropriate lines&amp;nbsp;below.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;parse_simple_eqn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;  
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot; Return TeX equivalent of a command without parentheses. &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;  
    &lt;span class="c1"&gt;# Define replacement rules.  &lt;/span&gt;
    &lt;span class="n"&gt;simple_replacements&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;  
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;**&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;^&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;*&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39; \cdot &amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;  
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;math.&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;np.&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;  
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;pi&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;\pi&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;tan&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="s1"&gt;an&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;  
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;cos&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;\cos&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;sin&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;\sin&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;  
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;sec&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;\sec&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;csc&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;\csc&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="p"&gt;]&lt;/span&gt;  
    &lt;span class="n"&gt;complex_replacements&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;^&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;{{&lt;/span&gt;&lt;span class="si"&gt;{i1}&lt;/span&gt;&lt;span class="s1"&gt;}}^{{&lt;/span&gt;&lt;span class="si"&gt;{i2}&lt;/span&gt;&lt;span class="s1"&gt;}}&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;  
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;_&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;{{&lt;/span&gt;&lt;span class="si"&gt;{i1}&lt;/span&gt;&lt;span class="s1"&gt;}}_{{&lt;/span&gt;&lt;span class="si"&gt;{i2}&lt;/span&gt;&lt;span class="s1"&gt;}}&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;  
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;/&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="se"&gt;\f&lt;/span&gt;&lt;span class="s1"&gt;rac{{&lt;/span&gt;&lt;span class="si"&gt;{i1}&lt;/span&gt;&lt;span class="s1"&gt;}}{{&lt;/span&gt;&lt;span class="si"&gt;{i2}&lt;/span&gt;&lt;span class="s1"&gt;}}&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;  
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;sqrt&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;\sqrt{{&lt;/span&gt;&lt;span class="si"&gt;{i2}&lt;/span&gt;&lt;span class="s1"&gt;}}&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="p"&gt;]&lt;/span&gt;  

    &lt;span class="c1"&gt;# Carry out simple replacements  &lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;pair&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;simple_replacements&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  
        &lt;span class="n"&gt;q&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pair&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;pair&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  

    &lt;span class="c1"&gt;# Now complex replacements  &lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;*&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;/&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;+&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;-&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;^&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;_&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;,&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;sqrt&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;  
        &lt;span class="n"&gt;q&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39; &amp;#39;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
        &lt;span class="n"&gt;q_split&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_split&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;  
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;pair&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;complex_replacements&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  
                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;pair&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;  
                    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;sqrt&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  
                        &lt;span class="n"&gt;match_str&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot; &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_split&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  
                    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  
                        &lt;span class="n"&gt;match_str&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot; &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_split&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  
                    &lt;span class="n"&gt;q&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;match_str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pair&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;  
                        &lt;span class="n"&gt;i1&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;q_split&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;i2&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;q_split&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;  
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt;



&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;command_to_latex&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;  
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot; Recursively eliminate parentheses, then apply parse_simple_eqn.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;  
    &lt;span class="n"&gt;open_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;close_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;q_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;(&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;open_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;q_index&lt;/span&gt;
        &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;)&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;close_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;q_index&lt;/span&gt;
            &lt;span class="k"&gt;break&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;open_index&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;o&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;open_index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;@&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;close_index&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
        &lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;open_index&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;close_index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;o_tex&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;command_to_latex&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;m_tex&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;command_to_latex&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="c1"&gt;# Clean up redundant parentheses at recombination&lt;/span&gt;
        &lt;span class="n"&gt;r_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;o_tex&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;@&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;o_tex&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;r_index&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;{&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;o_tex&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;@&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;m_tex&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;o_tex&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;@&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; 
                                 &lt;span class="s1"&gt;&amp;#39; &lt;/span&gt;&lt;span class="se"&gt;\\&lt;/span&gt;&lt;span class="s1"&gt;left (&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;m_tex&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39; &lt;/span&gt;&lt;span class="se"&gt;\\&lt;/span&gt;&lt;span class="s1"&gt;right )&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;parse_simple_eqn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;That&amp;#8217;s&amp;nbsp;it!&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Programming"></category></entry><entry><title>Deep reinforcement learning, battleship</title><link href="https://efavdb.com/battleship" rel="alternate"></link><published>2016-10-15T13:52:00-07:00</published><updated>2016-10-15T13:52:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2016-10-15:/battleship</id><summary type="html">&lt;p&gt;Here, we provide a brief introduction to reinforcement learning (&lt;span class="caps"&gt;RL&lt;/span&gt;) &amp;#8212; a general technique for training programs to play games efficiently. Our aim is to explain its practical implementation: We cover some basic theory and then walk through a minimal python program that trains a neural network to play the game …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Here, we provide a brief introduction to reinforcement learning (&lt;span class="caps"&gt;RL&lt;/span&gt;) &amp;#8212; a general technique for training programs to play games efficiently. Our aim is to explain its practical implementation: We cover some basic theory and then walk through a minimal python program that trains a neural network to play the game&amp;nbsp;battleship.&lt;/p&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Reinforcement learning (&lt;span class="caps"&gt;RL&lt;/span&gt;) techniques are methods that can be used to teach algorithms to play games efficiently. Like supervised machine-learning (&lt;span class="caps"&gt;ML&lt;/span&gt;) methods, &lt;span class="caps"&gt;RL&lt;/span&gt; algorithms learn from data &amp;#8212; in this case, past game play data. However, whereas supervised-learning algorithms train only on data that is already available, &lt;span class="caps"&gt;RL&lt;/span&gt; addresses the challenge of performing well while still in the process of collecting data. In particular, we seek design principles&amp;nbsp;that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Allow programs to identify good strategies from past&amp;nbsp;examples,&lt;/li&gt;
&lt;li&gt;Enable fast learning of new strategies through continued game&amp;nbsp;play.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The reason we particularly want our algorithms to learn fast here is that &lt;span class="caps"&gt;RL&lt;/span&gt; is most fruitfully applied in contexts where training data is limited &amp;#8212; or where the space of strategies is so large that it would be difficult to explore exhaustively. It is in these regimes that supervised techniques have trouble and &lt;span class="caps"&gt;RL&lt;/span&gt; methods&amp;nbsp;shine.&lt;/p&gt;
&lt;p&gt;In this post, we review one general &lt;span class="caps"&gt;RL&lt;/span&gt; training procedure: The policy-gradient, deep-learning scheme. We review the theory behind this approach in the next section. Following that, we walk through a simple python implementation that trains a neural network to play the game&amp;nbsp;battleship.&lt;/p&gt;
&lt;p&gt;Our python code can be downloaded from our github page, &lt;a href="https://github.com/EFavDB/battleship"&gt;here&lt;/a&gt;. It requires the jupyter, tensorflow, numpy, and matplotlib&amp;nbsp;packages.&lt;/p&gt;
&lt;h3&gt;Policy-gradient, deep &lt;span class="caps"&gt;RL&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Policy-gradient, deep &lt;span class="caps"&gt;RL&lt;/span&gt; algorithms consist of two main components: A policy network and a rewards function. We detail these two below and then describe how they work together to train good&amp;nbsp;models.&lt;/p&gt;
&lt;h4&gt;The policy&amp;nbsp;network&lt;/h4&gt;
&lt;p&gt;The policy for a given deep &lt;span class="caps"&gt;RL&lt;/span&gt; algorithm is a neural network that maps state values &lt;span class="math"&gt;\(s\)&lt;/span&gt; to probabilities for given game actions &lt;span class="math"&gt;\(a\)&lt;/span&gt;. In other words, the input layer of the network accepts a numerical encoding of the environment &amp;#8212; the state of the game at a particular moment. When this input is fed through the network, the values at the output layer correspond to the log probabilities that each of the actions available to us is optimal &amp;#8212; one output node is present for each possible action that we can choose. Note that if we knew with certainty which move we should take, only one output node would have a finite probability. However, if our network is uncertain which action is optimal, more than one output node will have finite&amp;nbsp;weight.&lt;/p&gt;
&lt;p&gt;To illustrate the above, we present a diagram of the network used in our battleship program below. (For a review of the rules of battleship, see footnote [1].) For simplicity, we work with a 1-d battleship grid. We then encode our current knowledge of the environment using one input neuron for each of our opponent&amp;#8217;s grid positions. In particular, we use the following encoding for each neuron /&amp;nbsp;index:&lt;/p&gt;
&lt;div class="math"&gt;\begin{align} \label{input} \tag{1}
x_{0,i} = \begin{cases}
-1 &amp;amp; \text{Have not yet bombed $i$} \\
\ 0 &amp;amp; \text{Have bombed $i$, no ship} \\
+1 &amp;amp; \text{Have bombed $i$, ship present}.
\end{cases}
\end{align}&lt;/div&gt;
&lt;p&gt;
In our example figure below, we have five input neurons, so the board is of size five. The first three neurons have value &lt;span class="math"&gt;\(-1\)&lt;/span&gt; implying we have not yet bombed those grid points. Finally, the last two are &lt;span class="math"&gt;\(+1\)&lt;/span&gt; and &lt;span class="math"&gt;\(0\)&lt;/span&gt;, respectively, implying that a ship does sit at the fourth site, but not at the&amp;nbsp;fifth.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2016/10/nn.jpg"&gt;&lt;img alt="network" src="https://efavdb.com/wp-content/uploads/2016/10/nn.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Note that in the output layer of the policy network shown, the first three values are labeled with log probabilities. These values correspond to the probabilities that we should next bomb each of these indices, respectively. We cannot re-bomb the fourth and fifth grid points, so although the network may output some values to these neurons, we&amp;#8217;ll ignore&amp;nbsp;them.&lt;/p&gt;
&lt;p&gt;Before moving on, we note that the reason we use a neural network for our policy is to allow for efficient generalization: For games like Go that have a very large number of states, it is not feasible to collect data on every possible board position. This is exactly the context where &lt;span class="caps"&gt;ML&lt;/span&gt; algorithms excel &amp;#8212; generalizing from past observations to make good predictions for new situations. In order to keep our focus on &lt;span class="caps"&gt;RL&lt;/span&gt;, we won&amp;#8217;t review how &lt;span class="caps"&gt;ML&lt;/span&gt; algorithms work in this post (however, you can check out our &lt;a href="http://efavdb.github.io/archives"&gt;archives&lt;/a&gt; section for relevant primers). Instead we simply note that &amp;#8212; utilizing these tools &amp;#8212; we can get good performance by training only on a &lt;em&gt;representative subset&lt;/em&gt; of games &amp;#8212; allowing us to avoid study of the full set, which can be much&amp;nbsp;larger.&lt;/p&gt;
&lt;h4&gt;The rewards&amp;nbsp;function&lt;/h4&gt;
&lt;p&gt;To train an &lt;span class="caps"&gt;RL&lt;/span&gt; algorithm, we must carry out an iterative game play / scoring process: We play games according to our current policy, selecting moves with frequencies proportional to the probabilities output by the network. If the actions taken resulted in good outcomes, we want to strengthen the probability of those actions going&amp;nbsp;forward.&lt;/p&gt;
&lt;p&gt;The rewards function is the tool we use to formally score our outcomes in past games &amp;#8212; we will encourage our algorithm to try to maximize this quantity during game play. In effect, it is a hyper-parameter for the &lt;span class="caps"&gt;RL&lt;/span&gt; algorithm: many different functions could be used, each resulting in different learning characteristics. For our battleship program, we have used the&amp;nbsp;function
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align} \label{rewards} \tag{2}
r(a;t_0) = \sum_{t \geq t_0} \left ( h(t) - \overline{h(t)} \right) (0.5)^{t-t0}
\end{align}&lt;/div&gt;
&lt;p&gt;
Given a completed game log, this function looks at the action &lt;span class="math"&gt;\(a\)&lt;/span&gt; taken at time &lt;span class="math"&gt;\(t_0\)&lt;/span&gt; and returns a weighted sum of hit values &lt;span class="math"&gt;\(h(t)\)&lt;/span&gt; for this and all future steps in the game. Here, &lt;span class="math"&gt;\(h(t)\)&lt;/span&gt; is &lt;span class="math"&gt;\(1\)&lt;/span&gt; if we had a hit at step &lt;span class="math"&gt;\(t\)&lt;/span&gt; and is &lt;span class="math"&gt;\(0\)&lt;/span&gt;&amp;nbsp;otherwise.&lt;/p&gt;
&lt;p&gt;In arriving at (\ref{rewards}), we admit that we did not carry out a careful search over the set of all possible rewards functions. However, we have confirmed that this choice results in good game play, and it is well-motivated: In particular, we note that the weighting term &lt;span class="math"&gt;\((0.5)^{t-t0}\)&lt;/span&gt; serves to strongly incentivize a hit on the current move (we get a reward of &lt;span class="math"&gt;\(1\)&lt;/span&gt; for a hit at &lt;span class="math"&gt;\(t_0\)&lt;/span&gt;), but a hit at &lt;span class="math"&gt;\((t_0 + 1)\)&lt;/span&gt; also rewards the action at &lt;span class="math"&gt;\(t_0\)&lt;/span&gt; &amp;#8212; with value &lt;span class="math"&gt;\(0.5\)&lt;/span&gt;. Similarly, a hit at &lt;span class="math"&gt;\((t_0 + 2)\)&lt;/span&gt; rewards &lt;span class="math"&gt;\(0.25\)&lt;/span&gt;, etc. This weighted look-ahead aspect of (\ref{rewards}) serves to encourage efficient exploration of the board: It forces the program to care about moves that will enable future hits. The other ingredient of note present in (\ref{rewards}) is the subtraction of &lt;span class="math"&gt;\(\overline{h(t)}\)&lt;/span&gt;. This is the expected rewards that a random network would obtain. By pulling this out, we only reward our network if it is outperforming random choices &amp;#8212; this results in a net speed-up of the learning&amp;nbsp;process.&lt;/p&gt;
&lt;h4&gt;Stochastic gradient&amp;nbsp;descent&lt;/h4&gt;
&lt;p&gt;In order to train our algorithm to maximize captured rewards during game play, we apply gradient descent. To carry this out, we imagine allowing our network parameters &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; to vary at some particular step in the game. Averaging over all possible actions, the gradient of the expected rewards is then&amp;nbsp;formally,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align} \nonumber
\partial_{\theta} \langle r(a \vert s) \rangle &amp;amp;\equiv &amp;amp; \partial_{\theta} \int p(a \vert \theta, s) r(a \vert s) da \\ \nonumber
&amp;amp;=&amp;amp; \int p(a \vert \theta, s) r(a \vert s) \partial_{\theta} \log \left ( p(a \vert \theta, s) \right) da \\
&amp;amp;\equiv &amp;amp; \langle r(a \vert s) \partial_{\theta} \log \left ( p(a \vert \theta, s) \right) \rangle. \tag{3} \label{formal_ev}
\end{align}&lt;/div&gt;
&lt;p&gt;
Here, the &lt;span class="math"&gt;\(p(a)\)&lt;/span&gt; values are the action probability outputs of our&amp;nbsp;network.&lt;/p&gt;
&lt;p&gt;Unfortunately, we usually can&amp;#8217;t evaluate the last line above. However, what we can do is approximate it using a sampled value: We simply play a game with our current network, then replace the expected value above by the reward actually captured on the &lt;span class="math"&gt;\(i\)&lt;/span&gt;-th&amp;nbsp;move,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
\hat{g}_i = r(a_i) \nabla_{\theta} \log p(a_i \vert s_i, \theta). \tag{4} \label{estimator}
\end{align}&lt;/div&gt;
&lt;p&gt;
Here, &lt;span class="math"&gt;\(a_i\)&lt;/span&gt; is the action that was taken, &lt;span class="math"&gt;\(r(a_i)\)&lt;/span&gt; is reward that was captured, and the derivative of the logarithm shown can be evaluated via back-propagation (aside for those experienced with neural networks: this is the derivative of the cross-entropy loss function that would apply if you treated the event like a supervised-learning training example &amp;#8212; with the selected action &lt;span class="math"&gt;\(a_i\)&lt;/span&gt; taken as the label). The function &lt;span class="math"&gt;\(\hat{g}_i\)&lt;/span&gt; provides a noisy estimate of the desired gradient, but taking many steps will result in a &amp;#8220;stochastic&amp;#8221; gradient descent, on average pushing us towards correct rewards&amp;nbsp;maximization.&lt;/p&gt;
&lt;h4&gt;Summary of the training&amp;nbsp;process&lt;/h4&gt;
&lt;p&gt;In summary, then, &lt;span class="caps"&gt;RL&lt;/span&gt; training proceeds iteratively: To initialize an iterative step, we first play a game with our current policy network, selecting moves stochastically according to the network&amp;#8217;s output. After the game is complete, we then score our outcome by evaluating the rewards captured on each move &amp;#8212; for example, in the battleship game we use (\ref{rewards}). Once this is done, we then estimate the gradient of the rewards function using (\ref{estimator}). Finally, we update the network parameters, moving &lt;span class="math"&gt;\(\theta \to \theta + \alpha \sum \hat{g}_i\)&lt;/span&gt;, with &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; a small step size parameter. To continue, we then play a new game with the updated network,&amp;nbsp;etc.&lt;/p&gt;
&lt;p&gt;To see that this process does, in fact, encourage actions that have resulted in good outcomes during training, note that (\ref{estimator}) is proportional to the rewards captured at the step &lt;span class="math"&gt;\(i\)&lt;/span&gt;. Consequently, when we adjust our parameters in the direction of (\ref{estimator}), we will strongly encourage those actions that have resulted in large rewards outcomes. Further, those moves with negative rewards are actually suppressed. In this way, over time, the network will learn to examine the system and suggest those moves that will likely produce the best&amp;nbsp;outcomes.&lt;/p&gt;
&lt;p&gt;That&amp;#8217;s it for the basics of deep, policy-gradient &lt;span class="caps"&gt;RL&lt;/span&gt;. We now turn to our python example,&amp;nbsp;battleship.&lt;/p&gt;
&lt;h3&gt;Python code walkthrough &amp;#8212; battleship &lt;span class="caps"&gt;RL&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Load the needed&amp;nbsp;packages.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorflow&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tf&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pylab&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Define our network &amp;#8212; a fully connected, three layer system. The code below is mostly tensorflow boilerplate that can be picked up by going through their first tutorials. The one unusual thing is that we have our learning rate in (26) set to the placeholder value (9). This will allow us to vary our step sizes with observed rewards captured&amp;nbsp;below.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;BOARD_SIZE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;
&lt;span class="n"&gt;SHIP_SIZE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;

&lt;span class="n"&gt;hidden_units&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BOARD_SIZE&lt;/span&gt;
&lt;span class="n"&gt;output_units&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BOARD_SIZE&lt;/span&gt;

&lt;span class="n"&gt;input_positions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;BOARD_SIZE&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[])&lt;/span&gt;
&lt;span class="c1"&gt;# Generate hidden layer&lt;/span&gt;
&lt;span class="n"&gt;W1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;BOARD_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hidden_units&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BOARD_SIZE&lt;/span&gt;&lt;span class="p"&gt;))))&lt;/span&gt;
&lt;span class="n"&gt;b1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hidden_units&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;span class="n"&gt;h1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tanh&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_positions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;W1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# Second layer -- linear classifier for action logits&lt;/span&gt;
&lt;span class="n"&gt;W2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;hidden_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_units&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hidden_units&lt;/span&gt;&lt;span class="p"&gt;))))&lt;/span&gt;
&lt;span class="n"&gt;b2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_units&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;span class="n"&gt;logits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;W2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b2&lt;/span&gt;
&lt;span class="n"&gt;probabilities&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;init&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;initialize_all_variables&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;cross_entropy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sparse_softmax_cross_entropy_with_logits&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;xentropy&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;train_step&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GradientDescentOptimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cross_entropy&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# Start TF session&lt;/span&gt;
&lt;span class="n"&gt;sess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;init&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Next, we define a method that will allow us to play a game using our network. The &lt;span class="caps"&gt;TRAINING&lt;/span&gt; variable specifies whether or not to take the optimal moves or to select moves stochastically. Note that the method returns a set of logs that record the game proceedings. These are needed for&amp;nbsp;training.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;TRAINING&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;play_game&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TRAINING&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot; Play game of battleship using network.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="c1"&gt;# Select random location for ship&lt;/span&gt;
    &lt;span class="n"&gt;ship_left&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BOARD_SIZE&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;SHIP_SIZE&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ship_positions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ship_left&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ship_left&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;SHIP_SIZE&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="c1"&gt;# Initialize logs for game&lt;/span&gt;
    &lt;span class="n"&gt;board_position_log&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="n"&gt;action_log&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="n"&gt;hit_log&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="c1"&gt;# Play through game&lt;/span&gt;
    &lt;span class="n"&gt;current_board&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BOARD_SIZE&lt;/span&gt;&lt;span class="p"&gt;)]]&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hit_log&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;SHIP_SIZE&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;board_position_log&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;current_board&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]]])&lt;/span&gt;
        &lt;span class="n"&gt;probs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;probabilities&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;input_positions&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;current_board&lt;/span&gt;&lt;span class="p"&gt;})[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;probs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;action_log&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;probs&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
        &lt;span class="n"&gt;probs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;probs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;probs&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;training&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;bomb_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;choice&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BOARD_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;probs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;bomb_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;probs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="c1"&gt;# update board, logs&lt;/span&gt;
        &lt;span class="n"&gt;hit_log&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bomb_index&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;ship_positions&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;current_board&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;bomb_index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bomb_index&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;ship_positions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;action_log&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bomb_index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;board_position_log&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action_log&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hit_log&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Our implementation of the rewards function&amp;nbsp;(\ref{rewards}):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;rewards_calculator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hit_log&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;gamma&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot; Discounted sum of future hits over trajectory&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;hit_log_weighted&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;
    &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;SHIP_SIZE&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hit_log&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BOARD_SIZE&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;gamma&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hit_log&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;[((&lt;/span&gt;&lt;span class="n"&gt;gamma&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hit_log_weighted&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;:])&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hit_log&lt;/span&gt;&lt;span class="p"&gt;))]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Finally, our training loop. Here, we iteratively play through many games, scoring after each game, then adjusting parameters &amp;#8212; setting the placeholder learning rate equal to &lt;span class="caps"&gt;ALPHA&lt;/span&gt; times the rewards&amp;nbsp;captured.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;game_lengths&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;TRAINING&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt; &lt;span class="c1"&gt;# Boolean specifies training mode&lt;/span&gt;
&lt;span class="n"&gt;ALPHA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.06&lt;/span&gt; &lt;span class="c1"&gt;# step size&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;game&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;board_position_log&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action_log&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hit_log&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;play_game&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TRAINING&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;game_lengths&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;action_log&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;rewards_log&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;rewards_calculator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hit_log&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;current_board&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rewards_log&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;board_position_log&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action_log&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# Take step along gradient&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;TRAINING&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;train_step&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;input_positions&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;current_board&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;:[&lt;/span&gt;&lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;ALPHA&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Running this last cell, we see that the training works! The following is an example trace from the play_game() method, with the variable &lt;span class="caps"&gt;TRAINING&lt;/span&gt; set to False. This illustrates an intelligent move selection&amp;nbsp;process.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Example game trace output&lt;/span&gt;
&lt;span class="p"&gt;([[[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;
&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;
&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;
&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;
&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]],&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here, the first five lines are the board encodings that the network was fed each step &amp;#8212; using (\ref{input}). The second to last row presents the sequential grid selections that were chosen. Finally, the last row is the hit log. Notice that the first two moves nicely sample different regions of the board. After this, a hit was recorded at &lt;span class="math"&gt;\(6\)&lt;/span&gt;. The algorithm then intelligently selects &lt;span class="math"&gt;\(7\)&lt;/span&gt; and &lt;span class="math"&gt;\(8\)&lt;/span&gt;, which it can infer must be the final locations of the&amp;nbsp;ship.&lt;/p&gt;
&lt;p&gt;The plot below provides further characterization of the learning process. This shows the running average game length (steps required to fully bomb ship) versus training epoch. The program learns the basics quite quickly, then continues to gradually improve over time&amp;nbsp;[2].&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2016/10/trace.jpg"&gt;&lt;img alt="trace" src="https://efavdb.com/wp-content/uploads/2016/10/trace.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;In this post, we have covered a variant of &lt;span class="caps"&gt;RL&lt;/span&gt; &amp;#8212; namely, the policy-gradient, deep &lt;span class="caps"&gt;RL&lt;/span&gt; scheme. This is a method that typically defaults to the currently best-known strategy, but occasionally samples from other approaches, ultimately resulting in an iterative improvement in policy. The two main ingredients here are the policy network and the rewards function. Although network architecture design is usually the place where most of the thinking is involved in supervised learning, it is the rewards function that typically requires the most thought in the &lt;span class="caps"&gt;RL&lt;/span&gt; context. A good choice should be as local in time as possible, so as to facilitate training (distant forecast dependence will result in a slow learning process). However, the rewards function should also directly attack the ultimate end of the process (&amp;#8220;winning&amp;#8221; the game &amp;#8212; encouragement of side quests that aren&amp;#8217;t necessary can often occur if care is not taken). Balancing these two competing demands can be a challenge, and rewards function design is therefore something of an art&amp;nbsp;form.&lt;/p&gt;
&lt;p&gt;Our brief introduction here was intended only to illustrate the gist of how &lt;span class="caps"&gt;RL&lt;/span&gt; is carried out in practice. For further details, we can recommend two resources: the text book by Sutton and Barto [3] and a recent talk by John Schulman&amp;nbsp;[4].&lt;/p&gt;
&lt;h3&gt;Footnotes and&amp;nbsp;references&lt;/h3&gt;
&lt;p&gt;[1] Game rules: Battleship is a two-player game. Both players begin with a finite regular grid of positions &amp;#8212; hidden from their opponent &amp;#8212; and a set of &amp;#8220;ships&amp;#8221;. Each player receives the same quantity of each type of ship. At the start of the game, each player places the ships on their grid in whatever locations they like, subject to some constraints: A ship of length 2, say, must occupy two contiguous indices on the board, and no two ships can occupy the same grid location. Once placed, the ships are fixed in position for the remainder of the game. At this point, game play begins, with the goal being to sink the opponent ships. The locations of the enemy ships are initially unknown because we cannot see the opponent&amp;#8217;s grid. To find the ships, one &amp;#8220;bombs&amp;#8221; indices on the enemy grid &amp;#8212; with bombing occurs in turns. When an opponent index is bombed, the opponent must truthfully state whether or not a ship was located at the index bombed. Whoever succeeds in bombing all their opponent&amp;#8217;s occupied indices first wins the game. Therefore, the problem reduces to finding the enemy ship indices as quickly as&amp;nbsp;possible.&lt;/p&gt;
&lt;p&gt;[2] One of my colleagues (&lt;span class="caps"&gt;HC&lt;/span&gt;) has suggested that the program likely begins to overfit at some point. However, the 1-d version of the game has so few possible ship locations that characterization of this effect via a training and test set split does not seem appropriate. However, this approach could work were we to move to higher dimensions and introduce multiple&amp;nbsp;ships.&lt;/p&gt;
&lt;p&gt;[3] Sutton and Barto, (2016). &amp;#8220;Reinforcement Learning: An Introduction&amp;#8221;. Text site, &lt;a href="https://webdocs.cs.ualberta.ca/~sutton/book/the-book.html"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;[4] John Schulman, (2016). &amp;#8220;Bay Area Deep Learning School&amp;#8221;. Youtube recording of talk available &lt;a href="https://www.youtube.com/watch?v=9dXiAecyJrY"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Methods, Theory"></category></entry><entry><title>GPU-accelerated Theano &amp; Keras with Windows 10</title><link href="https://efavdb.com/gpu-accelerated-theano-keras-with-windows-10" rel="alternate"></link><published>2016-09-22T21:48:00-07:00</published><updated>2016-09-22T21:48:00-07:00</updated><author><name>Damien RJ</name></author><id>tag:efavdb.com,2016-09-22:/gpu-accelerated-theano-keras-with-windows-10</id><summary type="html">&lt;p&gt;There are many tutorials with directions for how to use your Nvidia graphics card for &lt;span class="caps"&gt;GPU&lt;/span&gt;-accelerated Theano and Keras for Linux, but there is only limited information out there for you if you want to set everything up with Windows and the current &lt;span class="caps"&gt;CUDA&lt;/span&gt; toolkit. This is a shame …&lt;/p&gt;</summary><content type="html">&lt;p&gt;There are many tutorials with directions for how to use your Nvidia graphics card for &lt;span class="caps"&gt;GPU&lt;/span&gt;-accelerated Theano and Keras for Linux, but there is only limited information out there for you if you want to set everything up with Windows and the current &lt;span class="caps"&gt;CUDA&lt;/span&gt; toolkit. This is a shame however because there are a large number of computers out there with very nice video cards that are only running windows, and it is not always practical to use a Virtual Machine, or Dual-Boot.  So for today&amp;#8217;s post we will go over how to get everything running in Windows 10 by saving you all the trial and error I went through. (All of these steps should also work in earlier versions of&amp;nbsp;Windows).&lt;/p&gt;
&lt;h2&gt;Dependencies&lt;/h2&gt;
&lt;p&gt;Before getting started, make sure you have the&amp;nbsp;following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="caps"&gt;NVIDIA&lt;/span&gt; card that supports &lt;span class="caps"&gt;CUDA&lt;/span&gt; (&lt;a href="https://developer.nvidia.com/cuda-gpus"&gt;link&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Python 2.7 (&lt;a href="http://conda.pydata.org/miniconda.html"&gt;Anaconda&lt;/a&gt;&amp;nbsp;preferably)&lt;/li&gt;
&lt;li&gt;Compilers for&amp;nbsp;C/C++&lt;/li&gt;
&lt;li&gt;&lt;span class="caps"&gt;CUDA&lt;/span&gt;&amp;nbsp;7.5&lt;/li&gt;
&lt;li&gt;&lt;span class="caps"&gt;GCC&lt;/span&gt; for code generated by&amp;nbsp;Theano&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Setup&lt;/h2&gt;
&lt;h3&gt;Visual Studio 2013 Community Edition Update&amp;nbsp;4&lt;/h3&gt;
&lt;p&gt;First, go and download the installer for &lt;a href="https://www.visualstudio.com/en-us/news/vs2013-community-vs.aspx"&gt;Visual Studio 2013 Community Edition Update 4&lt;/a&gt;.  You can not use the 2015 version because it is still not supported by &lt;span class="caps"&gt;CUDA&lt;/span&gt;.  When installing, there is no need to install any of the optional packages.  When you are done add the compiler, &lt;strong&gt;C:\Program Files (x86)\Microsoft Visual Studio 12.0\&lt;span class="caps"&gt;VC&lt;/span&gt;\bin&lt;/strong&gt;,  to your windows&amp;nbsp;path.&lt;/p&gt;
&lt;p&gt;To  add something to your windows path go to System, and then Advanced system&amp;nbsp;settings.&lt;/p&gt;
&lt;p&gt;System → Advanced system settings → Environment Variables →&amp;nbsp;Path.&lt;/p&gt;
&lt;h3&gt;&lt;span class="caps"&gt;CUDA&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Next, go the &lt;span class="caps"&gt;NVIDIA&lt;/span&gt;&amp;#8217;s website and &lt;a href="https://developer.nvidia.com/cuda-downloads"&gt;download&lt;/a&gt; the &lt;span class="caps"&gt;CUDA&lt;/span&gt; 7.5 toolkit. Select the right version for you computer. When you are installing it, make sure to pick custom install if you don&amp;#8217;t want your video card drivers to be overwritten with the version that comes with the toolkit, which are often out of date.  If it turns out that your version of the drivers are older than what comes with the toolkit,then there is no harm in updating your drivers, otherwise only pick the three boxes starting with &lt;span class="caps"&gt;CUDA&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;span class="caps"&gt;GCC&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;The last thing we need to do  &lt;span class="caps"&gt;GCC&lt;/span&gt; compiler, I recommend &lt;a href="http://tdm-gcc.tdragon.net/download"&gt;&lt;span class="caps"&gt;TDM&lt;/span&gt;-gcc&lt;/a&gt;.  Install the 64 bit version, and then add the compiler to your windows path, the install has an option to do that for you automatically if you&amp;nbsp;wish.&lt;/p&gt;
&lt;p&gt;To make sure that everything is working at this point, run the the following command on the command line (cmd.exe) . If if finds the path for everything you are good to&amp;nbsp;go.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;where gcc where cl where nvcc where cudafe where cudafe++&lt;/code&gt;&lt;/p&gt;
&lt;h3&gt;Theano and&amp;nbsp;Keras&lt;/h3&gt;
&lt;p&gt;At this point it is easy to install Theano and Keras, just you pip (or conda and&amp;nbsp;pip)!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;conda install mingw libpython&lt;/span&gt;
&lt;span class="err"&gt;pip install theano&lt;/span&gt;
&lt;span class="err"&gt;pip install keras&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;After installing the python libraries you need to tell Theano to use the &lt;span class="caps"&gt;GPU&lt;/span&gt; instead of the &lt;span class="caps"&gt;CPU&lt;/span&gt;.  A lot of older posts would have you set this in the system environment, but it is possible to make a config file in your home directory named &amp;#8220;&lt;em&gt;.theanorc.txt&lt;/em&gt;&amp;#8221; instead.  This also makes it easy to switch out config files.  Inside the file put the&amp;nbsp;following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;[global]&lt;/span&gt;
&lt;span class="na"&gt;device&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;gpu&lt;/span&gt;
&lt;span class="na"&gt;floatX&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;float32&lt;/span&gt;

&lt;span class="k"&gt;[nvcc]&lt;/span&gt;
&lt;span class="na"&gt;compiler_bindir&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;C:\Program Files (x86)\Microsoft Visual Studio 12.0\VC\bin&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Lastly, set up the Keras config file &lt;code&gt;~/.keras/keras.json&lt;/code&gt;.  If you haven&amp;#8217;t started Keras yet, the folder and file won&amp;#8217;t be there but you can create it. Inside the config put the&amp;nbsp;following.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;{&lt;/span&gt;
&lt;span class="err"&gt; &amp;quot;image_dim_ordering&amp;quot;: &amp;quot;tf&amp;quot;,&lt;/span&gt;
&lt;span class="err"&gt; &amp;quot;epsilon&amp;quot;: 1e-07,&lt;/span&gt;
&lt;span class="err"&gt; &amp;quot;floatx&amp;quot;: &amp;quot;float32&amp;quot;,&lt;/span&gt;
&lt;span class="err"&gt; &amp;quot;backend&amp;quot;: &amp;quot;theano&amp;quot;&lt;/span&gt;
&lt;span class="err"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Testing Theano with &lt;span class="caps"&gt;GPU&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;Using the following python code,  check if your installation of Theano is using your &lt;span class="caps"&gt;GPU&lt;/span&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;theano&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;function&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shared&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sandbox&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;theano.tensor&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;T&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;time&lt;/span&gt;

&lt;span class="n"&gt;vlen&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;768&lt;/span&gt; &lt;span class="c1"&gt;# 10 x #cores x # threads per core&lt;/span&gt;
&lt;span class="n"&gt;iters&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;

&lt;span class="n"&gt;rng&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;RandomState&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;22&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;shared&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;asarray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rng&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vlen&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;floatX&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;function&lt;/span&gt;&lt;span class="p"&gt;([],&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;maker&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fgraph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;toposort&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="n"&gt;t0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;iters&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;t1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Looping &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s2"&gt; times took &lt;/span&gt;&lt;span class="si"&gt;%f&lt;/span&gt;&lt;span class="s2"&gt; seconds&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;iters&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;t1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;t0&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Result is &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;,))&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;any&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;op&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Elemwise&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;maker&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fgraph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;toposort&lt;/span&gt;&lt;span class="p"&gt;()]):&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Used the cpu&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Used the gpu&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Testing Keras with &lt;span class="caps"&gt;GPU&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;This code will make sure that everything is working and train a model on some random data. The first time might take a little longer because it the software needs to do some&amp;nbsp;compiling.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.models&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Sequential&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.layers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Activation&lt;/span&gt;

&lt;span class="c1"&gt;# for a single-input model with 2 classes (binary):&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Sequential&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;784&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;sigmoid&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;rmsprop&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;binary_crossentropy&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;accuracy&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# generate dummy data&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;784&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# train the model, iterating on the data in batches&lt;/span&gt;
&lt;span class="c1"&gt;# of 32 samples&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nb_epoch&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If everything works you will see something like&amp;nbsp;this!&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2016/09/output.png"&gt;&lt;img alt="output" src="https://efavdb.com/wp-content/uploads/2016/09/output.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Now you can start playing with neural networks using your &lt;span class="caps"&gt;GPU&lt;/span&gt;!&lt;/p&gt;</content><category term="Methods, Tools"></category></entry><entry><title>Hyperparameter sample-size dependence</title><link href="https://efavdb.com/model-selection" rel="alternate"></link><published>2016-08-21T00:00:00-07:00</published><updated>2016-08-21T00:00:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2016-08-21:/model-selection</id><summary type="html">&lt;p&gt;Here, we briefly review a subtlety associated with machine-learning model selection: the fact that the optimal hyperparameters for a model can vary with training set size, &lt;span class="math"&gt;\(N.\)&lt;/span&gt; To illustrate this point, we derive expressions for the optimal strength for both &lt;span class="math"&gt;\(L_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(L_2\)&lt;/span&gt; regularization in single-variable models. We find that …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Here, we briefly review a subtlety associated with machine-learning model selection: the fact that the optimal hyperparameters for a model can vary with training set size, &lt;span class="math"&gt;\(N.\)&lt;/span&gt; To illustrate this point, we derive expressions for the optimal strength for both &lt;span class="math"&gt;\(L_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(L_2\)&lt;/span&gt; regularization in single-variable models. We find that the optimal &lt;span class="math"&gt;\(L_2\)&lt;/span&gt; approaches a finite constant as &lt;span class="math"&gt;\(N\)&lt;/span&gt; increases, but that the optimal &lt;span class="math"&gt;\(L_1\)&lt;/span&gt; decays exponentially fast with &lt;span class="math"&gt;\(N.\)&lt;/span&gt; Sensitive dependence on &lt;span class="math"&gt;\(N\)&lt;/span&gt; such as this should be carefully extrapolated out when optimizing mission-critical&amp;nbsp;models.&lt;/p&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;There are two steps one must carry out to fit a machine-learning model. First, a specific model form and cost function must be selected, and second the model must be fit to the data. The first of these steps is often treated by making use of a training-test data split: One trains a set of candidate models to a fraction of the available data and then validates their performance using a hold-out, test set. The model that performs best on the latter is then selected for&amp;nbsp;production.&lt;/p&gt;
&lt;p&gt;Our purpose here is to highlight a subtlety to watch out for when carrying out an optimization as above: the fact that the optimal model can depend sensitively on training set size &lt;span class="math"&gt;\(N\)&lt;/span&gt;. This observation suggests that the training-test split paradigm must sometimes be applied with care: Because a subsample is used for training in the first, selection step, the model identified as optimal there may not be best when training on the full data&amp;nbsp;set.&lt;/p&gt;
&lt;p&gt;To illustrate the above points, our main effort here is to present some toy examples where the optimal hyperparameters can be characterized exactly: We derive the optimal &lt;span class="math"&gt;\(L_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(L_2\)&lt;/span&gt; regularization strength for models having only a single variable. These examples illustrate two opposite limits: The latter approaches a finite constant as &lt;span class="math"&gt;\(N\)&lt;/span&gt; increases, but the former varies exponentially with &lt;span class="math"&gt;\(N\)&lt;/span&gt;. This shows that strong &lt;span class="math"&gt;\(N\)&lt;/span&gt;-dependence can sometimes occur, but is not necessarily always an issue. In practice, a simple way to check for sensitivity is to vary the size of your training set during model selection: If a strong dependence is observed, care should be taken during the final&amp;nbsp;extrapolation.&lt;/p&gt;
&lt;p&gt;We now walk through our two&amp;nbsp;examples.&lt;/p&gt;
&lt;h3&gt;&lt;span class="math"&gt;\(L_2\)&lt;/span&gt;&amp;nbsp;optimization&lt;/h3&gt;
&lt;p&gt;We start off by positing that we have a method for generating a Bayesian posterior for a parameter &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; that is a function of a vector of &lt;span class="math"&gt;\(N\)&lt;/span&gt; random samples &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt;. To simplify our discussion, we assume that &amp;#8212; given a flat prior &amp;#8212; this is unbiased and normal with variance &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt;. We write &lt;span class="math"&gt;\(\theta_0 \equiv \theta_0(\textbf{x})\)&lt;/span&gt; for the maximum a posteriori (&lt;span class="caps"&gt;MAP&lt;/span&gt;) value under the flat prior. With the introduction of an &lt;span class="math"&gt;\(L_2\)&lt;/span&gt; prior, the posterior for &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; is&amp;nbsp;then
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{1}
P\left(\theta \vert \theta_0(\textbf{x})\right) \propto \exp\left( - \frac{(\theta - \theta_0)^2}{2 \sigma^2} - \Lambda \theta^2 \right).
$$&lt;/div&gt;
&lt;p&gt;
Setting the derivative of the above to zero, the point-estimate, &lt;span class="caps"&gt;MAP&lt;/span&gt; is given&amp;nbsp;by
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{2}
\hat{\theta} = \frac{\theta_0}{1 + 2 \Lambda \sigma^2}.
$$&lt;/div&gt;
&lt;p&gt;
The average squared error of this estimate is obtained by averaging over the possible &lt;span class="math"&gt;\(\theta_0\)&lt;/span&gt; values. Our assumptions above imply that &lt;span class="math"&gt;\(\theta_0\)&lt;/span&gt; is normal about the true parameter value, &lt;span class="math"&gt;\(\theta_*\)&lt;/span&gt;, so we&amp;nbsp;have
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
\langle (\hat{\theta} - \theta_*)^2 \rangle &amp;amp;\equiv&amp;amp; \int_{\infty}^{\infty} \frac{1}{\sqrt{2 \pi \sigma^2}}
e^{ - \frac{(\theta_0 - \theta_*)^2}{2 \sigma^2}} \left ( \frac{\theta_0}{1 + 2 \Lambda \sigma^2} - \theta_* \right)^2 d \theta_0 \\
&amp;amp;=&amp;amp; \frac{ 4 \Lambda^2 \sigma^4 \theta_*^2 }{(1 + 2 \Lambda \sigma^2 )^2} + \frac{\sigma^2}{\left( 1 + 2 \Lambda \sigma^2 \right)^2}. \tag{3} \label{error}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
The optimal &lt;span class="math"&gt;\(\Lambda\)&lt;/span&gt; is readily obtained by minimizing this average error. This&amp;nbsp;gives,
&lt;/p&gt;
&lt;div class="math"&gt;$$ \label{result}
\Lambda = \frac{1}{2 \theta_*^2}, \tag{4}
$$&lt;/div&gt;
&lt;p&gt;
a constant, independent of sample size. The mean squared error with this choice is obtained by plugging (\ref{result}) into (\ref{error}). This&amp;nbsp;gives
&lt;/p&gt;
&lt;div class="math"&gt;$$
\langle (\hat{\theta} - \theta_*)^2 \rangle = \frac{\sigma^2}{1 + \sigma^2 / \theta_*^2}. \tag{5}
$$&lt;/div&gt;
&lt;p&gt;
Notice that this is strictly less than &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt; &amp;#8212; the variance one would get without regularization &amp;#8212; and that the benefit is largest when &lt;span class="math"&gt;\(\sigma^2 \gg \theta_*^2\)&lt;/span&gt;. That is, &lt;span class="math"&gt;\(L_2\)&lt;/span&gt; regularization is most effective when &lt;span class="math"&gt;\(\theta_*\)&lt;/span&gt; is hard to differentiate from zero &amp;#8212; an intuitive&amp;nbsp;result!&lt;/p&gt;
&lt;h3&gt;&lt;span class="math"&gt;\(L_1\)&lt;/span&gt;&amp;nbsp;optimization&lt;/h3&gt;
&lt;p&gt;The analysis for &lt;span class="math"&gt;\(L_1\)&lt;/span&gt; optimization is similar to the above, but slightly more involved. We go through it quickly. The posterior with an &lt;span class="math"&gt;\(L_1\)&lt;/span&gt; prior is given&amp;nbsp;by
&lt;/p&gt;
&lt;div class="math"&gt;$$ \tag{6}
P\left(\theta \vert \theta_0(\textbf{x})\right) \propto \exp\left( - \frac{(\theta - \theta_0)^2}{2 \sigma^2} - \Lambda \vert \theta \vert \right).
$$&lt;/div&gt;
&lt;p&gt;
Assuming for simplicity that &lt;span class="math"&gt;\(\hat{\theta} &amp;gt; 0\)&lt;/span&gt;, the &lt;span class="caps"&gt;MAP&lt;/span&gt; value is&amp;nbsp;now
&lt;/p&gt;
&lt;div class="math"&gt;$$ \tag{7}
\hat{\theta} = \begin{cases}
\theta_0 - \Lambda \sigma^2 &amp;amp; \text{if } \theta_0 - \Lambda \sigma^2 &amp;gt; 0 \\
0 &amp;amp; \text{else}.
\end{cases}
$$&lt;/div&gt;
&lt;p&gt;
The mean squared error of the estimator&amp;nbsp;is
&lt;/p&gt;
&lt;div class="math"&gt;$$ \tag{8}
\langle (\hat{\theta} - \theta_*)^2 \rangle \equiv \int \frac{1}{\sqrt{2 \pi \sigma^2}}
e^{ - \frac{(\theta_0 - \theta_*)^2}{2 \sigma^2}} \left ( \hat{\theta} - \theta_* \right)^2 d \theta_0.
$$&lt;/div&gt;
&lt;p&gt;
This can be evaluated in terms of error functions. The optimal value of &lt;span class="math"&gt;\(\Lambda\)&lt;/span&gt; is obtained by differentiating the above. Doing this, one finds that it satisfies the&amp;nbsp;equation
&lt;/p&gt;
&lt;div class="math"&gt;$$ \tag{9}
e^{ - \frac{(\tilde{\Lambda}- \tilde{\theta_*})^2}{2} } + \sqrt{\frac{\pi}{2}} \tilde{\Lambda} \ \text{erfc}\left( \frac{\tilde{\Lambda} - \tilde{\theta_*}}{\sqrt{2}} \right ) = 0,
$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(\tilde{\Lambda} \equiv \sigma \Lambda\)&lt;/span&gt; and &lt;span class="math"&gt;\(\tilde{\theta_*} \equiv \theta_* / \sigma\)&lt;/span&gt;. In general, the equation above must be solved numerically. However, in the case where &lt;span class="math"&gt;\(\theta_* \gg \sigma\)&lt;/span&gt; &amp;#8212; relevant when &lt;span class="math"&gt;\(N\)&lt;/span&gt; is large &amp;#8212; we can obtain a clean asymptotic solution. In this case, we have &lt;span class="math"&gt;\(\tilde{\theta_*} \gg 1\)&lt;/span&gt; and we expect &lt;span class="math"&gt;\(\Lambda\)&lt;/span&gt; small. This implies that the above equation can be approximated&amp;nbsp;as
&lt;/p&gt;
&lt;div class="math"&gt;$$ \tag{10}
e^{ - \frac{\tilde{\theta_*}^2}{2} } - \sqrt{2 \pi} \tilde{\Lambda} \sim 0.
$$&lt;/div&gt;
&lt;p&gt;
Solving&amp;nbsp;gives
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{11}
\Lambda \sim \frac{1}{\sqrt{2 \pi \sigma^2}} e^{ - \frac{\theta_*^2}{2 \sigma^2}} \sim \frac{\sqrt{N}}{\sqrt{2 \pi \sigma_1^2}} e^{ - \frac{N \theta_*^2}{2 \sigma_1^2}}.
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Here, in the last line we have made the &lt;span class="math"&gt;\(N\)&lt;/span&gt;-dependence explicit, writing &lt;span class="math"&gt;\(\sigma^2 = \sigma_1^2 / N\)&lt;/span&gt; &amp;#8212; a form that follows when our samples &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; are independent. Whereas the optimal &lt;span class="math"&gt;\(L_2\)&lt;/span&gt; regularization strength approaches a constant, our result here shows that the optimal &lt;span class="math"&gt;\(L_1\)&lt;/span&gt; strength decays exponentially to zero as &lt;span class="math"&gt;\(N\)&lt;/span&gt;&amp;nbsp;increases.&lt;/p&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;The subtlety that we have discussed here is likely already familiar to those with significant applied modeling experience: optimal model hyperparameters can vary with training set size. However, the two toy examples we have presented are interesting in that they allow for this &lt;span class="math"&gt;\(N\)&lt;/span&gt; dependence to be derived explicitly. Interestingly, we have found that the &lt;span class="caps"&gt;MSE&lt;/span&gt;-minimizing &lt;span class="math"&gt;\(L_2\)&lt;/span&gt; regularization remains finite, even at large training set size, but the optimal &lt;span class="math"&gt;\(L_1\)&lt;/span&gt; regularization goes to zero in this same limit. For small and medium &lt;span class="math"&gt;\(N\)&lt;/span&gt;, this exponential dependence represents a strong sensitivity to &lt;span class="math"&gt;\(N\)&lt;/span&gt; &amp;#8212; one that must be carefully taken into account when extrapolating to the full training&amp;nbsp;set.&lt;/p&gt;
&lt;p&gt;One can imagine many other situations where hyperparameters vary strongly with &lt;span class="math"&gt;\(N\)&lt;/span&gt;. For example, very complex systems may allow for ever-increasing model complexity as more data becomes available. Again, in practice, the most straightforward method to check for this is to vary the size of the training set during model selection. If strong dependence is observed, this should be extrapolated out to obtain the truly optimal model for&amp;nbsp;production.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Statistics"></category></entry><entry><title>Bayesian Statistics: MCMC</title><link href="https://efavdb.com/metropolis" rel="alternate"></link><published>2016-08-07T18:37:00-07:00</published><updated>2016-08-07T18:37:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2016-08-07:/metropolis</id><summary type="html">&lt;p&gt;We review the Metropolis algorithm &amp;#8212; a simple Markov Chain Monte Carlo (&lt;span class="caps"&gt;MCMC&lt;/span&gt;) sampling method &amp;#8212; and its application to estimating posteriors in Bayesian statistics. A simple python example is&amp;nbsp;provided.&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;One of the central aims of statistics is to identify good methods for fitting models to data. One way to …&lt;/p&gt;</summary><content type="html">&lt;p&gt;We review the Metropolis algorithm &amp;#8212; a simple Markov Chain Monte Carlo (&lt;span class="caps"&gt;MCMC&lt;/span&gt;) sampling method &amp;#8212; and its application to estimating posteriors in Bayesian statistics. A simple python example is&amp;nbsp;provided.&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;One of the central aims of statistics is to identify good methods for fitting models to data. One way to do this is through the use of Bayes&amp;#8217; rule: If &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; is a vector of &lt;span class="math"&gt;\(k\)&lt;/span&gt; samples from a distribution and &lt;span class="math"&gt;\(\textbf{z}\)&lt;/span&gt; is a vector of model parameters, Bayes&amp;#8217; rule&amp;nbsp;gives
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align} \tag{1} \label{Bayes}
p(\textbf{z} \vert \textbf{x}) = \frac{p(\textbf{x} \vert \textbf{z}) p(\textbf{z})}{p(\textbf{x})}.
\end{align}&lt;/div&gt;
&lt;p&gt;
Here, the probability at left, &lt;span class="math"&gt;\(p(\textbf{z} \vert \textbf{x})\)&lt;/span&gt; &amp;#8212; the &amp;#8220;posterior&amp;#8221; &amp;#8212; is a function that tells us how likely it is that the underlying true parameter values are &lt;span class="math"&gt;\(\textbf{z}\)&lt;/span&gt;, given the information provided by our observations &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt;. Notice that if we could solve for this function, we would be able to identify which parameter values are most likely &amp;#8212; those that are good candidates for a fit. We could also use the posterior&amp;#8217;s variance to quantify how uncertain we are about the true, underlying parameter&amp;nbsp;values.&lt;/p&gt;
&lt;p&gt;Bayes&amp;#8217; rule gives us a method for evaluating the posterior &amp;#8212; now our goal: We need only evaluate the right side of (\ref{Bayes}). The quantities shown there&amp;nbsp;are&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(p(\textbf{x} \vert \textbf{z})\)&lt;/span&gt; &amp;#8212; This is the probability of seeing &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; at fixed parameter values &lt;span class="math"&gt;\(\textbf{z}\)&lt;/span&gt;. Note that if the model is specified, we can often immediately write this part down. For example, if we have a Normal distribution model, specifying &lt;span class="math"&gt;\(\textbf{z}\)&lt;/span&gt; means that we have specified the Normal&amp;#8217;s mean and variance. Given these, we can say how likely it is to observe any &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(p(\textbf{z})\)&lt;/span&gt; &amp;#8212; the &amp;#8220;prior&amp;#8221;. This is something we insert by hand before taking any data. We choose its form so that it covers the values we expect are reasonable for the parameters in&amp;nbsp;question.&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(p(\textbf{x})\)&lt;/span&gt; &amp;#8212; the denominator. Notice that this doesn&amp;#8217;t depend on &lt;span class="math"&gt;\(\textbf{z}\)&lt;/span&gt;, and so represents a normalization constant for the&amp;nbsp;posterior.&lt;/p&gt;
&lt;p&gt;It turns out that the last term above can sometimes be difficult to evaluate analytically, and so we must often resort to numerical methods for estimating the posterior. Monte Carlo sampling is one of the most common approaches taken for doing this. The idea behind Monte Carlo is to take many samples &lt;span class="math"&gt;\(\{\textbf{z}_i\}\)&lt;/span&gt; from the posterior (\ref{Bayes}). Once these are obtained, we can approximate population averages by averages over the samples. For example, the true posterior average &lt;span class="math"&gt;\(\langle\textbf{z} \rangle \equiv \int \textbf{z} p(\textbf{z} \vert \textbf{x}) d \textbf{z}\)&lt;/span&gt; can be approximated by &lt;span class="math"&gt;\(\overline{\textbf{z}} \equiv \frac{1}{N}\sum_i \textbf{z}_i\)&lt;/span&gt;, the sample average. By the law of large numbers, the sample averages are guaranteed to approach the distribution averages as &lt;span class="math"&gt;\(N \to \infty\)&lt;/span&gt;. This means that Monte Carlo can always be used to obtain very accurate parameter estimates, provided we take &lt;span class="math"&gt;\(N\)&lt;/span&gt; sufficiently large &amp;#8212; and that we can find a convenient way to sample from the posterior. In this post, we review one simple variant of Monte Carlo that allows for posterior sampling: the Metropolis&amp;nbsp;algorithm.&lt;/p&gt;
&lt;h2&gt;Metropolis&amp;nbsp;Algorithm&lt;/h2&gt;
&lt;h3&gt;Iterative&amp;nbsp;Procedure&lt;/h3&gt;
&lt;p&gt;Metropolis is an iterative, try-accept algorithm. We initialize the algorithm by selecting a parameter vector &lt;span class="math"&gt;\(\textbf{z}\)&lt;/span&gt; at random. Following this, we repeatedly carry out the following two steps to obtain additional posterior&amp;nbsp;samples:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Identify a next candidate sample &lt;span class="math"&gt;\(\textbf{z}_j\)&lt;/span&gt; via some random process. This candidate selection step can be informed by the current sample&amp;#8217;s position, &lt;span class="math"&gt;\(\textbf{z}_i\)&lt;/span&gt;. For example, one could require that the next candidate be selected from those parameter vectors a given step-size distance from the current sample, &lt;span class="math"&gt;\(\textbf{z}_j \in \{\textbf{z}_k: \vert \textbf{z}_i - \textbf{z}_k \vert = \delta \}\)&lt;/span&gt;. However, while the candidate selected can depend on the current sample, it must not depend on any prior history of the sampling process. Whatever the process chosen (there&amp;#8217;s some flexibility here), we write &lt;span class="math"&gt;\(t_{i,j}\)&lt;/span&gt; for the rate of selecting &lt;span class="math"&gt;\(\textbf{z}_j\)&lt;/span&gt; as the next candidate given the current sample is &lt;span class="math"&gt;\(\textbf{z}_i\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Once a candidate is identified, we either accept or reject it via a second random process. If it is accepted, we mark it down as the next sample, then go back to step one, using the current sample to inform the next candidate selection. Otherwise, we mark the current sample down again, taking it as a repeat sample, and then use it to return to candidate search step, as above. Here, we write &lt;span class="math"&gt;\(A_{i,j}\)&lt;/span&gt; for the rate of accepting &lt;span class="math"&gt;\(\textbf{z}_j\)&lt;/span&gt;, given that it was selected as the next candidate, starting from &lt;span class="math"&gt;\(\textbf{z}_i\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Selecting the trial and acceptance&amp;nbsp;rates&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2016/08/Untitled-1.jpg"&gt;&lt;img alt="Untitled-1" src="https://efavdb.com/wp-content/uploads/2016/08/Untitled-1.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In order to ensure that our above process selects samples according to the distribution (\ref{Bayes}), we need to appropriately set the &lt;span class="math"&gt;\(\{t_{i,j}\}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\{A_{i,j}\}\)&lt;/span&gt; values. To do that, note that at equilibrium one must see the same number of hops from &lt;span class="math"&gt;\(\textbf{z}_i\)&lt;/span&gt; to &lt;span class="math"&gt;\(\textbf{z}_j\)&lt;/span&gt; as hops from &lt;span class="math"&gt;\(\textbf{z}_j\)&lt;/span&gt; from &lt;span class="math"&gt;\(\textbf{z}_i\)&lt;/span&gt; (if this did not hold, one would see a net shifting of weight from one to the other over time, contradicting the assumption of equilibrium). If &lt;span class="math"&gt;\(\rho_i\)&lt;/span&gt; is the fraction of samples the process takes from state &lt;span class="math"&gt;\(i\)&lt;/span&gt;, this condition can be written&amp;nbsp;as
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align} \label{inter}
\rho_i t_{i,j} A_{i,j} = \rho_j t_{j,i} A_{j,i} \tag{3}
\end{align}&lt;/div&gt;
&lt;p&gt;
To select a process that returns the desired sampling weight, we solve for &lt;span class="math"&gt;\(\rho_i\)&lt;/span&gt; over &lt;span class="math"&gt;\(\rho_j\)&lt;/span&gt; in (\ref{inter}) and then equate this to the ratio required by (\ref{Bayes}). This&amp;nbsp;gives
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align} \tag{4} \label{cond}
\frac{\rho_i}{\rho_j} = \frac{t_{j,i} A_{j,i}}{t_{i,j} A_{i,j}}
\equiv \frac{p(\textbf{x} \vert \textbf{z}_i)p(\textbf{z}_i)}{p(\textbf{x} \vert \textbf{z}_j)p(\textbf{z}_j)}.
\end{align}&lt;/div&gt;
&lt;p&gt;
Now, the single constraint above is not sufficient to pin down all of our degrees of freedom. In the Metropolis case, we choose the following working balance: The trial rates between states are set equal, &lt;span class="math"&gt;\(t_{i,j} = t_{j,i}\)&lt;/span&gt; (but remain unspecified &amp;#8212; left to the discretion of the coder on a case-by-case basis), and we&amp;nbsp;set
&lt;/p&gt;
&lt;div class="math"&gt;$$ \tag{5}
A_{i,j} = \begin{cases}
1, &amp;amp; \text{if } p(\textbf{z}_j \vert \textbf{x}) &amp;gt; p(\textbf{z}_i \vert \textbf{x}) \\
\frac{p(\textbf{x} \vert \textbf{z}_j)p(\textbf{z}_j)}{p(\textbf{x} \vert \textbf{z}_i)p(\textbf{z}_i)} \equiv \frac{p(\textbf{z}_j \vert \textbf{x})}{p(\textbf{z}_i \vert \textbf{x})}, &amp;amp; \text{else}.
\end{cases}
$$&lt;/div&gt;
&lt;p&gt;
This last equation says that we choose to always accept a candidate sample if it is more likely than the current one. However, if the candidate is less likely, we only accept a fraction of the time &amp;#8212; with rate equal to the relative probability ratio of the two states. For example, if the candidate is only &lt;span class="math"&gt;\(80%\)&lt;/span&gt; as likely as the current sample, we accept it &lt;span class="math"&gt;\(80%\)&lt;/span&gt; of the time. That&amp;#8217;s it for Metropolis &amp;#8212; a simple &lt;span class="caps"&gt;MCMC&lt;/span&gt; algorithm, guaranteed to satisfy (\ref{cond}), and to therefore equilibrate to (\ref{Bayes})! An example&amp;nbsp;follows.&lt;/p&gt;
&lt;h3&gt;Coding&amp;nbsp;example&lt;/h3&gt;
&lt;p&gt;The following python snippet illustrates the Metropolis algorithm in action. Here, we take 15 samples from a Normal distribution of variance one and true mean also equal to one. We pretend not to know the mean (but assume we do know the variance), assume a uniform prior for the mean, and then run the algorithm to obtain two hundred thousand samples from the mean&amp;#8217;s posterior. &lt;a href="https://efavdb.com/wp-content/uploads/2016/08/result-1.png"&gt;&lt;img alt="result" src="https://efavdb.com/wp-content/uploads/2016/08/result-1.png"&gt;&lt;/a&gt; The histogram at right summarizes the results, obtained by dropping the first 1% of the samples (to protect against bias towards the initialization value). Averaging over the samples returns a mean estimate of &lt;span class="math"&gt;\(\mu \approx 1.4 \pm 0.5\)&lt;/span&gt; (95% confidence interval), consistent with the true value of &lt;span class="math"&gt;\(1\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="c1"&gt;# Take some samples&lt;/span&gt;
&lt;span class="n"&gt;true_mean&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;true_mean&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kp"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;total_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;200000&lt;/span&gt;

&lt;span class="c1"&gt;# Function used to decide move acceptance&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;posterior_numerator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="kp"&gt;prod&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="kp"&gt;prod&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="kp"&gt;prod&lt;/span&gt;

&lt;span class="c1"&gt;# Initialize MCMC, then iterate&lt;/span&gt;
&lt;span class="n"&gt;z1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="n"&gt;posterior_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;z1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;posterior_samples&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;total_samples&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;z_current&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;posterior_samples&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;z_candidate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;z_current&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;
    &lt;span class="n"&gt;rel_prob&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;posterior_numerator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;z_candidate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;posterior_numerator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z_current&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;rel_prob&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;posterior_samples&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z_candidate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;trial_toss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;trial_toss&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;rel_prob&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;posterior_samples&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z_candidate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;posterior_samples&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z_current&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Drop some initial samples and thin&lt;/span&gt;
&lt;span class="n"&gt;thinned_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;posterior_samples&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2000&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;thinned_samples&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Histogram of MCMC samples&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;To summarize, we have reviewed the application of &lt;span class="caps"&gt;MCMC&lt;/span&gt; to Bayesian statistics. &lt;span class="caps"&gt;MCMC&lt;/span&gt; is a general tool for obtaining samples from a probability distribution. It can be applied whenever one can conveniently specify the relative probability of two states &amp;#8212; and so is particularly apt for situations where only the normalization constant of a distribution is difficult to evaluate, precisely the problem with the posterior (\ref{Bayes}). The method entails carrying out an iterative try-accept algorithm, where the rates of trial and acceptance can be adjusted, but must be balanced so that the equilibrium distribution that results approaches the desired form. The key equation enabling us to strike this balance is (\ref{inter}) &amp;#8212; the zero flux condition (aka the &lt;em&gt;detailed balance&lt;/em&gt; condition to physicists) that holds between states at&amp;nbsp;equilibrium.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Methods, Theory"></category></entry><entry><title>Interpreting the results of linear regression</title><link href="https://efavdb.com/interpret-linear-regression" rel="alternate"></link><published>2016-06-29T14:54:00-07:00</published><updated>2016-06-29T14:54:00-07:00</updated><author><name>Cathy Yeh</name></author><id>tag:efavdb.com,2016-06-29:/interpret-linear-regression</id><summary type="html">&lt;p&gt;Our &lt;a href="http://efavdb.github.io/linear-regression"&gt;last post&lt;/a&gt; showed how to obtain the least-squares solution for linear regression and discussed the idea of sampling variability in the best estimates for the coefficients. In this post, we continue the discussion about uncertainty in linear regression &amp;#8212; both in the estimates of individual linear regression coefficients and the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Our &lt;a href="http://efavdb.github.io/linear-regression"&gt;last post&lt;/a&gt; showed how to obtain the least-squares solution for linear regression and discussed the idea of sampling variability in the best estimates for the coefficients. In this post, we continue the discussion about uncertainty in linear regression &amp;#8212; both in the estimates of individual linear regression coefficients and the quality of the overall&amp;nbsp;fit.&lt;/p&gt;
&lt;p&gt;Specifically, we&amp;#8217;ll discuss how to calculate the 95% confidence intervals and p-values from hypothesis tests that are output by many statistical packages like python&amp;#8217;s statsmodels or R. An example with code is provided at the&amp;nbsp;end.&lt;/p&gt;
&lt;h2&gt;Review&lt;/h2&gt;
&lt;p&gt;We wish to predict a scalar response variable &lt;span class="math"&gt;\(y_i\)&lt;/span&gt; given a vector of predictors &lt;span class="math"&gt;\(\vec{x}_i\)&lt;/span&gt; of dimension &lt;span class="math"&gt;\(K\)&lt;/span&gt;. In linear regression, we assume that &lt;span class="math"&gt;\(y_i\)&lt;/span&gt; is a linear function of &lt;span class="math"&gt;\(\vec{x}_i\)&lt;/span&gt;, parameterized by a set of coefficients &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt; and an error term &lt;span class="math"&gt;\(\epsilon_i\)&lt;/span&gt;. The linear model (in matrix format and dropping the arrows over the vectors) for predicting &lt;span class="math"&gt;\(N\)&lt;/span&gt; response variables&amp;nbsp;is
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\tag{1}
y = X\beta + \epsilon.
\end{align}&lt;/div&gt;
&lt;p&gt;The dimensions of each component are: dim(&lt;span class="math"&gt;\(X\)&lt;/span&gt;) = (&lt;span class="math"&gt;\(N\)&lt;/span&gt;,&lt;span class="math"&gt;\(K\)&lt;/span&gt;), dim(&lt;span class="math"&gt;\(\beta\)&lt;/span&gt;) = (&lt;span class="math"&gt;\(K\)&lt;/span&gt;,1), dim(&lt;span class="math"&gt;\(y\)&lt;/span&gt;) = dim(&lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;) = (&lt;span class="math"&gt;\(N\)&lt;/span&gt;,1), where &lt;span class="math"&gt;\(N\)&lt;/span&gt; = # of examples, &lt;span class="math"&gt;\(K\)&lt;/span&gt; = # of regressors / predictors, counting an optional intercept/constant&amp;nbsp;term.&lt;/p&gt;
&lt;p&gt;The ordinary least-squares best estimator of the coefficients, &lt;span class="math"&gt;\(\hat{\beta}\)&lt;/span&gt;, was &lt;a href="http://efavdb.github.io/linear-regression"&gt;derived last time&lt;/a&gt;:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\tag{2}\label{optimal}
\hat{\beta} = (X'X)^{-1}X'y,
\end{align}&lt;/div&gt;
&lt;p&gt;where the hat &amp;#8220;^&amp;#8221; denotes an estimator, not a true population&amp;nbsp;parameter.&lt;/p&gt;
&lt;p&gt;(\ref{optimal}) is a point estimate, but fitting different samples of data from the population will cause the best estimators to shift around. The amount of shifting can be explained by the variance-covariance matrix of &lt;span class="math"&gt;\(\hat{\beta}\)&lt;/span&gt;, &lt;a href="http://efavdb.github.io/linear-regression"&gt;also derived&lt;/a&gt; last time (independent of assumptions of&amp;nbsp;normality):
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\tag{3}\label{cov}
cov(\hat{\beta}, \hat{\beta}) = \sigma^2 (X'X)^{-1}.
\end{align}&lt;/div&gt;
&lt;h2&gt;Goodness of fit - &lt;span class="math"&gt;\(R^2\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;To get a better feel for (\ref{cov}), it&amp;#8217;s helpful to rewrite it in terms of the coefficient of determination &lt;span class="math"&gt;\(R^2\)&lt;/span&gt;. &lt;span class="math"&gt;\(R^2\)&lt;/span&gt; measures how much of the variation in the response variable &lt;span class="math"&gt;\(y\)&lt;/span&gt; is explained by variation in the regressors &lt;span class="math"&gt;\(X\)&lt;/span&gt; (as opposed to the unexplained variation from &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;The variation in &lt;span class="math"&gt;\(y\)&lt;/span&gt;, i.e. the &amp;#8220;total sum of squares&amp;#8221; &lt;span class="math"&gt;\(SST\)&lt;/span&gt;, can be partitioned into the sum of two terms, &amp;#8220;regression sum of squares&amp;#8221; and &amp;#8220;error sum of squares&amp;#8221;: &lt;span class="math"&gt;\(SST = SSR + SSE\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For convenience, let&amp;#8217;s center &lt;span class="math"&gt;\(y\)&lt;/span&gt; and &lt;span class="math"&gt;\(X\)&lt;/span&gt; around their means, e.g. &lt;span class="math"&gt;\(y \rightarrow y - \bar{y}\)&lt;/span&gt; so that the mean &lt;span class="math"&gt;\(\bar{y}=0\)&lt;/span&gt; for the centered variables.&amp;nbsp;Then,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\tag{4}\label{SS}
SST &amp;amp;= \sum_i^N (y - \bar{y})^2 = y'y \\
SSR &amp;amp;= \sum_i^N (X\hat{\beta} - \bar{y})^2 = \hat{y}'\hat{y} \\
SSE &amp;amp;= \sum_i^N (y - \hat{y})^2 = e'e,
\end{align}&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\hat{y} \equiv X\hat{\beta}\)&lt;/span&gt;. Then &lt;span class="math"&gt;\(R^2\)&lt;/span&gt; is defined as the ratio of the regression sum of squares to the total sum of&amp;nbsp;squares:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\tag{5}\label{R2}
R^2 \equiv \frac{SSR}{SST} = 1 - \frac{SSE}{SST}
\end{align}&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(R^2\)&lt;/span&gt; ranges between 0 and 1, with 1 being a perfect fit. According to (\ref{cov}), the variance of a single coefficient &lt;span class="math"&gt;\(\hat{\beta}_k\)&lt;/span&gt; is proportional to the quantity &lt;span class="math"&gt;\((X'X)_{kk}^{-1}\)&lt;/span&gt;, where &lt;span class="math"&gt;\(k\)&lt;/span&gt; denotes the kth diagonal element of &lt;span class="math"&gt;\((X'X)^{-1}\)&lt;/span&gt;, and can be rewritten&amp;nbsp;as
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\tag{6}\label{cov2}
var(\hat{\beta}_k) &amp;amp;= \sigma^2 (X'X)_{kk}^{-1} \\ &amp;amp;= \frac{\sigma^2}{(1 - R_k^2)\sum_i^N (x_{ik} - \bar{x}_k)^2},
\end{align}&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(R_k^2\)&lt;/span&gt; is the &lt;span class="math"&gt;\(R^2\)&lt;/span&gt; in the regression of the kth variable, &lt;span class="math"&gt;\(x_k\)&lt;/span&gt;, against the other predictors &lt;a href="#A1"&gt;[A1]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The key observation from (\ref{cov2}) is that the precision in the estimator decreases if the fit is made over highly correlated regressors, for which &lt;span class="math"&gt;\(R_k^2\)&lt;/span&gt; approaches 1. This problem of multicollinearity in linear regression will be manifested in our simulated&amp;nbsp;example.&lt;/p&gt;
&lt;p&gt;(\ref{cov2}) is also consistent with the observation from our previous post that, all things being equal, the precision in the estimator increases if the fit is made over a direction of greater variance in the&amp;nbsp;data.&lt;/p&gt;
&lt;p&gt;In the next section, &lt;span class="math"&gt;\(R^2\)&lt;/span&gt; will again be useful for interpreting the behavior of one of our test&amp;nbsp;statistics.&lt;/p&gt;
&lt;h2&gt;Calculating test&amp;nbsp;statistics&lt;/h2&gt;
&lt;p&gt;If we assume that the vector of residuals has a multivariate normal distribution, &lt;span class="math"&gt;\(\epsilon \sim N(0, \sigma^2I)\)&lt;/span&gt;, then we can construct test statistics to characterize the uncertainty in the regression. In this section, we&amp;#8217;ll&amp;nbsp;calculate&lt;/p&gt;
&lt;p&gt;​(a) &lt;strong&gt;confidence intervals&lt;/strong&gt; - random intervals around individual estimators &lt;span class="math"&gt;\(\hat{\beta}_k\)&lt;/span&gt; that, if constructed for regressions over multiple samples, would contain the true population parameter, &lt;span class="math"&gt;\(\beta_k\)&lt;/span&gt;, a certain fraction, e.g. 95%, of the time.
(b) &lt;strong&gt;p-value&lt;/strong&gt; - the probability of events as extreme or more extreme than an observed value (a test statistic) occurring under the null hypothesis. If the p-value is less than a given significance level &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; (a common choice is &lt;span class="math"&gt;\(\alpha = 0.05\)&lt;/span&gt;), then the null hypothesis is rejected, e.g. a regression coefficient is said to be&amp;nbsp;significant.&lt;/p&gt;
&lt;p&gt;From the assumption of the distribution of &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;, it follows that &lt;span class="math"&gt;\(\hat{\beta}\)&lt;/span&gt; has a multivariate normal distribution &lt;a href="#A2"&gt;[A2]&lt;/a&gt;:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\tag{7}
\hat{\beta} \sim N(\beta, \sigma^2 (X'X)^{-1}).
\end{align}&lt;/div&gt;
&lt;p&gt; To be explicit, a single coefficient, &lt;span class="math"&gt;\(\hat{\beta}_k\)&lt;/span&gt;, is distributed&amp;nbsp;as
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\tag{8}
\hat{\beta}_k \sim N(\beta_k, \sigma^2 (X'X)_{kk}^{-1}).
\end{align}&lt;/div&gt;
&lt;p&gt;This variable can be standardized as a&amp;nbsp;z-score:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\tag{9}
z_k = \frac{\hat{\beta}_k - \beta_k}{\sigma^2 (X'X)_{kk}^{-1}} \sim N(0,1)
\end{align}&lt;/div&gt;
&lt;p&gt;In practice, we don&amp;#8217;t know the population parameter, &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt;, so we can&amp;#8217;t use the z-score. Instead, we can construct a pivotal quantity, a t-statistic. The t-statistic for &lt;span class="math"&gt;\(\hat{\beta}_k\)&lt;/span&gt; follows a t-distribution with n-K degrees of freedom &lt;a href="#ref1"&gt;[1]&lt;/a&gt;,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\tag{10}\label{tstat}
t_{\hat{\beta}_k} = \frac{\hat{\beta}_k - \beta_k}{s(\hat{\beta}_k)} \sim t_{n-K},
\end{align}&lt;/div&gt;
&lt;p&gt; where &lt;span class="math"&gt;\(s(\hat{\beta}_k)\)&lt;/span&gt; is the standard error of &lt;span class="math"&gt;\(\hat{\beta}_k\)&lt;/span&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\tag{11}
s(\hat{\beta}_k)^2 = \hat{\sigma}^2 (X'X)_{kk}^{-1},
\end{align}&lt;/div&gt;
&lt;p&gt; and &lt;span class="math"&gt;\(\hat{\sigma}^2\)&lt;/span&gt; is the unbiased estimator of &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\tag{12}
\hat{\sigma}^2 = \frac{\epsilon'\epsilon}{n - K}.
\end{align}&lt;/div&gt;
&lt;h3&gt;Confidence intervals around regression&amp;nbsp;coefficients&lt;/h3&gt;
&lt;p&gt;The &lt;span class="math"&gt;\((1-\alpha)\)&lt;/span&gt; confidence interval around an estimator, &lt;span class="math"&gt;\(\hat{\beta}_k \pm \Delta\)&lt;/span&gt;, is defined such that the probability of a random interval containing the true population parameter is &lt;span class="math"&gt;\((1-\alpha)\)&lt;/span&gt;:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\tag{13}
P[\hat{\beta}_k - \Delta &amp;lt; \beta_k &amp;lt; \hat{\beta}_k + \Delta ] = 1 - \alpha,
\end{align}&lt;/div&gt;
&lt;p&gt; where &lt;span class="math"&gt;\(\Delta = t_{1-\alpha/2, n-K} s(\hat{\beta}_k)\)&lt;/span&gt;, and &lt;span class="math"&gt;\(t_{1-\alpha/2, n-K}\)&lt;/span&gt; is the &lt;span class="math"&gt;\(\alpha/2\)&lt;/span&gt;-level critical value for the t-distribution with &lt;span class="math"&gt;\(n-K\)&lt;/span&gt; degrees of&amp;nbsp;freedom.&lt;/p&gt;
&lt;h3&gt;t-test for the significance of a&amp;nbsp;predictor&lt;/h3&gt;
&lt;p&gt;Directly related to the calculation of confidence intervals is testing whether a regressor, &lt;span class="math"&gt;\(\hat{\beta}_k\)&lt;/span&gt;, is statistically significant. The t-statistic for the kth regression coefficient under the null hypothesis that &lt;span class="math"&gt;\(x_k\)&lt;/span&gt; and &lt;span class="math"&gt;\(y\)&lt;/span&gt; are independent follows a t-distribution with n-K degrees of freedom, c.f. (\ref{tstat}) with &lt;span class="math"&gt;\(\beta_k = 0\)&lt;/span&gt;:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\tag{14}
t = \frac{\hat{\beta}_k - 0}{s(\hat{\beta}_k)} \sim t_{n-K}.
\end{align}&lt;/div&gt;
&lt;p&gt;We reject the null-hypothesis if &lt;span class="math"&gt;\(P[t] &amp;lt; \alpha\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;According to (\ref{cov2}), &lt;span class="math"&gt;\(s(\hat{\beta}_k)\)&lt;/span&gt; increases with multicollinearity. Hence, the estimator must be more &amp;#8220;extreme&amp;#8221; in order to be statistically significant in the presence of&amp;nbsp;multicollinearity.&lt;/p&gt;
&lt;h3&gt;F-test for the significance of the&amp;nbsp;regression&lt;/h3&gt;
&lt;p&gt;Whereas the t-test considers the significance of a single regressor, the F-test evaluates the significance of the entire regression, where the null hypothesis is that &lt;em&gt;all&lt;/em&gt; the regressors except the constant are equal to zero: &lt;span class="math"&gt;\(\hat{\beta}_1 = \hat{\beta}_2 = ... = \hat{\beta}_{K-1} = 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The F-statistic under the null hypothesis follows an F-distribution with {K-1, N-K} degrees of freedom &lt;a href="#ref1"&gt;[1]&lt;/a&gt;:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\tag{15}\label{F}
F = \frac{SSR/(K-1)}{SSE/(N-K)} \sim F_{K-1, N-K}.
\end{align}&lt;/div&gt;
&lt;p&gt;It is useful to rewrite the F-statistic in terms of &lt;span class="math"&gt;\(R^2\)&lt;/span&gt; by substituting the expressions from (\ref{&lt;span class="caps"&gt;SS&lt;/span&gt;}) and&amp;nbsp;(\ref{R2}):
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\tag{16}\label{F2}
F = \frac{(N-K) R^2}{(K-1) (1-R^2)}
\end{align}&lt;/div&gt;
&lt;p&gt;Notice how, for fixed &lt;span class="math"&gt;\(R^2\)&lt;/span&gt;, the F-statistic decreases with an increasing number of predictors &lt;span class="math"&gt;\(K\)&lt;/span&gt;. Adding uninformative predictors to the model will decrease the significance of the regression, which motivates parsimony in constructing linear&amp;nbsp;models.&lt;/p&gt;
&lt;h2&gt;Example&lt;/h2&gt;
&lt;p&gt;With these formulas in hand, let&amp;#8217;s consider the problem of predicting the weight of adult women using some simulated data (loosely based on reality). We&amp;#8217;ll look at two models:
(1) &lt;strong&gt;weight ~ height&lt;/strong&gt;.
As expected, height will be a strong predictor of weight, corroborated by a significant p-value for the coefficient of height in the model.
(2) &lt;strong&gt;weight ~ height + shoe size&lt;/strong&gt;.
Height and shoe size are strongly correlated in the simulated data, while height is still a strong predictor of weight. We&amp;#8217;ll find that neither of the predictors has a significant individual p-value, a consequence of&amp;nbsp;collinearity.&lt;/p&gt;
&lt;p&gt;First, import some libraries. We use &lt;code&gt;statsmodels.api.OLS&lt;/code&gt; for the linear regression since it contains a much more detailed report on the results of the fit than &lt;code&gt;sklearn.linear_model.LinearRegression&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;statsmodels.api&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sm&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Next, set the population parameters for the simulated&amp;nbsp;data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;height&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inches&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;mean_height&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;65&lt;/span&gt;
&lt;span class="n"&gt;std_height&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;shoe&lt;/span&gt; &lt;span class="k"&gt;size&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inches&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;mean_shoe_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="n"&gt;std_shoe_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;correlation&lt;/span&gt; &lt;span class="k"&gt;between&lt;/span&gt; &lt;span class="n"&gt;height&lt;/span&gt; &lt;span class="k"&gt;and&lt;/span&gt; &lt;span class="n"&gt;shoe&lt;/span&gt; &lt;span class="k"&gt;size&lt;/span&gt;
&lt;span class="n"&gt;r_height_shoe&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;98&lt;/span&gt; &lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;height&lt;/span&gt; &lt;span class="k"&gt;and&lt;/span&gt; &lt;span class="n"&gt;shoe&lt;/span&gt; &lt;span class="k"&gt;size&lt;/span&gt; &lt;span class="k"&gt;are&lt;/span&gt; &lt;span class="n"&gt;highly&lt;/span&gt; &lt;span class="n"&gt;correlated&lt;/span&gt;
&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;covariance&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="n"&gt;height&lt;/span&gt; &lt;span class="k"&gt;and&lt;/span&gt; &lt;span class="n"&gt;shoe&lt;/span&gt; &lt;span class="k"&gt;size&lt;/span&gt;
&lt;span class="n"&gt;var_height_shoe&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;r_height_shoe&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;std_height&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;std_shoe_size&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;matrix&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="n"&gt;means&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;and&lt;/span&gt; &lt;span class="n"&gt;covariance&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov&lt;/span&gt;
&lt;span class="n"&gt;mu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean_height&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mean_shoe_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;cov&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std_height&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;var_height_shoe&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;var_height_shoe&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std_shoe_size&lt;/span&gt;&lt;span class="p"&gt;)]]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Generate the simulated&amp;nbsp;data:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="nb"&gt;number&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="k"&gt;data&lt;/span&gt; &lt;span class="n"&gt;points&lt;/span&gt;
&lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;

&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;85&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;height&lt;/span&gt; &lt;span class="k"&gt;and&lt;/span&gt; &lt;span class="n"&gt;shoe&lt;/span&gt; &lt;span class="k"&gt;size&lt;/span&gt;
&lt;span class="n"&gt;X1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multivariate_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;height&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alone&lt;/span&gt;
&lt;span class="n"&gt;X0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;weight&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;220&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X0&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Below is the simulated data plotted against each other.
&lt;a href="https://efavdb.com/wp-content/uploads/2016/06/scatter_height_weight_shoesize_cropped.png"&gt;&lt;img alt="scatterplots" src="https://efavdb.com/wp-content/uploads/2016/06/scatter_height_weight_shoesize_cropped.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Fit the linear&amp;nbsp;models:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="k"&gt;add&lt;/span&gt; &lt;span class="k"&gt;column&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="n"&gt;ones&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;intercept&lt;/span&gt;
&lt;span class="n"&gt;X0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sm&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_constant&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;X1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sm&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_constant&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;OLS&amp;quot;&lt;/span&gt; &lt;span class="n"&gt;stands&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;Ordinary&lt;/span&gt; &lt;span class="n"&gt;Least&lt;/span&gt; &lt;span class="n"&gt;Squares&lt;/span&gt;
&lt;span class="n"&gt;sm0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sm&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OLS&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X0&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;sm1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sm&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OLS&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Look at the summary report, &lt;code&gt;sm0.summary()&lt;/code&gt;, for the weight ~ height&amp;nbsp;model.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;OLS Regression Results&lt;/span&gt;
&lt;span class="err"&gt;==============================================================================&lt;/span&gt;
&lt;span class="err"&gt;Dep. Variable: y R-squared: 0.788&lt;/span&gt;
&lt;span class="c"&gt;Model: OLS Adj. R-squared: 0.776&lt;/span&gt;
&lt;span class="c"&gt;Method: Least Squares F-statistic: 66.87&lt;/span&gt;
&lt;span class="c"&gt;Date: Wed, 29 Jun 2016 Prob (F-statistic): 1.79e-07&lt;/span&gt;
&lt;span class="c"&gt;Time: 14:28:08 Log-Likelihood: -70.020&lt;/span&gt;
&lt;span class="err"&gt;No. Observations: 20 AIC: 144.0&lt;/span&gt;
&lt;span class="err"&gt;Df Residuals: 18 BIC: 146.0&lt;/span&gt;
&lt;span class="err"&gt;Df Model: 1&lt;/span&gt;
&lt;span class="err"&gt;Covariance Type: nonrobust&lt;/span&gt;
&lt;span class="err"&gt;==============================================================================&lt;/span&gt;
&lt;span class="err"&gt;coef std err t P&amp;gt;|t| [95.0% Conf. Int.]&lt;/span&gt;
&lt;span class="err"&gt;------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class="err"&gt;const -265.2764 49.801 -5.327 0.000 -369.905 -160.648&lt;/span&gt;
&lt;span class="err"&gt;x1 6.1857 0.756 8.178 0.000 4.596 7.775&lt;/span&gt;
&lt;span class="err"&gt;==============================================================================&lt;/span&gt;
&lt;span class="c"&gt;Omnibus: 0.006 Durbin-Watson: 2.351&lt;/span&gt;
&lt;span class="err"&gt;Prob(Omnibus): 0.997 Jarque-Bera (JB): 0.126&lt;/span&gt;
&lt;span class="c"&gt;Skew: 0.002 Prob(JB): 0.939&lt;/span&gt;
&lt;span class="c"&gt;Kurtosis: 2.610 Cond. No. 1.73e+03&lt;/span&gt;
&lt;span class="err"&gt;==============================================================================&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The height variable, &lt;code&gt;x1&lt;/code&gt;, is significant according to the t-test, as is the intercept, denoted &lt;code&gt;const&lt;/code&gt; in the report. Also, notice the coefficient used to simulate the dependence of weight on height (&lt;span class="math"&gt;\(\beta_1\)&lt;/span&gt; = 5.5), is contained in the 95% confidence interval of &lt;code&gt;x1&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Next, let&amp;#8217;s look at the summary report, &lt;code&gt;sm1.summary()&lt;/code&gt;, for the weight ~ height + shoe_size&amp;nbsp;model.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;OLS Regression Results&lt;/span&gt;
&lt;span class="err"&gt;==============================================================================&lt;/span&gt;
&lt;span class="err"&gt;Dep. Variable: y R-squared: 0.789&lt;/span&gt;
&lt;span class="c"&gt;Model: OLS Adj. R-squared: 0.765&lt;/span&gt;
&lt;span class="c"&gt;Method: Least Squares F-statistic: 31.86&lt;/span&gt;
&lt;span class="c"&gt;Date: Wed, 29 Jun 2016 Prob (F-statistic): 1.78e-06&lt;/span&gt;
&lt;span class="c"&gt;Time: 14:28:08 Log-Likelihood: -69.951&lt;/span&gt;
&lt;span class="err"&gt;No. Observations: 20 AIC: 145.9&lt;/span&gt;
&lt;span class="err"&gt;Df Residuals: 17 BIC: 148.9&lt;/span&gt;
&lt;span class="err"&gt;Df Model: 2&lt;/span&gt;
&lt;span class="err"&gt;Covariance Type: nonrobust&lt;/span&gt;
&lt;span class="err"&gt;==============================================================================&lt;/span&gt;
&lt;span class="err"&gt;coef std err t P&amp;gt;|t| [95.0% Conf. Int.]&lt;/span&gt;
&lt;span class="err"&gt;------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class="err"&gt;const -333.1599 204.601 -1.628 0.122 -764.829 98.510&lt;/span&gt;
&lt;span class="err"&gt;x1 7.4944 3.898 1.923 0.071 -0.729 15.718&lt;/span&gt;
&lt;span class="err"&gt;x2 -2.3090 6.739 -0.343 0.736 -16.527 11.909&lt;/span&gt;
&lt;span class="err"&gt;==============================================================================&lt;/span&gt;
&lt;span class="c"&gt;Omnibus: 0.015 Durbin-Watson: 2.342&lt;/span&gt;
&lt;span class="err"&gt;Prob(Omnibus): 0.993 Jarque-Bera (JB): 0.147&lt;/span&gt;
&lt;span class="c"&gt;Skew: 0.049 Prob(JB): 0.929&lt;/span&gt;
&lt;span class="c"&gt;Kurtosis: 2.592 Cond. No. 7.00e+03&lt;/span&gt;
&lt;span class="err"&gt;==============================================================================&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Neither of the regressors &lt;code&gt;x1&lt;/code&gt; and &lt;code&gt;x2&lt;/code&gt; is significant at a significance level of &lt;span class="math"&gt;\(\alpha=0.05\)&lt;/span&gt;. In the simulated data, adult female weight has a positive linear correlation with height and shoe size, but the strong collinearity of the predictors (simulated with a correlation coefficient of 0.98) causes each variable to fail a t-test in the model &amp;#8212; and even results in the wrong sign for the dependence on shoe&amp;nbsp;size.&lt;/p&gt;
&lt;p&gt;Although the predictors fail individual t-tests, the overall regression &lt;em&gt;is&lt;/em&gt; significant, i.e. the predictors are jointly informative, according to the&amp;nbsp;F-test.&lt;/p&gt;
&lt;p&gt;Notice, however, that the p-value of the F-test has decreased compared to the simple linear model, as expected from (\ref{F2}), since including the extra variable, shoe size, did not improve &lt;span class="math"&gt;\(R^2\)&lt;/span&gt; but did increase &lt;span class="math"&gt;\(K\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;#8217;s manually calculate the standard error, t-statistics, F-statistic, corresponding p-values, and confidence intervals using the equations from&amp;nbsp;above.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;OLS&lt;/span&gt; &lt;span class="n"&gt;solution&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;eqn&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="n"&gt;form&lt;/span&gt; &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;X)*beta_hat = X&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;
&lt;span class="n"&gt;beta_hat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;solve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;residuals&lt;/span&gt;
&lt;span class="n"&gt;epsilon&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;beta_hat&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;degrees&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="n"&gt;freedom&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="n"&gt;residuals&lt;/span&gt;
&lt;span class="n"&gt;dof&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;best&lt;/span&gt; &lt;span class="n"&gt;estimator&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;
&lt;span class="n"&gt;sigma_hat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;dof&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;standard&lt;/span&gt; &lt;span class="n"&gt;error&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="n"&gt;beta_hat&lt;/span&gt;
&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sigma_hat&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;diag&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="mi"&gt;95&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;confidence&lt;/span&gt; &lt;span class="n"&gt;intervals&lt;/span&gt;
&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="o"&gt;+/-&lt;/span&gt;&lt;span class="n"&gt;t_&lt;/span&gt;&lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="err"&gt;}&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nb"&gt;interval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dof&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;conf_intervals&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;beta_hat&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nb"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nb"&gt;interval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;95&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dof&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="k"&gt;statistics&lt;/span&gt; &lt;span class="k"&gt;under&lt;/span&gt; &lt;span class="k"&gt;null&lt;/span&gt; &lt;span class="n"&gt;hypothesis&lt;/span&gt;
&lt;span class="n"&gt;t_stat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;beta_hat&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="k"&gt;values&lt;/span&gt;
&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;survival&lt;/span&gt; &lt;span class="k"&gt;function&lt;/span&gt; &lt;span class="n"&gt;sf&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;CDF&lt;/span&gt;
&lt;span class="n"&gt;p_values&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t_stat&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dof&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;SSR&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;regression&lt;/span&gt; &lt;span class="k"&gt;sum&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="n"&gt;squares&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y_hat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;beta_hat&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y_mu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;mean_SSR&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;y_hat&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y_mu&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_hat&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y_mu&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;beta_hat&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;statistic&lt;/span&gt;
&lt;span class="n"&gt;f_stat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mean_SSR&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sigma_hat&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;f-statistic:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f_stat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;\n&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;statistic&lt;/span&gt;
&lt;span class="n"&gt;p_values_f_stat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f_stat&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dfn&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;beta_hat&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dfd&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dof&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;p-value of f-statistic:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p_values_f_stat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;\n&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The output values, below, from printing the manual calculations are consistent with the summary&amp;nbsp;report:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;beta_hat&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;333&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;15990097&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;49444671&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;30898743&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;degrees&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="n"&gt;freedom&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="n"&gt;residuals&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;17&lt;/span&gt;

&lt;span class="n"&gt;sigma_hat&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;66991550428&lt;/span&gt;

&lt;span class="n"&gt;standard&lt;/span&gt; &lt;span class="n"&gt;error&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="n"&gt;beta_hat&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;204&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;60056111&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;89776076&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;73900599&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;confidence&lt;/span&gt; &lt;span class="n"&gt;intervals&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="p"&gt;[[&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;64829352&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;02&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;85095501&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;01&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;29109662&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;01&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;57180031&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;01&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;65270473&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;01&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;19090724&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;01&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;

&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="k"&gt;statistics&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;62834305&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;92275698&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;34263027&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="k"&gt;values&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="k"&gt;statistics&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;1218417&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;07142839&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;73607656&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;statistic&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;31&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;8556171105&lt;/span&gt;

&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;statistic&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;77777555162&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;06&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The full code is available as an &lt;a href="https://github.com/EFavDB/linear-regression"&gt;IPython notebook on github&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;Assuming a multivariate normal distribution for the residuals in linear regression allows us to construct test statistics and therefore specify uncertainty in our&amp;nbsp;fits.&lt;/p&gt;
&lt;p&gt;A t-test judges the explanatory power of a predictor in isolation, although the standard error that appears in the calculation of the t-statistic is a function of the other predictors in the model. On the other hand, an F-test is a global test that judges the explanatory power of all the predictors together, and we&amp;#8217;ve seen that parsimony in choosing predictors can improve the quality of the overall&amp;nbsp;regression.&lt;/p&gt;
&lt;p&gt;We&amp;#8217;ve also seen that multicollinearity can throw off the results of individual t-tests as well as obscure the interpretation of the signs of the fitted coefficients. A symptom of multicollinearity is when none of the individual coefficients are significant but the overall F-test is&amp;nbsp;significant.&lt;/p&gt;
&lt;h3&gt;Reference&lt;/h3&gt;
&lt;p&gt;[1] Greene, W., Econometric Analysis, Seventh edition, Prentice Hall, 2011 - &lt;a href="http://people.stern.nyu.edu/wgreene/MathStat/Outline.htm"&gt;chapters available&amp;nbsp;online&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Appendix&lt;/h3&gt;
&lt;p&gt;[A1]
We specifically want the kth diagonal element from the inverse moment matrix, &lt;span class="math"&gt;\((X'X)^{-1}\)&lt;/span&gt;. The matrix &lt;span class="math"&gt;\(X\)&lt;/span&gt; can be &lt;a href="https://en.wikipedia.org/wiki/Block_matrix"&gt;partitioned&lt;/a&gt;&amp;nbsp;as &lt;/p&gt;
&lt;div class="math"&gt;$$[X_{(k)} \vec{x}_k],$$&lt;/div&gt;
&lt;p&gt; where &lt;span class="math"&gt;\(\vec{x}_k\)&lt;/span&gt; is an N x 1 column vector containing the kth variable of each of the N samples, and &lt;span class="math"&gt;\(X_{(k)}\)&lt;/span&gt; is the N x (K-1) matrix containing the rest of the variables and constant intercept. For convenience, let &lt;span class="math"&gt;\(X_{(k)}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\vec{x}_k\)&lt;/span&gt; be centered about their (column-wise)&amp;nbsp;means.&lt;/p&gt;
&lt;p&gt;Matrix multiplication of the block-partitioned form of &lt;span class="math"&gt;\(X\)&lt;/span&gt; with its transpose results in the following block&amp;nbsp;matrix:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
(X'X) =
\begin{bmatrix}
X_{(k)}'X_{(k)} &amp;amp; X_{(k)}'\vec{x}_k \
\vec{x}_k'X_{(k)} &amp;amp; \vec{x}_k'\vec{x}_k
\end{bmatrix}
\end{align}&lt;/div&gt;
&lt;p&gt;The above matrix has four blocks, and &lt;a href="https://en.wikipedia.org/wiki/Block_matrix#Block_matrix_inversion"&gt;can be inverted blockwise&lt;/a&gt; to obtain another matrix with four blocks. The lower right block corresponding to the kth diagonal element of the inverted matrix is a&amp;nbsp;scalar:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
(X'X)^{-1}_{kk} &amp;amp;= [\vec{x}_k'\vec{x}_k - \vec{x}_k'X_{(k)}(X_{(k)}'X_{(k)})^{-1}X_{(k)}'\vec{x}_k]^{-1} \\
&amp;amp;= \left[\vec{x}_k'\vec{x}_k \left( 1 - \frac{\vec{x}_k'X_{(k)}(X_{(k)}'X_{(k)})^{-1}X_{(k)}'\vec{x}_k}{\vec{x}_k'\vec{x}_k} \right)\right]^{-1}
\end{align}&lt;/div&gt;
&lt;p&gt;Then the numerator of the fraction in the parentheses above can be&amp;nbsp;simplified:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
\vec{x}_k'X_{(k)} ((X_{(k)}'X_{(k)})^{-1}X_{(k)}'\vec{x}_k) &amp;amp;= \vec{x}_k' X_{(k)} \hat{\beta}_{(k)} \\
&amp;amp;= (X_{(k)}\hat{\beta}_{(k)} + \epsilon_k)'X_{(k)}\hat{\beta}_{(k)} \\
&amp;amp;= \hat{x}_k'\hat{x}_k,
\end{align}&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\hat{\beta}_{(k)}\)&lt;/span&gt; is the &lt;span class="caps"&gt;OLS&lt;/span&gt; solution for the coefficients in the regression on the &lt;span class="math"&gt;\(\vec{x}_k\)&lt;/span&gt; by the remaining variables &lt;span class="math"&gt;\(X_{(k)}\)&lt;/span&gt;: &lt;span class="math"&gt;\(\vec{x}_k = X_{(k)} \beta_{(k)} + \epsilon_k\)&lt;/span&gt;. In the last line, we used one of the constraints on the residuals &amp;#8212; that the residuals and predictors are uncorrelated, &lt;span class="math"&gt;\(\epsilon_k'X_{(k)} = 0\)&lt;/span&gt;. Plugging in this simplification for the numerator and using the definition of &lt;span class="math"&gt;\(R^2\)&lt;/span&gt; from (\ref{R2}), we obtain our final&amp;nbsp;result:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
(X'X)^{-1}_{kk} &amp;amp;= \left[\vec{x}_k'\vec{x}_k \left( 1 - \frac{\hat{x}_k'\hat{x}_k}{\vec{x}_k'\vec{x}_k} \right)\right]^{-1} \\
&amp;amp;= \left[\vec{x}_k'\vec{x}_k ( 1 - R_k^2 )\right]^{-1}
\end{align}&lt;/div&gt;
&lt;p&gt;[A2]
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
\hat{\beta} &amp;amp;= (X'X)^{-1}X'y \\
&amp;amp;= (X'X)^{-1}X'(X\beta + \epsilon) \\
&amp;amp;= \beta + (X'X)^{-1}X'N(0, \sigma^2I) \\
&amp;amp; \sim N(\beta, \sigma^2 (X'X)^{-1})
\end{align}&lt;/div&gt;
&lt;p&gt; The last line is by properties of &lt;a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Affine_transformation"&gt;affine transformations on multivariate normal distributions&lt;/a&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Methods, Theory"></category><category term="statistics"></category></entry><entry><title>Linear Regression</title><link href="https://efavdb.com/linear-regression" rel="alternate"></link><published>2016-05-29T11:27:00-07:00</published><updated>2016-05-29T11:27:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2016-05-29:/linear-regression</id><summary type="html">&lt;p&gt;We review classical linear regression using vector-matrix notation. In particular, we derive a) the least-squares solution, b) the fit&amp;#8217;s coefficient covariance matrix &amp;#8212; showing that the coefficient estimates are most precise along directions that have been sampled over a large range of values (the high variance directions, a la &lt;span class="caps"&gt;PCA …&lt;/span&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;We review classical linear regression using vector-matrix notation. In particular, we derive a) the least-squares solution, b) the fit&amp;#8217;s coefficient covariance matrix &amp;#8212; showing that the coefficient estimates are most precise along directions that have been sampled over a large range of values (the high variance directions, a la &lt;span class="caps"&gt;PCA&lt;/span&gt;), and c) an unbiased estimate for the underlying sample variance (assuming normal sample variance in this last case). We then review how these last two results can be used to provide confidence intervals / hypothesis tests for the coefficient estimates. Finally, we show that similar results follow from a Bayesian&amp;nbsp;approach.&lt;/p&gt;
&lt;p&gt;Last edited July 23,&amp;nbsp;2016.&lt;/p&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Here, we consider the problem of fitting a linear curve to &lt;span class="math"&gt;\(N\)&lt;/span&gt; data points of the form &lt;span class="math"&gt;\((\vec{x}_i, y_i),\)&lt;/span&gt; where the &lt;span class="math"&gt;\(\{\vec{x}_i\}\)&lt;/span&gt; are column vectors of predictors that sit in an &lt;span class="math"&gt;\(L\)&lt;/span&gt;-dimensional space and the &lt;span class="math"&gt;\(\{y_i\}\)&lt;/span&gt; are the response values we wish to predict given the &lt;span class="math"&gt;\(\{x_i\}\)&lt;/span&gt;. The linear approximation will be defined by a set of coefficients, &lt;span class="math"&gt;\(\{\beta_j\}\)&lt;/span&gt; so&amp;nbsp;that
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
\hat{y}_i \equiv \sum_j x_{i,j} \beta_j = \vec{x}_i^T \cdot \vec{\beta} . \tag{1} \label{1}
\end{align}&lt;/div&gt;
&lt;p&gt;
We seek the &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt; that minimizes the average squared &lt;span class="math"&gt;\(y\)&lt;/span&gt;&amp;nbsp;error,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align} \tag{2} \label{2}
J = \sum_i \left ( y_i - \hat{y}_i \right)^2 = \sum_i \left (y_i - \vec{x}_i^T \cdot \vec{\beta} \right)^2.
\end{align}&lt;/div&gt;
&lt;p&gt;
It turns out that this is a problem where one can easily derive an analytic expression for the optimal solution. It&amp;#8217;s also possible to derive an expression for the variance in the optimal solution &amp;#8212; that is, how much we might expect the optimal parameter estimates to change were we to start with some other &lt;span class="math"&gt;\(N\)&lt;/span&gt; data points instead. These estimates can then be used to generate confidence intervals for the coefficient estimates. Here, we review these results, give a simple interpretation to the theoretical variance, and finally show that the same results follow from a Bayesian&amp;nbsp;approach.&lt;/p&gt;
&lt;h3&gt;Optimal&amp;nbsp;solution&lt;/h3&gt;
&lt;p&gt;We seek the coefficient vector that minimizes (\ref{2}). We can find this by differentiating this cost function with respect to &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt;, setting the result to zero. This&amp;nbsp;gives,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align} \tag{3}
\partial_{\beta_j} J = 2 \sum_i \left (y_i - \sum_k x_{i,k} \beta_k \right) x_{i,j} = 0.
\end{align}&lt;/div&gt;
&lt;p&gt;
We next define the matrix &lt;span class="math"&gt;\(X\)&lt;/span&gt; so that &lt;span class="math"&gt;\(X_{i,j} = \vec{x}_{i,j}\)&lt;/span&gt;. Plugging this into the above, we&amp;nbsp;obtain
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
\partial_{\beta_j} J &amp;amp;= 2 \sum_i X_{j,i}^T \left (y_i - \sum_k X_{i,k} \beta_k \right) = 0 \\
&amp;amp;= X^T \cdot \left ( \vec{y} - X \cdot \vec{\beta}\right ) = 0.\tag{4}
\end{align}&lt;/div&gt;
&lt;p&gt;
Rearranging&amp;nbsp;gives
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
X^T X \cdot \vec{\beta} = X^T \cdot \vec{y} \to
\vec{\beta} = (X^T X)^{-1} \cdot X^T \cdot \vec{y} \tag{5} \label{optimal}
\end{align}&lt;/div&gt;
&lt;p&gt;
This is the squared-error-minimizing&amp;nbsp;solution.&lt;/p&gt;
&lt;h3&gt;Parameter covariance&amp;nbsp;matrix&lt;/h3&gt;
&lt;p&gt;Now, when one carries out a linear fit to some data, the best line often does not go straight through all of the data. Here, we consider the case where the reason for the discrepancy is not that the posited linear form is incorrect, but that there are some hidden variables not measured that the &lt;span class="math"&gt;\(y\)&lt;/span&gt;-values also depend on. Assuming our data points represent random samples over these hidden variables, we can model their effect as adding a random noise term to the form (\ref{1}), so&amp;nbsp;that
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\tag{6} \label{noise}
y_i = \vec{x}_i^T \cdot \vec{\beta}_{true} + \epsilon_i,
\end{align}&lt;/div&gt;
&lt;p&gt;
with &lt;span class="math"&gt;\(\langle \epsilon_i \rangle =0\)&lt;/span&gt;, &lt;span class="math"&gt;\(\langle \epsilon_i^2 \rangle = \sigma^2\)&lt;/span&gt;, and &lt;span class="math"&gt;\(\vec{\beta}_{true}\)&lt;/span&gt; the exact (but unknown) coefficient&amp;nbsp;vector.&lt;/p&gt;
&lt;p&gt;Plugging (\ref{noise}) into (\ref{optimal}), we see that &lt;span class="math"&gt;\(\langle \vec{\beta} \rangle = \vec{\beta}_{true}\)&lt;/span&gt;. However, the variance of the &lt;span class="math"&gt;\(\epsilon_i\)&lt;/span&gt; injects some uncertainty into our fit: Each realization of the noise will generate slightly different &lt;span class="math"&gt;\(y\)&lt;/span&gt; values, causing the &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt; fit coefficients to vary. To estimate the magnitude of this effect, we can calculate the covariance matrix of &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt;. At fixed (constant) &lt;span class="math"&gt;\(X\)&lt;/span&gt;, plugging in (\ref{optimal}) for &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt;&amp;nbsp;gives
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
cov(\vec{\beta}, \vec{\beta}) &amp;amp;= cov \left( (X^T X)^{-1} \cdot X^T \cdot \vec{y} , \vec{y}^T \cdot X \cdot (X^T X)^{-1, T} \right) \\
&amp;amp;= (X^T X)^{-1} \cdot X^T \cdot cov(\vec{y}^T, \vec{y} ) \cdot X \cdot (X^T X)^{-1, T}
\\
&amp;amp;= \sigma^2 \left( X^T X \right)^{-1} \cdot X^T X \cdot \left( X^T X \right)^{-1, T} \\
&amp;amp;= \sigma^2 \left( X^T X \right)^{-1}. \tag{7} \label{cov}
\end{align}&lt;/div&gt;
&lt;p&gt;
In the third line here, note that we have assumed that the &lt;span class="math"&gt;\(\epsilon_i\)&lt;/span&gt; are independent, so that &lt;span class="math"&gt;\(cov(\vec{y},\vec{y}) = \sigma^2 I.\)&lt;/span&gt; We&amp;#8217;ve also used the fact that &lt;span class="math"&gt;\(X^T X\)&lt;/span&gt; is&amp;nbsp;symmetric.&lt;/p&gt;
&lt;p&gt;To get a feel for the significance of (\ref{cov}), it is helpful to consider the case where the average &lt;span class="math"&gt;\(x\)&lt;/span&gt; values are zero. In this&amp;nbsp;case,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
\left( X^T X \right)_{i,j} &amp;amp;\equiv&amp;amp; \sum_k \delta X_{k,i} \delta X_{k,j} \equiv N \times \langle x_i, x_j\rangle. \tag{8} \label{corr_mat}
\end{align}&lt;/div&gt;
&lt;p&gt;
&lt;a href="https://efavdb.com/wp-content/uploads/2016/05/scatter.jpg"&gt;&lt;img alt="margin around decision boundary" src="https://efavdb.com/wp-content/uploads/2016/05/scatter.jpg"&gt;&lt;/a&gt; That is, &lt;span class="math"&gt;\(X^T X\)&lt;/span&gt; is proportional to the correlation matrix of our &lt;span class="math"&gt;\(x\)&lt;/span&gt; values. This correlation matrix is real and symmetric, and thus has an orthonormal set of eigenvectors. The eigenvalue corresponding to the &lt;span class="math"&gt;\(k\)&lt;/span&gt;-th eigenvector gives the variance of our data set&amp;#8217;s &lt;span class="math"&gt;\(k\)&lt;/span&gt;-th component values in this basis &amp;#8212; details can be found in our &lt;a href="http://efavdb.github.io/principal-component-analysis"&gt;article on &lt;span class="caps"&gt;PCA&lt;/span&gt;&lt;/a&gt;. This implies a simple interpretation of (\ref{cov}): The variance in the &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt; coefficients will be lowest for predictors parallel to the highest variance &lt;span class="caps"&gt;PCA&lt;/span&gt; components (eg &lt;span class="math"&gt;\(x_1\)&lt;/span&gt; in the figure shown) and highest for predictors parallel to the lowest variance &lt;span class="caps"&gt;PCA&lt;/span&gt; components (&lt;span class="math"&gt;\(x_2\)&lt;/span&gt; in the figure). This observation can often be exploited during an experiment&amp;#8217;s design: If a particular coefficient is desired to high accuracy, one should make sure to sample the corresponding predictor over a wide&amp;nbsp;range.&lt;/p&gt;
&lt;p&gt;[Note: Cathy gives an interesting, alternative interpretation for the parameter estimate variances in a follow-up post, &lt;a href="http://efavdb.github.io/interpret-linear-regression"&gt;here&lt;/a&gt;.]&lt;/p&gt;
&lt;h3&gt;Unbiased estimator for &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;The result (\ref{cov}) gives an expression for the variance of the parameter coefficients in terms of the underlying sample variance &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt;. In practice, &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt; is often not provided and must be estimated from the observations at hand. Assuming that the &lt;span class="math"&gt;\(\{\epsilon_i\}\)&lt;/span&gt; in (\ref{noise}) are independent &lt;span class="math"&gt;\(\mathcal{N}(0, \sigma^2)\)&lt;/span&gt; random variables, we now show that the following provides an unbiased estimate for this&amp;nbsp;variance:
&lt;/p&gt;
&lt;div class="math"&gt;$$
S^2 \equiv \frac{1}{N-L} \sum_i \left ( y_i - \vec{x}_i^T \cdot \vec{\beta} \right) ^2. \tag{9} \label{S}
$$&lt;/div&gt;
&lt;p&gt;
Note that this is a normalized sum of squared residuals from our fit, with &lt;span class="math"&gt;\((N-L)\)&lt;/span&gt; as the normalization constant &amp;#8212; the number of samples minus the number of fit parameters. To prove that &lt;span class="math"&gt;\(\langle S^2 \rangle = \sigma^2\)&lt;/span&gt;, we plug in (\ref{optimal}) for &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt;, combining with (\ref{noise}) for &lt;span class="math"&gt;\(\vec{y}\)&lt;/span&gt;. This&amp;nbsp;gives
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align} \nonumber
S^2 &amp;amp;= \frac{1}{N-L} \sum_i \left ( y_i - \vec{x}_i^T \cdot (X^T X)^{-1} \cdot X^T \cdot \{ X \cdot \vec{\beta}_{true} + \vec{\epsilon} \} \right) ^2 \\ \nonumber
&amp;amp;= \frac{1}{N-L} \sum_i \left ( \{y_i - \vec{x}_i^T \cdot\vec{\beta}_{true} \} - \vec{x}_i^T \cdot (X^T X)^{-1} \cdot X^T \cdot \vec{\epsilon} \right) ^2 \\
&amp;amp;= \frac{1}{N-L} \sum_i \left ( \epsilon_i - \vec{x}_i^T \cdot (X^T X)^{-1} \cdot X^T \cdot \vec{\epsilon} \right) ^2 \tag{10}. \label{S2}
\end{align}&lt;/div&gt;
&lt;p&gt;
The second term in the last line is the &lt;span class="math"&gt;\(i\)&lt;/span&gt;-th component of the&amp;nbsp;vector
&lt;/p&gt;
&lt;div class="math"&gt;$$
X \cdot (X^T X)^{-1} \cdot X^T \cdot \vec{\epsilon} \equiv \mathbb{P} \cdot \vec{\epsilon}. \tag{11} \label{projection}
$$&lt;/div&gt;
&lt;p&gt;
Here, &lt;span class="math"&gt;\(\mathbb{P}\)&lt;/span&gt; is a projection operator &amp;#8212; this follows from the fact that &lt;span class="math"&gt;\(\mathbb{P}^2 = \mathbb{P}\)&lt;/span&gt;. When it appears in (\ref{projection}), &lt;span class="math"&gt;\(\mathbb{P}\)&lt;/span&gt; maps &lt;span class="math"&gt;\(\vec{\epsilon}\)&lt;/span&gt; into the &lt;span class="math"&gt;\(L\)&lt;/span&gt;-dimensional coordinate space spanned by the &lt;span class="math"&gt;\(\{\vec{x_i}\}\)&lt;/span&gt;, scales the result using (\ref{corr_mat}), then maps it back into its original &lt;span class="math"&gt;\(N\)&lt;/span&gt;-dimensional space. The net effect is to project &lt;span class="math"&gt;\(\vec{\epsilon}\)&lt;/span&gt; into an &lt;span class="math"&gt;\(L\)&lt;/span&gt;-dimensional subspace of the full &lt;span class="math"&gt;\(N\)&lt;/span&gt;-dimensional space (more on the &lt;span class="math"&gt;\(L\)&lt;/span&gt;-dimensional subspace just below). Plugging (\ref{projection}) into (\ref{S2}), we&amp;nbsp;obtain
&lt;/p&gt;
&lt;div class="math"&gt;$$
S^2 = \frac{1}{N-L} \sum_i \left ( \epsilon_i - (\mathbb{P} \cdot \vec{\epsilon})_i \right)^2 \equiv \frac{1}{N-L} \left \vert \vec{\epsilon} - \mathbb{P} \cdot \vec{\epsilon} \right \vert^2. \label{S3} \tag{12}
$$&lt;/div&gt;
&lt;p&gt;
This final form gives the result: &lt;span class="math"&gt;\(\vec{\epsilon}\)&lt;/span&gt; is an &lt;span class="math"&gt;\(N\)&lt;/span&gt;-dimensional vector of independent, &lt;span class="math"&gt;\(\mathcal{N}(0, \sigma^2)\)&lt;/span&gt; variables, and (\ref{S3}) shows that &lt;span class="math"&gt;\(S^2\)&lt;/span&gt; is equal to &lt;span class="math"&gt;\(1/(N-L)\)&lt;/span&gt; times the squared length of an &lt;span class="math"&gt;\((N-L)\)&lt;/span&gt;-dimensional projection of it (the part along &lt;span class="math"&gt;\(\mathbb{I} - \mathbb{P}\)&lt;/span&gt;). The length of this projection will on average be &lt;span class="math"&gt;\((N-L) \sigma^2\)&lt;/span&gt;, so that &lt;span class="math"&gt;\(\langle S^2 \rangle = \sigma^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We need to make two final points before moving on. First, because &lt;span class="math"&gt;\(S^2\)&lt;/span&gt; is a sum of &lt;span class="math"&gt;\((N-L)\)&lt;/span&gt; independent &lt;span class="math"&gt;\(\mathcal{N}(0, \sigma^2)\)&lt;/span&gt; random variables, it follows&amp;nbsp;that
&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{(N-L) S^2}{\sigma^2} \sim \chi_{N-L}^2. \tag{13} \label{chi2}
$$&lt;/div&gt;
&lt;p&gt;
Second, &lt;span class="math"&gt;\(S^2\)&lt;/span&gt; is independent of &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt;: We can see this by rearranging (\ref{optimal})&amp;nbsp;as
&lt;/p&gt;
&lt;div class="math"&gt;$$
\vec{\beta} = \vec{\beta}_{true} + (X^T X)^{-1} \cdot X^T \cdot \vec{\epsilon}. \tag{14} \label{beta3}
$$&lt;/div&gt;
&lt;p&gt;
We can left multiply this by &lt;span class="math"&gt;\(X\)&lt;/span&gt; without loss to&amp;nbsp;obtain
&lt;/p&gt;
&lt;div class="math"&gt;$$
X \cdot \vec{\beta} = X \cdot \vec{\beta}_{true} + \mathbb{P} \cdot \vec{\epsilon}, \tag{15} \label{beta2}
$$&lt;/div&gt;
&lt;p&gt;
where we have used (\ref{projection}). Comparing (\ref{beta2}) and (\ref{S3}), we see that the components of &lt;span class="math"&gt;\(\vec{\epsilon}\)&lt;/span&gt; that inform &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt; are in the subspace fixed by &lt;span class="math"&gt;\(\mathbb{P}\)&lt;/span&gt;. This is the space complementary to that informing &lt;span class="math"&gt;\(S^2\)&lt;/span&gt;, implying that &lt;span class="math"&gt;\(S^2\)&lt;/span&gt; is independent of &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;Confidence intervals and hypothesis&amp;nbsp;tests&lt;/h3&gt;
&lt;p&gt;The results above immediately provide us with a method for generating confidence intervals for the individual coefficient estimates (continuing with our Normal error assumption): From (\ref{beta3}), it follows that the coefficients are themselves Normal random variables, with variance given by (\ref{cov}). Further, &lt;span class="math"&gt;\(S^2\)&lt;/span&gt; provides an unbiased estimate for &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt;, proportional to a &lt;span class="math"&gt;\(\chi^2_{N-L}\)&lt;/span&gt; random variable. Combining these results&amp;nbsp;gives
&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\beta_{i,true} - \beta_{i}}{\sqrt{\left(X^T X\right)^{-1}_{ii} S^2}} \sim t_{(N-L)}. \tag{16}
$$&lt;/div&gt;
&lt;p&gt;
That is, the pivot at left follows a Student&amp;#8217;s &lt;span class="math"&gt;\(t\)&lt;/span&gt;-distribution with &lt;span class="math"&gt;\((N-L)\)&lt;/span&gt; degrees of freedom (i.e., it&amp;#8217;s proportional to the ratio of a standard Normal and the square root of a chi-squared variable with that many degrees of freedom). A rearrangement of the above gives the following level &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; confidence interval for the true&amp;nbsp;value:
&lt;/p&gt;
&lt;div class="math"&gt;$$
\beta_i - t_{(N-L), \alpha /2} \sqrt{\left(X^T X \right)^{-1}_{ii} S^2}\leq \beta_{i, true} \leq \beta_i + t_{(N-L), \alpha /2} \sqrt{\left(X^T X \right)^{-1}_{ii} S^2} \tag{17} \label{interval},
$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(\beta_i\)&lt;/span&gt; is obtained from the solution (\ref{optimal}). The interval above can be inverted to generate level &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; hypothesis tests. In particular, we note that a test of the null &amp;#8212; that a particular coefficient is actually zero &amp;#8212; would not be rejected if (\ref{interval}) contains the origin. This approach is often used to test whether some data is consistent with the assertion that a predictor is linearly related to the&amp;nbsp;response.&lt;/p&gt;
&lt;p&gt;[Again, see Cathy&amp;#8217;s follow-up post &lt;a href="http://efavdb.github.io/interpret-linear-regression"&gt;here&lt;/a&gt; for an alternate take on these&amp;nbsp;results.]&lt;/p&gt;
&lt;h3&gt;Bayesian&amp;nbsp;analysis&lt;/h3&gt;
&lt;p&gt;The final thing we wish to do here is consider the problem from a Bayesian perspective, using a flat prior on the &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt;. In this case, assuming a Gaussian form for the &lt;span class="math"&gt;\(\epsilon_i\)&lt;/span&gt; in (\ref{noise})&amp;nbsp;gives
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\tag{18} \label{18}
p(\vec{\beta} \vert \{y_i\}) \propto p(\{y_i\} \vert \vec{\beta}) p(\vec{\beta}) = \mathcal{N} \exp \left [ -\frac{1}{2 \sigma^2}\sum_i \left (y_i - \vec{\beta} \cdot \vec{x}_i \right)^2\right].
\end{align}&lt;/div&gt;
&lt;p&gt;
Notice that this posterior form for &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt; is also Gaussian, and is centered about the solution (\ref{optimal}). Formally, we can write the exponent here in the&amp;nbsp;form
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
-\frac{1}{2 \sigma^2}\sum_i \left (y_i - \vec{\beta} \cdot \vec{x}_i \right)^2 \equiv -\frac{1}{2} \vec{\beta}^T \cdot \frac{1}{\Sigma^2} \cdot \vec{\beta}, \tag{19}
\end{align}&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(\Sigma^2\)&lt;/span&gt; is the covariance matrix for the components of &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt;, as implied by the posterior form (\ref{18}). We can get the components of its inverse by differentiating (\ref{18}) twice. This&amp;nbsp;gives,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
\left ( \frac{1}{\Sigma^2}\right)_{jk} &amp;amp;= \frac{1}{2 \sigma^2} \partial_{\beta_j} \partial_{\beta_k} \sum_i \left (y_i - \vec{\beta} \cdot \vec{x}_i \right)^2 \\
&amp;amp;= -\frac{1}{\sigma^2}\partial_{\beta_j} \sum_i \left (y_i - \vec{\beta} \cdot \vec{x}_i \right) x_{i,k} \\
&amp;amp;= \frac{1}{\sigma^2} \sum_i x_{i,j} x_{i,k} = \frac{1}{\sigma^2} (X^T X)_{jk}. \tag{20}
\end{align}&lt;/div&gt;
&lt;p&gt;
In other words, &lt;span class="math"&gt;\(\Sigma^2 = \sigma^2 (X^T X)^{-1}\)&lt;/span&gt;, in agreement with the classical expression&amp;nbsp;(\ref{cov}).&lt;/p&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;In summary, we&amp;#8217;ve gone through one quick derivation of linear fit solution that minimizes the sum of squared &lt;span class="math"&gt;\(y\)&lt;/span&gt; errors for a given set of data. We&amp;#8217;ve also considered the variance of this solution, showing that the resulting form is closely related to the principal components of the predictor variables sampled. The covariance solution (\ref{cov}) tells us that all parameters have standard deviations that decrease like &lt;span class="math"&gt;\(1/\sqrt{N}\)&lt;/span&gt;, with &lt;span class="math"&gt;\(N\)&lt;/span&gt; the number of samples. However, the predictors that are sampled over wider ranges always have coefficient estimates that more precise. This is due to the fact that sampling over many different values allows one to get a better read on how the underlying function being fit varies with a predictor. Following this, assuming normal errors, we showed that &lt;span class="math"&gt;\(S^2\)&lt;/span&gt; provides an unbiased estimate, chi-squared estimator for the sample variance &amp;#8212; one that is independent of parameter estimates. This allowed us to then write down a confidence interval for the &lt;span class="math"&gt;\(i\)&lt;/span&gt;-th coefficient. The final thing we have shown is that the Bayesian, Gaussian approximation gives similar results: In this approach, the posterior that results is centered about the classical solution, and has a covariance matrix equal to that obtained by classical&amp;nbsp;approach.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Methods, Theory"></category></entry><entry><title>Average queue wait times with random arrivals</title><link href="https://efavdb.com/average-queue-wait-times-with-random-arrivals" rel="alternate"></link><published>2016-04-23T09:51:00-07:00</published><updated>2016-04-23T09:51:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2016-04-23:/average-queue-wait-times-with-random-arrivals</id><summary type="html">&lt;p&gt;Queries ping a certain computer server at random times, on average &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; arriving per second. The server can respond to one per second and those that can&amp;#8217;t be serviced immediately are queued up. What is the average wait time per query? Clearly if &lt;span class="math"&gt;\(\lambda \ll 1\)&lt;/span&gt;, the average wait …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Queries ping a certain computer server at random times, on average &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; arriving per second. The server can respond to one per second and those that can&amp;#8217;t be serviced immediately are queued up. What is the average wait time per query? Clearly if &lt;span class="math"&gt;\(\lambda \ll 1\)&lt;/span&gt;, the average wait time is zero. But if &lt;span class="math"&gt;\(\lambda &amp;gt; 1\)&lt;/span&gt;, the queue grows indefinitely and the answer is infinity! Here, we give a simple derivation of the general result &amp;#8212; (9)&amp;nbsp;below.&lt;/p&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;The mathematics of queue waiting times &amp;#8212; first worked out by &lt;a href="https://en.wikipedia.org/wiki/Erlang_(unit)"&gt;Agner Krarup Erlang&lt;/a&gt; &amp;#8212; is interesting for two reasons. First, as noted above, queues can exhibit phase-transition like behaviors: If the average arrival time is shorter than the average time it takes to serve a customer, the line will grow indefinitely, causing the average wait time to diverge. Second, when the average arrival time is less than the service time, waiting times are governed entirely by fluctuations &amp;#8212; and so can&amp;#8217;t be estimated well using mean-field arguments. For example, in the very low arrival rate limit, the only situation where anyone would ever have to wait at all is that where someone else happens to arrive just before them &amp;#8212; an unlucky, rare&amp;nbsp;event.&lt;/p&gt;
&lt;p&gt;Besides being interesting from a theoretical perspective, an understanding of queue formation phenomena is also critical for many practical applications &amp;#8212; both in computer science and in wider industry settings. Optimal staffing of a queue requires a careful estimate of the expected customer arrival rate. If too many workers are staffed, the average wait time will be nicely low, but workers will largely be idle. Staff too few, and the business could enter into the divergent queue length regime &amp;#8212; certainly resulting in unhappy customers and lost business (or dropped queries). Staffing just the right amount requires a sensitive touch &amp;#8212; and in complex cases, a good understanding of the&amp;nbsp;theory.&lt;/p&gt;
&lt;p&gt;In order to derive the average wait time for queues of different sorts, one often works within the framework of Markov processes. This approach is very general and elementary, but requires a bit of effort to develop the machinery needed get to the end results. Here, we demonstrate an alternative, sometimes faster approach that is based on writing down an integral equation for the wait time distribution. We consider only a simple case &amp;#8212; that where the queue is serviced by only one staff member, the customers arrive at random times via a Poisson process, and each customer requires the same time to service, one&amp;nbsp;second.&lt;/p&gt;
&lt;h3&gt;Integral equation&amp;nbsp;formulation&lt;/h3&gt;
&lt;p&gt;Suppose the &lt;span class="math"&gt;\(N\)&lt;/span&gt;-th customer arrives at time &lt;span class="math"&gt;\(0\)&lt;/span&gt;, and let &lt;span class="math"&gt;\(P(t)\)&lt;/span&gt; be the probability that this customer has to wait a time &lt;span class="math"&gt;\(t\geq 0\)&lt;/span&gt; before being served. This wait time can be written in terms of the arrival and wait times of the previous customer: If this previous customer arrived at time &lt;span class="math"&gt;\(t^{\prime}\)&lt;/span&gt; and has to wait a time &lt;span class="math"&gt;\(w\)&lt;/span&gt; before being served, his service will conclude at time &lt;span class="math"&gt;\(t = t^{\prime} + w + 1\)&lt;/span&gt;. If this is greater than &lt;span class="math"&gt;\(0\)&lt;/span&gt;, the &lt;span class="math"&gt;\(N\)&lt;/span&gt;-th customer will have to wait before being served. In particular, he will wait &lt;span class="math"&gt;\(t\)&lt;/span&gt; if the previous customer waited &lt;span class="math"&gt;\(w = t - t^{\prime} - 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The above considerations allow us to write down an equation satisfied by the wait time distribution. If we let the probability that the previous customer arrived at &lt;span class="math"&gt;\(t^{\prime}\)&lt;/span&gt; be &lt;span class="math"&gt;\(A(t^{\prime})\)&lt;/span&gt;, we have (for &lt;span class="math"&gt;\(t &amp;gt; 0\)&lt;/span&gt;)
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
\tag{1} \label{int_eqn}
P(t) &amp;amp;=&amp;amp; \int_{-\infty}^{0^-} A(t^{\prime}) P(t - t^{\prime} - 1) d t^{\prime}  \\
&amp;amp;=&amp;amp; \int_{-\infty}^{0^-} \lambda e^{\lambda t^{\prime}} P(t - t^{\prime} - 1) d t^{\prime}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Here, in the first equality we&amp;#8217;re simply averaging over the possible arrival times of the previous customer (which had to occur before the &lt;span class="math"&gt;\(N\)&lt;/span&gt;-th, at &lt;span class="math"&gt;\(0\)&lt;/span&gt;), multiplying by the probability &lt;span class="math"&gt;\(P(t - t^{\prime} - 1)\)&lt;/span&gt; that this customer had to wait the amount of time &lt;span class="math"&gt;\(w\)&lt;/span&gt; needed so that the &lt;span class="math"&gt;\(N\)&lt;/span&gt;-th customer will wait &lt;span class="math"&gt;\(t\)&lt;/span&gt;. We also use the symmetry that each customer has the same wait time distribution at steady state. In the second equality, we have plugged in the arrival time distribution appropriate for our Poisson&amp;nbsp;model.&lt;/p&gt;
&lt;p&gt;To proceed, we differentiate both sides of (\ref{int_eqn}) with respect to &lt;span class="math"&gt;\(t\)&lt;/span&gt;,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{2} \label{int2}
P^{\prime}(t) &amp;amp;=&amp;amp; \int_{-\infty}^{0^-} \lambda e^{\lambda t^{\prime}} \frac{d}{dt}P(t - t^{\prime} - 1) d t^{\prime} \\
&amp;amp;=&amp;amp; - \int_{-\infty}^{0^-} \lambda e^{\lambda t^{\prime}} \frac{d}{dt^{\prime}}P(t - t^{\prime} - 1) d t^{\prime}.
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
The second equality follows after noticing that we can switch the parameter being differentiated in the first. Integrating by parts, we&amp;nbsp;obtain
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
P^{\prime}(t) = \lambda \left [P(t) - P(t-1) \right], \tag{3} \label{sol}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
a delay differential equation for the wait time distribution. This could be integrated numerically to get the full solution. However, our interest here is primarily the mean waiting time &amp;#8212; as we show next, it&amp;#8217;s easy to extract this part of the solution&amp;nbsp;analytically.&lt;/p&gt;
&lt;h3&gt;Probability of no wait and the mean wait&amp;nbsp;time&lt;/h3&gt;
&lt;p&gt;We can obtain a series of useful relations by multiplying (\ref{sol}) by powers of &lt;span class="math"&gt;\(t\)&lt;/span&gt; and integrating. The first such expression is obtained by multiplying by &lt;span class="math"&gt;\(t^1\)&lt;/span&gt;. Doing this and integrating its left side, we&amp;nbsp;obtain
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{4} \label{int3}
\int_{0^{+}}^{\infty} P^{\prime}(t) t dt = \left . P(t) t \right |_{0^{+}}^{\infty} - \int_{0^+}^{\infty} P(t) dt = 1 - P(0).
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Similarly integrating its right side, we&amp;nbsp;obtain&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{5} \label{int4}
\lambda \int_{0^{+}}^{\infty} t \left [P(t) - P(t-1) \right] = \lambda [ \overline{t} - \overline{(t + 1)} ] = - \lambda.
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Equating the last two lines, we obtain the probability of no&amp;nbsp;wait,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{6} \label{int5}
P(0) = 1 - \lambda.
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
This shows that when the arrival rate is low, the probability of no wait goes to one &amp;#8212; an intuitively reasonable result. On the other hand, as &lt;span class="math"&gt;\(\lambda \to 1\)&lt;/span&gt;, the probability of no wait approaches zero. In between, the idle time fraction of our staffer (which is equal to the probability of no wait, given a random arrival time) grows linearly, connecting these two&amp;nbsp;limits.&lt;/p&gt;
&lt;p&gt;To obtain an expression for the average wait time, we carry out a similar analysis to that above, but multiply (\ref{sol}) by &lt;span class="math"&gt;\(t^2\)&lt;/span&gt; instead. The integral on left is&amp;nbsp;then
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{7} \label{int6}
\int_{0^{+}}^{\infty} P^{\prime}(t) t^2 dt = \left . P(t) t^2 \right |_{0^{+}}^{\infty} - 2\int_{0^+}^{\infty} P(t) t dt = - 2 \overline{t}.
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Similarly, the integral at right&amp;nbsp;is
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{8} \label{fin_int}
\lambda \int_{0^{+}}^{\infty} t^2 \left [P(t) - P(t-1) \right] &amp;amp;=&amp;amp; \lambda \overline{ t^2} - \overline{ (t + 1)^2} \\
&amp;amp;=&amp;amp; - \lambda (2 \overline{t} +1).
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Equating the last two lines and rearranging gives our solution for the average&amp;nbsp;wait,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{9} \label{fin}
\overline{t} = \frac{\lambda}{2 (1 - \lambda)}.
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
As advertised, this diverges as &lt;span class="math"&gt;\(\lambda \to 1\)&lt;/span&gt;, see illustration in the plot below. It&amp;#8217;s very interesting that even as &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; approaches this extreme limit, the line is still empty a finite fraction of the time &amp;#8212; see (\ref{int5}). Evidently a finite idle time fraction can&amp;#8217;t be avoided, even as one approaches the divergent &lt;span class="math"&gt;\(\lambda = 1\)&lt;/span&gt;&amp;nbsp;limit.&lt;/p&gt;
&lt;p&gt;&lt;img alt="average wait time" src="https://efavdb.com/wp-content/uploads/2016/04/Screen-Shot-2016-04-23-at-5.02.38-PM.png"&gt;&lt;/p&gt;
&lt;h3&gt;Conclusions and&amp;nbsp;extensions&lt;/h3&gt;
&lt;p&gt;To carry this approach further, one could consider the case where the queue feeds &lt;span class="math"&gt;\(k\)&lt;/span&gt; staff, rather than just one. I&amp;#8217;ve made progress on this effort in certain cases, but have been stumped on the general problem. One interesting thing you can intuit about this &lt;span class="math"&gt;\(k\)&lt;/span&gt;-staff version is that one approaches the mean-field analysis as &lt;span class="math"&gt;\(k\to \infty\)&lt;/span&gt; (adding more staff tends to smooth things over, resulting in a diminishing of the importance of the randomness of the arrival times). This means that as &lt;span class="math"&gt;\(k\)&lt;/span&gt; grows, we&amp;#8217;ll have very little average wait time for any &lt;span class="math"&gt;\(\lambda&amp;lt;1\)&lt;/span&gt;, but again divergent wait times for any &lt;span class="math"&gt;\(\lambda \geq 1\)&lt;/span&gt; &amp;#8212; like an infinite step function. Another direction one could pursue is to allow the service times to follow a distribution. Both cases can also be worked out using the Markov approach &amp;#8212; references to such work can be found in the link provided in the&amp;nbsp;introduction.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Statistics"></category></entry><entry><title>Improved Bonferroni correction factors for multiple pairwise comparisons</title><link href="https://efavdb.com/bonferroni-correction-for-multiple-pairwise-comparison-tests" rel="alternate"></link><published>2016-04-10T07:58:00-07:00</published><updated>2016-04-10T07:58:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2016-04-10:/bonferroni-correction-for-multiple-pairwise-comparison-tests</id><summary type="html">&lt;p&gt;A common task in applied statistics is the pairwise comparison of the responses of &lt;span class="math"&gt;\(N\)&lt;/span&gt; treatment groups in some statistical test &amp;#8212; the goal being to decide which pairs exhibit differences that are statistically significant. Now, because there is one comparison being made for each pairing, a naive application of the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A common task in applied statistics is the pairwise comparison of the responses of &lt;span class="math"&gt;\(N\)&lt;/span&gt; treatment groups in some statistical test &amp;#8212; the goal being to decide which pairs exhibit differences that are statistically significant. Now, because there is one comparison being made for each pairing, a naive application of the Bonferroni correction analysis suggests that one should set the individual pairwise test sizes to &lt;span class="math"&gt;\(\alpha_i \to \alpha_f/{N \choose 2}\)&lt;/span&gt; in order to obtain a desired family-wise type 1 error rate of &lt;span class="math"&gt;\(\alpha_f\)&lt;/span&gt;. Indeed, this solution is suggested by many texts. However, implicit in the Bonferroni analysis is the assumption that the comparisons being made are each mutually independent. This is not the case here, and we show that as a consequence the naive approach often returns type 1 error rates far from those desired. We provide adjusted formulas that allow for error-free Bonferroni-like corrections to be&amp;nbsp;made.&lt;/p&gt;
&lt;p&gt;(edit (7/4/2016): After posting this article, I&amp;#8217;ve since found that the method we suggest here is related to / is a generalization of Tukey&amp;#8217;s range test &amp;#8212; see &lt;a href="https://en.wikipedia.org/wiki/Tukey%27s_range_test"&gt;here&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;(edit (6/11/2018): I&amp;#8217;ve added the notebook used below to our Github, &lt;a href="https://github.com/EFavDB/improved_bonferroni"&gt;here&lt;/a&gt;)&lt;/p&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;In this post, we consider a particular kind of statistical test where one examines &lt;span class="math"&gt;\(N\)&lt;/span&gt; different treatment groups, measures some particular response within each, and then decides which of the &lt;span class="math"&gt;\({N \choose 2}\)&lt;/span&gt; pairs appear to exhibit responses that differ significantly. This is called the pairwise comparison problem (or sometimes &amp;#8220;posthoc analysis&amp;#8221;). It comes up in many contexts, and in general it will be of interest whenever one is carrying out a multiple-treatment&amp;nbsp;test.&lt;/p&gt;
&lt;p&gt;Our specific interest here is in identifying the appropriate individual measurement error bars needed to guarantee a given family-wise type 1 error rate, &lt;span class="math"&gt;\(\alpha_f\)&lt;/span&gt;. Briefly, &lt;span class="math"&gt;\(\alpha_f\)&lt;/span&gt; is the probability that we incorrectly make any assertion that two measurements differ significantly when the true effect sizes we&amp;#8217;re trying to measure are actually all the same. This can happen due to the nature of statistical fluctuations. For example, when measuring the heights of &lt;span class="math"&gt;\(N\)&lt;/span&gt; identical objects, measurement error can cause us to incorrectly think that some pair have slightly different heights, even though that&amp;#8217;s not the case. A classical approach to addressing this problem is given by the Bonferroni approximation: If we consider &lt;span class="math"&gt;\(\mathcal{N}\)&lt;/span&gt; independent comparisons, and each has an individual type 1 error rate of &lt;span class="math"&gt;\(\alpha_i,\)&lt;/span&gt; then the family-wise probability of not making any type 1 errors is simply the product of the probabilities that we don&amp;#8217;t make any individual type 1&amp;nbsp;errors,
&lt;/p&gt;
&lt;div class="math"&gt;$$ \tag{1} \label{bon1}
p_f = (1 - \alpha_f) = p_i^{\mathcal{N}} \equiv \left ( 1 - \alpha_i \right)^{\mathcal{N}} \approx 1 - \mathcal{N} \alpha_i.
$$&lt;/div&gt;
&lt;p&gt;
The last equality here is an expansion that holds when &lt;span class="math"&gt;\(p_f\)&lt;/span&gt; is close to &lt;span class="math"&gt;\(1\)&lt;/span&gt;, the limit we usually work in. Rearranging (\ref{bon1}) gives a simple&amp;nbsp;expression,
&lt;/p&gt;
&lt;div class="math"&gt;$$ \tag{2} \label{bon2}
\alpha_i = \frac{\alpha_f}{\mathcal{N}}.
$$&lt;/div&gt;
&lt;p&gt;
This is the (naive) Bonferroni approximation &amp;#8212; it states that one should use individual tests of size &lt;span class="math"&gt;\(\alpha_f / \mathcal{N}\)&lt;/span&gt; in order to obtain a family-wise error rate of &lt;span class="math"&gt;\(\alpha_f\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The reason why we refer to (\ref{bon2}) as the naive Bonferroni approximation is that it doesn&amp;#8217;t actually apply to the problem we consider here. The reason why is that &lt;span class="math"&gt;\(p_f \not = p_i^{\mathcal{N}}\)&lt;/span&gt; in (\ref{bon1}) if the &lt;span class="math"&gt;\(\mathcal{N}\)&lt;/span&gt; comparisons considered are not independent: This is generally the case for our system of &lt;span class="math"&gt;\(\mathcal{N} = {N \choose 2}\)&lt;/span&gt; comparisons, since they are based on an underlying set of measurements having only &lt;span class="math"&gt;\(N\)&lt;/span&gt; degrees of freedom (the object heights, in our example). Despite this obvious issue, the naive approximation is often applied in this context. Here, we explore the nature of the error incurred in such applications, and we find that it is sometimes very significant. We also show that it&amp;#8217;s actually quite simple to apply the principle behind the Bonferroni approximation without error: One need only find a way to evaluate the true &lt;span class="math"&gt;\(p_f\)&lt;/span&gt; for any particular choice of error bars. Inverting this then allows one to identify the error bars needed to obtain the desired &lt;span class="math"&gt;\(p_f\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;General&amp;nbsp;treatment&lt;/h3&gt;
&lt;p&gt;In this section, we derive a formal expression for the type 1 error rate in the pairwise comparison problem. For simplicity, we will assume 1) that the uncertainty in each of our &lt;span class="math"&gt;\(N\)&lt;/span&gt; individual measurements is the same (e.g., the variance in the case of Normal variables), and 2) that our pairwise tests assert that two measurements differ statistically if and only if they are more than &lt;span class="math"&gt;\(k\)&lt;/span&gt; units&amp;nbsp;apart.&lt;/p&gt;
&lt;p&gt;To proceed, we consider the probability that a type 1 error does not occur, &lt;span class="math"&gt;\(p_f\)&lt;/span&gt;. This requires that all &lt;span class="math"&gt;\(N\)&lt;/span&gt; measurements sit within &lt;span class="math"&gt;\(k\)&lt;/span&gt; units of each other. For any set of values satisfying this condition, let the smallest of the set be &lt;span class="math"&gt;\(x\)&lt;/span&gt;. We have &lt;span class="math"&gt;\(N\)&lt;/span&gt; choices for which of the treatments sit as this position. The remaining &lt;span class="math"&gt;\((N-1)\)&lt;/span&gt; values must all be within the region &lt;span class="math"&gt;\((x, x+k)\)&lt;/span&gt;. Because we&amp;#8217;re considering the type 1 error rate, we can assume that each of the independent measurements takes on the same distribution &lt;span class="math"&gt;\(P(x)\)&lt;/span&gt;. These considerations&amp;nbsp;imply
&lt;/p&gt;
&lt;div class="math"&gt;$$ \tag{3} \label{gen}
p_{f} \equiv 1 - \alpha_{f} = N \int_{-\infty}^{\infty} P(x) \left \{\int_x^{x+k} P(y) dy \right \}^{N-1} dx.
$$&lt;/div&gt;
&lt;p&gt;
Equation (\ref{gen}) is our main result. It is nice for a couple of reasons. First, its form implies that when &lt;span class="math"&gt;\(N\)&lt;/span&gt; is large it will scale like &lt;span class="math"&gt;\(a \times p_{1,eff}^N\)&lt;/span&gt;, for some &lt;span class="math"&gt;\(k\)&lt;/span&gt;-dependent numbers &lt;span class="math"&gt;\(a\)&lt;/span&gt; and &lt;span class="math"&gt;\(p_{1,eff}\)&lt;/span&gt;. This is reminiscent of the expression (\ref{bon1}), where &lt;span class="math"&gt;\(p_f\)&lt;/span&gt; took the form &lt;span class="math"&gt;\(p_i^{\mathcal{N}}\)&lt;/span&gt;. Here, we see that the correct value actually scales like some number to the &lt;span class="math"&gt;\(N\)&lt;/span&gt;-th power, not the &lt;span class="math"&gt;\(\mathcal{N}\)&lt;/span&gt;-th. This reflects the fact that we actually only have &lt;span class="math"&gt;\(N\)&lt;/span&gt; independent degrees of freedom here, not &lt;span class="math"&gt;\({N \choose 2}\)&lt;/span&gt;. Second, when the inner integral above can be carried out formally, (\ref{gen}) can be expressed as a single one-dimensional integral. In such cases, the integral can be evaluated numerically for any &lt;span class="math"&gt;\(k\)&lt;/span&gt;, allowing one to conveniently identify the &lt;span class="math"&gt;\(k\)&lt;/span&gt; that returns any specific, desired &lt;span class="math"&gt;\(p_f\)&lt;/span&gt;. We illustrate both points in the next two sections, where we consider Normal and Cauchy variables,&amp;nbsp;respectively.&lt;/p&gt;
&lt;h3&gt;Normally-distributed&amp;nbsp;responses&lt;/h3&gt;
&lt;p&gt;We now consider the case where the individual statistics are each Normally-distributed about zero, and we reject any pair if they are more than &lt;span class="math"&gt;\(k \times \sqrt{2} \sigma\)&lt;/span&gt; apart, with &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt; the variance of the individual statistics. In this case, the inner integral of (\ref{gen}) goes&amp;nbsp;to
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{4} \label{inner_g}
\frac{1}{\sqrt{2 \pi \sigma^2}} \int_x^{x+k \sqrt{2} \sigma} \exp\left [ -\frac{y^2}{2 \sigma^2} \right] dy = \frac{1}{2} \left [\text{erf}(k + \frac{x}{\sqrt{2} \sigma}) - \text{erf}(\frac{x}{\sqrt{2} \sigma})\right].
$$&lt;/div&gt;
&lt;p&gt;
Plugging this into (\ref{gen}), we&amp;nbsp;obtain
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{5} \label{exact_g}
p_f = \int \frac{N e^{-x^2 / 2 \sigma^2}}{\sqrt{2 \pi \sigma^2}} \exp \left ((N-1) \log \frac{1}{2} \left [\text{erf}(k + \frac{x}{\sqrt{2} \sigma}) - \text{erf}(\frac{x}{\sqrt{2} \sigma})\right]\right)dx.
$$&lt;/div&gt;
&lt;p&gt;
This exact expression (\ref{exact_g}) can be used to obtain the &lt;span class="math"&gt;\(k\)&lt;/span&gt; value needed to achieve any desired family-wise type 1error rate. Example solutions obtained in this way are compared to the &lt;span class="math"&gt;\(k\)&lt;/span&gt;-values returned by the naive Bonferroni approach in the table below. The last column &lt;span class="math"&gt;\(p_{f,Bon}\)&lt;/span&gt; shown is the family-wise success rate that you get when you plug in &lt;span class="math"&gt;\(k_{Bon},\)&lt;/span&gt; the naive Bonferroni &lt;span class="math"&gt;\(k\)&lt;/span&gt; value targeting &lt;span class="math"&gt;\(p_{f,exact}\)&lt;/span&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;span class="math"&gt;\(N\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class="math"&gt;\(p_{f,exact}\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class="math"&gt;\(k_{exact}\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class="math"&gt;\(k_{Bon}\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class="math"&gt;\(p_{f, Bon}\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;0.9&lt;/td&gt;
&lt;td&gt;2.29&lt;/td&gt;
&lt;td&gt;2.39&lt;/td&gt;
&lt;td&gt;0.921&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;0.9&lt;/td&gt;
&lt;td&gt;2.78&lt;/td&gt;
&lt;td&gt;2.91&lt;/td&gt;
&lt;td&gt;0.929&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;0.95&lt;/td&gt;
&lt;td&gt;2.57&lt;/td&gt;
&lt;td&gt;2.64&lt;/td&gt;
&lt;td&gt;0.959&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;0.95&lt;/td&gt;
&lt;td&gt;3.03&lt;/td&gt;
&lt;td&gt;3.1&lt;/td&gt;
&lt;td&gt;0.959&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Examining the table shown, you can see that the naive approach is consistently overestimating the &lt;span class="math"&gt;\(k\)&lt;/span&gt; values (error bars) needed to obtain the desired family-wise rates &amp;#8212; but not dramatically so. The reason for the near-accuracy is that two solutions basically scale the same way with &lt;span class="math"&gt;\(N\)&lt;/span&gt;. To see this, one can carry out an asymptotic analysis of (\ref{exact_g}). We skip the details and note only that at large &lt;span class="math"&gt;\(N\)&lt;/span&gt; we&amp;nbsp;have
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{6} \label{asy_g}
p_f \sim \text{erf} \left ( \frac{k}{2}\right)^N
\sim \left (1 - \frac{e^{-k^2 / 4}}{k \sqrt{\pi}/2} \right)^N.
$$&lt;/div&gt;
&lt;p&gt;
This is interesting because the individual pairwise tests have p-values given&amp;nbsp;by
&lt;/p&gt;
&lt;div class="math"&gt;$$ \tag{7} \label{asy_i}
p_i = \int_{-k\sqrt{2}\sigma}^{k\sqrt{2}\sigma} \frac{e^{-x^2 / (4 \sigma^2)}}{\sqrt{4 \pi \sigma^2 }} = \text{erf}(k /\sqrt{2}) \sim 1 - \frac{e^{-k^2/2}}{k \sqrt{\pi/2}}.
$$&lt;/div&gt;
&lt;p&gt;
At large &lt;span class="math"&gt;\(k\)&lt;/span&gt;, this is dominated by the exponential. Comparing with (\ref{asy_g}), this&amp;nbsp;implies
&lt;/p&gt;
&lt;div class="math"&gt;$$ \tag{8} \label{fin_g}
p_f \sim \left (1 - \alpha_i^{1/2} \right)^N \sim 1 - N \alpha_i^{1/2} \equiv 1 - \alpha_f.
$$&lt;/div&gt;
&lt;p&gt;
Fixing &lt;span class="math"&gt;\(\alpha_f\)&lt;/span&gt;, this requires that &lt;span class="math"&gt;\(\alpha_i\)&lt;/span&gt; scale like &lt;span class="math"&gt;\(N^{-2}\)&lt;/span&gt;, the same scaling with &lt;span class="math"&gt;\(N\)&lt;/span&gt; as the naive Bonferroni solution. Thus, in the case of Normal variables, the Bonferroni approximation provides an inexact, but reasonable approximation (nevertheless, we suggest going with the exact approach using (\ref{exact_g}), since it&amp;#8217;s just as easy!). We show in the next section that this is not the case for Cauchy&amp;nbsp;variables.&lt;/p&gt;
&lt;h3&gt;Cauchy-distributed&amp;nbsp;variables&lt;/h3&gt;
&lt;p&gt;We&amp;#8217;ll now consider the case of &lt;span class="math"&gt;\(N\)&lt;/span&gt; independent, identically-distributed Cauchy variables having half widths &lt;span class="math"&gt;\(a\)&lt;/span&gt;,
&lt;/p&gt;
&lt;div class="math"&gt;$$ \tag{9} \label{c_dist}
P(x) = \frac{a}{\pi} \frac{1}{a^2 + x^2}.
$$&lt;/div&gt;
&lt;p&gt;
When we compare any two, we will reject the null if they are more than &lt;span class="math"&gt;\(ka\)&lt;/span&gt; apart. With this choice, the inner integral of (\ref{gen}) is&amp;nbsp;now
&lt;/p&gt;
&lt;div class="math"&gt;$$
\tag{10} \label{inner_c}
\frac{a}{\pi} \int_x^{x+ k a} \frac{1}{a^2 + y^2} dy =\\ \frac{1}{\pi} \left [\tan^{-1}(k + x/a) - \tan^{-1}(x/a) \right].
$$&lt;/div&gt;
&lt;p&gt;
Plugging into into (\ref{gen}) now&amp;nbsp;gives&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{11} \label{exact_c}
p_f = \int \frac{N a/\pi}{a^2 + x^2} e^{(N-1) \log
\frac{1}{\pi} \left [\tan^{-1}(k + x/a) - \tan^{-1}(x/a) \right]
}.
$$&lt;/div&gt;
&lt;p&gt;
This is the analog of (\ref{exact_g}) for Cauchy variables &amp;#8212; it can be used to find the exact &lt;span class="math"&gt;\(k\)&lt;/span&gt; value needed to obtain a given family-wise type 1 error rate. The table below compares the exact values to those returned by the naive Bonferroni analysis [obtained using the fact that the difference between two independent Cauchy variables of width &lt;span class="math"&gt;\(a\)&lt;/span&gt; is itself a Cauchy distributed variable, but with width &lt;span class="math"&gt;\(2a\)&lt;/span&gt;].&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;span class="math"&gt;\(N\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class="math"&gt;\(p_{f,exact}\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class="math"&gt;\(k_{exact}\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class="math"&gt;\(k_{Bon}\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class="math"&gt;\(p_{f, Bon}\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;0.9&lt;/td&gt;
&lt;td&gt;27&lt;/td&gt;
&lt;td&gt;76&lt;/td&gt;
&lt;td&gt;0.965&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;0.9&lt;/td&gt;
&lt;td&gt;55&lt;/td&gt;
&lt;td&gt;350&lt;/td&gt;
&lt;td&gt;0.985&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;0.95&lt;/td&gt;
&lt;td&gt;53&lt;/td&gt;
&lt;td&gt;153&lt;/td&gt;
&lt;td&gt;0.983&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;0.95&lt;/td&gt;
&lt;td&gt;107&lt;/td&gt;
&lt;td&gt;700&lt;/td&gt;
&lt;td&gt;0.993&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In this case, you can see that the naive Bonferroni approximation performs badly. For example, in the last line, it suggests using error bars that are seven times too large for each point estimate. The error gets even worse as &lt;span class="math"&gt;\(N\)&lt;/span&gt; grows: Again, skipping the details, we note that in this limit, (\ref{exact_c}) scales&amp;nbsp;like
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{12} \label{asym_c}
p_f \sim \left [\frac{2}{\pi} \tan^{-1}(k/2) \right]^N.
$$&lt;/div&gt;
&lt;p&gt;
This can be related to the individual &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; values, which are given&amp;nbsp;by
&lt;/p&gt;
&lt;div class="math"&gt;$$ \tag{13} \label{asym2_c}
p_i = \int_{-ka}^{ka} \frac{2 a / \pi}{4 a^2 + x^2}dx = \frac{2}{\pi}\tan^{-1}(k/2).
$$&lt;/div&gt;
&lt;p&gt;
Comparing the last two lines, we&amp;nbsp;obtain
&lt;/p&gt;
&lt;div class="math"&gt;$$ \tag{14} \label{asym3_c}
p_f \equiv 1 - \alpha_f \sim p_i^N \sim 1 - N \alpha_i.
$$&lt;/div&gt;
&lt;p&gt;
Although we&amp;#8217;ve been a bit sloppy with coefficients here, (\ref{asym3_c}) gives the correct leading &lt;span class="math"&gt;\(N\)&lt;/span&gt;-dependence: &lt;span class="math"&gt;\(k_{exact} \sim 1/\alpha_i \propto N\)&lt;/span&gt;. We can see this linear scaling in the table above. This explains why &lt;span class="math"&gt;\(k_{exact}\)&lt;/span&gt; and &lt;span class="math"&gt;\(k_{Bon}\)&lt;/span&gt; &amp;#8212; which scales like &lt;span class="math"&gt;\({N \choose 2} \sim N^2\)&lt;/span&gt; &amp;#8212; differ more and more as &lt;span class="math"&gt;\(N\)&lt;/span&gt; grows. In this case, you should definitely never use the naive approximation, but instead stick to the exact analysis based on&amp;nbsp;(\ref{exact_c}).&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;Some people criticize the Bonferroni correction factor as being too conservative. However, our analysis here suggests that this feeling may be due in part to its occasional improper application. The naive approximation simply does not apply in the case of pairwise comparisons because the &lt;span class="math"&gt;\({N \choose 2}\)&lt;/span&gt; pairs considered are not independent &amp;#8212; there are only &lt;span class="math"&gt;\(N\)&lt;/span&gt; independent degrees of freedom in this problem. Although the naive correction does not apply to the problem of pairwise comparisons, we&amp;#8217;ve shown here that it remains a simple matter to correctly apply the principle behind it: One can easily select any desired family-wise type 1 error rate through an appropriate selection of the individual test sizes &amp;#8212; just use&amp;nbsp;(\ref{gen})!&lt;/p&gt;
&lt;p&gt;We hope you enjoyed this post &amp;#8212; we anticipate writing a bit more on hypothesis testing in the near&amp;nbsp;future.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Statistics"></category></entry><entry><title>Try Caffe pre-installed on a VirtualBox image</title><link href="https://efavdb.com/caffe-virtualbox" rel="alternate"></link><published>2016-03-22T15:02:00-07:00</published><updated>2016-03-22T15:02:00-07:00</updated><author><name>Cathy Yeh</name></author><id>tag:efavdb.com,2016-03-22:/caffe-virtualbox</id><summary type="html">&lt;p&gt;A previous &lt;a href="http://efavdb.github.io/deep-learning-with-jupyter-on-aws"&gt;post&lt;/a&gt; showed beginners how to try out deep learning libraries&amp;nbsp;by&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;using an Amazon Machine Image (&lt;span class="caps"&gt;AMI&lt;/span&gt;) pre-installed with deep learning&amp;nbsp;libraries&lt;/li&gt;
&lt;li&gt;setting up a Jupyter notebook server to play with said&amp;nbsp;libraries&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you have VirtualBox and &lt;a href="https://www.vagrantup.com/"&gt;Vagrant&lt;/a&gt;, you can follow a similar procedure on your own …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A previous &lt;a href="http://efavdb.github.io/deep-learning-with-jupyter-on-aws"&gt;post&lt;/a&gt; showed beginners how to try out deep learning libraries&amp;nbsp;by&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;using an Amazon Machine Image (&lt;span class="caps"&gt;AMI&lt;/span&gt;) pre-installed with deep learning&amp;nbsp;libraries&lt;/li&gt;
&lt;li&gt;setting up a Jupyter notebook server to play with said&amp;nbsp;libraries&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you have VirtualBox and &lt;a href="https://www.vagrantup.com/"&gt;Vagrant&lt;/a&gt;, you can follow a similar procedure on your own computer. The advantage is that you can develop locally, then deploy on an expensive &lt;span class="caps"&gt;AWS&lt;/span&gt; &lt;span class="caps"&gt;EC2&lt;/span&gt; gpu instance when your scripts are&amp;nbsp;ready.&lt;/p&gt;
&lt;p&gt;For example, &lt;a href="http://caffe.berkeleyvision.org/"&gt;Caffe&lt;/a&gt;, the machine vision framework, allows you to seamlessly transition between cpu- and gpu-mode, and is available as a &lt;a href="https://atlas.hashicorp.com/malthejorgensen/boxes/caffe-deeplearning"&gt;vagrant box&lt;/a&gt; running Ubuntu 14.04 (&lt;a href="#virtualization"&gt;**&lt;/a&gt;64-bit), with Caffe&amp;nbsp;pre-installed.&lt;/p&gt;
&lt;p&gt;To add the box, type on the command line:
&lt;code&gt;vagrant box add malthejorgensen/caffe-deeplearning&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;If you don&amp;#8217;t already have VirtualBox and Vagrant installed, you can find instructions online, or look at my &lt;a href="#vagrant_install"&gt;dotfiles&lt;/a&gt; to get an&amp;nbsp;idea.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Gotchas&lt;/h2&gt;
&lt;h3&gt;&lt;span class="caps"&gt;SSH&lt;/span&gt; authentication&amp;nbsp;failure&lt;/h3&gt;
&lt;p&gt;For me, the box had the wrong public key in &lt;code&gt;/home/vagrant/.ssh/authorized_keys file&lt;/code&gt;, which gave me “authentication failure” upon starting up the box with &lt;code&gt;vagrant up&lt;/code&gt;. This was fixed&amp;nbsp;by:&lt;/p&gt;
&lt;p&gt;Manually ssh into the box: &lt;code&gt;vagrant ssh&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Then type (key taken from &lt;a href="https://raw.githubusercontent.com/mitchellh/vagrant/master/keys/vagrant.pub"&gt;here&lt;/a&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEA6NF8iallvQVp22WDkTkyrtvp9eWW6A8YVr+kz4TjGYe7gHzIw+niNltGEFHzD8+v1I2YJ6oXevct1YeS0o9HZyN1Q9qgCgzUFtdOKLv6IedplqoPkcmF0aYet2PkEDo3MlTBckFXPITAMzF8dJSIFo9D8HfdOV0IAdx4O7PtixWKn5y2hMNG0zQPyUecp4pzC6kivAIhyfHilFR61RGL+GPXQ2MWZWFYbAGjyiYJnAmCP3NOTd0jMZEnDkbUvxhMmBYSdETk1rRgm+R4LOzFUGaHqHDLKLX+FIPKcF96hrucXzcWyLbIbEgE98OHlnVYCzRdK8jlqm8tehUc9c9WhQ== vagrant insecure public key&amp;quot;&lt;/span&gt; &amp;gt; ~/.ssh/authorized_keys
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Log out of the box, reload the box with &lt;code&gt;vagrant reload&lt;/code&gt;, and hopefully the ssh authentication error is&amp;nbsp;fixed.&lt;/p&gt;
&lt;h3&gt;Jupyter notebook&amp;nbsp;server&lt;/h3&gt;
&lt;p&gt;By default, the box has a notebook server on port 8003 that starts up from the /home/vagrant/caffe/examples directory, to be used in conjunction with port forwarding set in the Vagrant file:
&lt;code&gt;config.vm.network "forwarded_port", guest: 8003, host: 8003&lt;/code&gt;
With the default setup, go to &lt;code&gt;http://localhost:8003&lt;/code&gt; in your browser to access&amp;nbsp;/home/vagrant/caffe/examples.&lt;/p&gt;
&lt;p&gt;The default server setup limits access to only /home/vagrant/caffe/examples, so I prefer to set up my own configuration of the jupyter notebook server on port 8888 (allowing port forwarding of port 8888 in the Vagrantfile as well) and then start up the server from /home/vagrant, or wherever I&amp;#8217;m working. To do&amp;nbsp;this,&lt;/p&gt;
&lt;p&gt;Log in to the box: &lt;code&gt;vagrant ssh&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Then create the notebook config file &lt;code&gt;~/.jupyter/jupyter_notebook_config.py&lt;/code&gt; containing the following&amp;nbsp;lines:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;c.NotebookApp.ip = &amp;#39;*&amp;#39;&lt;/span&gt;
&lt;span class="err"&gt;c.NotebookApp.open_browser = False&lt;/span&gt;
&lt;span class="err"&gt;c.NotebookApp.port = 8888&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Vagrantfile&lt;/h3&gt;
&lt;p&gt;Here&amp;#8217;s the vagrant file that worked for&amp;nbsp;me:&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;Scripts to &lt;a href="https://github.com/frangipane/.dotfiles/blob/master/install/apt-get.sh"&gt;install Virtualbox&lt;/a&gt; (line 31 and onwards) and &lt;a href="https://github.com/frangipane/.dotfiles/blob/master/install/install-vagrant.sh"&gt;install Vagrant&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;** This is a 64-bit box, so you need to have Intel &lt;span class="caps"&gt;VT&lt;/span&gt;-x enabled in your &lt;span class="caps"&gt;BIOS&lt;/span&gt;.&lt;/p&gt;</content><category term="Tools"></category><category term="Caffe"></category><category term="deep learning"></category><category term="Jupyter"></category><category term="cloud"></category></entry><entry><title>Start deep learning with Jupyter notebooks in the cloud</title><link href="https://efavdb.com/deep-learning-with-jupyter-on-aws" rel="alternate"></link><published>2016-03-10T20:41:00-08:00</published><updated>2016-03-10T20:41:00-08:00</updated><author><name>Cathy Yeh</name></author><id>tag:efavdb.com,2016-03-10:/deep-learning-with-jupyter-on-aws</id><summary type="html">&lt;p&gt;Want a quick and easy way to play around with deep learning libraries? Puny &lt;span class="caps"&gt;GPU&lt;/span&gt; got you down? Thanks to Amazon Web Services (&lt;span class="caps"&gt;AWS&lt;/span&gt;) &amp;#8212; specifically, &lt;span class="caps"&gt;AWS&lt;/span&gt; Elastic Compute Cloud (&lt;span class="caps"&gt;EC2&lt;/span&gt;) &amp;#8212; no data scientist need be left&amp;nbsp;behind.&lt;/p&gt;
&lt;p&gt;Jupyter/IPython notebooks are indispensable tools for learning and tinkering. This post shows …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Want a quick and easy way to play around with deep learning libraries? Puny &lt;span class="caps"&gt;GPU&lt;/span&gt; got you down? Thanks to Amazon Web Services (&lt;span class="caps"&gt;AWS&lt;/span&gt;) &amp;#8212; specifically, &lt;span class="caps"&gt;AWS&lt;/span&gt; Elastic Compute Cloud (&lt;span class="caps"&gt;EC2&lt;/span&gt;) &amp;#8212; no data scientist need be left&amp;nbsp;behind.&lt;/p&gt;
&lt;p&gt;Jupyter/IPython notebooks are indispensable tools for learning and tinkering. This post shows how to set up a public Jupyter notebook server in &lt;span class="caps"&gt;EC2&lt;/span&gt; and then access it remotely through your web browser, just as you would if you were using a notebook launched from your own&amp;nbsp;laptop.&lt;/p&gt;
&lt;p&gt;For a beginner, having to both set up deep learning libraries and navigate the &lt;span class="caps"&gt;AWS&lt;/span&gt; menagerie feels like getting thrown into the deep end when you just want to stick a toe in. You can skip the hassle of setting up deep learning frameworks from scratch by choosing an Amazon Machine Image (&lt;span class="caps"&gt;AMI&lt;/span&gt;) that comes pre-installed with the libraries and their dependencies. (Concerned about costs? &amp;#8212; see the note&lt;a href="#note1"&gt;*&lt;/a&gt; at the bottom of this&amp;nbsp;post.)&lt;/p&gt;
&lt;p&gt;For example, the Stanford class, &lt;a href="http://cs231n.stanford.edu/"&gt;CS231n: Convolutional Neural Networksfor Visual Recognition&lt;/a&gt;, has provided a public &lt;span class="caps"&gt;AMI&lt;/span&gt; with these&amp;nbsp;specs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;cs231n_caffe_torch7_keras_lasagne_v2&lt;/li&gt;
&lt;li&gt;&lt;span class="caps"&gt;AMI&lt;/span&gt; &lt;span class="caps"&gt;ID&lt;/span&gt;: ami-125b2c72 in the us-west-1&amp;nbsp;region&lt;/li&gt;
&lt;li&gt;Use a g2.2xlarge&amp;nbsp;instance.&lt;/li&gt;
&lt;li&gt;Caffe, Torch7, Theano, Keras and Lasagne are pre-installed. Python bindings of caffe are available. It has &lt;span class="caps"&gt;CUDA&lt;/span&gt; 7.5 and CuDNN&amp;nbsp;v3.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you&amp;#8217;re new to &lt;span class="caps"&gt;AWS&lt;/span&gt;, CS231n provides a nice step-by-step &lt;a href="http://cs231n.github.io/aws-tutorial/"&gt;&lt;span class="caps"&gt;AWS&lt;/span&gt; tutorial&lt;/a&gt; with lots of screenshots. We&amp;#8217;re just going to tweak their procedure to enable access to Jupyter/IPython&amp;nbsp;notebooks.&lt;/p&gt;
&lt;p&gt;After you&amp;#8217;re done, you&amp;#8217;ll be able to work through tutorials in notebook format like those provided by caffe in their examples folder, e.g. &lt;a href="http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/00-classification.ipynb"&gt;00-classification.ipynb&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We&amp;#8217;ve written a little bash script &lt;code&gt;jupyter_userdata.sh&lt;/code&gt; to execute Jupyter&amp;#8217;s &lt;a href="http://jupyter-notebook.readthedocs.org/en/latest/public_server.html"&gt;instructions&lt;/a&gt; for setting up a public notebook server, so you don&amp;#8217;t have to manually configure the notebook server every time you want to spin up a new &lt;span class="caps"&gt;AMI&lt;/span&gt;&amp;nbsp;instance.&lt;/p&gt;
&lt;p&gt;For the script to work, Jupyter itself should already be installed &amp;#8212; which it is in the CS231n &lt;span class="caps"&gt;AMI&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;You just have to edit the password in the script. To generate a hashed password, use&amp;nbsp;IPython:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;notebook.auth&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;passwd&lt;/span&gt;
&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;passwd&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;Enter&lt;/span&gt; &lt;span class="n"&gt;password&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="n"&gt;Verify&lt;/span&gt; &lt;span class="n"&gt;password&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="n"&gt;Out&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;sha1:bcd259ccf...&amp;lt;your hashed password here&amp;gt;&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Replace the right hand side of line 24 in the script with the hashed password you just&amp;nbsp;generated.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Then, follow these steps to launch an &lt;span class="caps"&gt;EC2&lt;/span&gt;&amp;nbsp;instance.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt; First, follow the CS231n &lt;a href="http://cs231n.github.io/aws-tutorial/"&gt;&lt;span class="caps"&gt;AWS&lt;/span&gt; tutorial&lt;/a&gt; up until the step &lt;em&gt;&amp;#8220;Choose the instance type &lt;code&gt;g2.2xlarge&lt;/code&gt;, and click on &amp;#8220;Review and Launch&amp;#8221;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Don&amp;#8217;t click on &amp;#8220;Review and Launch&amp;#8221;&amp;nbsp;yet!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2.&lt;/strong&gt; Here&amp;#8217;s where we add a couple extra steps to the tutorial.&lt;a href="#note2"&gt;**&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We&amp;#8217;re going to supply the shell script as &lt;a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html"&gt;user-data&lt;/a&gt;, a way to pass in scripts to automate configurations to your &lt;span class="caps"&gt;AMI&lt;/span&gt;. Instead of clicking on &amp;#8220;Review and Launch&amp;#8221;, click on the gray button in the lower right &amp;#8220;Next: Configure Instance&amp;nbsp;Details&amp;#8221;.&lt;/p&gt;
&lt;p&gt;In the next page, click on the arrowhead next to &amp;#8220;Advanced Details&amp;#8221; to expand its options. Click on the radio button next to &amp;#8220;As text&amp;#8221;, then copy and paste the text from &lt;code&gt;jupyter_userdata.sh&lt;/code&gt; (modified with your password) into the&amp;nbsp;field.&lt;/p&gt;
&lt;p&gt;Warning: if you click on &amp;#8220;As file&amp;#8221; instead and browse to wherever you saved &lt;code&gt;jupyter_userdata.sh&lt;/code&gt;, the file must first be &lt;a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html"&gt;base64-encoded&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2016/03/Step3_Configure-Instance-Details.png"&gt;&lt;img alt="" src="https://efavdb.com/wp-content/uploads/2016/03/Step3_Configure-Instance-Details.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3.&lt;/strong&gt; Next, (skipping steps 4. and 5.) click on the link to &amp;#8220;6. Configure Security Group&amp;#8221; near the top of the page. By default, &lt;span class="caps"&gt;SSH&lt;/span&gt; is enabled, but we need to enable access to the notebook server, whose port we&amp;#8217;ve set as 8888 in the bash&amp;nbsp;script.&lt;/p&gt;
&lt;p&gt;Click on the grey button &amp;#8220;Add Rule&amp;#8221;, then for the new rule, choose Type: Custom &lt;span class="caps"&gt;TCP&lt;/span&gt; Rule; Protocol: &lt;span class="caps"&gt;TCP&lt;/span&gt;; Port Range: 8888; Source:&amp;nbsp;Anywhere.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2016/03/Step6_Configure-Security-Group.png"&gt;&lt;img alt="" src="https://efavdb.com/wp-content/uploads/2016/03/Step6_Configure-Security-Group.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4.&lt;/strong&gt; Now, pick up where you left off in the CS231n tutorial (&amp;#8220;&lt;em&gt;&amp;#8230; click on &amp;#8220;Review and Launch&lt;/em&gt;&amp;#8220;.), which takes you to &amp;#8220;Step 7. Review Instance Launch&amp;#8221;. Complete the&amp;nbsp;tutorial.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Check that the Jupyter notebook server was set up&amp;nbsp;correctly:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;ssh into your instance (see CS231n&amp;nbsp;instructions).&lt;/li&gt;
&lt;li&gt;Navigate to &lt;code&gt;~/caffe/examples&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Start the notebook server using the &lt;code&gt;jupyter notebook&lt;/code&gt; command.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In your web browser, access the notebook server with https://PUBLIC_IP:8888, where PUBLIC_IP is the public &lt;span class="caps"&gt;IP&lt;/span&gt; of your instance, displayed from the instance description on your &lt;span class="caps"&gt;AWS&lt;/span&gt; dashboard. Your browser will warn that your self-signed certificate is insecure or&amp;nbsp;unrecognized.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2016/03/scary-browser-warning.png"&gt;&lt;img alt="" src="https://efavdb.com/wp-content/uploads/2016/03/scary-browser-warning.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;That&amp;#8217;s ok &amp;#8212; click past the warnings, and you should get a sign-in page. Type in your&amp;nbsp;password.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Next, you should see the files and directories in &lt;code&gt;/home/ubuntu/caffe/examples&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Open one of the example notebooks, e.g. &lt;code&gt;00-classification.ipynb&lt;/code&gt;, and try running some cells to make sure everything is&amp;nbsp;working.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Voila! We hope this guide removes some obstacles to getting started. Happy&amp;nbsp;learning!&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;The cost of running a &lt;span class="caps"&gt;GPU&lt;/span&gt; instance is high compared to many other instance types, but still very reasonable if you&amp;#8217;re just tinkering for a few hours on a pre-trained model, not training a whole neural network from&amp;nbsp;scratch.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Check out the &lt;a href="https://aws.amazon.com/ec2/pricing/"&gt;pricing&lt;/a&gt; for an &lt;span class="caps"&gt;EC2&lt;/span&gt; instance in the section &amp;#8220;On-Demand Instance Prices&amp;#8221; and selecting the region of your &lt;span class="caps"&gt;AMI&lt;/span&gt;. At the time of writing, the cost of an on-demand &lt;code&gt;g2.2xlarge&lt;/code&gt; instance in the &lt;span class="caps"&gt;US&lt;/span&gt; West (Northern California) region was $0.7/hour, whereas the price of a &lt;a href="https://aws.amazon.com/ec2/spot/pricing/"&gt;spot&lt;/a&gt; instance (a cheaper alternative which will automatically terminate when the spot pricing exceeds your bid) was&amp;nbsp;$0.3/hour.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; If you followed the CS231n tutorial exactly and forgot to supply user data, you can still use this script. First modify the security configuration of your instance according to step &lt;/strong&gt;3**. Then use the &lt;code&gt;scp&lt;/code&gt; command to copy the script from your local computer to your instance, &lt;code&gt;ssh&lt;/code&gt; into your instance, then execute the script: &lt;code&gt;source jupyter_userdata.sh&lt;/code&gt;. If you need help with using &lt;code&gt;scp&lt;/code&gt;, see &amp;#8220;To use &lt;span class="caps"&gt;SCP&lt;/span&gt; to transfer a file&amp;#8221; in this &lt;a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AccessingInstancesLinux.html"&gt;guide&lt;/a&gt;.&lt;/p&gt;</content><category term="Tools"></category><category term="AWS"></category><category term="bash"></category><category term="cloud"></category><category term="deep learning"></category><category term="Jupyter"></category></entry><entry><title>Dotfiles for peace of mind</title><link href="https://efavdb.com/dotfiles" rel="alternate"></link><published>2016-02-23T12:18:00-08:00</published><updated>2016-02-23T12:18:00-08:00</updated><author><name>Cathy Yeh</name></author><id>tag:efavdb.com,2016-02-23:/dotfiles</id><summary type="html">&lt;p&gt;Reinstalling software and configuring settings on a new computer is a pain. After my latest hard drive failure set the stage for yet another round of download-extract-install and configuration file twiddling, it was time to overhaul my approach. &lt;em&gt;&amp;#8220;Enough is&amp;nbsp;enough!&amp;#8221;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This post walks&amp;nbsp;through&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;how to back up and …&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;p&gt;Reinstalling software and configuring settings on a new computer is a pain. After my latest hard drive failure set the stage for yet another round of download-extract-install and configuration file twiddling, it was time to overhaul my approach. &lt;em&gt;&amp;#8220;Enough is&amp;nbsp;enough!&amp;#8221;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This post walks&amp;nbsp;through&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;how to back up and automate the installation and configuration&amp;nbsp;process&lt;/li&gt;
&lt;li&gt;how to set up a minimal framework for data&amp;nbsp;science&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We&amp;#8217;ll use a &lt;a href="https://github.com/EFavDB/dotfiles"&gt;dotfiles repository&lt;/a&gt; on Github to illustrate both points in&amp;nbsp;parallel.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Dotfiles are named after the configuration files that start with a dot in Unix-based systems. These files are hidden from view in your home directory, but visible with a &lt;code&gt;$ ls -a&lt;/code&gt; command. Some examples are &lt;code&gt;.bashrc&lt;/code&gt; (for configuring the bash shell), &lt;code&gt;.gitconfig&lt;/code&gt; (for configuring git), and &lt;code&gt;.emacs&lt;/code&gt; (for configuring the Emacs text&amp;nbsp;editor).&lt;/p&gt;
&lt;p&gt;Let&amp;#8217;s provide a concrete example of a customization: suppose you have a hard time remembering the syntax to extract a file (&amp;#8220;Is it tar -xvf, -jxvf, or -zxvf?&amp;#8221;). If you&amp;#8217;re using a bash shell, you can define a function, &lt;code&gt;extract()&lt;/code&gt; in your &lt;code&gt;.bashrc&lt;/code&gt; file that makes life a little&amp;nbsp;easier:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;extract() {  &lt;/span&gt;
&lt;span class="err"&gt;if [ -f &amp;quot;$1&amp;quot; ]; then  &lt;/span&gt;
&lt;span class="err"&gt;case &amp;quot;$1&amp;quot; in  &lt;/span&gt;
&lt;span class="err"&gt;*.tar.bz2) tar -jxvf &amp;quot;$1&amp;quot; ;;  &lt;/span&gt;
&lt;span class="err"&gt;*.tar.gz) tar -zxvf &amp;quot;$1&amp;quot; ;;  &lt;/span&gt;
&lt;span class="err"&gt;*.bz2) bunzip2 &amp;quot;$1&amp;quot; ;;  &lt;/span&gt;
&lt;span class="err"&gt;*.dmg) hdiutil mount &amp;quot;$1&amp;quot; ;;  &lt;/span&gt;
&lt;span class="err"&gt;*.gz) gunzip &amp;quot;$1&amp;quot; ;;  &lt;/span&gt;
&lt;span class="err"&gt;*.tar) tar -xvf &amp;quot;$1&amp;quot; ;;  &lt;/span&gt;
&lt;span class="err"&gt;*.tbz2) tar -jxvf &amp;quot;$1&amp;quot; ;;  &lt;/span&gt;
&lt;span class="err"&gt;*.tgz) tar -zxvf &amp;quot;$1&amp;quot; ;;  &lt;/span&gt;
&lt;span class="err"&gt;*.zip) unzip &amp;quot;$1&amp;quot; ;;  &lt;/span&gt;
&lt;span class="err"&gt;*.ZIP) unzip &amp;quot;$1&amp;quot; ;;  &lt;/span&gt;
&lt;span class="err"&gt;*.pax) cat &amp;quot;$1&amp;quot; | pax -r ;;  &lt;/span&gt;
&lt;span class="err"&gt;*.pax.Z) uncompress &amp;quot;$1&amp;quot; --stdout | pax -r ;;  &lt;/span&gt;
&lt;span class="err"&gt;*.Z) uncompress &amp;quot;$1&amp;quot; ;;  &lt;/span&gt;
&lt;span class="err"&gt;*) echo &amp;quot;&amp;#39;$1&amp;#39; cannot be extracted/mounted via extract()&amp;quot; ;;  &lt;/span&gt;
&lt;span class="err"&gt;esac  &lt;/span&gt;
&lt;span class="err"&gt;else  &lt;/span&gt;
&lt;span class="err"&gt;echo &amp;quot;&amp;#39;$1&amp;#39; is not a valid file to extract&amp;quot;  &lt;/span&gt;
&lt;span class="err"&gt;fi  &lt;/span&gt;
&lt;span class="err"&gt;}  &lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So the next time you have to extract a file &lt;code&gt;some_file.tar.bz2&lt;/code&gt;, just type &lt;code&gt;extract some_file.tar.bz2&lt;/code&gt; in bash. (This example was found in this &lt;a href="https://github.com/webpro/dotfiles/blob/master/system/.function_fs#L23"&gt;dotfiles repo&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;The structure of my dotfiles takes after the &lt;a href="https://github.com/webpro/dotfiles"&gt;repo&lt;/a&gt; described by Lars Kappert in the article &lt;a href="https://medium.com/@webprolific/getting-started-with-dotfiles-43c3602fd789#.eis4hwbff"&gt;&amp;#8220;Getting Started With Dotfiles&amp;#8221;&lt;/a&gt;. However, my repo is pared down significantly, with minor modifications for my Linux Mint system (his is &lt;span class="caps"&gt;OS&lt;/span&gt; X) and a focus on packages for data&amp;nbsp;science.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;A framework for data&amp;nbsp;science&lt;/h2&gt;
&lt;p&gt;This starter environment only has a few parts. We need a text editor &amp;#8212; preferably one that can support multiple languages encountered in data science &amp;#8212; and a way to manage scientific/statistical software&amp;nbsp;packages.&lt;/p&gt;
&lt;h3&gt;Components&lt;/h3&gt;
&lt;p&gt;The setup consists&amp;nbsp;of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.gnu.org/software/emacs/"&gt;Emacs&lt;/a&gt; &amp;#8212; a powerful text editor that can be customized to provide an &lt;span class="caps"&gt;IDE&lt;/span&gt;-like experience for both python and R, while providing syntax highlighting for other languages, e.g. markdown, LaTeX, shell, lisp, and so on. (More on customizing Emacs in a future&amp;nbsp;post.)&lt;/li&gt;
&lt;li&gt;&lt;a href="http://conda.pydata.org/docs/"&gt;Conda&lt;/a&gt; &amp;#8212; both a package manager and environment manager. Advantages:&lt;ul&gt;
&lt;li&gt;Packages are easy to install compared to pip, e.g. see a post by the &lt;a href="http://technicaldiscovery.blogspot.com/2013/12/why-i-promote-conda.html"&gt;author of numpy&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Conda is language agnostic in terms of both managing packages and environments for different languages (as opposed to pip/virtualenv/venv). This feature is great if you use both python and&amp;nbsp;R.&lt;/li&gt;
&lt;li&gt;Standard python scientific computing libraries like numpy, scipy, matplotlib, etc. are available in the conda&amp;nbsp;repository.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I use the system package manager (i.e. &lt;code&gt;apt-get install ...&lt;/code&gt;) to install a few packages like git, but otherwise rely on Conda to install Python (multiple versions are okay!), R, and their&amp;nbsp;libraries.&lt;/p&gt;
&lt;p&gt;I like how clean the conda installation feels. Any packages installed by Conda, as well as different versions of Python itself, are neatly organized under the &lt;code&gt;miniconda3&lt;/code&gt; directory in my home directory. In contrast, my previous Linux setups were littered with software installations from various package managers, along with sometimes unsuccessful attempts to compile software from&amp;nbsp;source.&lt;/p&gt;
&lt;h3&gt;Workflow&lt;/h3&gt;
&lt;p&gt;My workflow with Conda follows this helpful &lt;a href="http://stiglerdiet.com/blog/2015/Nov/24/my-python-environment-workflow-with-conda/"&gt;post&lt;/a&gt; by Tim Hopper. Each project gets its own directory and is associated with an environment whose dependencies are specified by an &lt;code&gt;environment.yml&lt;/code&gt; file.&lt;/p&gt;
&lt;p&gt;For example, create a folder for a project, my_proj. Within the project folder, create a bare-bones &lt;code&gt;environment.yml&lt;/code&gt; file to specify a dependency on python 3 and&amp;nbsp;matplotlib:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;my_proj&lt;/span&gt;
&lt;span class="n"&gt;dependencies&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;python&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;
&lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;matplotlib&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Then, to create the conda environment named after that directory, run &lt;code&gt;$ conda env create&lt;/code&gt; inside the my_proj directory. To activate the virtual environment, run &lt;code&gt;$ source activate my_proj&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Activating a conda environment can be further automated with &lt;a href="https://github.com/kennethreitz/autoenv"&gt;autoenv&lt;/a&gt;. Autoenv automatically activates the environment for you when you &lt;code&gt;$ cd&lt;/code&gt; into a project directory. You just need to create a &lt;code&gt;.env&lt;/code&gt; file that contains the command to activate your environment, e.g. &lt;code&gt;source activate my_proj&lt;/code&gt;, under the project&amp;nbsp;directory.&lt;/p&gt;
&lt;p&gt;Tim has written a convenient bash function, &lt;code&gt;conda-env-file&lt;/code&gt; (see &lt;a href="#conda-env-file"&gt;below&lt;/a&gt;), for generating a basic &lt;code&gt;environment.yml&lt;/code&gt; file and &lt;code&gt;.env&lt;/code&gt; file, which I&amp;#8217;ve incorporated into my own dotfiles, along with autoenv. The order of commands that I type in bash then&amp;nbsp;follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;mkdir my_proj&lt;/code&gt; # create project&amp;nbsp;folder&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cd my_proj&lt;/code&gt; # enter project&amp;nbsp;directory&lt;/li&gt;
&lt;li&gt;&lt;code&gt;conda-env-file&lt;/code&gt; # execute homemade function to create environment.yml and&amp;nbsp;.env&lt;/li&gt;
&lt;li&gt;&lt;code&gt;conda env create&lt;/code&gt; # conda creates an environment &amp;#8220;my_proj&amp;#8221; that is named after the project directory (using&amp;nbsp;environment.yml)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cd ..&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cd my_proj&lt;/code&gt; # autoenv automatically activates environment (using the file .env) when you re-enter the&amp;nbsp;directory&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2&gt;The dotfiles&amp;nbsp;layout&lt;/h2&gt;
&lt;p&gt;Below is the layout of the directories and files (generated by the &lt;code&gt;tree&lt;/code&gt; command) in the &lt;a href="https://github.com/EFavDB/dotfiles"&gt;dotfiles repo&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;
&lt;span class="err"&gt;├── install&lt;/span&gt;
&lt;span class="err"&gt;│   ├── apt-get.sh&lt;/span&gt;
&lt;span class="err"&gt;│   ├── conda.sh&lt;/span&gt;
&lt;span class="err"&gt;│   ├── git.sh&lt;/span&gt;
&lt;span class="err"&gt;│   ├── install-emacs.sh&lt;/span&gt;
&lt;span class="err"&gt;│   └── install-miniconda.sh&lt;/span&gt;
&lt;span class="err"&gt;├── install.sh&lt;/span&gt;
&lt;span class="err"&gt;├── runcom&lt;/span&gt;
&lt;span class="err"&gt;│   ├── .bash_profile&lt;/span&gt;
&lt;span class="err"&gt;│   ├── .bashrc&lt;/span&gt;
&lt;span class="err"&gt;│   └── .profile&lt;/span&gt;
&lt;span class="err"&gt;└── system&lt;/span&gt;
&lt;span class="err"&gt;    ├── env&lt;/span&gt;
&lt;span class="err"&gt;    ├── functions&lt;/span&gt;
&lt;span class="err"&gt;    └── path&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Configuration&lt;/h3&gt;
&lt;p&gt;There any number of dotfiles that can be configured (for example, see the collection &lt;a href="http://dotfiles.github.io/"&gt;here&lt;/a&gt;), but this repo only provides customizations for the dotfiles &lt;code&gt;.profile&lt;/code&gt;, &lt;code&gt;.bash_profile&lt;/code&gt;, and &lt;code&gt;.bashrc&lt;/code&gt; &amp;#8212; located in the directory, &lt;code&gt;runcom&lt;/code&gt; (which stands for &amp;#8220;run commands&amp;#8221;) &amp;#8212; that contain commands that are executed at login or during interactive non-login shell sessions. For details about the role of shell initialization dotfiles, see the &lt;a href="#aside"&gt;end&lt;/a&gt; of this&amp;nbsp;post.&lt;/p&gt;
&lt;p&gt;Instead of putting all our customizations in one long, unwieldy dotfile, it&amp;#8217;s helpful to divide them into chunks, which we keep in the subfolder, &lt;code&gt;system&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The files &lt;code&gt;env&lt;/code&gt;, &lt;code&gt;functions&lt;/code&gt;, &lt;code&gt;path&lt;/code&gt; are sourced in a loop by the dotfiles in &lt;code&gt;runcom&lt;/code&gt;. For example, &lt;code&gt;.bashrc&lt;/code&gt; sources &lt;code&gt;functions&lt;/code&gt; and &lt;code&gt;env&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;for DOTFILE in &amp;quot;$DOTFILES_DIR&amp;quot;/system/{functions,env}; do  &lt;/span&gt;
&lt;span class="err"&gt;[ -f &amp;quot;$DOTFILE&amp;quot; ] &amp;amp;&amp;amp; . &amp;quot;$DOTFILE&amp;quot;  &lt;/span&gt;
&lt;span class="err"&gt;done  &lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Let&amp;#8217;s take a look at the configurations in each of these files:&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;env&lt;/strong&gt; - enables autoenv for activating virtual&amp;nbsp;environments&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;[ -f /opt/autoenv/activate.sh ] &amp;amp;&amp;amp; . /opt/autoenv/activate.sh&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;functions&lt;/strong&gt; - defines a custom function, &lt;code&gt;conda-env-file&lt;/code&gt;, that generates an &lt;code&gt;environment.yml&lt;/code&gt; that lists the dependencies for a conda virtual environment, and a one-line file &lt;code&gt;.env&lt;/code&gt; (not to be confused with &lt;code&gt;env&lt;/code&gt; in the previous bullet point) used by autoenv. (In addition to pip and python, I include the dependencies ipython, jedi, and flake8 needed by my Emacs python &lt;span class="caps"&gt;IDE&lt;/span&gt;&amp;nbsp;setup.)  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;function&lt;/span&gt; &lt;span class="n"&gt;conda&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;file&lt;/span&gt; &lt;span class="err"&gt;{&lt;/span&gt;  
&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="k"&gt;Create&lt;/span&gt; &lt;span class="n"&gt;conda&lt;/span&gt; &lt;span class="n"&gt;environment&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;yml&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt; &lt;span class="k"&gt;and&lt;/span&gt; &lt;span class="n"&gt;autoenv&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt;  
&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;based&lt;/span&gt; &lt;span class="k"&gt;on&lt;/span&gt; &lt;span class="n"&gt;directory&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;  
&lt;span class="n"&gt;autoenvfilename&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;.env&amp;#39;&lt;/span&gt;  
&lt;span class="n"&gt;condaenvfilename&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;environment.yml&amp;#39;&lt;/span&gt;  
&lt;span class="n"&gt;foldername&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;basename&lt;/span&gt; &lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;PWD&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="o"&gt;!&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;condaenvfilename&lt;/span&gt; &lt;span class="p"&gt;];&lt;/span&gt; &lt;span class="k"&gt;then&lt;/span&gt;  
&lt;span class="n"&gt;printf&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;name: $foldername\ndependencies:\n- pip\n- python\n- ipython\n- jedi\n- flake8&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;condaenvfilename&lt;/span&gt;  
&lt;span class="n"&gt;echo&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;$condaenvfilename created.&amp;quot;&lt;/span&gt;  
&lt;span class="k"&gt;else&lt;/span&gt;  
&lt;span class="n"&gt;echo&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;$condaenvfilename already exists.&amp;quot;&lt;/span&gt;  
&lt;span class="n"&gt;fi&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="o"&gt;!&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;autoenvfilename&lt;/span&gt; &lt;span class="p"&gt;];&lt;/span&gt; &lt;span class="k"&gt;then&lt;/span&gt;  
&lt;span class="n"&gt;printf&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;source activate $foldername\n&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;autoenvfilename&lt;/span&gt;  
&lt;span class="n"&gt;echo&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;$autoenvfilename created.&amp;quot;&lt;/span&gt;  
&lt;span class="k"&gt;else&lt;/span&gt;  
&lt;span class="n"&gt;echo&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;$autoenvfilename already exists.&amp;quot;&lt;/span&gt;  
&lt;span class="n"&gt;fi&lt;/span&gt;  
&lt;span class="err"&gt;}&lt;/span&gt;  
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;path&lt;/strong&gt; - prepends the miniconda3 path to the &lt;span class="caps"&gt;PATH&lt;/span&gt; environment variable. For example, calls to python will default to the Miniconda3 version (3.5.1 in my case) rather than my system version&amp;nbsp;(2.7).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;export PATH=&amp;quot;/home/$USER/miniconda3/bin:$PATH&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now, we&amp;#8217;re done with configuring the dotfiles in this repo (apart from Emacs, which is treated separately). We just have to create symlinks in our home directory to the dotfiles in &lt;code&gt;runcom&lt;/code&gt;, which is performed by the shell script, &lt;code&gt;install.sh&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;##&lt;/span&gt; &lt;span class="p"&gt;...&lt;/span&gt;

&lt;span class="n"&gt;ln&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;sfv&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;$DOTFILES_DIR/runcom/.bash_profile&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt;  
&lt;span class="n"&gt;ln&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;sfv&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;$DOTFILES_DIR/runcom/.profile&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt;  
&lt;span class="n"&gt;ln&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;sfv&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;$DOTFILES_DIR/runcom/.bashrc&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt;

&lt;span class="o"&gt;##&lt;/span&gt; &lt;span class="p"&gt;...&lt;/span&gt;  
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Installation&lt;/h3&gt;
&lt;p&gt;In addition to setting up dotfiles symlinks, &lt;code&gt;install.sh&lt;/code&gt; automates the installation of all our data science tools via calls to each of the scripts in the &lt;code&gt;install&lt;/code&gt; subfolder. Each script is named after the mechanism of installation (i.e. &lt;code&gt;apt-get&lt;/code&gt;, &lt;code&gt;conda&lt;/code&gt;, &lt;code&gt;git&lt;/code&gt;) or purpose (to install Miniconda and&amp;nbsp;Emacs).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;apt-get.sh&lt;/strong&gt; - installs a handful of programs using the system package manager, including &lt;code&gt;build-essentials&lt;/code&gt;, which is needed to compile programs from source. Also enables source-code repositories (not enabled by default in Linux Mint 17), to be used for compiling emacs from&amp;nbsp;source.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;install-emacs.sh&lt;/strong&gt; - build Emacs 24.4 from source, which is needed for compatibility with the Magit plug-in (git for Emacs). At the time of writing, only Emacs 24.3 was available on the system&amp;nbsp;repo.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;install-miniconda.sh&lt;/strong&gt; - &lt;a href="http://conda.pydata.org/docs/"&gt;miniconda&lt;/a&gt; includes just conda, conda-build, and python. I prefer this lightweight version to the Anaconda version, which comes with more than 150 scientific packages by default. &lt;em&gt;A note from the Miniconda downloads page: &amp;#8220;There are two variants of the installer: Miniconda is Python 2 based and Miniconda3 is Python 3 based&amp;#8230; the choice of which Miniconda is installed only affects the root environment. Regardless of which version of Miniconda you install, you can still install both Python 2.x and Python 3.x environments. The other difference is that the Python 3 version of Miniconda will default to Python 3 when creating new environments and building packages.&amp;#8221; (I chose&amp;nbsp;Miniconda3.)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;conda.sh&lt;/strong&gt; - Use conda to install popular scientific packages for python, R, some popular R packages, and packages for &lt;span class="caps"&gt;IDE&lt;/span&gt; support in&amp;nbsp;Emacs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;git.sh&lt;/strong&gt; - Install &lt;a href="https://github.com/kennethreitz/autoenv"&gt;autoenv&lt;/a&gt; for working with virtual environment directories. Also clone the configurations from my &lt;a href="https://github.com/frangipane/emacs"&gt;Emacs repo&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;The &lt;a href="https://github.com/EFavDB/dotfiles"&gt;dotfiles repo&lt;/a&gt; discussed in this post will remain in this minimal state on GitHub so that it can be easily parsed and built upon. It&amp;#8217;s the most straightforward to adopt if you are on a similar system (Linux Mint or Ubuntu 14.04), as I haven&amp;#8217;t put in checks for &lt;span class="caps"&gt;OSX&lt;/span&gt;. If you don&amp;#8217;t like Emacs, feel free to comment out the relevant lines in &lt;code&gt;install.sh&lt;/code&gt; and &lt;code&gt;install/git.sh&lt;/code&gt;, and replace with your editor of&amp;nbsp;choice.&lt;/p&gt;
&lt;p&gt;Also take a look at other collections of &lt;a href="https://github.com/webpro/awesome-dotfiles"&gt;awesome dotfiles&lt;/a&gt; for nuggets (like the &lt;code&gt;extract()&lt;/code&gt; function) to co-opt. And enjoy the peace of mind that comes with having dotfiles&amp;nbsp;insurance!&lt;/p&gt;
&lt;hr&gt;
&lt;h3&gt;&lt;em&gt;Notes on shell initialization&amp;nbsp;dotfiles&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;The handling of the dotfiles .profile, .bash_profile, and .bashrc is frequently a source of &lt;a href="http://superuser.com/questions/183870/difference-between-bashrc-and-bash-profile"&gt;confusion&lt;/a&gt; that we&amp;#8217;ll try to clear up&amp;nbsp;here.&lt;/p&gt;
&lt;p&gt;For example, .profile and .bash_profile are both recommended for setting environment variables, so what&amp;#8217;s the point of having&amp;nbsp;both?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;.profile&lt;/strong&gt;&lt;br&gt;
.profile is loaded upon login to a Unix system (for most distributions) and is where you should put customizations that apply to your whole session, e.g. environment variable assignments like &lt;code&gt;PATH&lt;/code&gt; that are not specifically related to bash. .profile holds anything that should be (1) available to graphical applications &amp;#8212; like launching a program from a &lt;span class="caps"&gt;GUI&lt;/span&gt; by clicking on an icon or menu &amp;#8212; or (2) to &lt;code&gt;sh&lt;/code&gt;, which is run by graphical &lt;a href="https://wiki.archlinux.org/index.php/display_manager"&gt;display managers&lt;/a&gt; like &lt;span class="caps"&gt;GDM&lt;/span&gt;/LightDM/&lt;span class="caps"&gt;MDM&lt;/span&gt; when your computer boots up in graphics mode (the most common scenario these days). Note, even though the default login shell is bash in Ubuntu, the default system shell that is used during the bootup process in Ubuntu is &lt;a href="https://wiki.ubuntu.com/DashAsBinSh"&gt;dash, not bash&lt;/a&gt;, (&lt;code&gt;readlink -f /bin/sh&lt;/code&gt; outputs &lt;code&gt;/bin/dash&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Let&amp;#8217;s give a concrete example of case (1): the miniconda installer provides a default option to add the miniconda binaries to the search path in .bashrc: &lt;code&gt;export PATH="/home/$USER/miniconda3/bin:$PATH"&lt;/code&gt;. Assuming you&amp;#8217;ve used &lt;code&gt;conda&lt;/code&gt; (not &lt;code&gt;apt-get&lt;/code&gt;) to install python scientific computing libraries and have set the path in .bashrc, if Emacs is launched from an icon on the desktop, then Emacs plugins that depend on those libraries (e.g. &lt;code&gt;ein&lt;/code&gt;, a plugin that integrates IPython with Emacs) will throw an error; since the graphical invocation only loads .profile, the miniconda binaries would not be in the search path. (On the other hand, there would be no problem launching Emacs from the terminal via &lt;code&gt;$ emacs&lt;/code&gt;.) For this reason, it&amp;#8217;s preferable to add the miniconda path in .profile instead of&amp;nbsp;.bashrc.&lt;/p&gt;
&lt;p&gt;For changes to .profile to take effect, you have to log out entirely and then log back&amp;nbsp;in.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;.bash_profile&lt;/strong&gt;&lt;br&gt;
Like .profile, .bash_profile should contain environment variable definitions. I haven&amp;#8217;t yet encountered a situation where a configuration can be set in .bash_profile that can&amp;#8217;t be set in .profile or&amp;nbsp;.bashrc.&lt;/p&gt;
&lt;p&gt;Therefore, my .bash_profile just loads .profile and .bashrc. Some choose to bypass .bash_profile entirely and only have .profile (which bash reads if .bash_profile or .bash_login don&amp;#8217;t exist) and&amp;nbsp;.bashrc.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;.bashrc&lt;/strong&gt;&lt;br&gt;
Definitions of alias, functions, and other settings you&amp;#8217;d want in an interactive command line should be put in .bashrc. .bashrc is sourced by interactive, non-login&amp;nbsp;shells.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;login, non-login, interactive, and non-interactive&amp;nbsp;shells&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To check if you&amp;#8217;re in a login shell, type on the command line &lt;code&gt;echo $0&lt;/code&gt;. If the output is &lt;code&gt;-bash&lt;/code&gt;, then you&amp;#8217;re in a login shell. If the output is &lt;code&gt;bash&lt;/code&gt;, then it&amp;#8217;s not a login shell (see &lt;code&gt;man bash&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Usually, a shell started from a new terminal in a &lt;span class="caps"&gt;GUI&lt;/span&gt; will be an interactive, non-login shell. The notable exception is &lt;span class="caps"&gt;OSX&lt;/span&gt;, whose terminal defaults to starting login shells. Thus, an &lt;span class="caps"&gt;OSX&lt;/span&gt; user may blithely sweep customizations that would ordinarily be placed in .bashrc &amp;#8212; like aliases and functions &amp;#8212; into .bash_profile and not bother with creating a .bashrc at all. However, those settings would not be properly initialized if the terminal default is changed to non-login&amp;nbsp;shells.&lt;/li&gt;
&lt;li&gt;If you ssh in or login on a text console, then you get an interactive, login&amp;nbsp;shell.&lt;/li&gt;
&lt;li&gt;More examples in &lt;a href="http://unix.stackexchange.com/questions/38175/difference-between-login-shell-and-non-login-shell/46856#46856"&gt;this StackExchange thread&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This discussion might seem pedantic since you can often get away with a less careful setup. In my experience, though, what can go wrong will probably go wrong, so best to be&amp;nbsp;proactive.&lt;/p&gt;</content><category term="Programming"></category><category term="programming"></category><category term="tools"></category><category term="bash"></category><category term="conda"></category></entry><entry><title>Independent component analysis</title><link href="https://efavdb.com/independent-component-analysis" rel="alternate"></link><published>2016-02-14T00:00:00-08:00</published><updated>2016-02-14T00:00:00-08:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2016-02-14:/independent-component-analysis</id><summary type="html">&lt;p&gt;Two microphones are placed in a room where two conversations are taking place simultaneously. Given these two recordings, can one &amp;#8220;remix&amp;#8221; them in some prescribed way to isolate the individual conversations? Yes! In this post, we review one simple approach to solving this type of problem, Independent Component Analysis (&lt;span class="caps"&gt;ICA …&lt;/span&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;Two microphones are placed in a room where two conversations are taking place simultaneously. Given these two recordings, can one &amp;#8220;remix&amp;#8221; them in some prescribed way to isolate the individual conversations? Yes! In this post, we review one simple approach to solving this type of problem, Independent Component Analysis (&lt;span class="caps"&gt;ICA&lt;/span&gt;). We share an ipython document implementing &lt;span class="caps"&gt;ICA&lt;/span&gt; and link to a youtube video illustrating its application to audio&amp;nbsp;de-mixing.&lt;/p&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;To formalize the problem posed in the abstract, let two desired conversation signals be represented by &lt;span class="math"&gt;\(c_1(t)\)&lt;/span&gt; and &lt;span class="math"&gt;\(c_2(t)\)&lt;/span&gt;, and two mixed microphone recordings of these by &lt;span class="math"&gt;\(m_1(t)\)&lt;/span&gt; and &lt;span class="math"&gt;\(m_2(t)\)&lt;/span&gt;. We&amp;#8217;ll assume that the latter are both linear combinations of the former,&amp;nbsp;with
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align} \label{mean}
m_1(t) &amp;amp;= a_1 c_1(t) + a_2 c_2(t) \\
m_2(t) &amp;amp;= a_3 c_1(t) + a_4 c_2(t). \label{1} \tag{1}
\end{align}&lt;/div&gt;
&lt;p&gt;
Here, we stress that the &lt;span class="math"&gt;\(a_i\)&lt;/span&gt; coefficients in (\ref{1}) are hidden from us: We only have access to the &lt;span class="math"&gt;\(m_i\)&lt;/span&gt;. Hypothetical illustrations are given in the figure below. Given only these mixed signals, we&amp;#8217;d like to recover the underlying &lt;span class="math"&gt;\(c_i\)&lt;/span&gt; used to construct them (spoiler: a sine wave and a saw-tooth function were used for this&amp;nbsp;figure).&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2016/02/mixed2.jpg"&gt;&lt;img alt="mixed" src="https://efavdb.com/wp-content/uploads/2016/02/mixed2.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Amazingly, it turns out that with the introduction of a modest assumption, a simple solution to our problem can be obtained: We need only assume that the desired &lt;span class="math"&gt;\(c_i\)&lt;/span&gt; are mutually independent&lt;span class="math"&gt;\(^1\)&lt;/span&gt;. This assumption is helpful because it turns out that when two independent signals are added together, the resulting mixture is always &amp;#8220;more Gaussian&amp;#8221; than either of the individual, independent signals (a la the central limit theorem). Seeking linear combinations of the available &lt;span class="math"&gt;\(m_i\)&lt;/span&gt; that locally extremize their non-Gaussian character therefore provides a way to identify the pure, unmixed signals. This approach to solving the problem is called &amp;#8220;Independent Component Analysis&amp;#8221;, or &lt;span class="caps"&gt;ICA&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Here, we demonstrate the principle of &lt;span class="caps"&gt;ICA&lt;/span&gt; through consideration of the audio de-mixing problem. This is a really impressive application. However, one should strive to remember that the algorithm is not a one-trick-pony. &lt;span class="caps"&gt;ICA&lt;/span&gt; is an unsupervised machine learning algorithm of general applicability &amp;#8212; similar in nature, and complementary to, the more familiar &lt;a href="http://efavdb.github.io/principal-component-analysis"&gt;&lt;span class="caps"&gt;PCA&lt;/span&gt;&lt;/a&gt; algorithm. Whereas in &lt;span class="caps"&gt;PCA&lt;/span&gt; we seek the feature-space directions that maximize captured variance, in &lt;span class="caps"&gt;ICA&lt;/span&gt; we seek those directions that maximize the &amp;#8220;interestingness&amp;#8221; of the distribution &amp;#8212; i.e., the non-Gaussian character of the resulting projections. It can be fruitfully applied in many contexts&lt;span class="math"&gt;\(^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We turn now to the problem of audio de-mixing via &lt;span class="caps"&gt;ICA&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;Audio&amp;nbsp;de-mixing&lt;/h3&gt;
&lt;p&gt;In this post, we use the kurtosis of a signal to quantify its degree of &amp;#8220;non-Gaussianess&amp;#8221;. For a given signal &lt;span class="math"&gt;\(x(t)\)&lt;/span&gt;, this is defined&amp;nbsp;as
&lt;/p&gt;
&lt;div class="math"&gt;$$
\kappa(x) \equiv \left \langle \left (x- \langle x \rangle \right)^4 \right \rangle - 3 \left \langle \left (x- \langle x \rangle \right)^2 \right \rangle^2, \label{2} \tag{2}
$$&lt;/div&gt;
&lt;p&gt;
where brackets represent an average over time (or index). It turns out that the kurtosis is always zero for a Gaussian-distributed signal, so (\ref{2}) is a natural choice of score function for measuring deviation away from Gaussian behavior&lt;span class="math"&gt;\(^3\)&lt;/span&gt;. Essentially, it&amp;#8217;s a measure of how flat a distribution is &amp;#8212; with numbers greater (smaller) than 0 corresponding to distributions that are more (less) flat than a&amp;nbsp;Gaussian.&lt;/p&gt;
&lt;p&gt;With (\ref{2}) chosen as our score function, we can now jump right into applying &lt;span class="caps"&gt;ICA&lt;/span&gt;. The code snippet below considers all possible mixtures of two mixed signals &lt;span class="math"&gt;\(m_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(m_2\)&lt;/span&gt;, obtains the resulting signal kurtosis values, and plots the&amp;nbsp;result.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;kurtosis_of_mixture&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;c2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;c1&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;c1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;m1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;c2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;m2&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;

&lt;span class="n"&gt;c_array&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.001&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;k_array&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;kurtosis_of_mixture&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;c_array&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c_array&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;k_array&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2016/02/k3.jpg"&gt;&lt;img alt="k" src="https://efavdb.com/wp-content/uploads/2016/02/k3.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In line &lt;span class="math"&gt;\((3)\)&lt;/span&gt; of the code here, we define the &amp;#8220;remixed&amp;#8221; signal &lt;span class="math"&gt;\(s\)&lt;/span&gt;, which is a linear combination of the two mixed signals &lt;span class="math"&gt;\(m_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(m_2\)&lt;/span&gt;. Note that in line &lt;span class="math"&gt;\((4)\)&lt;/span&gt;, we normalize the signal so that it always has variance &lt;span class="math"&gt;\(1\)&lt;/span&gt; &amp;#8212; this simply eliminates an arbitrary scale factor from the analysis. Similarly in line &lt;span class="math"&gt;\((2)\)&lt;/span&gt;, we specify &lt;span class="math"&gt;\(c_2\)&lt;/span&gt; as a function of &lt;span class="math"&gt;\(c_1\)&lt;/span&gt;, requiring the sum of their squared values to equal one &amp;#8212; this fixes another arbitrary scale&amp;nbsp;factor.&lt;/p&gt;
&lt;p&gt;When we applied the code above to the two signals shown in the introduction, we obtained the top plot at right. This shows the kurtosis of &lt;span class="math"&gt;\(s\)&lt;/span&gt; as a function of &lt;span class="math"&gt;\(c_1\)&lt;/span&gt;, the weight applied to signal &lt;span class="math"&gt;\(m_1\)&lt;/span&gt;. Notice that there are two internal extrema in this plot: a peak near &lt;span class="math"&gt;\(-0.9\)&lt;/span&gt; and a local minimum near &lt;span class="math"&gt;\(-0.7\)&lt;/span&gt;. These are the two &lt;span class="math"&gt;\(c_1\)&lt;/span&gt; weight choices that &lt;span class="caps"&gt;ICA&lt;/span&gt; suggests may relate to the pure, underlying signals we seek. To plot each of these signals, we used code similar to the following (the code shown is just for the&amp;nbsp;maximum)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;index1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;k_array&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k_array&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;c1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;c_array&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;c2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;c1&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;int16&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;c1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;c2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This code finds the index where the kurtosis was maximized, generates the corresponding remix, and plots the result. Applying this, the bottom figure at right popped out. It worked! &amp;#8212; and with just a few lines of code, which makes it seem all the more amazing. In summary, we looked for linear combinations of the &lt;span class="math"&gt;\(m_i\)&lt;/span&gt; shown in the introduction that resulted in a stationary kurtosis &amp;#8212; plotting these combinations, we found that these were precisely the pure signals we sought&lt;span class="math"&gt;\(^4\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A second application to actual audio clips is demoed in our youtube video linked below. The full ipython file utilized in the video can be downloaded on our github page, &lt;a href="https://github.com/EFavDB/ICA"&gt;here&lt;/a&gt;&lt;span class="math"&gt;\(^5\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;We hope this little post has you convinced that &lt;span class="caps"&gt;ICA&lt;/span&gt; is a powerful, yet straightforward algorithm&lt;span class="math"&gt;\(^6\)&lt;/span&gt;. Although we&amp;#8217;ve only discussed one application here, many others can be found online: Analysis of financial data, an idea to use &lt;span class="caps"&gt;ICA&lt;/span&gt; to isolate a desired wifi signal from a crowded frequency band, and the analysis of brain waves &amp;#8212; see discussion in the article mentioned in reference 2 &amp;#8212; etc. In general, the potential application set of &lt;span class="caps"&gt;ICA&lt;/span&gt; may be as large as that for &lt;span class="caps"&gt;PCA&lt;/span&gt;. Next time you need to do some unsupervised learning or data compression, definitely keep it in&amp;nbsp;mind.&lt;/p&gt;
&lt;h3&gt;Footnotes and&amp;nbsp;references&lt;/h3&gt;
&lt;p&gt;[1] Formally, saying that two signals are independent means that the evolution of one conveys no information about that of the&amp;nbsp;other.&lt;/p&gt;
&lt;p&gt;[2] For those interested in further reading on the theory and applications of &lt;span class="caps"&gt;ICA&lt;/span&gt;, we can recommend the review article by Hyvärinen and Oja &amp;#8212; &amp;#8220;Independent Component Analysis: Algorithms and Applications&amp;#8221; &amp;#8212; available for free&amp;nbsp;online.&lt;/p&gt;
&lt;p&gt;[3] Other metrics can also be used in the application of &lt;span class="caps"&gt;ICA&lt;/span&gt;. The kurtosis is easy to evaluate and is also well-motivated because of the fact that it is zero for any Gaussian. However, there are non-Gaussian distributions that also have zero kurtosis. Further, as seen in our linked youtube video, peaks in the kurtosis plot need not always correspond to the pure signals. A much more rigorous approach is to use the mutual information of the signals as your score. This function is zero if and only if you&amp;#8217;ve found a projection that results in a fully independent set of signals. Thus, it will always work. The problem with this choice is that it is much harder to evaluate &amp;#8212; thus, simpler scores are often used in practice, even though they aren&amp;#8217;t necessarily rigorously correct. The article mentioned in footnote 2 gives a good review of some other popular score function&amp;nbsp;choices.&lt;/p&gt;
&lt;p&gt;[4] In general, symmetry arguments imply that the pure signals will correspond to local extrema in the kurtosis landscape. This works because the kurtosis of &lt;span class="math"&gt;\(x_1 + a x_2\)&lt;/span&gt; is the same as that of &lt;span class="math"&gt;\(x_1 - a x_2\)&lt;/span&gt;, when &lt;span class="math"&gt;\(x_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(x_2\)&lt;/span&gt; are independent. To complete the argument, you need to consider coefficient expansions in the mixed space. The fact that the pure signals can sometimes sit at kurtosis local minima doesn&amp;#8217;t really jive with the intuitive argument about mixtures being more Gaussian &amp;#8212; but that was a vague statement anyways. A rigorous, alternative introduction could be made via mutual information, as mentioned in the previous&amp;nbsp;footnote.&lt;/p&gt;
&lt;p&gt;[5] To run the script, you&amp;#8217;ll need ipython installed, as well as the python packages: scipy, numpy, matplotlib, and pyaudio &amp;#8212; see instructions for the latter &lt;a href="https://people.csail.mit.edu/hubert/pyaudio/"&gt;here&lt;/a&gt;. The pip install command for pyaudio didn&amp;#8217;t work for me on my mac, but the following line did:
&lt;code&gt;pip install --global-option='build_ext' --global-option='-I/usr/local/include' --global-option='-L/usr/local/lib' pyaudio&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;[6] Of course, things get a bit more complicated when you have a large number of signals. However, fast, simple algorithms have been found to carry this out even in high dimensions. See the reference in footnote 2 for&amp;nbsp;discussion.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Methods, Theory"></category></entry><entry><title>Maximum-likelihood asymptotics</title><link href="https://efavdb.com/maximum-likelihood-asymptotics" rel="alternate"></link><published>2015-12-30T00:01:00-08:00</published><updated>2015-12-30T00:01:00-08:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2015-12-30:/maximum-likelihood-asymptotics</id><summary type="html">&lt;p&gt;In this post, we review two facts about maximum-likelihood estimators: 1) They are consistent, meaning that they converge to the correct values given a large number of samples, &lt;span class="math"&gt;\(N\)&lt;/span&gt;, and 2) They satisfy the &lt;a href="http://efavdb.github.io/multivariate-cramer-rao-bound"&gt;Cramer-Rao&lt;/a&gt; lower bound for unbiased parameter estimates in this same limit &amp;#8212; that is, they have the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In this post, we review two facts about maximum-likelihood estimators: 1) They are consistent, meaning that they converge to the correct values given a large number of samples, &lt;span class="math"&gt;\(N\)&lt;/span&gt;, and 2) They satisfy the &lt;a href="http://efavdb.github.io/multivariate-cramer-rao-bound"&gt;Cramer-Rao&lt;/a&gt; lower bound for unbiased parameter estimates in this same limit &amp;#8212; that is, they have the lowest possible variance of any unbiased estimator, in the &lt;span class="math"&gt;\(N\gg 1\)&lt;/span&gt;&amp;nbsp;limit.&lt;/p&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;We begin with a simple example maximum-likelihood inference problem: Suppose one has obtained &lt;span class="math"&gt;\(N\)&lt;/span&gt; independent samples &lt;span class="math"&gt;\(\{x_1, x_2, \ldots, x_N\}\)&lt;/span&gt; from a Gaussian distribution of unknown mean &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; and variance &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt;. In order to obtain a maximum-likelihood estimate for these parameters, one asks which &lt;span class="math"&gt;\(\hat{\mu}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\hat{\sigma}^2\)&lt;/span&gt; would be most likely to generate the samples observed. To find these, we first write down the probability of observing the samples, given our model. This is&amp;nbsp;simply
&lt;/p&gt;
&lt;div class="math"&gt;$$
P(\{x_1, x_2, \ldots, x_N\} \vert \mu, \sigma^2) =\\ \exp\left [ \sum_{i=1}^N \left (-\frac{1}{2} \log (2 \pi \sigma^2) -\frac{1}{2 \sigma^2} (x_i - \mu)^2\right ) \right ]. \tag{1} \label{1}
$$&lt;/div&gt;
&lt;p&gt;
To obtain the maximum-likelihood estimates, we maximize (\ref{1}): Setting its derivatives with respect to &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; and &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt; to zero and solving&amp;nbsp;gives
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\label{mean}
\hat{\mu} &amp;amp;= \frac{1}{N} \sum_i x_i \tag{2} \\
\hat{\sigma}^2 &amp;amp;= \frac{1}{N} \sum_i (x_i - \hat{\mu})^2. \tag{3} \label{varhat}
\end{align}&lt;/div&gt;
&lt;p&gt;
These are mean and variance values that would be most likely to generate our observation set &lt;span class="math"&gt;\(\{x_i\}\)&lt;/span&gt;. Our solutions show that they are both functions of the random observation set. Because of this, &lt;span class="math"&gt;\(\hat{\mu}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\hat{\sigma}^2\)&lt;/span&gt; are themselves random variables, changing with each sample set that happens to be observed. Their distributions can be characterized by their mean values, variances,&amp;nbsp;etc.&lt;/p&gt;
&lt;p&gt;The average squared error of a parameter estimator is determined entirely by its bias and variance &amp;#8212; see eq (2) of &lt;a href="http://efavdb.github.io/bayesian-linear-regression"&gt;prior post&lt;/a&gt;. Now, one can show that the &lt;span class="math"&gt;\(\hat{\mu}\)&lt;/span&gt; estimate of (\ref{mean}) is unbiased, but this is not the case for the variance estimator (\ref{varhat}) &amp;#8212; one should (famously) divide by &lt;span class="math"&gt;\(N-1\)&lt;/span&gt; instead of &lt;span class="math"&gt;\(N\)&lt;/span&gt; here to obtain an unbiased estimator&lt;span class="math"&gt;\(^1\)&lt;/span&gt;. This shows that maximum-likelihood estimators need not be unbiased. Why then are they so popular? One reason is that these estimators are guaranteed to be unbiased when &lt;span class="math"&gt;\(N\)&lt;/span&gt;, the sample size, is large. Further, in this same limit, these estimators achieve the minimum possible variance for any unbiased parameter estimate &amp;#8212; as set by the fundamental &lt;a href="http://efavdb.github.io/multivariate-cramer-rao-bound"&gt;Cramer-Rao&lt;/a&gt; bound. The purpose of this post is to review simple proofs of these latter two facts about maximum-likelihood estimators&lt;span class="math"&gt;\(^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;Consistency&lt;/h3&gt;
&lt;p&gt;Let &lt;span class="math"&gt;\(P(x \vert \theta^*)\)&lt;/span&gt; be some distribution characterized by a parameter &lt;span class="math"&gt;\(\theta^*\)&lt;/span&gt; that is unknown. We will show that the maximum-likelihood estimator converges to &lt;span class="math"&gt;\(\theta^*\)&lt;/span&gt; when &lt;span class="math"&gt;\(N\)&lt;/span&gt; is large: As in (\ref{1}), the maximum-likelihood solution is that &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; that&amp;nbsp;maximizes
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{4} \label{4}
J \equiv \frac{1}{N}\sum_{i=1}^N \log P(x_i \vert \theta),
$$&lt;/div&gt;
&lt;p&gt;
where the &lt;span class="math"&gt;\(\{x_i\}\)&lt;/span&gt; are the independent samples taken from &lt;span class="math"&gt;\(P(x \vert \theta^*)\)&lt;/span&gt;. By the law of large numbers, when &lt;span class="math"&gt;\(N\)&lt;/span&gt; is large, this average over the samples converges to its population mean. In other&amp;nbsp;words,
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{5}
\lim_{N \to \infty}J \rightarrow \int_x P(x \vert \theta^*) \log P(x \vert \theta) dx.
$$&lt;/div&gt;
&lt;p&gt;
We will show that &lt;span class="math"&gt;\(\theta^*\)&lt;/span&gt; is the &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; value that maximizes the above. We can do this directly,&amp;nbsp;writing
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
J(\theta) - J(\theta^*) &amp;amp; = \int_x P(x \vert \theta^*) \log \left ( \frac{P(x \vert \theta) }{P(x \vert \theta^*)}\right) \\
&amp;amp; \leq \int_x P(x \vert \theta^*) \left ( \frac{P(x \vert \theta) }{P(x \vert \theta^*)} - 1 \right) \\
&amp;amp; = \int_x P(x \vert \theta) - P(x \vert \theta^*) = 1 - 1 = 0. \tag{6} \label{6}
\end{align}
$$&lt;/div&gt;
&lt;p&gt;
Here, we have used &lt;span class="math"&gt;\(\log t \leq t-1\)&lt;/span&gt; in the second line. Rearranging the above shows that &lt;span class="math"&gt;\(J(\theta^*) \geq J(\theta)\)&lt;/span&gt; for all &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; &amp;#8212; when &lt;span class="math"&gt;\(N \gg 1\)&lt;/span&gt;, meaning that &lt;span class="math"&gt;\(J\)&lt;/span&gt; is maximized at &lt;span class="math"&gt;\(\theta^*\)&lt;/span&gt;. That is, the maximum-likelihood estimator &lt;span class="math"&gt;\(\hat{\theta} \to \theta^*\)&lt;/span&gt; in this limit&lt;span class="math"&gt;\(^3\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;Optimal&amp;nbsp;variance&lt;/h3&gt;
&lt;p&gt;To derive the variance of a general maximum-likelihood estimator, we will see how its average value changes upon introduction of a small Bayesian prior, &lt;span class="math"&gt;\(P(\theta) \sim \exp(\Lambda \theta)\)&lt;/span&gt;. The trick will be to evaluate the change in two separate ways &amp;#8212; this takes a few lines, but is quite straightforward. In the first approach, we do a direct maximization: The quantity to be maximized is&amp;nbsp;now
&lt;/p&gt;
&lt;div class="math"&gt;$$ \label{7}
J = \sum_{i=1}^N \log P(x_i \vert \theta) + \Lambda \theta. \tag{7}
$$&lt;/div&gt;
&lt;p&gt;
Because we take &lt;span class="math"&gt;\(\Lambda\)&lt;/span&gt; small, we can use a Taylor expansion to find the new solution,&amp;nbsp;writing
&lt;/p&gt;
&lt;div class="math"&gt;$$ \label{8}
\hat{\theta} = \theta^* + \theta_1 \Lambda + O(\Lambda^2). \tag{8}
$$&lt;/div&gt;
&lt;p&gt;
Setting the derivative of (\ref{7}) to zero, with &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; given by its value in (\ref{8}), we&amp;nbsp;obtain
&lt;/p&gt;
&lt;div class="math"&gt;$$
\sum_{i=1}^N \partial_{\theta} \left . \log P(x_i \vert \theta) \right \vert_{\theta^*} + \\ \sum_{i=1}^N \partial_{\theta}^2 \left . \log P(x_i \vert \theta) \right \vert_{\theta^*} \times \theta_1 \Lambda + \Lambda + O(\Lambda^2) = 0. \tag{9} \label{9}
$$&lt;/div&gt;
&lt;p&gt;
The first term here goes to zero at large &lt;span class="math"&gt;\(N\)&lt;/span&gt;, as above. Setting the terms at &lt;span class="math"&gt;\(O(\Lambda^1)\)&lt;/span&gt; to zero&amp;nbsp;gives
&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta_1 = - \frac{1}{ \sum_{i=1}^N \partial_{\theta}^2 \left . \log P(x_i \vert \theta) \right \vert_{\theta^*} }. \tag{10} \label{10}
$$&lt;/div&gt;
&lt;p&gt;
Plugging this back into (\ref{8}) gives the first order correction to &lt;span class="math"&gt;\(\hat{\theta}\)&lt;/span&gt; due to the perturbation. Next, as an alternative approach, we evaluate the change in &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; by maximizing the &lt;span class="math"&gt;\(P(\theta)\)&lt;/span&gt; distribution, expanding about its unperturbed global maximum, &lt;span class="math"&gt;\(\theta^*\)&lt;/span&gt;: We write,&amp;nbsp;formally,
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{11} \label{11}
P(\theta) = e^{ - a_0 - a_2 (\theta - \theta^*)^2 - a_3 (\theta - \theta^*)^3 + \ldots + \Lambda \theta}.
$$&lt;/div&gt;
&lt;p&gt;
Differentiating to maximize (\ref{11}), and again assuming a solution of form (\ref{8}), we&amp;nbsp;obtain
&lt;/p&gt;
&lt;div class="math"&gt;$$\label{12} \tag{12}
-2 a_2 \times \theta_1 \Lambda + \Lambda + O(\Lambda^2) = 0 \ \ \to \ \ \theta_1 = \frac{1}{2 a_2}.
$$&lt;/div&gt;
&lt;p&gt;
We now require consistency between our two approaches, equating (\ref{10}) and (\ref{12}). This gives an expression for &lt;span class="math"&gt;\(a_2\)&lt;/span&gt;. Plugging this back into (\ref{11}) then gives (for the unperturbed&amp;nbsp;distribution)
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{13} \label{13}
P(\theta) = \mathcal{N} \exp \left [ N \frac{ \langle \partial_{\theta}^2 \left . \log P(x, \theta) \right \vert_{\theta^*} \rangle }{2} (\theta - \theta^*)^2 + \ldots \right].
$$&lt;/div&gt;
&lt;p&gt;
Using this Gaussian approximation&lt;span class="math"&gt;\(^4\)&lt;/span&gt;, we can now read off the large &lt;span class="math"&gt;\(N\)&lt;/span&gt; variance of &lt;span class="math"&gt;\(\hat{\theta}\)&lt;/span&gt;&amp;nbsp;as
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{14} \label{14}
var(\hat{\theta}) = - \frac{1}{N} \times \frac{1}{\langle \partial_{\theta}^2 \left . \log P(x, \theta) \right \vert_{\theta^*} \rangle }.
$$&lt;/div&gt;
&lt;p&gt;
This is the lowest possible value for any unbiased estimator, as set by the Cramer-Rao bound. The proof shows that maximum-likelihood estimators always saturate this bound, in the large &lt;span class="math"&gt;\(N\)&lt;/span&gt; limit &amp;#8212; a remarkable result. We discuss the intuitive meaning of the Cramer-Rao bound in a &lt;a href="http://efavdb.github.io/multivariate-cramer-rao-bound"&gt;prior post&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Footnotes&lt;/h3&gt;
&lt;p&gt;[1] To see that (\ref{varhat}) is biased, we just need to evaluate the average of &lt;span class="math"&gt;\(\sum_i (x_i - \hat{\mu})^2\)&lt;/span&gt;. This&amp;nbsp;is&lt;/p&gt;
&lt;div class="math"&gt;$$
\overline{\sum_i x_i^2 - 2 \sum_{i,j} \frac{x_i x_j}{N} + \sum_{i,j,k} \frac{x_j x_k}{N^2}} = N \overline{x^2} - (N-1) \overline{x}^2 - \overline{x^2} \\
= (N-1) \left ( \overline{x^2} - \overline{x}^2 \right) \equiv (N-1) \sigma^2.
$$&lt;/div&gt;
&lt;p&gt;
Dividing through by &lt;span class="math"&gt;\(N\)&lt;/span&gt;, we see that &lt;span class="math"&gt;\(\overline{\hat{\sigma}^2} = \left(\frac{N-1}{N}\right)\sigma^2\)&lt;/span&gt;. The deviation from the true variance &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt; goes to zero at large &lt;span class="math"&gt;\(N\)&lt;/span&gt;, but is non-zero for any finite &lt;span class="math"&gt;\(N\)&lt;/span&gt;: The estimator is biased, but the bias goes to zero at large &lt;span class="math"&gt;\(N\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;[2] The consistency proof is taken from lecture notes by D. Panchenko, see &lt;a href="http://ocw.mit.edu/courses/mathematics/18-443-statistics-for-applications-fall-2006/lecture-notes/lecture3.pdf"&gt;here&lt;/a&gt;. Professor Panchenko is quite famous for having proven the correctness of the Parisi ansatz in replica theory. Our variance proof is original &amp;#8212; please let us know if you have seen it elsewhere. Note that it can also be easily extended to derive the covariance matrix of a set of maximum-likelihood estimators that are jointly distributed &amp;#8212; we cover only the scalar case here, for&amp;nbsp;simplicity.&lt;/p&gt;
&lt;p&gt;[3] The proof here actually only shows that there is no &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; that gives larger likelihood than &lt;span class="math"&gt;\(\theta^*\)&lt;/span&gt; in the large &lt;span class="math"&gt;\(N\)&lt;/span&gt; limit. However, for some problems, it is possible that more than one &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; maximizes the likelihood. A trivial example is given by the case where the distribution is actually only a function of &lt;span class="math"&gt;\((\theta - \theta_0)^2\)&lt;/span&gt;. In this case, both values &lt;span class="math"&gt;\(\theta_0 \pm (\theta^* - \theta_0)\)&lt;/span&gt; will necessarily maximize the&amp;nbsp;likelihood.&lt;/p&gt;
&lt;p&gt;[4] It&amp;#8217;s a simple matter to carry this analysis further, including the cubic and higher order terms in the expansion (\ref{11}). These lead to correction terms for (\ref{14}), smaller in magnitude than that given there. These terms become important when &lt;span class="math"&gt;\(N\)&lt;/span&gt; decreases in&amp;nbsp;magnitude.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Statistics"></category></entry><entry><title>Principal component analysis</title><link href="https://efavdb.com/principal-component-analysis" rel="alternate"></link><published>2015-12-05T22:22:00-08:00</published><updated>2015-12-05T22:22:00-08:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2015-12-05:/principal-component-analysis</id><summary type="html">&lt;p&gt;We review the two essentials of principal component analysis (&amp;#8220;&lt;span class="caps"&gt;PCA&lt;/span&gt;&amp;#8221;): 1) The principal components of a set of data points are the eigenvectors of the correlation matrix of these points in feature space. 2) Projecting the data onto the subspace spanned by the first &lt;span class="math"&gt;\(k\)&lt;/span&gt; of these &amp;#8212; listed in descending …&lt;/p&gt;</summary><content type="html">&lt;p&gt;We review the two essentials of principal component analysis (&amp;#8220;&lt;span class="caps"&gt;PCA&lt;/span&gt;&amp;#8221;): 1) The principal components of a set of data points are the eigenvectors of the correlation matrix of these points in feature space. 2) Projecting the data onto the subspace spanned by the first &lt;span class="math"&gt;\(k\)&lt;/span&gt; of these &amp;#8212; listed in descending eigenvalue order &amp;#8212; provides the best possible &lt;span class="math"&gt;\(k\)&lt;/span&gt;-dimensional approximation to the data, in the sense of captured&amp;nbsp;variance.&lt;/p&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;One way to introduce principal component analysis is to consider the problem of least-squares fits: Consider, for example, the figure shown below. To fit a line to this data, one might attempt to minimize the squared &lt;span class="math"&gt;\(y\)&lt;/span&gt; residuals (actual minus fit &lt;span class="math"&gt;\(y\)&lt;/span&gt; values). However, if the &lt;span class="math"&gt;\(x\)&lt;/span&gt; and &lt;span class="math"&gt;\(y\)&lt;/span&gt; values are considered to be on an equal footing, this &lt;span class="math"&gt;\(y\)&lt;/span&gt;-centric approach is not quite appropriate. A natural alternative is to attempt instead to find the line that minimizes the &lt;em&gt;total squared projection error&lt;/em&gt;: If &lt;span class="math"&gt;\((x_i, y_i)\)&lt;/span&gt; is a data point, and &lt;span class="math"&gt;\((\hat{x}_i, \hat{y}_i)\)&lt;/span&gt; is the point closest to it on the regression line (aka, its &amp;#8220;projection&amp;#8221; onto the line), we attempt to&amp;nbsp;minimize
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{1} \label{score}
J = \sum_i (x_i - \hat{x}_i)^2 + (y_i - \hat{y}_i)^2.
$$&lt;/div&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/12/projection.png"&gt;&lt;img alt="margin around decision boundary" src="https://efavdb.com/wp-content/uploads/2015/12/projection.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The summands here are illustrated in the figure: The dotted lines shown are the projection errors for each data point relative to the red line. The minimizer of (\ref{score}) is the line that minimizes the sum of the squares of these&amp;nbsp;values.&lt;/p&gt;
&lt;p&gt;Generalizing the above problem, one could ask which &lt;span class="math"&gt;\(k\)&lt;/span&gt;-dimensional hyperplane passes closest to a set of data points in &lt;span class="math"&gt;\(N\)&lt;/span&gt;-dimensions. Being able to identify the solution to this problem can be very helpful when &lt;span class="math"&gt;\(N \gg 1\)&lt;/span&gt;. The reason is that in high-dimensional, applied problems, many features are often highly-correlated. When this occurs, projection of the data onto a &lt;span class="math"&gt;\(k\)&lt;/span&gt;-dimensional subspace can often result in a great reduction in memory usage (one moves from needing to store &lt;span class="math"&gt;\(N\)&lt;/span&gt; values for each data point to &lt;span class="math"&gt;\(k\)&lt;/span&gt;) with minimal loss of information (if the points are all near the plane, replacing them by their projections causes little distortion). Projection onto subspaces can also be very helpful for visualization: For example, plots of &lt;span class="math"&gt;\(N\)&lt;/span&gt;-dimensional data projected onto a best two-dimensional subspace can allow one to get a feel for a dataset&amp;#8217;s&amp;nbsp;shape.&lt;/p&gt;
&lt;p&gt;At first glance, the task of actually minimizing (\ref{score}) may appear daunting. However, it turns out this can be done easily using linear algebra. One need only carry out the following three&amp;nbsp;steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Preprocessing: If appropriate, shift features and normalize so that they all have mean &lt;span class="math"&gt;\(\mu = 0\)&lt;/span&gt; and variance &lt;span class="math"&gt;\(\sigma^2 = 1\)&lt;/span&gt;. The latter, scaling step is needed to account for differences in units, which may cause variations along one component to look artificially large or small relative to those along other components (eg, one raw component might be a measure in centimeters, and another in&amp;nbsp;kilometers).&lt;/li&gt;
&lt;li&gt;Compute the covariance matrix. Assuming there are &lt;span class="math"&gt;\(m\)&lt;/span&gt; data points, the &lt;span class="math"&gt;\(i\)&lt;/span&gt;, &lt;span class="math"&gt;\(j\)&lt;/span&gt; component of this matrix is given by:
    &lt;div class="math"&gt;$$\tag{2} \label{2} \Sigma_{ij}^2 = \frac{1}{m}\sum_{l=1}^m \langle (f_{l,i} - \mu_i) (f_{l,j} - \mu_j) \rangle\\ = \langle x_i \vert \left (\frac{1}{m} \sum_{l=1}^m \vert \delta f_l \rangle \langle \delta f_l \vert \right) \vert x_j \rangle.$$&lt;/div&gt;
    Note that, at right, we are using bracket notation for vectors. We make further use of this below &amp;#8212; see footnote [1] at bottom for review. We&amp;#8217;ve also written &lt;span class="math"&gt;\(\vert \delta f_l \rangle\)&lt;/span&gt; for the vector &lt;span class="math"&gt;\(\vert f_l \rangle - \sum_{i = 1}^n \mu_i \vert x_i \rangle\)&lt;/span&gt; &amp;#8212; the vector &lt;span class="math"&gt;\(\vert f_l \rangle\)&lt;/span&gt; with the dataset&amp;#8217;s centroid subtracted&amp;nbsp;out.&lt;/li&gt;
&lt;li&gt;Project all feature vectors onto the &lt;span class="math"&gt;\(k\)&lt;/span&gt; eigenvectors &lt;span class="math"&gt;\(\{\vert v_j \rangle\)&lt;/span&gt;, &lt;span class="math"&gt;\(j = 1 ,2 \ldots, k\}\)&lt;/span&gt; of &lt;span class="math"&gt;\(\Sigma^2\)&lt;/span&gt; that have the largest eigenvalues &lt;span class="math"&gt;\(\lambda_j\)&lt;/span&gt;, writing
    &lt;div class="math"&gt;$$\tag{3} \label{3}
    \vert \delta f_i \rangle \approx \sum_{j = 1}^k \langle v_j \vert \delta f_i \rangle \times \vert v_j\rangle.
    $$&lt;/div&gt;
    The term &lt;span class="math"&gt;\(\langle v_j \vert \delta f_i \rangle\)&lt;/span&gt; above is the coefficient of the vector &lt;span class="math"&gt;\(\vert \delta f_i \rangle\)&lt;/span&gt; along the &lt;span class="math"&gt;\(j\)&lt;/span&gt;-th principal component. If we set &lt;span class="math"&gt;\(k = N\)&lt;/span&gt; above, (\ref{3}) becomes an identity. However, when &lt;span class="math"&gt;\(k &amp;lt; N\)&lt;/span&gt;, the expression represents an approximation only, with the vector &lt;span class="math"&gt;\(\vert \delta f_i \rangle\)&lt;/span&gt; approximated by its projection into the subspace spanned by the largest &lt;span class="math"&gt;\(k\)&lt;/span&gt; principal&amp;nbsp;components.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The steps above are all that are needed to carry out a &lt;span class="caps"&gt;PCA&lt;/span&gt; analysis/compression of any dataset. We show in the next section why this solution will indeed provide the &lt;span class="math"&gt;\(k\)&lt;/span&gt;-dimensional hyperplane resulting in minimal dataset projection&amp;nbsp;error.&lt;/p&gt;
&lt;h3&gt;Mathematics of &lt;span class="caps"&gt;PCA&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;To understand &lt;span class="caps"&gt;PCA&lt;/span&gt;, we proceed in three&amp;nbsp;steps.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Significance of a partial trace: Let &lt;span class="math"&gt;\(\{\textbf{u}_j \}\)&lt;/span&gt; be some arbitrary orthonormal basis set that spans our full &lt;span class="math"&gt;\(N\)&lt;/span&gt;-dimensional space, and consider the sum
    &lt;div class="math"&gt;\begin{align}\tag{4} \label{4}
    \sum_{j = 1}^k \Sigma^2_{jj} = \frac{1}{m} \sum_{i,j} \langle u_j \vert \delta f_i \rangle \langle \delta f_i \vert u_j \rangle\\ = \frac{1}{m} \sum_{i,j} \langle \delta f_i \vert u_j \rangle \langle u_j \vert \delta f_i \rangle\\ \equiv \frac{1}{m} \sum_{i} \langle \delta f_i \vert P \vert \delta f_i \rangle.
    \end{align}&lt;/div&gt;
    To obtain the first equality here, we have used &lt;span class="math"&gt;\(\Sigma^2 = \frac{1}{m} \sum_{i} \vert \delta f_i \rangle \langle \delta f_i \vert\)&lt;/span&gt;, which follows from (\ref{2}). To obtain the last, we have written &lt;span class="math"&gt;\(P\)&lt;/span&gt; for the projection operator onto the space spanned by the first &lt;span class="math"&gt;\(k\)&lt;/span&gt; &lt;span class="math"&gt;\(\{\textbf{u}_j \}\)&lt;/span&gt;. Note that this last equality implies that the partial trace is equal to the average squared length of the projected feature vectors &amp;#8212; that is, the variance of the projected data&amp;nbsp;set.&lt;/li&gt;
&lt;li&gt;Notice that the projection error is simply given by the total trace of &lt;span class="math"&gt;\(\Sigma^2\)&lt;/span&gt;, minus the partial trace above. Thus, minimization of the projection error is equivalent to maximization of the projected variance,&amp;nbsp;(\ref{4}).&lt;/li&gt;
&lt;li&gt;We now consider which basis maximizes (\ref{4}). To do that, we decompose the &lt;span class="math"&gt;\(\{\textbf{u}_i \}\)&lt;/span&gt; in terms of the eigenvectors &lt;span class="math"&gt;\(\{\textbf{v}_j\}\)&lt;/span&gt; of &lt;span class="math"&gt;\(\Sigma^2\)&lt;/span&gt;, writing
    &lt;div class="math"&gt;\begin{align} \tag{5} \label{5}
    \vert u_i \rangle = \sum_j \vert v_j \rangle \langle v_j \vert u_i \rangle \equiv \sum_j u_{ij} \vert v_j \rangle.
    \end{align}&lt;/div&gt;
    Here, we&amp;#8217;ve inserted the identity in the &lt;span class="math"&gt;\(\{v_j\}\)&lt;/span&gt; basis, and written &lt;span class="math"&gt;\( \langle v_j \vert u_i \rangle \equiv u_{ij}\)&lt;/span&gt;. With these definitions, the partial trace becomes
    &lt;div class="math"&gt;\begin{align}\tag{6} \label{6}
    \sum_{i=1}^k \langle u_i \vert \Sigma^2 \vert u_i \rangle = \sum_{i,j,l} u_{ij}u_{il} \langle v_j \vert \Sigma^2 \vert v_l \rangle \\= \sum_{i=1}^k\sum_{j} u_{ij}^2 \lambda_j.
    \end{align}&lt;/div&gt;
    The last equality here follows from the fact that the &lt;span class="math"&gt;\(\{\textbf{v}_i\}\)&lt;/span&gt; are the eigenvectors of &lt;span class="math"&gt;\(\Sigma^2\)&lt;/span&gt; &amp;#8212; we have also used the fact that they are orthonormal, which follows from the fact that &lt;span class="math"&gt;\(\Sigma^2\)&lt;/span&gt; is a real, symmetric matrix. The sum (\ref{6}) is proportional to a weighted average of the eigenvalues of &lt;span class="math"&gt;\(\Sigma^2\)&lt;/span&gt;. We have a total mass of &lt;span class="math"&gt;\(k\)&lt;/span&gt; to spread out amongst the &lt;span class="math"&gt;\(N\)&lt;/span&gt; eigenvalues. The maximum mass that can sit on any one eigenvalue is one. This follows since &lt;span class="math"&gt;\(\sum_{i = 1}^k u_{ij}^2 \leq \sum_{i = 1}^N u_{ij}^2 =1\)&lt;/span&gt;, the latter equality following from the fact that &lt;span class="math"&gt;\( \sum_{i = 1}^N u_{ij}^2\)&lt;/span&gt; is an expression for the squared length of &lt;span class="math"&gt;\(\vert v_j\rangle\)&lt;/span&gt; in the &lt;span class="math"&gt;\(\{u_i\}\)&lt;/span&gt; basis. Under these constraints, the maximum possible average one can get in (\ref{6}) occurs when all the mass sits on the largest &lt;span class="math"&gt;\(k\)&lt;/span&gt; eigenvalues, with each of these eigenvalues weighted with mass one. This condition occurs if and only if the first &lt;span class="math"&gt;\(k\)&lt;/span&gt; &lt;span class="math"&gt;\(\{\textbf{u}_i\}\)&lt;/span&gt; span the same space as that spanned by the first &lt;span class="math"&gt;\(k\)&lt;/span&gt; &lt;span class="math"&gt;\(\{\textbf{v}_j\}\)&lt;/span&gt; &amp;#8212; those with the &lt;span class="math"&gt;\(k\)&lt;/span&gt; largest&amp;nbsp;eigenvalues.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;That&amp;#8217;s it for the mathematics of &lt;span class="caps"&gt;PCA&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;Footnotes&lt;/h3&gt;
&lt;p&gt;[1] &lt;em&gt;Review of bracket notation&lt;/em&gt;: &lt;span class="math"&gt;\(\vert x \rangle\)&lt;/span&gt; represents a regular vector, &lt;span class="math"&gt;\(\langle x \vert\)&lt;/span&gt; is its transpose, and &lt;span class="math"&gt;\(\langle y \vert x \rangle\)&lt;/span&gt; represents the dot product of &lt;span class="math"&gt;\(x\)&lt;/span&gt; and &lt;span class="math"&gt;\(y\)&lt;/span&gt;. So, for example, when the term in parentheses at the right side of (\ref{2}) acts on the vector &lt;span class="math"&gt;\(\vert x_j \rangle\)&lt;/span&gt; to its right, you get &lt;span class="math"&gt;\( \frac{1}{m} \sum_{k=1}^m \vert \delta f_k \rangle \left (\langle \delta f_k \vert x_j \rangle\right).\)&lt;/span&gt; Here, &lt;span class="math"&gt;\( \left (\langle \delta f_k \vert x_j \rangle\right)\)&lt;/span&gt; is a dot product, a scalar, and &lt;span class="math"&gt;\(\vert \delta f_k \rangle\)&lt;/span&gt; is a vector. The result is thus a weighted sum of vectors. In other words, the bracketed term (\ref{2}) acts on a vector and returns a linear combination of other vectors. That means it is a matrix, as is any other object of form &lt;span class="math"&gt;\(\sum_i \vert a_i \rangle \langle b_i \vert\)&lt;/span&gt;. A special, important example is the identity matrix: Given any complete, orthonormal set of vectors &lt;span class="math"&gt;\(\{x_j\}\)&lt;/span&gt;, the identity matrix &lt;span class="math"&gt;\(I\)&lt;/span&gt; can be written as &lt;span class="math"&gt;\(I = \sum_i \vert x_i \rangle \langle x_i \vert\)&lt;/span&gt;. This identity is often used to make a change of&amp;nbsp;basis.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Methods, Theory"></category></entry><entry><title>NBA 2015-16!!!</title><link href="https://efavdb.com/nba-2015-16" rel="alternate"></link><published>2015-10-25T16:30:00-07:00</published><updated>2015-10-25T16:30:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2015-10-25:/nba-2015-16</id><summary type="html">&lt;p&gt;&lt;span class="caps"&gt;NBA&lt;/span&gt; is back this Tuesday! The &lt;a href="http://efavdb.github.io/nba-dash"&gt;dashboard&lt;/a&gt; and &lt;a href="http://efavdb.github.io/weekly-nba-predictions"&gt;weekly predictions&lt;/a&gt; are now live*, once again. These will each be updated daily, with game winner predictions, hypothetical who-would-beat-whom daily matchup predictions, and more. For a discussion on how we make our predictions, see our first &lt;a href="http://efavdb.github.io/nba-learner-2013-14-warmup"&gt;post&lt;/a&gt; on this topic. Note that …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;span class="caps"&gt;NBA&lt;/span&gt; is back this Tuesday! The &lt;a href="http://efavdb.github.io/nba-dash"&gt;dashboard&lt;/a&gt; and &lt;a href="http://efavdb.github.io/weekly-nba-predictions"&gt;weekly predictions&lt;/a&gt; are now live*, once again. These will each be updated daily, with game winner predictions, hypothetical who-would-beat-whom daily matchup predictions, and more. For a discussion on how we make our predictions, see our first &lt;a href="http://efavdb.github.io/nba-learner-2013-14-warmup"&gt;post&lt;/a&gt; on this topic. Note that our approach does not make use of any bookie predictions (unlike many other sites), and so provide an independent look on the&amp;nbsp;game.&lt;/p&gt;
&lt;p&gt;This season, we hope to crack 70%&amp;nbsp;accuracy!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Note that we have left up last season&amp;#8217;s completed games results, for review purposes. Once every team has played one game, we&amp;#8217;ll switch it over to the current season&amp;#8217;s&amp;nbsp;results.&lt;/li&gt;
&lt;/ul&gt;</content><category term="NBA prediction project"></category></entry><entry><title>Support Vector Machines for classification</title><link href="https://efavdb.com/svm-classification" rel="alternate"></link><published>2015-10-22T14:24:00-07:00</published><updated>2015-10-22T14:24:00-07:00</updated><author><name>Cathy Yeh</name></author><id>tag:efavdb.com,2015-10-22:/svm-classification</id><summary type="html">&lt;p&gt;To whet your appetite for support vector machines, here&amp;#8217;s a quote from machine learning researcher Andrew&amp;nbsp;Ng:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“SVMs are among the best (and many believe are indeed the best) ‘off-the-shelf’ supervised learning&amp;nbsp;algorithms.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="http://commons.wikimedia.org/wiki/File%3AAndrew_Ng.png" title="See page for author [CC BY 3.0 us (http://creativecommons.org/licenses/by/3.0/us/deed.en)], via Wikimedia Commons"&gt;&lt;img alt="Andrew Ng" src="//upload.wikimedia.org/wikipedia/commons/5/5c/Andrew_Ng.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Professor Ng covers SVMs in his excellent &lt;a href="https://www.coursera.org/learn/machine-learning"&gt;Machine Learning &lt;span class="caps"&gt;MOOC&lt;/span&gt;&lt;/a&gt;, a gateway for many into the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;To whet your appetite for support vector machines, here&amp;#8217;s a quote from machine learning researcher Andrew&amp;nbsp;Ng:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“SVMs are among the best (and many believe are indeed the best) ‘off-the-shelf’ supervised learning&amp;nbsp;algorithms.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="http://commons.wikimedia.org/wiki/File%3AAndrew_Ng.png" title="See page for author [CC BY 3.0 us (http://creativecommons.org/licenses/by/3.0/us/deed.en)], via Wikimedia Commons"&gt;&lt;img alt="Andrew Ng" src="//upload.wikimedia.org/wikipedia/commons/5/5c/Andrew_Ng.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Professor Ng covers SVMs in his excellent &lt;a href="https://www.coursera.org/learn/machine-learning"&gt;Machine Learning &lt;span class="caps"&gt;MOOC&lt;/span&gt;&lt;/a&gt;, a gateway for many into the realm of data science, but leaves out some details, motivating us to put together some notes here to answer the&amp;nbsp;question:&lt;/p&gt;
&lt;p&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;What are the &lt;em&gt;support vectors&lt;/em&gt; in support vector&amp;nbsp;machines?&amp;#8221;&lt;/p&gt;
&lt;p&gt;We also provide python (https://github.com/EFavDB/svm-classification/blob/master/svm.ipynb) using scikit-learn&amp;#8217;s svm module to fit a binary classification problem using a custom kernel, along with code to generate the (awesome!) interactive plots in Part&amp;nbsp;3.&lt;/p&gt;
&lt;p&gt;This post consists of three&amp;nbsp;sections:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Part 1 sets up the problem from a geometric point of view and then shows how it can be framed as an optimization&amp;nbsp;problem.&lt;/li&gt;
&lt;li&gt;Part 2 transforms the optimization problem and uncovers the support vectors in the&amp;nbsp;process.&lt;/li&gt;
&lt;li&gt;Part 3 discusses how kernels can be used to separate non-linearly separable&amp;nbsp;data.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2&gt;Part 1: Defining the&amp;nbsp;margin&lt;/h2&gt;
&lt;h3&gt;Maximizing the&amp;nbsp;margin&lt;/h3&gt;
&lt;p&gt;The figure below is a binary classification problem (points labeled &lt;span class="math"&gt;\(y_i = \pm 1\)&lt;/span&gt;) that is linearly&amp;nbsp;separable.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/05/binaryclass_2d.png"&gt;&lt;img alt="" src="https://efavdb.com/wp-content/uploads/2015/05/binaryclass_2d.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;There are many possible decision boundaries that would perfectly separate the two classes, but an &lt;span class="caps"&gt;SVM&lt;/span&gt; will choose the line in 2-d (or “hyperplane”, more generally) that maximizes the margin around the&amp;nbsp;boundary.&lt;/p&gt;
&lt;p&gt;Intuitively, we can be very confident about the labels of points that fall far from the boundary, but we’re less confident about points near the&amp;nbsp;boundary.
 
 &lt;/p&gt;
&lt;h3&gt;Formulating the margin with&amp;nbsp;geometry&lt;/h3&gt;
&lt;p&gt;Any point &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt; lying on the separating hyperplane satisfies:
&lt;span class="math"&gt;\(\boldsymbol{w} \cdot \boldsymbol{x} + b = 0\)&lt;/span&gt;
&lt;span class="math"&gt;\(\boldsymbol{w}\)&lt;/span&gt; is the vector normal to the plane, and &lt;span class="math"&gt;\(b\)&lt;/span&gt; is a constant that describes how much the plane is shifted relative to the origin.  The distance of the plane from the origin is &lt;span class="math"&gt;\(b / \| \boldsymbol{w} \|\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/05/binaryclass_margin.png"&gt;&lt;img alt="" src="https://efavdb.com/wp-content/uploads/2015/05/binaryclass_margin.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Now draw parallel planes on either side of the decision boundary, so we have what looks like a road, with the decision boundary as the median, and the additional planes as gutters.  The margin, i.e. the width of the road, is (&lt;span class="math"&gt;\(d_+ + d_-\)&lt;/span&gt;) and is restricted by the data points closest to the boundary, which lie on the&amp;nbsp;gutters.&lt;/p&gt;
&lt;p&gt;The half-spaces bounded by the planes on the gutters&amp;nbsp;are:&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\boldsymbol{w} \cdot \boldsymbol{x} + b \geq +a\)&lt;/span&gt;, for &lt;span class="math"&gt;\(y_i =&amp;nbsp;+1\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\boldsymbol{w} \cdot \boldsymbol{x} + b \leq -a\)&lt;/span&gt;, for &lt;span class="math"&gt;\(y_i =&amp;nbsp;-1\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;These two conditions can be put more&amp;nbsp;succinctly:&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(y_i (\boldsymbol{w} \cdot \boldsymbol{x} + b) \geq a, \forall \;&amp;nbsp;i\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Some arithmetic leads to the equation for the&amp;nbsp;margin:&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(d_+ + d_- = 2a / \| \boldsymbol{w}&amp;nbsp;\|\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Without loss of generality, we can set &lt;span class="math"&gt;\(a=1\)&lt;/span&gt;, since it only sets the scale (units) of &lt;span class="math"&gt;\(b\)&lt;/span&gt; and &lt;span class="math"&gt;\(\boldsymbol{w}\)&lt;/span&gt;.  So to maximize the margin, we have to maximize &lt;span class="math"&gt;\(1 / \| \boldsymbol{w} \|\)&lt;/span&gt;.  However, this is an unpleasant (non-convex) objective function.  Instead we minimize &lt;span class="math"&gt;\(\| \boldsymbol{w}\|^2\)&lt;/span&gt;, which is&amp;nbsp;convex.&lt;/p&gt;
&lt;h3&gt;The optimization&amp;nbsp;problem&lt;/h3&gt;
&lt;p&gt;Maximizing the margin boils down to a constrained optimization problem: minimize some quantity &lt;span class="math"&gt;\(f(w)\)&lt;/span&gt;, subject to constraints &lt;span class="math"&gt;\(g(w,b)\)&lt;/span&gt;.  This optimization problem is particularly nice because it is convex; the objective &lt;span class="math"&gt;\(\| \boldsymbol{w}\|^2\)&lt;/span&gt; is convex, as are the constraints, which are&amp;nbsp;linear.&lt;/p&gt;
&lt;p&gt;In other words, we are faced with a &lt;a href="http://en.wikipedia.org/wiki/Quadratic_programming"&gt;quadratic programming&lt;/a&gt; problem.  The standard format of the optimization problem for the separable case&amp;nbsp;is&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{1} \label{problem}
\begin{align}
\text{minimize} \quad &amp;amp; f(w) \equiv (1/2) \| \boldsymbol{w}\|^2 \
\text{subject to} \quad &amp;amp; g(w,b) \equiv -y_i (\boldsymbol{w} \cdot \boldsymbol{x} + b) + 1 \leq 0, \; i = 1 \ldots m
\end{align}
$$&lt;/div&gt;
&lt;p&gt;Before we address how to solve this optimization problem in Part 2, let&amp;#8217;s first consider the case when data is&amp;nbsp;non-separable.&lt;/p&gt;
&lt;h3&gt;Soft margin &lt;span class="caps"&gt;SVM&lt;/span&gt;: the non-separable problem and&amp;nbsp;regularization&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/05/softmargin.png"&gt;&lt;img alt="" src="https://efavdb.com/wp-content/uploads/2015/05/softmargin.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For non-separable data, we relax the constraints in (\ref{problem}) while penalizing misclassified points via a cost parameter &lt;span class="math"&gt;\(C\)&lt;/span&gt; and slack variables &lt;span class="math"&gt;\(\xi_i\)&lt;/span&gt; that define the amount by which data points are on the wrong side of the&amp;nbsp;margin.&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{2} \label{regularization}
\begin{align}
\text{minimize} \quad &amp;amp; (1/2) \| \boldsymbol{w}\|^2 + C \sum_i^m \xi_i \\
\text{subject to} \quad &amp;amp; y_i (\boldsymbol{w} \cdot \boldsymbol{x} + b) \geq 1 - \xi_i, \; i = 1 \ldots m \\
&amp;amp; \xi_i \geq 0, \quad i = 1 \ldots m
\end{align}
$$&lt;/div&gt;
&lt;p&gt;
A large penalty &amp;#8212; large &lt;span class="math"&gt;\(C\)&lt;/span&gt; &amp;#8212; for misclassifications will lead to learning a lower bias, higher variance &lt;span class="caps"&gt;SVM&lt;/span&gt;, and vice versa for small &lt;span class="math"&gt;\(C\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The soft margin is used in practice; even in the separable case, it can be desirable to allow tradeoffs between the size of the margin and number of misclassifications. Outliers can skew the decision boundary learned by (\ref{problem}) towards a model with small margins + perfect classification, in contrast to a possibly more robust model learned by (\ref{regularization}) with large margins + some misclassified&amp;nbsp;points.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Part 2: Solving the optimization&amp;nbsp;problem&lt;/h2&gt;
&lt;p&gt;In Part 1, we showed how to set up SVMs as an optimization problem. In this section, we&amp;#8217;ll see how the eponymous support vectors emerge when we rephrase the minimization problem as an equivalent maximization&amp;nbsp;problem.&lt;/p&gt;
&lt;p&gt;To recap: Given &lt;span class="math"&gt;\(m\)&lt;/span&gt; training points that are labeled &lt;span class="math"&gt;\(y_i = \pm 1\)&lt;/span&gt;, our goal is to maximize the margin of the hyperplane defined by &lt;span class="math"&gt;\(\boldsymbol{w} \cdot \boldsymbol{x} + b = 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We&amp;#8217;ll use the separable case (\ref{problem}) as our starting point, but the steps in the procedure and final result are similar for the non-separable case (also worked out in ref [&lt;a href="#3"&gt;3&lt;/a&gt;]).&lt;/p&gt;
&lt;h3&gt;The Lagrangian&amp;nbsp;formulation&lt;/h3&gt;
&lt;p&gt;How do we solve this optimization problem? Minimizing a function without constraints is probably familiar: set the derivative of the function (the objective) to zero and&amp;nbsp;solve.&lt;/p&gt;
&lt;p&gt;With constraints, the procedure is similar to setting the derivative of the objective equal to zero. Instead of taking the derivative of the objective itself, however, we&amp;#8217;ll operate on the Lagrangian &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt;, which combines the objective and inequality constraints into one&amp;nbsp;function:&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{3} \label{Lagrangian}
\mathcal{L}(w,b,\alpha) = f(w) + \sum_i^m \alpha_i g_i(w,b)
$$&lt;/div&gt;
&lt;p&gt;We&amp;#8217;ve just introduced additional variables &lt;span class="math"&gt;\(\alpha_i\)&lt;/span&gt;, Lagrange multipliers, that make it easier to work with the constraints (see Wikipedia about the &lt;a href="https://en.wikipedia.org/wiki/Lagrange_multiplier"&gt;method of Lagrange multipliers&lt;/a&gt;). Note, a more general form for the Lagrangian would include another summation term in (\ref{Lagrangian}) to uphold equality constraints. Since there are only inequality constraints here, we&amp;#8217;ll omit the extra&amp;nbsp;term.&lt;/p&gt;
&lt;h3&gt;Constructing the dual&amp;nbsp;problem&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Much of the following discussion is based off ref &lt;a href="#2"&gt;[2]&lt;/a&gt;, which has a nice introduction to duality in the context of&amp;nbsp;SVMs.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;First, let&amp;#8217;s make the following&amp;nbsp;observation:
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{Obs. 1}
\max_{\alpha} \mathcal{L}(w,b,\alpha) =
\begin{cases}
f(w), &amp;amp; \text{if } g_i(w) \leq 0, \; \text{(constraints satisfied)} \\
\infty, &amp;amp; \text{if } g_i(w) \gt 0, \; \text{(constraints violated)}
\end{cases}
$$&lt;/div&gt;
&lt;p&gt;Basically, if any constraint &lt;span class="math"&gt;\(j\)&lt;/span&gt; is violated, i.e. &lt;span class="math"&gt;\(g_j(w) &amp;gt; 0\)&lt;/span&gt;, then the Lagrange multiplier &lt;span class="math"&gt;\(\alpha_j\)&lt;/span&gt; that is multiplying &lt;span class="math"&gt;\(g_j(w)\)&lt;/span&gt; can be made arbitrarily large (&lt;span class="math"&gt;\(\rightarrow \infty\)&lt;/span&gt;) in order to maximize &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;On the other hand, if all the constraints are satisfied, &lt;span class="math"&gt;\(g_i(w) \leq 0\)&lt;/span&gt; &lt;span class="math"&gt;\(\forall \; i\)&lt;/span&gt;, then &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; is maximized by setting the &lt;span class="math"&gt;\(\alpha_i\)&lt;/span&gt;s that are multiplying negative quantities equal to zero. However, Lagrangian multipliers multiplying &lt;span class="math"&gt;\(g_i(w)\)&lt;/span&gt; that satisfy the constraints with equality, &lt;span class="math"&gt;\(g_i(w) = 0\)&lt;/span&gt;, can be non-zero without diminishing &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The last statement amounts to the property of &amp;#8220;complementary slackness&amp;#8221; in the &lt;a href="http://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions" title="KKT conditions"&gt;Karush-Kuhn-Tucker&lt;/a&gt; conditions for the&amp;nbsp;solution:
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{4} \label{complementarity}
\alpha_i g_i(w) = 0
$$&lt;/div&gt;
&lt;p&gt;Recall from the original geometric picture: only a few points lie exactly on the margins, and those points are described by &lt;span class="math"&gt;\(g_i(w) = 0\)&lt;/span&gt; (and thus have non-zero Lagrange multipliers). &lt;strong&gt;The points on the margin are the support&amp;nbsp;vectors.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Next, we make use of the Max-Min&amp;nbsp;inequality:
&lt;/p&gt;
&lt;div class="math"&gt;$$
\max_{\alpha} \min_{w,b} \mathcal{L}(w,b,\alpha) \leq \min_{w,b} \max_{\alpha} \mathcal{L}(w,b,\alpha)
$$&lt;/div&gt;
&lt;p&gt;This inequality is an equality under certain conditions, which our problem satisfies (convex &lt;span class="math"&gt;\(f\)&lt;/span&gt; and &lt;span class="math"&gt;\(g\)&lt;/span&gt;). The left side of the inequality is called the dual problem, and the right side is the primal&amp;nbsp;problem.&lt;/p&gt;
&lt;p&gt;Now we can put it all together: Observation 1 tells us that solving the right side (primal problem) of the Max-Min inequality is the same as solving the original problem. Because our problem is convex, solving the left side (dual) is equivalent to solving the primal problem by the Max-Min&amp;nbsp;inequality.&lt;/p&gt;
&lt;p&gt;Thus we&amp;#8217;re set to approach the solution via the dual problem, which is useful for dealing with nonlinear decision&amp;nbsp;boundaries.&lt;/p&gt;
&lt;h3&gt;Solving the dual&amp;nbsp;problem&lt;/h3&gt;
&lt;p&gt;The dual problem to solve is &lt;span class="math"&gt;\(\max_{\alpha} \min_{w,b} \mathcal{L}(w,b,\alpha)\)&lt;/span&gt;, subject to constraints&lt;a href="#note1"&gt;*&lt;/a&gt; on the Lagrange multipliers: &lt;span class="math"&gt;\(\alpha_i \geq 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;#8217;s work out the inner part of the expression explicitly. We obtain &lt;span class="math"&gt;\(\min_{w,b} \mathcal{L}(w,b,\alpha)\)&lt;/span&gt; by&amp;nbsp;setting:
&lt;/p&gt;
&lt;div class="math"&gt;$$
\nabla_\boldsymbol{w} \mathcal{L} = 0; \quad \partial_b \mathcal{L} = 0
$$&lt;/div&gt;
&lt;p&gt;These equations for the partial derivatives give us,&amp;nbsp;respectively:
&lt;/p&gt;
&lt;div class="math"&gt;$$
\boldsymbol{w} = \sum_i \alpha_i y_i \boldsymbol{x}_i; \quad \sum_i \alpha_i y_i = 0
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\boldsymbol{w}\)&lt;/span&gt; is a linear combination of the coordinates of the training data. Only the support vectors, which have non-zero &lt;span class="math"&gt;\(\alpha_i\)&lt;/span&gt;, contribute to the sum. To predict the label for a new test point &lt;span class="math"&gt;\(\boldsymbol{x_t}\)&lt;/span&gt;, simply evaluate the sign&amp;nbsp;of
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{5} \label{testing}
\boldsymbol{w} \cdot \boldsymbol{x} + b = \sum_i \alpha_i y_i \boldsymbol{x}_i \cdot \boldsymbol{x_t} + b
$$&lt;/div&gt;
&lt;p&gt;
where b can be computed from the &lt;span class="caps"&gt;KKT&lt;/span&gt; complementarity condition (\ref{complementarity}) by plugging in the values for any support vector. The equation for the separating hyperplane is entirely determined by the support&amp;nbsp;vectors.&lt;/p&gt;
&lt;p&gt;Plugging the last two equations into &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; leads to the dual formulation of the problem &lt;span class="math"&gt;\( \max_{\alpha} \mathcal{L}_D\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{6} \label{dual}
\begin{align}
\max_{\alpha} \quad &amp;amp; \sum_i \alpha_i - (1/2) \sum_{i,j} \alpha_i \alpha_j y_i y_j \boldsymbol{x_i} \cdot \boldsymbol{x_j} \\
\text{subject to} \quad &amp;amp; \alpha_i \geq 0, \; i = 1 \ldots m \\
&amp;amp; \sum_i \alpha_i y_i = 0
\end{align}
$$&lt;/div&gt;
&lt;p&gt;The dual for the non-separable primal Lagrangian (\ref{regularization}) &amp;#8212; derived using the same procedure we just followed &amp;#8212; looks just like the dual for the separable case (\ref{dual}), except that the Lagrange multipliers are bounded from above by the regularization constant: &lt;span class="math"&gt;\(0 \leq \alpha_i \leq C\)&lt;/span&gt;. Notably, the slack variables &lt;span class="math"&gt;\(\xi_i\)&lt;/span&gt; do not appear in the dual of the soft margin &lt;span class="caps"&gt;SVM&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The dual (called the Wolfe dual) is easier to solve because of the simpler form of its inequality constraints and is the form used in algorithms such as the &lt;a href="http://research.microsoft.com/pubs/68391/smo-book.pdf"&gt;Sequential Minimal Optimization&lt;/a&gt; algorithm, which is implemented in the popular &lt;span class="caps"&gt;SVM&lt;/span&gt; solver, &lt;a href="https://www.csie.ntu.edu.tw/~cjlin/libsvm/"&gt;&lt;span class="caps"&gt;LIBSVM&lt;/span&gt;&lt;/a&gt;. The key feature of the dual is that training vectors only appear as dot products &lt;span class="math"&gt;\(\boldsymbol{x_i} \cdot \boldsymbol{x_j}\)&lt;/span&gt;. This property allows us to generalize to the nonlinear case via the &amp;#8220;kernel trick&amp;#8221; discussed in Part 3 of this&amp;nbsp;post.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Some of you may be familiar with using Lagrangian multipliers to optimize some function &lt;span class="math"&gt;\(f(\boldsymbol{x})\)&lt;/span&gt; subject to equality constraints &lt;span class="math"&gt;\(g(\boldsymbol{x}) = 0\)&lt;/span&gt;, in which case the Lagrangian multipliers are unconstrained. The &lt;a href="http://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions" title="KKT conditions"&gt;Karush-Kuhn-Tucker conditions&lt;/a&gt; generalize the method to include inequality constraints &lt;span class="math"&gt;\(g(\boldsymbol{x}) \leq 0\)&lt;/span&gt;, which results in additional constraints on the associated Lagrangian multipliers (as we have&amp;nbsp;here).&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2&gt;Part 3:&amp;nbsp;Kernels&lt;/h2&gt;
&lt;p&gt;Data that is not linearly separable in the original input space may be separable if mapped to a different space. Consider the following example of nonlinearly separable, two-dimensional&amp;nbsp;data:&lt;/p&gt;
&lt;iframe width="600" height="533" frameborder="0" scrolling="no" src="//plot.ly/~frangipane/9.embed"&gt;&lt;/iframe&gt;

&lt;p&gt;However, if we map the 2-d input data &lt;span class="math"&gt;\(\boldsymbol{x} = (x, y)\)&lt;/span&gt; to 3-d feature space by a function &lt;span class="math"&gt;\(\Phi(\boldsymbol{x}) = (x,\; y,\; x^2 + y^2)\)&lt;/span&gt;, the blue and red points can be separated with a plane in the new (3-d) space. See the plot below of the decision boundary, the mapped points, as well as the the original data points in the x-y plane. Drag the figure to rotate it, or zoom in and out with your mouse&amp;nbsp;wheel!&lt;/p&gt;
&lt;iframe width="600" height="533" frameborder="0" src="//plot.ly/~frangipane/35.embed"&gt;&lt;/iframe&gt;

&lt;p&gt;Code to generate and fit the data in this example with scikit-learn&amp;#8217;s &lt;span class="caps"&gt;SVM&lt;/span&gt; module, as well as code to create the plot.ly interactive plots above, is available in IPython notebooks on &lt;a href="https://github.com/EFavDB/svm-classification"&gt;github&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;From maps to&amp;nbsp;kernels&lt;/h3&gt;
&lt;p&gt;So how do we incorporate mapping the data into the formulation of the&amp;nbsp;problem?&lt;/p&gt;
&lt;p&gt;Recall that the data appears as a dot product in the dual Lagrangian (\ref{dual}). If we decide to train an &lt;span class="caps"&gt;SVM&lt;/span&gt; on the mapped data, then the dot product of the input data in (\ref{dual}) is replaced by the dot product of the mapped data: &lt;span class="math"&gt;\(\boldsymbol{x_i} \cdot \boldsymbol{x_j} \rightarrow \Phi(\boldsymbol{x_i}) \cdot&amp;nbsp;\Phi(\boldsymbol{x_j})\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The kernel is simply the dot product of the mapping functions. In the example above, the inner product of the mapping function is an instance of a polynomial&amp;nbsp;kernel:
&lt;/p&gt;
&lt;div class="math"&gt;$$
K(x_i, x_j) = \Phi(\boldsymbol{x_i}) \cdot \Phi(\boldsymbol{x_j}) = x_i x_j + y_i y_j + (x_i^2 + y_i^2)(x_j^2 + y_j^2)
$$&lt;/div&gt;
&lt;p&gt;In practice, we work directly with the kernel &lt;span class="math"&gt;\(K(x_i, x_j)\)&lt;/span&gt; rather than explicitly computing the map of the data points&lt;a href="#note2"&gt;**&lt;/a&gt;. Computing the kernel directly allows us to sidestep the computationally expensive operation of mapping data to a high dimensional space and then taking a dot product (see ref [&lt;a href="#2"&gt;2&lt;/a&gt;] for examples comparing computational times of the two&amp;nbsp;methods).&lt;/p&gt;
&lt;p&gt;Using a kernel, the second term in the objective of the dual problem (\ref{dual})&amp;nbsp;becomes
&lt;/p&gt;
&lt;div class="math"&gt;$$
\sum_{i,j} \alpha_i \alpha_j y_i y_j K(x_i, x_j)
$$&lt;/div&gt;
&lt;p&gt;
The kernel also appears in the evaluation of (\ref{testing}) to predict the classification of a test point &lt;span class="math"&gt;\(\boldsymbol{x_t}\)&lt;/span&gt;:
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{8} \label{testingKernel}
sgn \left(\sum_i \alpha_i y_i K(x_i, x_t) + b \right)
$$&lt;/div&gt;
&lt;p&gt;Which functions are valid kernels to use in the kernel trick? i.e. given &lt;span class="math"&gt;\(K(x_i, x_j)\)&lt;/span&gt;, does some feature map &lt;span class="math"&gt;\(\Phi\)&lt;/span&gt; exist such that &lt;span class="math"&gt;\(K(x_i, x_j)=\Phi(\boldsymbol{x_i}) \cdot \Phi(\boldsymbol{x_j})\)&lt;/span&gt; for any &lt;span class="math"&gt;\(i,\ j\)&lt;/span&gt;? Mercer&amp;#8217;s condition states that a necessary and sufficient condition for &lt;span class="math"&gt;\(K\)&lt;/span&gt; to be a valid kernel is that it is symmetric and positive semi-definite&lt;a href="#note3"&gt;&lt;span class="math"&gt;\(^\dagger\)&lt;/span&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Some popular kernels&amp;nbsp;are:
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
\text{polynomial:} &amp;amp; \quad (\boldsymbol{x_i} \cdot \boldsymbol{x_j} + c)^p \\
\text{Gaussian radial basis function:} &amp;amp; \quad \exp(-\|\boldsymbol{x_i} - \boldsymbol{x_j} \|^2/2\sigma^2)
\end{align}
$$&lt;/div&gt;
&lt;p&gt;
The optimal parameters for the degree of the polynomial &lt;span class="math"&gt;\(p\)&lt;/span&gt; and spread of the Gaussian &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; (as well as the regularization parameter) are determined by cross-validation. Computing the above kernels takes &lt;span class="math"&gt;\(\mathcal{O}(d)\)&lt;/span&gt; time, where &lt;span class="math"&gt;\(d\)&lt;/span&gt; is the dimension of the input space, since we have to evaluate &lt;span class="math"&gt;\(\boldsymbol{x_i} \cdot \boldsymbol{x_j}\)&lt;/span&gt; in the polynomial kernel and &lt;span class="math"&gt;\(\boldsymbol{x_i} - \boldsymbol{x_j}\)&lt;/span&gt; in the Gaussian&amp;nbsp;kernel.&lt;/p&gt;
&lt;h3&gt;Comparing runtimes of linear and nonlinear&amp;nbsp;kernels&lt;/h3&gt;
&lt;p&gt;The computational complexity for &lt;strong&gt;classification/prediction&lt;/strong&gt;, i.e. at test time, can be obtained by eyeballing (\ref{testing}) and (\ref{testingKernel}). Let &lt;span class="math"&gt;\(d\)&lt;/span&gt; be the dimension of the input space and &lt;span class="math"&gt;\(n\)&lt;/span&gt; be the size of the training set, and assume the number of support vectors &lt;span class="math"&gt;\(n_S\)&lt;/span&gt; is some fraction of &lt;span class="math"&gt;\(n\)&lt;/span&gt;, &lt;span class="math"&gt;\(n_S \sim \mathcal{O}(n)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In the case of working with the linear kernel/original input space, &lt;span class="math"&gt;\(\boldsymbol{w}\)&lt;/span&gt; can be explicitly evaluated to obtain the separating hyperplane parameters, so that classification in (\ref{testing}) takes &lt;span class="math"&gt;\(\mathcal{O}(d)\)&lt;/span&gt; time. On the other hand, with the kernel trick, the hyperplane parameters are not explicitly evaluated. Assume calculating a kernel takes &lt;span class="math"&gt;\(\mathcal{O}(d)\)&lt;/span&gt; time, cf. the polynomial and Gaussian kernels; then test time for a nonlinear &lt;span class="math"&gt;\(K\)&lt;/span&gt; in (\ref{testingKernel}) takes &lt;span class="math"&gt;\(\mathcal{O}(nd)\)&lt;/span&gt;&amp;nbsp;time.&lt;/p&gt;
&lt;p&gt;Estimating the computational complexity for &lt;strong&gt;training&lt;/strong&gt; is complicated, so we defer the discussion to refs [&lt;a href="#4a"&gt;4a&lt;/a&gt;, &lt;a href="#4b"&gt;4b&lt;/a&gt;] and simply state the result: training for linear kernels is &lt;span class="math"&gt;\(\mathcal{O}(nd)\)&lt;/span&gt; while training for nonlinear kernels using the Sequential Minimal Optimization algorithm is &lt;span class="math"&gt;\(\mathcal{O}(n^2)\)&lt;/span&gt; to &lt;span class="math"&gt;\(\mathcal{O}(n^3)\)&lt;/span&gt; (dependent on the regularization parameter &lt;span class="math"&gt;\(C\)&lt;/span&gt;), making nonlinear kernel SVMs impractical for larger datasets (a couple of 10,000 samples according to &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"&gt;scikit-learn&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;** More than one mapping and feature space (dimension) may exist for a particular kernel. See section 4 of ref [&lt;a href="#1"&gt;1&lt;/a&gt;] for&amp;nbsp;examples.&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(^\dagger\)&lt;/span&gt; See ref [&lt;a href="#2"&gt;2&lt;/a&gt;] for a simple proof in terms of the Kernel (Gram) matrix, i.e. the kernel function evaluated on a finite set of&amp;nbsp;points.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Discussion&lt;/h2&gt;
&lt;p&gt;We&amp;#8217;ve glimpsed the elegant theory behind the construction of SVMs and seen how support vectors pop out of the mathematical machinery. Geometrically, the support vectors are the points lying on the margins of the decision&amp;nbsp;boundary.&lt;/p&gt;
&lt;p&gt;How about using SVMs in&amp;nbsp;practice?&lt;/p&gt;
&lt;p&gt;In his Coursera course, Professor Ng recommends linear and Gaussian kernels for most use cases. He also provides some rules of thumb (based on the current state of &lt;span class="caps"&gt;SVM&lt;/span&gt; algorithms) for different sample sizes &lt;span class="math"&gt;\(n\)&lt;/span&gt; and input data dimension/number of features &lt;span class="math"&gt;\(d\)&lt;/span&gt;, restated&amp;nbsp;here:&lt;/p&gt;
&lt;p&gt;case                                               method                                                           &lt;span class="math"&gt;\(n\)&lt;/span&gt;         &lt;span class="math"&gt;\(d\)&lt;/span&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;span class="math"&gt;\(n \ll d\)&lt;/span&gt;, e.g. genomics, bioinformatics data   linear kernel &lt;span class="caps"&gt;SVM&lt;/span&gt; or logistic regression                         10 - 1000     10,000
  &lt;span class="math"&gt;\(n\)&lt;/span&gt; intermediate, &lt;span class="math"&gt;\(d\)&lt;/span&gt; small                    Gaussian kernel &lt;span class="caps"&gt;SVM&lt;/span&gt;                                              10 - 10,000   1 - 1000
  &lt;span class="math"&gt;\(n \gg d\)&lt;/span&gt;                                       create features, then linear kernel &lt;span class="caps"&gt;SVM&lt;/span&gt; or logistic regression   50,000+       1 -&amp;nbsp;1000&lt;/p&gt;
&lt;p&gt;The creators of the &lt;span class="caps"&gt;LIBSVM&lt;/span&gt; and &lt;span class="caps"&gt;LIBLINEAR&lt;/span&gt; packages also provide a &lt;a href="https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf"&gt;user&amp;#8217;s guide&lt;/a&gt; for novices, which includes a study of when to use linear instead of radial basis function kernels. They recommend linear SVMs when &lt;span class="math"&gt;\(d\)&lt;/span&gt; and &lt;span class="math"&gt;\(n\)&lt;/span&gt; are both large, often encountered in document classification problems where bag-of-words approaches can generate huge numbers of features (in their example &lt;span class="math"&gt;\(n =\)&lt;/span&gt; 20,000, &lt;span class="math"&gt;\(d =\)&lt;/span&gt;&amp;nbsp;47,000).&lt;/p&gt;
&lt;p&gt;The idea is that if the input data is already high-dimensional, then it shouldn&amp;#8217;t be necessary to apply nonlinear transformations to it in order to obtain a separating&amp;nbsp;hyperplane.&lt;/p&gt;
&lt;p&gt;Tip: &lt;span class="caps"&gt;LIBLINEAR&lt;/span&gt; is specifically optimized for linear kernels and should be used instead of &lt;span class="caps"&gt;LIBSVM&lt;/span&gt; in the linear&amp;nbsp;case.&lt;/p&gt;
&lt;h3&gt;Further&amp;nbsp;reading&lt;/h3&gt;
&lt;p&gt;In addition to the many excellent written tutorials on SVMs online, we highly recommend viewing lectures 14 and 15 of Yaser Abu-Mostafa&amp;#8217;s &lt;span class="caps"&gt;MOOC&lt;/span&gt;, &lt;a href="https://work.caltech.edu/telecourse.html"&gt;Learning from Data&lt;/a&gt;, which cover SVMs at about the same level as this post, with the considerable added benefit of Professor Abu-Mostafa&amp;#8217;s explanations. He also discusses the generalization performance of SVMs as a function of the number of support vectors using &lt;span class="caps"&gt;VC&lt;/span&gt; theory (also see [&lt;a href="#1"&gt;1&lt;/a&gt;]).&lt;/p&gt;
&lt;p&gt;There is a lot more theory on SVMs that we haven&amp;#8217;t touched upon. For example, SVMs can be framed as a penalization method [&lt;a href="#3"&gt;3&lt;/a&gt;] or &lt;a href="http://cbcl.mit.edu/cbcl/publications/ps/evgeniou-reviewall.pdf"&gt;&amp;#8220;regularization network&amp;#8221;&lt;/a&gt;, c.f. ridge regression, but with a hinge loss rather than squared error. Insights about the choice of a &lt;a href="http://alex.smola.org/papers/1998/SmoSch98b.pdf"&gt;kernel&lt;/a&gt; have also been developed in that&amp;nbsp;framework.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;[&lt;a href="http://research.microsoft.com/pubs/67119/svmtutorial.pdf"&gt;1&lt;/a&gt;] Burges, &lt;span class="caps"&gt;C. J.C.&lt;/span&gt;(1998). A Tutorial on Support Vector Machines for Pattern Recognition. Knowledge Discovery and Data Mining 2 (2)&amp;nbsp;121-167.&lt;/p&gt;
&lt;p&gt;[&lt;a href="http://cs229.stanford.edu/notes/cs229-notes3.pdf"&gt;2&lt;/a&gt;] Ng, A. Support Vector Machines [&lt;span class="caps"&gt;PDF&lt;/span&gt; document]. Retrieved from lecture notes online: http://cs229.stanford.edu/notes/cs229-notes3.pdf
&lt;em&gt;Lecture notes by Andrew Ng for a more advanced class (but still in his signature intuitive&amp;nbsp;style).&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;[&lt;a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/"&gt;3&lt;/a&gt;] Hastie, T., Tibshirani, R., and Friedman, J. (2009). The Elements of Statistical Learning.
&lt;em&gt;See section 12.2.1, page 420, for derivation of the dual Lagrangian for the nonseparable&amp;nbsp;case.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;[&lt;a href="https://www.csie.ntu.edu.tw/~cjlin/papers/bottou_lin.pdf"&gt;4a&lt;/a&gt;] Bottou, L. and Lin C-J., (2006). Support Vector Machine Solvers.
[&lt;a href="http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf"&gt;4b&lt;/a&gt;] Chang, C-C. and Lin C-J., (2013). &lt;span class="caps"&gt;LIBSVM&lt;/span&gt;: A Library for Support Vector&amp;nbsp;Machines.&lt;/p&gt;
&lt;p&gt;[&lt;a href="http://www.cs.colostate.edu/~asa/pdfs/howto.pdf"&gt;5&lt;/a&gt;] Ben-Hur, A. and Weston, J. (2009). A User&amp;#8217;s Guide to Support Vector Machines. In Carugo, O. and Eisenhaber, F. (Eds.), Methods in Molecular Biology 609,&amp;nbsp;223-229.&lt;/p&gt;
&lt;p&gt;Andrew Ng photo credit: &lt;a href="https://commons.wikimedia.org/wiki/User:InverseHypercube"&gt;InverseHypercube&lt;/a&gt;, &lt;a href="http://creativecommons.org/licenses/by/3.0/us/deed.en"&gt;creative commons license&lt;/a&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Theory"></category><category term="kernel methods"></category><category term="Machine Learning"></category><category term="optimization"></category><category term="Python"></category><category term="quadratic programming"></category><category term="SVM"></category></entry><entry><title>A review of parameter regularization and Bayesian regression</title><link href="https://efavdb.com/bayesian-linear-regression" rel="alternate"></link><published>2015-10-11T00:01:00-07:00</published><updated>2015-10-11T00:01:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2015-10-11:/bayesian-linear-regression</id><summary type="html">&lt;p&gt;Here, we review parameter regularization, which is a method for improving regression models through the penalization of non-zero parameter estimates. Why is this effective? Biasing parameters towards zero will (of course!) unfavorably bias a model, but it will also reduce its variance. At times the latter effect can win out …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Here, we review parameter regularization, which is a method for improving regression models through the penalization of non-zero parameter estimates. Why is this effective? Biasing parameters towards zero will (of course!) unfavorably bias a model, but it will also reduce its variance. At times the latter effect can win out, resulting in a net reduction in generalization error. We also review Bayesian regressions &amp;#8212; in effect, these generalize the regularization approach, biasing model parameters to any specified prior estimates, not necessarily&amp;nbsp;zero.&lt;/p&gt;
&lt;p&gt;This is the second of a series of posts expounding on topics discussed in the text, &lt;a href="http://www-bcf.usc.edu/~gareth/ISL/"&gt;&amp;#8220;An Introduction to Statistical Learning&amp;#8221;&lt;/a&gt;. Here, we cover material from its Chapters 2 and 6. See prior post &lt;a href="http://efavdb.github.io/leave-one-out-cross-validation"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Introduction and&amp;nbsp;overview&lt;/h3&gt;
&lt;p&gt;In this post, we will be concerned with the problem of fitting a function of the&amp;nbsp;form
&lt;/p&gt;
&lt;div class="math"&gt;$$\label{function}
y(\vec{x}_i) = f(\vec{x}_i) + \epsilon_i \tag{1},
$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(f\)&lt;/span&gt; is the function&amp;#8217;s systematic part and &lt;span class="math"&gt;\(\epsilon_i\)&lt;/span&gt; is a random error. These errors have mean zero and are iid &amp;#8212; their presence is meant to take into account dependences in &lt;span class="math"&gt;\(y\)&lt;/span&gt; on features that we don&amp;#8217;t have access to. To &amp;#8220;fit&amp;#8221; such a function, we will suppose that one has chosen some appropriate regression algorithm (perhaps a linear model, a random forest, etc.) that can be used to generate an approximation &lt;span class="math"&gt;\(\hat{f}\)&lt;/span&gt; to &lt;span class="math"&gt;\(y\)&lt;/span&gt;, given a training set of example &lt;span class="math"&gt;\((\vec{x}_i, y_i)\)&lt;/span&gt;&amp;nbsp;pairs.&lt;/p&gt;
&lt;p&gt;The primary concern when carrying out a regression is often to find a fit that will be accurate when applied to points not included in the training set. There are two sources of error that one has to grapple with: Bias in the algorithm &amp;#8212; sometimes the result of using an algorithm that has insufficient flexibility to capture the nature of the function being fit, and variance &amp;#8212; this relates to how sensitive the resulting fit is to the samples chosen for the training set. The latter issue is closely related to the concept of &lt;a href="https://en.wikipedia.org/wiki/Overfitting"&gt;overfitting&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To mitigate overfitting, &lt;a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)"&gt;parameter regularization&lt;/a&gt; is often applied. As we detail below, this entails penalizing non-zero parameter estimates. Although this can favorably reduce the variance of the resulting model, it will also introduce bias. The optimal amount of regularization is therefore determined by appropriately balancing these two&amp;nbsp;effects.&lt;/p&gt;
&lt;p&gt;In the following, we carefully review the mathematical definitions of model bias and variance, as well as how these effects contribute to the error of an algorithm. We then show that regularization is equivalent to assuming a particular form of Bayesian prior that causes the parameters to be somewhat &amp;#8220;sticky&amp;#8221; around zero &amp;#8212; this stickiness is what results in model variance reduction. Because standard regularization techniques bias towards zero, they work best when the underlying true feature dependences are sparse. When this is not true, one should attempt an analogous variance reduction through application of the more general Bayesian regression&amp;nbsp;framework.&lt;/p&gt;
&lt;h3&gt;Squared error&amp;nbsp;decomposition&lt;/h3&gt;
&lt;p&gt;The first step to understanding regression error is the following identity: Given any fixed &lt;span class="math"&gt;\(\vec{x}\)&lt;/span&gt;, we&amp;nbsp;have
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
\overline{\left (\hat{f}(\vec{x}) - y(\vec{x}) \right)^2} &amp;amp;= \overline{\left (\hat{f}(\vec{x}) - \overline{\hat{f}(\vec{x})} \right)^2} + \left (\overline{\hat{f}(\vec{x})} - f(\vec{x}) \right)^2 + \overline{ \epsilon^2} \\
&amp;amp; \equiv var\left(\hat{f}(\vec{x})\right) + bias\left(\hat{f}(\vec{x})\right)^2 + \overline{\epsilon^2}. \tag{2}\label{error_decomp}
\end{align}
$$&lt;/div&gt;
&lt;p&gt;
Here, overlines represent averages over two things: The first is the random error &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; values, and the second is the training set used to construct &lt;span class="math"&gt;\(\hat{f}\)&lt;/span&gt;. The left side of (\ref{error_decomp}) gives the average squared error of our algorithm, at point &lt;span class="math"&gt;\(\vec{x}\)&lt;/span&gt; &amp;#8212; i.e., the average squared error we can expect to get, given a typical training set and &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; value. The right side of the equation decomposes this error into separate, independent components. The first term at right &amp;#8212; the variance of &lt;span class="math"&gt;\(\hat{f}(\vec{x})\)&lt;/span&gt; &amp;#8212; relates to how widely the estimate at &lt;span class="math"&gt;\(\vec{x}\)&lt;/span&gt; changes as one randomly samples from the space of possible training sets. Similarly, the second term &amp;#8212; the algorithm&amp;#8217;s squared bias &amp;#8212; relates to the systematic error of the algorithm at &lt;span class="math"&gt;\(\vec{x}\)&lt;/span&gt;. The third and final term above gives the average squared random error &amp;#8212; this provides a fundamental lower bound on the accuracy of any estimator of &lt;span class="math"&gt;\(y\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We turn now to the proof of (\ref{error_decomp}). We write the left side of this equation&amp;nbsp;as
&lt;/p&gt;
&lt;div class="math"&gt;$$\label{detail}
\begin{align} \tag{3}
\overline{\left (\hat{f}(\vec{x}) - y(\vec{x}) \right)^2} &amp;amp;= \overline{\left ( \left \{\hat{f}(\vec{x}) - f(\vec{x}) \right \} - \left \{ y(\vec{x}) - f(\vec{x}) \right \} \right)^2}\\
&amp;amp;=
\overline{\left ( \hat{f}(\vec{x}) - f(\vec{x}) \right)^2}
- 2 \overline{ \left (\hat{f}(\vec{x}) - f(\vec{x}) \right ) \left (y(\vec{x}) - f(\vec{x}) \right ) }
+ \overline{ \left (y(\vec{x}) - f(\vec{x}) \right)^2}.
\end{align}
$$&lt;/div&gt;
&lt;p&gt;
The middle term here is zero. To see this, note that it is the average of the product of two independent quantities: The first factor, &lt;span class="math"&gt;\(\hat{f}(\vec{x}) - f(\vec{x})\)&lt;/span&gt;, varies only with the training set, while the second factor, &lt;span class="math"&gt;\(y(\vec{x}) - f(\vec{x})\)&lt;/span&gt;, varies only with &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;. Because these two factors are independent, their average product is the product of their individual averages, the second of which is zero, by definition. Now, the third term in (\ref{detail}) is simply &lt;span class="math"&gt;\(\overline{\epsilon^2}\)&lt;/span&gt;. To complete the proof, we need only evaluate the first term above. To do that, we&amp;nbsp;write
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align} \tag{4} \label{detail2}
\overline{\left ( \hat{f}(\vec{x}) - f(\vec{x}) \right)^2} &amp;amp;=
\overline{\left ( \left \{ \hat{f}(\vec{x}) - \overline{\hat{f}(\vec{x})} \right \}- \left \{f(\vec{x}) -\overline{\hat{f}(\vec{x})} \right \}\right)^2} \\
&amp;amp;=
\overline{\left ( \hat{f}(\vec{x}) - \overline{\hat{f}(\vec{x})} \right)^2}
-2
\overline{ \left \{ \hat{f}(\vec{x}) - \overline{\hat{f}(\vec{x})} \right \} \left \{f(\vec{x}) -\overline{\hat{f}(\vec{x})} \right \} }
+
\left ( f(\vec{x}) -\overline{\hat{f}(\vec{x})} \right)^2.
\end{align}
$$&lt;/div&gt;
&lt;p&gt;
The middle term here is again zero. This is because its second factor is a constant, while the first averages to zero, by definition. The first and third terms above are the algorithm&amp;#8217;s variance and squared bias, respectively. Combining these observations with (\ref{detail}), we obtain&amp;nbsp;(\ref{error_decomp}).&lt;/p&gt;
&lt;h3&gt;Bayesian&amp;nbsp;regression&lt;/h3&gt;
&lt;p&gt;In order to introduce Bayesian regression, we focus on the special case of least-squares regressions. In this context, one posits that the samples generated take the form (\ref{function}), with the error &lt;span class="math"&gt;\(\epsilon_i\)&lt;/span&gt; terms now iid, Gaussian distributed with mean zero and standard deviation &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;. Under this assumption, the probability of observing values &lt;span class="math"&gt;\((y_1, y_2,\ldots, y_N)\)&lt;/span&gt; at &lt;span class="math"&gt;\((\vec{x}_1, \vec{x}_2,\ldots,\vec{x}_N)\)&lt;/span&gt; is given&amp;nbsp;by
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
\tag{5} \label{5}
P(\vec{y} \vert f) &amp;amp;= \prod_{i=1}^N \frac{1}{(2 \pi \sigma)^{1/2}} \exp \left [-\frac{1}{2 \sigma^2} (y_i - f(\vec{x}_i))^2 \right]\\
&amp;amp;= \frac{1}{(2 \pi \sigma)^{N/2}} \exp \left [-\frac{1}{2 \sigma^2} (\vec{y} - \vec{f})^2 \right],
\end{align}
$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(\vec{y} \equiv (y_1, y_2,\ldots, y_N)\)&lt;/span&gt; and &lt;span class="math"&gt;\(\vec{f} \equiv (f_1, f_2,\ldots, f_N)\)&lt;/span&gt;. In order to carry out a maximum-likelihood analysis, one posits a parameterization for &lt;span class="math"&gt;\(f(\vec{x})\)&lt;/span&gt;. For example, one could posit the linear&amp;nbsp;form,
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{6}
f(\vec{x}) = \vec{\theta} \cdot \vec{x}.
$$&lt;/div&gt;
&lt;p&gt;
Once a parameterization is selected, its optimal &lt;span class="math"&gt;\(\vec{\theta}\)&lt;/span&gt; values are selected by maximizing (\ref{5}), which gives the least-squares&amp;nbsp;fit.&lt;/p&gt;
&lt;p&gt;One sometimes would like to nudge (or bias) the parameters away from those that maximize (\ref{5}), towards some values considered reasonable ahead of time. A simple way to do this is to introduce a Bayesian prior for the parameters &lt;span class="math"&gt;\(\vec{\theta}\)&lt;/span&gt;. For example, one might posit a prior of the&amp;nbsp;form
&lt;/p&gt;
&lt;div class="math"&gt;$$ \tag{7} \label{7}
P(f) \equiv P(\vec{\theta}) \propto \exp \left [- \frac{1}{2\sigma^2} (\vec{\theta} - \vec{\theta}_0)
\Lambda (\vec{\theta} - \vec{\theta}_0)\right].
$$&lt;/div&gt;
&lt;p&gt;
Here, &lt;span class="math"&gt;\(\vec{\theta}_0\)&lt;/span&gt; represents a best guess for what &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; should be before any data is taken, and the matrix &lt;span class="math"&gt;\(\Lambda\)&lt;/span&gt; determines how strongly we wish to bias &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; to this value: If the components of &lt;span class="math"&gt;\(\Lambda\)&lt;/span&gt; are large (small), then we strongly (weakly) constrain &lt;span class="math"&gt;\(\vec{\theta}\)&lt;/span&gt; to sit near &lt;span class="math"&gt;\(\vec{\theta}_0\)&lt;/span&gt;. To carry out the regression, we combine (\ref{5}-\ref{7}) with Bayes&amp;#8217; rule,&amp;nbsp;giving
&lt;/p&gt;
&lt;div class="math"&gt;$$
\tag{8}
P(\vec{\theta} \vert \vec{y}) = \frac{P(\vec{y}\vert \vec{\theta}) P(\vec{\theta})}{P(\vec{y})}
\propto \exp \left [-\frac{1}{2 \sigma^2} (\vec{y} - \vec{\theta} \cdot \vec{x})^2 - \frac{1}{2\sigma^2} (\vec{\theta} - \vec{\theta}_0)
\Lambda (\vec{\theta} - \vec{\theta}_0)\right].
$$&lt;/div&gt;
&lt;p&gt;
The most likely &lt;span class="math"&gt;\(\vec{\theta}\)&lt;/span&gt; now minimizes the quadratic &amp;#8220;cost&amp;nbsp;function&amp;#8221;,
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{9} \label{9}
F(\theta) \equiv (\vec{y} - \vec{\theta} \cdot \vec{x})^2 +(\vec{\theta} - \vec{\theta}_0)
\Lambda (\vec{\theta} - \vec{\theta}_0),
$$&lt;/div&gt;
&lt;p&gt;
a Bayesian generalization of the usual squared error. With this, our heavy-lifting is at an end. We now move to a quick review of regularization, which will appear as a simple application of the Bayesian&amp;nbsp;method.&lt;/p&gt;
&lt;h3&gt;Parameter regularization as special&amp;nbsp;cases&lt;/h3&gt;
&lt;p&gt;The most common forms of regularization are the so-called &amp;#8220;ridge&amp;#8221; and &amp;#8220;lasso&amp;#8221;. In the context of least-squares fits, the former involves minimization of the quadratic&amp;nbsp;form
&lt;/p&gt;
&lt;div class="math"&gt;$$
\tag{10} \label{ridge}
F_{ridge}(\theta) \equiv (\vec{y} - \hat{f}(\vec{x}; \vec{\theta}))^2 + \Lambda \sum_i \theta_i^2,
$$&lt;/div&gt;
&lt;p&gt;
while in the latter, one&amp;nbsp;minimizes
&lt;/p&gt;
&lt;div class="math"&gt;$$
\tag{11} \label{lasso}
F_{lasso}(\theta) \equiv (\vec{y} - \hat{f}(\vec{x}; \vec{\theta}))^2 + \Lambda \sum_i \vert\theta_i \vert.
$$&lt;/div&gt;
&lt;p&gt;
The terms proportional to &lt;span class="math"&gt;\(\Lambda\)&lt;/span&gt; above are the so-called regularization terms. In elementary courses, these are generally introduced to least-squares fits in an ad-hoc manner: Conceptually, it is suggested that these terms serve to penalize the inclusion of too many parameters in the model, with individual parameters now taking on large values only if they are really essential to the&amp;nbsp;fit.&lt;/p&gt;
&lt;p&gt;While the conceptual argument above may be correct, the framework we&amp;#8217;ve reviewed here allows for a more sophisticated understanding of regularization: (\ref{ridge}) is a special case of (\ref{9}), with &lt;span class="math"&gt;\(\vec{\theta}_0\)&lt;/span&gt; set to &lt;span class="math"&gt;\((0,0,\ldots, 0)\)&lt;/span&gt;. Further, the lasso form (\ref{lasso}) is also a special-case form of Bayesian regression, with the prior set to &lt;span class="math"&gt;\(P(\vec{\theta}) \propto \exp \left (- \frac{\Lambda}{2 \sigma^2} \sum_i \vert \theta_i \vert \right)\)&lt;/span&gt;. As advertised, regularization is a form of Bayesian&amp;nbsp;regression.&lt;/p&gt;
&lt;p&gt;Why then does regularization &amp;#8220;work&amp;#8221;? For the same reason any other Bayesian approach does: Introduction of a prior will bias a model (if chosen well, hopefully not by much), but will also effect a reduction in its variance. The appropriate amount of regularization balances these two effects. Sometimes &amp;#8212; but not always &amp;#8212; a non-zero amount of bias is&amp;nbsp;required.&lt;/p&gt;
&lt;h3&gt;Discussion&lt;/h3&gt;
&lt;p&gt;In summary, our main points here were three-fold: (i) We carefully reviewed the mathematical definitions of model bias and variance, deriving (\ref{error_decomp}). (ii) We reviewed how one can inject Bayesian priors to regressions: The key is to use the random error terms to write down the probability of seeing a particular observational data point. (iii) We reviewed the fact that the ridge and lasso &amp;#8212; (\ref{ridge}) and (\ref{lasso}) &amp;#8212; can be considered Bayesian&amp;nbsp;priors.&lt;/p&gt;
&lt;p&gt;Intuitively, one might think introduction of a prior serves to reduce the bias in a model: Outside information is injected into a model, nudging its parameters towards values considered reasonable ahead of time. In fact, this nudging introduces bias! Bayesian methods work through reduction in variance, not bias &amp;#8212; A good prior is one that does not introduce too much&amp;nbsp;bias.&lt;/p&gt;
&lt;p&gt;When, then, should one use regularization? Only when one expects the optimal model to be largely sparse. This is often the case when working on machine learning algorithms, as one has the freedom there to throw a great many feature variables into a model, expecting only a small (a prior, unknown) minority of them to really prove informative. However, when not working in high-dimensional feature spaces, sparseness should not be expected. In this scenario, one should reason some other form of prior, and attempt a variance reduction through the more general Bayesian&amp;nbsp;framework.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Statistics"></category></entry><entry><title>Getting started with Pandas</title><link href="https://efavdb.com/pandas-tips-and-tricks" rel="alternate"></link><published>2015-10-05T12:00:00-07:00</published><updated>2015-10-05T12:00:00-07:00</updated><author><name>Damien RJ</name></author><id>tag:efavdb.com,2015-10-05:/pandas-tips-and-tricks</id><summary type="html">&lt;p&gt;We have made use of Python&amp;#8217;s Pandas package in a variety of posts on the site. These have showcased some of Pandas&amp;#8217; abilities including the&amp;nbsp;following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DataFrames for data manipulation with built in&amp;nbsp;indexing&lt;/li&gt;
&lt;li&gt;Handling of missing&amp;nbsp;data&lt;/li&gt;
&lt;li&gt;Data&amp;nbsp;alignment&lt;/li&gt;
&lt;li&gt;Melting/stacking and Pivoting/unstacking data&amp;nbsp;sets&lt;/li&gt;
&lt;li&gt;Groupby feature …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;We have made use of Python&amp;#8217;s Pandas package in a variety of posts on the site. These have showcased some of Pandas&amp;#8217; abilities including the&amp;nbsp;following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DataFrames for data manipulation with built in&amp;nbsp;indexing&lt;/li&gt;
&lt;li&gt;Handling of missing&amp;nbsp;data&lt;/li&gt;
&lt;li&gt;Data&amp;nbsp;alignment&lt;/li&gt;
&lt;li&gt;Melting/stacking and Pivoting/unstacking data&amp;nbsp;sets&lt;/li&gt;
&lt;li&gt;Groupby feature allowing split -&amp;gt; apply -&amp;gt; combine operations on data&amp;nbsp;sets&lt;/li&gt;
&lt;li&gt;Data merging and&amp;nbsp;joining&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Pandas is also a high performance library, with much of its code written in Cython or C. Unfortunately, Pandas can have a bit of a steep learning curve &amp;#8212; In this post, I&amp;#8217;ll cover some introductory tips and tricks to help one get started with this excellent&amp;nbsp;package.&lt;/p&gt;
&lt;p&gt;Notes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This post was partially inspired by Tom Augspurger&amp;#8217;s Pandas &lt;a href="https://github.com/tomaugspurger/pydataseattle"&gt;tutorial&lt;/a&gt;, which has a &lt;a href="https://www.youtube.com/watch?v=otCriSKVV_8"&gt;youtube video&lt;/a&gt; that can be viewed along side it. We also suggest some other excellent resource materials &amp;#8212; where relevant &amp;#8212;&amp;nbsp;below.&lt;/li&gt;
&lt;li&gt;The notebook we use below can be downloaded from our &lt;a href="https://github.com/EFavDB/Pandas"&gt;github page&lt;/a&gt;. Feel free to grab it and follow&amp;nbsp;along.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Jupyter (Formally IPython) notebook&amp;nbsp;tips&lt;/h3&gt;
&lt;p&gt;All the exercises I will be referring to here were carried out using IPython notebooks. To start off, here&amp;#8217;s a few quick tips on notebook navigation that can help make life easier:  First, the notebook has two modes, command and edit. If you are in edit mode you have the cursor in one of the cells (boxes) where you can enter code or text.  To enter command mode press the &lt;strong&gt;Esc&lt;/strong&gt; key and the cursor will disappear, but the cell will still be highlighted.  In command mode you have a variety of keyboard shortcuts, and you can see all of them if you press &lt;strong&gt;h&lt;/strong&gt;. &lt;strong&gt;a&lt;/strong&gt; and &lt;strong&gt;b &lt;/strong&gt;will make new empty cells below and above the current cell.  &lt;strong&gt;j &lt;/strong&gt;/ &lt;strong&gt;k&lt;/strong&gt; will navigate through the notebook. &lt;strong&gt;Shift + Enter&lt;/strong&gt; will execute the current cell and move to the next one, while &lt;strong&gt;Ctrl + Enter&lt;/strong&gt; will execute the cell but does not move to the next cell. This is nice if you are still tweaking the code in that&amp;nbsp;cell.&lt;/p&gt;
&lt;h3&gt;Pandas is built on top of&amp;nbsp;NumPy&lt;/h3&gt;
&lt;p&gt;To understand Pandas, you gotta understand NumPy, as Pandas is built on top of it. Here, we cover some of its NumPy&amp;#8217;s basic&amp;nbsp;properties.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ndarray&lt;/strong&gt;: ndarrays are central to NumPy, and are homogeneous N-dimensional arrays of fixed-size. NumPy also provides fast methods for the ndarrary that are written in C, often making use of vectorized operations such as element wise addition and multiplication. These methods provide a major resource for code speedup, and Pandas takes full advantage of them where possible. In addition to the fast methods, ndarray also requires less memory than a python list because python lists are an array of pointers to Python objects &amp;#8212; this is what allows lists to hold mixed data types.  This overhead combined with the overhead of the Python objects vs a numpy object can add up quickly. The variabilities in data type also makes it difficult to implement efficient C-loops because every iteration would need to make a call to Python to check the data type. This leads us to the next&amp;nbsp;point.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data types&lt;/strong&gt;: As mentioned earlier, unlike Python&amp;#8217;s list object, NumPy arrays can only contain one data type at a time.  Giving up mixed data types allows us to achieve much better performance through efficient C-loops.  One important thing to note is that missing values (NaN) will cast integer or boolean arrays to&amp;nbsp;floats.&lt;/p&gt;
&lt;p&gt;[caption id=&amp;#8221;&amp;#8221; align=&amp;#8221;aligncenter&amp;#8221; width=&amp;#8221;526&amp;#8221;]&lt;a href="http://docs.scipy.org/doc/numpy/_images/dtype-hierarchy.png"&gt;&lt;img alt="" src="http://docs.scipy.org/doc/numpy/_images/dtype-hierarchy.png"&gt;&lt;/a&gt; Graph showing data types in&amp;nbsp;NumPy[/caption]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Broadcasting:&lt;/strong&gt; Broadcasting describes how NumPy treats arrays with different shapes. Essentially the smaller array is “broadcast” across the larger array so that they have compatible shapes. This makes it possible to have vectorized array operations, enabling the use of C instead of Python for the looping. Broadcasting also prevents needless copies of data from being created by making a new array with repeating copies.  The simplest broadcasting example occurs when combining an array with a scaler. This is something Pandas uses for efficiency and ease of&amp;nbsp;use.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;a = np.array([1.0, 2.0, 3.0])&lt;/span&gt;
&lt;span class="err"&gt;b = 2.0&lt;/span&gt;
&lt;span class="err"&gt;a * b&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;&amp;gt;&amp;gt;&amp;gt; array([ 2., 4., 6.])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;When operating on two arrays, their shapes are compared starting with the trailing dimensions. Two dimensions are compatible&amp;nbsp;when:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;they are equal,&amp;nbsp;or&lt;/li&gt;
&lt;li&gt;one of them is&amp;nbsp;1&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- --&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;a      (3d array):  15 x 3 x 5&lt;/span&gt;
&lt;span class="err"&gt;b      (2d array):       3 x 1&lt;/span&gt;
&lt;span class="err"&gt;a*b (3d array):     15 x 3 x 5&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;For those interested, I recommend Jake Vanderplas&amp;#8217;s &lt;a href="https://www.youtube.com/watch?v=EEUXKG97YRw"&gt;talk&lt;/a&gt; for learning more about how one can reduce loop usage in your own code using&amp;nbsp;NumPy.&lt;/p&gt;
&lt;h3&gt;Pandas Data&amp;nbsp;Structures&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Series&lt;/strong&gt;
We now move to the Pandas-specific data types. First up are Series, which are one-dimensional labeled arrays. Unlike ndarrays, these are capable of holding mixed data types. The cells of the series are labeled via the series &lt;strong&gt;Index&lt;/strong&gt;. We will discuss indices more in a bit. The general method of creating a series is as&amp;nbsp;follows.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;s = Series(data, index=index)&lt;/span&gt;
&lt;span class="err"&gt;pd.Series([1,2,3,4], index=[&amp;#39;a&amp;#39;, &amp;#39;b&amp;#39;, &amp;#39;c&amp;#39;, &amp;#39;d&amp;#39;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;span class="err"&gt; a 1&lt;/span&gt;
&lt;span class="err"&gt; b 2&lt;/span&gt;
&lt;span class="err"&gt; c 3&lt;/span&gt;
&lt;span class="err"&gt; d 4&lt;/span&gt;
&lt;span class="err"&gt; dtype: int64&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;DataFrame&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As we have seen in other posts on the site, the DataFrame is the main attraction for Pandas. It is a 2-dimensional labeled data structure with columns of potentially different types. You can think of it like a spreadsheet or &lt;span class="caps"&gt;SQL&lt;/span&gt; table, or a dict of Series objects. Like Series, DataFrame accepts many different kinds of input. One way of making a DataFrame is using a dictionary as&amp;nbsp;input.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;one&amp;#39;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;.,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;.,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;.,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;.],&lt;/span&gt;
&lt;span class="s1"&gt;&amp;#39;two&amp;#39;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;.,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;.,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;.,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;.],&lt;/span&gt;
&lt;span class="s1"&gt;&amp;#39;good&amp;#39;&lt;/span&gt; &lt;span class="p"&gt;:[&lt;/span&gt;&lt;span class="k"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;True&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="err"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;b&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;c&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;d&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Running the code in a notebook will output a nicely formatted&amp;nbsp;table.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;good&lt;/th&gt;
&lt;th&gt;one&lt;/th&gt;
&lt;th&gt;two&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;a&lt;/td&gt;
&lt;td&gt;&lt;span class="caps"&gt;TRUE&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;b&lt;/td&gt;
&lt;td&gt;&lt;span class="caps"&gt;FALSE&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;c&lt;/td&gt;
&lt;td&gt;&lt;span class="caps"&gt;FALSE&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;d&lt;/td&gt;
&lt;td&gt;&lt;span class="caps"&gt;TRUE&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;&amp;nbsp;Index&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The index in Pandas provides axis labeling information for pandas objects &amp;#8212; it can serve many purposes. First, an index can provide metadata, potentially important for analysis, visualization, and so on. Second, it can enable automatic data alignment when preforming operations on multiple DataFrames or Series. Third, it can allow easy access to subsets of the&amp;nbsp;data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Selection&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A huge improvement over numpy arrays is labeled indexing. We can select subsets by column, row, or both. To select columns use&amp;nbsp;[].&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;df[&amp;#39;good&amp;#39;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;span class="err"&gt; a True&lt;/span&gt;
&lt;span class="err"&gt; b False&lt;/span&gt;
&lt;span class="err"&gt; c False&lt;/span&gt;
&lt;span class="err"&gt; d True&lt;/span&gt;
&lt;span class="err"&gt; Name: good, dtype: bool&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;As we can see here, Pandas will reduce dimensions when possible which is why the output above is a Series instead of a DataFrame &amp;#8212; if you wish to force the returned result to be a DataFrame, you must supply a list of arguments, eg &lt;code&gt;df[['good']]&lt;/code&gt;. You can also select individual columns with the dot (.) operator, for example &lt;code&gt;df.good&lt;/code&gt; will give the same result. However, when using this approach, the column name selected must not have any spaces or special characters, nor can it conflict with any DataFrame methods. In order to add a column to a DataFrame, we&amp;nbsp;write,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;df[&amp;#39;A&amp;#39;] = [1, 2, 3]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;You can also select multiple columns using a column name&amp;nbsp;list.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;df[[&amp;#39;good&amp;#39;, &amp;#39;two&amp;#39;]]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;good&lt;/th&gt;
&lt;th&gt;two&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;a&lt;/td&gt;
&lt;td&gt;&lt;span class="caps"&gt;TRUE&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;b&lt;/td&gt;
&lt;td&gt;&lt;span class="caps"&gt;FALSE&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;c&lt;/td&gt;
&lt;td&gt;&lt;span class="caps"&gt;FALSE&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;For row selection, use &lt;code&gt;.loc[row_lables, column_labels]&lt;/code&gt; for label-based indexing and use &lt;code&gt;.iloc[row_positions, column_positions]&lt;/code&gt; for ordinal/positional&amp;nbsp;selection.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;df.loc[[&amp;#39;a&amp;#39;, &amp;#39;d&amp;#39;]]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;good&lt;/th&gt;
&lt;th&gt;one&lt;/th&gt;
&lt;th&gt;two&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;a&lt;/td&gt;
&lt;td&gt;&lt;span class="caps"&gt;TRUE&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;d&lt;/td&gt;
&lt;td&gt;&lt;span class="caps"&gt;TRUE&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;br /&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;df.loc[&amp;#39;a&amp;#39;:&amp;#39;b&amp;#39;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;good&lt;/th&gt;
&lt;th&gt;one&lt;/th&gt;
&lt;th&gt;two&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;a&lt;/td&gt;
&lt;td&gt;&lt;span class="caps"&gt;TRUE&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;b&lt;/td&gt;
&lt;td&gt;&lt;span class="caps"&gt;FALSE&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Notice that the slice is &lt;strong&gt;inclusive&lt;/strong&gt;: It includes both the start and end index &amp;#8212; unlike normal python&amp;nbsp;indexing.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;df.iloc[0:2]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;good&lt;/th&gt;
&lt;th&gt;one&lt;/th&gt;
&lt;th&gt;two&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;a&lt;/td&gt;
&lt;td&gt;&lt;span class="caps"&gt;TRUE&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;b&lt;/td&gt;
&lt;td&gt;&lt;span class="caps"&gt;FALSE&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;df.loc[&amp;#39;b&amp;#39;, &amp;#39;good&amp;#39;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;&amp;gt;&amp;gt;&amp;gt; False&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Reading files into&amp;nbsp;Pandas&lt;/h3&gt;
&lt;p&gt;At this point, we require a larger data set in order to demonstrate other Pandas capabilities.  For that purpose, I decided to use unemployment data from (&lt;a href="http://data.bls.gov/"&gt;http://data.bls.gov/&lt;/a&gt;).  The data file is available in our Github repository along with the notebook that generated the examples above. I saved the data as a csv file. To read into the notebook, we&amp;#8217;ll make use of Pandas&amp;#8217; read_csv method &amp;#8212; a fast, simple method for directly reading csv files into a&amp;nbsp;DataFrame.&lt;/p&gt;
&lt;p&gt;After reading in the data, we see that each row corresponds to one unemployment measurement with a different column for each time point. The first column has a numeric id, so the first thing to do is to replace those with human readable strings.  Next, we see that we have incomplete data for Jan 2000, so we drop that column.  Setting the inplace flag to true here causes the modified DataFrame to replace the original &amp;#8212; in this way, we avoid having to copy the&amp;nbsp;original.&lt;/p&gt;
&lt;p&gt;Next, we assign our IDs to the DataFrame index using &lt;code&gt;set_index&lt;/code&gt;, and then drop the column since it is no longer needed.  Lastly we transpose the table so that each row corresponds to a different time point and the columns to the separate&amp;nbsp;measures.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;data.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Series ID&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Labor force&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Participation rate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="s1"&gt;&amp;#39;Rate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Rate - 16-19 yrs&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Rate - 20+ yrs (Men)&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="s1"&gt;&amp;#39;Rate - 20+ yrs (Women)&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Rate - White&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="s1"&gt;&amp;#39;Rate - Black or African American&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Rate - Asian&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="s1"&gt;&amp;#39;Rate - Hispanic or Latino&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;No High School Diploma&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="s1"&gt;&amp;#39;High School Graduates&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Some College or Associate Degree&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="s1"&gt;&amp;#39;Bachelor degree and higher&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Under 5 Weeks&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="s1"&gt;&amp;#39;5-14 Weeks&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;15 Weeks &amp;amp; over&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;27 Weeks &amp;amp; over&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Jan 2000&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;inplace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Series ID&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;inplace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Series ID&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;inplace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transpose&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;convert_objects&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;convert_numeric&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;With these steps, we take the original&amp;nbsp;table:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Series &lt;span class="caps"&gt;ID&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;Jan 2000&lt;/th&gt;
&lt;th&gt;Feb 2000&lt;/th&gt;
&lt;th&gt;Mar 2000&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="caps"&gt;LNS11000000&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;142267(1)&lt;/td&gt;
&lt;td&gt;142456&lt;/td&gt;
&lt;td&gt;142434&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="caps"&gt;LNS11300000&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;67.3&lt;/td&gt;
&lt;td&gt;67.3&lt;/td&gt;
&lt;td&gt;67.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="caps"&gt;LNS14000000&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;4.1&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="caps"&gt;LNS14000012&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;12.7&lt;/td&gt;
&lt;td&gt;13.8&lt;/td&gt;
&lt;td&gt;13.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="caps"&gt;LNS14000025&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;3.3&lt;/td&gt;
&lt;td&gt;3.5&lt;/td&gt;
&lt;td&gt;3.2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;to&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Series &lt;span class="caps"&gt;ID&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;Labor force&lt;/th&gt;
&lt;th&gt;Part. rate&lt;/th&gt;
&lt;th&gt;Rate&lt;/th&gt;
&lt;th&gt;Rate – 16-19 yrs&lt;/th&gt;
&lt;th&gt;Rate – 20+ yrs (Men)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Feb 2000&lt;/td&gt;
&lt;td&gt;142456&lt;/td&gt;
&lt;td&gt;67.3&lt;/td&gt;
&lt;td&gt;4.1&lt;/td&gt;
&lt;td&gt;13.8&lt;/td&gt;
&lt;td&gt;3.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mar 2000&lt;/td&gt;
&lt;td&gt;142434&lt;/td&gt;
&lt;td&gt;67.3&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;13.3&lt;/td&gt;
&lt;td&gt;3.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Apr 2000&lt;/td&gt;
&lt;td&gt;142751&lt;/td&gt;
&lt;td&gt;67.3&lt;/td&gt;
&lt;td&gt;3.8&lt;/td&gt;
&lt;td&gt;12.6&lt;/td&gt;
&lt;td&gt;3.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;May 2000&lt;/td&gt;
&lt;td&gt;142388&lt;/td&gt;
&lt;td&gt;67.1&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;12.8&lt;/td&gt;
&lt;td&gt;3.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Jun 2000&lt;/td&gt;
&lt;td&gt;142591&lt;/td&gt;
&lt;td&gt;67.1&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;12.3&lt;/td&gt;
&lt;td&gt;3.2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;Working with the&amp;nbsp;data&lt;/h3&gt;
&lt;p&gt;Now that we have the data in a nicely formatted within DataFrame, we can easily visualize it using the Pandas plot method. For example, to plot the general unemployment rate, we&amp;nbsp;write&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;df[&amp;#39;Rate&amp;#39;].plot()&lt;/span&gt;
&lt;span class="err"&gt;plt.ylabel(&amp;#39;Unemployment Rate (%)&amp;#39;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/07/fig1.png"&gt;&lt;img alt="fig1" src="https://efavdb.com/wp-content/uploads/2015/07/fig1.png"&gt;&lt;/a&gt; Similarly, the following plots unemployment for each of the available different levels of&amp;nbsp;education.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;df[[&amp;#39;Rate&amp;#39;, &amp;#39;No High School Diploma&amp;#39;, &amp;#39;Bachelor degree and higher&amp;#39;]].plot()&lt;/span&gt;
&lt;span class="err"&gt;plt.ylabel(&amp;#39;Unemployment Rate (%)&amp;#39;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/07/fig2.png"&gt;&lt;img alt="fig2" src="https://efavdb.com/wp-content/uploads/2015/07/fig2.png"&gt;&lt;/a&gt;
Interestingly, these unemployment rates seem to evolve in a similar manner. Notice that both the green and the red curves seem to have doubled during the recent&amp;nbsp;slow-down.&lt;/p&gt;
&lt;h3&gt;GroupBy&lt;/h3&gt;
&lt;p&gt;You can also used Pandas GroupBy functionality to do analysis on subsets of the data.  For this example we &lt;a href="http://pandas.pydata.org/pandas-docs/stable/groupby.html"&gt;GroupBy&lt;/a&gt; year, and then make a plot showing the mean unemployment per year. GroupBy allows one to easily split the data, apply a function to each group, and then combine the results. It is a very useful&amp;nbsp;feature!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;df[&amp;#39;Year&amp;#39;]=(df.index.to_datetime()).year&lt;/span&gt;
&lt;span class="err"&gt;years = df.groupby(&amp;#39;Year&amp;#39;)&lt;/span&gt;
&lt;span class="err"&gt;years[&amp;#39;Rate&amp;#39;].mean().plot(kind=&amp;#39;bar&amp;#39;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/07/fig3.png"&gt;&lt;img alt="fig3" src="https://efavdb.com/wp-content/uploads/2015/07/fig3.png"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;There are some functions like mean, and describe that can be run directly on a grouped&amp;nbsp;object.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;years.get_group(2005)[&amp;#39;Rate&amp;#39;].describe()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;span class="err"&gt; count 12.000000&lt;/span&gt;
&lt;span class="err"&gt; mean 5.083333&lt;/span&gt;
&lt;span class="err"&gt; std 0.158592&lt;/span&gt;
&lt;span class="err"&gt; min 4.900000&lt;/span&gt;
&lt;span class="err"&gt; 25% 5.000000&lt;/span&gt;
&lt;span class="err"&gt; 50% 5.000000&lt;/span&gt;
&lt;span class="err"&gt; 75% 5.200000&lt;/span&gt;
&lt;span class="err"&gt; max 5.400000&lt;/span&gt;
&lt;span class="err"&gt; Name: Rate, dtype: float64&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;It is possible to apply any function to the grouped function using &lt;a href="http://pandas.pydata.org/pandas-docs/stable/groupby.html#aggregation"&gt;agg()&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;years[&amp;#39;Rate&amp;#39;].agg([np.mean, np.std, max, min])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;mean&lt;/th&gt;
&lt;th&gt;std&lt;/th&gt;
&lt;th&gt;max&lt;/th&gt;
&lt;th&gt;min&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Year&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2000&lt;/td&gt;
&lt;td&gt;3.963636&lt;/td&gt;
&lt;td&gt;0.092442&lt;/td&gt;
&lt;td&gt;4.1&lt;/td&gt;
&lt;td&gt;3.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2001&lt;/td&gt;
&lt;td&gt;4.741667&lt;/td&gt;
&lt;td&gt;0.528219&lt;/td&gt;
&lt;td&gt;5.7&lt;/td&gt;
&lt;td&gt;4.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2002&lt;/td&gt;
&lt;td&gt;5.783333&lt;/td&gt;
&lt;td&gt;0.102986&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;5.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2003&lt;/td&gt;
&lt;td&gt;5.991667&lt;/td&gt;
&lt;td&gt;0.178164&lt;/td&gt;
&lt;td&gt;6.3&lt;/td&gt;
&lt;td&gt;5.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2004&lt;/td&gt;
&lt;td&gt;5.541667&lt;/td&gt;
&lt;td&gt;0.131137&lt;/td&gt;
&lt;td&gt;5.8&lt;/td&gt;
&lt;td&gt;5.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2005&lt;/td&gt;
&lt;td&gt;5.083333&lt;/td&gt;
&lt;td&gt;0.158592&lt;/td&gt;
&lt;td&gt;5.4&lt;/td&gt;
&lt;td&gt;4.9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2006&lt;/td&gt;
&lt;td&gt;4.608333&lt;/td&gt;
&lt;td&gt;0.131137&lt;/td&gt;
&lt;td&gt;4.8&lt;/td&gt;
&lt;td&gt;4.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2007&lt;/td&gt;
&lt;td&gt;4.616667&lt;/td&gt;
&lt;td&gt;0.164225&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;4.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2008&lt;/td&gt;
&lt;td&gt;5.8&lt;/td&gt;
&lt;td&gt;0.780443&lt;/td&gt;
&lt;td&gt;7.3&lt;/td&gt;
&lt;td&gt;4.9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2009&lt;/td&gt;
&lt;td&gt;9.283333&lt;/td&gt;
&lt;td&gt;0.696528&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;7.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2010&lt;/td&gt;
&lt;td&gt;9.608333&lt;/td&gt;
&lt;td&gt;0.219331&lt;/td&gt;
&lt;td&gt;9.9&lt;/td&gt;
&lt;td&gt;9.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2011&lt;/td&gt;
&lt;td&gt;8.941667&lt;/td&gt;
&lt;td&gt;0.206522&lt;/td&gt;
&lt;td&gt;9.2&lt;/td&gt;
&lt;td&gt;8.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2012&lt;/td&gt;
&lt;td&gt;8.066667&lt;/td&gt;
&lt;td&gt;0.214617&lt;/td&gt;
&lt;td&gt;8.3&lt;/td&gt;
&lt;td&gt;7.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2013&lt;/td&gt;
&lt;td&gt;7.366667&lt;/td&gt;
&lt;td&gt;0.342008&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;6.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2014&lt;/td&gt;
&lt;td&gt;6.15&lt;/td&gt;
&lt;td&gt;0.360555&lt;/td&gt;
&lt;td&gt;6.7&lt;/td&gt;
&lt;td&gt;5.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2015&lt;/td&gt;
&lt;td&gt;5.4125&lt;/td&gt;
&lt;td&gt;0.180772&lt;/td&gt;
&lt;td&gt;5.7&lt;/td&gt;
&lt;td&gt;5.1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;Boolean&amp;nbsp;Indexing&lt;/h3&gt;
&lt;p&gt;Another common operation is the use of boolean vectors to filter data. This allows one to easily select subsets of data. It also provides a quick method for counting &amp;#8212; this works because True and False are represented as 1 and 0, respectively, when&amp;nbsp;adding.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;sum(df[&amp;#39;Rate&amp;#39;] &amp;gt; 7)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;&amp;gt;&amp;gt;&amp;gt; 59&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;String&amp;nbsp;Methods&lt;/h3&gt;
&lt;p&gt;Pandas has very useful string methods which can be access via &lt;code&gt;str.&lt;/code&gt; This makes it easy to look for patterns in the text, do filtering, replacements, and so on. I have a couple of examples below but I highly recommend taking a look at the documentation page for many &lt;a href="http://pandas.pydata.org/pandas-docs/stable/text.html"&gt;examples&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;s = pd.Series([&amp;#39;Dog&amp;#39;, &amp;#39;Bat&amp;#39;, &amp;#39;Coon&amp;#39;, &amp;#39;cAke&amp;#39;, &amp;#39;bAnk&amp;#39;, &amp;#39;CABA&amp;#39;, &amp;#39;dog&amp;#39;, &amp;#39;cat&amp;#39;])&lt;/span&gt;
&lt;span class="err"&gt;s[s.str.contains(&amp;#39;B&amp;#39;)]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;1 Bat&lt;/span&gt;
&lt;span class="err"&gt;5 CABA&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;s.str.replace(&amp;#39;dog|cat&amp;#39;, &amp;#39;nope &amp;#39;, case=False)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;0 nope&lt;/span&gt;
&lt;span class="err"&gt;1 Bat&lt;/span&gt;
&lt;span class="err"&gt;2 Coon&lt;/span&gt;
&lt;span class="err"&gt;3 cAke&lt;/span&gt;
&lt;span class="err"&gt;4 bAnk&lt;/span&gt;
&lt;span class="err"&gt;5 CABA&lt;/span&gt;
&lt;span class="err"&gt;6 nope&lt;/span&gt;
&lt;span class="err"&gt;7 nope&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;&lt;/h3&gt;
&lt;h3&gt;Wrap&amp;nbsp;Up&lt;/h3&gt;
&lt;p&gt;Pandas is a very useful library that I highly recommend. Although it can have a bit of a steep learning curve, it&amp;#8217;s actually pretty easy to pick up once you get started. Give it a shot, and you won&amp;#8217;t regret&amp;nbsp;it!&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/EFavDB/Pandas" title="GitHub Repo"&gt;&lt;img alt="Open GitHub Repo" src="https://efavdb.com/wp-content/uploads/2015/03/GitHub_Logo.png"&gt;&lt;/a&gt;&lt;/p&gt;</content><category term="Programming"></category><category term="methods"></category><category term="programming"></category><category term="tools"></category></entry><entry><title>Stochastic geometric series</title><link href="https://efavdb.com/stochastic-geometric-series" rel="alternate"></link><published>2015-09-13T06:00:00-07:00</published><updated>2015-09-13T06:00:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2015-09-13:/stochastic-geometric-series</id><summary type="html">&lt;p&gt;Let &lt;span class="math"&gt;\(a_1, a_2, \ldots\)&lt;/span&gt; be an infinite set of non-negative samples taken from a distribution &lt;span class="math"&gt;\(P_0(a)\)&lt;/span&gt;, and&amp;nbsp;write
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{1} \label{problem}
S = 1 + a_1 + a_1 a_2 + a_1 a_2 a_3 + \ldots.
$$&lt;/div&gt;
&lt;p&gt;
Notice that if the &lt;span class="math"&gt;\(a_i\)&lt;/span&gt; were all the same, &lt;span class="math"&gt;\(S\)&lt;/span&gt; would be a regular geometric series, with value …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Let &lt;span class="math"&gt;\(a_1, a_2, \ldots\)&lt;/span&gt; be an infinite set of non-negative samples taken from a distribution &lt;span class="math"&gt;\(P_0(a)\)&lt;/span&gt;, and&amp;nbsp;write
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{1} \label{problem}
S = 1 + a_1 + a_1 a_2 + a_1 a_2 a_3 + \ldots.
$$&lt;/div&gt;
&lt;p&gt;
Notice that if the &lt;span class="math"&gt;\(a_i\)&lt;/span&gt; were all the same, &lt;span class="math"&gt;\(S\)&lt;/span&gt; would be a regular geometric series, with value &lt;span class="math"&gt;\(S = \frac{1}{1-a}\)&lt;/span&gt;. How will the introduction of &lt;span class="math"&gt;\(a_i\)&lt;/span&gt; randomness change this sum? Will &lt;span class="math"&gt;\(S\)&lt;/span&gt; necessarily converge? How is &lt;span class="math"&gt;\(S\)&lt;/span&gt; distributed? In this post, we discuss some simple techniques to answer these&amp;nbsp;questions.&lt;/p&gt;
&lt;p&gt;Note: This post covers work done in collaboration with my aged p, S.&amp;nbsp;Landy.&lt;/p&gt;
&lt;h3&gt;Introduction &amp;#8212; a stock dividend&amp;nbsp;problem&lt;/h3&gt;
&lt;p&gt;To motivate the sum (\ref{problem}), consider the problem of evaluating the total output of a stock that pays dividends each year in proportion to its present value &amp;#8212; say &lt;span class="math"&gt;\(x %\)&lt;/span&gt;. The price dynamics of a typical stock can be reasonably modeled as a geometric random walk&lt;span class="math"&gt;\(^1\)&lt;/span&gt;:
&lt;/p&gt;
&lt;div class="math"&gt;$$\label{prod} \tag{2}
price(t) = price(t-1) * a_t,
$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(a_t\)&lt;/span&gt; is a random variable, having distribution &lt;span class="math"&gt;\(P_0(a_t)\)&lt;/span&gt;. Assuming this form for our hypothetical stock, its total lifetime dividends output will&amp;nbsp;be
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{3}
x \times \sum_{t = 0}^{\infty} price(t) = x \times price(0) \left ( 1 + a_1 + a_1 a_2 + a_1 a_2 a_3 + \ldots \right)
$$&lt;/div&gt;
&lt;p&gt;
The inner term in parentheses here is precisely (\ref{problem}). More generally, a series of this form will be of interest pretty much whenever geometric series are: Population growth problems, the length of a cylindrical bacterium at a series of time steps&lt;span class="math"&gt;\(^2\)&lt;/span&gt;, etc. Will the nature of these sums change dramatically through the introduction of growth&amp;nbsp;variance?&lt;/p&gt;
&lt;p&gt;To characterize these types of stochastic geometric series, we will start below by considering their moments: This will allow us to determine the average value of (\ref{problem}), it&amp;#8217;s variance etc. This approach will also allow us to determine a condition that is both necessary and sufficient for the sum&amp;#8217;s convergence. Following this, we will introduce an integral equation satisfied by the &lt;span class="math"&gt;\(P(S)\)&lt;/span&gt; distribution. We demonstrate its application by solving the equation for a simple&amp;nbsp;example.&lt;/p&gt;
&lt;h3&gt;The moments of &lt;span class="math"&gt;\(S\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;To solve for the moments of &lt;span class="math"&gt;\(S\)&lt;/span&gt;, we use a trick similar to that used to sum the regular geometric series: We&amp;nbsp;write
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{4} \label{trick}
S = 1 + a_1 + a_1 a_2 + \ldots \equiv 1 + a_1 T,
$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(T = 1 + a_2 + a_2 a_3 + \ldots.\)&lt;/span&gt; Now, because we assume that the &lt;span class="math"&gt;\(a_i\)&lt;/span&gt; are independent, it follows that &lt;span class="math"&gt;\(a_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(T\)&lt;/span&gt; are independent. Further, &lt;span class="math"&gt;\(S\)&lt;/span&gt; and &lt;span class="math"&gt;\(T\)&lt;/span&gt; are clearly distributed identically, since they take the same form. Subtracting &lt;span class="math"&gt;\(1\)&lt;/span&gt; from both sides of the above equation, these observations&amp;nbsp;imply
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{5} \label{moments}
\overline{(S-1)^k} = \sum_j {k \choose j} (-1)^j \overline{S^{k-j}} = \overline{ a^k S^k} = \overline{a^k} \ \overline{S^k}.
$$&lt;/div&gt;
&lt;p&gt;
This expression can be used to relate the moments of &lt;span class="math"&gt;\(S\)&lt;/span&gt; to those of &lt;span class="math"&gt;\(a\)&lt;/span&gt; &amp;#8212; a useful result, whenever the distribution of &lt;span class="math"&gt;\(a\)&lt;/span&gt; is known, allowing for the direct evaluation of its&amp;nbsp;moments.&lt;/p&gt;
&lt;p&gt;To illustrate, let us get the first couple of moments of &lt;span class="math"&gt;\(S\)&lt;/span&gt;, using (\ref{moments}). Setting &lt;span class="math"&gt;\(k=1\)&lt;/span&gt; above, we&amp;nbsp;obtain
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{6} \label{mean}
\overline{S -1} = \overline{a} \overline{S} \ \ \to \ \ \overline{S} = \frac{1}{1 - \overline{a}}
$$&lt;/div&gt;
&lt;p&gt;
The right side here looks just like the usual geometric sum result, with &lt;span class="math"&gt;\(a\)&lt;/span&gt; replaced by its average value. Similarly, setting &lt;span class="math"&gt;\(k =2\)&lt;/span&gt; in (\ref{moments}), we can solve for the second moment of &lt;span class="math"&gt;\(S\)&lt;/span&gt;. Subtracting the square of the first gives the following expression for the sum&amp;#8217;s&amp;nbsp;variance,
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{7} \label{var}
var(S) = \frac{var(a)}{(1 - \overline{a})^2(1 - \overline{a^2})}.
$$&lt;/div&gt;
&lt;p&gt;
As one might intuit, the variance of &lt;span class="math"&gt;\(S\)&lt;/span&gt; is proportional to the variance of &lt;span class="math"&gt;\(a\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Expressions (\ref{mean}) and (\ref{var}) are the most practical results of this post: They provide formal general expressions for the mean and variance for a sum of form (\ref{problem}). They can be used to provide a statistical estimate and error bar for a sum of form &lt;span class="math"&gt;\(S\)&lt;/span&gt; in any practical context. It is interesting/nice that the mean takes such a natural looking form &amp;#8212; one that many people likely make use of already, without putting much thought&amp;nbsp;into.&lt;/p&gt;
&lt;p&gt;The expressions above are also of some theoretical interest: Note, for example, that as &lt;span class="math"&gt;\(\overline{a} \to 1\)&lt;/span&gt; from below, the average value of &lt;span class="math"&gt;\(S\)&lt;/span&gt; diverges, and then becomes negative as &lt;span class="math"&gt;\(a\)&lt;/span&gt; goes above this value. This is clearly impossible, as &lt;span class="math"&gt;\(S\)&lt;/span&gt; is a sum of positive terms. This indicates that &lt;span class="math"&gt;\(S\)&lt;/span&gt; has no first moment whenever &lt;span class="math"&gt;\(\overline{a} \geq 1\)&lt;/span&gt;, while (\ref{mean}) holds whenever &lt;span class="math"&gt;\(\overline{a} &amp;lt; 1\)&lt;/span&gt;. Similarly, (\ref{var}) indicates that the second moment of &lt;span class="math"&gt;\(S\)&lt;/span&gt; exists and is finite whenever &lt;span class="math"&gt;\(\overline{a^2} &amp;lt; 1\)&lt;/span&gt;. In fact, this pattern continues for all &lt;span class="math"&gt;\(k\)&lt;/span&gt;: &lt;span class="math"&gt;\(\overline{S^k}\)&lt;/span&gt; exists and is finite if and only if &lt;span class="math"&gt;\(\overline{a^k} &amp;lt; 1\)&lt;/span&gt; &amp;#8212; a result that can be obtained from (\ref{moments}). A rigorous and elementary proof of these statements can be found in an earlier work by Szabados and Szekeley&lt;span class="math"&gt;\(^3\)&lt;/span&gt;. The simple moment equation (\ref{moments}) can also be found&amp;nbsp;there.&lt;/p&gt;
&lt;h3&gt;Condition for the convergence of &lt;span class="math"&gt;\(S\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;A simple condition for the convergence of &lt;span class="math"&gt;\(S\)&lt;/span&gt; can also be obtained using (\ref{moments}). The trick is to consider the limit as &lt;span class="math"&gt;\(k\)&lt;/span&gt; goes to zero of the &lt;span class="math"&gt;\(k\)&lt;/span&gt;-th moments. This gives, for example, the average of &lt;span class="math"&gt;\(1\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(P(S)\)&lt;/span&gt;. If this is finite, then the distribution of &lt;span class="math"&gt;\(P\)&lt;/span&gt; is normalizable. Otherwise, &lt;span class="math"&gt;\(S\)&lt;/span&gt; must diverge: Setting &lt;span class="math"&gt;\(k = \epsilon\)&lt;/span&gt; in (\ref{moments}), expanding to first order in &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;&amp;nbsp;gives
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{8} \label{approximate_log}
\overline{ \exp [\epsilon \log (S -1) ]} \sim \overline{ 1 + \epsilon \log (S -1) } \sim \overline{ 1 + \epsilon \log S } \ \overline{ 1 + \epsilon \log a}.
$$&lt;/div&gt;
&lt;p&gt;
Solving for &lt;span class="math"&gt;\(\overline{1}_S\)&lt;/span&gt;, the average of &lt;span class="math"&gt;\(1\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(P(S)\)&lt;/span&gt;,&amp;nbsp;gives
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{9}
\overline{1}_S = \frac{\overline{\log( 1 - \frac{1}{S})}}{\log a} + O(\epsilon).
$$&lt;/div&gt;
&lt;p&gt;
Like the integer moment expressions above, the right side here is finite up to the point where its denominator diverges. That is, the series will converge, if and only if &lt;span class="math"&gt;\(\overline{\log a} &amp;lt; 0\)&lt;/span&gt;, a very simple condition&lt;span class="math"&gt;\(^4\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;Integral equation for the distribution &lt;span class="math"&gt;\(P(S)\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;We have also found that one can sometimes go beyond solving for the moments of &lt;span class="math"&gt;\(S\)&lt;/span&gt;, and instead solve directly for its full distribution: Integrating (\ref{trick}) over &lt;span class="math"&gt;\(a\)&lt;/span&gt;&amp;nbsp;gives
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
\tag{10} \label{int}
P(S_0) &amp;amp;=&amp;amp; \int da P_0(a) \int dS P(S) \delta(1+ a S - S_0) \\
&amp;amp;=&amp;amp; \int \frac{da}{a} P_0(a) P \left (\frac{S_0 -1}{a} \right).
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
This is a general, linear integral equation for &lt;span class="math"&gt;\(P(S)\)&lt;/span&gt;. At least in some cases, it can solved in closed-form. An example&amp;nbsp;follows.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Uniformly distributed &lt;span class="math"&gt;\(a_i\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To demonstrate how one might solve the equation (\ref{int}), we consider here the case where the &lt;span class="math"&gt;\(a_i\)&lt;/span&gt; are uniform on &lt;span class="math"&gt;\([0,1]\)&lt;/span&gt;. In this case, writing &lt;span class="math"&gt;\(a = \frac{S_0 -1}{v}\)&lt;/span&gt;, (\ref{int}) goes&amp;nbsp;to
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{11} \label{int2}
P(S_0) = \int_{S_0-1}^{\infty} P\left (v\right) \frac{1}{v}dv.
$$&lt;/div&gt;
&lt;p&gt;
To progress, we differentiate with respect to &lt;span class="math"&gt;\(S_0\)&lt;/span&gt;, which&amp;nbsp;gives
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{12} \label{delay}
P^{\prime} (S_0)\equiv - \frac{1 }{S_0 -1}\times P\left (S_0 -1\right).
$$&lt;/div&gt;
&lt;p&gt;
Equation (\ref{delay}) is a &lt;a href="https://en.wikipedia.org/wiki/Delay_differential_equation"&gt;delay differential equation&lt;/a&gt;. It can be solved through iterated integrations: To initiate the process, we note that &lt;span class="math"&gt;\(P(S_0)\)&lt;/span&gt; is equal to zero for all &lt;span class="math"&gt;\(S_0&amp;lt; 1\)&lt;/span&gt;. Plugging this observation into (\ref{delay}) implies that &lt;span class="math"&gt;\(P(S_0) \equiv J\)&lt;/span&gt; &amp;#8212; some constant &amp;#8212; for &lt;span class="math"&gt;\(S \in (1,2)\)&lt;/span&gt;. Continuing in this fashion, repeated integrations of (\ref{delay})&amp;nbsp;gives
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{13}
P (S_0) = \begin{cases}
J, \ \ \ S_0 \in (1,2) \\
J[1 - \log (S_0 -1)], \ \ \ S_0 \in (2,3) \\
J \left [ 1 - \log(S_0 - 1) + Li_2(2-S_0) + \frac{ \log(S_0 - 2)}{\log(S_0 - 1)} - Li_2(-1) \right ], \ \ S_0 \in (3,4) \\
\ldots,
\end{cases}
$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(Li_2\)&lt;/span&gt; is the polylogarithm&amp;nbsp;function.&lt;/p&gt;
&lt;p&gt;In practice, to find &lt;span class="math"&gt;\(J\)&lt;/span&gt; one can solve (\ref{delay}) numerically, requiring &lt;span class="math"&gt;\(P(S)\)&lt;/span&gt; to be normalized. The figure below compares the result to a simulation estimate, obtained via binning the results of 250,000 random sums of form (\ref{problem}). The two agree&amp;nbsp;nicely.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/08/Screen-Shot-2015-08-16-at-10.34.46-PM.png"&gt;&lt;img alt="Screen Shot 2015-08-16 at 10.34.46 PM" src="https://efavdb.com/wp-content/uploads/2015/08/Screen-Shot-2015-08-16-at-10.34.46-PM.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Discussion&lt;/h3&gt;
&lt;p&gt;Consideration of this problem was motivated by a geometric series of type (\ref{problem}) that arose in my work at Square. In this case, I was interested in understanding the bias and variance in the natural estimate (\ref{mean}) to this problem. After some weeks of tinkering with S Landy, I was delighted to find that rigorous, simple results could be obtained to characterize these sums, the simplest being the moment and convergence results above. We now realize that these particular issues have already been well- (and better-)studied, by others&lt;span class="math"&gt;\(^3\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As for the integral equation approach, we have not found any other works aimed at solving this problem in general. The method discussed in the example above can be used for any &lt;span class="math"&gt;\(P_0(a)\)&lt;/span&gt; that is uniform over a finite segment. We have also found solutions for a few other cases. Unfortunately, we have so far been unable to obtain a formal, general solution in closed form. However, we note that standard iterative approaches can always be used to estimate the solution to (\ref{int}). Finally, in cases where all moments exist, these can also be used to determine &lt;span class="math"&gt;\(P\)&lt;/span&gt;.&lt;/p&gt;
&lt;h4&gt;References and&amp;nbsp;comments&lt;/h4&gt;
&lt;p&gt;[1] For a discussion on the geometric random walk model for stocks, see &lt;a href="http://people.duke.edu/~rnau/411georw.htm"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;[2] Elongated bacteria &amp;#8212; eg., e. coli &amp;#8212; grow longer at an exponential rate &amp;#8212; see my &lt;a href="https://www.sites.google.com/site/Jonathan Landy/%282014%29cellgrowth.pdf?attredirects=0&amp;amp;d=1"&gt;paper on how cell shape affects growth rates&lt;/a&gt;. Due to randomness inherent in the growth rates, bacteria populations will have a length distribution, similar in form to &lt;span class="math"&gt;\(P(S)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;[3] &amp;#8220;An exponential functional of random walks&amp;#8221; by Szabados and Szekeley, Journal of Applied Probability&amp;nbsp;2003.&lt;/p&gt;
&lt;p&gt;[4] Although we have given only a hand-waving argument for this result, the authors of [3] state &amp;#8212; and give a reference for &amp;#8212; the fact that it can be proven using the law of large numbers: By independence of the &lt;span class="math"&gt;\(a_i\)&lt;/span&gt;, the &lt;span class="math"&gt;\(k\)&lt;/span&gt;-th term in the series approaches &lt;span class="math"&gt;\((\overline{\log a})^k\)&lt;/span&gt; with probability one, at large &lt;span class="math"&gt;\(k\)&lt;/span&gt;. Simple convergence criteria then give the&amp;nbsp;result.&lt;/p&gt;
&lt;p&gt;[5] The moment equation (\ref{moments}) can also be obtained from the integral equation (\ref{int}), where it arrises from the application of the convolution&amp;nbsp;theorem.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Finance"></category><category term="statistics"></category></entry><entry><title>Build a web scraper for a literature search - from soup to nuts</title><link href="https://efavdb.com/build-a-web-scraper-lit-search" rel="alternate"></link><published>2015-08-25T17:43:00-07:00</published><updated>2015-08-25T17:43:00-07:00</updated><author><name>Cathy Yeh</name></author><id>tag:efavdb.com,2015-08-25:/build-a-web-scraper-lit-search</id><summary type="html">&lt;p&gt;&lt;em&gt;Code, references, and examples of this project are on &lt;a href="https://github.com/EFavDB/PubmedCentral_Scraper"&gt;Github&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In this post, I&amp;#8217;ll describe the soup to nuts process of automating a literature search in &lt;a href="http://www.ncbi.nlm.nih.gov/pmc/"&gt;Pubmed Central&lt;/a&gt; using&amp;nbsp;R.&lt;/p&gt;
&lt;p&gt;It feels deeply satisfying to sit back and let the code do the dirty&amp;nbsp;work.&lt;/p&gt;
&lt;p&gt;Is it as satisfying …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;Code, references, and examples of this project are on &lt;a href="https://github.com/EFavDB/PubmedCentral_Scraper"&gt;Github&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In this post, I&amp;#8217;ll describe the soup to nuts process of automating a literature search in &lt;a href="http://www.ncbi.nlm.nih.gov/pmc/"&gt;Pubmed Central&lt;/a&gt; using&amp;nbsp;R.&lt;/p&gt;
&lt;p&gt;It feels deeply satisfying to sit back and let the code do the dirty&amp;nbsp;work.&lt;/p&gt;
&lt;p&gt;Is it as satisfying as a bowl of red-braised beef noodle soup with melt-in-your-mouth tendons from Taipei&amp;#8217;s Yong Kang Restaurant (featured&amp;nbsp;image)?&lt;/p&gt;
&lt;p&gt;If you have to do a lit search like this more than once, then I have to say the answer is yes &amp;#8212; unequivocally,&amp;nbsp;yes.&lt;/p&gt;
&lt;p&gt;The three components of the project&amp;nbsp;are&lt;/p&gt;
&lt;p&gt;&lt;a href="#Section1"&gt;I. Design a database to store the contents of a scraping session&lt;/a&gt;
&lt;a href="#Section2"&gt;&lt;span class="caps"&gt;II&lt;/span&gt;. Extract information via an &lt;span class="caps"&gt;API&lt;/span&gt; and web scraper&lt;/a&gt;
&lt;a href="#Section3"&gt;&lt;span class="caps"&gt;III&lt;/span&gt;. Generate summary&amp;nbsp;reports&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you want to skip the explanations and go straight to using the program, check out the quick-start &lt;span class="caps"&gt;HTML5&lt;/span&gt; &lt;a href="https://efavdb.com/wp-content/uploads/2015/08/PubmedCentralSlides.html"&gt;presentation&lt;/a&gt; or &lt;a href="https://efavdb.com/wp-content/uploads/2015/08/scraper_manual.html"&gt;manual&lt;/a&gt; and an example of a &lt;a href="https://github.com/EFavDB/PubmedCentral_Scraper/blob/master/example/scraper_TGI_plots_for_trastuzumab.md"&gt;report&lt;/a&gt; generated by this&amp;nbsp;project.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;The task is to capture plots of tumor growth inhibition (&lt;span class="caps"&gt;TGI&lt;/span&gt;), i.e. tumor growth as a function of time, in animals treated with a particular cancer drug, then aggregate all the plots in a summary&amp;nbsp;report.&lt;/p&gt;
&lt;p&gt;An example of a &lt;span class="caps"&gt;TGI&lt;/span&gt; plot is provided below (source: &lt;a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3792566/" title="PMC3792566"&gt;Qi 2011&lt;/a&gt;) for &lt;span class="caps"&gt;TGI&lt;/span&gt; in mice treated with the cancer drug,&amp;nbsp;Docetaxel.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/07/TGIplot.png"&gt;&lt;img alt="" src="https://efavdb.com/wp-content/uploads/2015/07/TGIplot-300x217.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="caps"&gt;TGI&lt;/span&gt; plots for several drugs (image credit: Qi&amp;nbsp;2011)&lt;/p&gt;
&lt;p&gt;Since the scraper was intended for a casual (non-exhaustive) literature search, I decided to confine the search to online articles in Pubmed Central (as opposed to Pubmed in general) since they are entirely open-access and available in a mostly uniform&amp;nbsp;format.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;I. Set up the the&amp;nbsp;database&lt;/h2&gt;
&lt;p&gt;This project called for data storage beyond the scope of data frames and external flat files, e.g. Excel spreadsheets or csv files, since the following attributes were required of the&amp;nbsp;data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Persistence outside the R environment =&amp;gt; data frames&amp;nbsp;unsuitable&lt;/li&gt;
&lt;li&gt;Ease of access and manipulation =&amp;gt; writing to and reading from text/csv files would be&amp;nbsp;cumbersome&lt;/li&gt;
&lt;li&gt;The data would be structured =&amp;gt; relational&amp;nbsp;database&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With a view towards expediency, we cover just enough to get things up and running and leave the finer details of relational database design to the&amp;nbsp;experts.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.sqlite.org/about.html" title="SQLite"&gt;SQLite&lt;/a&gt; is well-suited for this small project; it&amp;#8217;s self-contained and doesn&amp;#8217;t require fussing with servers. To lower the barrier even further, the &lt;a href="http://cran.r-project.org/web/packages/RSQLite/index.html" title="RSQLite"&gt;RSQLite&lt;/a&gt; package embeds SQLite in R (no separate installation needed) and allows you to very easily interface with your database within the R environment. The database itself is stored in a single file on your hard disk and easily&amp;nbsp;transferred.&lt;/p&gt;
&lt;h3&gt;What data will be&amp;nbsp;collected?&lt;/h3&gt;
&lt;p&gt;The first step is to decide what data should be collected during the web scraping process. We want to aggregate images of plots in a (html)&amp;nbsp;report.&lt;/p&gt;
&lt;p&gt;However, downloading the images themselves is inefficient; instead, we&amp;#8217;ll just grab the image URLs on Pubmed Central. (The image URLs are useful because they can be referred to by markdown code to embed the images in the html&amp;nbsp;report.)&lt;/p&gt;
&lt;p&gt;The image urls should be captured, along with associated information&amp;nbsp;below:&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;image data&lt;/strong&gt;                         image url, figure name in the article, image caption
  &lt;strong&gt;article metadata&lt;/strong&gt;                   Pubmed Central id, &lt;span class="caps"&gt;DOI&lt;/span&gt;, title, journal, year of publication, authors, abstracts, keywords
  &lt;strong&gt;search criteria met by the image&lt;/strong&gt;   topic/drug, type of&amp;nbsp;plot&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;The last point addresses the foreseeable need to be able to modify the search parameters. In the next section, we allow for the possibility that the same image might show up for more than one kind of drug or plot&amp;nbsp;type.&lt;/p&gt;
&lt;h3&gt;Decide on a layout for the&amp;nbsp;database&lt;/h3&gt;
&lt;p&gt;After pinning down the content itself, the next step is to decide how it should be arranged in the database.&amp;nbsp;Namely,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What tables are&amp;nbsp;needed?&lt;/li&gt;
&lt;li&gt;Which fields go in which&amp;nbsp;tables?&lt;/li&gt;
&lt;li&gt;How do the tables relate to one&amp;nbsp;another?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This particularly helpful &lt;a href="http://db.grussell.org/section008.html" title="normalization"&gt;page&lt;/a&gt; walks you through the concept of database normalization by providing lots of concrete examples, including &amp;#8220;anomalies&amp;#8221; that arise in poorly designed&amp;nbsp;databases.&lt;/p&gt;
&lt;p&gt;Some ideas on normalization are intuitive. Let&amp;#8217;s take a look at how to restructure a table to satisfy first normal form (&lt;span class="caps"&gt;1NF&lt;/span&gt;). The table below is not in &lt;span class="caps"&gt;1NF&lt;/span&gt; because it contains sets of values within single&amp;nbsp;rows.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;student_id&lt;/th&gt;
&lt;th&gt;name&lt;/th&gt;
&lt;th&gt;subjects&lt;/th&gt;
&lt;th&gt;grades&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1234&lt;/td&gt;
&lt;td&gt;Andrew&lt;/td&gt;
&lt;td&gt;a.i. &lt;br&gt; linear algebra &lt;br&gt; physics&lt;/td&gt;
&lt;td&gt;A &lt;br&gt; A &lt;br&gt; B+&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5678&lt;/td&gt;
&lt;td&gt;Yaser&lt;/td&gt;
&lt;td&gt;statistics &lt;br&gt; algorithms&lt;/td&gt;
&lt;td&gt;A- &lt;br&gt; A&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Instead, we can break it up into two&amp;nbsp;tables:&lt;/p&gt;
&lt;p&gt;table:&amp;nbsp;student&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;student_id&lt;/th&gt;
&lt;th&gt;name&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1234&lt;/td&gt;
&lt;td&gt;Andrew&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5678&lt;/td&gt;
&lt;td&gt;Yaser&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;table:grades:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;student_id&lt;/th&gt;
&lt;th&gt;subject&lt;/th&gt;
&lt;th&gt;grade&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1234&lt;/td&gt;
&lt;td&gt;a.i.&lt;/td&gt;
&lt;td&gt;A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1234&lt;/td&gt;
&lt;td&gt;linear algebra&lt;/td&gt;
&lt;td&gt;A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1234&lt;/td&gt;
&lt;td&gt;physics&lt;/td&gt;
&lt;td&gt;B+&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5678&lt;/td&gt;
&lt;td&gt;statistics&lt;/td&gt;
&lt;td&gt;A-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5678&lt;/td&gt;
&lt;td&gt;algorithms&lt;/td&gt;
&lt;td&gt;A&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Column names that are primary keys are underlined. A primary key is a column, or combination of columns, whose values uniquely identify a row in a table (and accordingly guards against duplicate rows). In a first go at a schema, a table without a logical primary key reared its ugly&amp;nbsp;head:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;pmcid&lt;/th&gt;
&lt;th&gt;topic&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;123456&lt;/td&gt;
&lt;td&gt;drug A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;123456&lt;/td&gt;
&lt;td&gt;drug B&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;100000&lt;/td&gt;
&lt;td&gt;drug A&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Note that the &lt;strong&gt;pmcid&lt;/strong&gt; value is not guaranteed to be unique in the above table because the same article may show up in searches for multiple drugs. This situation hinted at the need to restructure the tables. We finally settled on the three tables below, which all had natural primary keys&amp;nbsp;(underlined):&lt;/p&gt;
&lt;p&gt;table:&amp;nbsp;article&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;pmcid&lt;/th&gt;
&lt;th&gt;doi&lt;/th&gt;
&lt;th&gt;title&lt;/th&gt;
&lt;th&gt;journal&lt;/th&gt;
&lt;th&gt;year&lt;/th&gt;
&lt;th&gt;authors&lt;/th&gt;
&lt;th&gt;abstract&lt;/th&gt;
&lt;th&gt;keywords&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;table:&amp;nbsp;figure&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;topic&lt;/th&gt;
&lt;th&gt;plot_type&lt;/th&gt;
&lt;th&gt;img_url&lt;/th&gt;
&lt;th&gt;pmc_id&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;table:&amp;nbsp;figure_text&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;img_url&lt;/th&gt;
&lt;th&gt;fig_name&lt;/th&gt;
&lt;th&gt;caption&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/07/a_hanging.jpg"&gt;&lt;img alt="" src="https://efavdb.com/wp-content/uploads/2015/07/a_hanging-263x300.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Their tables were not in &lt;span class="caps"&gt;1NF&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The tables &amp;#8220;figure&amp;#8221; and &amp;#8220;figure_text&amp;#8221; are kept separate in order to minimize redundancies. For instance, the same img_url can appear in the table &amp;#8220;figure&amp;#8221; multiple times if it matches a number of different drugs or plot types, but its caption would only need to be stored once in the table&amp;nbsp;&amp;#8220;figure_text&amp;#8221;.&lt;/p&gt;
&lt;p&gt;The table &amp;#8220;figure&amp;#8221; is not in second normal form (&lt;span class="caps"&gt;2NF&lt;/span&gt;) because of a partial key dependency; the pmcid field only depends on img_url, rather than the entire composite key {topic, plot_type, and&amp;nbsp;img_url}.&lt;/p&gt;
&lt;p&gt;Although the normalization rules sound a bit intimidating, they are just guidelines&amp;#8212;apparently, one can even get carried away with&amp;nbsp;over-normalizing.&lt;/p&gt;
&lt;p&gt;The database has held up fine so far, but any suggestions on how to improve the design are very&amp;nbsp;welcome!&lt;/p&gt;
&lt;h3&gt;Creating a SQLite database in&amp;nbsp;R&lt;/h3&gt;
&lt;p&gt;With a schema in hand, creating the SQLite database in R is a matter of minutes. First, we load the packages for interfacing with the&amp;nbsp;database.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;DBI&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;RSQLite&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Then we create a connection to the database, which we&amp;#8217;ll call&amp;nbsp;&amp;#8220;myDb.sqlite&amp;#8221;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;con&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;dbConnect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;SQLite&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;dbname&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;myDb.sqlite&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Next, we create the three tables &amp;#8220;article&amp;#8221;, &amp;#8220;figure&amp;#8221;, and&amp;nbsp;&amp;#8220;figure_text&amp;#8221;.&lt;/p&gt;
&lt;p&gt;create figure_text&amp;nbsp;table:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;query&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;CREATE TABLE figure_text(img_url TEXT, fig_name TEXT, caption TEXT, PRIMARY KEY(img_url))&amp;#39;&lt;/span&gt;
&lt;span class="nf"&gt;dbGetQuery&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;con&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;create figure&amp;nbsp;table:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;query&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;CREATE TABLE figure(topic TEXT, plot_type TEXT, img_url TEXT, pmcid INTEGER, PRIMARY KEY(topic, plot_type, img_url))&amp;#39;&lt;/span&gt;
&lt;span class="nf"&gt;dbGetQuery&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;con&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;create article&amp;nbsp;table:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;query&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;CREATE TABLE article(pmcid INTEGER, doi TEXT, title TEXT, journal TEXT, year INTEGER, authors TEXT, abstract TEXT, keywords TEXT, PRIMARY KEY(pmcid))&amp;#39;&lt;/span&gt;
&lt;span class="nf"&gt;dbGetQuery&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;con&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Last, we close the connection to the&amp;nbsp;database.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nf"&gt;dbDisconnect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;con&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The SQLite database is now ready to be used! The script above is available on github as &lt;code&gt;createSQLiteDatabase.R&lt;/code&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;span class="caps"&gt;II&lt;/span&gt;. Scrape Pubmed Central&amp;nbsp;articles&lt;/h2&gt;
&lt;p&gt;The script &lt;code&gt;pubmedcentral_scraper.R&lt;/code&gt; is where the action happens. It takes input from the user to query the Pubmed Central Database, scrape articles, and load the extracted information into the&amp;nbsp;database.&lt;/p&gt;
&lt;h3&gt;Input keywords for literature search and labels in&amp;nbsp;database&lt;/h3&gt;
&lt;p&gt;The user input section is shown&amp;nbsp;below.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;## &amp;lt;---------USER INPUT STARTS HERE---------&amp;gt;&lt;/span&gt;

&lt;span class="c1"&gt;## name of database where scraper results are stored&lt;/span&gt;
&lt;span class="n"&gt;database.name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;myDb.sqlite&amp;quot;&lt;/span&gt;

&lt;span class="c1"&gt;## maximum number of results to retrieve from query&lt;/span&gt;
&lt;span class="n"&gt;retmax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;10&lt;/span&gt;

&lt;span class="c1"&gt;## topic terms to be queried via the pubmed search engine&lt;/span&gt;
&lt;span class="n"&gt;query.topic&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Docetaxel&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Docetaxol&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;## keywords to identify plot type to be captured&lt;/span&gt;
&lt;span class="c1"&gt;## terms should be lower-case&lt;/span&gt;
&lt;span class="n"&gt;query.plottype&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;tumor growth&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;tumor volume&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="s"&gt;&amp;quot;tumor size&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;tumor inhibition&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="s"&gt;&amp;quot;tumor growth inhibition&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;tgi&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="s"&gt;&amp;quot;tumor response&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;tumor regression&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The user input for variables &lt;code&gt;query.topic&lt;/code&gt; and &lt;code&gt;query.plottype&lt;/code&gt; are used to construct the query to Pubmed Central via the Entrez Programming Utilities interface (details in the &lt;a href="http://www.ncbi.nlm.nih.gov/books/NBK25499/"&gt;E-utilities guide&lt;/a&gt;), made available through the &lt;a href="http://www.ncbi.nlm.nih.gov/"&gt;National Center for Biotechnology&lt;/a&gt; (&lt;span class="caps"&gt;NCBI&lt;/span&gt;). To maximize hits to the query, the user can supply multiple terms for each&amp;nbsp;variable.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;## topic/drug label for database&lt;/span&gt;
&lt;span class="n"&gt;topic&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Docetaxel&amp;quot;&lt;/span&gt;

&lt;span class="c1"&gt;## plot type label for database&lt;/span&gt;
&lt;span class="n"&gt;plot_type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;TGI&amp;quot;&lt;/span&gt;

&lt;span class="c1"&gt;## &amp;lt;---------USER INPUT ENDS HERE-----------&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The variables &lt;code&gt;topic&lt;/code&gt; and &lt;code&gt;plot_type&lt;/code&gt; label the data in the SQLite database (the labels should be consistent between queries in order to simplify the information retrieval process, e.g. stick to one spelling convention for a particular drug, like &amp;#8220;Docetaxel&amp;#8221;, in&amp;nbsp;myDb.sqlite).&lt;/p&gt;
&lt;p&gt;The first E-utility we will use is ESearch, which returns the &lt;span class="caps"&gt;PMC&lt;/span&gt; ids of articles matching a query, along with other metadata. The E-utilities &lt;span class="caps"&gt;API&lt;/span&gt; is extremely easy to use. Simply string together the set of parameters (&lt;span class="caps"&gt;NCBI&lt;/span&gt; database name, utility name, etc.) and go to the &lt;span class="caps"&gt;URL&lt;/span&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;## compose url for eSearch&lt;/span&gt;
&lt;span class="n"&gt;url.esearch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;paste0&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url.base&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;esearch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;db&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&amp;amp;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;retmax&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&amp;amp;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sortmethod&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&amp;amp;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;## get and parse xml data returned by eSearch&lt;/span&gt;
&lt;span class="n"&gt;data.esearch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;getURL&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url.esearch&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The explicit &lt;span class="caps"&gt;URL&lt;/span&gt; constructed from the above example user input is:
&lt;em&gt;http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pmc&amp;amp;retmax=10&amp;amp;sort=relevance&amp;amp;term=(&amp;#34;Docetaxel&amp;#34;+&lt;span class="caps"&gt;OR&lt;/span&gt;+&amp;#34;Docetaxol&amp;#34;)+&lt;span class="caps"&gt;AND&lt;/span&gt;+(&amp;#34;tumor+growth&amp;#34;+&lt;span class="caps"&gt;OR&lt;/span&gt;+&amp;#34;tumor+volume&amp;#34;+&lt;span class="caps"&gt;OR&lt;/span&gt;+&amp;#34;tumor+size&amp;#34;+&lt;span class="caps"&gt;OR&lt;/span&gt;+&amp;#34;tumor+inhibition&amp;#34;+&lt;span class="caps"&gt;OR&lt;/span&gt;+&amp;#34;tumor+growth+inhibition&amp;#34;+&lt;span class="caps"&gt;OR&lt;/span&gt;+&amp;#34;tgi&amp;#34;+&lt;span class="caps"&gt;OR&lt;/span&gt;+&amp;#34;tumor+response&amp;#34;+&lt;span class="caps"&gt;OR&lt;/span&gt;+&amp;#34;tumor+regression&amp;#34;)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Try copying and pasting the above &lt;span class="caps"&gt;URL&lt;/span&gt; in your browser to see a sample of the xml file returned by E-utilities. Here&amp;#8217;s an excerpt of the &lt;span class="caps"&gt;XML&lt;/span&gt; document from a query to ESearch on August 25,&amp;nbsp;2015:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/08/esearchXML.jpg"&gt;&lt;img alt="" src="https://efavdb.com/wp-content/uploads/2015/08/esearchXML-164x300.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="caps"&gt;XML&lt;/span&gt; output from a query to &lt;span class="caps"&gt;PMC&lt;/span&gt; via the ESearch &lt;span class="caps"&gt;API&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We extract the &lt;span class="caps"&gt;PMC&lt;/span&gt; ids, which are sandwiched between the &lt;Id&gt; &lt;span class="caps"&gt;XML&lt;/span&gt; tags, using functions from the &lt;span class="caps"&gt;XML&lt;/span&gt; and rvest&amp;nbsp;packages:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;data.xml&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;xmlParse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data.esearch&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;## get pmcid&amp;#39;s&lt;/span&gt;
&lt;span class="n"&gt;pmcids&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data.xml&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt; &lt;span class="nf"&gt;xml_nodes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Id&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt; &lt;span class="nf"&gt;xml_text&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;code&gt;%&amp;gt;%&lt;/code&gt; is a pipe operator for chaining commands (from the &lt;a href="https://cran.r-project.org/web/packages/magrittr/index.html"&gt;magrittr&lt;/a&gt;&amp;nbsp;package).&lt;/p&gt;
&lt;p&gt;The URLs of the html article can be simply constructed from their &lt;span class="caps"&gt;PMC&lt;/span&gt; ids. For example, the html version of the article with &lt;span class="caps"&gt;PMC&lt;/span&gt; id 3792566 is found at:&amp;nbsp;http://www.ncbi.nlm.nih.gov/pmc/articles/3792566&lt;/p&gt;
&lt;h3&gt;Scrape &lt;span class="caps"&gt;HTML&lt;/span&gt;&amp;nbsp;articles&lt;/h3&gt;
&lt;p&gt;The scraping of the &lt;span class="caps"&gt;HTML&lt;/span&gt; article is performed by &lt;code&gt;scrapeArticle.R&lt;/code&gt;. Note, &lt;span class="caps"&gt;PMC&lt;/span&gt; ids returned by ESearch which have already been scraped for that particular combination of search terms are&amp;nbsp;skipped.&lt;/p&gt;
&lt;p&gt;The html version of the &lt;span class="caps"&gt;PMC&lt;/span&gt; articles only show excerpts of captions, so we have to extract the individual figure URLs in order to scrape their full captions (and search for keyword matches). In order to extract a data element from an html document, we need to identify the tag associated with that&amp;nbsp;element.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://selectorgadget.com/"&gt;SelectorGadget&lt;/a&gt; is a nifty tool to help you hone in on the &lt;span class="caps"&gt;CSS&lt;/span&gt; selectors of interest. Installation is ridiculously easy: just drag the link on the SelectorGadget page to your browser bookmark&amp;nbsp;bar!&lt;/p&gt;
&lt;p&gt;For example, let&amp;#8217;s identify the &lt;span class="caps"&gt;CSS&lt;/span&gt; selector for figure URLs using SelectorGadget in 3 clicks of the mouse. We&amp;#8217;ll demo SelectorGadget on a &lt;span class="caps"&gt;PMC&lt;/span&gt; &lt;a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3792566/"&gt;article&lt;/a&gt; that is returned in a query on Docetaxel and &lt;span class="caps"&gt;TGI&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In the screenshot below, I clicked on the &amp;#8220;Figure 3&amp;#8221; link as a starting point for selecting all such figure URLs. SelectorGadget identified the element as &amp;#8220;.figpopup&amp;#8221; in the gray toolbar at the bottom of the screenshot, highlighted the direct click in green, and highlighted all the other elements in yellow (total: 35 elements). Notice, however, that two links to Figure 4 have been automatically highlighted in the screenshot, one of which is a reference in the body of the&amp;nbsp;text.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/08/selectorgadg1.png"&gt;&lt;img alt="A click of the &amp;quot;Figure 3&amp;quot; link next to thumbnail highlights it in green, and similar elements are automatically highlighted in yellow." src="https://efavdb.com/wp-content/uploads/2015/08/selectorgadg1.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A click of the &amp;#8220;Figure 3&amp;#8221; link next to thumbnail highlights it in green. Similar elements are automatically highlighted in&amp;nbsp;yellow.&lt;/p&gt;
&lt;p&gt;To reduce the number of redundant figure &lt;span class="caps"&gt;URL&lt;/span&gt; links, I then clicked on the Figure 4 link in the body of the text in order to exclude it; it is accordingly highlighted in red to signify its&amp;nbsp;exclusion.&lt;/p&gt;
&lt;p&gt;The pattern-matching is momentarily worsened since the link to Figure 4 (bottom) is no longer highlighted. SelectorGadget&amp;#8217;s guess for the &lt;span class="caps"&gt;CSS&lt;/span&gt; selector becomes &amp;#8220;#lgnd_F3 .figpopup&amp;#8221;, of which there is only one element, highlighted in&amp;nbsp;green.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/08/selectorgadg2.png"&gt;&lt;img alt="Elements excluded from the pattern matching are highlighted in red." src="https://efavdb.com/wp-content/uploads/2015/08/selectorgadg2.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Elements excluded from the pattern matching are highlighted in&amp;nbsp;red.&lt;/p&gt;
&lt;p&gt;After making the pattern match more specific with an exclusion, we have to re-generalize by re-selecting the Figure 4 bottom link. This time, SelectorGadget gets the pattern right with the &lt;span class="caps"&gt;CSS&lt;/span&gt; selector &amp;#8220;.icnblk_cntnt .figpopup&amp;#8221;, which describes 5 elements on the&amp;nbsp;page.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/08/selectorgadg3.png"&gt;&lt;img alt="Third time's the charm: SelectorGadget has honed in on the CSS selectors that match the desired figure URLs." src="https://efavdb.com/wp-content/uploads/2015/08/selectorgadg3.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Third time&amp;#8217;s the charm: SelectorGadget has honed in on the &lt;span class="caps"&gt;CSS&lt;/span&gt; selectors that match the desired figure&amp;nbsp;URLs.&lt;/p&gt;
&lt;p&gt;Using rvest&amp;#8217;s &lt;code&gt;xml_nodes&lt;/code&gt; function, we extract components characterized by the &lt;span class="caps"&gt;CSS&lt;/span&gt; selector &lt;code&gt;.icnblk_cntnt .figpopup&lt;/code&gt; &amp;#8212; namely, the URLs of tables and&amp;nbsp;figures.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;popups.tags&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;article&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt; &lt;span class="nf"&gt;xml_nodes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;.icnblk_cntnt .figpopup&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;With some more parsing and filtering similar to the above, the full image captions can be grepped for keywords. Caption and image metadata for keyword matches are stored in the SQLite database by &lt;code&gt;pubmedcentral_scraper.R&lt;/code&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;span class="caps"&gt;III&lt;/span&gt;. Generate a report of the scraped&amp;nbsp;results&lt;/h2&gt;
&lt;p&gt;The results of the scraping can be examined by directly querying the SQLite&amp;nbsp;database.&lt;/p&gt;
&lt;p&gt;I also put together an R script, &lt;code&gt;markdown_and_plot.R&lt;/code&gt;, that automatically creates an html report in the simple case where only one topic and plot_type need to be included. The user only has to input the topic and plot_type, and the report is subsequently&amp;nbsp;generated.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;markdown_and_plot.R&lt;/code&gt; calls on &lt;code&gt;generate_markdown_code.R&lt;/code&gt;, which extracts the image URLs from the&amp;nbsp;database:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;query&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;sprintf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="err"&gt;SELECT *\&lt;/span&gt;
                    &lt;span class="nf"&gt;FROM &lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt; &lt;span class="n"&gt;JOIN&lt;/span&gt; &lt;span class="n"&gt;article&lt;/span&gt; &lt;span class="nf"&gt;USING &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pmcid&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="n"&gt;\&lt;/span&gt;
                    &lt;span class="n"&gt;JOIN&lt;/span&gt; &lt;span class="n"&gt;figure_text&lt;/span&gt; &lt;span class="nf"&gt;USING &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img_url&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="n"&gt;\&lt;/span&gt;
                    &lt;span class="nf"&gt;WHERE &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;topic&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;%s&amp;quot;&lt;/span&gt; &lt;span class="n"&gt;AND&lt;/span&gt; &lt;span class="n"&gt;plot_type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;%s&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="n"&gt;\&lt;/span&gt;
                    &lt;span class="n"&gt;ORDER&lt;/span&gt; &lt;span class="n"&gt;BY&lt;/span&gt; &lt;span class="n"&gt;pmcid&lt;/span&gt; &lt;span class="n"&gt;ASC&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="err"&gt;,topic, plot_type)&lt;/span&gt;
&lt;span class="n"&gt;images&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;dbGetQuery&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;con&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;## construct image URLs&lt;/span&gt;
&lt;span class="n"&gt;img_links&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;paste0&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;http://www.ncbi.nlm.nih.gov&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;img_url&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;code&gt;generate_markdown_code.R&lt;/code&gt; then loops through the &lt;em&gt;i&lt;/em&gt; images per article and, line by line, writes out markdown code of the image URLs and&amp;nbsp;captions.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nf"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="nf"&gt;seq_along&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img_links&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="c1"&gt;## ...&lt;/span&gt;
&lt;span class="n"&gt;img_md&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;paste0&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;![pmcid: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;pmcid[i]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;](\`r img_links[&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;]\`)&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img_md&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;outfile&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sep&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;## ...&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;code&gt;markdown_and_plot.R&lt;/code&gt; then reads in the markdown file and renders it into the final html report, containing images embedded via href links, using the &lt;code&gt;knit2html&lt;/code&gt; function in the knitr&amp;nbsp;package.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;html.filename&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;sprintf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;scraper_%s_plots_for_%s.html&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;plot_type&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;topic&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;knit2html&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;md.filename&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;html.filename&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;For a sample report that was generated for topic = Trastuzumab and plot_type = &lt;span class="caps"&gt;TGI&lt;/span&gt;, see &lt;a href="https://github.com/EFavDB/PubmedCentral_Scraper/blob/master/example/scraper_TGI_plots_for_trastuzumab.md"&gt;here&lt;/a&gt;. Note, github automatically renders markdown files into html, whereas html files are displayed as source code. However, the file that is actually intended for human perusal outside Github is the &lt;a href="https://github.com/EFavDB/PubmedCentral_Scraper/blob/master/example/scraper_TGI_plots_for_trastuzumab.html"&gt;html&lt;/a&gt; version, located in the same example subdirectory on&amp;nbsp;Github.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;A look at the example report shows that there are a few false positives, i.e. images that don&amp;#8217;t actually correspond to plots of &lt;span class="caps"&gt;TGI&lt;/span&gt;, but the simplistic grep-keyword-method works well overall. There&amp;#8217;s plenty of room for improving the code, but as it stands, this code sure beats compiling reports by&amp;nbsp;hand!&lt;/p&gt;
&lt;p&gt;We&amp;#8217;ve talked about the thought process behind building the program, but to put it to use, check it out on &lt;a href="https://github.com/EFavDB/PubmedCentral_Scraper"&gt;Github&lt;/a&gt;.&lt;/p&gt;</content><category term="Programming,"></category><category term="methods"></category><category term="tools"></category><category term="database"></category><category term="R"></category><category term="SQL"></category><category term="web scraping"></category></entry><entry><title>Leave-one-out cross-validation</title><link href="https://efavdb.com/leave-one-out-cross-validation" rel="alternate"></link><published>2015-08-01T16:08:00-07:00</published><updated>2015-08-01T16:08:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2015-08-01:/leave-one-out-cross-validation</id><summary type="html">&lt;p&gt;This will be the first of a series of short posts relating to subject matter discussed in the text, &lt;a href="http://www-bcf.usc.edu/~gareth/ISL/"&gt;&amp;#8220;An Introduction to Statistical Learning&amp;#8221;&lt;/a&gt;. This is an interesting read, but it often skips over statement proofs &amp;#8212; that&amp;#8217;s where this series of posts comes in! Here, I consider the content …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This will be the first of a series of short posts relating to subject matter discussed in the text, &lt;a href="http://www-bcf.usc.edu/~gareth/ISL/"&gt;&amp;#8220;An Introduction to Statistical Learning&amp;#8221;&lt;/a&gt;. This is an interesting read, but it often skips over statement proofs &amp;#8212; that&amp;#8217;s where this series of posts comes in! Here, I consider the content of Section 5.1.2: This gives a lightning-quick &amp;#8220;short cut&amp;#8221; method for evaluating a regression&amp;#8217;s leave-one-out cross-validation error. The method is applicable to any least-squares linear&amp;nbsp;fit.&lt;/p&gt;
&lt;h3&gt;Introduction: Leave-one-out&amp;nbsp;cross-validation&lt;/h3&gt;
&lt;p&gt;When carrying out a &lt;a href="https://en.wikipedia.org/wiki/Regression_analysis"&gt;regression analysis&lt;/a&gt;, one is often interested in two types of error measurement. The first is the training set error and the second is the generalization error. The former relates to how close the regression is to the data being fit. In contrast, the generalization error relates to how accurate the model will be when applied to other points. The latter is of particular interest whenever the regression will be used to make predictions on new&amp;nbsp;points.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)"&gt;Cross-validation&lt;/a&gt; provides one method for estimating generalization errors. The approach centers around splitting the training data available into two sets, &lt;em&gt;a cross-validation training set&lt;/em&gt; and &lt;em&gt;cross-validation test set&lt;/em&gt;. The first of these is used for training a regression model. Its accuracy on the test set then provides a generalization error estimate. Here, we focus on a special form of cross-validation, called &lt;em&gt;leave-one-out cross-validation&lt;/em&gt; (&lt;span class="caps"&gt;LOOCV&lt;/span&gt;). In this case, we pick only one point as the test set. We then build a model on all the remaining, complementary points, and evaluate its error on the single-point held out. A generalization error estimate is obtained by repeating this procedure for each of the training points available, averaging the&amp;nbsp;results.&lt;/p&gt;
&lt;p&gt;&lt;span class="caps"&gt;LOOCV&lt;/span&gt; can be computationally expensive because it generally requires one to construct many models &amp;#8212; equal in number to the size of the training set. However, for the special case of least-squares polynomial regression we have the following &amp;#8220;short cut&amp;#8221; identity:&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$ \label{theorem} \tag{1}  
\sum_i \left ( \tilde{y}_i - y_i\right)^2 = \sum_i \left ( \frac{\hat{y}_i - y_i}{1 - h_i}\right)^2.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Here, &lt;span class="math"&gt;\(y_i\)&lt;/span&gt; is the actual label value of training point &lt;span class="math"&gt;\(i\)&lt;/span&gt;, &lt;span class="math"&gt;\(\tilde{y}_i\)&lt;/span&gt; is the value predicted by the cross-validation model trained on all points except &lt;span class="math"&gt;\(i\)&lt;/span&gt;, &lt;span class="math"&gt;\(\hat{y}_i\)&lt;/span&gt; is the value predicted by the regression model trained on all points (including point &lt;span class="math"&gt;\(i\)&lt;/span&gt;), and &lt;span class="math"&gt;\(h_i\)&lt;/span&gt; is a function of the coordinate &lt;span class="math"&gt;\(\vec{x}_i\)&lt;/span&gt; &amp;#8212; this is defined further below. Notice that the left side of (\ref{theorem}) is the &lt;span class="caps"&gt;LOOCV&lt;/span&gt; sum of squares error (the quantity we seek), while the right can be evaluated given only the model trained on the full data set. Fantastically, this allows us to evaluate the &lt;span class="caps"&gt;LOOCV&lt;/span&gt; error using only a single&amp;nbsp;regression!&lt;/p&gt;
&lt;h3&gt;Statement&amp;nbsp;proof&lt;/h3&gt;
&lt;p&gt;Consider the &lt;span class="caps"&gt;LOOCV&lt;/span&gt; step where we construct a model trained on all points except training example &lt;span class="math"&gt;\(k\)&lt;/span&gt;. Using a linear model of form &lt;span class="math"&gt;\(\tilde{y}(\vec{x}) \equiv \vec{x}^T \cdot \vec{\beta}_k\)&lt;/span&gt; &amp;#8212; with &lt;span class="math"&gt;\(\vec{\beta}_k\)&lt;/span&gt; a coefficient vector &amp;#8212; the sum of squares that must be minimized is&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{2} \label{error_sum}  
J_k \equiv \sum_{i \not = k} \left ( \tilde{y}_i - y_i \right)^2 = \sum_{i \not = k} \left (\vec{x}^T_i \cdot \vec{\beta}_k - y_i \right)^2.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Here, we&amp;#8217;re using a subscript &lt;span class="math"&gt;\(k\)&lt;/span&gt; on &lt;span class="math"&gt;\(\vec{\beta}_k\)&lt;/span&gt; to highlight the fact that the above corresponds to the case where example &lt;span class="math"&gt;\(k\)&lt;/span&gt; is held out. We minimize (\ref{error_sum}) by taking the gradient with respect to &lt;span class="math"&gt;\(\vec{\beta}_k\)&lt;/span&gt;. Setting this to zero gives the equation&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{3}  
\left( \sum_{i \not = k} \vec{x}_i \vec{x}_i^T \right) \cdot \vec{\beta}_k = \sum_{i \not = k} y_i \vec{x}_i.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Similarly, the full model (trained on all points) coefficient vector &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt; satisfies&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{4} \label{full_con}  
\left( \sum_{i} \vec{x}_i \vec{x}_i^T \right) \cdot \vec{\beta} \equiv M \cdot \vec{\beta} = \sum_{i} y_i \vec{x}_i.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Combining the prior two equations gives,&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{5}  
\left (M - \vec{x}_k \vec{x}_k^T \right) \cdot \vec{\beta}_k = \left (\sum_{i} y_i \vec{x}_i\right) - y_k \vec{x}_k = M\cdot \vec{\beta} - y_k \vec{x}_k.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Using the definition of &lt;span class="math"&gt;\(\tilde{y}_k\)&lt;/span&gt;, rearrangement of the above leads to the identity&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{6}  
M \cdot \left ( \vec{\beta}_k - \vec{\beta} \right) = \left (\tilde{y}_k - y_k \right) \vec{x}_k.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Left multiplication by &lt;span class="math"&gt;\(\vec{x}_k^T M^{-1}\)&lt;/span&gt; gives,&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{7}  
\tilde{y}_k - \hat{y}_k = \left( \tilde{y}_k - y_k\right) - \left( \hat{y}_k - y_k \right) = \vec{x}_k^T M^{-1} \vec{x}_k \left (\tilde{y}_k - y_k \right).  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Finally, combining like-terms, squaring, and summing gives&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{8}  
\sum_k \left (\tilde{y}_k - y_k \right) ^2 = \sum_k \left (\frac{\hat{y}_k - y_k}{1 -\vec{x}_k^T M^{-1} \vec{x}_k } \right)^2.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
This is (\ref{theorem}), where we now see the parameter &lt;span class="math"&gt;\(h_k \equiv \vec{x}_k^T M^{-1} \vec{x}_k\)&lt;/span&gt;. This is referred to as the &amp;#8220;leverage&amp;#8221; of &lt;span class="math"&gt;\(\vec{x}_k\)&lt;/span&gt; in the text. Notice also that &lt;span class="math"&gt;\(M\)&lt;/span&gt; is proportional to the correlation matrix of the &lt;span class="math"&gt;\(\{\vec{x}_i\}\)&lt;/span&gt;. &lt;span class="math"&gt;\(\blacksquare\)&lt;/span&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Methods, Theory"></category></entry><entry><title>Machine learning to predict San Francisco crime</title><link href="https://efavdb.com/predicting-san-francisco-crimes" rel="alternate"></link><published>2015-07-20T03:01:00-07:00</published><updated>2015-07-20T03:01:00-07:00</updated><author><name>Damien RJ</name></author><id>tag:efavdb.com,2015-07-20:/predicting-san-francisco-crimes</id><summary type="html">&lt;p&gt;In today&amp;#8217;s post, we document our submission to the recent &lt;a href="https://www.kaggle.com/c/sf-crime"&gt;Kaggle&lt;/a&gt; competition aimed at predicting the category of San Francisco crimes, given only their time and location of occurrence. As a reminder, Kaggle is a site where one can compete with other data scientists on various data challenges.  We …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In today&amp;#8217;s post, we document our submission to the recent &lt;a href="https://www.kaggle.com/c/sf-crime"&gt;Kaggle&lt;/a&gt; competition aimed at predicting the category of San Francisco crimes, given only their time and location of occurrence. As a reminder, Kaggle is a site where one can compete with other data scientists on various data challenges.  We took this competition as an opportunity to explore the Naive Bayes algorithm. With the few steps discussed below, we were able to quickly move from the middle of the pack to the top 33% on the competition leader board, all the while continuing with this simple&amp;nbsp;model!&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;As in all cities, crime is a reality San Francisco: Everyone who lives in San Francisco seems to know someone whose car window has been smashed in, or whose bicycle was stolen within the past year or two. Even Prius&amp;#8217; car batteries are apparently considered &lt;a href="http://abc7news.com/news/exclusive-car-battery-thefts-from-hybrid-cars-on-the-rise-in-san-francisco-/725532/"&gt;fair game&lt;/a&gt; by the city&amp;#8217;s diligent thieves.  The challenge we tackle today involves attempting to guess the class of a crime committed within the city, given the time and location it took place. Such studies are representative of efforts by many police forces today: Using machine learning approaches, one can get an improved understanding of which crimes occur where and when in a city &amp;#8212; this then allows for better, &lt;a href="http://www.forbes.com/sites/emc/2014/06/03/data-analysis-helps-police-departments-fight-crime/"&gt;dynamic allocation of police resources&lt;/a&gt;. To aid in the &lt;span class="caps"&gt;SF&lt;/span&gt; &lt;a href="https://www.kaggle.com/c/sf-crime"&gt;challenge&lt;/a&gt;, Kaggle has provided about 12 years of crime reports from all over the city &amp;#8212; a data set that is pretty interesting to comb&amp;nbsp;through.&lt;/p&gt;
&lt;p&gt;Here, we outline our approach to tackling this problem, using the Naive Bayes classifier. This is one of the simplest classification algorithms, the essential ingredients of which include combining &lt;a href="https://en.wikipedia.org/wiki/Bayes%27_theorem" title="Bayes' theorem"&gt;Bayes&amp;#8217; theorem&lt;/a&gt; with an independence assumption on the features (this is the &amp;#8220;naive&amp;#8221; part).  Although simple, it is still a popular method for text categorization. For example, using word frequencies as features, this approach can accurately classify emails as spam, or whether a particular a piece of text was written by a specific author.  In fact, with careful preprocessing, the algorithm is often &lt;a href="http://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf"&gt;competitive&lt;/a&gt; with more advanced methods, including support vector&amp;nbsp;machines.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Loading package and&amp;nbsp;data&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Below, we show the relevant commands needed to load all the packages and training/test data we will be using. As in previous posts, we will work with &lt;a href="http://pandas.pydata.org/"&gt;Pandas&lt;/a&gt; for quick and easy data loading and wrangling. We will be having a post dedicated to Pandas in the near future, so stay tuned! We start off with using the parse_dates method to convert the Dates column of our provided data &amp;#8212; which can be downloaded &lt;a href="https://www.kaggle.com/c/sf-crime/data"&gt;here&lt;/a&gt;&amp;#8212; from string to datetime&amp;nbsp;format.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.cross_validation&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;preprocessing&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;log_loss&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.naive_bayes&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;BernoulliNB&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.linear_model&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;LogisticRegression&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="c1"&gt;#Load Data with pandas, and parse the first column into datetime&lt;/span&gt;
&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;train.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;parse_dates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Dates&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="kp"&gt;test&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;test.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;parse_dates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Dates&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The training data provided contains the following&amp;nbsp;fields:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Date&lt;/em&gt;&lt;/strong&gt; -  date + timestamp
&lt;strong&gt;&lt;em&gt;Category&lt;/em&gt;&lt;/strong&gt; - The type of crime, Larceny, etc.
&lt;strong&gt;&lt;em&gt;Descript&lt;/em&gt;&lt;/strong&gt; - A more detailed description of the crime.
&lt;strong&gt;&lt;em&gt;DayOfWeek&lt;/em&gt;&lt;/strong&gt; - Day of crime: Monday, Tuesday, etc.
&lt;strong&gt;&lt;em&gt;PdDistrict &lt;/em&gt;&lt;/strong&gt;- Police department district.
&lt;strong&gt;&lt;em&gt;Resolution&lt;/em&gt;&lt;/strong&gt;- What was the outcome, Arrest, Unfounded, None, etc.
&lt;strong&gt;&lt;em&gt;Address&lt;/em&gt;&lt;/strong&gt; - Street address of crime.
&lt;strong&gt;&lt;em&gt;X and Y&lt;/em&gt;&lt;/strong&gt; - &lt;span class="caps"&gt;GPS&lt;/span&gt; coordinates of&amp;nbsp;crime.&lt;/p&gt;
&lt;p&gt;As we mentioned earlier, the provided data spans almost 12 years, and both the training data set and the testing data set each have about 900k records. At this point we have all the data in memory. However, the majority of this data is categorical in nature, and so will require some more&amp;nbsp;preprocessing.&lt;/p&gt;
&lt;h2&gt;How to handle categorical&amp;nbsp;data&lt;/h2&gt;
&lt;p&gt;Many machine learning algorithms &amp;#8212; including that which we apply below &amp;#8212; will not accept categorical, or text, features. What is the best way to convert such data into numerical values? A natural idea is to convert each unique string to a unique value.  For example, in our data set we might take the crime category value to correspond to one numerical feature, with Larceny set to 1, Homicide to 2, etc.  However, this scheme can cause problems for many algorithms, because they will incorrectly assume that nearby numerical values imply some sort of similarity between the underlying categorical&amp;nbsp;values.&lt;/p&gt;
&lt;p&gt;To avoid the problem noted above, we will instead binarize our categorical data, using vectors of 1&amp;#8217;s and 0&amp;#8217;s. For example, we will&amp;nbsp;write&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;larceny = 1,0,0,0,...&lt;/span&gt;
&lt;span class="err"&gt;homicide = 0,1,0,0,...&lt;/span&gt;
&lt;span class="err"&gt;prostitution  = 0,0,1,0,...&lt;/span&gt;
&lt;span class="err"&gt;...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;There are a variety of methods to do this encoding, but Pandas has a particularly nice method called &lt;a href="http://pandas.pydata.org/pandas-docs/version/0.13.1/generated/pandas.get_dummies.html"&gt;get_dummies()&lt;/a&gt; that can go straight from your column of text to a binarized array.  Below, we also convert the crime category labels to integer values using the method &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"&gt;LabelEncoder&lt;/a&gt;, and use Pandas to extract the hour from each time point. We then convert the districts, weekday, and hour into binarized arrays and combine them into a new dataframe. &lt;strong&gt; &lt;/strong&gt; We then split up the train_data into a training and validation set so that we have a way of accessing the model performance while leaving the test data&amp;nbsp;untouched.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="k"&gt;Convert&lt;/span&gt; &lt;span class="n"&gt;crime&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="k"&gt;to&lt;/span&gt; &lt;span class="n"&gt;numbers&lt;/span&gt;
&lt;span class="n"&gt;le_crime&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;preprocessing&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LabelEncoder&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;crime&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;le_crime&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Category&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="k"&gt;Get&lt;/span&gt; &lt;span class="n"&gt;binarized&lt;/span&gt; &lt;span class="n"&gt;weekdays&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;districts&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;and&lt;/span&gt; &lt;span class="n"&gt;hours&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="n"&gt;days&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dummies&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DayOfWeek&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;district&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dummies&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;PdDistrict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;hour&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dates&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hour&lt;/span&gt;
&lt;span class="n"&gt;hour&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dummies&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hour&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="n"&gt;Build&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="nb"&gt;array&lt;/span&gt;
&lt;span class="n"&gt;train_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concat&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;hour&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;days&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;district&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;crime&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;crime&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="n"&gt;Repeat&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt; &lt;span class="k"&gt;data&lt;/span&gt;
&lt;span class="n"&gt;days&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dummies&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DayOfWeek&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;district&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dummies&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;PdDistrict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;hour&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dates&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hour&lt;/span&gt;
&lt;span class="n"&gt;hour&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dummies&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hour&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;test_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concat&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;hour&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;days&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;district&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;validation&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;60&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;&lt;strong&gt;Model&amp;nbsp;development&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;For this competition the metric used to rate the performance of the model is the multi-class &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html"&gt;log_loss&lt;/a&gt; &amp;#8212; smaller values of this loss correspond to improved&amp;nbsp;performance.&lt;/p&gt;
&lt;h4&gt;First&amp;nbsp;pass&lt;/h4&gt;
&lt;p&gt;For our first quick pass, we used just the day of the week and district for features in our classifier training. We also carried out a Logistic Regression (&lt;span class="caps"&gt;LR&lt;/span&gt;) on the data in order to get a feel for how the Naive Bayes (&lt;span class="caps"&gt;NB&lt;/span&gt;) model was performing. The results from the &lt;span class="caps"&gt;NB&lt;/span&gt; model gave us a log-loss of 2.62, while &lt;span class="caps"&gt;LR&lt;/span&gt; after tuning was able to give 2.62. However, &lt;span class="caps"&gt;LR&lt;/span&gt; took 60 seconds to run, while &lt;span class="caps"&gt;NB&lt;/span&gt; took only 1.5 seconds! As a reference, the current top score on the leader board is about 2.27, while the worst is around 35. Not bad&amp;nbsp;performance!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;&amp;#39;Friday&amp;#39;, &amp;#39;Monday&amp;#39;, &amp;#39;Saturday&amp;#39;, &amp;#39;Sunday&amp;#39;, &amp;#39;Thursday&amp;#39;, &amp;#39;Tuesday&amp;#39;,&lt;/span&gt;
&lt;span class="n"&gt;&amp;#39;Wednesday&amp;#39;, &amp;#39;BAYVIEW&amp;#39;, &amp;#39;CENTRAL&amp;#39;, &amp;#39;INGLESIDE&amp;#39;, &amp;#39;MISSION&amp;#39;,&lt;/span&gt;
&lt;span class="n"&gt;&amp;#39;NORTHERN&amp;#39;, &amp;#39;PARK&amp;#39;, &amp;#39;RICHMOND&amp;#39;, &amp;#39;SOUTHERN&amp;#39;, &amp;#39;TARAVAL&amp;#39;, &amp;#39;TENDERLOIN&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;validation&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;train_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;.60&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;BernoulliNB&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;&amp;#39;crime&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;predicted&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict_proba&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;validation&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;log_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;validation&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;&amp;#39;crime&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;predicted&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="n"&gt;#Logistic&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Regression&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;comparison&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;LogisticRegression&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;.01&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;&amp;#39;crime&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;predicted&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict_proba&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;validation&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;log_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;validation&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;&amp;#39;crime&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;predicted&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Submission&amp;nbsp;code&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;BernoulliNB&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;&amp;#39;crime&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;predicted&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict_proba&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="n"&gt;#Write&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;results&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;result&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predicted&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;le_crime&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;classes_&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;testResult.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;index&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;index_label&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Id&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;With the above model performing well, we used our code to write out our predictions on the test set to csv format, and submitted this to Kaggle. It turns out we got a score of 2.61 which is slightly better than our validation set estimate. The was a good enough score to put us in the to 50%. Pretty good for a first&amp;nbsp;try!&lt;/p&gt;
&lt;h4&gt;Second&amp;nbsp;pass&lt;/h4&gt;
&lt;p&gt;To improve the model further, we next added the time to the feature list used in training. This clearly provides some relevant information, as some types of crime happen more during the day than the night. For example, we expect public drunkenness to probably go up in the late evening.  Adding this feature we were able to push our log-loss score down to 2.58 &amp;#8212; quick and easy progress!  As a side note, we also tried leaving the hours as a continuous variable, but this did not lead to any score improvements.  After training on the whole data set again, we also get 2.58 on the test date. This moved us up another 32 spots, giving a final placement of&amp;nbsp;76/226!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Friday&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Monday&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Saturday&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Sunday&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Thursday&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Tuesday&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="s1"&gt;&amp;#39;Wednesday&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;BAYVIEW&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;CENTRAL&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;INGLESIDE&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;MISSION&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="s1"&gt;&amp;#39;NORTHERN&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;PARK&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;RICHMOND&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;SOUTHERN&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;TARAVAL&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;TENDERLOIN&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;features2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="n"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;features2&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Discussion&lt;/h2&gt;
&lt;p&gt;Although Naive Bayes is a fairly simple model, properly wielded it can give great results.  In fact, in this competition our results were competitive with teams who were using much more complicated models, e.g. neural nets. We also learned a few other interesting things here: For example, Pandas&amp;#8217; get_dummies() method looks like it will be a huge timesaver when dealing with categorical data. Till next time &amp;#8212; keep your Prius safe!
&lt;a href="https://github.com/EFavDB/SF-Crime" title="GitHub Repo"&gt;&lt;img alt="Open GitHub Repo" src="https://efavdb.com/wp-content/uploads/2015/03/GitHub_Logo.png"&gt;&lt;/a&gt;&lt;/p&gt;</content><category term="Case studies"></category></entry><entry><title>How not to sort by average rating, revisited</title><link href="https://efavdb.com/ranking-revisited" rel="alternate"></link><published>2015-07-11T20:30:00-07:00</published><updated>2015-07-11T20:30:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2015-07-11:/ranking-revisited</id><summary type="html">&lt;p&gt;What is the best method for ranking items that have positive and negative reviews? Some sites, including reddit, have adopted an algorithm suggested by &lt;a href="http://www.evanmiller.org/"&gt;Evan Miller&lt;/a&gt; to generate their item rankings. However, this algorithm can sometimes be unfairly pessimistic about new, good items. This is especially true of items whose …&lt;/p&gt;</summary><content type="html">&lt;p&gt;What is the best method for ranking items that have positive and negative reviews? Some sites, including reddit, have adopted an algorithm suggested by &lt;a href="http://www.evanmiller.org/"&gt;Evan Miller&lt;/a&gt; to generate their item rankings. However, this algorithm can sometimes be unfairly pessimistic about new, good items. This is especially true of items whose first few votes are negative &amp;#8212; an issue that can be &amp;#8220;gamed&amp;#8221; by adversaries. In this post, we consider three alternative ranking methods that can enable high-quality items to more-easily bubble-up. The last is the simplest, but continues to give good results: One simply seeds each item&amp;#8217;s vote count with a suitable fixed number of hidden &amp;#8220;starter&amp;#8221;&amp;nbsp;votes.&lt;/p&gt;
&lt;h3&gt;Introduction &amp;#8212; a review of Evan Miller&amp;#8217;s&amp;nbsp;post&lt;/h3&gt;
&lt;p&gt;In an &lt;a href="http://www.evanmiller.org/how-not-to-sort-by-average-rating.html"&gt;insightful prior post&lt;/a&gt;, Evan Miller (&lt;span class="caps"&gt;EM&lt;/span&gt;) considered the problem of ranking items that had been reviewed as positive or negative (up-voted or down-voted, represented by a 1 or a 0, respectively) by a sample of users. He began by illustrating that two of the more readily-arrived at solutions to this problem are highly flawed. To&amp;nbsp;review:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bad method 1:&lt;/strong&gt; Rank item &lt;span class="math"&gt;\(i\)&lt;/span&gt; by &lt;span class="math"&gt;\(n_i(1) - n_i(0)\)&lt;/span&gt;, its up-vote count minus its down-vote&amp;nbsp;count.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Issue:&lt;/em&gt; If one item has garnered 60 up-votes and 40 down-votes, it will get the same score as an item with only 20 votes, all positive. Yet, the latter has a 100% up-vote rate (20 for 20), suggesting that it is of very high quality. Despite this, the algorithm ranks the two&amp;nbsp;equally.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bad method 2:&lt;/strong&gt; Rank item &lt;span class="math"&gt;\(i\)&lt;/span&gt; by &lt;span class="math"&gt;\(\hat{p} \equiv n_i(1)/[n_i(0) + n_i(1)]\)&lt;/span&gt;, its sample up-vote rate (average&amp;nbsp;rating).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Issue:&lt;/em&gt; If any one item has only one vote, an up-vote, it will be given a perfect score by this algorithm. This means that it will be ranked above all other items, despite the fact that a single vote is not particularly informative/convincing. In general, this method can work well, but only once each item has a significant number of&amp;nbsp;votes.&lt;/p&gt;
&lt;p&gt;To avoid the issues of these two bad methods (BMs), &lt;span class="caps"&gt;EM&lt;/span&gt; suggests scoring and ranking each item by the &lt;em&gt;lower limit of its up-vote-rate confidence interval&lt;/em&gt;. This is (&lt;a href="https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Wilson_score_interval"&gt;&lt;span class="caps"&gt;E.B.&lt;/span&gt; Wilson, 1927&lt;/a&gt;),
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{1} \label{emsol}
p_{W} = \frac{\hat{p} + \frac{z_{\alpha/2}^2}{2n} - z_{\alpha/2} \sqrt{\frac{\hat{p}(1-\hat{p}) + \frac{z_{\alpha/2}^2}{4n} }{n}}}{1 + \frac{z_{\alpha/2}^2}{n}},
$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(\hat{p}\)&lt;/span&gt; is again the sample up-vote rate, &lt;span class="math"&gt;\(z_{\alpha/2}\)&lt;/span&gt; is a positive constant that sets the size of the confidence interval used, and &lt;span class="math"&gt;\(n\)&lt;/span&gt; is the total number of votes that have so far been recorded. The score &lt;span class="math"&gt;\(p_{W}\)&lt;/span&gt; approaches &lt;span class="math"&gt;\(\hat{p}\)&lt;/span&gt; once an item has a significant number of votes &amp;#8212; it consequently avoids the pitfall of &lt;span class="caps"&gt;BM1&lt;/span&gt; above. By construction, it also avoids the pitfall of &lt;span class="caps"&gt;BM2&lt;/span&gt;. With both of these pitfalls avoided, the &lt;span class="caps"&gt;EM&lt;/span&gt; method can sometimes provide a reasonable, practical ranking&amp;nbsp;system.&lt;/p&gt;
&lt;h3&gt;Potential issue with&amp;nbsp;(\ref{emsol})&lt;/h3&gt;
&lt;p&gt;Although (\ref{emsol}) does a good job of avoiding the pitfall associated with &lt;span class="caps"&gt;BM2&lt;/span&gt;, it can do a poor job of handling a related pitfall: If any new item has only a few votes, and these each happen to be down-votes, its sample up-vote rate will be &lt;span class="math"&gt;\(\hat{p} = 0\)&lt;/span&gt;. In this case, (\ref{emsol})&amp;nbsp;gives
&lt;/p&gt;
&lt;div class="math"&gt;$$\label{problem} \tag{2}
p_{W} = \left .\frac{\hat{p} + \frac{z_{\alpha/2}^2}{2n} - z_{\alpha/2} \sqrt{\frac{\hat{p}(1-\hat{p}) + \frac{z_{\alpha/2}^2}{4n} }{n}}}{1 + \frac{z_{\alpha/2}^2}{n}}\right \vert_{\hat{p} = 0} = 0.
$$&lt;/div&gt;
&lt;p&gt;
Now, &lt;span class="math"&gt;\(p_W\)&lt;/span&gt; is always between &lt;span class="math"&gt;\(0\)&lt;/span&gt; and &lt;span class="math"&gt;\(1\)&lt;/span&gt;, so (\ref{problem}) implies that any new, quickly-down-voted item will immediately be ranked below all others. This is extremely harsh and potentially unfair. For example, consider the case of a newly-opened restaurant: If an adversary were to quickly down-vote this restaurant on some ranking site &amp;#8212; the day of its opening &amp;#8212; the new restaurant would be ranked below all others, including the adversary. This would occur even if the new restaurant were of very high true quality. This could have potentially-damaging consequences, for both the restaurant and the ranking site &amp;#8212; whose lists should provide only the best&amp;nbsp;recommendations!&lt;/p&gt;
&lt;p&gt;An ideal ranking system should explicitly take into account the large uncertainty present when only a small number of votes have been recorded. The score (\ref{emsol}) does a good job of this on the high &lt;span class="math"&gt;\(\hat{p}\)&lt;/span&gt; end, but a poor job on the low &lt;span class="math"&gt;\(\hat{p}\)&lt;/span&gt; end. This approach may be appropriate for cases where one is risk-averse on the high end only, but in general one should protect against both sorts of quick, strong judgements. Below we consider some alternative, &lt;a href="https://en.wikipedia.org/wiki/Bayesian_statistics"&gt;Bayesian&lt;/a&gt; ranking solutions. The last is easy to understand and implement: One simply gives each item a hidden number of up- and down-votes to start with. These hidden &amp;#8220;starter&amp;#8221; votes can be chosen in various ways &amp;#8212; they serve to simply bias new items towards an intermediate value early on, with the bias becoming less important as more votes come in. This approach avoids each of the pitfalls we have&amp;nbsp;discussed.&lt;/p&gt;
&lt;h3&gt;Bayesian&amp;nbsp;formulation&lt;/h3&gt;
&lt;p&gt;Note: This section and the next are both fairly mathematical. They can be skipped for those wishing to focus on application method&amp;nbsp;only.&lt;/p&gt;
&lt;p&gt;To start our Bayesian analysis, we begin by positing a general beta distribution for the up-vote rate prior&amp;nbsp;distribution,
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{3}\label{beta}
P(p) = \tilde{\mathcal{N}} p^a (1-p)^b.
$$&lt;/div&gt;
&lt;p&gt;
Here, &lt;span class="math"&gt;\(\tilde{\mathcal{N}}\)&lt;/span&gt; is a normalization factor and &lt;span class="math"&gt;\(a\)&lt;/span&gt; and &lt;span class="math"&gt;\(b\)&lt;/span&gt; are some constants (we suggest methods for choosing their values in the discussion section). The function &lt;span class="math"&gt;\(P(p)\)&lt;/span&gt; specifies an initial guess &amp;#8212; in the absence of any reviews for an item &amp;#8212; for what we think the probability is that it will have up-vote rate &lt;span class="math"&gt;\(p\)&lt;/span&gt;. If item &lt;span class="math"&gt;\(i\)&lt;/span&gt; actually has been reviewed, we can update our guess for its distribution using &lt;a href="https://en.wikipedia.org/wiki/Bayes'_theorem"&gt;Bayes&amp;#8217; rule&lt;/a&gt;:
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align} \tag{4} \label{bayes_rule}
P(p \vert n_i(1), n_i(0)) =\frac{ P( n_i(1), n_i(0) \vert p ) P(p)}{P(n_i(1), n_i(0))} = \mathcal{N} p^{n_i(1)+a}(1-p)^{n_i(0)+b}.
\end{align}
$$&lt;/div&gt;
&lt;p&gt;
Here, we have evaluated &lt;span class="math"&gt;\( P( n(1), n(0) \vert p )\)&lt;/span&gt; using the &lt;a href="https://en.wikipedia.org/wiki/Binomial_distribution"&gt;binomial distribution&lt;/a&gt;, we&amp;#8217;ve plugged in (\ref{beta}) for &lt;span class="math"&gt;\(P(p)\)&lt;/span&gt;, and we&amp;#8217;ve collected all &lt;span class="math"&gt;\(p\)&lt;/span&gt;-independent factors into the new normalization factor &lt;span class="math"&gt;\(\mathcal{N}\)&lt;/span&gt;. The formula (\ref{bayes_rule}) provides the basis for the three ranking methods discussed&amp;nbsp;below.&lt;/p&gt;
&lt;h3&gt;Three Bayesian ranking&amp;nbsp;systems&lt;/h3&gt;
&lt;p&gt;Let&amp;#8217;s&amp;nbsp;rank!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bayesian method 1:&lt;/strong&gt; Choose the ordering that is most&amp;nbsp;likely.&lt;/p&gt;
&lt;p&gt;It is a simple matter to write down a formal expression for the probability of any ranking. For example, given two items we&amp;nbsp;have
&lt;/p&gt;
&lt;div class="math"&gt;$$
P(p_1 &amp;gt; p_2) = \int_0^1 dp_1 \int_0^{p_1} dp_2 P(p_1) P(p_2). \tag{5} \label{int}
$$&lt;/div&gt;
&lt;p&gt;
Plugging in (\ref{bayes_rule}) for the &lt;span class="math"&gt;\(P(p_i)\)&lt;/span&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt;s, this can be evaluated numerically. Evaluating the probability for the opposite ordering, we can then choose that which is most likely to be&amp;nbsp;correct.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span class="math"&gt;\(\bullet\)&lt;/span&gt; Pros:&lt;/em&gt; Approach directly optimizes for the object we&amp;#8217;re interested in, the ranking &amp;#8212; very&amp;nbsp;appealing!&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span class="math"&gt;\(\bullet\)&lt;/span&gt; Cons:&lt;/em&gt; Given &lt;span class="math"&gt;\(N\)&lt;/span&gt; items, one has &lt;span class="math"&gt;\(N!\)&lt;/span&gt; integrals to carry out &amp;#8212; untenable for large &lt;span class="math"&gt;\(N\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span class="math"&gt;\(\bullet\)&lt;/span&gt; Note:&lt;/em&gt; See posssiblywrong&amp;#8217;s post &lt;a href="https://possiblywrong.wordpress.com/2014/05/31/reddits-comment-ranking-algorithm-revisited/"&gt;here&lt;/a&gt; for some related, interesting&amp;nbsp;points.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bayesian method 2:&lt;/strong&gt; Rank item &lt;span class="math"&gt;\(i\)&lt;/span&gt; by its median &lt;span class="math"&gt;\(p\)&lt;/span&gt;-value.&lt;/p&gt;
&lt;p&gt;Sorting by an item score provides an approach that will scale well even at large &lt;span class="math"&gt;\(N\)&lt;/span&gt;. A natural score to consider is an item&amp;#8217;s median &lt;span class="math"&gt;\(p\)&lt;/span&gt;-value: that which it has a &lt;span class="math"&gt;\(50/50\)&lt;/span&gt; shot of being larger (or smaller) than. Using (\ref{bayes_rule}), this&amp;nbsp;satisfies
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{6}\label{m2}
\frac{\int_0^{p_{med}} p^{n_i(1)+a}(1-p)^{n_i(0)+b} dp}{\int_0^{1} p^{n_i(1)+a}(1-p)^{n_i(0)+b} dp} = 1/2.
$$&lt;/div&gt;
&lt;p&gt;
The integral at left actually has a name &amp;#8212; it&amp;#8217;s called the &lt;a href="http://mathworld.wolfram.com/IncompleteBetaFunction.html"&gt;incomplete beta function&lt;/a&gt;. Using a statistics package, it can be inverted to give &lt;span class="math"&gt;\(p_{med}\)&lt;/span&gt;. For example, if we set &lt;span class="math"&gt;\(a = b = 1\)&lt;/span&gt;, an item with a single up-vote and no down-votes would get a score of &lt;span class="math"&gt;\(0.614\)&lt;/span&gt;. In other words, we&amp;#8217;d guess there&amp;#8217;s a 50/50 shot that the item&amp;#8217;s up-vote rate falls above this value, so we&amp;#8217;d rank it higher than any other item whose &lt;span class="math"&gt;\(p\)&lt;/span&gt; value is known to be smaller than&amp;nbsp;this.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span class="math"&gt;\(\bullet\)&lt;/span&gt; Pros:&lt;/em&gt; Sorting is fast. Gives intuitive, meaningful score for each&amp;nbsp;item.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span class="math"&gt;\(\bullet\)&lt;/span&gt; Cons:&lt;/em&gt; Inverting (\ref{m2}) can be somewhat slow, e.g. &lt;span class="math"&gt;\(\sim 10^{-3}\)&lt;/span&gt; seconds in&amp;nbsp;Mathematica.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span class="math"&gt;\(\bullet\)&lt;/span&gt; Note&lt;/em&gt;: &lt;span class="caps"&gt;EM&lt;/span&gt; also derived this score function, in a follow-up to his original post. However, he motivated it in a slightly different way &amp;#8212; see &lt;a href="http://www.evanmiller.org/bayesian-average-ratings.html"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bayesian method 3:&lt;/strong&gt; Rank item &lt;span class="math"&gt;\(i\)&lt;/span&gt; by its most likely (aka &lt;a href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation"&gt;&lt;span class="caps"&gt;MAP&lt;/span&gt;&lt;/a&gt;) &lt;span class="math"&gt;\(p\)&lt;/span&gt;-value.&lt;/p&gt;
&lt;p&gt;The most likely &lt;span class="math"&gt;\(p\)&lt;/span&gt;-value for each item provides another natural score function. To find this, we simply set the derivative of (\ref{bayes_rule}) to&amp;nbsp;zero,
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
\partial_p p^{n_i(1)+a}(1-p)^{n_i(0)+b} &amp;amp;= \left (\frac{n_i(1)+a}{p} + \frac{n_i(0)+b}{1-p} \right ) p^{n_i(1)+a}(1-p)^{n_i(0)+b} = 0 \\
\to p = \tilde{p} &amp;amp;\equiv \frac{n_i(1)+a}{(n_i(1)+a) + (n_i(0)+b)}. \tag{7} \label{final}
\end{align}
$$&lt;/div&gt;
&lt;p&gt;
This form &lt;span class="math"&gt;\(\tilde{p}\)&lt;/span&gt; is interesting because it resembles the sample mean &lt;span class="math"&gt;\(\hat{p}\)&lt;/span&gt; considered above. However, the actual number of up- and down-votes, &lt;span class="math"&gt;\(n_i(1)\)&lt;/span&gt; and &lt;span class="math"&gt;\(n_i(0)\)&lt;/span&gt;, are supplemented in (\ref{final}) by &lt;span class="math"&gt;\(a\)&lt;/span&gt; and &lt;span class="math"&gt;\(b\)&lt;/span&gt;, respectively. We can thus interpret these values as effective &amp;#8220;starter votes&amp;#8221;, given to each item before any real reviews are recorded. Their effect is to bias our guess for &lt;span class="math"&gt;\(p\)&lt;/span&gt; towards the prior&amp;#8217;s peak value, with the bias being most strong when &lt;span class="math"&gt;\(a\)&lt;/span&gt; and &lt;span class="math"&gt;\(b\)&lt;/span&gt; are chosen large and/or when we have few actual votes present. For any non-zero choices, (\ref{final}) avoids each of the pitfalls discussed above. Further, it approaches the true up-vote rate in the limit of large review sample sizes, as&amp;nbsp;required.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span class="math"&gt;\(\bullet\)&lt;/span&gt; Pros:&lt;/em&gt; Sorting is fast. Simple method for avoiding the common&amp;nbsp;pitfalls.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span class="math"&gt;\(\bullet\)&lt;/span&gt; Cons:&lt;/em&gt; Have to pick &lt;span class="math"&gt;\(a\)&lt;/span&gt; and &lt;span class="math"&gt;\(b\)&lt;/span&gt; &amp;#8212; see below for suggested&amp;nbsp;methods.&lt;/p&gt;
&lt;h3&gt;Discussion&lt;/h3&gt;
&lt;p&gt;We consider each of the four ranking methods we&amp;#8217;ve discussed here to be interesting and useful &amp;#8212; the three Bayesian ranking systems, as well as &lt;a href="http://www.evanmiller.org/how-not-to-sort-by-average-rating.html"&gt;&lt;span class="caps"&gt;EM&lt;/span&gt;&amp;#8217;s original system&lt;/a&gt;, which works well when one only needs to protect against false positives (again, we note that Bayesian method 2 was also considered by &lt;span class="caps"&gt;EM&lt;/span&gt; in a &lt;a href="http://www.evanmiller.org/bayesian-average-ratings.html"&gt;follow-up&lt;/a&gt; to his original post). In practice, the three Bayesian approaches will each tend to return similar, but sometimes slightly different rankings. With regards to &amp;#8220;correctness&amp;#8221;, the essential point is that each method is well-motivated and avoids the common pitfalls. However, the final method is the easiest to apply, so it might be the most&amp;nbsp;practical.&lt;/p&gt;
&lt;p&gt;To apply the Bayesian methods, one must specify the &lt;span class="math"&gt;\(a\)&lt;/span&gt; and &lt;span class="math"&gt;\(b\)&lt;/span&gt; values defining the prior, (\ref{bayes_rule}). We suggest three methods for choosing these: 1) Choose these values to provide a good approximation to your actual distribution, fitting only to items for which you have good statistics. 2) A/B test to get the ranking that optimizes some quantity you are interested in, e.g. clicks. 3) Heuristics: For example, if simplicity is key, choose &lt;span class="math"&gt;\(a= b =1\)&lt;/span&gt;, which biases towards an up-vote rate of &lt;span class="math"&gt;\(0.5\)&lt;/span&gt;. If a conservative estimate is desired for new items, one can set &lt;span class="math"&gt;\(b\)&lt;/span&gt; larger than &lt;span class="math"&gt;\(a\)&lt;/span&gt;. Finally, if you want to raise the number of actual votes required before the sample rates dominate, simply increase the values of &lt;span class="math"&gt;\(a\)&lt;/span&gt; and &lt;span class="math"&gt;\(b\)&lt;/span&gt;&amp;nbsp;accordingly.&lt;/p&gt;
&lt;p&gt;To conclude, we present some example output in the table below. We show values for the Wilson score &lt;span class="math"&gt;\(p_W\)&lt;/span&gt;, with &lt;span class="math"&gt;\(z_{\alpha/2}\)&lt;/span&gt; set to &lt;span class="math"&gt;\(1.281\)&lt;/span&gt; in (\ref{emsol}) (the value &lt;a href="https://github.com/reddit/reddit/blob/62db2373f2555df17ebeb13968e243fccfbeff5f/r2/r2/lib/db/_sorts.pyx"&gt;reddit uses&lt;/a&gt;), and the seed score &lt;span class="math"&gt;\(\tilde{p}\)&lt;/span&gt;, with &lt;span class="math"&gt;\(a\)&lt;/span&gt; and &lt;span class="math"&gt;\(b\)&lt;/span&gt; set to &lt;span class="math"&gt;\(1\)&lt;/span&gt; in (\ref{final}). Notice that the two scores are in near-agreement for the last item shown, which has already accumulated a fair number of votes. However, &lt;span class="math"&gt;\(p_W\)&lt;/span&gt; is significantly lower than &lt;span class="math"&gt;\(\tilde{p}\)&lt;/span&gt; for each of the first three items. For example, the third has an up-vote rate of &lt;span class="math"&gt;\(66%\)&lt;/span&gt;, but is only given a Wilson score of &lt;span class="math"&gt;\(0.32\)&lt;/span&gt;: This means that it would be ranked below any mature item having an up-vote rate at least this high &amp;#8212; including fairly unpopular items liked by only one in three! This observation explains why it is nearly impossible to have new comments noticed on a reddit thread that has already hit the front page. Were reddit to move to a ranking system that were less pessimistic of new comments, its mature threads might remain&amp;nbsp;dynamic.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;up-votes&lt;/th&gt;
&lt;th&gt;down-votes&lt;/th&gt;
&lt;th&gt;&lt;span class="math"&gt;\(p_W\)&lt;/span&gt;, &lt;span class="math"&gt;\(z_{\alpha/2}= 1.281\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class="math"&gt;\(\tilde{p}\)&lt;/span&gt;, &lt;span class="math"&gt;\(a=b=1\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.38&lt;/td&gt;
&lt;td&gt;0.67&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.16&lt;/td&gt;
&lt;td&gt;0.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.32&lt;/td&gt;
&lt;td&gt;0.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;40&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;0.72&lt;/td&gt;
&lt;td&gt;0.79&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Statistics"></category></entry><entry><title>A review of the online course “Introduction to Big Data with Apache Spark”</title><link href="https://efavdb.com/review-intro-to-big-data-with-spark" rel="alternate"></link><published>2015-07-11T14:15:00-07:00</published><updated>2015-07-11T14:15:00-07:00</updated><author><name>Cathy Yeh</name></author><id>tag:efavdb.com,2015-07-11:/review-intro-to-big-data-with-spark</id><summary type="html">&lt;p&gt;This is a review of &lt;a href="https://www.edx.org/course/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x"&gt;Introduction to Big Data with Apache Spark&lt;/a&gt; (&lt;span class="caps"&gt;CS100&lt;/span&gt;.1x), the first in a two-part series introducing the big data processing engine, &lt;a href="https://spark.apache.org/"&gt;Spark&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The one month-long &lt;span class="caps"&gt;MOOC&lt;/span&gt; was offered on edX for the first time in June 2015, and its sequel, &lt;a href="https://www.edx.org/course/scalable-machine-learning-uc-berkeleyx-cs190-1x"&gt;Scalable Machine Learning&lt;/a&gt; (&lt;span class="caps"&gt;CS190&lt;/span&gt;.1x …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This is a review of &lt;a href="https://www.edx.org/course/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x"&gt;Introduction to Big Data with Apache Spark&lt;/a&gt; (&lt;span class="caps"&gt;CS100&lt;/span&gt;.1x), the first in a two-part series introducing the big data processing engine, &lt;a href="https://spark.apache.org/"&gt;Spark&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The one month-long &lt;span class="caps"&gt;MOOC&lt;/span&gt; was offered on edX for the first time in June 2015, and its sequel, &lt;a href="https://www.edx.org/course/scalable-machine-learning-uc-berkeleyx-cs190-1x"&gt;Scalable Machine Learning&lt;/a&gt; (&lt;span class="caps"&gt;CS190&lt;/span&gt;.1x) is currently under&amp;nbsp;way.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/07/IntroToSpark_screenshot.jpg"&gt;&lt;img alt="lecture screenshot" src="https://efavdb.com/wp-content/uploads/2015/07/IntroToSpark_screenshot.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Still from a lecture in &amp;#8220;Introduction to Big Data with Apache&amp;nbsp;Spark&amp;#8221;&lt;/p&gt;
&lt;p&gt;Learning a new syntax is an unavoidably dry exercise; despite that handicap, the course lectures are watchable and directly relevant to the&amp;nbsp;labs.&lt;/p&gt;
&lt;p&gt;Of course, the fun is in applying that hard-earned knowledge&amp;#8230;actually, the learning curve is quite shallow, since a dozen or so basic Spark commands are enough to get you started on a simple project, e.g. see an example on &lt;a href="https://github.com/EFavDB/Spark-example/blob/master/explore_model_bills.ipynb"&gt;github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The weekly assignments, provided in the form of IPython notebooks, are well-designed, keeping the focus on learning Spark (rather than coding/debugging python) by providing a skeleton of pre-filled code chunks for each&amp;nbsp;assignment.&lt;/p&gt;
&lt;p&gt;The labs&amp;nbsp;are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;word counting all of Shakespeare’s&amp;nbsp;plays&lt;/li&gt;
&lt;li&gt;&lt;span class="caps"&gt;NASA&lt;/span&gt; Apache web server log&amp;nbsp;analysis&lt;/li&gt;
&lt;li&gt;entity resolution of Google and Amazon product&amp;nbsp;listings&lt;/li&gt;
&lt;li&gt;collaborative filtering to make movie&amp;nbsp;recommendations&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Students only have to fill in sections of code that relate to the application of Spark. The structure of the assignments thus results in a gratifying amount of payoff for a small amount of work. You don’t need to be fluent in python&amp;#8212;far from it&amp;#8212;in order to complete the labs, and can likely pick up what you need along the way if you have experience in some other programming&amp;nbsp;language.&lt;/p&gt;
&lt;p&gt;I did find the labs to be much more time-consuming than the course estimates, partly&amp;nbsp;because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Error tracebacks in Spark tend to hit you like a great wall of angry text, and are hard to parse (to the uninitiated, at&amp;nbsp;least).&lt;/li&gt;
&lt;li&gt;I was more hesitant to experiment/test freely in the IPython Notebooks out of a fear that some unsanctioned code would cause problems with the tetchy lab Autograder. However, it never did freeze on me, although it did for others who were too liberal in their use of &lt;a href="https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collect"&gt;collect()&lt;/a&gt;&amp;nbsp;statements.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The instructors were extremely responsive to student questions and nimble to implement improvements in real-time. I&amp;#8217;m sure the course will continue to improve over the next&amp;nbsp;iterations.&lt;/p&gt;
&lt;p&gt;A few lucky students, including myself, were randomly selected to have access to the &lt;a href="http://www.databricks.com/"&gt;Databricks&lt;/a&gt; cloud-computing platform for the duration of the&amp;nbsp;course.&lt;/p&gt;
&lt;p&gt;Although the procedures for running Spark in the standard course software (a prepared virtual machine) and Databricks cluster are almost identical, having cloud minions doing your bidding may tickle megalomaniacal tendencies you didn&amp;#8217;t know you&amp;nbsp;had.&lt;/p&gt;</content><category term="Review"></category><category term="cloud"></category><category term="mooc"></category><category term="review"></category><category term="spark"></category></entry><entry><title>Multivariate Cramer-Rao inequality</title><link href="https://efavdb.com/multivariate-cramer-rao-bound" rel="alternate"></link><published>2015-06-20T09:00:00-07:00</published><updated>2015-06-20T09:00:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2015-06-20:/multivariate-cramer-rao-bound</id><summary type="html">&lt;p&gt;The Cramer-Rao inequality addresses the question of how accurately one can estimate a set of parameters &lt;span class="math"&gt;\(\vec{\theta} = \{\theta_1, \theta_2, \ldots, \theta_m \}\)&lt;/span&gt; characterizing a probability distribution &lt;span class="math"&gt;\(P(x) \equiv P(x; \vec{\theta})\)&lt;/span&gt;, given only some samples &lt;span class="math"&gt;\(\{x_1, \ldots, x_n\}\)&lt;/span&gt; taken from &lt;span class="math"&gt;\(P\)&lt;/span&gt;. Specifically, the inequality provides a rigorous lower …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The Cramer-Rao inequality addresses the question of how accurately one can estimate a set of parameters &lt;span class="math"&gt;\(\vec{\theta} = \{\theta_1, \theta_2, \ldots, \theta_m \}\)&lt;/span&gt; characterizing a probability distribution &lt;span class="math"&gt;\(P(x) \equiv P(x; \vec{\theta})\)&lt;/span&gt;, given only some samples &lt;span class="math"&gt;\(\{x_1, \ldots, x_n\}\)&lt;/span&gt; taken from &lt;span class="math"&gt;\(P\)&lt;/span&gt;. Specifically, the inequality provides a rigorous lower bound on the covariance matrix of any unbiased set of estimators to these &lt;span class="math"&gt;\(\{\theta_i\}\)&lt;/span&gt; values. In this post, we review the general, multivariate form of the inequality, including its significance and&amp;nbsp;proof.&lt;/p&gt;
&lt;h3&gt;Introduction and theorem&amp;nbsp;statement&lt;/h3&gt;
&lt;p&gt;The analysis of data very frequently requires one to attempt to characterize a probability distribution. For instance, given some random, stationary process that generates samples &lt;span class="math"&gt;\(\{x_i\}\)&lt;/span&gt;, one might wish to estimate the mean &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; of the probability distribution &lt;span class="math"&gt;\(P\)&lt;/span&gt; characterizing this process. To do this, one could construct an estimator function &lt;span class="math"&gt;\(\hat{\mu}(\{x_i\})\)&lt;/span&gt; &amp;#8212; a function of some samples taken from &lt;span class="math"&gt;\(P\)&lt;/span&gt; &amp;#8212; that is intended to provide an approximation to &lt;span class="math"&gt;\(\mu\)&lt;/span&gt;. Given &lt;span class="math"&gt;\(n\)&lt;/span&gt; samples, a natural choice is provided&amp;nbsp;by
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
\hat{\mu}(\{x_i\}) = \frac{1}{n}\sum_{i = 1}^n x_i, \tag{1}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
the mean of the samples. This particular choice of estimator will always be unbiased given a stationary &lt;span class="math"&gt;\(P\)&lt;/span&gt; &amp;#8212; meaning that it will return the correct result, on average. However, each particular sample set realization will return a slightly different mean estimate. This means that &lt;span class="math"&gt;\(\hat{\mu}\)&lt;/span&gt; is itself a random variable having its own distribution and&amp;nbsp;width.&lt;/p&gt;
&lt;p&gt;More generally, one might be interested in a distribution characterized by a set of &lt;span class="math"&gt;\(m\)&lt;/span&gt; parameters &lt;span class="math"&gt;\(\{\theta_i\}\)&lt;/span&gt;. Consistently good estimates to these values require estimators with distributions that are tightly centered around the true &lt;span class="math"&gt;\(\{\theta_i\}\)&lt;/span&gt; values. The Cramer-Rao inequality tells us that there is a fundamental limit to how tightly centered such estimators can be, given only &lt;span class="math"&gt;\(n\)&lt;/span&gt; samples. We state the result&amp;nbsp;below.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Theorem:&lt;/b&gt; The multivariate Cramer-Rao&amp;nbsp;inequality&lt;/p&gt;
&lt;p&gt;Let &lt;span class="math"&gt;\(P\)&lt;/span&gt; be a distribution characterized by a set of &lt;span class="math"&gt;\(m\)&lt;/span&gt; parameters &lt;span class="math"&gt;\(\{\theta_i\}\)&lt;/span&gt;, and let &lt;span class="math"&gt;\(\{\hat{\theta_i}\equiv \hat{\theta_i}(\{x_i\})\}\)&lt;/span&gt; be an unbiased set of estimator functions for these parameters. Then, the covariance matrix (see definition below) for the &lt;span class="math"&gt;\(\hat{\{\theta_i\}}\)&lt;/span&gt;&amp;nbsp;satisfies,&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{2} \label{cramer_rao_bound}
cov(\hat{\theta}, \hat{\theta}) \geq \frac{1}{n} \times \frac{1}{ cov(\nabla_{\vec{\theta}} \log P(x),\nabla_{\vec{\theta}} \log P(x) )}.
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;Here, the inequality holds in the sense that left side of the above equation, minus the right, is positive semi-definite. We discuss the meaning and significance of this equation in the next&amp;nbsp;section.&lt;/p&gt;
&lt;h3&gt;Interpretation of the&amp;nbsp;result&lt;/h3&gt;
&lt;p&gt;To understand (\ref{cramer_rao_bound}), we must first review a couple of definitions. These&amp;nbsp;follow.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 1&lt;/strong&gt;. Let &lt;span class="math"&gt;\(\vec{u}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\vec{v}\)&lt;/span&gt; be two jointly-distributed vectors of stationary random variables. The covariance matrix of &lt;span class="math"&gt;\(\vec{u}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\vec{v}\)&lt;/span&gt; is defined&amp;nbsp;by
&lt;/p&gt;
&lt;div class="math"&gt;$$
cov(\vec{u}, \vec{v})_{ij} = \overline{(u_{i}- \overline{u_i})(v_{j}- \overline{v_j})} \equiv \overline{\delta u_{i} \delta v_{j}}\tag{3} \label{cov},
$$&lt;/div&gt;
&lt;p&gt;
where we use overlines for averages. In words, (\ref{cov}) states that &lt;span class="math"&gt;\(cov(\vec{u}, \vec{v})_{ij}\)&lt;/span&gt; is the correlation function of the fluctuations of &lt;span class="math"&gt;\(u_i\)&lt;/span&gt; and &lt;span class="math"&gt;\(v_j\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 2&lt;/strong&gt;. A real, square matrix &lt;span class="math"&gt;\(M\)&lt;/span&gt; is said to be positive semi-definite&amp;nbsp;if
&lt;/p&gt;
&lt;div class="math"&gt;$$
\vec{a}^T\cdot M \cdot \vec{a} \geq 0 \tag{4} \label{pd}
$$&lt;/div&gt;
&lt;p&gt;
for all real vectors &lt;span class="math"&gt;\(\vec{a}\)&lt;/span&gt;. It is positive definite if the &amp;#8220;&lt;span class="math"&gt;\(\geq\)&lt;/span&gt;&amp;#8221; above can be replaced by a &amp;#8220;&lt;span class="math"&gt;\(&amp;gt;\)&lt;/span&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The interesting consequences of (\ref{cramer_rao_bound}) follow from the following&amp;nbsp;observation:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Observation&lt;/strong&gt;. For any constant vectors &lt;span class="math"&gt;\(\vec{a}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\vec{b}\)&lt;/span&gt;, we&amp;nbsp;have
&lt;/p&gt;
&lt;div class="math"&gt;$$
cov(\vec{a}^T\cdot\vec{u}, \vec{b}^T \cdot \vec{v}) = \vec{a}^T \cdot cov(\vec{u}, \vec{v}) \cdot \vec{b}. \tag{5} \label{fact}
$$&lt;/div&gt;
&lt;p&gt;
This follows from the definition&amp;nbsp;(\ref{cov}).&lt;/p&gt;
&lt;p&gt;Taking &lt;span class="math"&gt;\(\vec{a}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\vec{b}\)&lt;/span&gt; to both be along &lt;span class="math"&gt;\(\hat{i}\)&lt;/span&gt; in (\ref{fact}), and combining with (\ref{pd}), we see that (\ref{cramer_rao_bound}) implies&amp;nbsp;that
&lt;/p&gt;
&lt;div class="math"&gt;$$
\sigma^2(\hat{\theta}_i^2) \geq \frac{1}{n} \times \left (\frac{1}{ cov(\nabla_{\vec{\theta}} \log P(x),\nabla_{\vec{\theta}} \log P(x) )} \right)_{ii},\tag{6}\label{CRsimple}
$$&lt;/div&gt;
&lt;p&gt;
where we use &lt;span class="math"&gt;\(\sigma^2(x)\)&lt;/span&gt; to represent the variance of &lt;span class="math"&gt;\(x\)&lt;/span&gt;. The left side of (\ref{CRsimple}) is the variance of the estimator function &lt;span class="math"&gt;\(\hat{\theta}_i\)&lt;/span&gt;, whereas the right side is a function of &lt;span class="math"&gt;\(P\)&lt;/span&gt; only. This tells us that there is fundamental &amp;#8212; distribution-dependent &amp;#8212; lower limit on the uncertainty one can achieve when attempting to estimate &lt;em&gt;any parameter characterizing a distribution&lt;/em&gt;. In particular, (\ref{CRsimple}) states that the best variance one can achieve scales like &lt;span class="math"&gt;\(O(1/n)\)&lt;/span&gt;, where &lt;span class="math"&gt;\(n\)&lt;/span&gt; is the number of samples available&lt;span class="math"&gt;\(^1\)&lt;/span&gt; &amp;#8212; very&amp;nbsp;interesting!&lt;/p&gt;
&lt;p&gt;Why is there a relationship between the left and right matrices in (\ref{cramer_rao_bound})? Basically, the right side relates to the inverse rate at which the probability of a given &lt;span class="math"&gt;\(x\)&lt;/span&gt; changes with &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;: If &lt;span class="math"&gt;\(P(x \vert \theta)\)&lt;/span&gt; is highly peaked, the gradient of &lt;span class="math"&gt;\(P(x \vert \theta)\)&lt;/span&gt; will take on large values. In this case, a typical observation &lt;span class="math"&gt;\(x\)&lt;/span&gt; will provide significant information relating to the true &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; value, allowing for unbiased &lt;span class="math"&gt;\(\hat{\theta}\)&lt;/span&gt; estimates that have low variance. In the opposite limit, where typical observations are not very &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;-informative, unbiased &lt;span class="math"&gt;\(\hat{\theta}\)&lt;/span&gt; estimates must have large variance&lt;span class="math"&gt;\(^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We now turn to the proof of&amp;nbsp;(\ref{cramer_rao_bound}).&lt;/p&gt;
&lt;h3&gt;Theorem&amp;nbsp;proof&lt;/h3&gt;
&lt;p&gt;Our discussion here expounds on that in the &lt;a href="http://sfb649.wiwi.hu-berlin.de/fedc_homepage/xplore/tutorials/mvahtmlframe74.html"&gt;online text&lt;/a&gt; of Cízek, Härdle, and Weron. We start by deriving a few simple lemmas. We state and derive these sequentially&amp;nbsp;below.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lemma 1&lt;/strong&gt; Let &lt;span class="math"&gt;\(T_j(\{x_i\}) \equiv \partial_{\theta_j} \log P(\{x_i\}; \vec{\theta})\)&lt;/span&gt; be a function of a set of independent sample values &lt;span class="math"&gt;\(\{x_i\}\)&lt;/span&gt;. Then, the average of &lt;span class="math"&gt;\(T_j(\{x_i\})\)&lt;/span&gt; is&amp;nbsp;zero.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Proof:&lt;/em&gt; We obtain the average of &lt;span class="math"&gt;\(T_j(\{x_i\})\)&lt;/span&gt; through integration over the &lt;span class="math"&gt;\(\{x_i\}\)&lt;/span&gt;, weighted by &lt;span class="math"&gt;\(P\)&lt;/span&gt;,
&lt;/p&gt;
&lt;div class="math"&gt;$$
\int P(\{x_i\};\vec{\theta}) \partial_{\theta_j} \log P(\{x_i\}; \vec{\theta}) d\vec{x} = \int P \frac{\partial_{\theta_j} P}{P} d\vec{x} = \partial_{\theta_j} \int P d\vec{x} = \partial_{\theta_j} 1 = 0. \tag{7}
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Lemma 2&lt;/strong&gt;. The covariance matrix of an unbiased &lt;span class="math"&gt;\(\hat{\theta}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\vec{T}\)&lt;/span&gt; is the identity&amp;nbsp;matrix.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Proof:&lt;/em&gt; Using (\ref{cov}), the assumed fact that &lt;span class="math"&gt;\(\hat{\theta}\)&lt;/span&gt; is unbiased, and Lemma 1, we&amp;nbsp;have
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
cov \left (\hat{\theta}(\{x_i\}), \vec{T}(\{x_i\}) \right)_{jk} &amp;amp;= \int P(\{x_i\}) (\hat{\theta}_j - \theta_j ) \partial_{\theta_k} \log P(\{x_i\}) d\vec{x}\\ &amp;amp; = \int (\hat{\theta}_j - \theta_j ) \partial_{\theta_k} P d\vec{x} \\
&amp;amp;= -\int P \partial_{\theta_k} (\hat{\theta}_j - \theta_j ) d \vec{x} \tag{8}
\end{align}
$$&lt;/div&gt;
&lt;p&gt;
Here, we have integrated by parts in the last line. Now, &lt;span class="math"&gt;\(\partial_{\theta_k} \theta_j = \delta_{jk}\)&lt;/span&gt;. Further, &lt;span class="math"&gt;\(\partial_{\theta_k} \hat{\theta}_j = 0\)&lt;/span&gt;, since &lt;span class="math"&gt;\(\hat{\theta}\)&lt;/span&gt; is a function of the samples &lt;span class="math"&gt;\(\{x_i\}\)&lt;/span&gt; only. Plugging these results into the last line, we&amp;nbsp;obtain
&lt;/p&gt;
&lt;div class="math"&gt;$$
cov \left (\hat{\theta}, \vec{T} \right)_{jk} = \delta_{jk} \int P d\vec{x} = \delta_{jk}. \tag{9}
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Lemma 3&lt;/strong&gt;. The covariance matrix of &lt;span class="math"&gt;\(\vec{T}\)&lt;/span&gt; is &lt;span class="math"&gt;\(n\)&lt;/span&gt; times the covariance matrix of &lt;span class="math"&gt;\(\nabla_{\vec{\theta}} \log P(x_1 ; \vec{\theta})\)&lt;/span&gt; &amp;#8212; a single-sample version of &lt;span class="math"&gt;\(\vec{T}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Proof:&lt;/em&gt; From the definition of &lt;span class="math"&gt;\(\vec{T}\)&lt;/span&gt;, we&amp;nbsp;have
&lt;/p&gt;
&lt;div class="math"&gt;$$
T_j = \partial_{\theta_j} \log P(\{x_i\}, \vec{\theta}) = \sum_{i=1}^n \partial_{\theta_j} \log P(x_i, \vec{\theta}), \tag{10}
$$&lt;/div&gt;
&lt;p&gt;
where the last line follows from the fact that the &lt;span class="math"&gt;\(\{x_i\}\)&lt;/span&gt; are independent, so that &lt;span class="math"&gt;\(P(\{x_i\}, \vec{\theta}) = \prod P(x_i; \vec{\theta})\)&lt;/span&gt;. The sum on the right side of the above equation is a sum of &lt;span class="math"&gt;\(n\)&lt;/span&gt; independent, identically-distributed random variables. If follows that their covariance matrix is &lt;span class="math"&gt;\(n\)&lt;/span&gt; times that for any&amp;nbsp;individual.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lemma 4&lt;/strong&gt;. Let &lt;span class="math"&gt;\(x\)&lt;/span&gt; and &lt;span class="math"&gt;\(y\)&lt;/span&gt; be two scalar stationary random variables. Then, their correlation coefficient is defined to be &lt;span class="math"&gt;\(\rho \equiv \frac{cov(x,y)}{\sigma(x) \sigma(y)}\)&lt;/span&gt;. This&amp;nbsp;satisfies
&lt;/p&gt;
&lt;div class="math"&gt;$$
-1 \leq \rho \leq 1 \label{c_c} \tag{11}
$$&lt;/div&gt;
&lt;p&gt;&lt;em&gt;Proof:&lt;/em&gt; Consider the variance of &lt;span class="math"&gt;\(\frac{x}{\sigma(x)}+\frac{y}{\sigma(y)}\)&lt;/span&gt;. This&amp;nbsp;is
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
var \left( \frac{x}{\sigma(x)}+\frac{y}{\sigma(y)} \right) &amp;amp;= \frac{\sigma^2(x)}{\sigma^2(x)} + 2\frac{ cov(x,y)}{\sigma(x) \sigma(y)} + \frac{\sigma^2(y)}{\sigma^2(y)} \\
&amp;amp;= 2 + 2 \frac{ cov(x,y)}{\sigma(x) \sigma(y)} \geq 0. \tag{12}
\end{align}
$$&lt;/div&gt;
&lt;p&gt;
This gives the left side of (\ref{c_c}). Similarly, considering the variance of &lt;span class="math"&gt;\(\frac{x}{\sigma(x)}-\frac{y}{\sigma(y)}\)&lt;/span&gt; gives the right&amp;nbsp;side.&lt;/p&gt;
&lt;p&gt;We&amp;#8217;re now ready to prove the Cramer-Rao&amp;nbsp;result.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof of Cramer-Rao inequality&lt;/strong&gt;. Consider the correlation coefficient of the two scalars &lt;span class="math"&gt;\(\vec{a} \cdot \hat{\theta}\)&lt;/span&gt; and &lt;span class="math"&gt;\( \vec{b} \cdot \vec{T}\)&lt;/span&gt;, with &lt;span class="math"&gt;\(\vec{a}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\vec{b}\)&lt;/span&gt; some constant vectors. Using (\ref{fact}) and Lemma 2, this can be written&amp;nbsp;as
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
\rho &amp;amp; \equiv \frac{cov(\vec{a} \cdot \hat{\theta} ,\vec{b} \cdot \vec{T})}{\sqrt{var(\vec{a} \cdot \hat{\theta})var(\vec{b} \cdot \vec{T})}} \\
&amp;amp;= \frac{\vec{a}^T \cdot \vec{b}}{\left(\vec{a}^T \cdot cov(\hat{\theta}, \hat{\theta}) \cdot \vec{a} \right)^{1/2} \left( \vec{b}^T \cdot cov(\vec{T},\vec{T}) \cdot \vec{b} \right)^{1/2}}\leq 1. \tag{13}
\end{align}
$$&lt;/div&gt;
&lt;p&gt;
The last inequality here follows from Lemma 4. We can find the direction &lt;span class="math"&gt;\(\hat{b}\)&lt;/span&gt; where the bound above is most tight &amp;#8212; at fixed &lt;span class="math"&gt;\(\vec{a}\)&lt;/span&gt; &amp;#8212; by maximizing the numerator while holding the denominator fixed in value. Using a Lagrange multiplier to hold &lt;span class="math"&gt;\(\left( \vec{b}^T \cdot cov(\vec{T},\vec{T}) \cdot \vec{b} \right) \equiv 1\)&lt;/span&gt;, the numerator&amp;#8217;s extremum occurs&amp;nbsp;where
&lt;/p&gt;
&lt;div class="math"&gt;$$
\vec{a}^T + 2 \lambda \vec{b}^T \cdot cov(\vec{T},\vec{T}) = 0 \ \ \to \ \ \vec{b}^T = - \frac{1}{2 \lambda} \vec{a}^T \cdot cov(\vec{T}, \vec{T})^{-1}. \tag{14}
$$&lt;/div&gt;
&lt;p&gt;
Plugging this form into the prior line, we&amp;nbsp;obtain
&lt;/p&gt;
&lt;div class="math"&gt;$$
- \frac{\vec{a}^T \cdot cov(\vec{T},\vec{T})^{-1} \cdot \vec{a}}{\left(\vec{a}^T \cdot cov(\hat{\theta}, \hat{\theta}) \cdot \vec{a} \right)^{1/2} \left(\vec{a}^T \cdot cov(\vec{T},\vec{T})^{-1} \cdot \vec{a} \right)^{1/2}}\leq 1. \tag{15}
$$&lt;/div&gt;
&lt;p&gt;
Squaring and rearranging terms, we&amp;nbsp;obtain
&lt;/p&gt;
&lt;div class="math"&gt;$$
\vec{a}^T \cdot \left (cov(\hat{\theta},\hat{\theta}) - cov(\vec{T},\vec{T})^{-1} \right ) \cdot \vec{a} \geq 0. \tag{16}
$$&lt;/div&gt;
&lt;p&gt;
This holds for any \(\vec{a}\), implying that &lt;span class="math"&gt;\(cov(\hat{\theta}, \hat{\theta}) - cov(\vec{T},\vec{T})^{-1}\)&lt;/span&gt; is positive semi-definite &amp;#8212; see (\ref{pd}). Applying Lemma 3, we obtain the result&lt;span class="math"&gt;\(^3\)&lt;/span&gt;. &lt;span class="math"&gt;\(\blacksquare\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Thank you for reading &amp;#8212; we hope you&amp;nbsp;enjoyed.&lt;/p&gt;
&lt;p&gt;[1] More generally, (\ref{fact}) tells us that an observation similar to (\ref{CRsimple}) holds for any linear combination of the &lt;span class="math"&gt;\(\{\theta_i\}\)&lt;/span&gt;. Notice also that the proof we provide here could also be applied to any individual &lt;span class="math"&gt;\(\theta_i\)&lt;/span&gt;, giving &lt;span class="math"&gt;\(\sigma^2(\hat{\theta}_i) \geq 1/n \times 1/\langle(\partial_{\theta_i} \log P)^2\rangle\)&lt;/span&gt;. This is easier to apply than (\ref{cramer_rao_bound}), but is less&amp;nbsp;stringent.&lt;/p&gt;
&lt;p&gt;[2] It might be challenging to intuit the exact function that appears on the right side of &lt;span class="math"&gt;\((\ref{cramer_rao_bound})\)&lt;/span&gt;. However, the appearance of &lt;span class="math"&gt;\(\log P\)&lt;/span&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt;s does make some intuitive sense, as it allows the derivatives involved to measure rates of change relative to typical values, &lt;span class="math"&gt;\(\nabla_{\theta} P / P\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;[3] The discussion here covers the &amp;#8220;standard proof&amp;#8221; of the Cramer-Rao result. Its brilliance is that it allows one to work with scalars. In contrast, when attempting to find my own proof, I began with the fact that all covariance matrices are positive definite. Applying this result to the covariance matrix of a linear combination of &lt;span class="math"&gt;\(\hat{\theta}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\vec{T}\)&lt;/span&gt;, one can quickly get to results similar in form to the Cramer-Rao bound, but not quite identical. After significant work, I was eventually able to show that &lt;span class="math"&gt;\(\sqrt{cov(\hat{\theta},\hat{\theta})} - 1/\sqrt{cov(\vec{T},\vec{T}) } \geq 0\)&lt;/span&gt;. However, I have yet to massage my way to the final result using this approach &amp;#8212; the difficulty being that the matrices involved don&amp;#8217;t commute. By working with scalars from the start, the proof here cleanly avoids all such&amp;nbsp;issues.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Statistics"></category></entry><entry><title>Reshaping Data in R</title><link href="https://efavdb.com/reshaping-data-in-r" rel="alternate"></link><published>2015-06-06T19:17:00-07:00</published><updated>2015-06-06T19:17:00-07:00</updated><author><name>Cathy Yeh</name></author><id>tag:efavdb.com,2015-06-06:/reshaping-data-in-r</id><summary type="html">&lt;p&gt;Today, we&amp;#8217;ll talk about reshaping data in R. At the same time, we&amp;#8217;ll see how for-loops can be avoided by using R functionals (functions of functions). Functionals are faster than for-loops and make code easier to read by clearly laying out the intent of a&amp;nbsp;loop.&lt;/p&gt;
&lt;h3&gt;The&amp;nbsp;Task …&lt;/h3&gt;</summary><content type="html">&lt;p&gt;Today, we&amp;#8217;ll talk about reshaping data in R. At the same time, we&amp;#8217;ll see how for-loops can be avoided by using R functionals (functions of functions). Functionals are faster than for-loops and make code easier to read by clearly laying out the intent of a&amp;nbsp;loop.&lt;/p&gt;
&lt;h3&gt;The&amp;nbsp;Task&lt;/h3&gt;
&lt;p&gt;Suppose you&amp;#8217;re given the two tables below, saved as individual .csv files.  Each table contains measurements of plant height on specific days. The plants are split into fertilizer treatment groups A and B, and each plant can be uniquely identified by its&amp;nbsp;id.&lt;/p&gt;
&lt;p&gt;Day&amp;nbsp;1&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;day&lt;/th&gt;
&lt;th&gt;group&lt;/th&gt;
&lt;th&gt;id&lt;/th&gt;
&lt;th&gt;height&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;A&lt;/td&gt;
&lt;td&gt;A.1&lt;/td&gt;
&lt;td&gt;1.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;A&lt;/td&gt;
&lt;td&gt;A.2&lt;/td&gt;
&lt;td&gt;1.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;B&lt;/td&gt;
&lt;td&gt;B.1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;B&lt;/td&gt;
&lt;td&gt;B.2&lt;/td&gt;
&lt;td&gt;0.9&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Day&amp;nbsp;2&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;day&lt;/th&gt;
&lt;th&gt;group&lt;/th&gt;
&lt;th&gt;id&lt;/th&gt;
&lt;th&gt;height&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;A&lt;/td&gt;
&lt;td&gt;A.1&lt;/td&gt;
&lt;td&gt;1.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;A&lt;/td&gt;
&lt;td&gt;A.2&lt;/td&gt;
&lt;td&gt;1.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;B&lt;/td&gt;
&lt;td&gt;B.1&lt;/td&gt;
&lt;td&gt;2.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;B&lt;/td&gt;
&lt;td&gt;B.2&lt;/td&gt;
&lt;td&gt;1.9&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Your task is to split the data by treatment group rather than day. The desired output is one file per group, with day on the vertical of the new tables, plant id on the horizontal, and height as the value inside the table. For example, the new table for group A would&amp;nbsp;be:&lt;/p&gt;
&lt;p&gt;Group&amp;nbsp;A&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;day&lt;/th&gt;
&lt;th&gt;A.1&lt;/th&gt;
&lt;th&gt;A.2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1.1&lt;/td&gt;
&lt;td&gt;1.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1.5&lt;/td&gt;
&lt;td&gt;1.5&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This example can be manually formatted in Excel pretty quickly, but in real life, data only looks like this if you&amp;#8217;re in first grade. So, we&amp;#8217;re going to take a little time to write a script now in order to save a lot of time in the&amp;nbsp;future.&lt;/p&gt;
&lt;h3&gt;The code broken&amp;nbsp;down&lt;/h3&gt;
&lt;p&gt;First, let&amp;#8217;s load some libraries that are useful for reshaping&amp;nbsp;data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;plyr&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;reshape2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;(To install, e.g. the plyr package, simply type &lt;code&gt;install.packages("plyr")&lt;/code&gt; in the R&amp;nbsp;console.)&lt;/p&gt;
&lt;p&gt;Then store the full names, including directory path, of the files in a character vector. Here, the input .csv files are stored in the subdirectory&amp;nbsp;&amp;#8220;input&amp;#8221;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;in_files&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;list.files&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;input&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pattern&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;*.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;full.names&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;T&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now, we&amp;#8217;ll use &lt;code&gt;lapply(...)&lt;/code&gt;, one of base R&amp;#8217;s functionals, to read the input files into a list of data frames, which we call&amp;nbsp;list_dfperday.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;list_dfperday&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;lapply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;in_files&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;read.csv&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;code&gt;lapply()&lt;/code&gt; loops over the the names of the files in &lt;code&gt;in_files&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;to each file, apply the function &lt;code&gt;read.csv()&lt;/code&gt;, which reads the contents of the file into a data&amp;nbsp;frame&lt;/li&gt;
&lt;li&gt;output the result into a list of data frames, called dfs, of the same length as the character vector containing the file&amp;nbsp;names&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Bind together (rbind) the data frames in the list by rows into a single data frame, so we can more conveniently subset the data by&amp;nbsp;group.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;ldply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;list_dfperday&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rbind&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In the &lt;code&gt;plyr&lt;/code&gt; package, the first two letters in the &amp;#8220;&amp;#8212;ply&amp;#8221; functions indicate what type of object is being transformed into another. In this case, we are reshaping a list (&amp;#8216;l&amp;#8217;) of data frames into a data frame (&amp;#8216;d&amp;#8217;), hence&amp;nbsp;&amp;#8220;ldply&amp;#8221;.&lt;/p&gt;
&lt;p&gt;Define a function to cast the data into a data frame with a shape specified by the formula &lt;code&gt;day ~ id&lt;/code&gt;: per &lt;em&gt;day&lt;/em&gt; (x-variable), output the corresponding height (value.var) of each plant &lt;em&gt;id&lt;/em&gt;&amp;nbsp;(y-variable).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;cast_short&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mydata&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="nf"&gt;dcast&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mydata&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;day&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="n"&gt;id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;value.var&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;height&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So the function &amp;#8220;cast_short&amp;#8221; takes as its argument a data frame &amp;#8220;mydata&amp;#8221; (containing columns &amp;#8220;day&amp;#8221; and &amp;#8220;id&amp;#8221;) and returns a reshaped data frame with &amp;#8220;day&amp;#8221; on the rows and &amp;#8220;id&amp;#8221; on the&amp;nbsp;columns.&lt;/p&gt;
&lt;p&gt;Now apply the function we just defined to each treatment group. Store the outcome of each application of &lt;code&gt;cast_short&lt;/code&gt; into a list. (Also compare to the earlier use of &lt;code&gt;ldply&lt;/code&gt;.)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;list_dfpergroup&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;dlply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;.(group&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;cast_short&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;It&amp;#8217;s finally time to write the formatted data frames to .csv files. We&amp;#8217;ll output them in a new directory called &amp;#8220;output&amp;#8221; with the aid of another base R functional &lt;code&gt;mapply&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;outdir&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;output&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;outnames&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;paste0&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;names&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;list_dfpergroup&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nf"&gt;dir.create&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;outdir&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;mapply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="nf"&gt;write.csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nf"&gt;file.path&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;outdir&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;row.names&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;F&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="n"&gt;list_dfpergroup&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;outnames&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;code&gt;mapply&lt;/code&gt; allows functions with multiple arguments to be applied in a loop. Here, we&amp;#8217;ve defined an anonymous, i.e. unnamed, function within mapply that has two arguments: the value of &lt;em&gt;x&lt;/em&gt; is given by list_dfpergroup, and &lt;em&gt;y&lt;/em&gt; is given by the character vector outnames. In other words, mapply steps through the list and vector simultaneously, writing list_dfpergroup&lt;span class="math"&gt;\(_i\)&lt;/span&gt; to a file named outnames&lt;span class="math"&gt;\(_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;The code in one&amp;nbsp;piece&lt;/h3&gt;
&lt;p&gt;Now here&amp;#8217;s the code all in one piece. Note that the core reshaping takes place in three lines, starting at the line containing &lt;code&gt;ldply&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;plyr&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;reshape2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;in_files&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;list.files&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;input&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pattern&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;*.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;full.names&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;T&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;list_dfperday&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;lapply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;in_files&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;read.csv&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;ldply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;list_dfperday&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rbind&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;cast_short&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mydata&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="nf"&gt;dcast&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mydata&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;day&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="n"&gt;id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;value.var&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;height&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;list_dfpergroup&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;dlply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;.(group&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;cast_short&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;outdir&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;output&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;outnames&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;paste0&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;names&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;list_dfpergroup&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nf"&gt;dir.create&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;outdir&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;mapply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="nf"&gt;write.csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nf"&gt;file.path&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;outdir&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;row.names&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;F&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="n"&gt;list_dfpergroup&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;outnames&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Programming"></category><category term="programming"></category><category term="R"></category></entry><entry><title>MLB week 7 overview, week 8 posted</title><link href="https://efavdb.com/mlb-week-7-overview-week-8-posted" rel="alternate"></link><published>2015-05-30T22:32:00-07:00</published><updated>2015-05-30T22:32:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2015-05-30:/mlb-week-7-overview-week-8-posted</id><content type="html">&lt;p&gt;52/99 this past week, or 53% accuracy overall. Breakdown by point spread follows, and new predictions are &lt;a href="http://efavdb.github.io/weekly-mlb-predictions"&gt;posted&lt;/a&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Point spread&lt;/th&gt;
&lt;th&gt;# games&lt;/th&gt;
&lt;th&gt;Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&amp;lt;= 1&lt;/td&gt;
&lt;td&gt;32&lt;/td&gt;
&lt;td&gt;56%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;lt;=2&lt;/td&gt;
&lt;td&gt;17&lt;/td&gt;
&lt;td&gt;53%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;lt;=5&lt;/td&gt;
&lt;td&gt;34&lt;/td&gt;
&lt;td&gt;47%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;gt;5&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;td&gt;56%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content><category term="MLB prediction project"></category></entry><entry><title>MLB week 6 overview, week 7 posted</title><link href="https://efavdb.com/mlb-week-6-overview-week-7-posted" rel="alternate"></link><published>2015-05-25T15:27:00-07:00</published><updated>2015-05-25T15:27:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2015-05-25:/mlb-week-6-overview-week-7-posted</id><content type="html">&lt;p&gt;Super tough week this past week. We went 43/94, or 47% accuracy overall. Breakdown by point spread below. New predictions are &lt;a href="http://efavdb.github.io/weekly-mlb-predictions"&gt;posted&lt;/a&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Point spread&lt;/th&gt;
&lt;th&gt;# games&lt;/th&gt;
&lt;th&gt;Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&amp;lt;= 1&lt;/td&gt;
&lt;td&gt;32&lt;/td&gt;
&lt;td&gt;53%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;lt;=2&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;td&gt;33%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;lt;=5&lt;/td&gt;
&lt;td&gt;33&lt;/td&gt;
&lt;td&gt;45%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;gt;5&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;td&gt;43%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content><category term="MLB prediction project"></category></entry><entry><title>MLB week 5 review, week 6 posted</title><link href="https://efavdb.com/mlb-week-5-review-week-6-posted" rel="alternate"></link><published>2015-05-16T09:07:00-07:00</published><updated>2015-05-16T09:07:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2015-05-16:/mlb-week-5-review-week-6-posted</id><summary type="html">&lt;p&gt;Back on track: 52/96, or 54% accuracy overall this past week. We&amp;#8217;ve now made a number of improvements to the algorithm, and we&amp;#8217;re excited to see how our performance continues from here on out. New predictions are &lt;a href="http://efavdb.github.io/weekly-nba-predictions"&gt;posted&lt;/a&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Point spread&lt;/th&gt;
&lt;th&gt;# games&lt;/th&gt;
&lt;th&gt;Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&amp;lt;= 1&lt;/td&gt;
&lt;td&gt;34&lt;/td&gt;
&lt;td&gt;53%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;lt;=2&lt;/td&gt;
&lt;td&gt;16 …&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;</summary><content type="html">&lt;p&gt;Back on track: 52/96, or 54% accuracy overall this past week. We&amp;#8217;ve now made a number of improvements to the algorithm, and we&amp;#8217;re excited to see how our performance continues from here on out. New predictions are &lt;a href="http://efavdb.github.io/weekly-nba-predictions"&gt;posted&lt;/a&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Point spread&lt;/th&gt;
&lt;th&gt;# games&lt;/th&gt;
&lt;th&gt;Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&amp;lt;= 1&lt;/td&gt;
&lt;td&gt;34&lt;/td&gt;
&lt;td&gt;53%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;lt;=2&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;td&gt;56%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;lt;=5&lt;/td&gt;
&lt;td&gt;26&lt;/td&gt;
&lt;td&gt;50%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;gt;5&lt;/td&gt;
&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;60%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content><category term="MLB prediction project"></category></entry><entry><title>MLB week 4 review, week 5 posted</title><link href="https://efavdb.com/mlb-week-4-review-week-5-posted" rel="alternate"></link><published>2015-05-10T20:34:00-07:00</published><updated>2015-05-10T20:34:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2015-05-10:/mlb-week-4-review-week-5-posted</id><summary type="html">&lt;p&gt;Relatively poor results this past week, 44 of 94 games, 47% accuracy overall. Given our relatively strong performance in the &lt;span class="caps"&gt;NBA&lt;/span&gt; this past season, our early &lt;span class="caps"&gt;MLB&lt;/span&gt; results have been a bit disappointing. However, after doing a bit of learnin&amp;#8217;, we realized an interesting point: Professional baseball teams are simply …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Relatively poor results this past week, 44 of 94 games, 47% accuracy overall. Given our relatively strong performance in the &lt;span class="caps"&gt;NBA&lt;/span&gt; this past season, our early &lt;span class="caps"&gt;MLB&lt;/span&gt; results have been a bit disappointing. However, after doing a bit of learnin&amp;#8217;, we realized an interesting point: Professional baseball teams are simply much more balanced than their &lt;span class="caps"&gt;NBA&lt;/span&gt; counterparts. A statistic illustrating this point is provided by the winning rates of the top teams: This past year, the Warriors had a win rate just over 80%, while the top team in the &lt;span class="caps"&gt;MLB&lt;/span&gt; last season, the Angels, had just under a 61% win rate. This difference explains why we can&amp;#8217;t achieve 70% win prediction accuracy in the &lt;span class="caps"&gt;MLB&lt;/span&gt;. Instead, we&amp;#8217;ll be shooting for just 56% accuracy this season &amp;#8212; this may sound modest, but we expect this rate will be a challenge to meet. To work towards that goal, we&amp;#8217;ve already begun to implement a number of improvements to our algorithm. New predictions are &lt;a href="http://efavdb.github.io/weekly-nba-predictions"&gt;posted&lt;/a&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Point spread&lt;/th&gt;
&lt;th&gt;# games&lt;/th&gt;
&lt;th&gt;Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&amp;lt;= 1&lt;/td&gt;
&lt;td&gt;23&lt;/td&gt;
&lt;td&gt;43%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;lt;=2&lt;/td&gt;
&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;45%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;lt;=5&lt;/td&gt;
&lt;td&gt;32&lt;/td&gt;
&lt;td&gt;41%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;gt;5&lt;/td&gt;
&lt;td&gt;19&lt;/td&gt;
&lt;td&gt;63%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content><category term="MLB prediction project"></category></entry><entry><title>MLB week 3 review, week 4 predictions</title><link href="https://efavdb.com/mlb-week-3-review-week-4-predictions" rel="alternate"></link><published>2015-05-02T08:24:00-07:00</published><updated>2015-05-02T08:24:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2015-05-02:/mlb-week-3-review-week-4-predictions</id><summary type="html">&lt;p&gt;This past week, we correctly predicted 51 of 93 games, which equates to a 54.8% accuracy. This is consistent with past weeks. It&amp;#8217;s strange that our accuracy level here is so far below what we were achieving for the &lt;span class="caps"&gt;NBA&lt;/span&gt;. Presumably, this represents some underlying increased variance in …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This past week, we correctly predicted 51 of 93 games, which equates to a 54.8% accuracy. This is consistent with past weeks. It&amp;#8217;s strange that our accuracy level here is so far below what we were achieving for the &lt;span class="caps"&gt;NBA&lt;/span&gt;. Presumably, this represents some underlying increased variance in the &lt;span class="caps"&gt;MLB&lt;/span&gt; games that have happened so far. At any rate, our poor early start has motivated us to get back to work on algorithm improvements&amp;#8230; New predictions are now &lt;a href="http://efavdb.github.io/weekly-nba-predictions"&gt;posted&lt;/a&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Point spread&lt;/th&gt;
&lt;th&gt;# games&lt;/th&gt;
&lt;th&gt;Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&amp;lt;= 1&lt;/td&gt;
&lt;td&gt;26&lt;/td&gt;
&lt;td&gt;65%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;lt;=2&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;td&gt;57%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;lt;=5&lt;/td&gt;
&lt;td&gt;34&lt;/td&gt;
&lt;td&gt;44%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;gt;5&lt;/td&gt;
&lt;td&gt;19&lt;/td&gt;
&lt;td&gt;58%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content><category term="MLB prediction project"></category></entry><entry><title>MLB week 2 review, week 3 predictions</title><link href="https://efavdb.com/mlb-week-2-review-week-3-predictions" rel="alternate"></link><published>2015-04-25T09:01:00-07:00</published><updated>2015-04-25T09:01:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2015-04-25:/mlb-week-2-review-week-3-predictions</id><summary type="html">&lt;p&gt;This week, we went 52/95, giving 54.7% accuracy, a value comparable to the previous week. We believe this relatively modest early showing is largely due to the fact that we haven&amp;#8217;t yet got many games to train on for the current season. As more games come in …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This week, we went 52/95, giving 54.7% accuracy, a value comparable to the previous week. We believe this relatively modest early showing is largely due to the fact that we haven&amp;#8217;t yet got many games to train on for the current season. As more games come in, our accuracy should increase, perhaps approaching 70% &amp;#8212; the value we were hitting at the end of this last &lt;span class="caps"&gt;NBA&lt;/span&gt; season. We also are working on implementing a few improvements to our algorithm, which we hope will also give us a boost. New predictions are now &lt;a href="http://efavdb.github.io/weekly-nba-predictions"&gt;posted&lt;/a&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Point spread&lt;/th&gt;
&lt;th&gt;# games&lt;/th&gt;
&lt;th&gt;Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&amp;lt;= 1&lt;/td&gt;
&lt;td&gt;31&lt;/td&gt;
&lt;td&gt;52%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;lt;=2&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;td&gt;50%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;lt;=5&lt;/td&gt;
&lt;td&gt;32&lt;/td&gt;
&lt;td&gt;56%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;gt;5&lt;/td&gt;
&lt;td&gt;18&lt;/td&gt;
&lt;td&gt;61%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content><category term="MLB prediction project"></category></entry><entry><title>The mean shift clustering algorithm</title><link href="https://efavdb.com/mean-shift" rel="alternate"></link><published>2015-04-21T09:17:00-07:00</published><updated>2015-04-21T09:17:00-07:00</updated><author><name>Damien RJ</name></author><id>tag:efavdb.com,2015-04-21:/mean-shift</id><summary type="html">&lt;h3&gt;Mean shift&amp;nbsp;clustering&lt;/h3&gt;
&lt;p&gt;Mean shift clustering is a general non-parametric cluster finding procedure &amp;#8212; introduced by Fukunaga and Hostetler [&lt;a href="#1"&gt;1&lt;/a&gt;], and popular within the computer vision field. Nicely, and in contrast to the more-well-known K-means clustering algorithm, the output of mean shift does not depend on any explicit assumptions on the …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Mean shift&amp;nbsp;clustering&lt;/h3&gt;
&lt;p&gt;Mean shift clustering is a general non-parametric cluster finding procedure &amp;#8212; introduced by Fukunaga and Hostetler [&lt;a href="#1"&gt;1&lt;/a&gt;], and popular within the computer vision field. Nicely, and in contrast to the more-well-known K-means clustering algorithm, the output of mean shift does not depend on any explicit assumptions on the shape of the point distribution, the number of clusters, or any form of random&amp;nbsp;initialization.&lt;/p&gt;
&lt;p&gt;We describe the mean shift algorithm in some detail in the &lt;a href="#Tech"&gt;technical background section&lt;/a&gt; at the end of this post. However, its essence is readily explained in a few words: Essentially, mean shift treats the clustering problem by supposing that all points given represent samples from some underlying probability density function, with regions of high sample density corresponding to the local maxima of this distribution. To find these local maxima, the algorithm works by allowing the points to attract each other, via what might be considered a short-ranged &amp;#8220;gravitational&amp;#8221; force. Allowing the points to gravitate towards areas of higher density, one can show that they will eventually coalesce at a series of points, close to the local maxima of the distribution. Those data points that converge to the same local maxima are considered to be members of the same&amp;nbsp;cluster.&lt;/p&gt;
&lt;p&gt;In the next couple of sections, we illustrate application of the algorithm to a couple of problems. We make use of the python package &lt;a href="http://scikit-learn.org/stable/"&gt;SkLearn&lt;/a&gt;, which contains a mean shift implementation. Following this, we provide a quick discussion and an appendix on technical&amp;nbsp;details.&lt;/p&gt;
&lt;h3&gt;Mean shift clustering in&amp;nbsp;action&lt;/h3&gt;
&lt;p&gt;In today&amp;#8217;s post we will have two examples. First, we will show how to use mean shift clustering to identify clusters of data in a 2D data set. Second, we will use the algorithm to segment a picture based on the colors in the image. To do this we need a handful of libraries from sklearn, numpy, matplotlib, and the Python Imaging Library (&lt;span class="caps"&gt;PIL&lt;/span&gt;) to handle reading in a jpeg&amp;nbsp;image.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.cluster&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;MeanShift&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;estimate_bandwidth&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.datasets.samples_generator&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;make_blobs&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;itertools&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;cycle&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;PIL&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Image&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Finding clusters in a 2D data&amp;nbsp;set&lt;/h4&gt;
&lt;p&gt;This first example is based off of the sklearn &lt;a href="http://scikit-learn.org/stable/auto_examples/cluster/plot_mean_shift.html"&gt;tutorial&lt;/a&gt; for mean shift clustering: We generate data points centered at 4 locations, making use of sklearn&amp;#8217;s make_blobs library. To apply the clustering algorithm to the points generated, we must first set the attractive interaction length between examples, also know as the algorithm&amp;#8217;s bandwidth. Sklearn&amp;#8217;s implementation contains a built-in function that allows it to automatically estimate a reasonable value for this, based upon the typical distance between examples. We make use of that below, carry out the clustering, and then plot the&amp;nbsp;results.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="o"&gt;%%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Generate&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;data&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;centers&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;[1, 1&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;-.75, -1&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;1, -1&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;-3, 2&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="err"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;make_blobs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;centers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;centers&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;cluster_std&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.6&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="o"&gt;%%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;Compute&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;clustering&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;with&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;MeanShift&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;The&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;bandwidth&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;can&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;be&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;automatically&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;estimated&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;bandwidth&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;estimate_bandwidth&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;quantile&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;ms&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;MeanShift&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bandwidth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;bandwidth&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;bin_seeding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;ms&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;ms&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels_&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;cluster_centers&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;ms&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cluster_centers_&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="n"&gt;n_clusters_&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="o"&gt;%%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Plot&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;result&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="n"&gt;colors&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;cycle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;bgrcmykbgrcmykbgrcmykbgrcmyk&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ow"&gt;in&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_clusters_&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;colors&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;my_members&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;cluster_center&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;cluster_centers&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;my_members, 0&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;my_members, 1&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;.&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cluster_center&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;0&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;cluster_center&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;1&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;o&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;markerfacecolor&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;markeredgecolor&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;k&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;markersize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Estimated number of clusters: %d&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;n_clusters_&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;As you can see below, the algorithm has found clusters centered on each of the blobs we&amp;nbsp;generated.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/03/plot1.png"&gt;&lt;img alt="Plot 1" src="https://efavdb.com/wp-content/uploads/2015/03/plot1.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;Segmenting a color&amp;nbsp;photo&lt;/h4&gt;
&lt;p&gt;In the first example, we were using mean shift clustering to look for spatial clusters. In our second example, we will instead explore 3D color space, &lt;span class="caps"&gt;RGB&lt;/span&gt;, by considering pixel values taken from an image of a toy car. The procedure is similar &amp;#8212; here, we cluster points in 3d, but instead of having data(x,y) we have data(r,g,b) taken from the image&amp;#8217;s &lt;span class="caps"&gt;RGB&lt;/span&gt; pixel values. Clustering these color values in this 3d space returns a series of clusters, where the pixels in those clusters are similar in &lt;span class="caps"&gt;RGB&lt;/span&gt; space. Recoloring pixels according to their cluster, we obtain a segmentation of the original&amp;nbsp;image.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;#%%&lt;/span&gt; &lt;span class="n"&gt;Part&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Color&lt;/span&gt; &lt;span class="n"&gt;image&lt;/span&gt; &lt;span class="n"&gt;segmentation&lt;/span&gt; &lt;span class="k"&gt;using&lt;/span&gt; &lt;span class="n"&gt;mean&lt;/span&gt; &lt;span class="n"&gt;shift&lt;/span&gt;

&lt;span class="n"&gt;image&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Image&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;toy.jpg&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;image&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nb"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="n"&gt;Need&lt;/span&gt; &lt;span class="k"&gt;to&lt;/span&gt; &lt;span class="k"&gt;convert&lt;/span&gt; &lt;span class="n"&gt;image&lt;/span&gt; &lt;span class="k"&gt;into&lt;/span&gt; &lt;span class="n"&gt;feature&lt;/span&gt; &lt;span class="nb"&gt;array&lt;/span&gt; &lt;span class="n"&gt;based&lt;/span&gt;
&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="k"&gt;on&lt;/span&gt; &lt;span class="n"&gt;rgb&lt;/span&gt; &lt;span class="n"&gt;intensities&lt;/span&gt;
&lt;span class="n"&gt;flat_image&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="n"&gt;Estimate&lt;/span&gt; &lt;span class="n"&gt;bandwidth&lt;/span&gt;
&lt;span class="n"&gt;bandwidth2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;estimate_bandwidth&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;flat_image&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;quantile&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ms&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;MeanShift&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bandwidth2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bin_seeding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ms&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;flat_image&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ms&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels_&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;Plot&lt;/span&gt; &lt;span class="n"&gt;image&lt;/span&gt; &lt;span class="n"&gt;vs&lt;/span&gt; &lt;span class="n"&gt;segmented&lt;/span&gt; &lt;span class="n"&gt;image&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;off&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;851&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1280&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;off&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The bottom image below illustrates that one can effectively use this approach to identify the key shapes within an image, all without doing any image processing to get rid of glare or background &amp;#8212; pretty great!
&lt;a href="https://efavdb.com/wp-content/uploads/2015/03/test-e1428358370930.png"&gt;&lt;img alt="" src="https://efavdb.com/wp-content/uploads/2015/03/test-e1428358370930.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Discussion&lt;/h3&gt;
&lt;p&gt;Although mean shift is a reasonably versatile algorithm, it has primarily been applied to problems in computer vision, where it has been used for image segmentation, clustering, and video tracking. Application to big data problems can be challenging due to the fact the algorithm can become relatively slow in this limit. However, research is presently underway to speed up its convergence, which should enable its application to larger data&amp;nbsp;sets.&lt;/p&gt;
&lt;p&gt;Mean shift&amp;nbsp;pros:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;No assumptions on the shape or number of data&amp;nbsp;clusters.&lt;/li&gt;
&lt;li&gt;The procedure only has one parameter, the&amp;nbsp;bandwidth.&lt;/li&gt;
&lt;li&gt;Output doesn&amp;#8217;t depend on&amp;nbsp;initializations.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Mean shift&amp;nbsp;cons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Output does depend on bandwidth: too small and convergence is slow, too large and some clusters may be&amp;nbsp;missed.&lt;/li&gt;
&lt;li&gt;Computationally expensive for large feature&amp;nbsp;spaces.&lt;/li&gt;
&lt;li&gt;Often slower than K-Means&amp;nbsp;clustering&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Technical details&amp;nbsp;follow.&lt;/p&gt;
&lt;h3 id="Tech"&gt;Technical&amp;nbsp;background&lt;/h3&gt;
&lt;h4&gt;Kernel density&amp;nbsp;estimation&lt;/h4&gt;
&lt;p&gt;A general formulation of the mean shift algorithm can be developed through consideration of density kernels. These effectively work by smearing out each point example in space over some small window. Summing up the mass from each of these smeared units gives an estimate for the probability density at every point in space (by smearing, we are able to obtain estimates at locations that do not sit exactly atop any example). This approach is often referred to as &lt;a href="http://en.wikipedia.org/wiki/Kernel_density_estimation"&gt;kernel density estimation&lt;/a&gt; &amp;#8212; a method for density estimation that often converges more quickly than binning, or histogramming, and one that also nicely returns a continuous estimate for the density&amp;nbsp;function.&lt;/p&gt;
&lt;p&gt;To illustrate, suppose we are given a data set &lt;span class="math"&gt;\(\{\textbf{u}_i\}\)&lt;/span&gt; of points in d-dimensional space, sampled from some larger population, and that we have chosen a kernel &lt;span class="math"&gt;\(K\)&lt;/span&gt; having bandwidth parameter &lt;span class="math"&gt;\(h\)&lt;/span&gt;. Together, these data and kernel function return the following kernel density estimator for the full population&amp;#8217;s density&amp;nbsp;function
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
f_K(\textbf{u}) = \frac{1}{nh^d}\sum\limits_{i=1}^n K(\frac{\textbf{u}-\textbf{u}_i}{h})
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
The kernel (smearing) function here is required to satisfy the following two&amp;nbsp;conditions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\int K(\textbf{u})d\textbf{u} =&amp;nbsp;1\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(K(\textbf{u})=K(\vert \textbf{u} \vert)\)&lt;/span&gt; for all values of &lt;span class="math"&gt;\(\textbf{u}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The first requirement is needed to ensure that our estimate is normalized, and the second is associated with the symmetry of our space. Two popular kernel functions that satisfy these conditions are given&amp;nbsp;by&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Flat/Uniform &lt;span class="math"&gt;\(&lt;div class="math"&gt;\begin{align}
    K(\textbf{u}) = \frac{1}{2}\left\{
    \begin{array}{lr}
    1 &amp;amp; -1 \le \vert \textbf{u} \vert \le 1\
    0 &amp;amp; else
    \end{array}
    \right.
    \end{align}&lt;/div&gt;\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Gaussian = &lt;span class="math"&gt;\(K(\textbf{u}) = \frac{1}{\left(2\pi\right)^{d/2}} e^{-\frac{1}{2} \vert \textbf{u}&amp;nbsp;\vert^2}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Below we plot an example in 1-d using the gaussian kernel to estimate the density of some population along the x-axis. You can see that each sample point adds a small Gaussian to our estimate, centered about it: The equations above may look a bit intimidating, but the graphic here should clarify that the concept is pretty&amp;nbsp;straightforward.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/03/KDE-plot.png"&gt;&lt;img alt="" src="https://efavdb.com/wp-content/uploads/2015/03/KDE-plot.png"&gt;&lt;/a&gt;
Example of a kernel density estimation using a gaussian kernel for each data point: Adding up small Gaussians about each example returns our net estimate for the total density, the black&amp;nbsp;curve.&lt;/p&gt;
&lt;h4&gt;Mean shift&amp;nbsp;algorithm&lt;/h4&gt;
&lt;p&gt;Recall that the basic goal of the mean shift algorithm is to move particles in the direction of local increasing density. To obtain an estimate for this direction, a gradient is applied to the kernel density estimate discussed above. Assuming an angularly symmetric kernel function, &lt;span class="math"&gt;\(K(\textbf{u}) = K(\vert \textbf{u} \vert)\)&lt;/span&gt;, one can show that this gradient takes the&amp;nbsp;form
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
\nabla f_K(\textbf{u}) = \frac{2}{nh^{d+2}} \left ( \sum\limits_{i=1}^n g(\left \vert \frac{\textbf{u}-\textbf{u}_i}{h} \right \vert) \right ) \textbf{m}(\textbf{u}).
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
where
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \textbf{m}(\textbf{u}) = \left ( \frac{\sum\limits_{i=1}^n \textbf{u}_i g(\left \vert \frac{\textbf{u}-\textbf{u}_i}{h} \right \vert)}{\sum\limits_{i=1}^n g(\left \vert \frac{\textbf{u}-\textbf{u}_i}{h} \right \vert)}-\textbf{u} \right ),
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
and &lt;span class="math"&gt;\(g(\vert \textbf{u} \vert ) = -K'(\vert \textbf{u} \vert)\)&lt;/span&gt; is the derivative of the selected kernel profile. The vector &lt;span class="math"&gt;\(\textbf{m}(\textbf{u})\)&lt;/span&gt; here, called the mean shift vector, points in the direction of increasing density &amp;#8212; the direction we must move our example. With this estimate, then, the mean shift algorithm protocol&amp;nbsp;becomes&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Compute the mean shift vector &lt;span class="math"&gt;\(\textbf{m}(\textbf{u}_i)\)&lt;/span&gt;, evaluated at the location of each training example &lt;span class="math"&gt;\(\textbf{u}_i\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Move each example from &lt;span class="math"&gt;\(\textbf{u}_i \to \textbf{u}_i +&amp;nbsp;\textbf{m}(\textbf{u}_i)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Repeat until convergence &amp;#8212; ie, until the particles have reached&amp;nbsp;equilibrium.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a final step, one determines which examples have ended up at the same points, marking them as members of the same&amp;nbsp;cluster.&lt;/p&gt;
&lt;p&gt;For a proof of convergence and further mathematical details, see &lt;a href="https://courses.csail.mit.edu/6.869/handouts/PAMIMeanshift.pdf"&gt;Comaniciu &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; Meer (2002)&lt;/a&gt; [&lt;a href="#2"&gt;2&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;​1. Fukunaga and Hostetler, &amp;#8220;The Estimation of the Gradient of a Density Function, with Applications in Pattern Recognition&amp;#8221;, &lt;span class="caps"&gt;IEEE&lt;/span&gt; Transactions on Information Theory vol 21 , pp 32-40 ,1975
2. Dorin Comaniciu and Peter Meer, Mean Shift : A Robust approach towards feature space analysis, &lt;span class="caps"&gt;IEEE&lt;/span&gt; Transactions on Pattern Analysis and Machine Intelligence vol 24 No 5 May&amp;nbsp;2002.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Methods, Theory"></category></entry><entry><title>NBA week 22 results, NBA season review, MLB week 1</title><link href="https://efavdb.com/nba-week-22-results-nba-season-review-mlb-week-1" rel="alternate"></link><published>2015-04-19T19:46:00-07:00</published><updated>2015-04-19T19:46:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2015-04-19:/nba-week-22-results-nba-season-review-mlb-week-1</id><summary type="html">&lt;p&gt;We finished the &lt;span class="caps"&gt;NBA&lt;/span&gt; regular season strong, with correct guesses on 41 of 56 games this past week &amp;#8212; 73% accuracy. Overall, our season average was roughly 65%. This is significantly lower than those returned by the bookies this year, whose accuracy overall sat just under 70%. The differences there likely …&lt;/p&gt;</summary><content type="html">&lt;p&gt;We finished the &lt;span class="caps"&gt;NBA&lt;/span&gt; regular season strong, with correct guesses on 41 of 56 games this past week &amp;#8212; 73% accuracy. Overall, our season average was roughly 65%. This is significantly lower than those returned by the bookies this year, whose accuracy overall sat just under 70%. The differences there likely have much to do with information relating to individual players (eg injuries) that would be difficult to take into account in our team-centric modeling approach. It also took us a number of weeks to come up with a solid way of choosing our model&amp;#8217;s hyperparameters. With these issues identified/settled, we anticipate improvements on these numbers next year. We won&amp;#8217;t be modeling the &lt;span class="caps"&gt;NBA&lt;/span&gt; playoffs, so this will be our last basketball post for the current season &amp;#8212; we had a&amp;nbsp;blast!&lt;/p&gt;
&lt;p&gt;With regards to the &lt;span class="caps"&gt;MLB&lt;/span&gt;, in our first week, we guessed correctly on 52/93 games, giving 56%. A somewhat poor result, fueled in part by the lackluster Giants. New predictions are up, with better accuracy hoped for this&amp;nbsp;week!&lt;/p&gt;</content><category term="MLB prediction project, NBA prediction project"></category></entry><entry><title>MLB predictions take off!</title><link href="https://efavdb.com/mlb-predictions-take-off" rel="alternate"></link><published>2015-04-11T10:04:00-07:00</published><updated>2015-04-11T10:04:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2015-04-11:/mlb-predictions-take-off</id><summary type="html">&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-11-at-10.51.35-AM.png"&gt;&lt;img alt="Screen Shot 2015-04-11 at 10.51.35 AM" src="https://efavdb.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-11-at-10.51.35-AM.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Announcing: EFavDB&amp;#8217;s first major league baseball prediction project!&lt;/strong&gt; Just as in our corresponding &lt;span class="caps"&gt;NBA&lt;/span&gt; project, we will be providing free-of-charge &lt;a href="http://efavdb.github.io/weekly-mlb-predictions"&gt;game winner predictions&lt;/a&gt; on a weekly basis. In addition, we&amp;#8217;ve implemented a &lt;a href="http://efavdb.github.io/mlb-dash"&gt;&lt;span class="caps"&gt;MLB&lt;/span&gt; dashboard&lt;/a&gt; (screenshot above) where you can get a quick summary of each team&amp;#8217;s prior …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-11-at-10.51.35-AM.png"&gt;&lt;img alt="Screen Shot 2015-04-11 at 10.51.35 AM" src="https://efavdb.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-11-at-10.51.35-AM.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Announcing: EFavDB&amp;#8217;s first major league baseball prediction project!&lt;/strong&gt; Just as in our corresponding &lt;span class="caps"&gt;NBA&lt;/span&gt; project, we will be providing free-of-charge &lt;a href="http://efavdb.github.io/weekly-mlb-predictions"&gt;game winner predictions&lt;/a&gt; on a weekly basis. In addition, we&amp;#8217;ve implemented a &lt;a href="http://efavdb.github.io/mlb-dash"&gt;&lt;span class="caps"&gt;MLB&lt;/span&gt; dashboard&lt;/a&gt; (screenshot above) where you can get a quick summary of each team&amp;#8217;s prior results. You can also use the dashboard to check out our guesses for who beat whom, were they to play today,&amp;nbsp;etc.&lt;/p&gt;
&lt;p&gt;The algorithm we&amp;#8217;ll be applying this season is similar to that discussed &lt;a href="http://efavdb.github.io/nba-learner-2013-14-warmup"&gt;here&lt;/a&gt;, for the &lt;span class="caps"&gt;NBA&lt;/span&gt;. We&amp;#8217;ve set its parameters to generate reasonably conservative predictions, but ones that will also lead to interesting upset predictions when appropriate. Unlike many other sites, our predictions do not take into account the over-under values published by bookies, and so are independent of their opinions. Looking forward to a great&amp;nbsp;season!&lt;/p&gt;</content><category term="MLB prediction project"></category></entry><entry><title>NBA week 21 summary, week 22 predictions</title><link href="https://efavdb.com/nba-week-21-summary-week-22-predictions" rel="alternate"></link><published>2015-04-10T21:15:00-07:00</published><updated>2015-04-10T21:15:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2015-04-10:/nba-week-21-summary-week-22-predictions</id><summary type="html">&lt;p&gt;Another great week for the algorithm, going 35 for 49, or 71.4% accuracy! The coming week&amp;#8217;s predictions are now &lt;a href="http://efavdb.github.io/weekly-nba-predictions"&gt;up&lt;/a&gt;. Again, apologies for the delayed post this week. Automation should be coming&amp;nbsp;soon.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Point spread&lt;/th&gt;
&lt;th&gt;# games&lt;/th&gt;
&lt;th&gt;Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&amp;lt; 6&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;td&gt;60%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5-10&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;55%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;11-15&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;78%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;gt;15&lt;/td&gt;
&lt;td&gt;14 …&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;</summary><content type="html">&lt;p&gt;Another great week for the algorithm, going 35 for 49, or 71.4% accuracy! The coming week&amp;#8217;s predictions are now &lt;a href="http://efavdb.github.io/weekly-nba-predictions"&gt;up&lt;/a&gt;. Again, apologies for the delayed post this week. Automation should be coming&amp;nbsp;soon.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Point spread&lt;/th&gt;
&lt;th&gt;# games&lt;/th&gt;
&lt;th&gt;Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&amp;lt; 6&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;td&gt;60%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5-10&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;55%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;11-15&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;78%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;gt;15&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;td&gt;93%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content><category term="NBA prediction project"></category></entry><entry><title>NBA week 20 summary, week 21 predictions</title><link href="https://efavdb.com/nba-week-20-summary-week-21-predictions" rel="alternate"></link><published>2015-04-03T17:48:00-07:00</published><updated>2015-04-03T17:48:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2015-04-03:/nba-week-20-summary-week-21-predictions</id><summary type="html">&lt;p&gt;Excellent bounce-back from last week: 41 for 53, or 77.4% accuracy! The new predictions are &lt;a href="http://efavdb.github.io/weekly-nba-predictions"&gt;posted&lt;/a&gt;. Apologies for the delay, which was due to our new-found employment. We hope to automate the posting and accuracy checks&amp;nbsp;soon.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Point spread&lt;/th&gt;
&lt;th&gt;# games&lt;/th&gt;
&lt;th&gt;Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&amp;lt; 6&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;td&gt;63%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5-10&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;67%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;11-15&lt;/td&gt;
&lt;td&gt;16 …&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;</summary><content type="html">&lt;p&gt;Excellent bounce-back from last week: 41 for 53, or 77.4% accuracy! The new predictions are &lt;a href="http://efavdb.github.io/weekly-nba-predictions"&gt;posted&lt;/a&gt;. Apologies for the delay, which was due to our new-found employment. We hope to automate the posting and accuracy checks&amp;nbsp;soon.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Point spread&lt;/th&gt;
&lt;th&gt;# games&lt;/th&gt;
&lt;th&gt;Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&amp;lt; 6&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;td&gt;63%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5-10&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;67%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;11-15&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;td&gt;94%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;gt;15&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;83%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content><category term="NBA prediction project"></category></entry><entry><title>NBA week 19 summary, week 20 predictions</title><link href="https://efavdb.com/nba-week-19-summary-week-20-predictions" rel="alternate"></link><published>2015-03-27T09:58:00-07:00</published><updated>2015-03-27T09:58:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2015-03-27:/nba-week-19-summary-week-20-predictions</id><content type="html">&lt;p&gt;Sadly lackluster week: 28 for 54, or 51.9% accuracy. The new predictions are now &lt;a href="http://efavdb.github.io/weekly-nba-predictions"&gt;posted&lt;/a&gt;, which will hopefully bring us better luck. Details by point-spread for previous week&amp;nbsp;below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Point spread&lt;/th&gt;
&lt;th&gt;# games&lt;/th&gt;
&lt;th&gt;Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&amp;lt; 6&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;33%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5-10&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;27%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;11-15&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;td&gt;64%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;gt;15&lt;/td&gt;
&lt;td&gt;17&lt;/td&gt;
&lt;td&gt;71%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content><category term="NBA prediction project"></category></entry><entry><title>Forecasting Bike Sharing Demand</title><link href="https://efavdb.com/bike-share-forecasting" rel="alternate"></link><published>2015-03-26T09:20:00-07:00</published><updated>2015-03-26T09:20:00-07:00</updated><author><name>Damien RJ</name></author><id>tag:efavdb.com,2015-03-26:/bike-share-forecasting</id><summary type="html">&lt;p&gt;In today&amp;#8217;s post, we document our efforts at applying a gradient boosted trees model to forecast bike sharing demand &amp;#8212; a problem posed in a recent &lt;a href="https://www.kaggle.com/c/bike-sharing-demand"&gt;Kaggle&lt;/a&gt;competition. For those not familiar, Kaggle is a site where one can compete with other data scientists on various data challenges. Top scorers …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In today&amp;#8217;s post, we document our efforts at applying a gradient boosted trees model to forecast bike sharing demand &amp;#8212; a problem posed in a recent &lt;a href="https://www.kaggle.com/c/bike-sharing-demand"&gt;Kaggle&lt;/a&gt;competition. For those not familiar, Kaggle is a site where one can compete with other data scientists on various data challenges. Top scorers often win prize money, but the site more generally serves as a great place to grab interesting datasets to explore and play with. With the simple optimization steps discussed below, we managed to quickly move from the bottom 10% of the competition &amp;#8212; our first-pass attempt&amp;#8217;s score &amp;#8212; to the top 10%: no&amp;nbsp;sweat!&lt;/p&gt;
&lt;p&gt;Our work here was inspired by a &lt;a href="http://blog.dato.com/using-gradient-boosted-trees-to-predict-bike-sharing-demand"&gt;post&lt;/a&gt; by the people at &lt;a href="http://blog.dato.com/"&gt;Dato.com&lt;/a&gt;, who used the bike sharing competition as an opportunity to demonstrate their software. Here, we go through a similar, but more detailed discussion using the python package &lt;a href="http://scikit-learn.org/stable/"&gt;SKlearn&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Bike sharing systems are gaining popularity around the world &amp;#8212; there are over 500 different programs currently operating in various cities, and counting!  These programs are generally funded through rider membership fees, or through pay-to-ride one time rental fees. Key to the convenience of these programs is the fact that riders who pick up a bicycle from one station can return the bicycle to any other in the network.  These systems generate a great deal of data relating to various ride details, including travel time, departure location, arrival location, and so on.  This data has the potential to be very useful for studying city mobility. The data we look at today comes from Washington D. C.&amp;#8217;s &lt;a href="https://www.capitalbikeshare.com/"&gt;Capital Bikeshare&lt;/a&gt; program. The goal of the Kaggle competition is to leverage the historical data provided in order to forecast future bike rental demand within the&amp;nbsp;city.&lt;/p&gt;
&lt;p&gt;As we detailed in an earlier &lt;a href="http://efavdb.github.io/notes-on-trees"&gt;post&lt;/a&gt;, boosting provides a general method for increasing a machine learning algorithm&amp;#8217;s performance. Here, in order to model the Capital Bikeshare program&amp;#8217;s demand curves, we&amp;#8217;ll be applying a gradient boosted trees model (&lt;span class="caps"&gt;GBM&lt;/span&gt;).  Simply put, &lt;span class="caps"&gt;GBM&lt;/span&gt;&amp;#8217;s are constructed by iteratively fitting a series of simple trees to a training set, where each new tree attempts to fit the residuals, or errors, of the trees that came before it. With the addition of each new tree the training error is further reduced, typically asymptoting to a reasonably accurate model &amp;#8212; but one must watch out for overfitting &amp;#8212; see&amp;nbsp;below!&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Loading package and&amp;nbsp;data&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Below, we show the relevant commands needed to load all the packages and training/test data we will be using. We work with the package &lt;a href="http://pandas.pydata.org/"&gt;Pandas&lt;/a&gt;, whose DataFrame data structure enables quick and easy data loading and wrangling. We take advantage of this package immediately below, where in the last lines we use its parse_dates method to convert the first column of our provided data &amp;#8212; which can be downloaded &lt;a href="https://www.kaggle.com/c/bike-sharing-demand"&gt;here&lt;/a&gt; &amp;#8212; from string to datetime&amp;nbsp;format.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;math&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;ensemble&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.cross_validation&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;mean_absolute_error&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.grid_search&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;GridSearchCV&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;datetime&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;

&lt;span class="c1"&gt;#Load Data with pandas, and parse the&lt;/span&gt;
&lt;span class="c1"&gt;#first column into datetime&lt;/span&gt;
&lt;span class="n"&gt;train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;train.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;parse_dates&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="kp"&gt;test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;test.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;parse_dates&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The training data provided contains the following&amp;nbsp;fields:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;datetime&lt;/em&gt;&lt;/strong&gt; - hourly date + timestamp
&lt;strong&gt;&lt;em&gt;season&lt;/em&gt;&lt;/strong&gt; -  1 = spring, 2 = summer, 3 = fall, 4 = winter
&lt;strong&gt;&lt;em&gt;holiday&lt;/em&gt;&lt;/strong&gt; - whether the day is considered a holiday
&lt;strong&gt;&lt;em&gt;workingday&lt;/em&gt;&lt;/strong&gt; - whether the day is neither a weekend nor holiday
&lt;strong&gt;&lt;em&gt;weather&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Clear, Few clouds, Partly cloudy, Partly&amp;nbsp;cloudy&lt;/li&gt;
&lt;li&gt;Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds,&amp;nbsp;Mist&lt;/li&gt;
&lt;li&gt;Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered&amp;nbsp;clouds&lt;/li&gt;
&lt;li&gt;Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow +&amp;nbsp;Fog&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;temp&lt;/em&gt;&lt;/strong&gt; - temperature in Celsius
&lt;strong&gt;&lt;em&gt;atemp&lt;/em&gt;&lt;/strong&gt; - &amp;#8220;feels like&amp;#8221; temperature in Celsius
&lt;strong&gt;&lt;em&gt;humidity&lt;/em&gt;&lt;/strong&gt; - relative humidity
&lt;strong&gt;&lt;em&gt;windspeed&lt;/em&gt;&lt;/strong&gt; - wind speed
&lt;strong&gt;&lt;em&gt;casual&lt;/em&gt;&lt;/strong&gt; - number of non-registered user rentals initiated
&lt;strong&gt;&lt;em&gt;registered&lt;/em&gt;&lt;/strong&gt; - number of registered user rentals initiated
&lt;strong&gt;&lt;em&gt;count&lt;/em&gt;&lt;/strong&gt; - number of total&amp;nbsp;rentals&lt;/p&gt;
&lt;p&gt;The data provided spans two years. The training set contains the first 19 days of each month considered, while the test set data corresponds to the remaining days in each&amp;nbsp;month.&lt;/p&gt;
&lt;p&gt;Looking ahead, we anticipate that the year, month, day of week, and hour will serve as important features for characterizing the bike demand at any given moment.  These features are easily extracted from the datetime formatted-values loaded above. In the following lines, we add these features to our&amp;nbsp;DataFrames.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="n"&gt;Feature&lt;/span&gt; &lt;span class="n"&gt;engineering&lt;/span&gt;
&lt;span class="n"&gt;temp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DatetimeIndex&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;datetime&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;year&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;temp&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;year&lt;/span&gt;
&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;month&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;temp&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;month&lt;/span&gt;
&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;hour&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;temp&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hour&lt;/span&gt;
&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;weekday&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;temp&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weekday&lt;/span&gt;

&lt;span class="n"&gt;temp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DatetimeIndex&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;datetime&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;year&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;temp&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;year&lt;/span&gt;
&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;month&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;temp&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;month&lt;/span&gt;
&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;hour&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;temp&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hour&lt;/span&gt;
&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;weekday&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;temp&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weekday&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="n"&gt;Define&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="n"&gt;vector&lt;/span&gt;
&lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;season&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;holiday&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;workingday&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;weather&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="s1"&gt;&amp;#39;temp&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;atemp&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;humidity&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;windspeed&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;year&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="s1"&gt;&amp;#39;month&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;weekday&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;hour&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;&lt;strong&gt;Evaluation&amp;nbsp;metric&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The evaluation metric that Kaggle uses to rank competing algorithms is the Root Mean Squared Logarithmic Error (&lt;span class="caps"&gt;RMSLE&lt;/span&gt;).&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
J = \sqrt{\frac{1}{n} \sum_{i=1}^n [\ln(p_i + 1) - \ln(a_i+1)]^2 }
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Here,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(n\)&lt;/span&gt; is the number of hours in the test&amp;nbsp;set&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(p_i\)&lt;/span&gt; is the predicted number of bikes rented in a given&amp;nbsp;hour&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(a_i\)&lt;/span&gt; is the actual rent&amp;nbsp;count&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(ln(x)\)&lt;/span&gt; is the natural&amp;nbsp;logarithm&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With ranking determined as above, our aim becomes to accurately guess the natural logarithm of bike demand at different times (actually demand count plus one, in order to avoid infinities associated with times where demand is nil). To facilitate this, we add the logarithm of the casual, registered, and total counts to our training DataFrame&amp;nbsp;below.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;#the evaluation metric is the RMSE in the log domain,&lt;/span&gt;
&lt;span class="c1"&gt;#so we should transform the target columns into log domain as well.&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;col&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;casual&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;registered&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
  &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;log-&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log1p&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Notice that in the code above we use the &lt;span class="math"&gt;\(log1p()\)&lt;/span&gt; function instead of the more familiar &lt;span class="math"&gt;\(log(1+x)\)&lt;/span&gt;. For large values of &lt;span class="math"&gt;\(x\)&lt;/span&gt;, these two functions are actually equivalent. However, at very small values of &lt;span class="math"&gt;\(x\)&lt;/span&gt;, the two can disagree. The source of the discrepancy is floating point error: For very small &lt;span class="math"&gt;\(x\)&lt;/span&gt;, python will send &lt;span class="math"&gt;\(1+x \to 1\)&lt;/span&gt;, which when supplied as an argument to &lt;span class="math"&gt;\(log(1+x)\)&lt;/span&gt; will return &lt;span class="math"&gt;\(log(1)=0\)&lt;/span&gt;. The function &lt;span class="math"&gt;\(log1p(x) \sim x\)&lt;/span&gt; in this limit. The difference is not very important when the result is being added to other numbers, but can be very important in a multiplicative operation. We use this function instead for this reason. The inverse of &lt;span class="math"&gt;\(log(x+1)\)&lt;/span&gt; is &lt;span class="math"&gt;\(e^{x} -1\)&lt;/span&gt; &amp;#8212; an operation we will also need to make use of later, in order to return linear-scale demand values. We&amp;#8217;ll use an analog of the &lt;span class="math"&gt;\(log1p()\)&lt;/span&gt; function, numpy&amp;#8217;s &lt;span class="math"&gt;\(expm1()\)&lt;/span&gt; function, to carry out this&amp;nbsp;inversion.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Model&amp;nbsp;development&lt;/strong&gt;&lt;/h2&gt;
&lt;h4&gt;&lt;strong&gt;A first&amp;nbsp;pass&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;The Gradient Boosting Machine (&lt;span class="caps"&gt;GBM&lt;/span&gt;) we will be using has some associated hyperparameters that will eventually need to be optimized. These&amp;nbsp;include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;n_estimators = the number of boosting stages, or trees, to&amp;nbsp;use.&lt;/li&gt;
&lt;li&gt;max_depth = maximum depth of the individual regression&amp;nbsp;trees.&lt;/li&gt;
&lt;li&gt;learning_rate = shrinks the contribution of each tree by the learning&amp;nbsp;rate.&lt;/li&gt;
&lt;li&gt;in_samples_leaf = the minimum number of samples required to be at a leaf&amp;nbsp;node&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, in order to get our feet wet, we&amp;#8217;ll begin by just picking some ad hoc values for these parameters. The code below fits a &lt;span class="caps"&gt;GBM&lt;/span&gt; to the log-demand training data, and then converts predicted log-demand into the competition&amp;#8217;s required format &amp;#8212; in particular, the demand is output in linear&amp;nbsp;scale.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;ensemble&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GradientBoostingRegressor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;n_estimators&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;max_depth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;&amp;#39;log-count&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;result&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;result&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expm1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;result&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;datetime&amp;#39;&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;&amp;#39;datetime&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;count&amp;#39;&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="k"&gt;result&lt;/span&gt;&lt;span class="err"&gt;}&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;results1.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;index&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=[&lt;/span&gt;&lt;span class="n"&gt;&amp;#39;datetime&amp;#39;,&amp;#39;count&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In the last lines above, we have used the DataFrames to_csv() method in order to output results for competition submission. Example output is shown below. Without a hitch, we successfully submitted the results of this preliminary analysis to Kaggle. The only bad news was that our model scored in the bottom 10%. Fortunately, some simple optimizations that follow led to significant improvements in our&amp;nbsp;standing.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;datetime&lt;/th&gt;
&lt;th&gt;count&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;2011-01-20 0:00:00&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2011-01-20 0:01:00&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2011-01-20 0:02:00&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;&lt;strong&gt;Hyperparameter&amp;nbsp;tuning&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;We now turn to the challenge of tuning our &lt;span class="caps"&gt;GBM&lt;/span&gt;&amp;#8217;s hyperparameters. In order to carry this out, we segmented our training data into a training set and a validation set. The validation set allowed us to check the accuracy of our model locally, without having to submit to Kaggle. This also helped us to avoid overfitting&amp;nbsp;issues.&lt;/p&gt;
&lt;p&gt;As mentioned earlier, the training data provided covers the first 19 days of each month. In segmenting this data, we opted to use days 17-19 for validation. We then used this validation set to optimize the model&amp;#8217;s hyperparameters. As a first-pass at this, we again chose an ad hoc value for n_estimators, but optimized over the remaining degrees of freedom. The code follows, where we make use of GridSearchCV() to perform our parameter&amp;nbsp;sweep.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;#Split data into training and validation sets&lt;/span&gt;
&lt;span class="n"&gt;temp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DatetimeIndex&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;datetime&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;training&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;temp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;day&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;validation&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;temp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;day&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;param_grid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;learning_rate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.05&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
              &lt;span class="s1"&gt;&amp;#39;max_depth&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
              &lt;span class="s1"&gt;&amp;#39;min_samples_leaf&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;est&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ensemble&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GradientBoostingRegressor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_estimators&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# this may take awhile&lt;/span&gt;
&lt;span class="n"&gt;gs_cv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GridSearchCV&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;span class="n"&gt;est&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;param_grid&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_jobs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;log-count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# best hyperparameter setting&lt;/span&gt;
&lt;span class="n"&gt;gs_cv&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_params_&lt;/span&gt;

&lt;span class="c1"&gt;#Baseline error&lt;/span&gt;
&lt;span class="n"&gt;error_count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mean_absolute_error&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;validation&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;log-count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;gs_cv&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;validation&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;

&lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gs_cv&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expm1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;datetime&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;datetime&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;results2.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;datetime&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;Note: If you want to run n_jobs &amp;gt; 1 on a Windows machine, the script needs to be in an &amp;#8220;if &lt;strong&gt;name&lt;/strong&gt; == &amp;#8216;&lt;strong&gt;main&lt;/strong&gt;&amp;#8216;:&amp;#8221; block. Otherwise the script will&amp;nbsp;fail.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;day&lt;/th&gt;
&lt;th&gt;Best Parms&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;learning_rate&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;max_depth&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;min_samples_leaf&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The optimized parameters are shown above. Submitting the resulting model to Kaggle, we found that we had moved from the bottom 10% of models to the top 20%!  An awesome improvement, but we still have one final hyperparameter to&amp;nbsp;optimize.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Tuning the number of&amp;nbsp;estimators&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;In boosted models, training set performance will always improve as the number of estimators is increased. However, at large estimator number, overfitting can start to become an issue. Learning curves provide a method for optimization. These are constructed by plotting the error on both the training and validation sets as a function of the number of estimators used. The code below generates such a curve for our&amp;nbsp;model.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;error_train&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;error_validation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;501&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="n"&gt;clf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ensemble&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GradientBoostingRegressor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
  &lt;span class="n"&gt;n_estimators&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;05&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_depth&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;min_samples_leaf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;log-count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
  &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
  &lt;span class="n"&gt;error_train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
  &lt;span class="n"&gt;mean_absolute_error&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;log-count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;

  &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;validation&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
  &lt;span class="n"&gt;error_validation&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
  &lt;span class="n"&gt;mean_absolute_error&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;validation&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;log-count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;

&lt;span class="c1"&gt;#Plot the data&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;501&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;style&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;use&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;ggplot&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;error_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;k&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;error_validation&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;b&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Number of Estimators&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;18&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Error&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;18&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Train&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Validation&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;18&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Error vs. Number of Estimators&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="Error vs Number of Estimators" src="https://efavdb.com/wp-content/uploads/2015/03/figure_1-e1427234375629.png"&gt;&lt;/p&gt;
&lt;p&gt;Notice in the plot that by the time the number estimators in our &lt;span class="caps"&gt;GBM&lt;/span&gt; reaches about 80, the error of our model as applied to the validation set starts to slowly increase, though the error on the training set continues to decrease steadily. The diagnosis is that the model begins to overfit at this point. Moving forward, we will set n_estimators to 80, rather than 500, the value we were using above. Reducing the number of estimators reduced the calculated error and moved us to a higher position on the&amp;nbsp;leaderboard.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Separate models for registered and casual&amp;nbsp;users&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Reviewing the data, we see that we have info regarding two types of riders: casual and registered riders. It is plausible that each group&amp;#8217;s behavior differs, and that we might be able to improve our performance by modeling each separately. Below, we carry this out, and then also merge the two group&amp;#8217;s predicted values to obtain a net predicted demand. We also repeat the hyperparameter sweep steps covered above &amp;#8212; this returned similar values. Resubmitting the resulting model, we found we had increased our standing in the competition by a few&amp;nbsp;percent.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;merge_predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;model2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="c1"&gt;# Combine the predictions of two separately trained models.&lt;/span&gt;
  &lt;span class="c1"&gt;# The input models are in the log domain and returns the predictions&lt;/span&gt;
  &lt;span class="c1"&gt;# in original domain.&lt;/span&gt;
  &lt;span class="n"&gt;p1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expm1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
  &lt;span class="n"&gt;p2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expm1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
  &lt;span class="n"&gt;p_total&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p1&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;p2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p_total&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;est_casual&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ensemble&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GradientBoostingRegressor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_estimators&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;80&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;05&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;est_registered&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ensemble&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GradientBoostingRegressor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_estimators&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;80&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;05&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;param_grid2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;max_depth&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
               &lt;span class="s1"&gt;&amp;#39;_samples_leaf&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;gs_casual&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GridSearchCV&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;est_casual&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;param_grid2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_jobs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;log-casual&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;gs_registered&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GridSearchCV&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;est_registered&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;param_grid2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_jobs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;log-registered&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;result3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;merge_predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gs_casual&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;gs_registered&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;datetime&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;datetime&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;result3&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;results3.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;datetime&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The last step is to submit a final set of model predictions, this time training on the full labeled dataset provided. With these simple steps, we ended up in the top 11% on the competition&amp;#8217;s leaderboard with a rank of&amp;nbsp;280/2467!&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/03/score.png"&gt;&lt;img alt="score" src="https://efavdb.com/wp-content/uploads/2015/03/score.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;pre&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="n"&gt;est_casual&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ensemble&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GradientBoostingRegressor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;span class="n"&gt;n_estimators&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;80&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;05&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_depth&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;min_samples_leaf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;est_registered&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ensemble&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GradientBoostingRegressor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;span class="n"&gt;n_estimators&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;80&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;05&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_depth&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;min_samples_leaf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;est_casual&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;log-casual&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;est_registered&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;log-registered&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;result4&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;merge_predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;est_casual&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;est_registered&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;datetime&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;datetime&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;result4&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;results4.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;datetime&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;DISCUSSION&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;By iteratively tuning a &lt;span class="caps"&gt;GBM&lt;/span&gt;, we were able to quickly climb the leaderboard for this particular Kaggle competition. With further feature extraction work, we believe further improvements could readily be made. However, our goal here was only to practice our rapid development skills, so we won&amp;#8217;t be spending much time on further fine-tuning. At any rate, our results have convinced us that simple boosted models can often provide excellent&amp;nbsp;results.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/EFavDB/bike-forecast" title="GitHub Repo"&gt;&lt;img alt="Open GitHub Repo" src="https://efavdb.com/wp-content/uploads/2015/03/GitHub_Logo.png"&gt;&lt;/a&gt;
Open GitHub&amp;nbsp;Repo&lt;/p&gt;
&lt;p&gt;Note: With this post, we have begun to post our python scripts and data at GitHub. Clicking on the icon at left will take you to our repository. Feel free to stop by and take a&amp;nbsp;look!&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Case studies"></category></entry><entry><title>NBA week 18 summary, week 19 predictions</title><link href="https://efavdb.com/nba-week-18-summary-week-19-predictions" rel="alternate"></link><published>2015-03-20T10:29:00-07:00</published><updated>2015-03-20T10:29:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2015-03-20:/nba-week-18-summary-week-19-predictions</id><summary type="html">&lt;p&gt;Excellent week: 39 for 53, or 73.6% accuracy! New predictions are &lt;a href="http://efavdb.github.io/weekly-nba-predictions"&gt;up&lt;/a&gt;. This week, the algorithm continues to surprise, with lots of upset predictions. This includes a predicted &lt;span class="caps"&gt;OKC&lt;/span&gt; defeat of Atlanta tonight. Checking the &lt;a href="http://efavdb.github.io/nba-dash"&gt;dashboard&lt;/a&gt; for potential insight, we think this might be reasonable, given &lt;span class="caps"&gt;OKC&lt;/span&gt;&amp;#8217;s excellent …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Excellent week: 39 for 53, or 73.6% accuracy! New predictions are &lt;a href="http://efavdb.github.io/weekly-nba-predictions"&gt;up&lt;/a&gt;. This week, the algorithm continues to surprise, with lots of upset predictions. This includes a predicted &lt;span class="caps"&gt;OKC&lt;/span&gt; defeat of Atlanta tonight. Checking the &lt;a href="http://efavdb.github.io/nba-dash"&gt;dashboard&lt;/a&gt; for potential insight, we think this might be reasonable, given &lt;span class="caps"&gt;OKC&lt;/span&gt;&amp;#8217;s excellent home record coupled with Atlanta&amp;#8217;s relatively poor away record. You heard it here first&amp;nbsp;folks.&lt;/p&gt;
&lt;p&gt;&lt;span class="caps"&gt;UPDATE&lt;/span&gt;: &lt;span class="caps"&gt;OKC&lt;/span&gt; &lt;span class="caps"&gt;BEATS&lt;/span&gt; &lt;span class="caps"&gt;ATLANTA&lt;/span&gt;!!!&lt;/p&gt;
&lt;p&gt;Point-spread details of the past week are given&amp;nbsp;below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Point spread&lt;/th&gt;
&lt;th&gt;# games&lt;/th&gt;
&lt;th&gt;Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&amp;lt; 6&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;td&gt;67%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5-10&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;td&gt;71%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;11-15&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;73%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;gt;15&lt;/td&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;td&gt;85%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content><category term="NBA prediction project"></category></entry><entry><title>Machine Learning Methods: Decision trees and forests</title><link href="https://efavdb.com/notes-on-trees" rel="alternate"></link><published>2015-03-13T09:40:00-07:00</published><updated>2015-03-13T09:40:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2015-03-13:/notes-on-trees</id><summary type="html">&lt;p&gt;This post contains our crib notes on the basics of decision trees and forests. We first discuss the construction of individual trees, and then introduce random and boosted forests. We also discuss efficient implementations of greedy tree construction algorithms, showing that a single tree can be constructed in &lt;span class="math"&gt;\(O(k …&lt;/span&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;This post contains our crib notes on the basics of decision trees and forests. We first discuss the construction of individual trees, and then introduce random and boosted forests. We also discuss efficient implementations of greedy tree construction algorithms, showing that a single tree can be constructed in &lt;span class="math"&gt;\(O(k \times n \log n)\)&lt;/span&gt; time, given &lt;span class="math"&gt;\(n\)&lt;/span&gt; training examples having &lt;span class="math"&gt;\(k\)&lt;/span&gt; features each. We provide exercises on interesting related points and an appendix containing relevant python/sk-learn function&amp;nbsp;calls.  &lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Decision trees constitute a class of simple functions that are frequently used for carrying out regression and classification. They are constructed by hierarchically splitting a feature space into disjoint regions, where each split divides into two one of the already existing regions. In most common implementations, the splits are always taken along one of the feature axes, which causes the regions to be rectangular in shape. An example is shown in Fig. 1 below. In this example, a two-dimensional feature space is first split by a tree on &lt;span class="math"&gt;\(f_1\)&lt;/span&gt; &amp;#8212; one of the two features characterizing the space &amp;#8212; at value &lt;span class="math"&gt;\(s_a\)&lt;/span&gt;. This separates the space into two sets, that where &lt;span class="math"&gt;\(f_1 &amp;lt; s_a\)&lt;/span&gt; and that where &lt;span class="math"&gt;\(f_1 \geq s_a\)&lt;/span&gt;. Next, the tree further splits the first of these sets on feature &lt;span class="math"&gt;\(f_2\)&lt;/span&gt; at value &lt;span class="math"&gt;\(s_b\)&lt;/span&gt;. With these combined splits, the tree partitions the space into three disjoint regions, labeled &lt;span class="math"&gt;\(R_1, R_2,\)&lt;/span&gt; and &lt;span class="math"&gt;\(R_3\)&lt;/span&gt;, where, e.g., &lt;span class="math"&gt;\(R_1 = \{ \textbf{f} \vert f_1 &amp;lt; s_a, f_2 &amp;lt; s_b \}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/03/tree1.jpg"&gt;&lt;img alt="tree1" src="https://efavdb.com/wp-content/uploads/2015/03/tree1.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Once a decision tree is constructed, it can be used for making predictions on unlabeled feature vectors &amp;#8212; i.e., points in feature space not included in our training set. This is done by first deciding which of the regions a new feature vector belongs to, and then returning as its hypothesis label an average over the training example labels within that region: The mean of the region&amp;#8217;s training labels is returned for regression problems and the mode for classification problems. For instance, the tree in Fig. 1 would return an average of the five training examples in &lt;span class="math"&gt;\(R_1\)&lt;/span&gt; (represented by red dots) when asked to make a hypothesis for any and all other points in that&amp;nbsp;region.&lt;/p&gt;
&lt;p&gt;The art and science of tree construction is in deciding how many splits should be taken and where those splits should take place. The goal is to find a tree that provides a reasonable, piece-wise constant approximation to the underlying distribution or function that has generated the training data provided. This can be attempted through choosing a tree that breaks space up into regions such that the examples in any given region have identical &amp;#8212; or at least similar &amp;#8212; labels. We discuss some common approaches to finding such trees in the next&amp;nbsp;section.&lt;/p&gt;
&lt;p&gt;Individual trees have the important benefit of being easy to interpret and visualize, but they are often not as accurate as other common machine learning algorithms. However, individual trees can be used as simple building blocks with which to construct more complex, competitive models. In the third section of this note, we discuss three very popular constructions of this sort: bagging, random forests (a variant on bagging), and boosting. We then discuss the runtime complexity of tree/forest construction and conclude with a summary, exercises, and an appendix containing example python&amp;nbsp;code.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Constructing individual decision&amp;nbsp;trees&lt;/strong&gt;&lt;/h2&gt;
&lt;h4&gt;&lt;strong&gt;Regression&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Regression tree construction typically proceeds by attempting to minimize a squared error cost function: Given a training set &lt;span class="math"&gt;\(T \equiv \{t_j = (\textbf{f}_j, y_j) \}\)&lt;/span&gt; of feature vectors and corresponding real-valued labels, this is given by&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{treecost} \tag{1} 
J = \sum_{R_i} \sum_{t_j \in R_i } \left ( \overline{y}_{R_i} - y_j \right)^2,  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
where &lt;span class="math"&gt;\(\overline{y}_{R_i}\)&lt;/span&gt; is the mean training label in region &lt;span class="math"&gt;\(R_i\)&lt;/span&gt;. This mean training label is the hypothesis returned by the tree for all points in &lt;span class="math"&gt;\(R_i\)&lt;/span&gt;, including its training examples. Therefore, (\ref{treecost}) is a measure of the accuracy of the tree as applied to the training&amp;nbsp;set.&lt;/p&gt;
&lt;p&gt;Unfortunately, actually minimizing (\ref{treecost}) over any large subset of trees can be a numerically challenging task. This is true whenever you have a large number of features or training examples. Consequently, different approximate methods are generally taken to find good candidate trees. Two typical methods&amp;nbsp;follow:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Greedy algorithm&lt;/em&gt;: The tree is constructed recursively, one branching step at a time. At each step, one takes the split that will most significantly reduce the cost function &lt;span class="math"&gt;\(J\)&lt;/span&gt;, relative to its current value. In this way, after &lt;span class="math"&gt;\(k-1\)&lt;/span&gt; splits, a tree with &lt;span class="math"&gt;\(k\)&lt;/span&gt; regions (leaves) is obtained &amp;#8212; Fig. 2 provides an illustration of this process. The algorithm terminates whenever some specified stopping criterion is satisfied, examples of which are given&amp;nbsp;below.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Randomized algorithm&lt;/em&gt;: Randomized tree-search protocols can sometimes find global minima inaccessible to the gradient-descent-like greedy algorithm. These randomized protocols also proceed recursively. However, at each step, some randomization is introduced by hand. For example, one common approach is to select &lt;span class="math"&gt;\(r\)&lt;/span&gt; candidate splits through random sampling at each branching point. The candidate split that most significantly reduces &lt;span class="math"&gt;\(J\)&lt;/span&gt; is selected, and the process repeats. The benefit of this approach is that it can sometimes find paths that appear suboptimal in their first few steps, but are ultimately&amp;nbsp;favorable.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/03/treebuild.jpg"&gt;&lt;img alt="treebuild" src="https://efavdb.com/wp-content/uploads/2015/03/treebuild.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Classification&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;In classification problems, the training labels take on a discrete set of values, often having no numerical significance. This means that a squared-error cost function, like that in (\ref{treecost}) &amp;#8212; cannot be directly applied as a useful accuracy score for guiding classification tree construction. Instead, three other cost functions are often considered, each providing a different measure of the class purity of the different regions &amp;#8212; that is, they attempt to measure whether or not a given region consists of training examples that are mostly of the same class. These three measures are the error rate (&lt;span class="math"&gt;\(E\)&lt;/span&gt;), the Gini index (&lt;span class="math"&gt;\(G\)&lt;/span&gt;), and the cross-entropy (&lt;span class="math"&gt;\(CE\)&lt;/span&gt;): If we write &lt;span class="math"&gt;\(N_i\)&lt;/span&gt; for the number of training examples in region &lt;span class="math"&gt;\(R_i\)&lt;/span&gt;, and &lt;span class="math"&gt;\(p_{i,j}\)&lt;/span&gt; for the fraction of these that have class label &lt;span class="math"&gt;\(j\)&lt;/span&gt;, then these three cost functions are given by&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{errorrate}  \tag{2} 
E &amp;amp;=&amp;amp; \sum_{R_i} N_i \times \left ( 1 - \max_{j} p_{i,j}\right) \\ \label{gini} \tag{3} 
G &amp;amp;=&amp;amp; \sum_{R_i, j}N_i \times p_{i,j}\left ( 1 - p_{i,j} \right) \\ \label{crossentropy} \tag{4} 
CE &amp;amp;=&amp;amp; - \sum_{R_i, j} N_i \times p_{i,j} \log p_{i,j}.  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Each of the summands here are plotted in Fig. 3 for the special case of binary classification (two labels only). Each is unfavorably maximized at the most mixed state, where &lt;span class="math"&gt;\(p_1 = 0.5\)&lt;/span&gt;, and minimized in the pure states, where &lt;span class="math"&gt;\(p_1 = 0,1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/03/tree_errors.jpg"&gt;&lt;img alt="tree_errors" src="https://efavdb.com/wp-content/uploads/2015/03/tree_errors.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Although &lt;span class="math"&gt;\(E\)&lt;/span&gt; is perhaps the most intuitive of the three measures above (it&amp;#8217;s simply the number of training examples misclassified by the tree &amp;#8212; this follows from the fact that the tree returns as hypothesis the mode in each region) the latter two have the benefit of being characterized by negative curvature as a function of the &lt;span class="math"&gt;\(p_{i,j}\)&lt;/span&gt;. This property tends to enhance the favorability of splits that generate region pairs where at least one is highly pure. At times, this can simultaneously result in the other region of the pair ending up relatively impure &amp;#8212; see Exercise 1 for details. Such moves are often ultimately beneficial, since any highly impure node that results can always be broken up in later splits anyways. The plot in Fig. 3 shows that the cross-entropy has the larger curvature of the two, and so should more highly favor such splits, at least in the binary classification case. Another nice feature of the Gini and cross-entropy functions is that &amp;#8212; in contrast to the error rate &amp;#8212; they are both smooth functions of the &lt;span class="math"&gt;\(p_{i,j}\)&lt;/span&gt;, which facilitates numerical optimization. For these reasons, one of these two functions is typically used to guide tree construction, even if &lt;span class="math"&gt;\(E\)&lt;/span&gt; is the quantity one would actually like to minimize. Tree construction proceeds as in the regression case, typically by a greedy or randomized construction, each step taken so as to minimize (\ref{gini}) or (\ref{crossentropy}), whichever is&amp;nbsp;chosen.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Bias-variance trade-off and stopping&amp;nbsp;conditions&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Decision trees that are allowed to split indefinitely will have low bias but will over-fit their training data. Placing different stopping criteria on a tree&amp;#8217;s growth can ameliorate this latter effect. Two typical conditions often used for this purpose are given by a) placing an upper bound on the number of levels permitted in the tree, or b) requiring that each region (tree leaf) retains at least some minimum number of training examples. To optimize over such constraints, one can apply&amp;nbsp;cross-validation.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Bagging, random forests, and&amp;nbsp;boosting&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Another approach to alleviating the high-variance, over-fitting issue associated with decision trees is to average over many of them. This approach is motivated by the observation that the sum of &lt;span class="math"&gt;\(N\)&lt;/span&gt; independent random variables &amp;#8212; each with variance &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt; &amp;#8212; has a relatively reduced variance, &lt;span class="math"&gt;\(\sigma^2/N\)&lt;/span&gt;. Two common methods for carrying out summations of this sort are discussed&amp;nbsp;below.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Bagging and random&amp;nbsp;forests&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Bootstrap aggregation&lt;/em&gt;, or &amp;#8220;bagging&amp;#8221;, provides one common method for constructing ensemble tree models. In this approach, one samples with replacement to obtain &lt;span class="math"&gt;\(k\)&lt;/span&gt; separate bootstrapped training sets from the original training data. To obtain a bootstrapped subsample of a data set of size &lt;span class="math"&gt;\(N\)&lt;/span&gt;, one draws randomly from the set &lt;span class="math"&gt;\(N\)&lt;/span&gt; times with replacement. Because one samples with replacement, each bootstrapped set can contain multiple copies of some examples. The average number of unique examples in a given bootstrap is simply &lt;span class="math"&gt;\(N\)&lt;/span&gt; times the probability that any individual example makes it into the training set. This is&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{5} 
N \left [ 1 - \left(\frac{N-1}{N} \right)^N \right ] \approx N (1 - e^{-1}) \approx 0.63N,  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
where the latter forms are accurate in the large &lt;span class="math"&gt;\(N\)&lt;/span&gt; limit. Once the bootstrapped data sets are constructed, an individual decision tree is fit to each, and an average or majority rule vote over the full set is used to provide the final&amp;nbsp;prediction.&lt;/p&gt;
&lt;p&gt;One nice thing about bagging methods, in general, is that one can train on the entire set of available labeled training data and still obtain an estimate of the generalization error. Such estimates are obtained by considering the error on each point in the training set, in each case averaging only over those trees that did not train on the point in question. The resulting estimate, called the out-of-bag error, typically provides a slight overestimate to the generalization error. This is because accuracy generally improves with growing ensemble size, and the full ensemble is usually about three times larger than the sub-ensemble used to vote on any particular training example in the out-of-bag error&amp;nbsp;analysis.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Random forests&lt;/em&gt; provide a popular variation on the bagging method. The individual decision trees making up a random forest are, again, each fit to an independent, bootstrapped subsample of the training data. However, at each step in their recursive construction process, these trees are restricted in that they are only allowed to split on &lt;span class="math"&gt;\(r\)&lt;/span&gt; randomly selected candidate feature directions; a new set of &lt;span class="math"&gt;\(r\)&lt;/span&gt; directions is chosen at random for each step in the tree construction. These restrictions serve to effect a greater degree of independence in the set of trees averaged over in a random forest, which in turn serves to reduce the ensemble&amp;#8217;s variance &amp;#8212; see Exercise 5 for related analysis. In general, the value of &lt;span class="math"&gt;\(r\)&lt;/span&gt; should be optimized through&amp;nbsp;cross-validation.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Boosting&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;The final method we&amp;#8217;ll discuss is &lt;em&gt;boosting&lt;/em&gt;, which again consists of a set of individual trees that collectively determine the ultimate prediction returned by the model. However, in the boosting scenario, one fits each of the trees to the full data set, rather than to a small sample. Because they are fit to the full data set, these trees are usually restricted to being only two or three levels deep, so as to avoid over-fitting. Further, the individual trees in a boosted forest are constructed sequentially. For instance, in regression, the process typically works as follows: In the first step, a tree is fit to the full, original training set &lt;span class="math"&gt;\(T = \{t_i = (\textbf{f}_i, y_i)\}\)&lt;/span&gt;. Next, a second tree is constructed on the same training feature vectors, but with the original labels replaced by residuals. These residuals are obtained by subtracting out a scaled version of the predictions &lt;span class="math"&gt;\(\hat{y}^1\)&lt;/span&gt; returned by the first tree,&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{6} 
y_i^{(1)} \equiv y_i - \alpha \hat{y}_i^1.  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Here, &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; is the scaling factor, or learning rate &amp;#8212; choosing its value small results in a gradual learning process, which often leads to very good predictions. Once the second tree is constructed, a third tree is fit to the new residuals, obtained by subtracting out the scaled hypothesis of the second tree, &lt;span class="math"&gt;\(y_i^{(2)} \equiv y_i^{(1)} - \alpha \hat{y}_i^2\)&lt;/span&gt;. The process repeats until &lt;span class="math"&gt;\(m\)&lt;/span&gt; trees are constructed, with their &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;-scaled hypotheses summing to a good estimate to the underlying&amp;nbsp;function.&lt;/p&gt;
&lt;p&gt;Boosted classification tree ensembles are constructed in a fashion similar to that above. However, in contrast to the regression scenario, the same, original training labels are used to fit each new tree in the ensemble (as opposed to an evolving residual). To bring about a similar, gradual learning process, boosted classification ensembles instead sample from the training set with weights that are sample-dependent and that change over time: When constructing a new tree for the ensemble, one more heavily weights those examples that have been poorly fit in prior iterations. AdaBoost is a popular algorithm for carrying out boosted classification. This and other generalizations are covered in the text &lt;a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/"&gt;Elements of Statistical Learning&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Implementation runtime&amp;nbsp;complexity&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Before concluding, we take here a moment to consider the runtime complexity of tree construction. This exercise gives one a sense of how tree algorithms are constructed in practice. We begin by considering the greedy construction of a single classification tree. The extension to regression trees is&amp;nbsp;straightforward.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Individual decision&amp;nbsp;trees&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Consider the problem of greedily training a single classification tree on a set of &lt;span class="math"&gt;\(n\)&lt;/span&gt; training examples having &lt;span class="math"&gt;\(k\)&lt;/span&gt; features. In order to construct our tree, we take as a first step the sorting of the &lt;span class="math"&gt;\(n\)&lt;/span&gt; training vectors along each of the &lt;span class="math"&gt;\(k\)&lt;/span&gt; directions, which will facilitate later optimal split searches. Recall that optimized algorithms, e.g. &lt;a href="http://en.wikipedia.org/wiki/Merge_sort"&gt;merge-sort&lt;/a&gt;, require &lt;span class="math"&gt;\(O(n \log n)\)&lt;/span&gt; time to sort along any one feature direction, so sorting along all &lt;span class="math"&gt;\(k\)&lt;/span&gt; will require &lt;span class="math"&gt;\(O(k \times n \log n)\)&lt;/span&gt; time. After this pre-sort step is complete, we must seek the currently optimal split, carry it out, and then iterate. We will show that &amp;#8212; with care &amp;#8212; the full iterative process can also be carried out in &lt;span class="math"&gt;\(O(k \times n \log n)\)&lt;/span&gt;&amp;nbsp;time.&lt;/p&gt;
&lt;p&gt;Focus on an intermediate moment in the construction process where one particular node has just been split, resulting in two new regions, &lt;span class="math"&gt;\(R_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(R_2\)&lt;/span&gt; containing &lt;span class="math"&gt;\(n_{R_1}\)&lt;/span&gt; and &lt;span class="math"&gt;\(n_{R_2}\)&lt;/span&gt; training examples, respectively. We can assume that we have already calculated and stored the optimal split for every other region in the tree during prior iterations. Therefore, to determine which region contains the next optimal split, the only new searches we need to carry out are within regions &lt;span class="math"&gt;\(R_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(R_2\)&lt;/span&gt;. Focus on &lt;span class="math"&gt;\(R_1\)&lt;/span&gt; and suppose that we have been passed down the following information characterizing it: the number of training examples of each class that it contains, its total number of training examples &lt;span class="math"&gt;\(n_{R_1}\)&lt;/span&gt;, its cost function value &lt;span class="math"&gt;\(J\)&lt;/span&gt; (cross entropy, say), and for each of the &lt;span class="math"&gt;\(k\)&lt;/span&gt; feature directions, a separate list of the region&amp;#8217;s examples, sorted along that direction. To find the optimal split, we must consider all &lt;span class="math"&gt;\(k \times (n_{R_1}-1)\)&lt;/span&gt; possible cuts of this region [&lt;em&gt;Aside&lt;/em&gt;: We must check all possible cuts because the cost function can have many local minima. The precludes the use of gradient-descent-like algorithms to find the optimal split.], evaluating the cost function reduction for&amp;nbsp;each.&lt;/p&gt;
&lt;p&gt;The left side of Fig. 4 illustrates one method for efficiently carrying out these test cuts: For each feature direction, we proceed sequentially through that direction&amp;#8217;s ordered list, considering one cut at a time. In the first cut, we take only one example in the left sub-region induced, and all others on the right. In the second cut, we have the first two examples in the left sub-region, etc. Proceeding in this way, it turns out that the cost function of each new candidate split considered can always be evaluated in &lt;span class="math"&gt;\(O(1)\)&lt;/span&gt; time. This is because we start with knowledge of the cost function &lt;span class="math"&gt;\(J\)&lt;/span&gt; before any cut is taken, and the cost functions we consider here can each be updated in &lt;span class="math"&gt;\(O(1)\)&lt;/span&gt; time whenever only a single example is either added to or removed from a given region &amp;#8212; see exercises 3 and 4 for details. Using this approach, we can therefore try all possible cuts of region &lt;span class="math"&gt;\(R_1\)&lt;/span&gt; in &lt;span class="math"&gt;\(O(k \times n_{R_1})\)&lt;/span&gt;&amp;nbsp;time.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/03/tree_complexity.jpg"&gt;&lt;img alt="tree_complexity" src="https://efavdb.com/wp-content/uploads/2015/03/tree_complexity.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The above analysis gives the time needed to search for the optimal split within &lt;span class="math"&gt;\(R_1\)&lt;/span&gt;, and a similar form holds for &lt;span class="math"&gt;\(R_2\)&lt;/span&gt;. Once these are determined, we can quickly select the current, globally-optimal split [&lt;em&gt;Aside&lt;/em&gt;: Using a heap data structure, the global minimum can be obtained in at most &lt;span class="math"&gt;\(O(\log n)\)&lt;/span&gt; time. Summing this effort over all nodes of the tree will lead to roughly &lt;span class="math"&gt;\(O(n \log n)\)&lt;/span&gt; evaluations.]. Carrying out this split entails partitioning the region selected into two and passing the necessary information down to each. We leave as an exercise the fact that the passing of needed information &amp;#8212; ordered lists, etc. &amp;#8212; can be carried out in &lt;span class="math"&gt;\(O(k \times n_s)\)&lt;/span&gt; time, with &lt;span class="math"&gt;\(n_s\)&lt;/span&gt; the size of the parent region being split. The total tree construction time can now be obtained by summing up each node&amp;#8217;s search and split work, which both require &lt;span class="math"&gt;\(O(k \times n_s\)&lt;/span&gt;) computations. Assuming a roughly balanced tree having about &lt;span class="math"&gt;\(\log n\)&lt;/span&gt; layers &amp;#8212; see right side of Fig. 4 &amp;#8212; we obtain &lt;span class="math"&gt;\(O(k \times n \log n)\)&lt;/span&gt;, the runtime scaling&amp;nbsp;advertised.&lt;/p&gt;
&lt;p&gt;In summary, we see that achieving &lt;span class="math"&gt;\(O(k \times n \log n)\)&lt;/span&gt; scaling requires a) a pre-sort, b) a data structure for storing certain important facts about each region, including its optimal split, once determined, and also pointers to its parent and children, c) an efficient method for passing relevant information down to daughter regions during a split instance, d) a heap to enable quick selection of the currently optimal split, and e) a cost function that can be updated efficiently under single training example insertions or&amp;nbsp;removals.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Forests,&amp;nbsp;parallelization&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;If a forest of &lt;span class="math"&gt;\(N\)&lt;/span&gt; trees is to be constructed, each will require &lt;span class="math"&gt;\(O(k \times n \log n)\)&lt;/span&gt; time to construct. Recall, however, that the trees of a bagged forest can be constructed independently of one another. This allows for bagged forest constructions to take advantage of parallelization, facilitating their application in the large &lt;span class="math"&gt;\(N\)&lt;/span&gt; limit. In contrast, the trees of a boosted forest are constructed in sequence and so cannot be parallelized in a similar manner. However, note that optimal split searches along different feature directions can always be run in parallel. This can speed up individual tree construction times in either&amp;nbsp;case.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Discussion&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;In this note, we&amp;#8217;ve quickly reviewed the basics of tree-based models and their constructions. Looking back over what we have learned, we can now consider some of the reasons why tree-based methods are so popular among practitioners. First &amp;#8212; and very importantly &amp;#8212; individual trees are often useful for gaining insight into the geometry of datasets in high dimensions. This is because tree structures can be visualized using simple diagrams, like that in Fig. 1. In contrast, most other machine learning algorithm outputs cannot be easily visualized &amp;#8212; consider, e.g., support-vector machines, which return hyper-plane decision boundaries. A related point is that tree-based approaches are able to automatically fit non-linear decision boundaries. In contrast, linear algorithms can only fit such boundaries if appropriate non-linear feature combinations are constructed. This requires that one first identify these appropriate feature combinations, which can be a challenging task for feature spaces that cannot be directly visualized. Three additional positive qualities of decision trees are given by a) the fact that they are insensitive to feature scale, which reduces the need for related data preprocessing, b) the fact that they can make use of data missing certain feature values, and c) that they are relatively robust against outliers and noisy-labeling&amp;nbsp;issues.&lt;/p&gt;
&lt;p&gt;Although boosted and random forests are not as easily visualized as individual decision trees, these ensemble methods are popular because they are often quite competitive. Boosted forests typically have a slightly lower generalization error than their random forest counterparts. For this reason, they are often used when accuracy is highly-valued &amp;#8212; see last figure for an example learning curve consistent with this rule of thumb: Generalization error rate versus training set size for a hand-written digits learning problem. However, the individual trees in a bagged forest can be constructed in parallel. This benefit &amp;#8212; not shared by boosted forests &amp;#8212; can favor random forests as a go-to, out-of-box approach for treating large-scale machine learning&amp;nbsp;problems.&lt;/p&gt;
&lt;p&gt;Exercises follow that detail some further points of interest relating to decision trees and their&amp;nbsp;construction.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/03/tree_learning.jpg"&gt;&lt;img alt="tree_learning" src="https://efavdb.com/wp-content/uploads/2015/03/tree_learning.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;[1] &lt;a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/"&gt;Elements of Statistical Learning&lt;/a&gt;, by Hastie, Tibshirani, Friedman&lt;br&gt;
[2] &lt;a href="http://www-bcf.usc.edu/~gareth/ISL/"&gt;An Introduction to Statistical Learning&lt;/a&gt;, by James, Witten, Hastie, and Tibshirani&lt;br&gt;
[3] &lt;a href="http://link.springer.com/article/10.1023%2FA%3A1010933404324"&gt;Random Forests&lt;/a&gt;, by Breiman (Machine Learning, 45, 2001).&lt;br&gt;
[4] &lt;a href="http://scikit-learn.org/stable/modules/tree.html"&gt;Sk-learn documentation&lt;/a&gt; on runtime complexity, see section&amp;nbsp;1.8.4.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Exercises&lt;/strong&gt;&lt;/h2&gt;
&lt;h4&gt;1) Jensen&amp;#8217;s inequality and classification tree cost&amp;nbsp;functions&lt;/h4&gt;
&lt;p&gt;​a) Consider a real function &lt;span class="math"&gt;\(y(x)\)&lt;/span&gt; with non-positive curvature. Consider sampling &lt;span class="math"&gt;\(y\)&lt;/span&gt; at values &lt;span class="math"&gt;\(\{x_1, x_2, \ldots, x_m\}\)&lt;/span&gt;. By considering graphically the centroid of the points &lt;span class="math"&gt;\(\{(x_i, y(x_i))\}\)&lt;/span&gt;, prove Jensen&amp;#8217;s inequality,&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{7}
y\left ( \frac{1}{m} \sum_i x_i \right) \geq \frac{1}{m}\sum_i y(x_i).  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
When does equality&amp;nbsp;hold?&lt;/p&gt;
&lt;p&gt;​b) Consider binary tree classification guided by the minimization of the error rate (\ref{errorrate}). If all possible cuts of a particular region always leave class &lt;span class="math"&gt;\(0\)&lt;/span&gt; in the minority in both resulting sub-regions, will a cut here ever be&amp;nbsp;made?&lt;/p&gt;
&lt;p&gt;​c) How about if (\ref{gini}) or (\ref{crossentropy}) is used as the cost&amp;nbsp;function?&lt;/p&gt;
&lt;h4&gt;2) Decision tree prediction runtime&amp;nbsp;complexity&lt;/h4&gt;
&lt;p&gt;Suppose one has constructed an approximately balanced decision tree, where each node contains one of the &lt;span class="math"&gt;\(n\)&lt;/span&gt; training examples used for its construction. In general, approximately how long will it take to determine the region &lt;span class="math"&gt;\(R_i\)&lt;/span&gt; to which a supplied feature vector belongs? How about for ensemble models? Any difference between typical bagged and boosted&amp;nbsp;forests?&lt;/p&gt;
&lt;h4&gt;3) Classification tree construction runtime&amp;nbsp;complexity&lt;/h4&gt;
&lt;p&gt;​a) Consider a region &lt;span class="math"&gt;\(R\)&lt;/span&gt; within a classification tree containing &lt;span class="math"&gt;\(n_i\)&lt;/span&gt; training examples of class &lt;span class="math"&gt;\(i\)&lt;/span&gt;, with &lt;span class="math"&gt;\(\sum_i n_i = N\)&lt;/span&gt;. Now, suppose a cut is considered in which a single training example of class &lt;span class="math"&gt;\(1\)&lt;/span&gt; is removed from the region. If the region&amp;#8217;s cross-entropy before the cut is given by &lt;span class="math"&gt;\(CE_0\)&lt;/span&gt;, show that its entropy after the cut will be given by&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{DEntropy} \tag{8}
CE_f = CE_0 - N \log\left (\frac{N}{N-1} \right) + \log \left (\frac{n_1}{N-1} \right) - (n_1 -1) \log \left (\frac{n_1 - 1}{n_1} \right).  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
If &lt;span class="math"&gt;\(CE_0\)&lt;/span&gt;, &lt;span class="math"&gt;\(N\)&lt;/span&gt;, and the &lt;span class="math"&gt;\(\{n_i\}\)&lt;/span&gt; values are each stored in memory for a given region, this equation can be used to evaluate in &lt;span class="math"&gt;\(O(1)\)&lt;/span&gt; time the change in its entropy with any single example removal. Similarly, the change in entropy of a region upon addition of a single training example can also be evaluated in &lt;span class="math"&gt;\(O(1)\)&lt;/span&gt; time. Taking advantage of this is essential for obtaining an efficient tree construction&amp;nbsp;algorithm.&lt;/p&gt;
&lt;p&gt;​b) Show that a region&amp;#8217;s Gini coefficient (\ref{gini}) can also be updated in &lt;span class="math"&gt;\(O(1)\)&lt;/span&gt; time with any single training example&amp;nbsp;removal.&lt;/p&gt;
&lt;h4&gt;4)Regression tree construction runtime&amp;nbsp;complexity.&lt;/h4&gt;
&lt;p&gt;Consider a region &lt;span class="math"&gt;\(R\)&lt;/span&gt; within a regression tree containing &lt;span class="math"&gt;\(N\)&lt;/span&gt; training examples, characterized by mean label value &lt;span class="math"&gt;\(\overline{y}\)&lt;/span&gt; and cost value (\ref{treecost}) given by &lt;span class="math"&gt;\(J\)&lt;/span&gt; (&lt;span class="math"&gt;\( N\)&lt;/span&gt; times the region&amp;#8217;s label variance). Suppose a cut is considered in which a single training example having label &lt;span class="math"&gt;\(y\)&lt;/span&gt; is removed from the region. Show that after the cut is taken the new mean training label and cost function values within the region are given by&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{9}
\overline{y}_f &amp;amp;=&amp;amp; \frac{1}{N-1} \left ( N \overline{y} - y \right) \ \label{regression_cost_change}  
J_f &amp;amp;=&amp;amp; J - \frac{N}{N-1} \left ( \overline{y} - y\right)^2.  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
These results allow for the cost function of a region to be updated in &lt;span class="math"&gt;\(O(1)\)&lt;/span&gt; time as single examples are either inserted or removed from it. Their simplicity is a special virtue of the squared error cost function. Other cost function choices will generally require significant increases in tree construction runtime complexity, as most require a fresh evaluation with each new subset of examples&amp;nbsp;considered.&lt;/p&gt;
&lt;h4&gt;5) Chebychev&amp;#8217;s inequality and random forest classifier&amp;nbsp;accuracy&lt;/h4&gt;
&lt;p&gt;Adapted from&amp;nbsp;[3].&lt;/p&gt;
&lt;p&gt;​a) Let &lt;span class="math"&gt;\(x\)&lt;/span&gt; be a random variable with well-defined mean &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; and variance &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt;. Prove Chebychev&amp;#8217;s inequality,&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{Cheby} \tag{10}
P(x \geq \mu + t) \leq \frac{\sigma^2}{t^2}.  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;​b) Consider a binary classification problem aimed at fitting a sampled function &lt;span class="math"&gt;\(y(\textbf{f})\)&lt;/span&gt; that takes values in &lt;span class="math"&gt;\(\{ 0,1\}\)&lt;/span&gt;. Suppose a decision tree &lt;span class="math"&gt;\(h_{\theta}(\textbf{f})\)&lt;/span&gt; is constructed on the samples using a greedy, randomized approach, where the randomization is characterized by the parameter &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;. Define the classifier&amp;#8217;s &lt;em&gt;margin&lt;/em&gt; &lt;span class="math"&gt;\(m\)&lt;/span&gt; at &lt;span class="math"&gt;\(\textbf{f}\)&lt;/span&gt; by&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{tree_margin_def} \tag{11}
m(\theta, \textbf{f}) =-1 + 2 \left [ y * h_{\theta}+ (1- y) * (1 - h_{\theta}) \right ]  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
This is equal to &lt;span class="math"&gt;\(1\)&lt;/span&gt; if &lt;span class="math"&gt;\(h_{\theta}\)&lt;/span&gt; and &lt;span class="math"&gt;\(y\)&lt;/span&gt; agree at &lt;span class="math"&gt;\(\textbf{f}\)&lt;/span&gt;, and &lt;span class="math"&gt;\(-1\)&lt;/span&gt; otherwise. Now, consider a random forest, consisting of many such trees, each obtained by sampling from the same &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; distribution. Argue using (\ref{Cheby}), (\ref{tree_margin_def}), and the law of large numbers that the generalization error &lt;span class="math"&gt;\(GE\)&lt;/span&gt; of the forest is bounded by&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{rf_bound} \tag{12}
GE \leq \frac{var_{\textbf{f}}\left( \langle m(\theta, \textbf{f}) \rangle_{\theta} \right)}{\langle m(\theta, \textbf{f}) \rangle_{\theta, \textbf{f}}^2 }  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;​c) Show that&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{margin_var} \tag{13} 
var_{\textbf{f}}\left( \langle m(\theta, \textbf{f}) \rangle_{\theta} \right) = \langle cov_{\textbf{f}}(m(\theta, \textbf{f}), m(\theta^{\prime},\textbf{f})) \rangle_{\theta, \theta^{\prime}}  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;​d) Writing,&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{14} 
\rho \equiv \frac{\langle cov_{\textbf{f}}(m(\theta, \textbf{f}), m(\theta^{\prime},\textbf{f})) \rangle_{\theta, \theta^{\prime}}}  
{\langle \sqrt{var_{\textbf{f}}(m(\theta, \textbf{f}))} \rangle_{\theta}^2},  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
for the &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, &lt;span class="math"&gt;\(\theta^{\prime}\)&lt;/span&gt;-averaged margin-margin correlation coefficient, show that&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}  
var_{\textbf{f}}\left( \langle m(\theta, \textbf{f}) \rangle_{\theta} \right) \leq \rho \langle var_{\textbf{f}}(m(\theta, \textbf{f})) \rangle_{\theta} \leq \rho \left ( 1 - \langle m(\theta, \textbf{f}) \rangle_{\theta, \textbf{f}}^2\right).  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Combining with (\ref{rf_bound}), this gives&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{tree_bound_final} \tag{15} 
GE \leq \rho \times \frac{ 1 - \langle m(\theta, \textbf{f}) \rangle_{\theta, \textbf{f}}^2 }{ \langle m(\theta, \textbf{f}) \rangle_{\theta, \textbf{f}}^2 }.  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
The bound (\ref{tree_bound_final}) implies that a random forest&amp;#8217;s generalization error is reduced if the individual trees making up the forest have a large average margin, and also if the trees are relatively-uncorrelated with each&amp;nbsp;other.&lt;/p&gt;
&lt;p&gt;Cover image by &lt;a href="https://www.flickr.com/photos/roberts87/2798303714"&gt;roberts87&lt;/a&gt;, &lt;a href="https://creativecommons.org/licenses/by-nc-sa/2.0/legalcode"&gt;creative commons license&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Appendix: python/sk-learn&amp;nbsp;implementations&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Here, we provide the python/sk-learn code used to construct the final figure in the body of this note: Learning curves on sk-learn&amp;#8217;s &amp;#8220;digits&amp;#8221; dataset for a single tree, a random forest, and a boosted&amp;nbsp;forest.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.datasets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;load_digits&lt;/span&gt;  
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.tree&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;DecisionTreeClassifier&lt;/span&gt;  
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.ensemble&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;RandomForestClassifier&lt;/span&gt;  
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.ensemble&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;GradientBoostingClassifier&lt;/span&gt;  
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="c1"&gt;# load data: digits.data and digits.target,  &lt;/span&gt;
&lt;span class="c1"&gt;# array of features and labels, resp.  &lt;/span&gt;
&lt;span class="n"&gt;digits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;load_digits&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_class&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;n_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;  
&lt;span class="n"&gt;t1_accuracy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;  
&lt;span class="n"&gt;t2_accuracy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;  
&lt;span class="n"&gt;t3_accuracy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;

&lt;span class="c1"&gt;# below, we average over &amp;quot;trials&amp;quot; num of fits for each sample  &lt;/span&gt;
&lt;span class="c1"&gt;# size in order to estimate the average generalization error.  &lt;/span&gt;
&lt;span class="n"&gt;trials&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;25&lt;/span&gt;

&lt;span class="n"&gt;clf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DecisionTreeClassifier&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  
&lt;span class="n"&gt;clf2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GradientBoostingClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_depth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;clf3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;RandomForestClassifier&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;num_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;500&lt;/span&gt;

&lt;span class="c1"&gt;# loop over different training set sizes  &lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;num_train&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;num_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

&lt;span class="n"&gt;acc1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;acc2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;acc3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;trials&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;  
    &lt;span class="n"&gt;perm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;perm&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;num_train&lt;/span&gt;&lt;span class="p"&gt;]]))&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  
        &lt;span class="n"&gt;perm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;permutation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="n"&gt;clf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;perm&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;num_train&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;
        &lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;perm&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;num_train&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;  
    &lt;span class="n"&gt;acc1&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;perm&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;num_test&lt;/span&gt;&lt;span class="p"&gt;:]],&lt;/span&gt;
        &lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;perm&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;num_test&lt;/span&gt;&lt;span class="p"&gt;:]])&lt;/span&gt;

    &lt;span class="n"&gt;clf2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;clf2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;perm&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;num_train&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;
        &lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;perm&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;num_train&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;  
    &lt;span class="n"&gt;acc2&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;clf2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;perm&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;num_test&lt;/span&gt;&lt;span class="p"&gt;:]],&lt;/span&gt;
        &lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;perm&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;num_test&lt;/span&gt;&lt;span class="p"&gt;:]])&lt;/span&gt;

    &lt;span class="n"&gt;clf3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;clf3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;perm&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;num_train&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;
        &lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;perm&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;num_train&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;  
    &lt;span class="n"&gt;acc3&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;clf3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;perm&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;num_test&lt;/span&gt;&lt;span class="p"&gt;:]],&lt;/span&gt;
        &lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;perm&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;num_test&lt;/span&gt;&lt;span class="p"&gt;:]])&lt;/span&gt;

    &lt;span class="n"&gt;n_train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
    &lt;span class="n"&gt;t1_accuracy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;acc1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;trials&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
    &lt;span class="n"&gt;t2_accuracy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;acc2&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;trials&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
    &lt;span class="n"&gt;t3_accuracy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;acc3&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;trials&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;pylab&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;t1_accuracy&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;red&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;t2_accuracy&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;green&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;t3_accuracy&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;blue&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;/pre&gt;&lt;/div&gt;


&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Methods, Theory"></category></entry><entry><title>NBA week 17 summary, week 18 predictions</title><link href="https://efavdb.com/nba-week-17-summary-week-18-predictions" rel="alternate"></link><published>2015-03-13T09:38:00-07:00</published><updated>2015-03-13T09:38:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2015-03-13:/nba-week-17-summary-week-18-predictions</id><content type="html">&lt;p&gt;We went 35 for 56 this past week, 62.5% accuracy. Just one fewer correct game than last week. New predictions &lt;a href="http://efavdb.github.io/weekly-nba-predictions"&gt;are up&lt;/a&gt;, and point-spread details of the past week are given&amp;nbsp;below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Point spread&lt;/th&gt;
&lt;th&gt;# games&lt;/th&gt;
&lt;th&gt;Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&amp;lt; 6&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;56%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5-10&lt;/td&gt;
&lt;td&gt;30&lt;/td&gt;
&lt;td&gt;63%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;11-15&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;50%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;gt;15&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;78%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content><category term="NBA prediction project"></category></entry><entry><title>NBA week 16 summary, week 17 predictions</title><link href="https://efavdb.com/nba-week-16-summary-week-17-predictions" rel="alternate"></link><published>2015-03-06T11:23:00-08:00</published><updated>2015-03-06T11:23:00-08:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2015-03-06:/nba-week-16-summary-week-17-predictions</id><summary type="html">&lt;p&gt;This week, we went 36 for 56, giving 64% accuracy &amp;#8212; decent. New predictions &lt;a href="http://efavdb.com/weekly-nba-predictions/"&gt;are up&lt;/a&gt; &amp;#8212; again, lots of predicted upsets this week! One such upset predicted is a Clippers victory over the home team Warriors on Sunday&amp;#8230; a game that one of us will be going to. Although we root …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This week, we went 36 for 56, giving 64% accuracy &amp;#8212; decent. New predictions &lt;a href="http://efavdb.com/weekly-nba-predictions/"&gt;are up&lt;/a&gt; &amp;#8212; again, lots of predicted upsets this week! One such upset predicted is a Clippers victory over the home team Warriors on Sunday&amp;#8230; a game that one of us will be going to. Although we root for the local team, it&amp;#8217;s always hard to feel bad when our baby guesses correctly, so we&amp;#8217;ll be happy either way. Past week details&amp;nbsp;below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Point spread&lt;/th&gt;
&lt;th&gt;# games&lt;/th&gt;
&lt;th&gt;Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&amp;lt; 6&lt;/td&gt;
&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;75%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5-10&lt;/td&gt;
&lt;td&gt;17&lt;/td&gt;
&lt;td&gt;65%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;11-15&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;57%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;gt;15&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;50%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content><category term="NBA prediction project"></category></entry><entry><title>NBA week 15 summary, week 16 predictions</title><link href="https://efavdb.com/nba-week-15-summary-week-16-predictions" rel="alternate"></link><published>2015-02-27T11:39:00-08:00</published><updated>2015-02-27T11:39:00-08:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2015-02-27:/nba-week-15-summary-week-16-predictions</id><summary type="html">&lt;p&gt;Not a terribly impressive week for the &lt;span class="caps"&gt;NBA&lt;/span&gt; algorithm. We went 31/53, equating to 58% accuracy. New predictions &lt;a href="http://efavdb.github.io/weekly-nba-predictions"&gt;are up&lt;/a&gt; &amp;#8212; lots of predicted upsets this week, including two tonight. As usual, I&amp;#8217;ve found checking the &lt;a href="http://efavdb.github.io/nba-dash"&gt;dashboard&lt;/a&gt; to be a great way to gain insight into such picks. Details …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Not a terribly impressive week for the &lt;span class="caps"&gt;NBA&lt;/span&gt; algorithm. We went 31/53, equating to 58% accuracy. New predictions &lt;a href="http://efavdb.github.io/weekly-nba-predictions"&gt;are up&lt;/a&gt; &amp;#8212; lots of predicted upsets this week, including two tonight. As usual, I&amp;#8217;ve found checking the &lt;a href="http://efavdb.github.io/nba-dash"&gt;dashboard&lt;/a&gt; to be a great way to gain insight into such picks. Details of past week by point spread&amp;nbsp;below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Point spread&lt;/th&gt;
&lt;th&gt;# games&lt;/th&gt;
&lt;th&gt;Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&amp;lt; 5&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;40%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5-9&lt;/td&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;td&gt;62%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10-14&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;td&gt;64%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;gt;14&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;td&gt;63%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content><category term="NBA prediction project"></category></entry><entry><title>Measles vaccination rate by USA state and relation to mean outbreak size</title><link href="https://efavdb.com/vaccination-rates" rel="alternate"></link><published>2015-02-25T13:40:00-08:00</published><updated>2015-02-25T13:40:00-08:00</updated><author><name>Damien RJ</name></author><id>tag:efavdb.com,2015-02-25:/vaccination-rates</id><summary type="html">&lt;p&gt;In this post, we provide a quick overview of the data and science of measles spread. Making use of python (code provided) we extract from a &lt;span class="caps"&gt;CDC&lt;/span&gt; data set the 2012 youth vaccination rate for each &lt;span class="caps"&gt;USA&lt;/span&gt; state &amp;#8212; see figure below. To aid in the interpretation of this data, we …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In this post, we provide a quick overview of the data and science of measles spread. Making use of python (code provided) we extract from a &lt;span class="caps"&gt;CDC&lt;/span&gt; data set the 2012 youth vaccination rate for each &lt;span class="caps"&gt;USA&lt;/span&gt; state &amp;#8212; see figure below. To aid in the interpretation of this data, we also review and describe the results of a generalized &amp;#8220;&lt;span class="caps"&gt;SIR&lt;/span&gt;&amp;#8221; model for disease spread. The model &amp;#8212; analyzed in &lt;a href="http://http://efavdb.github.io/math-of-measles"&gt;our last post&lt;/a&gt; &amp;#8212; predicts that measles outbreaks are supported only if the vaccination rate is below 94%. At higher rates, infection spread is suppressed, and outbreaks do not occur. As seen in the figure, the majority of the states sit just below this critical number, and so are predicted to support youth&amp;nbsp;outbreaks.&lt;/p&gt;
&lt;iframe width="600" height="533" frameborder="0" src="//plot.ly/~damienrj/179.embed"&gt;&lt;/iframe&gt;

&lt;p&gt;Measles vaccination rate by &lt;span class="caps"&gt;USA&lt;/span&gt; state for youths under 36 months in age, 2012. Dashed line at 94% is estimated critical vaccination rate needed to suppress outbreaks. The mean &lt;span class="caps"&gt;USA&lt;/span&gt; youth rate is about&amp;nbsp;91%.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Introduction: History of measles in&amp;nbsp;America&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Measles is a highly-contagious, serious illness estimated to currently be contracted by about 20 million individuals each year, globally. Just prior to the arrival of the first measles vaccine in 1963 (developed by &lt;a href="http://en.wikipedia.org/wiki/John_Franklin_Enders"&gt;John Enders&lt;/a&gt; and his colleagues) approximately 500,000 Americans contracted the disease annually, approximately 50,000 of which required hospitalization &amp;#8212; a rate of 1/10. Among these individuals, approximately 500 would die each year, equating to a 1/1000 mortality rate. As yet, there is &lt;a href="http://www.mayoclinic.org/diseases-conditions/measles/basics/treatment/con-20019675"&gt;no treatment&lt;/a&gt; available to combat the measles virus, once contracted. Consequently, similar hospitalization and mortality rates &lt;a href="http://en.wikipedia.org/wiki/Measles#Epidemiology"&gt;continue to hold today&lt;/a&gt;. Sadly, the mortality rate among the malnourished can be as high as&amp;nbsp;1/10.&lt;/p&gt;
&lt;iframe width="600" height="533" frameborder="0" src="//plot.ly/~damienrj/262.embed"&gt;&lt;/iframe&gt;

&lt;p&gt;Here, the vaccination rate shown is for youths, under 48 months in first brach (up to 1985), and under 36 months in the latter&amp;nbsp;branch.&lt;/p&gt;
&lt;p&gt;Contemporary contraction rates in the &lt;span class="caps"&gt;USA&lt;/span&gt; are now extremely low, on the order of &lt;a href="http://www.cdc.gov/measles/cases-outbreaks.html"&gt;50-500 per year&lt;/a&gt;. This is likely a consequence of the strong local adoption of measles vaccinations, with 91% of young children here now receiving the vaccine prior to their third birthday. The most recent large-scale &lt;span class="caps"&gt;USA&lt;/span&gt; outbreak of the disease happened between the years of 1989 and 1991, when the vaccination rates of children were significantly lower, hovering around 70%. This outbreak centered within poorer, urban areas where the vaccination rates were &lt;a href="http://www.cdc.gov/vaccines/pubs/pinkbook/meas.html"&gt;substantially lower&lt;/a&gt; than the national average. A summary plot of &lt;span class="caps"&gt;USA&lt;/span&gt; contraction counts and youth vaccination rates by year is shown above &amp;#8212; the two curves are highly anti-correlated. Note that this plot is &lt;strong&gt;click and drag zoomable&lt;/strong&gt;, which is useful for setting the scale appropriately for recent years. Data &lt;a href="http://jid.oxfordjournals.org/content/189/Supplement_1/S17.long"&gt;source 1&lt;/a&gt;, &lt;a href="http://www.cdc.gov/mmwr/preview/mmwrhtml/mm6316a4.htm"&gt;source 2&lt;/a&gt;; no youth vaccination rate data available between 1985-1991. Full-population averages have been in the 90&amp;#8217;s &lt;a href="http://www.nature.com/news/measles-by-the-numbers-a-race-to-eradication-1.16897"&gt;for some decades&lt;/a&gt;, likely due to elementary school matriculation&amp;nbsp;requirements.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Modeling disease&amp;nbsp;spread&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;The striking &lt;span class="caps"&gt;USA&lt;/span&gt; historical data suggests a very strong relationship between the vaccination rate within a community and the frequency and size of the measles outbreaks that it supports. In fact, simple models for disease spread suggest a phase-transition-like (exhibiting abrupt changes) outbreak size dependence on vaccination rates. This is illustrated below, where we plot the predicted measles contraction rate against a population&amp;#8217;s vaccination rate &amp;#8212; as returned by a generalized &lt;a href="http://en.wikipedia.org/wiki/Epidemic_model#The_SIR_model"&gt;&lt;span class="caps"&gt;SIR&lt;/span&gt; model for disease spread&lt;/a&gt;: Notice that in the far left side of this plot, the model predicts that disease spread is completely suppressed. However, below a critical vaccination rate (the stated 94% mark &amp;#8212; a number consistent with &lt;a href="http://jid.oxfordjournals.org/content/196/10/1433.full"&gt;published estimates for measles&lt;/a&gt;), outbreaks begin to be supported, growing in size with further decrease of the vaccination&amp;nbsp;rate.&lt;/p&gt;
&lt;iframe width="600" height="533" frameborder="0" src="//plot.ly/~damienrj/275.embed"&gt;&lt;/iframe&gt;

&lt;p&gt;The generalized &lt;span class="caps"&gt;SIR&lt;/span&gt; model predicts that a measles outbreak size changes in a phase-transition-like manner with vaccination rate. Further, at about 85% vaccination, the outbreak population fraction and the unvaccinated fraction curves cross. At this point, the outbreak captures nearly all unvaccinated and also a finite fraction of the vaccinated, who begin to become infected due to frequent encounters with&amp;nbsp;disease.&lt;/p&gt;
&lt;p&gt;A detailed study of the &lt;span class="caps"&gt;SIR&lt;/span&gt; model&amp;#8217;s &lt;a href="http://efavdb.github.io/math-of-measles"&gt;solution&lt;/a&gt; is not necessary to understand why disease spread exhibits a phase-transition-like form. Qualitatively, this behavior is a consequence of a simple balance of rates: If the rate at which a disease spreads is greater than the rate at which the ill recover, outbreaks grow and expand. However, if patients recover more quickly than they can spread the disease, outbreak expansion is suppressed, and the number of infected individuals will decrease with time. The balance of these competing effects is tuned by the frequency of vaccination, which directly affects the first of these rates &amp;#8212; that at which the disease can spread. Because measles is &lt;a href="http://www.cdc.gov/measles/about/transmission.html"&gt;highly contagious&lt;/a&gt;, its balance point occurs around the relatively-high 94% mark seen in the figure &amp;#8212; this is the vaccination rate needed to have the average rate of disease spread just equal to the average rate of recovery. An info-graphic illustrating these points can be found &lt;a href="http://www.vaccines.gov/basics/protection/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Model implications for &lt;span class="caps"&gt;USA&lt;/span&gt;&amp;nbsp;youth&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;At the start of this post, we presented &lt;span class="caps"&gt;CDC&lt;/span&gt; estimates for the mean, by-state vaccination rates within the &lt;span class="caps"&gt;USA&lt;/span&gt;. Now that we have reviewed the results of the &lt;span class="caps"&gt;SIR&lt;/span&gt; model, we can begin to appreciate the significance of this data more deeply. First &amp;#8212; as we noted above &amp;#8212; we see that many states have youth vaccination rates that sit just below the critical 94% level, and thus on average should support small measles outbreaks. The graph of the previous section also allows us to estimate the mean size of a measles outbreak (once sparked) within any community below the critical vaccination rate: The further a state is from the 94% mark, the larger its mean outbreak size should be. Although all states are doing reasonably well, when considered on &lt;a href="http://www.npr.org/blogs/goatsandsoda/2015/02/06/384068229/measles-vaccination-rates-tanzania-does-better-than-u-s"&gt;a global scale&lt;/a&gt;, it is important to realize that many sit in a region of the plot where the response to a decrease in vaccination rate is most dramatic &amp;#8212; the curve&amp;#8217;s slope is largest just to the right of 94% vaccination. This means, e.g., that a 1% decrease in vaccination rate will result in a greater than 1% increase in the size of the average outbreak supported within that state. This observation is modestly worrisome, as the risk of contraction for all individuals &amp;#8212; even those vaccinated &amp;#8212; is always proportional to the number of cases present in any&amp;nbsp;outbreak.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Important&amp;nbsp;caveats:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Vaccination rates fluctuate between cities, neighborhoods, etc. This means that simple state averages may not accurately characterize your local community&amp;#8217;s vaccination rate &amp;#8212; the quantity most relevant to your personal infection risk. See, for instance, the plot for California by county shown &lt;a href="http://www.huffingtonpost.com/2015/02/03/measles-us-facts_n_6581922.html"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Our results are based on a descriptive, but simple model. They are intended only to provide one with a qualitative picture of the forces governing measles spread. Detailed, peer-reviewed treatments (this is just a blog&amp;#8230;) can be found in the&amp;nbsp;literature.&lt;/li&gt;
&lt;li&gt;Consult a licensed physician for qualified information on vaccines, measles&amp;nbsp;etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;strong&gt;Discussion&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Interestingly, &lt;a href="http://en.wikipedia.org/wiki/Measles#Cause"&gt;humans are the only known carriers&lt;/a&gt; of the measles virus. Consequently, with &lt;a href="http://www.nature.com/news/measles-by-the-numbers-a-race-to-eradication-1.16897"&gt;growing global vaccination rates&lt;/a&gt;, it may one day soon be possible to totally extinguish the disease. Looking back on the historical &lt;span class="caps"&gt;USA&lt;/span&gt; data helps one realize that this would be a truly remarkable accomplishment! In fact, were the &lt;span class="caps"&gt;USA&lt;/span&gt; in isolation, our current vaccination rates would likely be sufficient to bring this about. However, we are not, and sporadic outbreaks continue to occur here, ignited through international travel of infected individuals. These outbreaks can occur because our youth vaccination rates are below the critical 94% level needed to suppress&amp;nbsp;them.&lt;/p&gt;
&lt;p&gt;The science of disease spread is very interesting. For those with a mathematical background, we suggest taking a look at &lt;a href="http://http://efavdb.github.io/math-of-measles"&gt;our prior post&lt;/a&gt;, where we solve the &lt;span class="caps"&gt;SIR&lt;/span&gt; model analytically. Many additional insights into disease spread &amp;#8212; not covered here &amp;#8212; can be gleaned through the study of this model. Likewise, those with a programming background can play with the code that we provide below to sort through the &lt;span class="caps"&gt;CDC&lt;/span&gt;&amp;#8217;s data sets in different ways. For instance, one can easily alter this code to view how vaccination rates vary by socio-economic background, rather than by state, etc. One can also use the same procedures to sort through many other interesting data sets provided by the &lt;span class="caps"&gt;CDC&lt;/span&gt; and&amp;nbsp;others.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Methods: Wrangling the &lt;span class="caps"&gt;CDC&lt;/span&gt; measles data&amp;nbsp;sets&lt;/strong&gt;&lt;/h4&gt;
&lt;h5&gt;&lt;strong&gt;Grabbing and loading&amp;nbsp;data&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;In this section, we outline our numerical analysis of the &lt;span class="caps"&gt;CDC&lt;/span&gt; measles vaccination rate data set. To follow along, you must first download the files.  Current data can be found &lt;a href="http://www.cdc.gov/nchs/nis/data_files.htm"&gt;here&lt;/a&gt;, and data corresponding to years before 2009 &lt;a href="http://www.cdc.gov/nchs/nis/data_files_09_prior.htm"&gt;here&lt;/a&gt;.  Three files are needed.  The &lt;a href="ftp://ftp.cdc.gov/pub/Health_Statistics/NCHS/Datasets/nis/nispuf12.dat"&gt;Dataset file&lt;/a&gt;, the &lt;a href="ftp://ftp.cdc.gov/pub/Health_Statistics/NCHS/Dataset_Documentation/NIS/NISPUF12_CODEBOOK.PDF"&gt;Codebook&lt;/a&gt; that explains how to read the dataset files, and the &lt;a href="ftp://ftp.cdc.gov/pub/Health_Statistics/NCHS/Dataset_Documentation/NIS/NISPUF12_DUG.PDF"&gt;Data User’s Guide&lt;/a&gt; which provides details about the data, including methodology and statistics&amp;nbsp;descriptions.&lt;/p&gt;
&lt;p&gt;In our analysis, we will make use of a few python packages.  In particular, we will use &lt;a href="http://pandas.pydata.org/"&gt;Pandas&lt;/a&gt; to construct high performance data structures, called DataFrames. These are easy to use, and they allow for fast, straightforward data manipulation &amp;#8212; both helpful features for data wrangling. We will utilize the groupby DataFrame method, which enables one to easily segment data according to values along different feature directions. To illustrate, we will split our data by state name.  We will then use the &lt;a href="https://plot.ly/"&gt;Plot.ly&lt;/a&gt; package to generate the interactive plots included&amp;nbsp;above.&lt;/p&gt;
&lt;p&gt;To get a feel for the &lt;span class="caps"&gt;CDC&lt;/span&gt; data, a good first step is to look at the data in a text editor.  Doing this, we quickly notice multiple rows of characters having no vernacular significance.  The codebook allows one to interpret these characters.  It also explains that each row corresponds to a different child surveyed, and that each row has a fixed number of entries, many corresponding to different vaccines. We will read these rows in one at a time &amp;#8212; making use of a for loop &amp;#8212; to save on memory. As each line is processed, we keep only what we need &amp;#8212; here that info relevant to the &lt;span class="caps"&gt;MMR&lt;/span&gt;&amp;nbsp;vaccine.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;plotly.plotly&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;py&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;plotly.graph_objs&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;
&lt;span class="n"&gt;py&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sign_in&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;username&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;password&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;#Create some empty lists to store the data&lt;/span&gt;
&lt;span class="n"&gt;child_ID&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;house_ID&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;patient_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;measles&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;stratum&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt; &lt;span class="c1"&gt;#No virgin islands&lt;/span&gt;
&lt;span class="n"&gt;weights_v&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;mmr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;

&lt;span class="c1"&gt;#First we need to read in the data file, and parse the data&lt;/span&gt;
&lt;span class="c1"&gt;#using the datasheet&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;nispuf12.dat&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;child_ID&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;house_ID&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;patient_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;measles&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;260&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;261&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;181&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;183&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;stratum&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;88&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;92&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;69&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;weights_v&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;31&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;mmr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;261&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;262&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h5&gt;&lt;strong&gt;Cleaning&amp;nbsp;data&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;Now that everything is parsed and loaded into arrays, we will insert our data into a dictionary with key given by the column header.  We then convert this into a DataFrame. To streamline the process, we will also replace all the missing values with a &lt;span class="caps"&gt;NAN&lt;/span&gt;, and also convert all relevant entries from string to number format.  We will also replace the stateID numbers with their written names.  Lastly, we will remove all the incomplete patient&amp;nbsp;files.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;#Create a dataframe in pandas for data manipulation&lt;/span&gt;
&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;child&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;child_ID&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;house&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;house_ID&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;state&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;patient_data&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;patient_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;measles&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;measles&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;MMR&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;mmr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;stratum&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;stratum&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;weights&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;weights_v&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;weights_v&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

&lt;span class="c1"&gt;#Clean up the data to assign . to NAN&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;measles&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;measles&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;0&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;.&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;NAN&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;#Convert the data to numberic values&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;convert_objects&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;convert_numeric&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;#Replace the state ID code with the state&amp;#39;s name&lt;/span&gt;
&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loadtxt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;state_ID.txt&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;unpack&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;delimiter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;,&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;state&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;state&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int64&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;#Find all the values where there is complete provider data.&lt;/span&gt;
&lt;span class="c1"&gt;#We will look at only the complete patient records,&lt;/span&gt;
&lt;span class="c1"&gt;#and remove records column which is now only one value.&lt;/span&gt;
&lt;span class="n"&gt;ind&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;where&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;patient_data&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iloc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;ind&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;patient_data&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;inplace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Below, we provide some examples of our resulting, cleaned data points. The weight columns here are described further below. The first of these is used for analyses including the Virgin Islands, the other when they are&amp;nbsp;not.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;MMR child house measles state stratum weights weights_v&lt;/span&gt;
&lt;span class="err"&gt;0 1 11 1 1 Texas 1054 65.155698 120.163273&lt;/span&gt;
&lt;span class="err"&gt;1 1 21 2 1 Texas 2055 33.652064 52.360361&lt;/span&gt;
&lt;span class="err"&gt;3 1 41 4 1 Massachusetts 1002 216.529889 271.502218&lt;/span&gt;
&lt;span class="err"&gt;5 1 61 6 1 Georgia 1025 231.557156 562.130094&lt;/span&gt;
&lt;span class="err"&gt;6 1 71 7 1 South Carolina 1030 150.109737 238.018808&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h5&gt;&lt;strong&gt;Weighting to get averaged&amp;nbsp;statistics&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;We now have our data cleaned and ready to go. However, some additional work needs to be done before we can evaluate various statistics of interest. This is because the &lt;span class="caps"&gt;CDC&lt;/span&gt; data set is not a random sample, but instead a &lt;a href="http://en.wikipedia.org/wiki/Stratified_sampling"&gt;stratified sample&lt;/a&gt; &amp;#8212; i.e. one geared towards obtaining reasonable accuracy among many minority groups, and not simply among the averaged population. The weight factors are the key to extracting averaged statistics from this data, as explained in the user guide. For example, the average is essentially just a weighted average, and the standard error can be calculated using a Taylor-Series approach. The easiest way to apply this to our data is to make use of custom functions. Once constructed, these can then be easily applied to different DataFrame&amp;nbsp;groupings.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Lets define a function to caluclate the vaccination rate&lt;/span&gt;
&lt;span class="c1"&gt;# for the group we are looking at, and the standard error&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;calculate_rate_and_error&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;# The rate is calucated useing a weighted average&lt;/span&gt;
    &lt;span class="n"&gt;rate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MMR&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights_v&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights_v&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="c1"&gt;# The error is calculated using the formula from the data sheet&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Z&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights_v&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MMR&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;rate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights_v&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;zhi&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;groupby&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;stratum&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;house&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;agg&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Z&lt;/span&gt;
    &lt;span class="n"&gt;zh&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;zhi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;groupby&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;level&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;stratum&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="c1"&gt;# Number of households per stratum&lt;/span&gt;
    &lt;span class="n"&gt;nk&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;zhi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;groupby&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;level&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;stratum&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;zh&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;zh&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;nk&lt;/span&gt;

    &lt;span class="n"&gt;stratum_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;zh&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;
    &lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nk&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;ind&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;stratum_labels&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;delta2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;zhi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;zh&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;
        &lt;span class="n"&gt;delta2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;delta2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;ind&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nk&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nk&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;delta2&lt;/span&gt;
        &lt;span class="n"&gt;ind&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

    &lt;span class="n"&gt;standard_error&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;rate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;standard_error&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If we apply this function to our whole DataFrame, we will get the national &lt;span class="caps"&gt;MMR&lt;/span&gt; vaccination rate and standard&amp;nbsp;error.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;calculate_rate_and_error(data)&lt;/span&gt;
&lt;span class="err"&gt;# output: (0.90767842904073048, 0.0043003916851249895)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h5&gt;&lt;strong&gt;Data segmentation &amp;#8212; stats by&amp;nbsp;group&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;But we can also do more! If we first apply the DateFrame&amp;#8217;s groupby method, we can split the data along any feature of interest. For example, below we split the data along the state column. This generates subgroups for each state. Next, we use the apply method to run our custom function on all the different groups of data. We then clean up the output, unzip the tuple and generate a graph showing the &lt;span class="caps"&gt;MMR&lt;/span&gt; vaccination rate by state. It should be evident that with only modest effort, one can modify this code to group the data in many varying ways &amp;#8212; all that needs to be done is to adjust the arguments of the groupby&amp;nbsp;command.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Now that we have our function it is easy to calculate values&lt;/span&gt;
&lt;span class="c1"&gt;# for any group.&lt;/span&gt;

&lt;span class="c1"&gt;# To examine rates by state, we will group by state&lt;/span&gt;
&lt;span class="n"&gt;grouped&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;groupby&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;state&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# We then apply our function to the grouped data, and save&lt;/span&gt;
&lt;span class="c1"&gt;# it as a data frame&lt;/span&gt;
&lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;grouped&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;calculate_rate_and_error&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# To conver the results from:&lt;/span&gt;
&lt;span class="c1"&gt;# state&lt;/span&gt;
&lt;span class="c1"&gt;# Alabama (0.931323319509, 0.017837146103)&lt;/span&gt;
&lt;span class="c1"&gt;# Alaska (0.862202058663, 0.0257844894834)&lt;/span&gt;
&lt;span class="c1"&gt;# to&lt;/span&gt;
&lt;span class="c1"&gt;# Rate Standard Error&lt;/span&gt;
&lt;span class="c1"&gt;# state&lt;/span&gt;
&lt;span class="c1"&gt;# Alabama 0.931323 0.017837&lt;/span&gt;
&lt;span class="c1"&gt;# Alaska 0.862202 0.025784&lt;/span&gt;
&lt;span class="c1"&gt;# We use the following code&lt;/span&gt;

&lt;span class="n"&gt;new_col_list&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Rate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Standard Error&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;new_col_list&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;inplace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Make a sorted copy of the data&lt;/span&gt;
&lt;span class="nb"&gt;sorted&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;by&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Rate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ascending&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Rate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Rate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;

&lt;span class="c1"&gt;# Save a csv file if wanted&lt;/span&gt;
&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Rate 2012.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;float_format&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="si"&gt;%5.2f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Generate an online plot&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Data&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;
    &lt;span class="n"&gt;Bar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;sorted&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Rate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="n"&gt;orientation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;h&amp;#39;&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;plot_url&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;py&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;plot&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="Case studies"></category></entry><entry><title>Mathematics of measles</title><link href="https://efavdb.com/math-of-measles" rel="alternate"></link><published>2015-02-25T13:37:00-08:00</published><updated>2015-02-25T13:37:00-08:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2015-02-25:/math-of-measles</id><summary type="html">&lt;p&gt;Here, we introduce &amp;#8212; and outline a solution to &amp;#8212; a generalized &lt;span class="caps"&gt;SIR&lt;/span&gt; model for infectious disease. This is referenced in our &lt;a href="http://efavdb.github.io/vaccination-rates"&gt;following post&lt;/a&gt; on measles and vaccination rates. Our generalized &lt;span class="caps"&gt;SIR&lt;/span&gt; model differs from the &lt;a href="http://en.wikipedia.org/wiki/Epidemic_model#The_SIR_model"&gt;original &lt;span class="caps"&gt;SIR&lt;/span&gt; model&lt;/a&gt; of Kermack and McKendrick in that we allow for two susceptible sub-populations, one …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Here, we introduce &amp;#8212; and outline a solution to &amp;#8212; a generalized &lt;span class="caps"&gt;SIR&lt;/span&gt; model for infectious disease. This is referenced in our &lt;a href="http://efavdb.github.io/vaccination-rates"&gt;following post&lt;/a&gt; on measles and vaccination rates. Our generalized &lt;span class="caps"&gt;SIR&lt;/span&gt; model differs from the &lt;a href="http://en.wikipedia.org/wiki/Epidemic_model#The_SIR_model"&gt;original &lt;span class="caps"&gt;SIR&lt;/span&gt; model&lt;/a&gt; of Kermack and McKendrick in that we allow for two susceptible sub-populations, one vaccinated against disease and one not. We conclude by presenting some python code that integrates the equations numerically. An example solution obtained using this code is given&amp;nbsp;below.&lt;/p&gt;
&lt;iframe width="600" height="533" frameborder="0" scrolling="no" src="//plot.ly/~jslandy/58.embed"&gt;&lt;/iframe&gt;

&lt;h4&gt;&lt;strong&gt;The&amp;nbsp;model&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;The equations describing our generalized &lt;span class="caps"&gt;SIR&lt;/span&gt; model&amp;nbsp;are
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{1} \label{eq1}
\dot{S}_{U} &amp;amp;=&amp;amp; - b_{U} S_{U} I \\
\tag{2} \label{eq2}
\dot{S}_{U} &amp;amp;=&amp;amp; - b_{U} S_{U} I \\
\tag{3} \label{eq3}
\dot{R} &amp;amp;=&amp;amp; k I \\
\tag{4} \label{eq4}
1 &amp;amp;=&amp;amp; I + R + S_U + S_V
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;Here, &lt;span class="math"&gt;\(S_{U}\)&lt;/span&gt;, &lt;span class="math"&gt;\(S_{V}\)&lt;/span&gt;, &lt;span class="math"&gt;\(I\)&lt;/span&gt;, and &lt;span class="math"&gt;\(R\)&lt;/span&gt; are population fractions corresponding to those unvaccinated and as yet uninfected, vaccinated and as yet uninfected, currently infected and contagious, and once contagious but no longer (recovered, perhaps), respectively. The first two equations above are instances of the &lt;a href="http://en.wikipedia.org/wiki/Law_of_mass_action"&gt;law of mass action&lt;/a&gt;. They approximate the infection rates as being proportional to the rates of susceptible-infected individual encounters. We refer to &lt;span class="math"&gt;\(b_{U}\)&lt;/span&gt; and &lt;span class="math"&gt;\(b_{V}\)&lt;/span&gt; here as the &lt;em&gt;infection rate parameters&lt;/em&gt; of the two subpopulations. The third equation above approximates the dynamics of recovery: The form chosen supposes that an infected individual has a fixed probability of returning to health each day. We will refer to &lt;span class="math"&gt;\(k\)&lt;/span&gt; as the &lt;em&gt;recovery rate parameter&lt;/em&gt;. The final equation above simply states that the subpopulation fractions have to always sum to&amp;nbsp;one.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Parameter&amp;nbsp;estimation&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;We can estimate the values &lt;span class="math"&gt;\(b_{U}\)&lt;/span&gt; and &lt;span class="math"&gt;\(b_{V}\)&lt;/span&gt; by introducing a close contact number (&lt;span class="math"&gt;\(ccn\)&lt;/span&gt;) variable, which is the average number of close contacts that individual infected, contagious people make per day. As a rough ball park, let us suppose that &lt;span class="math"&gt;\(ccn \approx 3\)&lt;/span&gt;. According to the &lt;span class="caps"&gt;CDC&lt;/span&gt;, an un-vaccinated person making close contact with someone with measles has a 90&lt;span class="math"&gt;\(%\)&lt;/span&gt; chance of contracting the illness. On the other hand, those who have been vaccinated a single time have a 95&lt;span class="math"&gt;\(%\)&lt;/span&gt; chance of being immune to the disease. Let&amp;#8217;s estimate that the combined population of individuals who have been vaccinated have a 1&lt;span class="math"&gt;\(%\)&lt;/span&gt; chance of contracting the illness upon close contact. These considerations&amp;nbsp;suggest
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
b_{U} \approx 3 \times 0.9 &amp;amp;= 0.27, \\
b_{V} \approx 3 \times 0.01 &amp;amp;= 0.03
\end{align}&lt;/div&gt;
&lt;p&gt;
The value of &lt;span class="math"&gt;\(k\)&lt;/span&gt; can be simply estimated using the fact that infected individuals are only contagious for about &lt;span class="math"&gt;\(8\)&lt;/span&gt; days, only four of which occur before rash appears. Assuming those who are showing symptoms quickly stop circulating, this suggests about five &amp;#8220;effectively contagious&amp;#8221; days,&amp;nbsp;or
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
k \approx 1/5 = 0.2.
\end{align}&lt;/div&gt;
&lt;p&gt;
Note that here and elsewhere, we measure time in units of&amp;nbsp;days.&lt;/p&gt;
&lt;p&gt;It&amp;#8217;s important to note that, although the qualitative properties of the solutions to our model are insensitive to parameter value variations, this is not true for the numerical values that it predicts. We have chosen parameter values that seem reasonable to us. Further, with these choices, many of the model&amp;#8217;s key quantitative values line up with corresponding &lt;span class="caps"&gt;CDC&lt;/span&gt; estimates. Those interested can experiment to see what sort of flexibility is allowed through modest parameter&amp;nbsp;variation.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Solution by&amp;nbsp;quadrature&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Equations (\ref{eq1}-\ref{eq3})&amp;nbsp;give
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align} \tag{5} \label{Svals}
S_{U} = S_{U0} e^{ - \frac{b_{U} R}{k}}, \\
S_{V} = S_{V0} e^{- \frac{b_{V} R}{k}}.
\end{align}&lt;/div&gt;
&lt;p&gt;
Combining with (\ref{eq4}) and integrating&amp;nbsp;gives
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
\frac{\dot{R}}{k} =I_0 -S_{U0} \left [ e^{ - \frac{b_{U} R}{k}}- 1 \right ] - S_{20} \left [e^{ - \frac{b_{V} R}{k}}- 1 \right ] - R
\end{align}&lt;/div&gt;
&lt;p&gt;
Integrating&amp;nbsp;again,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align} \tag{6} \label{solution}
kt = \int_{0}^R \frac{d R^{\prime}}{I_0 -S_{U0} \left [ e^{ - \frac{b_{U} R^{\prime}}{k}}- 1 \right ] - S_{V0} \left [e^{ - \frac{b_{V} R^{\prime}}{k}}- 1 \right ] - R^{\prime}} .
\end{align}&lt;/div&gt;
&lt;p&gt;
This implicitly defines &lt;span class="math"&gt;\(R\)&lt;/span&gt; as function of&amp;nbsp;time.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Small time&amp;nbsp;behavior&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;At small &lt;span class="math"&gt;\(t\)&lt;/span&gt;, &lt;span class="math"&gt;\(R\)&lt;/span&gt; is also small, so (\ref{solution}) can be approximated&amp;nbsp;as
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
k t = \int_{0}^R \frac{d R^{\prime}}{I_0 + \left [ \frac{ b_{U} S_{U0}}{k} +\frac{ b_{V} S_{V0}}{k} - 1 \right ]R^{\prime}}.
\end{align}&lt;/div&gt;
&lt;p&gt;
This form can be integrated analytically. Doing so, and solving for &lt;span class="math"&gt;\(R\)&lt;/span&gt;, we&amp;nbsp;obtain
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
R = \frac{k}{b_{U} S_{U0} + b_{V} S_{V0} - k} \left \{e^{ (b_{U} S_{U0} + b_{V} S_{V0} - k )t } -1 \right \}, \ \ \
I = I_0 e^{ (b_{U} S_{U0} + b_{V} S_{V0} - k )t}.
\end{align}&lt;/div&gt;
&lt;p&gt;
Early disease spread is characterized by either exponential growth or decay, governed by the sign of the parameter combination &lt;span class="math"&gt;\(b_{U} S_{U0} + b_{V} S_{V0} - k\)&lt;/span&gt;: a phase&amp;nbsp;transition!&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Total&amp;nbsp;contractions&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;The total number of people infected in an outbreak can be obtained by evaluating &lt;span class="math"&gt;\(R\)&lt;/span&gt; at long times, where &lt;span class="math"&gt;\(I = 0\)&lt;/span&gt;. In this limit, using (\ref{eq4}) and (\ref{Svals}), we&amp;nbsp;have
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
S_{U0} e^{- \frac{b_{U} R}{k}}+ S_{V0} e^{ - \frac{b_{V} R}{k}}+ R = 1.
\end{align}&lt;/div&gt;
&lt;p&gt;
This equation can be solved numerically to obtain the total contraction count as a function of the model parameters and initial conditions. A plot against &lt;span class="math"&gt;\(S_{U0}\)&lt;/span&gt; of such a solution for our measles-appropriate parameter estimates is given in our &lt;a href="http://efavdb.github.io/vaccination-rates"&gt;next post&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Numerical integration in&amp;nbsp;python&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Below, we provide code that can be used to integrate (\ref{eq1}-\ref{eq4}). The plot shown in our introduction provides one example solution. It&amp;#8217;s quite interesting to see how the solutions vary with parameter values, and we suggest that those interested try it&amp;nbsp;out.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;#Solving the SIR model for infectious disease. JSL 2/18/2015&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;math&lt;/span&gt;

&lt;span class="n"&gt;ccn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="c1"&gt;# &amp;quot;close contact number&amp;quot; = people per day&lt;/span&gt;
&lt;span class="c1"&gt;#interacting closely with typical infected person&lt;/span&gt;

&lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;       &lt;span class="c1"&gt;# Rate of &amp;#39;recovery&amp;#39; [1].&lt;/span&gt;
&lt;span class="n"&gt;b1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ccn&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mf"&gt;0.9&lt;/span&gt;   &lt;span class="c1"&gt;# Approximate infection rate un-vaccinated [3].&lt;/span&gt;
&lt;span class="n"&gt;b2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ccn&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mf"&gt;0.01&lt;/span&gt;  &lt;span class="c1"&gt;# Approximate infection rate un-vaccinated [4].&lt;/span&gt;

&lt;span class="c1"&gt;#Initial conditions (fraction of people in each category)&lt;/span&gt;
&lt;span class="n"&gt;I0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.001&lt;/span&gt;          &lt;span class="c1"&gt;# initial population fraction infected.&lt;/span&gt;
&lt;span class="n"&gt;S10&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt;           &lt;span class="c1"&gt;# population fraction unvaccinated.&lt;/span&gt;
&lt;span class="n"&gt;S20&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;I0&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;S10&lt;/span&gt;  &lt;span class="c1"&gt;# population fraction vacccinated.&lt;/span&gt;
&lt;span class="n"&gt;R0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;            &lt;span class="c1"&gt;# intial recovered fraction.&lt;/span&gt;

&lt;span class="n"&gt;dt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.01&lt;/span&gt;           &lt;span class="c1"&gt;# integration time step&lt;/span&gt;
&lt;span class="n"&gt;days&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;          &lt;span class="c1"&gt;# total days considered&lt;/span&gt;

&lt;span class="n"&gt;I&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;I0&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;days&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="p"&gt;))]&lt;/span&gt;
&lt;span class="n"&gt;S1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;S10&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;days&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="p"&gt;))]&lt;/span&gt;
&lt;span class="n"&gt;S2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;S20&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;days&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="p"&gt;))]&lt;/span&gt;
&lt;span class="n"&gt;R&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;R0&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;days&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="p"&gt;))]&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;days&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
    &lt;span class="n"&gt;S1&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;S1&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;b1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;S1&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;dt&lt;/span&gt;
    &lt;span class="n"&gt;S2&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;S2&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;b2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;S2&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;dt&lt;/span&gt;
    &lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;S1&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; \
    &lt;span class="n"&gt;b2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;S2&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;dt&lt;/span&gt;
    &lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;dt&lt;/span&gt;

&lt;span class="n"&gt;time&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;days&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="p"&gt;))]&lt;/span&gt;

&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;pylab&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;red&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;S1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;blue&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;S2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;green&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;black&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;1400&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;I0&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;b1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;S10&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;S20&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;1400&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;
    &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;purple&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;yscale&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;log&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;time [days]&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;population %&lt;/span&gt;&lt;span class="se"&gt;\&amp;#39;&lt;/span&gt;&lt;span class="s1"&gt;s&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# [1] Measles patients are contagious for eight days&lt;/span&gt;
&lt;span class="c1"&gt;# four of which are before symptoms appear. [2]&lt;/span&gt;
&lt;span class="c1"&gt;# [2] http://www.cdc.gov/measles/about/transmission.html&lt;/span&gt;
&lt;span class="c1"&gt;# [3] Assume infected have close contact with five people/day.&lt;/span&gt;
&lt;span class="c1"&gt;# 90% of the un-vaccinated get sick in such situations.&lt;/span&gt;
&lt;span class="c1"&gt;# [4] Single vaccination gives ~95% immunity rate [5]. Many&lt;/span&gt;
&lt;span class="c1"&gt;# have two doses, which drops rate to very low.&lt;/span&gt;
&lt;span class="c1"&gt;# [5] http://www.cdc.gov/mmwr/preview/mmwrhtml/00053391.htm&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Statistics"></category></entry><entry><title>NBA week 14 summary, week 15 predictions</title><link href="https://efavdb.com/nba-week-14-summary-week-15-predictions" rel="alternate"></link><published>2015-02-20T11:12:00-08:00</published><updated>2015-02-20T11:12:00-08:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2015-02-20:/nba-week-14-summary-week-15-predictions</id><summary type="html">&lt;p&gt;Our first ever 100% accuracy week! We will never beat this. Breakdown by point spread below, and new predictions are&amp;nbsp;up.&lt;/p&gt;
&lt;p&gt;In other news: This week, efavdb.com welcomes a new, official member to our team, &lt;a href="http://efavdb.github.io/authors"&gt;Damien Ramunno-Johnson&lt;/a&gt;! He previously contributed two very interesting guest posts to the site, one …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Our first ever 100% accuracy week! We will never beat this. Breakdown by point spread below, and new predictions are&amp;nbsp;up.&lt;/p&gt;
&lt;p&gt;In other news: This week, efavdb.com welcomes a new, official member to our team, &lt;a href="http://efavdb.github.io/authors"&gt;Damien Ramunno-Johnson&lt;/a&gt;! He previously contributed two very interesting guest posts to the site, one on &lt;a href="http://efavdb.github.io/machine-learning-for-facial-recognition-3"&gt;facial recognition software&lt;/a&gt; and one on the &lt;a href="http://efavdb.github.io/machine-learning-with-wearable-sensors"&gt;interpretation of wearable sensor data&lt;/a&gt; (eg, &lt;a href="https://jawbone.com/"&gt;Jawbone&amp;#8217;s up band&lt;/a&gt;). We aim to eventually bring site membership up to about four or five like-minded, yet complementary contributors, each interested in contributing about one machine-learning related post a month. We&amp;#8217;re especially excited to welcome Damien on board &amp;#8212; he&amp;#8217;s an old pal, and also one with a keen eye for finding topics of general interest. In fact, we&amp;#8217;re working on a joint post together right now that is both timely and interesting &amp;#8212; stay&amp;nbsp;tuned!&lt;/p&gt;
&lt;p&gt;Details of our perfect&amp;nbsp;week:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Point spread&lt;/th&gt;
&lt;th&gt;# games&lt;/th&gt;
&lt;th&gt;Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&amp;lt; 5&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;100%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5-9&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;&lt;span class="caps"&gt;NA&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10-14&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;&lt;span class="caps"&gt;NA&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;gt;14&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;100%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content><category term="NBA prediction project"></category></entry><entry><title>Analyzing Analysts</title><link href="https://efavdb.com/analyzinganalysts" rel="alternate"></link><published>2015-02-08T14:15:00-08:00</published><updated>2015-02-08T14:15:00-08:00</updated><author><name>Dustin McIntosh</name></author><id>tag:efavdb.com,2015-02-08:/analyzinganalysts</id><summary type="html">&lt;p&gt;In this post, Dustin provides an overview of some of his work from his time in the &lt;a href="http://insightdatascience.com/"&gt;Insight Data Science Fellowship&lt;/a&gt; program — work done in collaboration with &lt;a href="https://modeanalytics.com/"&gt;Mode Analytics&lt;/a&gt;, an online collaborative &lt;span class="caps"&gt;SQL&lt;/span&gt;&amp;nbsp;platform.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="//www.slideshare.net/DustinMcIntosh/insight-demo-44864932" title="Insight Demo"&gt;Insight Demo&lt;/a&gt;&lt;/strong&gt; from &lt;strong&gt;&lt;a href="//www.slideshare.net/DustinMcIntosh"&gt;Dustin&amp;nbsp;McIntosh&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Many high-value business insights have their answers rooted in data. Companies …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In this post, Dustin provides an overview of some of his work from his time in the &lt;a href="http://insightdatascience.com/"&gt;Insight Data Science Fellowship&lt;/a&gt; program — work done in collaboration with &lt;a href="https://modeanalytics.com/"&gt;Mode Analytics&lt;/a&gt;, an online collaborative &lt;span class="caps"&gt;SQL&lt;/span&gt;&amp;nbsp;platform.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="//www.slideshare.net/DustinMcIntosh/insight-demo-44864932" title="Insight Demo"&gt;Insight Demo&lt;/a&gt;&lt;/strong&gt; from &lt;strong&gt;&lt;a href="//www.slideshare.net/DustinMcIntosh"&gt;Dustin&amp;nbsp;McIntosh&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Many high-value business insights have their answers rooted in data. Companies hire data analysts to extract these insights by dissecting their data, largely through querying databases with &lt;span class="caps"&gt;SQL&lt;/span&gt; (Structured Query&amp;nbsp;Language).&lt;/p&gt;
&lt;p&gt;However, data analysis can often be difficult. &lt;a href="https://modeanalytics.com"&gt;Mode Analytics&lt;/a&gt; is a company that aims to streamline the process of learning and engaging in data analysis with &lt;span class="caps"&gt;SQL&lt;/span&gt;, both for experts and novices alike. As an Insight Data Science Fellow, I have been working with Mode to help them analyze their users&amp;#8217; &lt;span class="caps"&gt;SQL&lt;/span&gt; code, mistakes and all, to identify opportunities for product improvement. Specifically, I&amp;#8217;ve been trying to address the following&amp;nbsp;questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What types of &lt;span class="caps"&gt;SQL&lt;/span&gt; errors do users make? Do these vary with user ability&amp;nbsp;level?&lt;/li&gt;
&lt;li&gt;Do early struggles dishearten beginners, hampering them from becoming power&amp;nbsp;users?&lt;/li&gt;
&lt;li&gt;More generally, what characteristics differentiate the &lt;span class="caps"&gt;SQL&lt;/span&gt; queries of aspiring and expert&amp;nbsp;analysts?&lt;/li&gt;
&lt;li&gt;Given answers to these questions, how can Mode modify their product to make analysis easier for their&amp;nbsp;users?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;My analysis uncovered a number of actionable insights for&amp;nbsp;Mode:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Mostly, analysts make the same sorts of errors regardless of their ability level. The good news is that many of these errors are avoidable: Mode can&amp;nbsp;help.&lt;/li&gt;
&lt;li&gt;Novice users actually do not seem to churn due to frustration with errors. It&amp;#8217;s more likely that they leave due simply to having completed the tutorials - Mode can increase user retention by proactively encouraging users to upload and interact with their own private data after finishing the&amp;nbsp;tutorials.&lt;/li&gt;
&lt;li&gt;Mode can customize the user experience soon after a user joins the site in order to better serve users based on their &lt;span class="caps"&gt;SQL&lt;/span&gt; skills. To this end, I built a model to classify users as experts or novices based on the content of their queries. This model reveals that experts take extra care in the formatting of their queries, implying an advanced knowledge of the &lt;span class="caps"&gt;SQL&lt;/span&gt; structure that could be emphasized in the&amp;nbsp;tutorials.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;WHAT&lt;/span&gt; &lt;span class="caps"&gt;IS&lt;/span&gt; &lt;span class="caps"&gt;MODE&lt;/span&gt; &lt;span class="caps"&gt;AND&lt;/span&gt; &lt;span class="caps"&gt;WHO&lt;/span&gt; &lt;span class="caps"&gt;ARE&lt;/span&gt; &lt;span class="caps"&gt;THEIR&lt;/span&gt; &lt;span class="caps"&gt;USERS&lt;/span&gt;?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://modeanalytics.com/"&gt;Mode Analytics&lt;/a&gt; provides a one-stop shop for all the needs of a data analyst. Users can easily connect their data to Mode&amp;#8217;s web-based app, query it with &lt;span class="caps"&gt;SQL&lt;/span&gt;, and create and share visualizations of their analysis to convey the insights they&amp;#8217;ve extracted. In addition, Mode offers free instruction via their &lt;a href="http://sqlschool.modeanalytics.com/"&gt;&lt;span class="caps"&gt;SQL&lt;/span&gt; school&lt;/a&gt;, which instructs novices on the basics of &lt;span class="caps"&gt;SQL&lt;/span&gt; while querying some public, tutorial datasets.&lt;br&gt;
&lt;a href="https://efavdb.com/wp-content/uploads/2015/02/errorRate31.png"&gt;&lt;img alt="errorRate3" src="https://efavdb.com/wp-content/uploads/2015/02/errorRate31.png"&gt;&lt;/a&gt;&lt;br&gt;
With the goal of cohorting users based on their ability level, I group users based on the number of queries they have submitted to Mode (see plot at right of tutorial reference rate and error rate by&amp;nbsp;cohort):&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Power users (1000 + queries): this group primarily queries private data sources with no need for the tutorials, making only occasional&amp;nbsp;errors.&lt;/li&gt;
&lt;li&gt;Novices (10-1000 queries): the majority of these users are heavily invested in the tutorials and are making a lot more errors than the experts as they&amp;nbsp;learn.&lt;/li&gt;
&lt;li&gt;Infrequent queriers (&amp;lt;10 queries): these users have only visited Mode&amp;#8217;s website long enough to make a few queries - most of which are easy tutorial exercises on which they are making very few&amp;nbsp;errors.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The demarcations at 10 and 1000 queries are somewhat arbitrary at this point. Undoubtedly, there are many experts hidden in the 10-1000 category who already possess advanced &lt;span class="caps"&gt;SQL&lt;/span&gt; knowledge, but are yet to extensively use Mode’s platform for their analysis. As we will see, we can build a model to distinguish these users based on the content of their queries rather than their query&amp;nbsp;count.&lt;/p&gt;
&lt;p&gt;The primary goal, from a business perspective, is to move users up this list: get infrequent queriers invested in the product and get novice tutorial users up to speed with &lt;span class="caps"&gt;SQL&lt;/span&gt; so they can enjoy all aspects of Mode&amp;#8217;s product. For the remainder of the post, I ignore the infrequent queriers [1] in favor of analyzing novices and the differences between them and the&amp;nbsp;experts.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;WHAT&lt;/span&gt; &lt;span class="caps"&gt;ERRORS&lt;/span&gt; &lt;span class="caps"&gt;DO&lt;/span&gt; &lt;span class="caps"&gt;SQL&lt;/span&gt; &lt;span class="caps"&gt;USERS&lt;/span&gt; &lt;span class="caps"&gt;MAKE&lt;/span&gt;?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="caps"&gt;SQL&lt;/span&gt; is a very simple language. With a simple, declarative style and only about 200 keywords in total, of which only a few dozen are in common usage [2], there is very little to memorize for &lt;span class="caps"&gt;SQL&lt;/span&gt; users. Thus, most errors should be recognizable and avoidable in product design. Examining the types of errors &lt;span class="caps"&gt;SQL&lt;/span&gt; users make informs us how we can make an analyst&amp;#8217;s experience better through the Mode platform.  In the plot below, I count the number of each type of error made by the two cohorts (experts and novices).&lt;br&gt;
&lt;a href="https://efavdb.com/wp-content/uploads/2015/02/errorClass1.png"&gt;&lt;img alt="errorClass" src="https://efavdb.com/wp-content/uploads/2015/02/errorClass1.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The most striking thing about this analysis is that the two most common error types are forgetting and/or misspelling table and column names. Fortunately, these errors are easily addressed through product design: simply prominently displaying the table/column names may significantly reduce errors. It may even be possible in some instances to auto-fill the&amp;nbsp;names.&lt;/p&gt;
&lt;p&gt;There are a few subtle differences between expert and novice errors worth noting. Novice users tend to make more syntax errors, which is not a surprise given they are less familiar with the language. Power users, on the other hand, make a broader distribution of errors (e.g., a lot more rare errors). Further, experts much more frequently run into the limits of the system (e.g., timeouts, internal errors), an indication that they are running more complex queries. From a product perspective, correcting errors for novices is the priority as they make the most errors and are less likely to know how to correct them. Further, as novices tend to make a narrower range of errors, it is likely possible to parse out many of these errors and either auto-correct them or provide more personalized help to the user than the standard &lt;span class="caps"&gt;SQL&lt;/span&gt; error&amp;nbsp;messages.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;DO&lt;/span&gt; &lt;span class="caps"&gt;NOVICES&lt;/span&gt; &lt;span class="caps"&gt;GET&lt;/span&gt; &lt;span class="caps"&gt;FRUSTRATED&lt;/span&gt; &lt;span class="caps"&gt;AND&lt;/span&gt; &lt;span class="caps"&gt;QUIT&lt;/span&gt;?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A principal concern for educating the novice users is that they may consistently run into certain types of errors, not understand how to correct them, and quit out of frustration. Should Mode customize the standard error messages to better direct novices to their problems? To see whether or not this is the case, I looked at the set of churned novice users (those that have not made a query since November) and examined their final few queries. If user frustration is causing churn, we expect the error rate to increase as the users approach churn.&lt;br&gt;
&lt;a href="https://efavdb.com/wp-content/uploads/2015/02/errorChurn.png"&gt;&lt;img alt="errorChurn" src="https://efavdb.com/wp-content/uploads/2015/02/errorChurn.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Surprisingly, the rate at which users commit errors before they churn is actually lower than the group&amp;#8217;s average error rate and decreases until they churn. Thus, we can infer that the average novice does &lt;strong&gt;not&lt;/strong&gt; quit due to frustration with errors they are making - this is not a major concern for customer&amp;nbsp;retention.&lt;/p&gt;
&lt;p&gt;However, Mode could be more proactive about transitioning customers from their &lt;span class="caps"&gt;SQL&lt;/span&gt; school to connecting their own data and using their visualization tools. 88% of users that churn have not connected their own data sources and are working exclusively on the tutorial datasets. One potential product enhancement is to periodically remind tutorial users of the possibility of connecting their own data and to add tutorial exercises introducing the visualization&amp;nbsp;tools.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;WHAT&lt;/span&gt;&amp;#8217;S &lt;span class="caps"&gt;IN&lt;/span&gt; A &lt;span class="caps"&gt;SQL&lt;/span&gt; &lt;span class="caps"&gt;EXPERT&lt;/span&gt;&amp;#8217;S &lt;span class="caps"&gt;QUERY&lt;/span&gt;?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As discussed above, drawing a line at 1000 queries is not a very useful classification metric for experts versus novices. There are many experts that come to Mode with a lot of expertise in &lt;span class="caps"&gt;SQL&lt;/span&gt;, but have not made 1000 queries yet.  Thus, I developed a model based on a Random Forest classifier to differentiate expert and novice &lt;span class="caps"&gt;SQL&lt;/span&gt; users based on the content of their queries [3]. This model accurately classifies users based on a single query roughly 65% of the time for both classes. Accuracy of the prediction will go up the more unique queries a user submits; for example, after ten queries accuracy may be as high as 90%&amp;nbsp;[4].&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/02/featureImportance5.png"&gt;&lt;img alt="featureImportance5" src="https://efavdb.com/wp-content/uploads/2015/02/featureImportance5.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;One interesting aspect of the Random Forest model is that it determines the most important features that define an expert&amp;#8217;s query from a novice&amp;#8217;s. The result is clear: Of the top five most important features, three of them have to do with formatting; experts take more care in their use of white space, line breaks, and parentheses. In particular, in their queries, experts have a higher density of white space, a lower density of line breaks, and a higher density of parentheses. Thus, experts tend to have longer lines of code with more white space. The two remaining features in the top five, query length and frequency of “select” and “from”, indicate that experts tend to write longer queries with more&amp;nbsp;subqueries.&lt;/p&gt;
&lt;p&gt;Being able to classify a user as an expert or novice based on the content of their queries would be extremely useful for personalizing user experience. If a user connects their data early on and it becomes clear that they have limited experience with &lt;span class="caps"&gt;SQL&lt;/span&gt;, Mode would like to be able to direct those users to relevant tutorials or perhaps to work related to theirs that was performed by experts. Likewise, if a user immediately demonstrates advanced &lt;span class="caps"&gt;SQL&lt;/span&gt; knowledge, Mode would like to direct them to, for example, the data visualization tools that make Mode&amp;#8217;s platform uniquely useful to the experienced analyst. Implementation of my model would permit&amp;nbsp;this.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;CONCLUSIONS&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This analysis provides a number of actionable insights for Mode in serving their user&amp;nbsp;base:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Everyone using the platform is affected by errors relating to misspelling or altogether forgetting the names of the tables and their contained fields in their databases. Implementing an auto-complete of these names or prominently displaying them for the user could considerably reduce the number of errors encountered on&amp;nbsp;Mode.&lt;/li&gt;
&lt;li&gt;Most users that churn are not leaving out of frustration with the errors they are making. The vast majority of churning users, however, are exclusively querying tutorial datasets. Mode can try to get tutorial users more invested in their product by prompting them to connect their data periodically throughout the&amp;nbsp;tutorial.&lt;/li&gt;
&lt;li&gt;In addition to tutorial-using novices and the true &lt;span class="caps"&gt;SQL&lt;/span&gt; experts, Mode has a third class of users. Their query history does not reveal much about them: they only visit Mode briefly, submitting just a few tutorial exercises without making many errors. Looking into who these visitors are via, for example, google analytics data, may provide insight into how to retain these potential&amp;nbsp;users.&lt;/li&gt;
&lt;li&gt;User ability level can be determined based on the content of their first several queries to Mode. This is useful as Mode can effectively personalize response to users as they join the&amp;nbsp;platform.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;FOOTNOTES&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;[1] The query history of the infrequent queriers does not provide very much information as to who these users are. Gathering additional information on these users may prove useful in determining how to retain these&amp;nbsp;customers.&lt;/p&gt;
&lt;p&gt;[2] &lt;em&gt;Top-Heavy Nature of &lt;span class="caps"&gt;SQL&lt;/span&gt; -&lt;/em&gt;Of the approximately 200 keywords in the &lt;span class="caps"&gt;SQL&lt;/span&gt; vocabulary, a very small subset are even remotely common. Looking at the fraction of queries containing each keyword from the expert dataset demonstrates just how top heavy the language is. Each keyword not present on this plot appears in fewer than 4% of all unique queries in the dataset.&lt;br&gt;
&lt;a href="https://efavdb.com/wp-content/uploads/2015/02/SQL_keywords_expert.png"&gt;&lt;img alt="SQL_keywords_expert" src="https://efavdb.com/wp-content/uploads/2015/02/SQL_keywords_expert.png"&gt;&lt;/a&gt;[3] &lt;em&gt;Labeling the Training Set -&lt;/em&gt;I define users with &amp;gt;1000 queries as experts and take only a subset of their most recent queries (those submitted since November 2014) - assuming that they have become better queriers with time. For the novice training set, it is not possible to strictly take the users from the 10-1000 query group as there are likely some new-comer experts in that category (experienced &lt;span class="caps"&gt;SQL&lt;/span&gt; users that have just recently joined Mode). Instead, I took only users with 100-1000 queries that had participated extensively in the tutorials. I then remove those tutorial queries from their set and label those remaining as novice queries for training.&lt;br&gt;
&lt;em&gt;Feature Selection/Engineering&lt;/em&gt;- I use a bag-of-words approach with language defined by &lt;span class="caps"&gt;SQL&lt;/span&gt;&amp;#8217;s keywords. Additional features include query length, number of unique keywords in the query (diversity), as well as fraction of the query that is white space, line breaks, and parentheses. Several keywords are strongly correlated with one another; for this reason, I combine some together (e.g., “select” and “from”) and remove others from the analysis entirely (e.g., “as”, “by” and&amp;nbsp;“on”).&lt;/p&gt;
&lt;p&gt;[4]&lt;em&gt;Growing confidence of classification -&lt;/em&gt;If a user&amp;#8217;s successive queries were independent of one another we would expect confidence in the prediction to grow in accordance with a binomial distribution: &lt;span class="math"&gt;\(P(\)&lt;/span&gt;misclassify&lt;span class="math"&gt;\() = \sum_{i = 0}^{\lfloor n/2 \rfloor} \binom{n}{i} p^i (1-p)^{n-i}\)&lt;/span&gt; with &lt;span class="math"&gt;\(p \approx 0.65\)&lt;/span&gt; and &lt;span class="math"&gt;\(n\)&lt;/span&gt; the number of queries. After ten queries the misclassification error reduces to roughly 10%; after 20 queries - 5%. However, a user&amp;#8217;s queries are typically not independent of one another, so this is likely a generous&amp;nbsp;estimate.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Case studies"></category></entry><entry><title>NBA week 13 summary, week 14 predictions</title><link href="https://efavdb.com/nba-week-13-summary-week-14-predictions" rel="alternate"></link><published>2015-02-06T11:30:00-08:00</published><updated>2015-02-06T11:30:00-08:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2015-02-06:/nba-week-13-summary-week-14-predictions</id><summary type="html">&lt;p&gt;Correct predictions on 31/50 or 62% of the games this past week, including a number of the significant upsets. However, we didn&amp;#8217;t catch them all. Details by point spread below, and new predictions are&amp;nbsp;up.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Point spread&lt;/th&gt;
&lt;th&gt;# games&lt;/th&gt;
&lt;th&gt;Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&amp;lt;= 5&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;36%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;6-10&lt;/td&gt;
&lt;td&gt;17&lt;/td&gt;
&lt;td&gt;82%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;11-15&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;58 …&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;</summary><content type="html">&lt;p&gt;Correct predictions on 31/50 or 62% of the games this past week, including a number of the significant upsets. However, we didn&amp;#8217;t catch them all. Details by point spread below, and new predictions are&amp;nbsp;up.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Point spread&lt;/th&gt;
&lt;th&gt;# games&lt;/th&gt;
&lt;th&gt;Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&amp;lt;= 5&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;36%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;6-10&lt;/td&gt;
&lt;td&gt;17&lt;/td&gt;
&lt;td&gt;82%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;11-15&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;58%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;gt;15&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;60%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content><category term="NBA prediction project"></category></entry><entry><title>NBA week 12 summary, week 13 predictions</title><link href="https://efavdb.com/nba-week-12-summary-week-13-predictions" rel="alternate"></link><published>2015-01-30T12:36:00-08:00</published><updated>2015-01-30T12:36:00-08:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2015-01-30:/nba-week-12-summary-week-13-predictions</id><summary type="html">&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/01/10941913_10101474381893663_4472456147644800505_n.jpg"&gt;&lt;img alt="10941913_10101474381893663_4472456147644800505_n" src="https://efavdb.com/wp-content/uploads/2015/01/10941913_10101474381893663_4472456147644800505_n.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We correctly predicted 36/53 of the games this past week, which equates to an accuracy of 67.9% &amp;#8212; a slight improvement over the past couple of weeks, but still short of the 70% level we strive for (off by about one game!). The breakdown by point spread is below …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/01/10941913_10101474381893663_4472456147644800505_n.jpg"&gt;&lt;img alt="10941913_10101474381893663_4472456147644800505_n" src="https://efavdb.com/wp-content/uploads/2015/01/10941913_10101474381893663_4472456147644800505_n.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We correctly predicted 36/53 of the games this past week, which equates to an accuracy of 67.9% &amp;#8212; a slight improvement over the past couple of weeks, but still short of the 70% level we strive for (off by about one game!). The breakdown by point spread is below, and the new predictions are&amp;nbsp;up.&lt;/p&gt;
&lt;p&gt;This week, we also updated the design of the &lt;a href="http://efavdb.github.io/weekly-nba-predictions"&gt;weekly &lt;span class="caps"&gt;NBA&lt;/span&gt; predictions page&lt;/a&gt;. We&amp;#8217;ve improved the visual look and also opted to print out the win-loss records next to each team. This is actually pretty useful because it provides a quick check whether our predictions are unusual in some way. Often, I&amp;#8217;ve found that if a prediction disagrees with the record-based favorite, a quick check at the dashboard wheel can give some insight into why (eg, checking prior matches of the pair, or perhaps checking whether the predicted winner has managed to beat some surprisingly good teams, etc.). This sort of situation is actually where I have the most fun using the wheel. We&amp;#8217;ve also implemented some changes to our weekly predictions page processing code that allows for it to be viewed nicely on any screen size &amp;#8212; a significant improvement for us cell phone viewers! We&amp;#8217;ll try to implement a similarly flexible version of the dashboard soon,&amp;nbsp;too.&lt;/p&gt;
&lt;p&gt;One last point: Thanks to James Kung for inviting us to what turned out to be a very memorable game last Friday (pic above, also courtesy of James)! edit: the &lt;a href="https://www.youtube.com/watch?v=Wxjsz92v53M"&gt;youtube vid&lt;/a&gt; of Klay&amp;#8217;s historic quarter. edit 2: What&amp;#8217;s not shown in the video, and what really got the crowd excited from the start of Klay&amp;#8217;s run, was the fact that the Warriors seemingly missed their previous 20 shots. This let the Kings come back from a large early Warriors lead. The crowd was on their feet Klay&amp;#8217;s second shot&amp;nbsp;onwards.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Point spread&lt;/th&gt;
&lt;th&gt;# games&lt;/th&gt;
&lt;th&gt;Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&amp;lt; 5&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;50%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5-9&lt;/td&gt;
&lt;td&gt;21&lt;/td&gt;
&lt;td&gt;76%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10-14&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;67%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;gt;14&lt;/td&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;td&gt;69%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content><category term="NBA prediction project"></category></entry><entry><title>NBA week 11 summary, week 12 predictions</title><link href="https://efavdb.com/nba-week-11-summary-week-12-predictions" rel="alternate"></link><published>2015-01-23T12:12:00-08:00</published><updated>2015-01-23T12:12:00-08:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2015-01-23:/nba-week-11-summary-week-12-predictions</id><summary type="html">&lt;p&gt;Again, reasonably good accuracy this past week &amp;#8212; 36/54, or 66.6% overall. The breakdown by point spread is given below, and the new predictions are&amp;nbsp;up.&lt;/p&gt;
&lt;p&gt;We made a couple of changes to the dashboard this week. First, it has a new, cleaner look, thanks to &lt;a href="http://www.jairehtecarro.com/"&gt;Jaireh Tecarro&lt;/a&gt;, who …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Again, reasonably good accuracy this past week &amp;#8212; 36/54, or 66.6% overall. The breakdown by point spread is given below, and the new predictions are&amp;nbsp;up.&lt;/p&gt;
&lt;p&gt;We made a couple of changes to the dashboard this week. First, it has a new, cleaner look, thanks to &lt;a href="http://www.jairehtecarro.com/"&gt;Jaireh Tecarro&lt;/a&gt;, who kindly donated a redesign. Secondly, we discovered that our prior processing code implementation was inadvertently defaulting to a non-smooth rendering mode as a consequence of some coordinate transformations that we were making use of. This behavior was not explained in the general processing references, and so came as a surprise to us. We&amp;#8217;ve reimplemented some of the drawing commands in order to avoid the problem, and we&amp;#8217;ve documented it in our &lt;a href="http://efavdb.github.io/processing-and-processing-js-tips-and-tricks"&gt;processing tips and tricks post&lt;/a&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Point spread&lt;/th&gt;
&lt;th&gt;# games&lt;/th&gt;
&lt;th&gt;Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&amp;lt; 5&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;45%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5-9&lt;/td&gt;
&lt;td&gt;23&lt;/td&gt;
&lt;td&gt;65%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10-14&lt;/td&gt;
&lt;td&gt;18&lt;/td&gt;
&lt;td&gt;78%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;gt;14&lt;/td&gt;
&lt;td&gt;18&lt;/td&gt;
&lt;td&gt;72%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content><category term="NBA prediction project"></category></entry><entry><title>Machine learning for facial recognition</title><link href="https://efavdb.com/machine-learning-for-facial-recognition-3" rel="alternate"></link><published>2015-01-21T17:12:00-08:00</published><updated>2015-01-21T17:12:00-08:00</updated><author><name>Damien RJ</name></author><id>tag:efavdb.com,2015-01-21:/machine-learning-for-facial-recognition-3</id><summary type="html">&lt;p&gt;A guest post, contributed by Damien Ramunno-Johnson (&lt;a href="https://www.linkedin.com/profile/view?id=60223336&amp;amp;authType=NAME_SEARCH&amp;amp;authToken=LOV_&amp;amp;locale=en_US&amp;amp;trk=tyah2&amp;amp;trkInfo=tarId%3A1420748440448%2Ctas%3Adamien%2Cidx%3A1-1-1"&gt;LinkedIn&lt;/a&gt;, &lt;a href="https://efavdb.github.io/pages/authors.html"&gt;bio-sketch&lt;/a&gt;)&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;The ability to identify faces is a skill that people develop very early in life and can apply almost effortlessly. One reason for this is that our brains are very well adapted for pattern recognition. In contrast, facial recognition can …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A guest post, contributed by Damien Ramunno-Johnson (&lt;a href="https://www.linkedin.com/profile/view?id=60223336&amp;amp;authType=NAME_SEARCH&amp;amp;authToken=LOV_&amp;amp;locale=en_US&amp;amp;trk=tyah2&amp;amp;trkInfo=tarId%3A1420748440448%2Ctas%3Adamien%2Cidx%3A1-1-1"&gt;LinkedIn&lt;/a&gt;, &lt;a href="https://efavdb.github.io/pages/authors.html"&gt;bio-sketch&lt;/a&gt;)&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;The ability to identify faces is a skill that people develop very early in life and can apply almost effortlessly. One reason for this is that our brains are very well adapted for pattern recognition. In contrast, facial recognition can be a somewhat difficult problem for computers. Today, given a full frontal image of a face, computer facial recognition software works well. However, problems can arise given large camera angles, poor lighting, or exaggerated facial expressions: Computers have a ways to go before they catch up with us in this&amp;nbsp;arena.&lt;/p&gt;
&lt;p&gt;Although facial recognition algorithms remain imperfect, the methods that exist now are already quite useful and are being applied by many different companies. Two examples, first up Facebook: When you upload pictures to their website, it will now automatically suggest names for the people in your photos. This application is well-suited for machine learning for two reasons. First, every tagged photo already uploaded to the site provides labeled examples on which to train an algorithm, and second, people often post full face images in decent lighting. A second example is provided by Google&amp;#8217;s Android phone &lt;span class="caps"&gt;OS&lt;/span&gt;, which has a face unlock mode. To get this to work, you first have to train your phone by taking images of your face in different lighting conditions and from different angles. After training, the phone can attempt to recognize you. This is another cool application that also often works&amp;nbsp;well.&lt;/p&gt;
&lt;p&gt;In this post, we&amp;#8217;re going to develop our own basic facial learning algorithm. We&amp;#8217;ll find that it is actually pretty straightforward to set one up that is reasonably accurate. Our post follows and expands upon the tutorial found &lt;a href="http://scikit-learn.org/stable/auto_examples/applications/face_recognition.html"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Loading packages and&amp;nbsp;data&lt;/strong&gt;&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;__future__&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;print_function&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;time&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.cross_validation&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.datasets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;fetch_lfw_people&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.grid_search&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;GridSearchCV&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;classification_report&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;confusion_matrix&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.decomposition&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;RandomizedPCA&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.svm&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;SVC&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The sklearn function &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_lfw_people.html"&gt;fetch_lfw_people&lt;/a&gt;, imported above, will download the data that we need, if not already present in the faces folder. The dataset we are downloading consists of a set of preprocessed images from &lt;a href="http://vis-www.cs.umass.edu/lfw/"&gt;Labeled Faces in the Wild (&lt;span class="caps"&gt;LFW&lt;/span&gt;)&lt;/a&gt;, a database designed for studying unconstrained face recognition. The data set contains more than 13,000 images of faces collected from the web, each labeled with the name of the person pictured. 1680 of the people pictured have two or more distinct photos in the data&amp;nbsp;set.&lt;/p&gt;
&lt;p&gt;In our analysis here, we will impose two&amp;nbsp;conditions.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;First, we will only consider folks that have a minimum of 70 pictures in the data&amp;nbsp;set.&lt;/li&gt;
&lt;li&gt;We will resize the images so that they each have a 0.4 aspect&amp;nbsp;ratio.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;print(&amp;#39;Loading Data&amp;#39;)&lt;/span&gt;
&lt;span class="err"&gt;people = fetch_lfw_people(&lt;/span&gt;
&lt;span class="err"&gt;&amp;#39;./faces&amp;#39;, min_faces_per_person=70, resize=0.4)&lt;/span&gt;
&lt;span class="err"&gt;print(&amp;#39;Done!&amp;#39;)&lt;/span&gt;
&lt;span class="err"&gt;&amp;amp;gt;&amp;amp;gt;&lt;/span&gt;
&lt;span class="err"&gt;Loading Data&lt;/span&gt;
&lt;span class="err"&gt;Done!&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The object &lt;strong&gt;people&lt;/strong&gt; contains the following&amp;nbsp;data.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;people.data: a numpy array with the shape(n_samples, h*w), each row corresponds to a unravelled&amp;nbsp;face.&lt;/li&gt;
&lt;li&gt;people.images: a numpy array with the shape(n_samples, h, w), where each row corresponds to a face. The remaining indices here contain gray-scale values for the pixels of each&amp;nbsp;image.&lt;/li&gt;
&lt;li&gt;people.target: a numpy array with the shape(n_samples), where each row is the label for the&amp;nbsp;face.&lt;/li&gt;
&lt;li&gt;people.target_name: a numpy array with the shape(n_labels), where each row is the name for the&amp;nbsp;label.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For the algorithm we will be using, we don&amp;#8217;t need the relative position data, so we will use the unraveled&amp;nbsp;people.data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;#&lt;/span&gt;&lt;span class="nn"&gt;Find&lt;/span&gt; &lt;span class="nt"&gt;out&lt;/span&gt; &lt;span class="nt"&gt;how&lt;/span&gt; &lt;span class="nt"&gt;many&lt;/span&gt; &lt;span class="nt"&gt;faces&lt;/span&gt; &lt;span class="nt"&gt;we&lt;/span&gt; &lt;span class="nt"&gt;have&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;and&lt;/span&gt;
&lt;span class="p"&gt;#&lt;/span&gt;&lt;span class="nn"&gt;the&lt;/span&gt; &lt;span class="nt"&gt;size&lt;/span&gt; &lt;span class="nt"&gt;of&lt;/span&gt; &lt;span class="nt"&gt;each&lt;/span&gt; &lt;span class="nt"&gt;picture&lt;/span&gt; &lt;span class="nt"&gt;from&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
&lt;span class="nt"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;h&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;people&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;images&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;shape&lt;/span&gt;

&lt;span class="nt"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;people&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;data&lt;/span&gt;
&lt;span class="nt"&gt;n_features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;X&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;shape&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;

&lt;span class="nt"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;people&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;target&lt;/span&gt;
&lt;span class="nt"&gt;target_names&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;people&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;target_names&lt;/span&gt;
&lt;span class="nt"&gt;n_classes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;target_names&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;shape&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;

&lt;span class="nt"&gt;print&lt;/span&gt;&lt;span class="o"&gt;(&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;quot&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;span class="nt"&gt;Total&lt;/span&gt; &lt;span class="nt"&gt;dataset&lt;/span&gt; &lt;span class="nt"&gt;size&lt;/span&gt;&lt;span class="o"&gt;:&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;quot&lt;/span&gt;&lt;span class="o"&gt;;)&lt;/span&gt;
&lt;span class="nt"&gt;print&lt;/span&gt;&lt;span class="o"&gt;(&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;quot&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;span class="nt"&gt;n_images&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="nt"&gt;d&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;quot&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nt"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="nt"&gt;print&lt;/span&gt;&lt;span class="o"&gt;(&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;quot&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;span class="nt"&gt;n_features&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="nt"&gt;d&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;quot&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nt"&gt;n_features&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="nt"&gt;print&lt;/span&gt;&lt;span class="o"&gt;(&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;quot&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;span class="nt"&gt;n_classes&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="nt"&gt;d&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;quot&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nt"&gt;n_classes&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;gt&lt;/span&gt;&lt;span class="o"&gt;;&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;gt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;span class="nt"&gt;Total&lt;/span&gt; &lt;span class="nt"&gt;dataset&lt;/span&gt; &lt;span class="nt"&gt;size&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="nt"&gt;n_images&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nt"&gt;1288&lt;/span&gt;
&lt;span class="nt"&gt;n_features&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nt"&gt;1850&lt;/span&gt;
&lt;span class="nt"&gt;n_classes&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nt"&gt;7&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Looking above we see that our dataset currently has 1288 images. Each image has 1850 pixels, or features. We also have 7 classes, meaning images of 7 different&amp;nbsp;people.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Data segmentation and dimensional&amp;nbsp;reduction&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;At this point we need to segment our data. We are going to use &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html"&gt;train_test_split&lt;/a&gt;, which will take care of splitting our data into random training and testing data sets. Next, we note that we have a lot of features and that there are advantages to having fewer: First, the computational cost is reduced. Second, having fewer features reduces the data’s dimension which can also reduce the complexity of the model and help avoid overfitting. Instead of dropping individual pixels outright, we will carry out a dimensional reduction via a Principle Component Analysis &lt;a href="http://en.wikipedia.org/wiki/Principal_component_analysis"&gt;&lt;span class="caps"&gt;PCA&lt;/span&gt;&lt;/a&gt;. &lt;span class="caps"&gt;PCA&lt;/span&gt; works by attempting to represent the variance in the training data with as few dimensions as possible. So instead of dropping features, as we did in our &lt;a href="http://efavdb.github.io/machine-learning-with-wearable-sensors"&gt;wearable sensor example&lt;/a&gt; analysis, here we will compress features together, and then use only the most important feature combinations. When this is done to images, the features returned by &lt;span class="caps"&gt;PCA&lt;/span&gt; are commonly called eigenfaces (some examples are given&amp;nbsp;below).&lt;/p&gt;
&lt;p&gt;The function we are going to use to carry out our &lt;span class="caps"&gt;PCA&lt;/span&gt; is &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.RandomizedPCA.html"&gt;RandomizedPCA&lt;/a&gt;. We&amp;#8217;ll keep the top 150 eigenfaces, and we&amp;#8217;ll also whiten the data &amp;#8212; ie normalize our new, principal component feature set. The goal of whitening is to make the input less redundant. Whitening is performed by rotating into the coordinate space of the principal components, dividing each dimension by square root of variance in that direction (giving the feature unit variance), and then rotating back to pixel&amp;nbsp;space.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;split&lt;/span&gt; &lt;span class="k"&gt;into&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;training&lt;/span&gt; &lt;span class="k"&gt;and&lt;/span&gt; &lt;span class="n"&gt;testing&lt;/span&gt; &lt;span class="k"&gt;set&lt;/span&gt;
&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;Compute&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;PCA&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;eigenfaces&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;on&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;face&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;
&lt;span class="n"&gt;n_components&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;150&lt;/span&gt;

&lt;span class="n"&gt;pca&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;RandomizedPCA&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;span class="n"&gt;n_components&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_components&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;whiten&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;True&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;eigenfaces&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pca&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;components_&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;n_components&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;X_train_pca&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pca&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;&lt;strong&gt;Visualizing the&amp;nbsp;eigenfaces&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Let&amp;#8217;s now take a moment to examine the dataset&amp;#8217;s principal eigenfaces: the set of images that we will project each example onto to obtain independent features. To do that we will use the following helper function to make life easier &amp;#8212; visual&amp;nbsp;follows.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;helper&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;function&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;to&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;make&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;plots&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;of&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;faces&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;plot_gallery&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;titles&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;n_row&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;n_col&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.8&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;n_col&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;2.4&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;n_row&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots_adjust&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bottom&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;left&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;.01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;right&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;.99&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;top&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;.90&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;hspace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;.35&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ow"&gt;in&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_row&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;n_col&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_row&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;n_col&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cm&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gray&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;titles&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xticks&lt;/span&gt;&lt;span class="p"&gt;(())&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;yticks&lt;/span&gt;&lt;span class="p"&gt;(())&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Plot&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;gallery&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;of&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;most&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;significative&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;eigenfaces&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;eigenface_titles&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;&lt;/span&gt;
&lt;span class="n"&gt;&amp;amp;quot;eigenface %d&amp;amp;quot; % i for i in range(eigenfaces.shape[0&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="err"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="n"&gt;plot_gallery&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;eigenfaces&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;eigenface_titles&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/01/Screen-Shot-2015-01-21-at-12.36.09-PM.png"&gt;&lt;img alt="Screen Shot 2015-01-21 at 12.36.09 PM" src="https://efavdb.com/wp-content/uploads/2015/01/Screen-Shot-2015-01-21-at-12.36.09-PM.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Training a&amp;nbsp;model&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Now that we have reduced the dimensionality of the data it is time to go ahead and train a model. I am going to use the same &lt;span class="caps"&gt;SVM&lt;/span&gt; and GridSearchCV method I explained in my previous &lt;a href="http://efavdb.github.io/machine-learning-with-wearable-sensors"&gt;post&lt;/a&gt;. However, instead of using a linear kernel, as we did last time, I&amp;#8217;ll use instead a &lt;a href="http://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html"&gt;radial basis function (&lt;span class="caps"&gt;RBF&lt;/span&gt;)&lt;/a&gt; kernel. The &lt;span class="caps"&gt;RBF&lt;/span&gt; kernel is a good choice here since we&amp;#8217;d like to have non-linear decision boundaries &amp;#8212; in general, it&amp;#8217;s a reasonable idea to try this out whenever the number of training examples outnumbers the number of features characterizing those examples. The parameter C here acts as a regularization term: Small C values give you smooth decision boundaries, while large C values give complicated boundaries that attempt to fit/accommodate all training data. The gamma parameter defines how far the influence of a single point example extends (the width of the &lt;span class="caps"&gt;RBF&lt;/span&gt;&amp;nbsp;kernel).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;#&lt;/span&gt;&lt;span class="nn"&gt;Train&lt;/span&gt; &lt;span class="nt"&gt;a&lt;/span&gt; &lt;span class="nt"&gt;SVM&lt;/span&gt; &lt;span class="nt"&gt;classification&lt;/span&gt; &lt;span class="nt"&gt;model&lt;/span&gt;

&lt;span class="nt"&gt;print&lt;/span&gt;&lt;span class="o"&gt;(&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;quot&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;span class="nt"&gt;Fitting&lt;/span&gt; &lt;span class="nt"&gt;the&lt;/span&gt; &lt;span class="nt"&gt;classifier&lt;/span&gt; &lt;span class="nt"&gt;to&lt;/span&gt; &lt;span class="nt"&gt;the&lt;/span&gt; &lt;span class="nt"&gt;training&lt;/span&gt; &lt;span class="nt"&gt;set&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;quot&lt;/span&gt;&lt;span class="o"&gt;;)&lt;/span&gt;
&lt;span class="nt"&gt;t0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;time&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt;
&lt;span class="nt"&gt;param_grid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;C&amp;#39;:&lt;/span&gt; &lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="nx"&gt;e3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="nx"&gt;e3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="nx"&gt;e4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="nx"&gt;e4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="nx"&gt;e5&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="err"&gt;,&lt;/span&gt;
&lt;span class="err"&gt;&amp;#39;gamma&amp;#39;:&lt;/span&gt; &lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0001&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.0005&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.001&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.005&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="err"&gt;,&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="nt"&gt;clf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;GridSearchCV&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;
&lt;span class="nt"&gt;SVC&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;kernel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;rbf&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;class_weight&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;auto&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;),&lt;/span&gt; &lt;span class="nt"&gt;param_grid&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="nt"&gt;clf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;clf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;fit&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;X_train_pca&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;y_train&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="nt"&gt;print&lt;/span&gt;&lt;span class="o"&gt;(&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;quot&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;span class="nt"&gt;done&lt;/span&gt; &lt;span class="nt"&gt;in&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="nt"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;3fs&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;quot&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;time&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="nt"&gt;-&lt;/span&gt; &lt;span class="nt"&gt;t0&lt;/span&gt;&lt;span class="o"&gt;))&lt;/span&gt;
&lt;span class="nt"&gt;print&lt;/span&gt;&lt;span class="o"&gt;(&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;quot&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;span class="nt"&gt;Best&lt;/span&gt; &lt;span class="nt"&gt;estimator&lt;/span&gt; &lt;span class="nt"&gt;found&lt;/span&gt; &lt;span class="nt"&gt;by&lt;/span&gt; &lt;span class="nt"&gt;grid&lt;/span&gt; &lt;span class="nt"&gt;search&lt;/span&gt;&lt;span class="o"&gt;:&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;quot&lt;/span&gt;&lt;span class="o"&gt;;)&lt;/span&gt;
&lt;span class="nt"&gt;print&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;clf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;best_estimator_&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;

&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;gt&lt;/span&gt;&lt;span class="o"&gt;;&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;gt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;span class="nt"&gt;Fitting&lt;/span&gt; &lt;span class="nt"&gt;the&lt;/span&gt; &lt;span class="nt"&gt;classifier&lt;/span&gt; &lt;span class="nt"&gt;to&lt;/span&gt; &lt;span class="nt"&gt;the&lt;/span&gt; &lt;span class="nt"&gt;training&lt;/span&gt; &lt;span class="nt"&gt;set&lt;/span&gt;
&lt;span class="nt"&gt;done&lt;/span&gt; &lt;span class="nt"&gt;in&lt;/span&gt; &lt;span class="nt"&gt;16&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;056s&lt;/span&gt;
&lt;span class="nt"&gt;Best&lt;/span&gt; &lt;span class="nt"&gt;estimator&lt;/span&gt; &lt;span class="nt"&gt;found&lt;/span&gt; &lt;span class="nt"&gt;by&lt;/span&gt; &lt;span class="nt"&gt;grid&lt;/span&gt; &lt;span class="nt"&gt;search&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="nt"&gt;SVC&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;C&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nt"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;0&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;cache_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nt"&gt;200&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;class_weight&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;auto&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;coef0&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nt"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;0&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;
&lt;span class="nt"&gt;degree&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nt"&gt;3&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;gamma&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nt"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;001&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;kernel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;rbf&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;max_iter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nt"&gt;-1&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;
&lt;span class="nt"&gt;probability&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nt"&gt;False&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nt"&gt;None&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;shrinking&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nt"&gt;True&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;
&lt;span class="nt"&gt;tol&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nt"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;001&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;verbose&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nt"&gt;False&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;&lt;strong&gt;Model&amp;nbsp;validation&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;That&amp;#8217;s it for training! Next we&amp;#8217;ll validate our model on the testing data set. Below, we first use our &lt;span class="caps"&gt;PCA&lt;/span&gt; model to transform the testing data into our current feature space. Then, we apply our model to make predictions on this set. To get a feel for how well the model is doing, we print a &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html"&gt;classification_report&lt;/a&gt; and a &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html"&gt;confusion matrix&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="n"&gt;Quantitative&lt;/span&gt; &lt;span class="n"&gt;evaluation&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="n"&gt;quality&lt;/span&gt; &lt;span class="k"&gt;on&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt; &lt;span class="k"&gt;set&lt;/span&gt;
&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="n"&gt;Validate&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="k"&gt;data&lt;/span&gt;
&lt;span class="n"&gt;X_test_pca&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pca&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test_pca&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;classification_report&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target_names&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;target_names&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Confusion Matrix&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="n"&gt;Make&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="k"&gt;data&lt;/span&gt; &lt;span class="n"&gt;frame&lt;/span&gt; &lt;span class="n"&gt;so&lt;/span&gt; &lt;span class="n"&gt;we&lt;/span&gt; &lt;span class="n"&gt;can&lt;/span&gt; &lt;span class="n"&gt;have&lt;/span&gt; &lt;span class="k"&gt;some&lt;/span&gt; &lt;span class="n"&gt;nice&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;
&lt;span class="n"&gt;cm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;confusion_matrix&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_classes&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;target_names&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;target_names&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;gt&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;gt&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;precision&lt;/span&gt; &lt;span class="n"&gt;recall&lt;/span&gt; &lt;span class="n"&gt;f1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt; &lt;span class="n"&gt;support&lt;/span&gt;

&lt;span class="n"&gt;Ariel&lt;/span&gt; &lt;span class="n"&gt;Sharon&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;81&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;85&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;83&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;
&lt;span class="n"&gt;Colin&lt;/span&gt; &lt;span class="n"&gt;Powell&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;82&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;78&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;80&lt;/span&gt; &lt;span class="mi"&gt;54&lt;/span&gt;
&lt;span class="n"&gt;Donald&lt;/span&gt; &lt;span class="n"&gt;Rumsfeld&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;78&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;67&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;72&lt;/span&gt; &lt;span class="mi"&gt;27&lt;/span&gt;
&lt;span class="n"&gt;George&lt;/span&gt; &lt;span class="n"&gt;W&lt;/span&gt; &lt;span class="n"&gt;Bush&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;87&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;95&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;91&lt;/span&gt; &lt;span class="mi"&gt;139&lt;/span&gt;
&lt;span class="n"&gt;Gerhard&lt;/span&gt; &lt;span class="n"&gt;Schroeder&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;86&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;73&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;79&lt;/span&gt; &lt;span class="mi"&gt;26&lt;/span&gt;
&lt;span class="n"&gt;Hugo&lt;/span&gt; &lt;span class="n"&gt;Chavez&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;00&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;75&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;86&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;
&lt;span class="n"&gt;Tony&lt;/span&gt; &lt;span class="n"&gt;Blair&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;84&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;89&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;86&lt;/span&gt; &lt;span class="mi"&gt;36&lt;/span&gt;

&lt;span class="k"&gt;avg&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;85&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;85&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;85&lt;/span&gt; &lt;span class="mi"&gt;322&lt;/span&gt;

&lt;span class="n"&gt;Confusion&lt;/span&gt; &lt;span class="n"&gt;Matrix&lt;/span&gt;
&lt;span class="n"&gt;Ariel&lt;/span&gt; &lt;span class="n"&gt;Sharon&lt;/span&gt; &lt;span class="n"&gt;Colin&lt;/span&gt; &lt;span class="n"&gt;Powell&lt;/span&gt; &lt;span class="n"&gt;Donald&lt;/span&gt; &lt;span class="n"&gt;Rumsfeld&lt;/span&gt; &lt;span class="n"&gt;George&lt;/span&gt; &lt;span class="n"&gt;W&lt;/span&gt; &lt;span class="n"&gt;Bush&lt;/span&gt; &lt;span class="err"&gt;\&lt;/span&gt;
&lt;span class="n"&gt;Ariel&lt;/span&gt; &lt;span class="n"&gt;Sharon&lt;/span&gt; &lt;span class="mi"&gt;17&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="n"&gt;Colin&lt;/span&gt; &lt;span class="n"&gt;Powell&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;42&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;
&lt;span class="n"&gt;Donald&lt;/span&gt; &lt;span class="n"&gt;Rumsfeld&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;18&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
&lt;span class="n"&gt;George&lt;/span&gt; &lt;span class="n"&gt;W&lt;/span&gt; &lt;span class="n"&gt;Bush&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="mi"&gt;132&lt;/span&gt;
&lt;span class="n"&gt;Gerhard&lt;/span&gt; &lt;span class="n"&gt;Schroeder&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
&lt;span class="n"&gt;Hugo&lt;/span&gt; &lt;span class="n"&gt;Chavez&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;
&lt;span class="n"&gt;Tony&lt;/span&gt; &lt;span class="n"&gt;Blair&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;

&lt;span class="n"&gt;Gerhard&lt;/span&gt; &lt;span class="n"&gt;Schroeder&lt;/span&gt; &lt;span class="n"&gt;Hugo&lt;/span&gt; &lt;span class="n"&gt;Chavez&lt;/span&gt; &lt;span class="n"&gt;Tony&lt;/span&gt; &lt;span class="n"&gt;Blair&lt;/span&gt;
&lt;span class="n"&gt;Ariel&lt;/span&gt; &lt;span class="n"&gt;Sharon&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="n"&gt;Colin&lt;/span&gt; &lt;span class="n"&gt;Powell&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="n"&gt;Donald&lt;/span&gt; &lt;span class="n"&gt;Rumsfeld&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;George&lt;/span&gt; &lt;span class="n"&gt;W&lt;/span&gt; &lt;span class="n"&gt;Bush&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;Gerhard&lt;/span&gt; &lt;span class="n"&gt;Schroeder&lt;/span&gt; &lt;span class="mi"&gt;19&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="n"&gt;Hugo&lt;/span&gt; &lt;span class="n"&gt;Chavez&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="n"&gt;Tony&lt;/span&gt; &lt;span class="n"&gt;Blair&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;As a quick reminder, lets define what the terms above&amp;nbsp;are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;precision is the ratio Tp / (Tp + Fp) where Tp is the number of true positives and Fp the number of false&amp;nbsp;positives.&lt;/li&gt;
&lt;li&gt;recall is the ration of Tp / (Tp + Fn) where Fn is the number of false&amp;nbsp;negatives.&lt;/li&gt;
&lt;li&gt;f1-score is (precision * recall) / (precision +&amp;nbsp;recall)&lt;/li&gt;
&lt;li&gt;support is the total number of occurrences of each&amp;nbsp;face.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In our second table here, we have printed a confusion matrix, which provides a nice summary visualization of our results: Each row is the actual class, and the columns are the predicted class. For example, in row 1 there are 17 correct identifications of Arial Sharon, and 5 wrong ones. Using our previously defined helper plotting function, we show some examples of predicted vs true names below. Our simple algorithm&amp;#8217;s accuracy is imperfect, yet&amp;nbsp;satisfying!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;#Plot&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;on&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;portion&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;of&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;set&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;target_names&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;pred_name&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;target_names&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;y_pred[i&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="err"&gt;]&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rsplit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;-1&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;true_name&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;target_names&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;y_test[i&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="err"&gt;]&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rsplit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;-1&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;predicted: %s\ntrue: %s&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pred_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;true_name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="n"&gt;prediction_titles&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;title(y_pred, y_test, target_names, i)&lt;/span&gt;
&lt;span class="n"&gt;for i in range(y_pred.shape[0&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="err"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="n"&gt;plot_gallery&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;prediction_titles&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/01/download.png"&gt;&lt;img alt="download" src="https://efavdb.com/wp-content/uploads/2015/01/download.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Discussion&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;85% average accuracy shows that &lt;span class="caps"&gt;PCA&lt;/span&gt; (Eigenface analysis) can provide accurate face recognition results, given just a modest amount of training data. There are pros and cons to eigenfaces&amp;nbsp;however:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pros&lt;ol&gt;
&lt;li&gt;Training can be&amp;nbsp;automated.&lt;/li&gt;
&lt;li&gt;Once the the eigenfaces are calculated, face recognition can be performed in real&amp;nbsp;time.&lt;/li&gt;
&lt;li&gt;Eigenfaces can handle large&amp;nbsp;databases.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Cons&lt;ol&gt;
&lt;li&gt;Sensitive to lighting&amp;nbsp;conditions.&lt;/li&gt;
&lt;li&gt;Expression changes are not handled&amp;nbsp;well.&lt;/li&gt;
&lt;li&gt;Has trouble when the face angle&amp;nbsp;changes.&lt;/li&gt;
&lt;li&gt;Difficult to interpret eigenfaces: Eg, one can&amp;#8217;t easily read off from these eye separation distance,&amp;nbsp;etc.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are more advanced facial recognition methods that take advantage of features special to faces. One example is provided by the &lt;a href="http://en.wikipedia.org/wiki/Active_appearance_model"&gt;Active Appearance Model (&lt;span class="caps"&gt;AAM&lt;/span&gt;)&lt;/a&gt;, which finds facial features (nose, mouth, etc.), and then identifies relationships between these to carry out identifications. Whatever the approach, the overall methodology is the same for all facial recognition&amp;nbsp;algorithms:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Take a labeled set of&amp;nbsp;faces.&lt;/li&gt;
&lt;li&gt;Extract features from those faces using some method of choice (eg&amp;nbsp;eigenfaces).&lt;/li&gt;
&lt;li&gt;Train a machine learning model on those&amp;nbsp;features.&lt;/li&gt;
&lt;li&gt;Extract features from a new face, and predict the&amp;nbsp;identity.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The story doesn&amp;#8217;t end with finding faces in photos. Facial recognition is just a subset of machine vision, which is currently being applied widely in industry. For example, Intel and other semiconductor manufactures use machine vision to detect defects in the chips being produced &amp;#8212; one application where by-hand (human) analysis is not possible and computers have the upper&amp;nbsp;hand.&lt;/p&gt;</content><category term="Case studies"></category><category term="machine learning"></category></entry><entry><title>Quick tutorial on MySQL</title><link href="https://efavdb.com/quick-tutorial-on-mysql" rel="alternate"></link><published>2015-01-16T16:07:00-08:00</published><updated>2015-01-16T16:07:00-08:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2015-01-16:/quick-tutorial-on-mysql</id><summary type="html">&lt;p&gt;Here, we give a quick (&amp;lt; 30 mins) introduction to the open source database software package MySQL. The post is intended to be useful for folks totally new to the program, as well as for those who find that they often need reminders on its basic syntax (that is, people like …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Here, we give a quick (&amp;lt; 30 mins) introduction to the open source database software package MySQL. The post is intended to be useful for folks totally new to the program, as well as for those who find that they often need reminders on its basic syntax (that is, people like&amp;nbsp;us).&lt;/p&gt;
&lt;p&gt;&lt;a href="http://twitter.com/efavdb"&gt;Follow&amp;nbsp;@efavdb&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Follow us on twitter for new submission&amp;nbsp;alerts!&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Getting&amp;nbsp;started&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;MySQL is a database software package that allows users to quickly access subsets of data contained within tables, and also to carry out simple operations on this data. The software is quite powerful, but it can be surprisingly unintuitive for beginners. The best way to get the hang of it is to play around with it a bit. This post provides a set of commands that should help you get a feel for how it works. If you&amp;#8217;re a beginner reading this, we suggest &lt;a href="http://dev.mysql.com/doc/refman/5.5/en/installing.html"&gt;installing&lt;/a&gt; it on your personal computer or server, and following along by trying each of the commands we go through here. Once it&amp;#8217;s installed and you have its server running, you can often access MySQL from the command line by&amp;nbsp;typing&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;mysql
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;On a mac, you may need to use the following&amp;nbsp;though&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;/usr/local/mysql/bin/mysql -uroot
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Once mysql is loaded, you can see what databases are available by&amp;nbsp;typing&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;SHOW&lt;/span&gt; &lt;span class="n"&gt;DATABASES&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Notice that a semi-colon is used to terminate commands: In general, these can extend across multiple lines and the semi-colon tells the program where the command stops. Also, MySQL is case-insensitive, but it is considered good form to have all command calls capitalized for easier reading. If no databases yet exist, you can create one as&amp;nbsp;follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;CREATE&lt;/span&gt; &lt;span class="k"&gt;DATABASE&lt;/span&gt; &lt;span class="n"&gt;animalDB&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here, &lt;code&gt;animalDB&lt;/code&gt; is the name of the database. From the list of available databases,
you can select one of interest with the &lt;code&gt;USE&lt;/code&gt; command. For&amp;nbsp;example,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;USE&lt;/span&gt; &lt;span class="n"&gt;animalDB&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Each database can contain many tables. To see the tables contained in a database, use the &lt;span class="caps"&gt;SHOW&lt;/span&gt;&amp;nbsp;command,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;SHOW&lt;/span&gt; &lt;span class="n"&gt;tables&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;&lt;strong&gt;Table creation and&amp;nbsp;alteration&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;&lt;span class="caps"&gt;SQL&lt;/span&gt; tables have a name and a set of rows and columns. The columns have types that are defined upon table creation (&lt;code&gt;INT, BIGINT, FLOAT, DOUBLE, CHAR, VARCHAR&lt;/code&gt;, etc.). The rows correspond to individual table entries. To illustrate, we’ll now create a table called “MyPets”, with a column for pet name, species, and age. This is done with the&amp;nbsp;command&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;CREATE&lt;/span&gt; &lt;span class="k"&gt;TABLE&lt;/span&gt; &lt;span class="n"&gt;MyPets&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="nb"&gt;VARCHAR&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;species&lt;/span&gt; &lt;span class="nb"&gt;VARCHAR&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;age&lt;/span&gt; &lt;span class="nb"&gt;INT&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here, we are using the &lt;code&gt;VARCHAR&lt;/code&gt; type for our two string columns. The argument supplied allows us to use strings up to length &lt;code&gt;10&lt;/code&gt; for these entries. We could also have used the &lt;code&gt;CHAR(10)&lt;/code&gt; type here, but that would result in trailing spaces following names shorter than &lt;code&gt;10&lt;/code&gt; characters. We now insert some entries using the &lt;code&gt;INSERT&lt;/code&gt; command,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;INSERT&lt;/span&gt; &lt;span class="k"&gt;INTO&lt;/span&gt; &lt;span class="n"&gt;MyPets&lt;/span&gt; &lt;span class="k"&gt;VALUES&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;Bottles&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;Dog&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="k"&gt;INSERT&lt;/span&gt; &lt;span class="k"&gt;INTO&lt;/span&gt; &lt;span class="n"&gt;MyPets&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;species&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;VALUES&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;Mac&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;Dog&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="k"&gt;INSERT&lt;/span&gt; &lt;span class="k"&gt;INTO&lt;/span&gt; &lt;span class="n"&gt;MyPets&lt;/span&gt; &lt;span class="k"&gt;VALUES&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;Hector&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;Cat&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here, we’ve illustrated two different methods to do insertion. In the first and third lines, we have values for all columns. However, in the second, no age is supplied, so we have to specify which columns the values we are supplying correspond to. The age column for this entry will read &lt;code&gt;NULL&lt;/code&gt;, since no value was provided for it. To view the table, we write – with &lt;code&gt;SELECT&lt;/code&gt; and &lt;code&gt;∗&lt;/code&gt; meaning &amp;#8220;retrieve&amp;#8221; and “all”, respectively&amp;nbsp;–&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="n"&gt;MyPets&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
    &lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="c1"&gt;---------+---------+------+&lt;/span&gt;
    &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;species&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;age&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt;
    &lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="c1"&gt;---------+---------+------+&lt;/span&gt;
    &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Bottles&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Dog&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt;
    &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Mac&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Dog&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="k"&gt;NULL&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt;
    &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Hector&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Cat&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt;
    &lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="c1"&gt;---------+---------+------+&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Additional &lt;code&gt;SELECT&lt;/code&gt; queries are given below that illustrate how one can select and operate on subsets of the columns and rows. To add an age for Mac, we use the &lt;code&gt;UPDATE, SET&lt;/code&gt;, and &lt;code&gt;WHERE&lt;/code&gt; commands,&amp;nbsp;writing&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;UPDATE&lt;/span&gt; &lt;span class="n"&gt;MyPets&lt;/span&gt;
    &lt;span class="k"&gt;SET&lt;/span&gt; &lt;span class="n"&gt;age&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;
    &lt;span class="k"&gt;WHERE&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;Mac&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;To see that this and the other commands that follow work as expected, try running the &lt;code&gt;SELECT&lt;/code&gt; command above after each application. To delete a row from the table, we use the &lt;span class="caps"&gt;DELETE&lt;/span&gt;&amp;nbsp;command,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;DELETE&lt;/span&gt; &lt;span class="k"&gt;from&lt;/span&gt; &lt;span class="n"&gt;MyPets&lt;/span&gt;
    &lt;span class="k"&gt;WHERE&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;Hector&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;It is also possible to add or subtract columns from a table. To add a column, we use
the &lt;code&gt;ALTER&lt;/code&gt; and &lt;code&gt;ADD COLUMN&lt;/code&gt; commands,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;ALTER&lt;/span&gt; &lt;span class="k"&gt;TABLE&lt;/span&gt; &lt;span class="n"&gt;MyPets&lt;/span&gt;
    &lt;span class="k"&gt;ADD&lt;/span&gt; &lt;span class="k"&gt;COLUMN&lt;/span&gt; &lt;span class="n"&gt;litters&lt;/span&gt; &lt;span class="nb"&gt;INT&lt;/span&gt;
    &lt;span class="k"&gt;DEFAULT&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The last line here is not necessary. Without it, the command would create the column and set each row’s value there to &lt;code&gt;NULL&lt;/code&gt;. To delete a column, we use the &lt;code&gt;DROP&lt;/code&gt; command,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;ALTER&lt;/span&gt; &lt;span class="k"&gt;TABLE&lt;/span&gt; &lt;span class="n"&gt;MyPets&lt;/span&gt;
    &lt;span class="k"&gt;DROP&lt;/span&gt; &lt;span class="n"&gt;litters&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;em&gt;Caveat:&lt;/em&gt; While row addition and removal can always be carried out quickly, addition and removal of columns scales linearly with table size. The reason is that these operations are generally carried out by copying the original table into a second table having the desired new structure. For this reason, it is generally a good idea to plan ahead and make sure any new table has all the columns you foresee might be&amp;nbsp;needed.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;&lt;span class="caps"&gt;SELECTION&lt;/span&gt; queries &amp;#8212; learn by&amp;nbsp;example&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Example conditional&amp;nbsp;commands:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;What is the name and age each of my&amp;nbsp;pets?&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;age&lt;/span&gt; &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="n"&gt;mypets&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ol&gt;
&lt;li&gt;How many dogs have I&amp;nbsp;got?&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="k"&gt;COUNT&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="n"&gt;mypets&lt;/span&gt;
    &lt;span class="k"&gt;WHERE&lt;/span&gt; &lt;span class="n"&gt;species&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;dog&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ol&gt;
&lt;li&gt;Show me just the first two pets in my&amp;nbsp;table.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;
    &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="n"&gt;mypets&lt;/span&gt;
    &lt;span class="k"&gt;LIMIT&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ol&gt;
&lt;li&gt;Show me my pets in age-descending&amp;nbsp;order.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;
    &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="n"&gt;MyPets&lt;/span&gt;
    &lt;span class="k"&gt;ORDER&lt;/span&gt; &lt;span class="k"&gt;BY&lt;/span&gt; &lt;span class="n"&gt;age&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ol&gt;
&lt;li&gt;Which of my dogs are under 4 years&amp;nbsp;old?&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;
    &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="n"&gt;MyPets&lt;/span&gt;
    &lt;span class="k"&gt;WHERE&lt;/span&gt; &lt;span class="n"&gt;age&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;
      &lt;span class="k"&gt;AND&lt;/span&gt; &lt;span class="n"&gt;species&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;dog&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ol&gt;
&lt;li&gt;Which animals have names that start with the letter&amp;nbsp;“M”?&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;
    &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="n"&gt;mypets&lt;/span&gt;
    &lt;span class="k"&gt;WHERE&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="k"&gt;LIKE&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;M%&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ol&gt;
&lt;li&gt;Which animals have the letter “E” somewhere in their&amp;nbsp;name?&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;
    &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="n"&gt;mypets&lt;/span&gt;
    &lt;span class="k"&gt;WHERE&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="k"&gt;LIKE&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;%E%&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Example &lt;span class="caps"&gt;GROUP&lt;/span&gt; &lt;span class="caps"&gt;BY&lt;/span&gt; commands (see also &lt;code&gt;MIN, MAX, SUM, STD&lt;/code&gt;,&amp;nbsp;etc.):&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;How many pets have I got of each&amp;nbsp;species?&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="n"&gt;species&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;count&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="n"&gt;mypets&lt;/span&gt;
    &lt;span class="k"&gt;GROUP&lt;/span&gt; &lt;span class="k"&gt;BY&lt;/span&gt; &lt;span class="n"&gt;species&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ol&gt;
&lt;li&gt;What is the average age of my pets, grouped by&amp;nbsp;species?&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="n"&gt;species&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;AVG&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;age&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="n"&gt;mypets&lt;/span&gt;
    &lt;span class="k"&gt;GROUP&lt;/span&gt; &lt;span class="k"&gt;BY&lt;/span&gt; &lt;span class="n"&gt;species&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;&lt;strong&gt;Actions on multiple&amp;nbsp;tables&lt;/strong&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;To solidify what we&amp;#8217;ve learned above, try to now create a second table, called &lt;code&gt;PetDetails&lt;/code&gt;, like that above but with different age and species values. You can add other columns to it if you like. Once that&amp;#8217;s done, apply the &lt;code&gt;SHOW TABLES&lt;/code&gt; command to see that both tables are available. Next, learn to copy specific values from this new table into the first one, using commands&amp;nbsp;like&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="k"&gt;UPDATE&lt;/span&gt; &lt;span class="n"&gt;MyPets&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;PetDetails&lt;/span&gt;
      &lt;span class="k"&gt;SET&lt;/span&gt; &lt;span class="n"&gt;MyPets&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;age&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;PetDetails&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;age&lt;/span&gt;
      &lt;span class="k"&gt;WHERE&lt;/span&gt; &lt;span class="n"&gt;MyPets&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;PetDetails&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;Note the use of the period here to specify from which table a certain column is to be selected from.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ol&gt;
&lt;li&gt;&lt;em&gt;The&lt;/em&gt; &lt;code&gt;JOIN/ON&lt;/code&gt; &lt;em&gt;commands&lt;/em&gt;. The &lt;code&gt;JOIN&lt;/code&gt; command essentially creates something like a flattened outer product of two tables: If there are &lt;span class="math"&gt;\(n\)&lt;/span&gt; entries in the first table and &lt;span class="math"&gt;\(m\)&lt;/span&gt; in the second, the command returns a table with &lt;span class="math"&gt;\(n \times m\)&lt;/span&gt; rows. There is one row for each possible pairing, one entry taken from the first table and one from the second. All columns from both tables are then included in the new table. The &lt;span class="caps"&gt;ON&lt;/span&gt; command can be used to specify conditions on which pairs are to be included in the combined table. To illustrate, let’s define a new table of pet-trick&amp;nbsp;pairs&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="k"&gt;CREATE&lt;/span&gt; &lt;span class="k"&gt;TABLE&lt;/span&gt; &lt;span class="n"&gt;PetTricks&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="nb"&gt;VARCHAR&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;trick&lt;/span&gt; &lt;span class="nb"&gt;VARCHAR&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
    &lt;span class="k"&gt;INSERT&lt;/span&gt; &lt;span class="k"&gt;INTO&lt;/span&gt; &lt;span class="n"&gt;PetTricks&lt;/span&gt; &lt;span class="k"&gt;VALUES&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;Bottles&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;Shake&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="k"&gt;INSERT&lt;/span&gt; &lt;span class="k"&gt;INTO&lt;/span&gt; &lt;span class="n"&gt;PetTricks&lt;/span&gt; &lt;span class="k"&gt;VALUES&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;Bottles&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;Play dead&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="k"&gt;INSERT&lt;/span&gt; &lt;span class="k"&gt;INTO&lt;/span&gt; &lt;span class="n"&gt;PetTricks&lt;/span&gt; &lt;span class="k"&gt;VALUES&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;Mac&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;Shake&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="k"&gt;INSERT&lt;/span&gt; &lt;span class="k"&gt;INTO&lt;/span&gt; &lt;span class="n"&gt;PetTricks&lt;/span&gt; &lt;span class="k"&gt;VALUES&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;Dogbert&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;Consulting&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;With the following, we get the number of tricks each of my pets can&amp;nbsp;do&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="n"&gt;MyPets&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;count&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;num_tricks&lt;/span&gt;
    &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;MyPets&lt;/span&gt; &lt;span class="k"&gt;JOIN&lt;/span&gt; &lt;span class="n"&gt;PetTricks&lt;/span&gt;
    &lt;span class="k"&gt;ON&lt;/span&gt; &lt;span class="n"&gt;MyPets&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;PetTricks&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;GROUP&lt;/span&gt; &lt;span class="k"&gt;BY&lt;/span&gt; &lt;span class="n"&gt;MyPets&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here, we see for the first time that it is possible to select values from a table created &amp;#8220;on the fly&amp;#8221; (the table in parentheses, which you can print using the &lt;code&gt;SELECT&lt;/code&gt; command). We also see for the first time the concept of aliasing, applied through use of the &lt;code&gt;AS&lt;/code&gt; command.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Our last &amp;#8212; and most complicated &amp;#8212; example combines many of the ideas discussed above. If you can get to the point where you can replicate commands like this one, you&amp;#8217;ll be pretty much set to construct your own complex &lt;span class="caps"&gt;SQL&lt;/span&gt; queries: Let’s add a trick count to our first table, and then fill it in by querying the PetTricks&amp;nbsp;table.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="k"&gt;ALTER&lt;/span&gt; &lt;span class="k"&gt;TABLE&lt;/span&gt; &lt;span class="n"&gt;MyPets&lt;/span&gt;
      &lt;span class="k"&gt;ADD&lt;/span&gt; &lt;span class="k"&gt;COLUMN&lt;/span&gt; &lt;span class="n"&gt;num_tricks&lt;/span&gt; &lt;span class="nb"&gt;INT&lt;/span&gt;
      &lt;span class="k"&gt;DEFAULT&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

    &lt;span class="k"&gt;UPDATE&lt;/span&gt; &lt;span class="n"&gt;MyPets&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;T1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;Select&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;count&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;tot&lt;/span&gt;
        &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="n"&gt;PetTricks&lt;/span&gt;
        &lt;span class="k"&gt;GROUP&lt;/span&gt; &lt;span class="k"&gt;BY&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;T2&lt;/span&gt;
        &lt;span class="k"&gt;SET&lt;/span&gt; &lt;span class="n"&gt;T1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;num_tricks&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;T2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tot&lt;/span&gt;
        &lt;span class="k"&gt;WHERE&lt;/span&gt; &lt;span class="n"&gt;T1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;T2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

    &lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="n"&gt;MyPets&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
        &lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="c1"&gt;---------+---------+------+------------+&lt;/span&gt;
        &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;species&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;age&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;num_tricks&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt;
        &lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="c1"&gt;---------+---------+------+------------+&lt;/span&gt;
        &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Bottles&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Dog&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt;
        &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Mac&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Dog&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt;
        &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Hector&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Cat&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt;
        &lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="c1"&gt;---------+---------+------+------------+&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;&lt;strong&gt;Other&amp;nbsp;tips&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Lastly, a few one-off tips that can be very&amp;nbsp;helpful.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Creating a new table similar another.&lt;/em&gt; The following command can come in handy when you’re dealing with tables that have many&amp;nbsp;columns:&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="k"&gt;CREATE&lt;/span&gt; &lt;span class="k"&gt;TABLE&lt;/span&gt; &lt;span class="n"&gt;TNew&lt;/span&gt; &lt;span class="k"&gt;LIKE&lt;/span&gt; &lt;span class="n"&gt;T1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here, the command creates &lt;code&gt;TNew&lt;/code&gt;, a new table with column names and types like those of &lt;code&gt;T1&lt;/code&gt;. The entries of &lt;code&gt;T1&lt;/code&gt; are not copied over. If you want to copy some of them over, you can do that with a command&amp;nbsp;like&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="k"&gt;INSERT&lt;/span&gt; &lt;span class="k"&gt;INTO&lt;/span&gt; &lt;span class="n"&gt;TNew&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="n"&gt;T1&lt;/span&gt; &lt;span class="k"&gt;WHERE&lt;/span&gt; &lt;span class="p"&gt;...);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Saving to a text file.&lt;/em&gt; Printing a table to a text file can sometimes be useful. To proceed, you first need to create a directory that MySQL can have write access to. On a mac, you can accomplish this from the terminal with the&amp;nbsp;following&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="nb"&gt;cd&lt;/span&gt; /usr/local
    mkdir MySQLOutput
    sudo chmod -R &lt;span class="m"&gt;777&lt;/span&gt; MySQLOutput
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This creates the directory &lt;code&gt;/usr/local/MYSQLOutput&lt;/code&gt; with global read, write, and execute permissions. With this setup, we can write to a file from within MySQL with a command&amp;nbsp;like&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="n"&gt;MyPets&lt;/span&gt;
        &lt;span class="k"&gt;INTO&lt;/span&gt; &lt;span class="n"&gt;OUTFILE&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;/usr/local/MySQLOutput/test.txt&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Scripts.&lt;/em&gt; For complicated queries, or queries that you would like to be able to run multiple times, it is useful to employ scripts. These can then be executed from within mysql using the &lt;code&gt;SOURCE&lt;/code&gt; command. To illustrate, suppose we have a text file called &lt;code&gt;/usr/local/MySQLOutput/test.txt&lt;/code&gt; within which we have written the&amp;nbsp;commands&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="k"&gt;CREATE&lt;/span&gt; &lt;span class="k"&gt;table&lt;/span&gt; &lt;span class="n"&gt;Bad_dogs&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;DogID&lt;/span&gt; &lt;span class="nb"&gt;BIGINT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Barks&lt;/span&gt; &lt;span class="nb"&gt;INT&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="k"&gt;INSERT&lt;/span&gt; &lt;span class="k"&gt;INTO&lt;/span&gt; &lt;span class="n"&gt;Bad_dogs&lt;/span&gt; &lt;span class="k"&gt;VALUES&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1234567890&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;666&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We can run this from within MySQL using the&amp;nbsp;command&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="k"&gt;SOURCE&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;usr&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="k"&gt;local&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;MySQLOutput&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;txt&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This creates the table and inserts the example&amp;nbsp;entry.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Indexing.&lt;/em&gt; By creating an index, one can speed up &lt;code&gt;SELECT&lt;/code&gt; calls on large tables. You can think of an index heuristically as a second table having two columns: The first is a sorted version of one of the original table’s columns, and the second column is a pointer to the memory block where its corresponding entry sits (actually, an index usually sits in a B-tree, a structure similar to a binary-search tree). Entries can be quickly accessed via the index, generally in logarithmic time. To add a key to our first table,&amp;nbsp;write&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="k"&gt;ALTER&lt;/span&gt; &lt;span class="k"&gt;TABLE&lt;/span&gt; &lt;span class="n"&gt;MyPets&lt;/span&gt;
        &lt;span class="k"&gt;ADD&lt;/span&gt; &lt;span class="k"&gt;PRIMARY&lt;/span&gt; &lt;span class="k"&gt;KEY&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This selects the name column as our index, which will speed up all &lt;code&gt;SELECT&lt;/code&gt; calls seeking entries with &lt;code&gt;name&lt;/code&gt; values satisfying some condition &amp;#8212; specified using &lt;code&gt;WHERE name = ...&lt;/code&gt;. You can actually index as many columns of a table as you like. However, this takes up disk space, and so should be avoided when the extra indexes are not useful. It is also possible to specify that you want one or more columns to be keys upon table&amp;nbsp;creation.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Further study.&lt;/em&gt; At this point, we have covered most of the basics, but only the basics. If you get stumped by any tricky queries moving forward, we suggest visiting both &lt;a href="http://stackoverflow.com/"&gt;stackoverflow.com&lt;/a&gt; &amp;#8212; which has tons of interesting discussions on the topic &amp;#8212; and the &lt;a href="http://dev.mysql.com/doc/"&gt;MySQL documentation page&lt;/a&gt;, which goes over most everything and includes a tutorial in chapter 3 similar to this one. Both are excellent&amp;nbsp;resources.&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Tools"></category></entry><entry><title>NBA week 10 summary, week 11 predictions</title><link href="https://efavdb.com/nba-week-10-summary-week-11-predictions" rel="alternate"></link><published>2015-01-16T14:42:00-08:00</published><updated>2015-01-16T14:42:00-08:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2015-01-16:/nba-week-10-summary-week-11-predictions</id><content type="html">&lt;p&gt;Fairly good accuracy this past week &amp;#8212; 33/49, or 67.8% overall. The breakdown by point spread is given below, and the new predictions are&amp;nbsp;up!&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Point spread&lt;/th&gt;
&lt;th&gt;# games&lt;/th&gt;
&lt;th&gt;Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&amp;lt; 5&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;44%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5-9&lt;/td&gt;
&lt;td&gt;17&lt;/td&gt;
&lt;td&gt;53%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10-14&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;td&gt;67%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;gt;14&lt;/td&gt;
&lt;td&gt;18&lt;/td&gt;
&lt;td&gt;94%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content><category term="NBA prediction project"></category></entry><entry><title>Machine learning with wearable sensors</title><link href="https://efavdb.com/machine-learning-with-wearable-sensors" rel="alternate"></link><published>2015-01-09T12:34:00-08:00</published><updated>2015-01-09T12:34:00-08:00</updated><author><name>Damien RJ</name></author><id>tag:efavdb.com,2015-01-09:/machine-learning-with-wearable-sensors</id><summary type="html">&lt;p&gt;A guest post, contributed by Damien Ramunno-Johnson (&lt;a href="https://www.linkedin.com/profile/view?id=60223336&amp;amp;authType=NAME_SEARCH&amp;amp;authToken=LOV_&amp;amp;locale=en_US&amp;amp;trk=tyah2&amp;amp;trkInfo=tarId%3A1420748440448%2Ctas%3Adamien%2Cidx%3A1-1-1"&gt;LinkedIn&lt;/a&gt;, &lt;a href="http://www.efavdb.com/about"&gt;bio-sketch&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;br&gt;
Wearable sensors have become increasingly popular over the last few years with the success of smartphones, fitness trackers, and smart watches. All of these devices create a large amount of data that is ideal for machine learning. Two early examples …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A guest post, contributed by Damien Ramunno-Johnson (&lt;a href="https://www.linkedin.com/profile/view?id=60223336&amp;amp;authType=NAME_SEARCH&amp;amp;authToken=LOV_&amp;amp;locale=en_US&amp;amp;trk=tyah2&amp;amp;trkInfo=tarId%3A1420748440448%2Ctas%3Adamien%2Cidx%3A1-1-1"&gt;LinkedIn&lt;/a&gt;, &lt;a href="http://www.efavdb.com/about"&gt;bio-sketch&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;br&gt;
Wearable sensors have become increasingly popular over the last few years with the success of smartphones, fitness trackers, and smart watches. All of these devices create a large amount of data that is ideal for machine learning. Two early examples are the FitBit and Jawbone&amp;#8217;s up band, both of which analyze sensor input to determine how many steps the user has taken, a metric which is helpful for measuring physical activity. There is no reason to stop there: With all of this data available it is also possible to extract more information. For example, fitness trackers coming out now can also analyze your&amp;nbsp;sleep.&lt;/p&gt;
&lt;p&gt;In that spirit, I&amp;#8217;m going to show here that it is pretty straightforward to make an algorithm that can differentiate between 6 different&amp;nbsp;states.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Walking&lt;/li&gt;
&lt;li&gt;Walking&amp;nbsp;Upstairs&lt;/li&gt;
&lt;li&gt;Walking&amp;nbsp;Downstairs&lt;/li&gt;
&lt;li&gt;Sitting&lt;/li&gt;
&lt;li&gt;Standing&lt;/li&gt;
&lt;li&gt;Laying&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To do this I am going to use &lt;a href="https://www.python.org/"&gt;Python&lt;/a&gt;, &lt;a href="http://scikit-learn.org/"&gt;Sklearn&lt;/a&gt; and &lt;a href="https://plot.ly"&gt;Plot.ly&lt;/a&gt;. Plot.ly is a wonderful plotting package that makes interactive graphs you can share. The first step is to import all of the relevant&amp;nbsp;packages.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Load packages and source data&lt;/strong&gt;&lt;br&gt;
For this example, I used one of the datasets available from the &lt;a href="https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones"&gt;&lt;span class="caps"&gt;UCI&lt;/span&gt; Machine Learning Repository&lt;/a&gt;. For this data set 30 subjects were recorded performing activities of daily living (&lt;span class="caps"&gt;ADL&lt;/span&gt;) while carrying a waist-mounted smartphone (Samsung Galaxy &lt;span class="caps"&gt;II&lt;/span&gt;) with embedded inertial sensors. A testing dataset and training dataset are provided. The dataset has 561 features which were created from the sensor data: &lt;span class="caps"&gt;XYZ&lt;/span&gt; acceleration,&amp;nbsp;etc.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="kp"&gt;loadtxt&lt;/span&gt;  
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;  
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;svm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;grid_search&lt;/span&gt;  
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f1_score&lt;/span&gt;  
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.feature_selection&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;SelectPercentile&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f_classif&lt;/span&gt;  
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;plotly.plotly&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;py&lt;/span&gt;  
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;plotly.graph_objs&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;  
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now that we have loaded all of our packages, it is time to import the data into memory. This data set is not large enough to cause any memory issues, so go ahead and load the whole&amp;nbsp;thing.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;data_test = loadtxt(&amp;quot;./Wearable/UCI_HAR_Dataset/test/X_test.txt&amp;quot;)  &lt;/span&gt;
&lt;span class="err"&gt;label_test=loadtxt(&amp;quot;./Wearable/UCI_HAR_Dataset/test/y_test.txt&amp;quot;)  &lt;/span&gt;
&lt;span class="err"&gt;data_train = loadtxt(&amp;quot;./Wearable/UCI_HAR_Dataset/train/X_train.txt&amp;quot;)  &lt;/span&gt;
&lt;span class="err"&gt;label_train = loadtxt(&amp;quot;./Wearable/UCI_HAR_Dataset/train/y_train.txt&amp;quot;)  &lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Feature selection&lt;/strong&gt;&lt;br&gt;
Given that this data set has training and testing data with labels, it makes sense to do supervised machine learning. We have over 500 potential features to use, which is a lot. Let&amp;#8217;s see if we can get by with fewer features. To do that, we will use &lt;span class="caps"&gt;SK&lt;/span&gt;-learn&amp;#8217;€™s SelectKBest to keep the top 20 percent of the features, and then transform the&amp;nbsp;data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;selector = SelectPercentile(f_classif, 20)  &lt;/span&gt;
&lt;span class="err"&gt;selector.fit(data_train, label_train)  &lt;/span&gt;
&lt;span class="err"&gt;data_train_transformed = selector.transform(data_train)  &lt;/span&gt;
&lt;span class="err"&gt;data_test_transformed = selector.transform(data_test)  &lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Machine learning&lt;/strong&gt;&lt;br&gt;
At this point you need to decide which algorithm you want to use. I tried a few of them and got the best results using a &lt;a href="http://scikit-learn.org/stable/modules/svm.html"&gt;Support Vector Machine&lt;/a&gt; (&lt;span class="caps"&gt;SVM&lt;/span&gt;). SVMs attempt to determine the decision boundary between two classes that is as far away from the data of both classes as possible. In general they work pretty&amp;nbsp;well.&lt;/p&gt;
&lt;p&gt;Let&amp;#8217;s try some parameters and see how good our results&amp;nbsp;are.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;clf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;svm&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SVC&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;rbf&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;C&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_train_transformed&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;pred&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_test_transformed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;print&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;Accuracy is %.4f and the f1-score is %.4f &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;  
&lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label_test&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;f1_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;label_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pred&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;&amp;gt;&amp;gt;Accuracy is 0.8812 and the f1-score is 0.8788  &lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Optimization&lt;/strong&gt;&lt;br&gt;
That&amp;#8217;s not too bad, but I think we can still optimize our results some more. We could change the parameters manually, or we can automate the task using a &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html"&gt;grid search&lt;/a&gt;. This is a handy module that allows you to do a parameter sweep. Below, I set up a sweep using two different kernels and various penalty term values (C) to see if we can raise our&amp;nbsp;accuracy.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;parameters&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;kernel&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;linear&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;rbf&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;C&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="err"&gt;}&lt;/span&gt;  
&lt;span class="n"&gt;svr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;svm&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SVC&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  
&lt;span class="n"&gt;clf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;grid_search&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GridSearchCV&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;svr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_train_transformed&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;pred&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_test_transformed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;print&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;Accuracy is %.4f and the f1-score is %.4f &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;  
&lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label_test&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;f1_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;label_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pred&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;&amp;gt;&amp;gt;Accuracy is 0.9430 and the f1-score is 0.9430  &lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Visualization&lt;/strong&gt;&lt;br&gt;
Looks like we are getting pretty good accuracy for using only 20% of the features available to us. You may have also noticed that I am outputting the &lt;a href="http://en.wikipedia.org/wiki/F1_score"&gt;F1-Score&lt;/a&gt; which is another measure of the accuracy which takes into account the precision and the&amp;nbsp;recall.&lt;/p&gt;
&lt;p&gt;Now let&amp;#8217;s plot some of these data points to see if we can visualize why this is all working. Here, I am using Plot.ly to make the plot. You can make the plots many different ways including converting matplotlib plots into these online plots. If you click on the &amp;#8220;play with this data&amp;#8221; link at the bottom of the figure (or click &lt;a href="https://plot.ly/~Damien RJ/104"&gt;here&lt;/a&gt;) you can see the code used to make the&amp;nbsp;plot.&lt;/p&gt;
&lt;p&gt;[iframe src=&amp;#8221;https://plot.ly/~Damien &lt;span class="caps"&gt;RJ&lt;/span&gt;/104&amp;#8221; width=&amp;#8221;100%&amp;#8221;&amp;nbsp;height=&amp;#8221;680&amp;#8221;]&lt;/p&gt;
&lt;p&gt;I picked two of the features to plot, the z acceleration average, and the z acceleration standard deviation. Note, the gravity component of the acceleration was removed and placed into its own feature. Only 3/6 labels are being plotted to make it a little easier to see what is going on. For example, it is easy to see that the walking profile in the top graph differs significantly from those of standing and laying in the bottom&amp;nbsp;two.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;&lt;br&gt;
From the graphs above alone, it would be difficult to differentiate between laying and standing. We might be able to comb through different combinations of features to find a set that is more easily distinguishable, but we are limited by the simple fact that it is hard to visualize data in more than 3 dimensions. If it turns out that more than a handful of features need to be considered simultaneously to separate the different classes, this approach will fail. In contrast, we have seen in our &lt;span class="caps"&gt;SVM&lt;/span&gt; analysis above that it is actually pretty easy to use machine learning to pick out, with high accuracy, a variety of motions from the sensor data. This is a neat application that is currently being applied widely in industry. It illustrates why machine learning is so interesting in general: It allows us to automate data analysis, and apply it to problems where a by-hand, visual analysis is not&amp;nbsp;possible.&lt;/p&gt;</content><category term="Case studies"></category><category term="machine learning"></category></entry><entry><title>NBA week 9 summary, week 10 predictions</title><link href="https://efavdb.com/nba-week-9-summary-week-10-predictions" rel="alternate"></link><published>2015-01-09T12:22:00-08:00</published><updated>2015-01-09T12:22:00-08:00</updated><author><name>Dustin McIntosh</name></author><id>tag:efavdb.com,2015-01-09:/nba-week-9-summary-week-10-predictions</id><summary type="html">&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/01/week9Table.png"&gt;&lt;img alt="week9Table" src="https://efavdb.com/wp-content/uploads/2015/01/week9Table.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A poorer than average, but not entirely out of the ordinary performance this week (given &lt;a href="http://efavdb.github.io/nba-week-5-summary-week-6-predictions"&gt;expected fluctuations&lt;/a&gt;): &lt;span class="math"&gt;\(32/54 = 59.3%\)&lt;/span&gt;. A slightly poorer performance might be expected on weeks like this - where a blockbuster deal completely changes part of the league: Apparently the Pistons are now unbeatable sans Josh …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/01/week9Table.png"&gt;&lt;img alt="week9Table" src="https://efavdb.com/wp-content/uploads/2015/01/week9Table.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A poorer than average, but not entirely out of the ordinary performance this week (given &lt;a href="http://efavdb.github.io/nba-week-5-summary-week-6-predictions"&gt;expected fluctuations&lt;/a&gt;): &lt;span class="math"&gt;\(32/54 = 59.3%\)&lt;/span&gt;. A slightly poorer performance might be expected on weeks like this - where a blockbuster deal completely changes part of the league: Apparently the Pistons are now unbeatable sans Josh Smith. Summary by point spread is at right. Week 10 predictions&amp;nbsp;up.&lt;/p&gt;
&lt;p&gt;This past week, we also finally managed to implement the automation of our &lt;a href="http://efavdb.github.io/nba-dash"&gt;dashboard&lt;/a&gt; data feed: Now, if you go to the dashboard after 6am &lt;span class="caps"&gt;PST&lt;/span&gt;, records will always be up to date, as will the upcoming games listings and predictions for these games. A caveat: As new data comes in , our daily-updated predictions on the dashboard can now differ slightly from those posted in our official, &lt;a href="http://efavdb.github.io/weekly-nba-predictions"&gt;weekly listings&lt;/a&gt; &amp;#8212; We put some by-hand effort into the weekly listings, so they&amp;#8217;re &amp;#8220;official&amp;#8221;. Nevertheless, it&amp;#8217;s the same algorithm being applied to obtain the Dashboard results. Automating our feed required using &lt;a href="http://www.crummy.com/software/BeautifulSoup/"&gt;beautiful soup&lt;/a&gt; to do some scraping, as well as learning how to set up &lt;a href="http://en.wikipedia.org/wiki/Cron"&gt;cron jobs&lt;/a&gt; &amp;#8212; Thanks to J. Bergknoff for suggesting we look into the latter. Automation is a beautiful thing: With all the free time now opened up, we can start to consider other items on our wish list (eg, improving our&amp;nbsp;predictions&amp;#8230;)&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="NBA prediction project"></category><category term="NBA"></category></entry><entry><title>Processing and processing.js tips and tricks in WordPress</title><link href="https://efavdb.com/processing-and-processing-js-tips-and-tricks" rel="alternate"></link><published>2015-01-05T00:12:00-08:00</published><updated>2015-01-05T00:12:00-08:00</updated><author><name>Dustin McIntosh</name></author><id>tag:efavdb.com,2015-01-05:/processing-and-processing-js-tips-and-tricks</id><summary type="html">&lt;p&gt;We recently developed our &lt;a href="http://efavdb.github.io/nba-dash"&gt;&lt;span class="caps"&gt;NBA&lt;/span&gt; dashboard&lt;/a&gt; in the programming language Processing. In addition, we have Processing apps in our post on &lt;a href="http://efavdb.github.io/methods-regression-without-negative-examples"&gt;classification without negative examples&lt;/a&gt; as well as our &lt;a href="http://efavdb.github.io/weekly-nba-predictions"&gt;weekly &lt;span class="caps"&gt;NBA&lt;/span&gt; predictions&lt;/a&gt;. Here, we will briefly describe (and recommend) Processing and discuss some tips and tricks we have discovered in …&lt;/p&gt;</summary><content type="html">&lt;p&gt;We recently developed our &lt;a href="http://efavdb.github.io/nba-dash"&gt;&lt;span class="caps"&gt;NBA&lt;/span&gt; dashboard&lt;/a&gt; in the programming language Processing. In addition, we have Processing apps in our post on &lt;a href="http://efavdb.github.io/methods-regression-without-negative-examples"&gt;classification without negative examples&lt;/a&gt; as well as our &lt;a href="http://efavdb.github.io/weekly-nba-predictions"&gt;weekly &lt;span class="caps"&gt;NBA&lt;/span&gt; predictions&lt;/a&gt;. Here, we will briefly describe (and recommend) Processing and discuss some tips and tricks we have discovered in developing and deploying our above-mentioned apps to our WordPress&amp;nbsp;blog.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://twitter.com/efavdb"&gt;Follow&amp;nbsp;@efavdb&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Follow us on twitter for new submission&amp;nbsp;alerts!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Processing&amp;nbsp;Intro&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Your browser does not support the canvas&amp;nbsp;tag.&lt;/p&gt;
&lt;p&gt;JavaScript is required to view the contents of this&amp;nbsp;page.&lt;/p&gt;
&lt;p&gt;Processing is a high-level language built on top of java designed for producing neat interactive visualizations of ideas, data, and art.  A major benefit of Processing is that it is extremely accessible (see for example, &lt;a href="http://it-ebooks.info/book/244/"&gt;this ebook&lt;/a&gt; by &lt;a href="http://reas.com/"&gt;Reas&lt;/a&gt;and &lt;a href="http://benfry.com/"&gt;Fry&lt;/a&gt;): while it can now be used to produce considerably sophisticated visualizations, it was originally developed as a tool for teaching programming to people with little to no experience with computers (e.g., visual arts students). The little guy above is an example of a processing app. You can find the (remarkably simple) code for it &lt;a href="https://efavdb.com/wp-content/uploads/sketches/testSketch2/testSketch2.pde"&gt;here&lt;/a&gt; (we found this example in &lt;a href="http://processingjs.org/articles/p5QuickStart.html"&gt;this tutorial&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;For people with technical backgrounds, you can be up and developing basic programs in a matter of hours (or perhaps even minutes). Here, our programming language of choice is typically Python and, fortunately, the people at Processing recently implemented &lt;a href="http://py.processing.org"&gt;Python-mode&lt;/a&gt; which made our transition all the more smooth (see Quirks below before you get too excited about this, however).  In just a couple of days we were able to build a quick prototype of our tool. Porting it to the web, however, we found to be a bit more&amp;nbsp;difficult.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Processing in WordPress&amp;nbsp;Tutorial&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We spent some time trying to figure out how to get everything working in WordPress. We thought it would be useful to outline the steps below for others interested in getting some Processing-based dynamic content on their own blog. In the end it is very straight forward, but getting some more advanced features of Processing (e.g., importing external images or data files) to work on the web requires some tricks which we will tackle in the next&amp;nbsp;section.&lt;/p&gt;
&lt;p&gt;1.  &lt;a href="http://processingjs.org/download/"&gt;Download&lt;/a&gt; the processing.js script and put it on your server.  This script defines a &amp;#8220;canvas&amp;#8221; element where your Processing code is executed on your&amp;nbsp;webpage.&lt;/p&gt;
&lt;p&gt;​2. In your theme&amp;#8217;s header file (header.php), add the following line of code to the body of the file (after &lt;body&gt; and before &lt;/body&gt;), changing the url to point to the script from step&amp;nbsp;1:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="x"&gt;&amp;lt;script src=&amp;quot;url_to_processing.js&amp;quot; type=&amp;quot;text/javascript&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;You can change this file using your dashboard in the Appearance - Editor menu, selecting &amp;#8220;Header (header.php)&amp;#8221; for the Template (see this screenshot - the first line is a comment).
&lt;a href="https://efavdb.com/wp-content/uploads/2014/12/WordPressHeader.png"&gt;&lt;img alt="" src="https://efavdb.com/wp-content/uploads/2014/12/WordPressHeader.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;​3. Upload your pde script(s) to your server and they can be referenced using code like the following (replace the urls to point to your pde&amp;nbsp;scripts):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="x"&gt;&amp;lt;canvas id=&amp;quot;testsketch&amp;quot; class=&amp;quot;alignright&amp;quot; data-processing-sources=&lt;/span&gt;
&lt;span class="x"&gt;&amp;quot;{static}/wp-content/uploads/sketches/testSketch2/testSketch2.pde&lt;/span&gt;
&lt;span class="x"&gt;additional_url.pde  yet_another_url.pde&amp;quot;&amp;gt;&lt;/span&gt;
&lt;span class="x"&gt;Your browser does not support the canvas tag.&amp;lt;/canvas&amp;gt;&lt;/span&gt;
&lt;span class="x"&gt;&amp;lt;noscript&amp;gt;JavaScript is required to view the contents of this page.&lt;/span&gt;
&lt;span class="x"&gt;&amp;lt;/noscript&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;You simply place this code in the &amp;#8220;text&amp;#8221; view of any post or page in WordPress. Note that you don&amp;#8217;t need to have multiple pde scripts, in most simple cases you will only have one.  However, sometimes it is useful to call on multiple (see below quirk #3). To include multiple, simply place multiple urls (separated by spaces) between the quotes in the canvas&amp;nbsp;tag.&lt;/p&gt;
&lt;p&gt;​4. Enjoy! Wasn&amp;#8217;t that simple? Now you can mess around and create all sorts of cool, dynamic apps for your&amp;nbsp;blog.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Running list of processing quirks and&amp;nbsp;tips
&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;​1) It took us a while to come to the conclusion that there is, currently, no easy way to get Python-mode Processing code on the web (please let us know if we are wrong about that&amp;#8230;).  We ended up translating our Processing code from Python syntax to Javascript syntax and using &lt;a href="http://processingjs.org/"&gt;processing.js&lt;/a&gt; (as implied in our tutorial&amp;nbsp;above).&lt;/p&gt;
&lt;p&gt;​2) In order to load images for display in your app (if applicable), you will have to use the &lt;a href="http://processingjs.org/reference/preload/"&gt;preload directive&lt;/a&gt; in your pde script.  This comes in the form a of a comment at the top of the script: &lt;em&gt;/&lt;/em&gt; @pjs preload=&amp;#8221;url_to_myimage.jpg&amp;#8221;; &lt;em&gt;/&lt;/em&gt;. The &amp;#8220;@pjs&amp;#8221; tags this comment for processing.js to read and execute before the rest of the script runs.  It forces the browser to complete downloading and caching of the image before attempting to use&amp;nbsp;it.&lt;/p&gt;
&lt;p&gt;​3) We could not figure out a clean way of importing data from an external file (e.g., in csv or txt format).  Instead, we found that saving the external data in Javascript format in a separate pde file and running that file in addition to your main script (see step 3 from the tutorial above) worked well. As an example, &lt;a href="http://efavdb.com/wp-content/uploads/nba/completed.pde"&gt;here&lt;/a&gt; is a pde file that we use to feed in the results of &lt;span class="caps"&gt;NBA&lt;/span&gt; games that have already been played this season into our &lt;span class="caps"&gt;NBA&lt;/span&gt; dashboard. We scrape this data off the web and run a preprocessing python script to get the data in this format and save the completed.pde&amp;nbsp;file.&lt;/p&gt;
&lt;p&gt;​4) As a security precaution, many browsers (e.g., Chrome) won’t run a sketch that links to images or data stored on a separate server. As a consequence, when developing a sketch on your computer that links to data on your server, it may appear to not be working, even if coded properly. It&amp;#8217;s easy to misinterpret this issue as a bug, so keep it in mind. One way to avoid this is to try running your sketch in other browsers. Regardless, once a sketch is uploaded to your server, the sketch should work fine on any&amp;nbsp;browser.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/12/mouseDisplacement.png"&gt;&lt;img alt="" src="https://efavdb.com/wp-content/uploads/2014/12/mouseDisplacement.png"&gt;&lt;/a&gt; 5) Interestingly, when logged in to WordPress, we have found that the Processing apps work, but function as if the mouse were several pixels lower than it actually is. This symptom is only apparent when logged in to WordPress and viewing the page (see example screenshot at&amp;nbsp;right).&lt;/p&gt;
&lt;p&gt;​6) Processing has two rendering modes, &lt;a href="https://www.processing.org/reference/smooth_.html"&gt;smooth&lt;/a&gt; and not smooth. I believe the latter requires less memory to implement, and that this is why it exists. We prefer smooth drawing, and tried to implement this from the start. However, we recently discovered that if you apply coordinate transformations in your processing sketch (translations or rotations), the program forces back to the not smooth mode. This came as a surprise to us, since it&amp;#8217;s not mentioned in any of the basic documentation. Our original &lt;a href="http://efavdb.github.io/nba-dash"&gt;&lt;span class="caps"&gt;NBA&lt;/span&gt; wheel sketch&lt;/a&gt; made use of a translation; we&amp;#8217;ve reimplemented that without it, and see a marked aesthetic&amp;nbsp;improvement.&lt;/p&gt;
&lt;p&gt;​7) We noticed that running our first &lt;a href="http://efavdb.github.io/nba-dash"&gt;&lt;span class="caps"&gt;NBA&lt;/span&gt; wheel sketch&lt;/a&gt; would often cause a laptop to quickly heat up. This was due to the fact that we were having the sketch refresh with a frame rate of about 30 hz, which is totally unnecessary for this particular application. Changing the draw program so that the image refreshes only when some new information is requested totally fixed our heating&amp;nbsp;issues.&lt;/p&gt;</content><category term="Tools"></category><category term="programming"></category><category term="visualization"></category></entry><entry><title>NBA week 8 results, week 9 predictions</title><link href="https://efavdb.com/nba-week-8-results-week-9-predictions" rel="alternate"></link><published>2015-01-02T16:43:00-08:00</published><updated>2015-01-02T16:43:00-08:00</updated><author><name>Dustin McIntosh</name></author><id>tag:efavdb.com,2015-01-02:/nba-week-8-results-week-9-predictions</id><summary type="html">&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/01/week8Table.png"&gt;&lt;img alt="week8Table" src="https://efavdb.com/wp-content/uploads/2015/01/week8Table.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A pretty good bounce-back from last week&amp;#8217;s results: &lt;span class="math"&gt;\(35/52 = 67.3%\)&lt;/span&gt;. Breakdown by point spread at&amp;nbsp;right.&lt;/p&gt;
&lt;p&gt;Week 9 predictions are up now on both our &lt;a href="http://efavdb.github.io/nba-dash"&gt;&lt;span class="caps"&gt;NBA&lt;/span&gt; Dashboard&lt;/a&gt; and the new and improved &lt;a href="http://efavdb.github.io/weekly-nba-predictions"&gt;Weekly &lt;span class="caps"&gt;NBA&lt;/span&gt; Predictions Page&lt;/a&gt; (this also done in&amp;nbsp;Processing).&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align …&lt;/script&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/01/week8Table.png"&gt;&lt;img alt="week8Table" src="https://efavdb.com/wp-content/uploads/2015/01/week8Table.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A pretty good bounce-back from last week&amp;#8217;s results: &lt;span class="math"&gt;\(35/52 = 67.3%\)&lt;/span&gt;. Breakdown by point spread at&amp;nbsp;right.&lt;/p&gt;
&lt;p&gt;Week 9 predictions are up now on both our &lt;a href="http://efavdb.github.io/nba-dash"&gt;&lt;span class="caps"&gt;NBA&lt;/span&gt; Dashboard&lt;/a&gt; and the new and improved &lt;a href="http://efavdb.github.io/weekly-nba-predictions"&gt;Weekly &lt;span class="caps"&gt;NBA&lt;/span&gt; Predictions Page&lt;/a&gt; (this also done in&amp;nbsp;Processing).&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="NBA prediction project"></category><category term="NBA"></category></entry><entry><title>Quantifying the NBA Christmas week flop: one in ten thousand?</title><link href="https://efavdb.com/an-nba-christmas" rel="alternate"></link><published>2014-12-28T21:55:00-08:00</published><updated>2014-12-28T21:55:00-08:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2014-12-28:/an-nba-christmas</id><summary type="html">&lt;p&gt;There were a number of upsets in the &lt;span class="caps"&gt;NBA&lt;/span&gt; this past Christmas week. Here, we offer no explanation, but do attempt to quantify just how bad those upsets were, taken in aggregate. Short answer: real bad! To argue this point, we review and then apply a very simple predictive model …&lt;/p&gt;</summary><content type="html">&lt;p&gt;There were a number of upsets in the &lt;span class="caps"&gt;NBA&lt;/span&gt; this past Christmas week. Here, we offer no explanation, but do attempt to quantify just how bad those upsets were, taken in aggregate. Short answer: real bad! To argue this point, we review and then apply a very simple predictive model for sporting event outcomes &amp;#8212; python code given in&amp;nbsp;footnotes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Quick review of x-mas week&lt;/strong&gt;
The Christmas holiday week&lt;span class="math"&gt;\(^1\)&lt;/span&gt; (Dec. 19 - 25) provided a steady stream of frustrating upsets. The two most perplexing, perhaps, were the Lakers win over the Warriors and the Jazz win over the Grizzlies: two of this year&amp;#8217;s greats losing to two of its most lackluster. In all, &lt;span class="math"&gt;\(24\)&lt;/span&gt; of the &lt;span class="math"&gt;\(49\)&lt;/span&gt; games that week were upsets (with an upset defined here to be one where the winning team started the game with a lower win percentage than the loser). That comes out to an upset ratio just under &lt;span class="math"&gt;\(49%\)&lt;/span&gt;, much higher than the typical rate, about &lt;span class="math"&gt;\(34%\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A general sporting model&lt;/strong&gt;
A &lt;span class="math"&gt;\(49%\)&lt;/span&gt; upset rate sounds significant. However, this metric does not quite capture the emotional magnitude of the debacle. To move towards obtaining such a metric, we first review here a &amp;#8220;standard&amp;#8221;&lt;span class="math"&gt;\(^2\)&lt;/span&gt; sporting model that will allow us to quantify the probability of observing a week as bad as this just past. For each team &lt;span class="math"&gt;\(i\)&lt;/span&gt;, we introduce a variable &lt;span class="math"&gt;\(h_i\)&lt;/span&gt; called its mean scoring potential: Subtracting from this the analogous value for team &lt;span class="math"&gt;\(j\)&lt;/span&gt; gives the expected number of points team &lt;span class="math"&gt;\(i\)&lt;/span&gt; would win by, were it to play team &lt;span class="math"&gt;\(j\)&lt;/span&gt;. More formally, if we let the win-difference for any particular game be &lt;span class="math"&gt;\(y_{ij}\)&lt;/span&gt;, we&amp;nbsp;have 
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{1}
h_i - h_j \equiv \langle score(i) - score(j) \rangle \equiv \langle y_{ij} \rangle,
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
 where we average over hypothetical outcomes on the right in order to account for the variability characterizing each individual&amp;nbsp;game.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/12/Screen-Shot-2015-01-01-at-3.51.42-AM.png"&gt;&lt;img alt="Screen Shot 2015-01-01 at 3.51.42 AM" src="https://efavdb.com/wp-content/uploads/2014/12/Screen-Shot-2015-01-01-at-3.51.42-AM.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;By taking into account the games that have already occurred this season, one can estimate the set of &lt;span class="math"&gt;\(\{h_i\}\)&lt;/span&gt; values. For example, summing the above equation over all past games played by team &lt;span class="math"&gt;\(1\)&lt;/span&gt;, we&amp;nbsp;obtain 
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{2}
 \sum_{j\text{ (past opponents of 1)}} (h_1 - h_j) = \sum_j \langle y_{1j} \rangle \approx \sum_j y_{1j}.
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Here, in the sum on right we have approximated the averaged sum in the middle by the score differences actually observed in the games already played (note that in the sum on &lt;span class="math"&gt;\(j\)&lt;/span&gt; here, each team appears exactly the number of times they have already played team &lt;span class="math"&gt;\(1\)&lt;/span&gt; &amp;#8212; this could be zero, once, twice, etc.) Writing down all equations analogous to this last one (one for each team) returns a system of &lt;span class="math"&gt;\(30\)&lt;/span&gt; linear equations in the &lt;span class="math"&gt;\(30\)&lt;/span&gt; &lt;span class="math"&gt;\(\{h_i\}\)&lt;/span&gt; variables. This system can be easily solved using a computer&lt;span class="math"&gt;\(^3\)&lt;/span&gt;. We did this, applying the algorithm to the complete set of 2014-15 games played prior to the Christmas week, and obtained the set of &lt;span class="math"&gt;\(h\)&lt;/span&gt; values shown at right&lt;span class="math"&gt;\(^4\)&lt;/span&gt;. The ranking looks quite reasonable, from top to&amp;nbsp;bottom.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A Gaussian &lt;span class="caps"&gt;NBA&lt;/span&gt;&lt;/strong&gt;
Now that we have the &lt;span class="math"&gt;\(\{h_i\}\)&lt;/span&gt; values, we can use them to estimate the mean score difference for any game. For example, in a Warriors-76ers game, we&amp;#8217;d expect the Warriors to win, since they have the larger &lt;span class="math"&gt;\(h\)&lt;/span&gt; value. Further, on average, we&amp;#8217;d expect them to win by about &lt;span class="math"&gt;\(h_{\text{War's}} - h_{\text{76's}}\)&lt;/span&gt; &lt;span class="math"&gt;\( = 9.24 - (-11.96) \approx 21\)&lt;/span&gt; points. These two actually played this week, on Dec 30, and the Warriors won by &lt;span class="math"&gt;\(40\)&lt;/span&gt;, a much larger margin than&amp;nbsp;predicted.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/12/hist.jpg"&gt;&lt;img alt="hist" src="https://efavdb.com/wp-content/uploads/2014/12/hist.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The distinction between our predicted and the actual Warriors-76ers outcome motivates further consideration of the variability characterizing &lt;span class="caps"&gt;NBA&lt;/span&gt; games. It turns out that if we analyze the complete set of games already played this year, something simple pops out: Plotting a histogram of our estimate errors, &lt;span class="math"&gt;\(\epsilon_{ij} \equiv (h_i - h_j) - y_{ij}\)&lt;/span&gt;, we see that the actual score difference distribution of &lt;span class="caps"&gt;NBA&lt;/span&gt; games looks a lot like a &lt;a href="http://en.wikipedia.org/wiki/Gaussian_function"&gt;Gaussian&lt;/a&gt;, or bell curve. This is centered about our predicted value and has a standard deviation of &lt;span class="math"&gt;\(\sigma \approx 11\)&lt;/span&gt; points, as shown in the figure at right. These observations allow us to estimate various quantities of interest. For instance, we can estimate the frequency with which the Warriors should beat the 76ers by 40 or more points, as they did this week. This is simply equal to the frequency with which we underestimate the winning margin by at least &lt;span class="math"&gt;\(40 - 21 = 19\)&lt;/span&gt; points. This, in turn, can be estimated by counting how often this has already occurred in past games, using our histogram. Alternatively, we can use the fact that our errors are Gaussian distributed to write this&amp;nbsp;as 
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{3}
 P(\epsilon \leq -19) = \frac{1}{\sqrt{2 \pi \sigma^2}} \int_{-\infty}^{-19} e^{-\frac{\epsilon^2}{2\sigma^2}} d \epsilon \approx 0.042,
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
where we have evaluated the integral by computer. This result says that a Warriors win by 40 or more points will only occur about &lt;span class="math"&gt;\(4.2%\)&lt;/span&gt; of the time. Using a similar argument, one can show that the 76ers should beat the Warriors only about &lt;span class="math"&gt;\(2.8 %\)&lt;/span&gt; of the&amp;nbsp;time.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Christmas week, quantified&lt;/strong&gt;
It is now a simple matter to extend our analysis method so that we can estimate the joint likelihood of a given set of outcomes all happening the same week: We need only make use of the &lt;a href="%20http://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables"&gt;fact&lt;/a&gt; that the mean estimate error &lt;span class="math"&gt;\(\langle \epsilon \rangle\)&lt;/span&gt; of our predictions on a set of &lt;span class="math"&gt;\(N\)&lt;/span&gt; games &lt;span class="math"&gt;\((\langle \epsilon \rangle = \frac{1}{N}\sum_{\text{games }i = 1}^N \epsilon_i)\)&lt;/span&gt; will also be Gaussian distributed, but now with standard deviation &lt;span class="math"&gt;\(\sigma/ \sqrt{N}\)&lt;/span&gt;. The &lt;span class="math"&gt;\(1/\sqrt{N}\)&lt;/span&gt; factor here reduces the width of the mean error distribution, relative to that of the single games &amp;#8212; it takes into account the significant cancellations that typically occur when you sum over many games, some with positive and some with negative errors. A typical week has about &lt;span class="math"&gt;\(50\)&lt;/span&gt; games, so the mean error standard deviation will usually be about &lt;span class="math"&gt;\(11/\sqrt{50} \approx 1.6\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In the four figures below, we plot histograms of our prediction errors for four separate weeks: Christmas week is shown last (in red), and the other subplots correspond to the three weeks preceding it (each in green). We also show in each subplot (in gray) a histogram of all game errors preceding the week highlighted in that subplot &amp;#8212; notice that each is quite well-fit by a Gaussian. In the first week, &lt;span class="math"&gt;\(53\)&lt;/span&gt; games were played, and our average error on these games was just &lt;span class="math"&gt;\(\langle \epsilon \rangle = 0.5\)&lt;/span&gt; points. The probability of observing an average overestimate of &lt;span class="math"&gt;\(0.5\)&lt;/span&gt; or greater in such a week is given&amp;nbsp;by, 
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{4}
P(\langle \epsilon \rangle \geq 0.5) = \frac{1}{\sqrt{2 \pi \sigma^2/53}} \int_{0.5}^{\infty} e^{-\frac{\epsilon^2}{2\sigma^2/53}} d \epsilon \approx 0.38.
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
That is, a weekly average overestimate of &lt;span class="math"&gt;\(\langle \epsilon \rangle \geq 0.5\)&lt;/span&gt; will happen about &lt;span class="math"&gt;\(38%\)&lt;/span&gt; of the time, and so is pretty common. Similarly, in the second, third, and fourth weeks, the number of games played and average estimate errors were &lt;span class="math"&gt;\((N,\langle \epsilon \rangle) = (52,0.8),\)&lt;/span&gt; &lt;span class="math"&gt;\((55,2.2)\)&lt;/span&gt;, and &lt;span class="math"&gt;\((49,5.7)\)&lt;/span&gt;, respectively. Calculating as above, overestimates of these magnitudes or larger occur with frequency &lt;span class="math"&gt;\(30%\)&lt;/span&gt;, &lt;span class="math"&gt;\(7%\)&lt;/span&gt;, and &lt;span class="math"&gt;\(0.01 %\)&lt;/span&gt;, respectively. The previous two are both fairly common, &lt;em&gt;but &amp;#8212; on average &amp;#8212; it would apparently take about ten thousand trials to find a week as bad as Christmas week&amp;nbsp;2014.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/12/xmas_plots2.jpg"&gt;&lt;img alt="xmas_plots" src="https://efavdb.com/wp-content/uploads/2014/12/xmas_plots2.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;
A week in ten thousand is equivalent to about one week in every &lt;span class="math"&gt;\(400\)&lt;/span&gt; seasons! We don&amp;#8217;t really take this estimate too seriously. In fact, we suspect that one of the following might be happening here: a) there may have been something peculiar about the games held this Christmas week that caused their outcomes to not be distributed in the same manner as other games this season&lt;span class="math"&gt;\(^5\)&lt;/span&gt;, b) alternatively, there may be long tails in the error distribution that we can&amp;#8217;t easily observe, or c) it may be that improvements to our model (e.g., taking into account home team advantage, etc.) would result in a larger frequency estimate. Maybe all three are true, or maybe this really was a week in ten thousand. Either way, it&amp;#8217;s clear that this past Christmas week was a singular&amp;nbsp;one.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Footnotes&lt;/strong&gt;
[1] The &lt;span class="caps"&gt;NBA&lt;/span&gt; workweek starts on&amp;nbsp;Friday.&lt;/p&gt;
&lt;p&gt;[2] We first read about this modeling method &lt;a href="http://www.pro-football-reference.com/blog/?p=37"&gt;here&lt;/a&gt;. In the addendum, it&amp;#8217;s stated that the author thinks that nobody in particular is credited with having developed it, and that it&amp;#8217;s been around for a long&amp;nbsp;time.&lt;/p&gt;
&lt;p&gt;[3] Notice that we can shift all &lt;span class="math"&gt;\(h_i \to h_i +c\)&lt;/span&gt;, with &lt;span class="math"&gt;\(c\)&lt;/span&gt; some common constant. This invariance means that the solution obtained by solving the system of equations is not unique. Consequently, the matrix of coefficients is not invertible, and the system needs to be solved by Gaussian elimination, or some other irritating&amp;nbsp;means.&lt;/p&gt;
&lt;p&gt;[4] Python code and data for evaluating the &lt;span class="caps"&gt;NBA&lt;/span&gt; &lt;span class="math"&gt;\(h\)&lt;/span&gt; values given &lt;a href="%20http://efavdb.github.io/nba-h-model"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;[5] Note, however, that carrying out a similar analysis over the past 9 seasons showed no similar anomalies in their respective Christmas&amp;nbsp;weeks.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="NBA prediction project"></category><category term="NBA"></category><category term="statistics"></category><category term="methods"></category></entry><entry><title>NBA week 7 results, week 8 predictions</title><link href="https://efavdb.com/nba-week-7-results-week-8-predictions" rel="alternate"></link><published>2014-12-25T23:59:00-08:00</published><updated>2014-12-25T23:59:00-08:00</updated><author><name>Dustin McIntosh</name></author><id>tag:efavdb.com,2014-12-25:/nba-week-7-results-week-8-predictions</id><summary type="html">&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/12/week7Table.png"&gt;&lt;img alt="week7Table" src="https://efavdb.com/wp-content/uploads/2014/12/week7Table.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The week that defeated the &lt;span class="caps"&gt;NBA&lt;/span&gt; predictor&amp;#8230; &lt;span class="math"&gt;\(24/49 = 49.0%\)&lt;/span&gt;. That&amp;#8217;s right, worse than a coin flip. Not exactly the Christmas present we had in mind. We certainly weren&amp;#8217;t expecting things to be this bad, but Christmas week is extremely difficult to predict, historically. It&amp;#8217;s not …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/12/week7Table.png"&gt;&lt;img alt="week7Table" src="https://efavdb.com/wp-content/uploads/2014/12/week7Table.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The week that defeated the &lt;span class="caps"&gt;NBA&lt;/span&gt; predictor&amp;#8230; &lt;span class="math"&gt;\(24/49 = 49.0%\)&lt;/span&gt;. That&amp;#8217;s right, worse than a coin flip. Not exactly the Christmas present we had in mind. We certainly weren&amp;#8217;t expecting things to be this bad, but Christmas week is extremely difficult to predict, historically. It&amp;#8217;s not like our algorithm went crazy or anything, everything still looks sane, we still put the 76ers at terrible and the Warriors at awesome. Of course, that didn&amp;#8217;t help us much when the former beat the Heat and the latter lost to the Lakers. Utah over Memphis and New Orleans over &lt;span class="caps"&gt;OKC&lt;/span&gt; are a couple more head-scratchers. I know, I know, more excuses, but seriously, this is well below our estimated range of plausible results (see comments &lt;a href="http://efavdb.github.io/nba-week-5-summary-week-6-predictions"&gt;here&lt;/a&gt;). This deserves an explanation - we are investigating, more to come on the &lt;span class="caps"&gt;NBA&lt;/span&gt; grinch that stole our&amp;nbsp;Christmas.&lt;/p&gt;
&lt;p&gt;In the meantime, a hopefully much better set of predictions for week 8 are now&amp;nbsp;up.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="NBA prediction project"></category><category term="NBA"></category></entry><entry><title>Machine Learning Methods: Classification without negative examples</title><link href="https://efavdb.com/methods-regression-without-negative-examples" rel="alternate"></link><published>2014-12-20T09:58:00-08:00</published><updated>2014-12-20T09:58:00-08:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2014-12-20:/methods-regression-without-negative-examples</id><summary type="html">&lt;p&gt;Here, we discuss some methods for carrying out classification when only positive examples are available. The latter half of our discussion borrows heavily from &lt;span class="caps"&gt;W.S.&lt;/span&gt; Lee and B. Liu, Proc. &lt;span class="caps"&gt;ICML&lt;/span&gt;-2003 (2003), which we supplement&amp;nbsp;somewhat.  &lt;/p&gt;
&lt;h2&gt;Generic logistic&amp;nbsp;regression&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://en.wikipedia.org/wiki/Logistic_regression"&gt;Logistic regression&lt;/a&gt; is a commonly used tool for estimating …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Here, we discuss some methods for carrying out classification when only positive examples are available. The latter half of our discussion borrows heavily from &lt;span class="caps"&gt;W.S.&lt;/span&gt; Lee and B. Liu, Proc. &lt;span class="caps"&gt;ICML&lt;/span&gt;-2003 (2003), which we supplement&amp;nbsp;somewhat.  &lt;/p&gt;
&lt;h2&gt;Generic logistic&amp;nbsp;regression&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://en.wikipedia.org/wiki/Logistic_regression"&gt;Logistic regression&lt;/a&gt; is a commonly used tool for estimating the level sets of a Boolean function &lt;span class="math"&gt;\(y\)&lt;/span&gt; on a set of feature vectors &lt;span class="math"&gt;\(\textbf{F}\)&lt;/span&gt;: In a sense, you can think of it as a method for playing the game &amp;#8220;Battleship&amp;#8221; on whatever data set you&amp;#8217;re interested in. Its application requires knowledge of the &lt;span class="math"&gt;\(\{(\textbf{f}_i,y_i)\}\)&lt;/span&gt; pairs on a training set &lt;span class="math"&gt;\(\textbf{E} \subseteq \textbf{F}\)&lt;/span&gt;, with label &lt;span class="math"&gt;\(y_i = 0,1\)&lt;/span&gt; for negative and positive examples, respectively. Given these training examples, logistic regression estimates for arbitrary feature vector &lt;span class="math"&gt;\(\textbf{f}\)&lt;/span&gt;,&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
h(\textbf{f}) = \frac{1}{1 + \exp \left [ - \textbf{T} \cdot \textbf{f} \right]} \approx y \tag{1}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
where the coefficient vector &lt;span class="math"&gt;\(\textbf{T}\)&lt;/span&gt; is taken to be that vector that minimizes&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{2}
J(h) \equiv -\frac{1}{\vert \textbf{E} \vert}\sum_{i=1}^{\vert \textbf{E} \vert} y_i \log(h_i) + (1-y_i) \log(1- h_i) + \frac{\Lambda}{2}\sum_j T_j^2,
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
a convex cost function that strongly penalizes poor estimates on the training&amp;nbsp;set.&lt;/p&gt;
&lt;h2&gt;Problem statement: no negative&amp;nbsp;examples&lt;/h2&gt;
&lt;p&gt;Consider now a situation where all training examples given are positive &amp;#8212; i.e., no negative examples are available. One realistic realization of this scenario might involve a simple data set of movies already viewed by some Netflix customer. From this information, one would like to estimate the full subset of the available movies that the customer would watch, given time. We&amp;#8217;ll assign value &lt;span class="math"&gt;\(y = 1\)&lt;/span&gt; to such movies and &lt;span class="math"&gt;\(y=0\)&lt;/span&gt; to movies he wouldn&amp;#8217;t watch. Notice that the generic logistic regression approach outlined above would return a default-positive result if applied to this problem: Assigning &lt;span class="math"&gt;\(h = 1\)&lt;/span&gt; to all of &lt;span class="math"&gt;\(\textbf{F}\)&lt;/span&gt; minimizes &lt;span class="math"&gt;\(J\)&lt;/span&gt;. This means that no information contained in &lt;span class="math"&gt;\(\textbf{E}\)&lt;/span&gt; is actually utilized in the logistic learning process &amp;#8212; a counterintuitive choice for structured &lt;span class="math"&gt;\(\textbf{E}\)&lt;/span&gt; (e.g., the case where all movies watched thus far have been in a single category &amp;#8212; martial arts films,&amp;nbsp;say).&lt;/p&gt;
&lt;h2&gt;Noisy&amp;nbsp;labeling&lt;/h2&gt;
&lt;p&gt;Some reasonable, alternative approaches do not return the default-positive response in the situation above. To see this, we first review here noisy labeling problems. Suppose we are given a training set with noisy labeling &lt;span class="math"&gt;\(y^{\prime}\)&lt;/span&gt;: Truly-positive examples &lt;span class="math"&gt;\((y = 1)\)&lt;/span&gt; are stochastically mislabeled in this set with frequency &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; as negative &lt;span class="math"&gt;\((y^{\prime} = 0)\)&lt;/span&gt;, and truly-negative examples &lt;span class="math"&gt;\((y=0)\)&lt;/span&gt; are mislabeled with frequency &lt;span class="math"&gt;\(\beta\)&lt;/span&gt; as positive &lt;span class="math"&gt;\((y^{\prime} = 1)\)&lt;/span&gt;. For hypothesis &lt;span class="math"&gt;\(h\)&lt;/span&gt;,&amp;nbsp;let
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{3}
C(h) = Pr[h = 0 \vert y = 1]+ Pr[h = 1 \vert y= 0],
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
the rate at which &lt;span class="math"&gt;\(h\)&lt;/span&gt; mislabels positive examples in the training set added to the rate at which it mislabels negative examples. Similarly, we define &lt;span class="math"&gt;\(C^{\prime}(h)\)&lt;/span&gt; as above, but with &lt;span class="math"&gt;\(y\)&lt;/span&gt; replaced by &lt;span class="math"&gt;\(y^{\prime}\)&lt;/span&gt;. Because &lt;span class="math"&gt;\(y^{\prime}\)&lt;/span&gt; is stochastic, we also average it in this case,&amp;nbsp;giving
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{4}
C^{\prime}(h) = \left \langle Pr[h = 0 \vert y^{\prime} = 1]+ Pr[h = 1 \vert y^{\prime}= 0] \right \rangle_{y^{\prime}}.
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
With these definitions, we have [see Blum and Michael (1998) or derive&amp;nbsp;yourself] 
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{5}
C(h) \propto C^{\prime}(h),
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
with &lt;span class="math"&gt;\(\text{sign}(C) = \text{sign}(1 - \alpha - \beta) \times \text{sign}(C^{\prime})\)&lt;/span&gt;. This result is very useful whenever we take &lt;span class="math"&gt;\(C(h)\)&lt;/span&gt; as our cost function&lt;span class="math"&gt;\(^1\)&lt;/span&gt;: Provided the total noise rate &lt;span class="math"&gt;\(\alpha + \beta &amp;lt;1\)&lt;/span&gt;, it implies that we can find the &amp;#8220;&lt;span class="math"&gt;\(C\)&lt;/span&gt;-optimizing&amp;#8221; &lt;span class="math"&gt;\(h\)&lt;/span&gt; within any class of hypotheses by optimizing instead &lt;span class="math"&gt;\(C^{\prime}\)&lt;/span&gt; &amp;#8212; a quantity that we can estimate given any particular noisy labeling realization &lt;span class="math"&gt;\(y^{\prime}_0\)&lt;/span&gt;&amp;nbsp;as
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{6}
C^{\prime}(h) \approx \left (Pr[h = 0 \vert y^{\prime} = 1]+ Pr[h = 1 \vert y^{\prime}= 0] \right ) \vert_{y^{\prime} =y^{\prime}_0}.
\end{eqnarray}&lt;/div&gt;
&lt;h2&gt;Application to no-negatives&amp;nbsp;problem&lt;/h2&gt;
&lt;p&gt;To make connection between the no-negatives and noisy-labeling problems, one can remodel the former as one where all unlabeled examples are considered to actually be negative examples (&lt;span class="math"&gt;\(y^{\prime}_0 = 0\)&lt;/span&gt;). This relabeling gives a correct label to all examples in the original training set &lt;span class="math"&gt;\(\textbf{E}\)&lt;/span&gt; (where &lt;span class="math"&gt;\(y = y^{\prime}_0 = 1\)&lt;/span&gt;) as well as to all truly-negative examples (where &lt;span class="math"&gt;\(y = y^{\prime}_0 = 0\)&lt;/span&gt;). However, all positive examples not in &lt;span class="math"&gt;\(\textbf{E}\)&lt;/span&gt; are now incorrectly labeled (they are assigned &lt;span class="math"&gt;\(y^{\prime}_0 = 0\)&lt;/span&gt;): This new labeling &lt;span class="math"&gt;\(y^{\prime}_0\)&lt;/span&gt; is noisy, with &lt;span class="math"&gt;\(\alpha = Pr(y^{\prime}_0 =0 \vert y =1)\)&lt;/span&gt; and &lt;span class="math"&gt;\(\beta = Pr(y^{\prime}_0 =1 \vert y = 0 ) = 0\)&lt;/span&gt;. We can now apply the Blum and Michael approach: We first approximate &lt;span class="math"&gt;\(C^{\prime}\)&lt;/span&gt; as above, making use of the particular noisy label we have access to. Second, we minimize the approximated &lt;span class="math"&gt;\(C^{\prime}\)&lt;/span&gt; over some class of hypotheses &lt;span class="math"&gt;\(\{h\}\)&lt;/span&gt;. This will in general return a non-uniform hypothesis (i.e., one that now makes use of the information contained in &lt;span class="math"&gt;\(\textbf{E}\)&lt;/span&gt;).&lt;/p&gt;
&lt;h2&gt;Hybrid noisy-logistic approach of Lee and Liu (plus a&amp;nbsp;tweak)&lt;/h2&gt;
&lt;p&gt;The &lt;span class="math"&gt;\(C \propto C^{\prime}\)&lt;/span&gt; result is slick and provides a rigorous method for attacking the no-negatives problem. Unfortunately, &lt;span class="math"&gt;\(C^{\prime}\)&lt;/span&gt; is not convex, and as a consequence it can be difficult to minimize for large &lt;span class="math"&gt;\(\vert \textbf{F} \vert\)&lt;/span&gt; &amp;#8212; in fact, its minimization is &lt;span class="caps"&gt;NP&lt;/span&gt;-hard. To mitigate this issue, Lee and Liu combine the noisy relabeling idea &amp;#8212; now well-motivated by the Blum and Michael analysis &amp;#8212; with logistic regression. They also suggest a particular re-weighting of the observed samples. However, we think that their particular choice of weighting is not very well-motivated, and we suggest here that one should instead pick an optimal weighting through consideration of a cross-validation set. With this approach, the method&amp;nbsp;becomes:&lt;/p&gt;
&lt;p&gt;​1) As above, assign examples in &lt;span class="math"&gt;\(\textbf{E}\)&lt;/span&gt; label &lt;span class="math"&gt;\(y^{\prime} = 1\)&lt;/span&gt; and examples in &lt;span class="math"&gt;\(\textbf{F} - \textbf{E}\)&lt;/span&gt; label &lt;span class="math"&gt;\(y^{\prime} = 0\)&lt;/span&gt;.&lt;br&gt;
2) Construct the weighted logistic cost&amp;nbsp;function
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{7}
J(h; \rho) \equiv -\frac{1}{\vert \textbf{E} \vert}\sum_{i=1}^{\vert \textbf{E} \vert}  
\rho y^{\prime}_i \log(h_i) + (1-\rho) (1-y^{\prime}_i) \log(1- h_i) + \frac{\Lambda}{2}\sum_j T_j^2,
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
with &lt;span class="math"&gt;\(\rho \in [0,1]\)&lt;/span&gt;, a re-weighting factor. (Lee and Liu suggest&lt;span class="math"&gt;\(^2\)&lt;/span&gt; using &lt;span class="math"&gt;\(\rho = 1-\frac{\vert \textbf{E} \vert}{\vert \textbf{F} \vert}\)&lt;/span&gt;).&lt;br&gt;
3) Minimize &lt;span class="math"&gt;\(J\)&lt;/span&gt;. By evaluating performance on a cross-validation set using your favorite criteria, optimize &lt;span class="math"&gt;\(\rho\)&lt;/span&gt; and &lt;span class="math"&gt;\(\Lambda\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2&gt;Toy&amp;nbsp;example&lt;/h2&gt;
&lt;p&gt;Here, we provide a toy system that allows for a sense of how the latter method discussed above works in practice. Given is a set of &lt;span class="math"&gt;\(60\)&lt;/span&gt; grid points in the plane, which can be added/subtracted individually to the positive training set (&lt;span class="math"&gt;\(\textbf{E}\)&lt;/span&gt;, green fill) by mouse click (a few are selected by default). The remaining points are considered to not be in the training set, but are relabeled as negative examples &amp;#8212; this introduces noise, as described above. Clicking compute returns the &lt;span class="math"&gt;\(h\)&lt;/span&gt; values for each grid point, determined by minimizing the weighted cost function &lt;span class="math"&gt;\(J\)&lt;/span&gt; above: Here, we use the features &lt;span class="math"&gt;\(\{1,x,y,x^2,xy,\)&lt;/span&gt; &lt;span class="math"&gt;\(y^2,x^3, x^2 y,\)&lt;/span&gt; &lt;span class="math"&gt;\(x y^2, y^3\}\)&lt;/span&gt; to characterize each point. Those points with &lt;span class="math"&gt;\(h\)&lt;/span&gt; values larger than &lt;span class="math"&gt;\(0.5\)&lt;/span&gt; (i.e., those the hypothesis estimates as positive) are outlined in black. We have found that by carefully choosing the &lt;span class="math"&gt;\(\rho\)&lt;/span&gt; and &lt;span class="math"&gt;\(\Lambda\)&lt;/span&gt; values (often to be large and small, respectively), one can get a good fit to most training sets. By eye, the optimal weighting seems to often be close &amp;#8212; but not necessarily equal to &amp;#8212; the value suggested by Lee and&amp;nbsp;Liu.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fig. 1: Interactive weighted noisy-no-negatives solver. Click &amp;#8220;compute&amp;#8221; to run logistic regression.&lt;/em&gt;
[&lt;span class="caps"&gt;NOTE&lt;/span&gt;:  new site does not yet support processing - I hope to reinsert the interactive object here as soon as&amp;nbsp;possible].&lt;/p&gt;
&lt;h2&gt;Discussion&lt;/h2&gt;
&lt;p&gt;In this note, we have discussed methods for tackling classification sans negative examples &amp;#8212; a problem that we found perplexing at first sight. It is interesting that standard logistic regression returns a default-positive result for such problems, while the two latter methods we discussed here are based on assuming that all points in &lt;span class="math"&gt;\(\textbf{F} - \textbf{E}\)&lt;/span&gt; are negatives. In fact, this assumption seems to be the essence of all the other methods referenced in Lee and Liu&amp;#8217;s paper. Ultimately, these methods will only work if the training set provides a good sampling of the truly-positive space. If this is the case, then &amp;#8220;defocusing&amp;#8221; a bit, or blurring one&amp;#8217;s eyes, will give a good sense of where the positive space sits. In the noisy-logistic approach, a good choice of &lt;span class="math"&gt;\(\rho\)&lt;/span&gt; and &lt;span class="math"&gt;\(\Lambda\)&lt;/span&gt; should effect a good result. Of course, when the training set does not sample the full positive space well, one can still use this approach to get a good approximation for the outline of the subspace&amp;nbsp;sampled.&lt;/p&gt;
&lt;h2&gt;Footnotes&lt;/h2&gt;
&lt;p&gt;&lt;span class="math"&gt;\([1]\)&lt;/span&gt;: The target function &lt;span class="math"&gt;\(y\)&lt;/span&gt; provides the unique minimum of &lt;span class="math"&gt;\(C\)&lt;/span&gt;. Therefore, choosing &lt;span class="math"&gt;\(C\)&lt;/span&gt; as our cost function and minimizing it over some class of hypotheses &lt;span class="math"&gt;\(\{h\}\)&lt;/span&gt; should return a reasonable estimate for &lt;span class="math"&gt;\(y\)&lt;/span&gt; (indeed, if &lt;span class="math"&gt;\(y\)&lt;/span&gt; is in the search class, we will find&amp;nbsp;it).&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\([2]\)&lt;/span&gt;: Lee and Liu justify their weighting suggestion on the basis that it means that a randomly selected positive example contributes with expected weight &lt;span class="math"&gt;\(&amp;gt;0.5\)&lt;/span&gt; (see their paper). Yet, other weighting choices give even larger expected weights to the positive examples, so this is a poor justification. Nevertheless, their weighting choice does have the nice feature that the positive and negative spaces are effectively sampled with equal frequency. If optimizing over &lt;span class="math"&gt;\(\rho\)&lt;/span&gt; is too resource-costly for some application, using their weighting suggestion may be reasonable for this&amp;nbsp;reason.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Methods, Theory"></category><category term="methods"></category></entry><entry><title>NBA week 6 results, week 7 predictions, intro to dash</title><link href="https://efavdb.com/nba-week-6-predictions-week-7-predictions" rel="alternate"></link><published>2014-12-19T10:44:00-08:00</published><updated>2014-12-19T10:44:00-08:00</updated><author><name>Dustin McIntosh</name></author><id>tag:efavdb.com,2014-12-19:/nba-week-6-predictions-week-7-predictions</id><summary type="html">&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/12/week6Table.png"&gt;&lt;img alt="week6Table" src="https://efavdb.com/wp-content/uploads/2014/12/week6Table.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(37/55 = 67.2%\)&lt;/span&gt; accuracy this week. Summary by point spread is at&amp;nbsp;right.&lt;/p&gt;
&lt;p&gt;This week, we have posted a beta of our &lt;a href="http://efavdb.github.io/nba-dash"&gt;&lt;span class="caps"&gt;NBA&lt;/span&gt; dashboard&lt;/a&gt;.  A screenshot is given below &amp;#8212; the interactive version is linked to in our page&amp;#8217;s header.  We developed the tool using the programming language &lt;a href="https://processing.org/"&gt;processing …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/12/week6Table.png"&gt;&lt;img alt="week6Table" src="https://efavdb.com/wp-content/uploads/2014/12/week6Table.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(37/55 = 67.2%\)&lt;/span&gt; accuracy this week. Summary by point spread is at&amp;nbsp;right.&lt;/p&gt;
&lt;p&gt;This week, we have posted a beta of our &lt;a href="http://efavdb.github.io/nba-dash"&gt;&lt;span class="caps"&gt;NBA&lt;/span&gt; dashboard&lt;/a&gt;.  A screenshot is given below &amp;#8212; the interactive version is linked to in our page&amp;#8217;s header.  We developed the tool using the programming language &lt;a href="https://processing.org/"&gt;processing&lt;/a&gt; and displayed on the web using &lt;a href="http://processingjs.org/"&gt;processing.js&lt;/a&gt;. Our thanks to &lt;a href="http://annmh.com/"&gt;Ann Hermundstad&lt;/a&gt; for introducing us to this tool, which we have really enjoyed!   The wheel design of the dashboard was inspired by Jawbone&amp;#8217;s correlated food &lt;a href="https://jawbone.com/blog/jawbone-up-common-pairings/"&gt;visual&lt;/a&gt;, by &lt;a href="https://twitter.com/eminomura"&gt;Emi Nomura&lt;/a&gt;.  We think this approach does a good job of quickly conveying the state of the league: both what has happened in prior games already, and what we think will happen in upcoming&amp;nbsp;games.&lt;/p&gt;
&lt;p&gt;Predictions for week 7 are now up.  They can now also be viewed on the dashboard using the third&amp;nbsp;&amp;#8220;Mode&amp;#8221;.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://efavdb.github.io/nba-dash"&gt;&lt;img alt="NBADashScreenShot" src="https://efavdb.com/wp-content/uploads/2014/12/NBADashScreenShot.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="NBA prediction project"></category><category term="NBA"></category><category term="programming"></category></entry><entry><title>NBA week 5 summary, week 6 predictions</title><link href="https://efavdb.com/nba-week-5-summary-week-6-predictions" rel="alternate"></link><published>2014-12-12T15:39:00-08:00</published><updated>2014-12-12T15:39:00-08:00</updated><author><name>Dustin McIntosh</name></author><id>tag:efavdb.com,2014-12-12:/nba-week-5-summary-week-6-predictions</id><summary type="html">&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/12/week5Table.png"&gt;&lt;img alt="week5Table" src="https://efavdb.com/wp-content/uploads/2014/12/week5Table.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Another pretty good result this week: &lt;span class="math"&gt;\(36/52 = 69.2%\)&lt;/span&gt;. Accuracy by spread is at&amp;nbsp;right.&lt;/p&gt;
&lt;p&gt;A quick note on accuracy as an evaluation metric: We actually predict significant fluctuations in performance of our model from week to week. If we estimate our prediction accuracy at 70%, then for a …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/12/week5Table.png"&gt;&lt;img alt="week5Table" src="https://efavdb.com/wp-content/uploads/2014/12/week5Table.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Another pretty good result this week: &lt;span class="math"&gt;\(36/52 = 69.2%\)&lt;/span&gt;. Accuracy by spread is at&amp;nbsp;right.&lt;/p&gt;
&lt;p&gt;A quick note on accuracy as an evaluation metric: We actually predict significant fluctuations in performance of our model from week to week. If we estimate our prediction accuracy at 70%, then for a week with &lt;span class="math"&gt;\(n\approx 50\)&lt;/span&gt; games we expect a distribution of accuracies from week to week with standard deviation given by the binomial distribution: &lt;span class="math"&gt;\(\sigma = \sqrt{n p (1-p)}/n \approx 6.5%\)&lt;/span&gt;. This is a rather broad distribution - we might expect results ranging from &lt;span class="math"&gt;\(60%\)&lt;/span&gt; to &lt;span class="math"&gt;\(80%\)&lt;/span&gt; to be quite common from week to week. For &lt;span class="math"&gt;\(n \approx 15\)&lt;/span&gt; games (as in each row of our tables comparing by point spread), the standard deviation increases by a factor of &lt;span class="math"&gt;\(\approx 2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We have a functioning visualization tool for our &lt;span class="caps"&gt;NBA&lt;/span&gt; content, but are still working on loading it up to our server. Expect that, and a post about our difficulties, to show up soon. Our week 6 predictions are now&amp;nbsp;up.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="NBA prediction project"></category><category term="NBA"></category></entry><entry><title>NBA week 4 summary, week 5 predictions</title><link href="https://efavdb.com/nba-week-4-week-5-predictions" rel="alternate"></link><published>2014-12-05T23:30:00-08:00</published><updated>2014-12-05T23:30:00-08:00</updated><author><name>Dustin McIntosh</name></author><id>tag:efavdb.com,2014-12-05:/nba-week-4-week-5-predictions</id><summary type="html">&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/12/week4Table1.png"&gt;&lt;img alt="week4Table" src="https://efavdb.com/wp-content/uploads/2014/12/week4Table1.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Another good week for the &lt;span class="caps"&gt;NBA&lt;/span&gt; predictor: correct predictions for 39/53 = 73.6% of the games played.  This is an excellent result considering our algorithm had a difficult time cross-validating with this week’s results from last year.  Also, the 76ers won a game - there goes one of our …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/12/week4Table1.png"&gt;&lt;img alt="week4Table" src="https://efavdb.com/wp-content/uploads/2014/12/week4Table1.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Another good week for the &lt;span class="caps"&gt;NBA&lt;/span&gt; predictor: correct predictions for 39/53 = 73.6% of the games played.  This is an excellent result considering our algorithm had a difficult time cross-validating with this week’s results from last year.  Also, the 76ers won a game - there goes one of our sure-thing&amp;nbsp;predictions.&lt;/p&gt;
&lt;p&gt;Predictions for next week are up.  We are also working on a nice dynamic visualization for our &lt;span class="caps"&gt;NBA&lt;/span&gt; content - coming&amp;nbsp;soon!&lt;/p&gt;</content><category term="NBA prediction project"></category><category term="NBA"></category></entry><entry><title>NBA week 3 summary, week 4 predictions</title><link href="https://efavdb.com/nba-week-3-summary-week-4-predictions" rel="alternate"></link><published>2014-11-28T21:39:00-08:00</published><updated>2014-11-28T21:39:00-08:00</updated><author><name>Dustin McIntosh</name></author><id>tag:efavdb.com,2014-11-28:/nba-week-3-summary-week-4-predictions</id><summary type="html">&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/11/week3Table.png"&gt;&lt;img alt="week3Table" src="https://efavdb.com/wp-content/uploads/2014/11/week3Table.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Our &lt;span class="caps"&gt;NBA&lt;/span&gt; predictor did considerably better this week.  In fact, it did better than we even expected/hoped.  We had an accuracy of 40/51 = 78.4%, making up for last week’s&amp;nbsp;lull.  &lt;/p&gt;
&lt;p&gt;Week 4 predictions are now up.  This week set a high bar and our cross-validation indicates …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/11/week3Table.png"&gt;&lt;img alt="week3Table" src="https://efavdb.com/wp-content/uploads/2014/11/week3Table.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Our &lt;span class="caps"&gt;NBA&lt;/span&gt; predictor did considerably better this week.  In fact, it did better than we even expected/hoped.  We had an accuracy of 40/51 = 78.4%, making up for last week’s&amp;nbsp;lull.  &lt;/p&gt;
&lt;p&gt;Week 4 predictions are now up.  This week set a high bar and our cross-validation indicates that the coming week may be harder to predict..  but we are still&amp;nbsp;optimistic.  &lt;/p&gt;</content><category term="NBA prediction project"></category><category term="NBA"></category></entry><entry><title>NBA week 2 summary, week 3 predictions</title><link href="https://efavdb.com/nba-week-2-summary-week-3-predictions" rel="alternate"></link><published>2014-11-21T23:25:00-08:00</published><updated>2014-11-21T23:25:00-08:00</updated><author><name>Dustin McIntosh</name></author><id>tag:efavdb.com,2014-11-21:/nba-week-2-summary-week-3-predictions</id><summary type="html">&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/11/week2Table.png"&gt;&lt;img alt="week2Table" src="https://efavdb.com/wp-content/uploads/2014/11/week2Table.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This was an odd week in the &lt;span class="caps"&gt;NBA&lt;/span&gt; - plenty of upsets to go around.  You’re right, we’re making excuses - our &lt;span class="caps"&gt;NBA&lt;/span&gt; predictor had a tough week this round.  We had a total accuracy of 27/50 = 54%.  The summary by point spread is at right.  But seriously, who …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/11/week2Table.png"&gt;&lt;img alt="week2Table" src="https://efavdb.com/wp-content/uploads/2014/11/week2Table.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This was an odd week in the &lt;span class="caps"&gt;NBA&lt;/span&gt; - plenty of upsets to go around.  You’re right, we’re making excuses - our &lt;span class="caps"&gt;NBA&lt;/span&gt; predictor had a tough week this round.  We had a total accuracy of 27/50 = 54%.  The summary by point spread is at right.  But seriously, who would have predicted results like back-to-back Laker victories over Atlanta and Houston (aside from die-hard Kobe fans)?  Well, we certainly&amp;nbsp;didn’t.&lt;/p&gt;
&lt;p&gt;For the coming week we have further refined our algorithm in two ways.  First, we have now incorporated a team’s away performance into their predicted home performance and vice-versa.  And second, we now select the parameters from our model with a more thorough and systematic cross-validation method.  Predictions are now&amp;nbsp;up!&lt;/p&gt;</content><category term="NBA prediction project"></category><category term="NBA"></category></entry><entry><title>NBA week 1 summary, week 2 predictions</title><link href="https://efavdb.com/nba-week-1-summary-week-2-predictions" rel="alternate"></link><published>2014-11-14T23:22:00-08:00</published><updated>2014-11-14T23:22:00-08:00</updated><author><name>Dustin McIntosh</name></author><id>tag:efavdb.com,2014-11-14:/nba-week-1-summary-week-2-predictions</id><summary type="html">&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/11/week1Table.png"&gt;&lt;img alt="week1Table" src="https://efavdb.com/wp-content/uploads/2014/11/week1Table.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The first week of our &lt;span class="caps"&gt;NBA&lt;/span&gt; game outcome prediction experiment is in the books!   We had a prediction accuracy of 32/51 (= 63%).  A summary of the results broken down by game point spread is given at right. The point spreads shown are from the actual games, and the accuracy …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/11/week1Table.png"&gt;&lt;img alt="week1Table" src="https://efavdb.com/wp-content/uploads/2014/11/week1Table.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The first week of our &lt;span class="caps"&gt;NBA&lt;/span&gt; game outcome prediction experiment is in the books!   We had a prediction accuracy of 32/51 (= 63%).  A summary of the results broken down by game point spread is given at right. The point spreads shown are from the actual games, and the accuracy values shown are the fraction of correct predictions for games within the particular point spread range specified in that&amp;nbsp;row. &lt;/p&gt;
&lt;p&gt;Our accuracy this past week was significantly lower than we achieved on the 2013-2014 historical data.  In training on the first 800 games of that season we achieved approximately 70% accuracy on the remaining 430 games, after &lt;a href="http://efavdb.github.io/nba-weekly-predictions-up"&gt;incorporating momentum&lt;/a&gt; into the prediction.  This past week, our model had only 70 games from the current season to train on, relying on last year’s data to supplement the training.   For this reason, we feel 63% is actually pretty reasonable for our first week. In fact, we had a few very good days late in the week — and, of course, we are particularly proud to have correctly predicted &lt;a href="http://gfycat.com/ScaredConsciousGoldfinch"&gt;this&lt;/a&gt;.  &lt;/p&gt;
&lt;p&gt;Our predictions for the next week are now up as well.  This week we have further incorporated fatigue into the model - keeping track of the number of games each team has played in the five days preceding a game. When applied to the prior season data, we found that this feature helped boost our accuracy by about&amp;nbsp;2%.&lt;/p&gt;</content><category term="NBA prediction project"></category><category term="NBA"></category></entry><entry><title>Historic daily traffic patterns and the time scale of deviations</title><link href="https://efavdb.com/historic-daily-traffic-patterns-and-the-time-scale" rel="alternate"></link><published>2014-11-11T05:25:00-08:00</published><updated>2014-11-11T05:25:00-08:00</updated><author><name>Dustin McIntosh</name></author><id>tag:efavdb.com,2014-11-11:/historic-daily-traffic-patterns-and-the-time-scale</id><summary type="html">&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/11/WeekdayModeAverageDynamics.png"&gt;&lt;img alt="WeekdayModeAverageDynamics" src="https://efavdb.com/wp-content/uploads/2014/11/WeekdayModeAverageDynamics.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Daily traffic patterns can be decomposed into a historic average plus fluctuations from this&amp;nbsp;average.&lt;/p&gt;
&lt;p&gt;Here, we examine the daily dynamics of traffic as a function of weekday to provide the first piece of this puzzle. To do this, we average the time-dependent scores &lt;span class="math"&gt;\(c_i(t)\)&lt;/span&gt; for each day of …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/11/WeekdayModeAverageDynamics.png"&gt;&lt;img alt="WeekdayModeAverageDynamics" src="https://efavdb.com/wp-content/uploads/2014/11/WeekdayModeAverageDynamics.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Daily traffic patterns can be decomposed into a historic average plus fluctuations from this&amp;nbsp;average.&lt;/p&gt;
&lt;p&gt;Here, we examine the daily dynamics of traffic as a function of weekday to provide the first piece of this puzzle. To do this, we average the time-dependent scores &lt;span class="math"&gt;\(c_i(t)\)&lt;/span&gt; for each day of the week (see plot to the&amp;nbsp;right).&lt;/p&gt;
&lt;p&gt;As discussed &lt;a href="http://efavdb.github.io/daily-traffic-evolution-and-the-super-bowl"&gt;previously&lt;/a&gt;, modes one and two are general indicators of  overall traffic density and directional commuter density, respectively. Interestingly, we can clearly see systematic deviations in these two mode amplitudes across the days of the week: During rush hour, Mondays and Fridays have generally lower levels of traffic by both measures - most likely a consequence of people taking three-day weekends. In addition, if you look closely, you can actually see evidence of slackers taking off early on Friday&amp;nbsp;afternoons.&lt;/p&gt;
&lt;p&gt;Examining higher modes, the average signal dies away, eventually being lost in the noise. The final two panels in the figure have different y-axis scales - zoomed in to show the signal-to-noise ratio. By mode 100 the signal is already quite weak, but systematic deviations from zero can still be seen above the noise. However, by mode 1000, there are no systematic or significant deviations from zero.  These minor principal components likely represent rare events such as traffic accidents and thus are not reflected at all in the daily&amp;nbsp;averages.&lt;/p&gt;
&lt;p&gt;Is the historic average the best we can do for prediction?  To answer this we must examine the predictability of the fluctuations away from these means.  Here, we examine the autocorrelation&lt;span class="math"&gt;\(^1\)&lt;/span&gt; of the fluctuations from the mean to find the memory time scale of each principal component’s fluctuation memory.  This quantity characterizes the time scale over which we can extrapolate each amplitude deviation into the future (see plot&lt;span class="math"&gt;\(^2\)&lt;/span&gt; below). The time scale of initial decay of the correlation decreases monotonically with the mode index - the modes that capture the most variance have the longest memory: several&amp;nbsp;hours.&lt;/p&gt;
&lt;p&gt;This is excellent news: we can do better than just using the historic traffic patterns.  We can, in fact, project fluctuations away from the historic average several hours into the future and expect some&amp;nbsp;improvement.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/11/TuesAutocorr.png"&gt;&lt;img alt="TuesAutocorr" src="https://efavdb.com/wp-content/uploads/2014/11/TuesAutocorr.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[1] &lt;em&gt;The autocorrellation&lt;/em&gt;: The autocorrelation of a stochastic signal is a measure of its memory. In this particular case, \(R_{i}(t) \propto \mathbb{E}[\Delta c_i(s) \cdot \Delta c_i(s+t)]\) where the expectation value is over &lt;span class="math"&gt;\(s\)&lt;/span&gt; and &lt;span class="math"&gt;\(\Delta c_i(t) = c_i(t) - \langle c_i(t) \rangle\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;[2] &lt;em&gt;On this plot:&lt;/em&gt; The plot shown is for Tuesdays — other days have exhibit similar characteristics.  Note that mode 2 takes on negative autocorrelation values for a period of time (\(t =\) 6-9 hours). This is not surprising since mode 2, being an “odd” mode, tends to reverse sign between rush hours. The inset shows a few mean-subtracted signals for the first mode, ($\Delta c_1(t) $ above). The long-time correlations of these fluctuations are apparent&amp;nbsp;here.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Case studies"></category><category term="traffic"></category></entry><entry><title>NBA weekly predictions: up</title><link href="https://efavdb.com/nba-weekly-predictions-up" rel="alternate"></link><published>2014-11-08T00:09:00-08:00</published><updated>2014-11-08T00:09:00-08:00</updated><author><name>Dustin McIntosh</name></author><id>tag:efavdb.com,2014-11-08:/nba-weekly-predictions-up</id><summary type="html">&lt;p&gt;Every Friday, we will now be reporting our algorithm’s &lt;span class="caps"&gt;NBA&lt;/span&gt; win/loss outcome predictions for the following week.  These can be accessed by clicking the &lt;a href="http://efavdb.github.io/weekly-nba-predictions"&gt;“Weekly &lt;span class="caps"&gt;NBA&lt;/span&gt; predictions”&lt;/a&gt; link in the header section of our blog.  This week, we make predictions for a total of 51 games, training both …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Every Friday, we will now be reporting our algorithm’s &lt;span class="caps"&gt;NBA&lt;/span&gt; win/loss outcome predictions for the following week.  These can be accessed by clicking the &lt;a href="http://efavdb.github.io/weekly-nba-predictions"&gt;“Weekly &lt;span class="caps"&gt;NBA&lt;/span&gt; predictions”&lt;/a&gt; link in the header section of our blog.  This week, we make predictions for a total of 51 games, training both on last year’s results, as well as the 70 games that have already been played this&amp;nbsp;season.&lt;/p&gt;
&lt;p&gt;As the year progresses, we will continuously tweak our program for improved accuracy (we’re learning, too!).  This week, we implemented one significant change:  Our algorithm now takes into account the momentum of each team — in the form of its most recent 10 win/loss outcomes — when attempting to predict its subsequent win/loss result.  Interestingly, we found that having a home team’s algorithm track its own momentum led to no measurable improvement in its predictive power.  However, when fed the momentum features of the opposing, away team, the accuracy of the algorithm improved by five percent (as tested on 2013/14 stats).  Whether this is a consequence of some peculiarity of our algorithm’s structure, or instead a manifestation of some meaningful psychological effect, we do not&amp;nbsp;know.&lt;/p&gt;</content><category term="NBA prediction project"></category><category term="NBA"></category></entry><entry><title>Daily traffic evolution and the Super Bowl</title><link href="https://efavdb.com/daily-traffic-evolution-and-the-super-bowl" rel="alternate"></link><published>2014-10-31T04:02:00-07:00</published><updated>2014-10-31T04:02:00-07:00</updated><author><name>Dustin McIntosh</name></author><id>tag:efavdb.com,2014-10-31:/daily-traffic-evolution-and-the-super-bowl</id><summary type="html">&lt;p&gt;With an eye towards predicting traffic evolution, we begin by examining the time-dependence of the contribution from the first principal components on different days of the week. Traffic throughout the day \(\vert x(t) \rangle\) can be represented in the basis of principal components; \(\vert x(t) \rangle = \sum_{i …&lt;/p&gt;</summary><content type="html">&lt;p&gt;With an eye towards predicting traffic evolution, we begin by examining the time-dependence of the contribution from the first principal components on different days of the week. Traffic throughout the day \(\vert x(t) \rangle\) can be represented in the basis of principal components; \(\vert x(t) \rangle = \sum_{i} c_i(t) \vert \phi_i \rangle \)&lt;span class="math"&gt;\(^1\)&lt;/span&gt;, where &lt;span class="math"&gt;\(\vert \phi_i \rangle\)&lt;/span&gt; is the ith principle component. The coefficients &lt;span class="math"&gt;\(c_i(t)\)&lt;/span&gt;, sometimes called the “scores” of \(\vert x(t) \rangle\) in the basis of principal components, carry all of the&amp;nbsp;dynamics.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/10/ModeTDependence.png"&gt;&lt;img alt="ModeTDependence" src="https://efavdb.com/wp-content/uploads/2014/10/ModeTDependence.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The largest deviations in the traffic patterns (and of the scores) are during weekday rush hours (around 8 am and 5 pm) - see plot of the scores for several modes throughout Jan. 15.  There is an abundance of interesting information to be gleaned here.  First, note that, generally, the amplitude of the lowest modes is the largest, again, &lt;a href="https://efavdb.github.io/data-reduction-by-pca"&gt;as expected&lt;/a&gt;.  In addition, there appears to be some large wavelength structure (primarily correlated with the rush hours) sitting on top of a background of higher frequency noise.  Ignoring for the moment the noise, the modes generally come in two classes - something like “even” and “odd”.  That is, some fluctuate with the same sign in both the morning and evening, while others fluctuate with opposite&amp;nbsp;signs.&lt;/p&gt;
&lt;p&gt;Let us consider the first two modes in this plot.  The first mode is an even mode and we attribute this essentially to an overall shift in general speed throughout the system. Whenever there are more cars on the road, the speed generally decreases everywhere.  Indeed, this is the only mode that deviates significantly from zero at night and it deviates positive - the roads are generally completely clear at night and thus the speed is somewhat higher than average.  The second mode is an odd mode, which we attribute generally to directional traffic - when everyone is going to work it has one sign, when they are coming home it has the other.  We have previously &lt;a href="http://efavdb.github.io/traffic-patterns-of-the-year-2014-edition"&gt;visualized these two modes&lt;/a&gt; and, indeed, these concepts are reflected in their spatial structure: the first mode is generally uniform while the second has regions with either sign and many sections of highway are positive in one direction and negative in the&amp;nbsp;other.&lt;/p&gt;
&lt;p&gt;The time-dependence of these scores is remarkably consistent from week to week (See Wednesdays plot below of the scores for modes 1 and 2).  On the right, we plot the same quantities for several Sundays as well.  Not surprisingly, the fluctuations are smaller on Sundays than on weekdays, reflecting more homogeneous speeds in sparse traffic.  However, they are still reproducible from week to week - see Sundays Jan. 5, 12, 19, and 26 - apparently there is a slight slow-down around 6 pm.  Feb. 2 was Super Bowl Sunday and the traffic pattern differs qualitatively from other Sundays.  Remarkably, we can identify the time of the Super Bowl kickoff from this data - before the kickoff there is slightly more traffic than the average Sunday and immediately after,&amp;nbsp;less.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/10/WednesdayModes.png"&gt;&lt;img alt="WednesdayModes" src="https://efavdb.com/wp-content/uploads/2014/10/WednesdayModes.png"&gt;&lt;/a&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/10/SuperBowlModes.png"&gt;&lt;img alt="SuperBowlModes" src="https://efavdb.com/wp-content/uploads/2014/10/SuperBowlModes.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[1] &lt;em&gt;On projecting into principal components&lt;/em&gt;: In the original basis, our data reads \(\vert x(t) \rangle = \sum_j a_j(t) \vert \ell_j \rangle\), where the sum runs over all loop locations(there are some 2,000 loops in the Bay area) and \(\vert \ell_j \rangle\) is a unit fluctuation in speed at the location of loop &lt;span class="math"&gt;\(j\)&lt;/span&gt;. Changing bases, to the principal components \(\vert \phi_i \rangle\), \(\vert x(t) \rangle = \sum_{i,j} a_j(t) \vert \phi_i \rangle \langle \phi_i \vert \ell_j \rangle\) \(= \sum_i c_i(t) \vert \phi_i \rangle\) where \(c_i(t) = \sum_j a_j(t) \langle \phi_i \vert \ell_j \rangle\). The coefficient \(\langle \phi_i \vert \ell_j \rangle\) is often called the “loading” of \(\vert \ell_j \rangle\) into \(\vert \phi_i&amp;nbsp;\rangle\).&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Case studies"></category><category term="traffic"></category></entry><entry><title>NBA learner: 2013-14 warmup</title><link href="https://efavdb.com/nba-learner-2013-14-warmup" rel="alternate"></link><published>2014-10-30T22:07:00-07:00</published><updated>2014-10-30T22:07:00-07:00</updated><author><name>Dustin McIntosh</name></author><id>tag:efavdb.com,2014-10-30:/nba-learner-2013-14-warmup</id><summary type="html">&lt;p&gt;We’ve spent the last couple of evenings training some preliminary algorithms on the &lt;span class="caps"&gt;NBA&lt;/span&gt; 2013-14, regular season data, which we grabbed from &lt;a href="http://www.basketball-reference.com/"&gt;basketball-reference.com&lt;/a&gt;.  Each of the 30 &lt;span class="caps"&gt;NBA&lt;/span&gt; teams play 82 times a season, summing to 1230 games total — a sizable number that we can comfortably attempt to …&lt;/p&gt;</summary><content type="html">&lt;p&gt;We’ve spent the last couple of evenings training some preliminary algorithms on the &lt;span class="caps"&gt;NBA&lt;/span&gt; 2013-14, regular season data, which we grabbed from &lt;a href="http://www.basketball-reference.com/"&gt;basketball-reference.com&lt;/a&gt;.  Each of the 30 &lt;span class="caps"&gt;NBA&lt;/span&gt; teams play 82 times a season, summing to 1230 games total — a sizable number that we can comfortably attempt to model.  Here, we cover our first pass at the prediction problem, what we’ve learned so far, and challenges we’re looking forward to tackling&amp;nbsp;soon.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/10/NBAlearnV0Results.png"&gt;&lt;img alt="NBAlearnV0Results" src="https://efavdb.com/wp-content/uploads/2014/10/NBAlearnV0Results.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The first algorithm:  As mentioned in the prior post, we decided to initially train only on historical win-loss data triples of the form (home team, away team, y), where the Boolean y equals one if the home team won, zero otherwise.  For prediction, we use logistic classification:  We attempt to identify which teams team &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; would likely beat, were they to play them at home.  In order to accomplish this task, our logistic model has at its disposal a set of variable features characterizing each team:  a home feature vector &lt;span class="math"&gt;\(\textbf{H}_{\alpha}\)&lt;/span&gt; and an away feature vector &lt;span class="math"&gt;\(\textbf{A}_{\alpha}\)&lt;/span&gt;, each of length 10.  The model predicts a home team &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; win over away team &lt;span class="math"&gt;\(\beta\)&lt;/span&gt; probability of &lt;span class="math"&gt;\(h =  1/[1 + \exp(- \textbf{H}_{\alpha} \cdot \textbf{A}_{\beta})]\)&lt;/span&gt; &lt;span class="math"&gt;\( \in [0,1]\)&lt;/span&gt;.  In training, the model is initially fed random feature vectors, which are then relaxed to minimize the logistic cost function, &lt;span class="math"&gt;\(J \equiv&lt;div class="math"&gt;$$ - \sum_{i = 1}^{m} y_i \log h_i$$&lt;/div&gt;+ (1-y_i) \log (1 - h_i) $, where the sum is over all training examples.  The cost function $J\)&lt;/span&gt; heavily penalizes large mismatch between the actual outcome &lt;span class="math"&gt;\(y\)&lt;/span&gt; and the predicted outcome &lt;span class="math"&gt;\(h\)&lt;/span&gt; for any training example — we also added to this a suppression term that prevents&amp;nbsp;over-fitting.&lt;/p&gt;
&lt;p&gt;Results:  We trained the above model on the first 800 games of the 2013-14 season, and then tested the accuracy of the model on the remaining 430 games it did not train on.  Sample output is shown in the figure.  As you can see in the last line, the algorithm correctly predicted the outcome of 64% of these games.  As a first pass, this compares favorably to, for example, the accuracy of the predictions provided by &lt;a href="http://www.teamrankings.com/nba/betting-models/detailed-splits/"&gt;teamrankings.com&lt;/a&gt; (about 68% for the 2013-14 season). Further, after implementing a quick improvement to the first model above, basing predictions on prior score-differentials, rather than simply win-loss results, we managed to pop our accuracy up to 69% on the same data&amp;nbsp;set.&lt;/p&gt;
&lt;p&gt;Caveats &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; future directions:  Our comparison to teamrankings.com (&lt;span class="caps"&gt;TR&lt;/span&gt;) above isn’t really a fair one.  The reason is that our analysis was only carried out on the final 2/3rds of the last season, whereas &lt;span class="caps"&gt;TR&lt;/span&gt;’s average covered its entirety.  Early-season prediction is necessarily less accurate for all bettors, given the paucity of relevant data available at that time.  Nevertheless, we’re encouraged by our first attempts here.  To improve, we aim next to incorporate the information provided by prior seasons.  A closely related challenge will be to figure out how to intelligently weight data according to its age:  We want to be able to capture timely effects, like current momentum and injuries, while retaining all relevant long-term&amp;nbsp;trends.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="NBA prediction project"></category><category term="methods"></category><category term="NBA"></category></entry><entry><title>Announcing: NBA learner v0.1</title><link href="https://efavdb.com/announcing-nba-learner-v0-1" rel="alternate"></link><published>2014-10-29T02:04:00-07:00</published><updated>2014-10-29T02:04:00-07:00</updated><author><name>Dustin McIntosh</name></author><id>tag:efavdb.com,2014-10-29:/announcing-nba-learner-v0-1</id><summary type="html">&lt;p&gt;Tonight is the opening night of the 2014-15 &lt;span class="caps"&gt;NBA&lt;/span&gt; season.  This year, we
will be running a machine learning algorithm aimed at estimating
underlying features characterizing each team.  With these features, we
hope to identify interesting match-ups (including potential upsets),
similar team-playing-style categories, and win-loss probabilities for
future games.  As …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Tonight is the opening night of the 2014-15 &lt;span class="caps"&gt;NBA&lt;/span&gt; season.  This year, we
will be running a machine learning algorithm aimed at estimating
underlying features characterizing each team.  With these features, we
hope to identify interesting match-ups (including potential upsets),
similar team-playing-style categories, and win-loss probabilities for
future games.  As of now, the only source data that we intend to feed
our system will be win-loss results of completed games.  As the season
progresses, our algorithm will thus have more and more data informing
it —  It will be interesting to see if it can begin to provide
accurate predictions by the end of the season.  Stay tuned for
periodic updates on this&amp;nbsp;experiment!&lt;/p&gt;</content><category term="NBA prediction project"></category><category term="NBA"></category></entry><entry><title>Data reduction by PCA</title><link href="https://efavdb.com/data-reduction-by-pca" rel="alternate"></link><published>2014-10-23T23:52:00-07:00</published><updated>2014-10-23T23:52:00-07:00</updated><author><name>Dustin McIntosh</name></author><id>tag:efavdb.com,2014-10-23:/data-reduction-by-pca</id><summary type="html">&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/10/evals.png"&gt;&lt;img alt="evals" src="https://efavdb.com/wp-content/uploads/2014/10/evals.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here, we characterize the data compression benefits of projection onto a subset of the eigenvectors of our traffic system’s covariance matrix.  We address this compression from two different perspectives:  First, we consider the partial traces of the covariance matrix, and second we present visual comparisons of the actual vs …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/10/evals.png"&gt;&lt;img alt="evals" src="https://efavdb.com/wp-content/uploads/2014/10/evals.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here, we characterize the data compression benefits of projection onto a subset of the eigenvectors of our traffic system’s covariance matrix.  We address this compression from two different perspectives:  First, we consider the partial traces of the covariance matrix, and second we present visual comparisons of the actual vs. projected traffic&amp;nbsp;plots.&lt;/p&gt;
&lt;p&gt;Partial traces:  From the footnote to our last post, we have \(\text{Tr}(H^{-1}) \equiv \sum_i e_i = \sum_a \left (\delta v_a \right)^2\). That is, the trace of the covariance matrix tells us the net variance in our traffic system’s speed, summed over all loops. More generally&lt;span class="math"&gt;\(^1\)&lt;/span&gt;, the fraction of system variance contained within some subset of the modes is given by the eigenvalue sum over these modes, all divided by \(\text{Tr}(H^{-1})\).  The eigenvalues thus provide us with a simple method for quantifying the significance of any particular mode.  At right, is a log-log-plot of the fractional-variance-captured for each mode, ordered from largest to smallest (we also include analogous plots for the covariance matrix associated with just one week and one month&lt;span class="math"&gt;\(^2\)&lt;/span&gt;)  As shown, the eigenvalues decay like one over eigenvalue index at first, but eventually begin to decay much more quickly.   Only 5 modes are needed to capture 50% of the variance; 25 for 65%; 793 for&amp;nbsp;95%.&lt;/p&gt;
&lt;p&gt;Visualizations:  The above discussion suggests that the basic essence of a given set of traffic conditions is determined by only the first few modes, but that a large number might be needed to get correct all details.  We tested this conclusion by visually inspecting plots of projected traffic conditions (again, for Jan 15, at 5:30 pm), and comparing across number of modes retained.  The results are striking:  upon projecting the 2,000+ original features to only 25, minimal error appears to be introduced.  Further, the error that does occur tends to be highly localized to the particularly slow regions, where projected speeds are overestimates (&lt;em&gt;e.g.&lt;/em&gt;, the traffic jams east/south-bound out of Oakland).  Increasing the mode count to 100 or greater, these problem spots are quickly ameliorated, and the error is no longer systematic in slow regions (see&amp;nbsp;insets).&lt;/p&gt;
&lt;p&gt;Conclusions:   The data provided by the &lt;span class="caps"&gt;PEMS&lt;/span&gt; system is highly redundant — as anticipated — in the sense that traffic conditions can be determined from far fewer measurements than it provides.  If other states wanted to replicate this system, they could probably get away with reducing the number of measures by at least one order of magnitude per mile of highway.  For our part, we intend to project our data onto the top 10% of the modes, or fewer:  We anticipate that this will provide minimal loss, but substantial&amp;nbsp;speedups.&lt;/p&gt;
&lt;p&gt;[1]  &lt;em&gt;Partial eigenvalue sums, physics perspective:  &lt;/em&gt;Consider suppressing all modes that you don’t want to include in a projection. This can be done by setting the energies of these modes to &lt;span class="math"&gt;\(\infty\)&lt;/span&gt;, which will result in their corresponding \(H^{-1}\) eigenvalues going to zero.  When this altered system is thermally driven, its variance will again be given its covariance trace.  This altered system trace is precisely equal to the retained mode partial sum of the original&amp;nbsp;matrix.&lt;/p&gt;
&lt;p&gt;[2] &lt;em&gt;Sampling time dependence of covariance matrix:  &lt;/em&gt;The first figure above shows the mode variance ratio for the first 1000 principal components over three time scales: 1 week, 1 month, and 2014 year to date (~9.5 months). Notice that the plots become more shallow given a longer sampling period. This is because the larger data sets exhibit a more diverse class of fluctuations, and more modes are needed to capture&amp;nbsp;these.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Methods"></category><category term="traffic"></category><category term="methods"></category></entry><entry><title>Traffic patterns of the year: 2014 edition</title><link href="https://efavdb.com/traffic-patterns-of-the-year-2014-edition" rel="alternate"></link><published>2014-10-21T23:47:00-07:00</published><updated>2014-10-21T23:47:00-07:00</updated><author><name>Dustin McIntosh</name></author><id>tag:efavdb.com,2014-10-21:/traffic-patterns-of-the-year-2014-edition</id><summary type="html">&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/10/Mode1.png"&gt;&lt;img alt="Mode1" src="https://efavdb.com/wp-content/uploads/2014/10/Mode1.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/10/Mode2.png"&gt;&lt;img alt="Mode2" src="https://efavdb.com/wp-content/uploads/2014/10/Mode2.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As we mentioned in the last post, there are currently over 2000 active speed loop detectors within the Bay Area highway system.  The information provided by these loops is often highly redundant because speeds at neighboring sites typically differ little from one another.  This observation suggests that a higher level …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/10/Mode1.png"&gt;&lt;img alt="Mode1" src="https://efavdb.com/wp-content/uploads/2014/10/Mode1.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/10/Mode2.png"&gt;&lt;img alt="Mode2" src="https://efavdb.com/wp-content/uploads/2014/10/Mode2.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As we mentioned in the last post, there are currently over 2000 active speed loop detectors within the Bay Area highway system.  The information provided by these loops is often highly redundant because speeds at neighboring sites typically differ little from one another.  This observation suggests that a higher level, “macro” picture of traffic conditions could provide more insight:  Rather than stating the speed at each detector, we might instead offer info like “101S is rather slow right now”.   In fact, we aim to characterize traffic conditions as efficiently as possible.  To move towards this goal, we have carried out a principal component analysis (&lt;span class="caps"&gt;PCA&lt;/span&gt;)&lt;span class="math"&gt;\(^1\)&lt;/span&gt; of the full 2014 (year to date) &lt;span class="caps"&gt;PEMS&lt;/span&gt; data&amp;nbsp;set.&lt;/p&gt;
&lt;p&gt;As described in [1] below, &lt;span class="caps"&gt;PCA&lt;/span&gt; provides us with a slick, automated method for identifying the most common “traffic patterns” or “modes” that get excited in our system.  By adding together these patterns — with appropriate time-specific amplitudes — we can reconstruct the site-by-site traffic conditions observed at any particular moment.  Importantly, summing over only the most significant modes will provide us with a system-tailored, minimal-loss method of data compression that will simplify our later prediction analysis. We will discuss this compression benefit further in the next post. Here, we present the two dominant modes of the Bay Area traffic system (see figures above). Notice that the first is fairly uniform, which presumably captures some nearly-site-independent changes in mean speed associated with night vs. daytime driving. In contrast, the second mode captures some interesting structure, showing slowdowns for some highways/directions and speedups for others. Evidently, this structure is the second most highly exhibited pattern in the Bay Area system; We couldn’t have intuited this pattern, but it has been captured automatically via our &lt;span class="caps"&gt;PCA&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;[1]  *Statistical physics of &lt;span class="caps"&gt;PCA&lt;/span&gt;:  *  One way of thinking about &lt;span class="caps"&gt;PCA&lt;/span&gt; as applied here is to imagine that the traffic system is harmonic.  That is, we suppose that the traffic dynamics observed can be characterized by an energy cost function that is quadratic in the speeds of the different loops, measured relative to their average values, &lt;span class="math"&gt;\(E = \frac{\beta^{-1}}{2} \delta \textbf{v}^{T}  \cdot H \cdot \delta \textbf{v}.\)&lt;/span&gt;   Here, &lt;span class="math"&gt;\(\delta v_i = v_i - \langle v_i \rangle $ and $H\)&lt;/span&gt; is a matrix Hamiltonian.  Under some effective, &lt;a href="http://en.wikipedia.org/wiki/Partition_function_(statistical_mechanics)#Canonical_partition_function"&gt;thermal driving&lt;/a&gt;, the pair correlation for two sites will be given by &lt;span class="math"&gt;\(\langle \delta v_a  \delta v_b \rangle \equiv&lt;div class="math"&gt;$$ \frac{1}{Z} \int_{{\delta \textbf{v}_i }} e^{- \frac{1}{2} \delta \textbf{v}^{T}  \cdot H \cdot \delta \textbf{v}} \delta v_a  \delta v_b =$$&lt;/div&gt; H^{-1}_{ab}\)&lt;/span&gt;.  It is this pair correlation function that is measured when one carries out a &lt;span class="caps"&gt;PCA&lt;/span&gt; analysis, and the matrix &lt;span class="math"&gt;\(H^{-1}\)&lt;/span&gt; is called the covariance matrix.  Its eigenvectors are the modes of the system — the independent traffic patterns that we discuss above.  The low lying modes are those with a larger &lt;span class="math"&gt;\(H^{-1}\)&lt;/span&gt; eigenvalue.  These have low energy, are consequently often highly excited, and generally dominate the traffic conditions that we&amp;nbsp;observe.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Case studies"></category><category term="Machine Learning"></category><category term="traffic"></category></entry><entry><title>Obtaining and visualizing traffic data</title><link href="https://efavdb.com/obtaining-and-visualizing-traffic-data" rel="alternate"></link><published>2014-10-16T22:07:00-07:00</published><updated>2014-10-16T22:07:00-07:00</updated><author><name>Dustin McIntosh</name></author><id>tag:efavdb.com,2014-10-16:/obtaining-and-visualizing-traffic-data</id><summary type="html">&lt;p&gt;In our first set of posts here, we explore the possibility of using historical traffic data to train a machine learning algorithm capable of predicting near-term highway conditions — say, up to an hour into the future, at any given time.  To try our hand at this, we will be working …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In our first set of posts here, we explore the possibility of using historical traffic data to train a machine learning algorithm capable of predicting near-term highway conditions — say, up to an hour into the future, at any given time.  To try our hand at this, we will be working with publicly available data provided by the &lt;a href="http://pems.dot.ca.gov/"&gt;California Performance Measurement System (&lt;span class="caps"&gt;PEMS&lt;/span&gt;)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/10/Jan15_1730.png"&gt;&lt;img alt="Jan15_1730" src="https://efavdb.com/wp-content/uploads/2014/10/Jan15_1730.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The data provided by &lt;span class="caps"&gt;PEMS&lt;/span&gt; takes the form of time-averaged speed measurements for a large set of points throughout the California highway system (&lt;span class="math"&gt;\(\gtrsim 2000\)&lt;/span&gt;  in the Bay Area alone). These speeds are measured using devices called “inductive-loop detectors&lt;span class="math"&gt;\(^1\)&lt;/span&gt;” that are embedded just below the pavement at each site of interest. The same sort of devices are used at traffic lights to detect waiting vehicles: If you have ever noticed what looks like a circular or rectangular cut in the concrete at a traffic light, that’s what that is — &lt;a href="https://www.youtube.com/watch?v=8GAacxGiV4A"&gt;informative youtube video on how bikers might more easily trigger these&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As we dissect the traffic data, we will require a visualization tool. We have developed a tool to do this from scratch in Python&lt;span class="math"&gt;\(^2\)&lt;/span&gt;. This tool plots by color the speed of traffic along each highway. The figure above provides an example (traffic conditions for Jan 15 2014 at 5:30 pm) on top of a silhouette background of the Bay Area (courtesy of &lt;a href="http://www.lesterlee.org/"&gt;Lester Lee&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Stay tuned for updates on this and other related&amp;nbsp;projects!&lt;/p&gt;
&lt;p&gt;[1] &lt;em&gt;Aside on inductive loop detectors&lt;/em&gt;: Inductive detectors are essentially large wire loops (solenoids) that are constantly being driven by an alternating current source. When a large metallic object (car) is above the loop, the voltage needed to drive the current changes (see below).  This effect allows the loops to infer vehicle proximity.  In order to estimate speeds, average vehicle-loop crossing times are combined with predetermined average vehicle lengths.  [To see why the voltage across a loop changes as a car passes, you can play with these equations:  &lt;span class="math"&gt;\( V  = L \partial_t I\)&lt;/span&gt;, &lt;span class="math"&gt;\(\Phi = L I\)&lt;/span&gt;, &lt;span class="math"&gt;\(\nabla \times \textbf{E} = - \partial_t \textbf{B}\)&lt;/span&gt;, &lt;span class="math"&gt;\(\nabla \times \textbf{H} = \textbf{J}_f\)&lt;/span&gt;.  See also &lt;a href="http://en.wikipedia.org/wiki/Electromagnet#Magnetic_core"&gt;link&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;[2] &lt;em&gt;Aside on plotting algorithm&lt;/em&gt;: Traffic data on the &lt;span class="caps"&gt;PEMS&lt;/span&gt; website is provided via downloadable text files (1 file per day). Each file provides average traffic speed data, for each five minute window period in a day, for each functioning detector. The detectors themselves are identified with a 6-digit &lt;span class="caps"&gt;ID&lt;/span&gt; number. The latitude and longitude of these detectors, as well as which highway they are on, the direction of the highway, and their absolute mile marker position, are located in separate meta data files. We simply plot a line between each adjacent pair of detectors on a given highway, with color determined by the average speed of the two. North-South / East-West highway counterparts are separated by a small space by shifting their position perpendicular to the local highway tangent vector at each point. Missing data is imputed via the value of the nearest functional&amp;nbsp;detector.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Case studies"></category><category term="programming"></category><category term="traffic"></category></entry></feed>