<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Distill</title>
    <link>https://distill.pub</link>
    <atom:link href="https://distill.pub/rss.xml" rel="self" type="application/rss+xml"/>
    <description>Homepage articles from Distill</description>
    <lastBuildDate>Tue, 02 Aug 2022 21:49:04 -0400</lastBuildDate>
    <image>
      <title>Distill</title>
      <url>https://distill.pub/favicon.png</url>
      <link>https://distill.pub</link>
    </image>
    <language>en-us</language>
    <ttl>60</ttl>
    <item>
      <title>Understanding Convolutions on Graphs</title>
      <link>https://distill.pub/2021/understanding-gnns</link>
      <description>Understanding the building blocks and design choices of graph neural networks.</description>
      <guid>https://distill.pub/2021/understanding-gnns</guid>
      <pubDate>Thu, 02 Sep 2021 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>A Gentle Introduction to Graph Neural Networks</title>
      <link>https://distill.pub/2021/gnn-intro</link>
      <description>What components are needed for building learning algorithms that leverage the structure and properties of graphs?</description>
      <guid>https://distill.pub/2021/gnn-intro</guid>
      <pubDate>Thu, 02 Sep 2021 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>Distill Hiatus</title>
      <link>https://distill.pub/2021/distill-hiatus</link>
      <description>After five years, Distill will be taking a break.</description>
      <guid>https://distill.pub/2021/distill-hiatus</guid>
      <pubDate>Fri, 02 Jul 2021 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>Adversarial Reprogramming of Neural Cellular Automata</title>
      <link>https://distill.pub/selforg/2021/adversarial</link>
      <description>Reprogramming Neural CA to exhibit novel behaviour, using adversarial attacks.</description>
      <guid>https://distill.pub/selforg/2021/adversarial</guid>
      <pubDate>Thu, 06 May 2021 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>Weight Banding</title>
      <link>https://distill.pub/2020/circuits/weight-banding</link>
      <description>Weights in the final layer of common visual models appear as horizontal bands. We investigate how and why.</description>
      <guid>https://distill.pub/2020/circuits/weight-banding</guid>
      <pubDate>Thu, 08 Apr 2021 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>Branch Specialization</title>
      <link>https://distill.pub/2020/circuits/branch-specialization</link>
      <description>When a neural network layer is divided into multiple branches, neurons self-organize into coherent groupings.</description>
      <guid>https://distill.pub/2020/circuits/branch-specialization</guid>
      <pubDate>Mon, 05 Apr 2021 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>Multimodal Neurons in Artificial Neural Networks</title>
      <link>https://distill.pub/2021/multimodal-neurons</link>
      <description>We report the existence of multimodal neurons in artificial neural networks, similar to those found in the human brain.</description>
      <guid>https://distill.pub/2021/multimodal-neurons</guid>
      <pubDate>Thu, 04 Mar 2021 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>Self-Organising Textures</title>
      <link>https://distill.pub/selforg/2021/textures</link>
      <description>Neural Cellular Automata learn to generate textures, exhibiting surprising properties.</description>
      <guid>https://distill.pub/selforg/2021/textures</guid>
      <pubDate>Thu, 11 Feb 2021 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>Visualizing Weights</title>
      <link>https://distill.pub/2020/circuits/visualizing-weights</link>
      <description>We present techniques for visualizing, contextualizing, and understanding neural network weights.</description>
      <guid>https://distill.pub/2020/circuits/visualizing-weights</guid>
      <pubDate>Thu, 04 Feb 2021 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>Curve Circuits</title>
      <link>https://distill.pub/2020/circuits/curve-circuits</link>
      <description>Reverse engineering the curve detection algorithm from InceptionV1 and reimplementing it from scratch.</description>
      <guid>https://distill.pub/2020/circuits/curve-circuits</guid>
      <pubDate>Sat, 30 Jan 2021 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>High-Low Frequency Detectors</title>
      <link>https://distill.pub/2020/circuits/frequency-edges</link>
      <description>A family of early-vision neurons reacting to directional transitions from high to low spatial frequency.</description>
      <guid>https://distill.pub/2020/circuits/frequency-edges</guid>
      <pubDate>Wed, 27 Jan 2021 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>Naturally Occurring Equivariance in Neural Networks</title>
      <link>https://distill.pub/2020/circuits/equivariance</link>
      <description>Neural networks naturally learn many transformed copies of the same feature, connected by symmetric weights.</description>
      <guid>https://distill.pub/2020/circuits/equivariance</guid>
      <pubDate>Tue, 08 Dec 2020 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>Understanding RL Vision</title>
      <link>https://distill.pub/2020/understanding-rl-vision</link>
      <description>With diverse environments, we can analyze, diagnose and edit deep reinforcement learning models using attribution.</description>
      <guid>https://distill.pub/2020/understanding-rl-vision</guid>
      <pubDate>Tue, 17 Nov 2020 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>Communicating with Interactive Articles</title>
      <link>https://distill.pub/2020/communicating-with-interactive-articles</link>
      <description>Examining the design of interactive articles by synthesizing theory from disciplines such as education, journalism, and visualization.</description>
      <guid>https://distill.pub/2020/communicating-with-interactive-articles</guid>
      <pubDate>Fri, 11 Sep 2020 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>Thread: Differentiable Self-organizing Systems</title>
      <link>https://distill.pub/2020/selforg</link>
      <description>A collection of articles and comments with the goal of understanding how to design robust and general purpose self-organizing systems.</description>
      <guid>https://distill.pub/2020/selforg</guid>
      <pubDate>Thu, 27 Aug 2020 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>Self-classifying MNIST Digits</title>
      <link>https://distill.pub/2020/selforg/mnist</link>
      <description>Training an end-to-end differentiable, self-organising cellular automata for classifying MNIST digits.</description>
      <guid>https://distill.pub/2020/selforg/mnist</guid>
      <pubDate>Thu, 27 Aug 2020 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>Curve Detectors</title>
      <link>https://distill.pub/2020/circuits/curve-detectors</link>
      <description>Part one of a three part deep dive into the curve neuron family.</description>
      <guid>https://distill.pub/2020/circuits/curve-detectors</guid>
      <pubDate>Wed, 17 Jun 2020 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>Exploring Bayesian Optimization</title>
      <link>https://distill.pub/2020/bayesian-optimization</link>
      <description>How to tune hyperparameters for your machine learning model using Bayesian optimization.</description>
      <guid>https://distill.pub/2020/bayesian-optimization</guid>
      <pubDate>Tue, 05 May 2020 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>An Overview of Early Vision in InceptionV1</title>
      <link>https://distill.pub/2020/circuits/early-vision</link>
      <description>An overview of all the neurons in the first five layers of InceptionV1, organized into a taxonomy of &#39;neuron groups.&#39;</description>
      <guid>https://distill.pub/2020/circuits/early-vision</guid>
      <pubDate>Wed, 01 Apr 2020 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>Visualizing Neural Networks with the Grand Tour</title>
      <link>https://distill.pub/2020/grand-tour</link>
      <description>By focusing on linear dimensionality reduction, we show how to visualize many dynamic phenomena in neural networks.</description>
      <guid>https://distill.pub/2020/grand-tour</guid>
      <pubDate>Mon, 16 Mar 2020 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>Thread: Circuits</title>
      <link>https://distill.pub/2020/circuits</link>
      <description>What can we learn if we invest heavily in reverse engineering a single neural network?</description>
      <guid>https://distill.pub/2020/circuits</guid>
      <pubDate>Tue, 10 Mar 2020 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>Zoom In: An Introduction to Circuits</title>
      <link>https://distill.pub/2020/circuits/zoom-in</link>
      <description>By studying the connections between neurons, we can find meaningful algorithms in the weights of neural networks.</description>
      <guid>https://distill.pub/2020/circuits/zoom-in</guid>
      <pubDate>Tue, 10 Mar 2020 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>Growing Neural Cellular Automata</title>
      <link>https://distill.pub/2020/growing-ca</link>
      <description>Training an end-to-end differentiable, self-organising cellular automata model of morphogenesis, able to both grow and regenerate specific patterns.</description>
      <guid>https://distill.pub/2020/growing-ca</guid>
      <pubDate>Tue, 11 Feb 2020 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>Visualizing the Impact of Feature Attribution Baselines</title>
      <link>https://distill.pub/2020/attribution-baselines</link>
      <description>Exploring the baseline input hyperparameter, and how it impacts interpretations of neural network behavior.</description>
      <guid>https://distill.pub/2020/attribution-baselines</guid>
      <pubDate>Fri, 10 Jan 2020 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>Computing Receptive Fields of Convolutional Neural Networks</title>
      <link>https://distill.pub/2019/computing-receptive-fields</link>
      <description>Detailed derivations and open-source code to analyze the receptive fields of convnets.</description>
      <guid>https://distill.pub/2019/computing-receptive-fields</guid>
      <pubDate>Mon, 04 Nov 2019 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>The Paths Perspective on Value Learning</title>
      <link>https://distill.pub/2019/paths-perspective-on-value-learning</link>
      <description>A closer look at how Temporal Difference Learning merges paths of experience for greater statistical efficiency</description>
      <guid>https://distill.pub/2019/paths-perspective-on-value-learning</guid>
      <pubDate>Mon, 30 Sep 2019 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>A Discussion of &#39;Adversarial Examples Are Not Bugs, They Are Features&#39;</title>
      <link>https://distill.pub/2019/advex-bugs-discussion</link>
      <description>Six comments from the community and responses from the original authors</description>
      <guid>https://distill.pub/2019/advex-bugs-discussion</guid>
      <pubDate>Tue, 06 Aug 2019 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>A Discussion of &#39;Adversarial Examples Are Not Bugs, They Are Features&#39;: Adversarial Example Researchers Need to Expand What is Meant by &#39;Robustness&#39;</title>
      <link>https://distill.pub/2019/advex-bugs-discussion/response-1</link>
      <description>The main hypothesis in Ilyas et al. (2019) happens to be a special case of a more general principle that is commonly accepted in the robustness to distributional shift literature</description>
      <guid>https://distill.pub/2019/advex-bugs-discussion/response-1</guid>
      <pubDate>Tue, 06 Aug 2019 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>A Discussion of &#39;Adversarial Examples Are Not Bugs, They Are Features&#39;: Robust Feature Leakage</title>
      <link>https://distill.pub/2019/advex-bugs-discussion/response-2</link>
      <description>An example project using webpack and svelte-loader and ejs to inline SVGs</description>
      <guid>https://distill.pub/2019/advex-bugs-discussion/response-2</guid>
      <pubDate>Tue, 06 Aug 2019 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>A Discussion of &#39;Adversarial Examples Are Not Bugs, They Are Features&#39;: Two Examples of Useful, Non-Robust Features</title>
      <link>https://distill.pub/2019/advex-bugs-discussion/response-3</link>
      <description>An example project using webpack and svelte-loader and ejs to inline SVGs</description>
      <guid>https://distill.pub/2019/advex-bugs-discussion/response-3</guid>
      <pubDate>Tue, 06 Aug 2019 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>A Discussion of &#39;Adversarial Examples Are Not Bugs, They Are Features&#39;: Adversarially Robust Neural Style Transfer</title>
      <link>https://distill.pub/2019/advex-bugs-discussion/response-4</link>
      <description>An experiment showing adversarial robustness makes neural style transfer work on a non-VGG architecture</description>
      <guid>https://distill.pub/2019/advex-bugs-discussion/response-4</guid>
      <pubDate>Tue, 06 Aug 2019 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>A Discussion of &#39;Adversarial Examples Are Not Bugs, They Are Features&#39;: Adversarial Examples are Just Bugs, Too</title>
      <link>https://distill.pub/2019/advex-bugs-discussion/response-5</link>
      <description>Refining the source of adversarial examples</description>
      <guid>https://distill.pub/2019/advex-bugs-discussion/response-5</guid>
      <pubDate>Tue, 06 Aug 2019 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>A Discussion of &#39;Adversarial Examples Are Not Bugs, They Are Features&#39;: Learning from Incorrectly Labeled Data</title>
      <link>https://distill.pub/2019/advex-bugs-discussion/response-6</link>
      <description>Section 3.2 of Ilyas et al. (2019) shows that training a model on only adversarial errors leads to non-trivial generalization on the original test set. We show that these experiments are a specific case of learning from errors.</description>
      <guid>https://distill.pub/2019/advex-bugs-discussion/response-6</guid>
      <pubDate>Tue, 06 Aug 2019 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>A Discussion of &#39;Adversarial Examples Are Not Bugs, They Are Features&#39;: Discussion and Author Responses</title>
      <link>https://distill.pub/2019/advex-bugs-discussion/original-authors</link>
      <description></description>
      <guid>https://distill.pub/2019/advex-bugs-discussion/original-authors</guid>
      <pubDate>Tue, 06 Aug 2019 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>Open Questions about Generative Adversarial Networks</title>
      <link>https://distill.pub/2019/gan-open-problems</link>
      <description>What we&#39;d like to find out about GANs that we don&#39;t know yet.</description>
      <guid>https://distill.pub/2019/gan-open-problems</guid>
      <pubDate>Tue, 09 Apr 2019 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>A Visual Exploration of Gaussian Processes</title>
      <link>https://distill.pub/2019/visual-exploration-gaussian-processes</link>
      <description>How to turn a collection of small building blocks into a versatile tool for solving regression problems.</description>
      <guid>https://distill.pub/2019/visual-exploration-gaussian-processes</guid>
      <pubDate>Tue, 02 Apr 2019 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>Visualizing memorization in RNNs</title>
      <link>https://distill.pub/2019/memorization-in-rnns</link>
      <description>Inspecting gradient magnitudes in context can be a powerful tool to see when recurrent units use short-term or long-term contextual understanding.</description>
      <guid>https://distill.pub/2019/memorization-in-rnns</guid>
      <pubDate>Mon, 25 Mar 2019 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>Activation Atlas</title>
      <link>https://distill.pub/2019/activation-atlas</link>
      <description>By using feature inversion to visualize millions of activations from an image classification network, we create an explorable activation atlas of features the network has learned and what concepts it typically represents.</description>
      <guid>https://distill.pub/2019/activation-atlas</guid>
      <pubDate>Wed, 06 Mar 2019 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>AI Safety Needs Social Scientists</title>
      <link>https://distill.pub/2019/safety-needs-social-scientists</link>
      <description>If we want to train AI to do what humans want, we need to study humans.</description>
      <guid>https://distill.pub/2019/safety-needs-social-scientists</guid>
      <pubDate>Tue, 19 Feb 2019 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>Distill Update 2018</title>
      <link>https://distill.pub/2018/editorial-update</link>
      <description>An Update from the Editorial Team</description>
      <guid>https://distill.pub/2018/editorial-update</guid>
      <pubDate>Tue, 14 Aug 2018 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>Differentiable Image Parameterizations</title>
      <link>https://distill.pub/2018/differentiable-parameterizations</link>
      <description>A powerful, under-explored tool for neural network visualizations and art.</description>
      <guid>https://distill.pub/2018/differentiable-parameterizations</guid>
      <pubDate>Wed, 25 Jul 2018 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>Feature-wise transformations</title>
      <link>https://distill.pub/2018/feature-wise-transformations</link>
      <description>A simple and surprisingly effective family of conditioning mechanisms.</description>
      <guid>https://distill.pub/2018/feature-wise-transformations</guid>
      <pubDate>Mon, 09 Jul 2018 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>The Building Blocks of Interpretability</title>
      <link>https://distill.pub/2018/building-blocks</link>
      <description>Interpretability techniques are normally studied in isolation. We explore the powerful interfaces that arise when you combine them -- and the rich structure of this combinatorial space.</description>
      <guid>https://distill.pub/2018/building-blocks</guid>
      <pubDate>Tue, 06 Mar 2018 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>Using Artificial Intelligence to Augment Human Intelligence</title>
      <link>https://distill.pub/2017/aia</link>
      <description>By creating user interfaces which let us work with the representations inside machine learning models, we can give people new tools for reasoning.</description>
      <guid>https://distill.pub/2017/aia</guid>
      <pubDate>Mon, 04 Dec 2017 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>Sequence Modeling with CTC</title>
      <link>https://distill.pub/2017/ctc</link>
      <description>A visual guide to Connectionist Temporal Classification, an algorithm used to train deep neural networks in speech recognition, handwriting recognition and other sequence problems.</description>
      <guid>https://distill.pub/2017/ctc</guid>
      <pubDate>Mon, 27 Nov 2017 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>Feature Visualization</title>
      <link>https://distill.pub/2017/feature-visualization</link>
      <description>How neural networks build up their understanding of images</description>
      <guid>https://distill.pub/2017/feature-visualization</guid>
      <pubDate>Tue, 07 Nov 2017 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>Why Momentum Really Works</title>
      <link>http://distill.pub/2017/momentum</link>
      <description>We often think of optimization with momentum as a ball rolling down a hill. This isn&#39;t wrong, but there is much more to the story.</description>
      <guid>http://distill.pub/2017/momentum</guid>
      <pubDate>Tue, 04 Apr 2017 16:00:00 -0400</pubDate>
    </item>
    <item>
      <title>Research Debt</title>
      <link>https://distill.pub/2017/research-debt</link>
      <description>Science is a human activity. When we fail to distill and explain research, we accumulate a kind of debt...</description>
      <guid>https://distill.pub/2017/research-debt</guid>
      <pubDate>Wed, 22 Mar 2017 20:0:0 Z</pubDate>
    </item>
    <item>
      <title>Experiments in Handwriting with a Neural Network</title>
      <link>http://distill.pub/2016/handwriting</link>
      <description>Several interactive visualizations of a generative model of handwriting. Some are fun, some are serious.</description>
      <guid>http://distill.pub/2016/handwriting</guid>
      <pubDate>Tue, 06 Dec 2016 15:00:00 -0500</pubDate>
    </item>
    <item>
      <title>Deconvolution and Checkerboard Artifacts</title>
      <link>http://distill.pub/2016/deconv-checkerboard</link>
      <description>When we look very closely at images generated by neural networks, we often see a strange checkerboard pattern of artifacts.</description>
      <guid>http://distill.pub/2016/deconv-checkerboard</guid>
      <pubDate>Mon, 17 Oct 2016 16:00:00 -0400</pubDate>
    </item>
    <item>
      <title>How to Use t-SNE Effectively</title>
      <link>http://distill.pub/2016/misread-tsne</link>
      <description>Although extremely useful for visualizing high-dimensional data, t-SNE plots can sometimes be mysterious or misleading.</description>
      <guid>http://distill.pub/2016/misread-tsne</guid>
      <pubDate>Thu, 13 Oct 2016 16:00:00 -0400</pubDate>
    </item>
    <item>
      <title>Attention and Augmented Recurrent Neural Networks</title>
      <link>http://distill.pub/2016/augmented-rnns</link>
      <description>A visual overview of neural attention, and the powerful extensions of neural networks being built on top of it.</description>
      <guid>http://distill.pub/2016/augmented-rnns</guid>
      <pubDate>Thu, 08 Sep 2016 16:00:00 -0400</pubDate>
    </item>
  </channel>
</rss>
