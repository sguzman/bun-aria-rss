<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/"><channel><title>IEEE Spectrum</title><link>https://spectrum.ieee.org/</link><description>IEEE Spectrum</description><atom:link href="https://spectrum.ieee.org/feeds/feed.rss" rel="self"></atom:link><language>en-us</language><lastBuildDate>Fri, 04 Nov 2022 20:47:09 -0000</lastBuildDate><image><url>https://spectrum.ieee.org/media-library/eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy8yNjg4NDUyMC9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTY5OTk5OTQzOX0.aimbeagNFKGtififsLPFvztNYGr1_NMvLOOT1mPOjEU/image.png?width=210</url><link>https://spectrum.ieee.org/</link><title>IEEE Spectrum</title></image><item><title>Introduction to the 5 Pillars of Data Acquisition</title><link>https://go.teledynelecroy.com/ieee_five_pillars_webinar</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/image.png?id=28133682&width=980"/><br/><br/><p>Join Teledyne SP Devices for a live webinar about the foundations of data acquisition. </p><p>Explore digitizer fundamentals and learn more about analog front-end (AFE), triggering, clocking, signal processing, and the use of general-purpose input/output (GPIO).</p><p><a href="https://go.teledynelecroy.com/ieee_five_pillars_webinar" rel="noopener noreferrer" target="_blank">Register now for this free webinar!</a></p>]]></description><pubDate>Fri, 04 Nov 2022 19:04:21 +0000</pubDate><guid>https://go.teledynelecroy.com/ieee_five_pillars_webinar</guid><category>Analog</category><category>Data acquisition</category><category>Digitizer</category><category>Signal processing</category><category>Teledyne</category><category>Type:webinar</category><dc:creator>Teledyne</dc:creator><media:content medium="image" type="image/png" url="https://assets.rbl.ms/28133682/origin.png"></media:content></item><item><title>Video Friday: Uncrewed</title><link>https://spectrum.ieee.org/video-friday-uncrewed</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-black-hawk-helicopter-with-the-darpa-logo-on-it-flies-over-the-desert-with-no-crew-on-board.jpg?id=32042319&width=1245&height=700&coordinates=0%2C106%2C0%2C107"/><br/><br/><p>Video Friday is your weekly selection of awesome robotics videos, collected by your friends at <em>IEEE Spectrum</em> robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please <a href="mailto:automaton@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday">send us your events</a> for inclusion.<br/></p><h5><a href="https://www.xprize.org/prizes/avatar/finals-testing">ANA Avatar XPRIZE Finals</a>: 4–5 November 2022, LOS ANGELES</h5><h5><a href="https://corl2022.org/">CoRL 2022</a>: 14–18 December 2022, AUCKLAND, NEW ZEALAND</h5><p>Enjoy today’s videos!</p><hr/><div style="page-break-after: always"><span style="display:none"> </span></div><p>Sikorsky, a Lockheed Martin company, and the Defense Advanced Research Projects Agency (DARPA) have successfully demonstrated to the U.S. Army for the first time how an uninhabited Black Hawk helicopter flying autonomously can safely and reliably perform internal and external cargo-resupply missions and a rescue operation.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="d9abae4d109ee3fce8ed2b965fe420f3" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/dYcq_pzLsjA?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.lockheedmartin.com/en-us/news/features/2022/sikorsky-and-darpa-demonstrate-future-battlefield-logistics-missions-with-autonomous-utility-helicopter.html">Lockheed Martin</a> ]</p><div class="horizontal-rule"></div><blockquote><em>Taking inspiration from nature, SEAS researchers designed a new type of soft, robotic gripper that uses a collection of thin tentacles to entangle and ensnare objects, similar to how jellyfish collect stunned prey. Alone, individual tentacles, or filaments, are weak. But together, the collection of filaments can grasp and securely hold heavy and oddly shaped objects. The gripper relies on simple inflation to wrap around objects and doesn’t require sensing, planning, or feedback control.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="12c40ac113bfff56ebc0c521ccb697ad" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/SayuM8E_WaQ?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.seas.harvard.edu/news/2022/10/tentacle-robot-can-gently-grasp-fragile-objects">Harvard</a> ]</p><div class="horizontal-rule"></div><p>Agility Robotics’ Digit does not have bird legs. Birds have robot legs.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="20ad7b3f4a398a0743393a474cf20afb" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/9B2CVZe0jFE?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://agilityrobotics.com/">Agility Robotics</a> ]</p><div class="horizontal-rule"></div><blockquote><em>At TRI, we are developing robotic capabilities with the goal of improving the quality of everyday life for all. To reach this goal, we define “challenge tasks” that are exciting to work on, that drive our development towards general purpose robot capabilities, and that allow for rigorous quantitative testing.</em><br/><em>Autonomous order fulfillment in grocery stores is a particularly good way to drive our development of mobile manipulation capabilities because it encompasses a host of difficult challenges for robots, including perceiving and manipulating a large variety of objects, navigating an ever-changing environment, and reacting to unexpected circumstances.</em></blockquote><p class="shortcode-media shortcode-media-vimeo">
<iframe class="rm-shortcode" data-rm-shortcode-id="615ff17c43a4e64d0acf1de39f3aaaee" frameborder="0" height="480" scrolling="no" src="https://player.vimeo.com/video/764216312" width="100%"></iframe>
</p><p>[ <a href="https://medium.com/toyotaresearch/why-we-make-our-robots-shop-for-groceries-73ea8108caae">TRI</a> ]</p><p>Thanks, Lukas!</p><div class="horizontal-rule"></div><blockquote><em>On Halloween don’t come empty-handed to the MAB robotics’ basement. This is a spooky season, you’d better have a treat for the honey badger legged robot.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="1270de418fab51b0644eb0ecc72fa94a" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/UmIsr2h27BM?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.mabrobotics.pl/">MAB Robotics</a> ]</p><p>Thanks, Jakub!</p><div class="horizontal-rule"></div><p>The most important skill in humanoid robotics is knowing how to shove your robot in just the right way.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="96ffc6189a1be97e81d55457612edf02" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/_htcbZz_jd0?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="http://robots.ihmc.us/">IHMC</a> ]</p><div class="horizontal-rule"></div><p>If this is a humanlike workspace, I need to take Pilates or something.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="f32da978c858122b2d9fe191f70650e3" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/HN8osJJZH1k?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://apptronik.com/our-work/">Apptronik</a> ]</p><div class="horizontal-rule"></div><p>A Spooky Lab Tour of KIMLAB!</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="cabd836f241c7138b5f38e04dd4238a5" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/5O2HC9aiIYU?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://publish.illinois.edu/kimlab2020/">KIMLAB</a> ]</p><div class="horizontal-rule"></div><p>I know I say this every time, but I still cannot believe that this is a commercial system.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="26addb6aa355883564510bf5c179d185" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/3oYw035gYyk?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.tevel-tech.com/">Tevel</a> ]</p><div class="horizontal-rule"></div><p>Amazon has a prototype autonomous mobile robot, or AMR, for transporting oversize packages through warehouses. Its name is Bluebell.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="9ee65dca91695f456ae759254d5c48d2" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/Kget9r-0kGs?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.amazon.science/research-areas/robotics">Amazon</a> ]</p><div class="horizontal-rule"></div><blockquote><em>Using GPT3, Ameca answers user-submitted questions for you in the first installment of Ask Ameca!</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="8597176660fbe24a35986fed01b37092" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/fQ2JpkFBAOs?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.engineeredarts.co.uk/robot/ameca/">Engineered Arts</a> ]</p><div class="horizontal-rule"></div><blockquote><em>If insects can discern up from down while flying without fancy accelerometers, could we develop drones to do the same? In a new article published in </em>Nature<em>, scientists from Delft University of Technology (Netherlands) and Aix-Marseille University (France) describe how insects detect gravity. And how we could perhaps copy from nature.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="5d78a286f8cd3fcff5ed03433c1c631f" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/ugY0RTMjH1s?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.tudelft.nl/en/2022/tu-delft/how-flying-insects-and-drones-can-discern-up-from-down">TU Delft</a> ]</p><div class="horizontal-rule"></div><blockquote><em>We show a new method to handle fabric using a robot. Our approach relies on a finger-tip-size electroadhesive skin to lift fabric up. A pinch-type grasp is then used to securely hold the separated sheet of fabric, enabling easy manipulation thereafter.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="6426d340885a8f385b3f292c08309ab6" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/wBoLqbNxwtc?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://ieeexplore.ieee.org/document/9636095">Paper</a> ]</p><div class="horizontal-rule"></div><blockquote><em>We present FLEX-SDK: an open-source software development kit that allows creating a social robot from two simple tablet screens. FLEX-SDK involves tools for designing the robot face and its facial expressions, creating screens for input/output interactions, controlling the robot through a Wizard-of-Oz interface, and scripting autonomous interactions through a simple text-based programming interface.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="b4b3f502734dbfeb4090567916d07752" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/H19xNKsx6uo?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://dl.acm.org/doi/10.1145/3526113.3545707">Paper</a> ]</p><div class="horizontal-rule"></div><blockquote><em>D’Manus is a 10 DoF, low-cost, reliable prehensile hand.  It is fully 3D printable, and features integrated large-area ReSkin sensing.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="4564bde91a7c22a414e41e11a5f345d0" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/oiNdePCi_5k?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://sites.google.com/view/roboticsbenchmarks/platforms/dmanus">D'Manus</a> ]</p><div class="horizontal-rule"></div><p>10,000 cheese sticks per hour.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="951e43b0722a104f71e0c8d346a9e09b" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/KZOr5k2zSx8?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.kuka.com/food-processing-automation">Kuka</a> ]</p><div class="horizontal-rule"></div><blockquote><em>We present UltraBat, an interactive 3D side-scrolling game inspired by Flappy Bird, in which the game character, a bat, is physically levitated in mid-air using ultrasound. Players aim to navigate the bat through a stalagmite tunnel that scrolls to one side as the bat travels, which is implemented using a pin-array display to create a shape-changing passage.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="9346d97f664d0e4149cbabbed5b4b5d5" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/jMYAQzzQ_PI?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://dl.acm.org/doi/10.1145/3526114.3561344">UltraBat</a> ]</p><div class="horizontal-rule"></div><blockquote><em>The next generation of robots will rely on machine learning in one way or another. However, when machine learning algorithms (or their results) are deployed on robots in the real world, studying their safety is important. In this talk, I will summarize the findings of our recent review paper “Safe Learning in Robotics: From Learning-Based Control to Safe Reinforcement Learning”.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="cd41edfc17b2195a2457607d6485d088" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/g6eHhvHMSy8?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.dynsyslab.org/vision-news/">UofT</a> ]</p><div class="horizontal-rule"></div><blockquote><em>On October 20, 2022, Kimberly Hambuchen of NASA talked to Robotics students as a speaker in the Undergraduate Robotics Pathways & Careers Speaker Series, which aims to answer the question: “What can I do with a robotics degree?”</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="149910571bbe34a5d07981b8881c4e27" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/6mYRJf2Wbcs?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://robotics.umich.edu/academics/undergraduate/robotics-pathways-speaker-series/">Michigan Robotics</a> ]</p><div class="horizontal-rule"></div>]]></description><pubDate>Fri, 04 Nov 2022 16:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/video-friday-uncrewed</guid><category>Video friday</category><category>Robotics</category><dc:creator>Evan Ackerman</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-black-hawk-helicopter-with-the-darpa-logo-on-it-flies-over-the-desert-with-no-crew-on-board.jpg?id=32042319&amp;width=980"></media:content></item><item><title>Nominate a Colleague for an IEEE Major Award</title><link>https://spectrum.ieee.org/call-for-nominations-2024-awards</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-group-of-gold-ieee-medals-on-black-background.jpg?id=25536101&width=1245&height=700&coordinates=0%2C116%2C0%2C117"/><br/><br/><p>Each year IEEE pays tribute to technical professionals whose outstanding contributions have made a lasting impact on technology and the engineering profession for humanity. The <a href="https://corporate-awards.ieee.org/" rel="noopener noreferrer" target="_blank">IEEE Awards program</a> seeks nominations annually for IEEE's top awards—Medals, Recognitions, and Technical Field Awards—that are given on behalf of the <a href="https://corporate-awards.ieee.org/ieee-board-of-directors/" rel="noopener noreferrer" target="_blank">IEEE Board of Directors</a>.</p><p> You don’t have to be an IEEE member to receive, nominate, or endorse someone for an award.</p><p>Nominations for 2024 Medals and Recognitions will be open from 1 December to 15 June 2023. All nominations must be submitted through the IEEE Awards online portal set up for<a href="https://ieee.secure-platform.com/a/page/ieeemedals_recognitions_techfieldawards/ieee_medals" rel="noopener noreferrer" target="_blank"> Medals</a> and<a href="https://ieee.secure-platform.com/a/page/ieeemedals_recognitions_techfieldawards/ieee_recognitions" rel="noopener noreferrer" target="_blank"> Recognitions</a>.</p><p> These awards are presented at the annual IEEE Honors Ceremony. The <a href="https://corporate-awards.ieee.org/event/2023-vic-summit-honors-ceremony-gala/" rel="noopener noreferrer" target="_blank">2023 IEEE Vision, Innovation, and Challenges Summit and Honors Ceremony</a> will be held on 5 May at the <a href="https://www.google.com/aclk?sa=l&ai=DChcSEwjNsZ_9nMz6AhXDyZQJHQx2ATUYABAAGgJ5bQ&sig=AOD64_0ThHJE1TuoNVh0jJ06MaJNSld1qw&q&adurl&ved=2ahUKEwiboZb9nMz6AhVorokEHS53BB8Q0Qx6BAgEEAE" rel="noopener noreferrer" target="_blank">Hilton Atlanta</a>. Planning for the 2023 event is currently underway, and more information will be announced in the coming months.</p><p>The IEEE Awards Board has an ongoing initiative to increase diversity among the selection committees and candidates, including their technical discipline, geography, and gender. You can help by nominating a colleague for one of the following awards.</p><p><strong>IEEE Medal of Honor</strong></p><p>For an exceptional contribution or an extraordinary career in the IEEE fields of interest.</p><p><em>SPONSOR: </em><a href="https://www.ieeefoundation.org/" rel="noopener noreferrer" target="_blank"><em>IEEE Foundation</em></a></p><p><strong>IEEE Frances E. Allen Medal</strong></p><p>For innovative work in computing leading to lasting impact on other aspects of engineering, science, technology, or society.</p><p><em>SPONSOR: </em><a href="https://www.ibm.com/about" rel="noopener noreferrer" target="_blank"><em>IBM</em></a></p><p><strong>IEEE Alexander Graham Bell Medal</strong></p><p>For exceptional contributions to communications and networking sciences and engineering.</p><p><em>SPONSOR: </em><a href="https://www.bell-labs.com/" rel="noopener noreferrer" target="_blank"><em>Nokia Bell Labs</em></a></p><p><strong>IEEE Mildred Dresselhaus Medal</strong></p><p>For outstanding technical contributions in science and engineering of great impact to IEEE fields of interest.</p><p><em>SPONSOR:</em> <a href="https://about.google/" rel="noopener noreferrer" target="_blank"><em>Google, LLC</em></a></p><p><strong>IEEE Edison Medal</strong></p><p>For a career of meritorious achievement in electrical science, electrical engineering, or the electrical arts.</p><p><em>SPONSOR:</em> <a href="https://www.samsung.com/us/aboutsamsung/home/" rel="noopener noreferrer" target="_blank"><em>Samsung Electronics Co., Ltd.</em></a></p><p><strong>IEEE Medal for Environmental and Safety Technologies</strong></p><p>For outstanding accomplishments in the application of technology in the fields of interest of IEEE that improve the environment and/or public safety.</p><p><em>SPONSOR: </em><a href="https://global.toyota/en/" rel="noopener noreferrer" target="_blank"><em>Toyota Motor Corp.</em></a></p><p><strong>IEEE Founders Medal</strong></p><p>For outstanding contributions in the leadership, planning, and administration of affairs of great value to the electrical and electronics engineering profession.</p><p><em>SPONSOR: The </em><em>IEEE Richard and Mary Jo Stanley Memorial Fund of the IEEE Foundation </em></p><p><strong>IEEE Richard W. Hamming Medal</strong></p><p>For exceptional contributions to information sciences, systems, and technology.</p><p><em>SPONSOR: </em><a href="https://www.qualcomm.com/" rel="noopener noreferrer" target="_blank"><em>Qualcomm, Inc.</em></a></p><p><strong>IEEE Medal for Innovations in Healthcare Technology</strong></p><p>For exceptional contributions to technologies and applications benefitting health care, medicine, and the health sciences.</p><p><em>SPONSOR: </em><a href="https://www.embs.org/" rel="noopener noreferrer" target="_blank"><em>IEEE Engineering in Medicine and Biology Society</em></a></p><p><strong>IEEE Nick Holonyak, Jr. Medal for Semiconductor Optoelectronic</strong></p><p>For outstanding contributions to semiconductor optoelectronic devices and systems including high-efficiency semiconductor devices and electronics.</p><p><em>SPONSOR: Friends of Nick Holonyak, Jr. </em></p><p><strong>IEEE Jack S. Kilby Signal Processing Medal</strong></p><p>For outstanding achievements in signal processing.</p><p><em>SPONSOR: </em><a href="https://www.apple.com/business/" rel="noopener noreferrer" target="_blank"><em>Apple</em></a></p><p><strong>IEEE/RSE James Clerk Maxwell Medal</strong></p><p>For groundbreaking contributions that have had an exceptional impact on the development of electronics and electrical engineering or related fields.</p><p><em>SPONSOR: </em><a href="https://www.arm.com/" rel="noopener noreferrer" target="_blank"><em>ARM, Ltd.</em></a></p><p><strong>IEEE James H. Mulligan, Jr. Education Medal</strong></p><p>For a career of outstanding contributions to education in the fields of interest of IEEE.</p><p><em>SPONSORS: </em><a href="https://www.mathworks.com/" rel="noopener noreferrer" target="_blank"><em>MathWorks</em></a><em>, </em><a href="https://www.lockheedmartin.com/en-us/index.html" rel="noopener noreferrer" target="_blank"><em>Lockheed Martin Corp.</em></a><em>, and the </em><a href="https://www.ieee.org/communities/life-members/fund.html" rel="noopener noreferrer" target="_blank"><em>IEEE Life Members Fund</em></a></p><p><strong>IEEE Jun-ichi Nishizawa Medal</strong></p><p>For outstanding contributions to material and device science and technology, including practical application.</p><p><em>SPONSOR: The Jun-ichi Nishizawa Medal Fund</em></p><p><em></em><strong>IEEE Robert N. Noyce Medal</strong></p><p>For exceptional contributions to the microelectronics industry.</p><p><em>SPONSOR: </em><a href="https://www.intel.com/content/www/us/en/company-overview/company-overview.html" rel="noopener noreferrer" target="_blank"><em>Intel</em></a></p><p><strong>IEEE Dennis J. Picard Medal for Radar Technologies and Applications</strong></p><p>For outstanding accomplishments in advancing the fields of radar technologies and their applications.</p><p><em>SPONSOR: </em><a href="https://www.rtx.com/en" rel="noopener noreferrer" target="_blank"><em>Raytheon Technologies</em></a></p><p><strong>IEEE Medal in Power Engineering</strong></p><p>For outstanding contributions to the technology associated with the generation, transmission, distribution, application, and utilization of electric power for the betterment of society.</p><p><em>SPONSORS: </em><a href="https://ias.ieee.org/" rel="noopener noreferrer" target="_blank"><em>IEEE Industry Applications</em></a><em>, IEEE </em><a href="http://www.ieee-ies.org/" rel="noopener noreferrer" target="_blank"><em>Industrial Electronics</em></a><em>, IEEE </em><a href="https://www.ieee-pels.org/" rel="noopener noreferrer" target="_blank"><em>Power Electronics</em></a><em>, and IEEE </em><a href="https://www.ieee-pes.org/" rel="noopener noreferrer" target="_blank"><em>Power & Energy</em></a><em> societies</em></p><p><strong>IEEE Simon Ramo Medal</strong></p><p>For exceptional achievement in systems engineering and systems science.</p><p><em>SPONSOR: </em><a href="https://www.northropgrumman.com/" rel="noopener noreferrer" target="_blank"><em>Northrop Grumman Corp.</em></a></p><p><strong>IEEE John von Neumann Medal</strong></p><p>For outstanding achievements in computer-related science and technology.</p><p><em>SPONSOR: </em><a href="https://www.ibm.com/us-en/" rel="noopener noreferrer" target="_blank"><em>IBM</em></a></p><h2>IEEE Recognitions</h2><p><strong>IEEE Corporate Innovation Award</strong></p><p>For an outstanding innovation by an organization in an IEEE field of interest.</p><p><em>SPONSOR:</em><a href="https://www.ieee.org/" rel="noopener noreferrer" target="_blank"><em> IEEE</em></a></p><p><strong>IEEE Honorary Membership</strong></p><p>To individuals not members of the IEEE, who have rendered meritorious service to humanity in IEEE's designated fields of interest.</p><p><em>SPONSOR: </em><a href="https://www.ieee.org/" rel="noopener noreferrer" target="_blank"><em>IEEE</em></a></p><p><strong>IEEE Theodore W. Hissey Outstanding Young Professional Award</strong></p><p>Awarded to a young professional for contributions to the technical community and IEEE fields of interest.</p><p><em>SPONSOR: </em><a href="https://yp.ieee.org/" rel="noopener noreferrer" target="_blank"><em>IEEE Young Professionals</em></a><em>, and the </em><a href="https://www.photonicssociety.org/" rel="noopener noreferrer" target="_blank"><em>IEEE Photonics</em></a><em> and IEEE </em><a href="https://www.ieee-pes.org/" rel="noopener noreferrer" target="_blank"><em>Power & Energy</em></a><em> societies</em></p><h2>IEEE Service Awards</h2><p><strong>IEEE Richard M. Emberson Award</strong></p><p>For distinguished service advancing the technical objectives of the IEEE.</p><p><em>SPONSOR: </em><a href="https://www.ieee.org/about/volunteers/tab.html" rel="noopener noreferrer" target="_blank"><em>IEEE Technical Activities Board</em></a></p><p><strong>IEEE Haraden Pratt Award</strong></p><p>For outstanding volunteer service to the IEEE.</p><p><em>SPONSOR: </em><a href="https://www.ieeefoundation.org/" rel="noopener noreferrer" target="_blank"><em>IEEE Foundation</em></a></p><p>If you have questions, email awards@ieee.org or call +1 732 562 3844.</p>]]></description><pubDate>Thu, 03 Nov 2022 18:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/call-for-nominations-2024-awards</guid><category>Ieee news</category><category>Ieee medals</category><category>Ieee awards</category><category>Ieee medal of honor</category><category>Call for nominations</category><category>Type:ti</category><dc:creator>Leslie Russell</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-group-of-gold-ieee-medals-on-black-background.jpg?id=25536101&amp;width=980"></media:content></item><item><title>Can Autonomous Weapons Be Compatible With International Humanitarian Law?</title><link>https://spectrum.ieee.org/autonomous-weapons-law</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/abstract-illustration-of-green-connected-lines-the-silhouette-of-heads-and-a-globe.jpg?id=31988636&width=1245&height=700&coordinates=0%2C337%2C0%2C338"/><br/><br/><p><em><em>This article is part of our <a href="https://spectrum.ieee.org/collections/autonomous-weapons-challenges/" target="_self">Autonomous Weapons Challenges</a> series. The IEEE Standards Association is <a href="https://saforms.ieee.org/autonomous-weapons-participation/" rel="noopener noreferrer" target="_blank">looking for your feedback</a> on this topic, and has invites you to answer <a href="https://saforms.ieee.org/autonomous-weapons-participation/" rel="noopener noreferrer" target="_blank">these questions</a>.</em></em></p><p>
	The real world is anything but binary. It is fuzzy and indistinct, with lots of options and potential outcomes, full of complexity and nuance. Our societies create laws and cultural norms to provide and maintain some semblance of order, but such structures are often open to interpretation, and they shift and evolve over time.
</p><p>
	This fuzziness can be challenging for any autonomous system navigating the uncertainty of a human world—such as Alexa reacting to the wrong conversations, or self-driving cars being stymied by 
	<a href="https://www.archyde.com/tesla-hits-a-white-truck-again-with-two-passengers-rushing-to-icu-tesla-tesla-electric-car/" rel="noopener noreferrer" target="_blank">white trucks</a> and<a href="https://www.theverge.com/2021/5/14/22436584/waymo-driverless-stuck-traffic-roadside-assistance-video" rel="noopener noreferrer" target="_blank"> orange traffic cones</a>. But not having clarity on "right or wrong" is especially problematic when considering autonomous weapons systems (AWS).
</p><p>
	International Humanitarian Law (IHL) is the body of laws that govern international military conflicts, and they provide rules about how weapons should be used. The fundamentals of IHL were developed before the widespread use of personal computers, satellites, the Internet, and social media, and before private data became a commodity that could be accessed remotely and often without a person’s knowledge or consent. 
	<a href="https://www.stopkillerrobots.org/a-global-push/member-organisations/" rel="noopener noreferrer" target="_blank">Many groups</a> are concerned that the existing laws don’t cover the myriad issues that recent and emerging technologies have created, and the International Committee of the Red Cross, the <a href="https://www.icrc.org/en/mandate-and-mission" rel="noopener noreferrer" target="_blank">watchdog of IHL</a>, has <a href="https://www.icrc.org/en/document/icrc-position-autonomous-weapon-systems" rel="noopener noreferrer" target="_blank">recommended new, legally binding rules </a>to cover AWS.
</p><p>
<a href="https://documents-dds-ny.un.org/doc/UNDOC/GEN/G19/343/64/PDF/G1934364.pdf?OpenElement" rel="noopener noreferrer" target="_blank">Ethical principles</a> have been developed to help address gaps between changing cultural norms and technologies and established laws, but such principles also tend to be vague and difficult to translate into legal code. For example, even if everyone agrees on an ethical principle like minimizing bias in an autonomous system, how would that be programmed? Who would determine whether an algorithmic bias has been sufficiently “minimized” for the system to be deployed?
</p><p>
	All countries involved in the autonomous weapons systems (AWS) debate at the United Nations have stated that AWS must follow international law. However, they don’t agree on what these laws and ethics mean in practice, and there’s additional disagreement over whether some AWS capabilities must be preemptively banned in order to ensure that IHL is honored.
</p><h3>IHL, Emerging Technology, and AWS</h3><p>
	Much of the disagreement at the United Nations stems from the uncertainty surrounding the technology and how the technology will evolve in the future. Though existing weapons systems have some autonomous capabilities, and though there have been reports of 
	<a href="https://www.npr.org/2021/06/01/1002196245/a-u-n-report-suggests-libya-saw-the-first-battlefield-killing-by-an-autonomous-d" rel="noopener noreferrer" target="_blank">AWS being used in Libya</a> and <a href="https://www.csis.org/analysis/russia-probably-has-not-used-ai-enabled-weapons-ukraine-could-change" rel="noopener noreferrer" target="_blank">questions about AWS being used</a> in Ukraine, the extent to which AI and autonomy will change warfare remains unknown. Even when IHL mandates already exist, it’s unclear that AWS will be able to follow them: For example, can a machine be trained to reliably recognize when a combatant is injured or surrendering? Is it possible for a machine to learn the difference between a civilian and a combatant dressed as a civilian?
</p><p>
	Cyberthreats pose new risks to national security, and the ability of companies and governments to collect personal data is already a controversial legal and ethical issue. These risks are only exacerbated when paired with AWS, which could be biased, hacked, trained on bad data, or otherwise compromised as a result of weak regulations surrounding emerging technologies.
</p><p>
	Moreover, for AI systems to work, they typically need to be trained on huge data sets. But military conflict and battlefields can be chaotic and unpredictable, and large, reliable data sets may not exist. AWS may also be subject to greater adversarial manipulation, which, essentially, involves tricking the system into misunderstanding the situation–something that can be as easy to do as 
	<a href="https://www.theverge.com/2018/1/3/16844842/ai-computer-vision-trick-adversarial-patches-google" rel="noopener noreferrer" target="_blank">placing a sticker on or near an object</a>. Is it possible for AWS algorithms to receive sufficient training and supervision to ensure they won’t violate international laws, and who makes that decision?
</p><p>
	AWS are complex, with various people and organizations involved at different stages of development, and communication between designers and users of the systems may not exist. Additionally, the algorithms and AI software used in AWS may not have originally been intended for military use, or they may have been intended for the military but not for weapons specifically. To ensure the safety and reliability of AWS, new standards for testing, evaluation, verification, and validation are needed. And if an automated weapons system acts inappropriately or unexpectedly and causes unintended harm, will it be clear who is at fault?
</p><h3>Nonmilitary Use of AWS</h3><p>
	While certain international laws cover human rights issues during a war, separate laws cover human rights issues in all other circumstances. Simply prohibiting a weapons system from being used during wartime does not guarantee that the system can’t be used outside of military combat. For example, 
	<a href="https://theconversation.com/tear-gas-and-pepper-spray-are-chemical-weapons-so-why-can-police-use-them-140364" rel="noopener noreferrer" target="_blank">tear gas</a> has been classified as a chemical weapon and banned in warfare since 1925, but it remains legal for law enforcement to use for riot control.
</p><p>
	If new international laws are developed to regulate the wartime use of AI and autonomy in weapons systems, human rights violations committed outside of the scope of a military action could—and likely would—still occur. The latter could include actions by private security companies, police, border control agencies, and nonstate armed groups.
</p><p>
	Ultimately, in order to ensure that laws, policy, and ethics are well adapted to the new technologies of AWS—and that AWS are designed to better abide by international laws and norms—policymakers need to have a stronger understanding of the technical capabilities and limitations of the weapons, and how the weapons might be used.
</p><div class="ieee-sidebar-large">
<h3>What Do You Think?</h3>
<p>
	We want your feedback! To help bring clarity to these AWS discussions, the 
	<a href="https://standards.ieee.org/" rel="noopener noreferrer" target="_blank">IEEE Standards Association</a> convened an expert group in 2020, to consider the ethical and technical challenges of translating AWS principles into practice and what that might mean for future development and governance. Last year, the expert group published its findings in a report entitled “<a href="https://standards.ieee.org/wp-content/uploads/import/documents/other/ethical-technical-challenges-autonomous-weapons-systems.pdf" rel="noopener noreferrer" target="_blank">Ethical and Technical Challenges in the Development, Use, and Governance of Autonomous Weapons Systems</a>.” Many of the AWS challenges are similar to those arising in other fields that are developing autonomous systems. We expect and hope that IEEE members and readers of <em>IEEE Spectrum</em> will have insights from their own fields that can inform the discussion around AWS technologies.
</p>
<p>
	We’ve put together a <a href="https://saforms.ieee.org/autonomous-weapons-participation/" rel="noopener noreferrer" target="_blank">series of questions</a> in the <a href="https://standards.ieee.org/wp-content/uploads/import/documents/other/ethical-technical-challenges-autonomous-weapons-systems.pdf" rel="noopener noreferrer" target="_blank">Challenges document</a> that we hope you’ll answer, to help us better understand how people in other fields are addressing these issues. Autonomous capabilities will increasingly be applied to weapons systems, much as they are being applied in other realms, and we hope that by looking at the challenges in more detail, we can help establish effective technical solutions, while contributing to discussions about what can and should be legally acceptable. Your feedback will help us move toward this ultimate goal. <a href="https://saforms.ieee.org/autonomous-weapons-participation/" rel="noopener noreferrer" target="_blank">Public comments will be open through 7 December 2022</a>.
</p></div>]]></description><pubDate>Thu, 03 Nov 2022 13:24:56 +0000</pubDate><guid>https://spectrum.ieee.org/autonomous-weapons-law</guid><category>Autonomous weapons</category><category>Aws</category><category>Robotics</category><dc:creator>Ariel Conn</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/abstract-illustration-of-green-connected-lines-the-silhouette-of-heads-and-a-globe.jpg?id=31988636&amp;width=980"></media:content></item><item><title>What Does “Human Control” Over Autonomous Systems Mean?</title><link>https://spectrum.ieee.org/autonomous-weapons-control</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/conceptual-illustration-of-two-humanoid-heads-facing-away-from-each-other-with-a-pattern-of-nested-squares.jpg?id=32029419&width=1245&height=700&coordinates=0%2C511%2C0%2C512"/><br/><br/><p><em><em>This article is part of our <a href="https://spectrum.ieee.org/collections/autonomous-weapons-challenges/" target="_self">Autonomous Weapons Challenges</a> series. The IEEE Standards Association is <a href="https://saforms.ieee.org/autonomous-weapons-participation/" rel="noopener noreferrer" target="_blank">looking for your feedback</a> on this topic, and has invites you to answer <a href="https://saforms.ieee.org/autonomous-weapons-participation/" rel="noopener noreferrer" target="_blank">these questions</a>.</em></em></p><p>
<a href="https://spectrum.ieee.org/how-the-boeing-737-max-disaster-looks-to-a-software-developer" target="_self">Two Boeing 737 Max planes crashed</a> in 2018 and 2019 due to sensor failures that led to autopilot malfunctions that two human pilots were unable to overcome. Also in 2018, an Uber autonomous vehicle struck and killed a pedestrian in Arizona, even though a person in the car was supposed to be overseeing the system. These examples highlight many of the issues that arise when considering what “human control” over an autonomous system really means.
</p><p>
	The development of these autonomous technologies occurred within enormously complex bureaucratic frameworks. A huge number of people were involved—in engineering a number of autonomous capabilities to function within a single system, in determining how the systems would respond to an unknown or emergency situation, and in training people to oversee the systems. A failure in any of these steps could, and did, lead to a catastrophic failure in which the people overseeing the system weren’t able to prevent it from causing unintended harm.
</p><hr/><p>
	These examples underscore the basic human psychology that developers need to understand in order to design and test autonomous systems. Humans are <a href="https://www.bbc.com/future/article/20211019-why-we-place-too-much-trust-in-machines" rel="noopener noreferrer" target="_blank">prone to over-trusting machines</a> and become increasingly complacent the more they use a system and nothing bad happens. Humans are notoriously bad at maintaining the level of focus necessary to catch an error in such situations, typically <a href="https://slate.com/technology/2018/03/safety-drivers-attention-spans-might-slow-self-driving-car-progress.html" rel="noopener noreferrer" target="_blank">losing focus after about 20 minutes</a>. And the human response to an emergency situation can be <a href="https://www.tudelft.nl/en/2019/lr/unpredictable-training-better-prepares-pilots-for-emergencies/" rel="noopener noreferrer" target="_blank">unpredictable</a>.
</p><p>
	Ultimately, “human control” is hard to define and has become a controversial issue in discussions about autonomous weapons systems, with many similar phrases used in international debates, including “meaningful human control,” “human responsibility,” and “appropriate human judgment.” But regardless of the phrase that’s used, the problem remains the same: Simply assigning a human the task of overseeing an AWS may not prevent the system from doing something it shouldn’t, and it’s not clear who would be at fault.
</p><h3>Responsibility and Accountability</h3><p>
	Autonomous weapons systems can process data at speeds that far exceed a human’s cognitive capabilities, which means a human involved will need to know when to trust the data and when to question it.
</p><p>
	In the examples above, people were directly overseeing a single commercial system. In the very near future, a single soldier might be expected to monitor an entire swarm of hundreds of weaponized drones, with testing already taking place for soldiers. Each drone may be detecting and processing data in real time. If a human can’t keep up with a single autonomous system, they certainly wouldn’t be able to keep up with the data coming in from a swarm. Additional autonomous systems may thus be added to filter and package the data, introducing even more potential points of failure. Among other issues, this raises legal concerns, given that responsibility and accountability could quickly become unclear if the system behaves unexpectedly only after it’s been deployed.</p><h3>Human-Machine Teams</h3><p>
	Artificial intelligence often relies on machine learning, which can turn AI-based systems into black boxes, with the AI taking unexpected actions and leaving its designers and users uncertain as to why it did what it did. It remains unclear how humans working with AWS will respond to their machine partners or what type of training will be necessary to ensure the human understands the capabilities and limitations of the system. Human-machine teaming also presents challenges both in terms of training people to use the system and of developing a better understanding of the trust dynamic between humans and AWS. While the human-robot handoff may be a technical challenge in many fields, it quickly becomes a question of international humanitarian law if the handoff doesn’t go smoothly for a weapons system.
</p><p>
	Ensuring responsibility and accountability for AWS is a general point of agreement among those involved in the international debate. But without sufficient understanding of human psychology or how human-machine teams should work, is it reasonable to expect the human to be responsible and accountable for any unintended consequences of the system’s deployment?
</p><div class="ieee-sidebar-large">
<h3>What Do You Think?</h3>
<p>
	We want your feedback! To help bring clarity to these AWS discussions, the<a href="https://standards.ieee.org/" rel="noopener noreferrer" target="_blank"> IEEE Standards Association</a> convened an expert group in 2020, to consider the ethical and technical challenges of translating AWS principles into practice and what that might mean for future development and governance. Last year, the expert group published its findings in a report entitled<a href="https://standards.ieee.org/wp-content/uploads/import/documents/other/ethical-technical-challenges-autonomous-weapons-systems.pdf" rel="noopener noreferrer" target="_blank"> “<u>Ethical and Technical Challenges in the Development, Use, and Governance of Autonomous Weapons Systems</u></a>.” Many of the AWS challenges are similar to those arising in other fields that are developing autonomous systems. We expect and hope that IEEE members and readers of <em>IEEE Spectrum</em> will have insights from their own fields that can inform the discussion around AWS technologies.
</p><p>
We’ve put together a<a href="https://saforms.ieee.org/autonomous-weapons-participation/" rel="noopener noreferrer" target="_blank"> series of questions</a> on <a href="https://standards.ieee.org/wp-content/uploads/import/documents/other/ethical-technical-challenges-autonomous-weapons-systems.pdf" rel="noopener noreferrer" target="_blank">the Challenges document</a> that we hope you’ll answer, to help us better understand how people in other fields are addressing these issues. Autonomous capabilities will increasingly be applied to weapons systems, much as they are being applied in other technical realms, and we hope that by looking at the challenges in more detail, we can help establish effective technical solutions while contributing to discussions about what can and should be legally acceptable. Your feedback will help us move toward this ultimate goal.<a href="https://saforms.ieee.org/autonomous-weapons-participation/" rel="noopener noreferrer" target="_blank"> Public comments will be open through 7 December 2022</a>.
</p></div>]]></description><pubDate>Thu, 03 Nov 2022 13:20:08 +0000</pubDate><guid>https://spectrum.ieee.org/autonomous-weapons-control</guid><category>Autonomous weapons</category><category>Aws</category><category>Robotics</category><dc:creator>Ariel Conn</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/conceptual-illustration-of-two-humanoid-heads-facing-away-from-each-other-with-a-pattern-of-nested-squares.jpg?id=32029419&amp;width=980"></media:content></item><item><title>How Can We Make Sure Autonomous Weapons Are Used Responsibly?</title><link>https://spectrum.ieee.org/autonomous-weapons-trust</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/abstract-illustration-of-a-light-shining-through-overlapping-rectangles-of-different-sizes.jpg?id=31988677&width=1245&height=700&coordinates=0%2C326%2C0%2C327"/><br/><br/><p>
<em><em>This article is part of our <a href="https://spectrum.ieee.org/collections/autonomous-weapons-challenges/" target="_self">Autonomous Weapons Challenges</a> series. The IEEE Standards Association is <a href="https://saforms.ieee.org/autonomous-weapons-participation/" rel="noopener noreferrer" target="_blank">looking for your feedback</a> on this topic, and has invites you to answer <a href="https://saforms.ieee.org/autonomous-weapons-participation/" rel="noopener noreferrer" target="_blank">these questions</a>.</em></em>
</p><p>
	International discussions about autonomous weapons systems (AWS) often focus on a fundamental question: Is it legal for a machine to make the decision to take a human life? But woven into this question is another fundamental issue: Can an automated weapons system be trusted to do what it’s expected to do?
</p><p>
	If the technical challenges of developing and using AWS can’t be addressed, then the answer to both questions is likely “no.”
</p><hr/><h3>AI Challenges Are Magnified When Applied to Weapons </h3><p>
	Many of the known issues with AI and machine learning become even more problematic when associated with weapons. For example, AI systems could help process data from images far faster than human analysts can, and the majority of the results would be accurate. But the algorithms used for this functionality are known to introduce or exacerbate issues of bias and discrimination, targeting certain demographics more than others. Given that, is it reasonable to use image-recognition software to help humans identify potential targets?
</p><p>
	But concerns about the technical abilities of AWS extend beyond object recognition and algorithmic bias. Autonomy in weapons systems requires a slew of technologies, including sensors, communications, and onboard computing power, each of which poses its own challenges for developers. These components are often designed and programmed by different organizations, and it can be hard to predict how the components will function together within the system, as well as how they’ll react to a variety of real-world situations and adversaries.
</p><h3>Testing for Assurance and Risk</h3><p>
	It’s also not at all clear how militaries can test these systems to ensure the AWS will do what’s expected and comply with International Humanitarian Law. And yet militaries typically want weapons to be tested and proven to act consistently, legally, and without harming their own soldiers before the systems are deployed. If commanders don’t trust a weapons system, they likely won’t use it. But standardized testing is especially complicated for an AI program that can learn from its interactions in the field—in fact, such standardized testing for AWS simply doesn’t exist.
</p><p>
	We know how software updates can alter how a system behaves and may introduce bugs that cause a system to behave erratically. But an automated weapons system powered by AI may also update its behavior based on real-world experience, and changes to the AWS behavior could be much harder for users to track. New information that the system accesses in the field could even trigger it to start to shift away from its original goals.
</p><p>
	Similarly, cyberattacks and adversarial attacks pose a known threat, which developers try to guard against. But if an attack is successful, what would testing look like to identify that the system has been hacked, and how would a user know to implement such tests?
</p><h3>Physical Challenges of Autonomous Weapons</h3><p>
	Though recent advancements in artificial intelligence have led to greater concern about the use of AWS, the technical challenges of autonomy in weapons systems extends beyond AI. Physical challenges already exist for conventional weapons and for nonweaponized autonomous systems, but these same problems are further exacerbated and complicated in AWS.
</p><p>
	For example, many autonomous systems are getting smaller, even as their computational needs grow, including navigation, data acquisition and analysis, and decision making—and potentially all while out of communication with commanders. Can the automated weapons system maintain the necessary and legal functionality throughout the mission, even if communication is lost? How is data protected if the system falls into enemy hands?
</p><p>
	Issues similar to these may also arise with other autonomous systems, but the consequences of failure are magnified with AWS, and extra features will likely be necessary to ensure that, for example, a weaponized autonomous vehicle in the battlefield doesn’t violate International Humanitarian Law or mistake a friendly vehicle for an enemy target. Because these problems are so new, weapons developers and lawmakers will need to work with and learn from experts in the robotics space to be able to solve the technical challenges and create useful policy.
</p><p>
	There are many technical advances that will contribute to various types of weapons systems. Some will prove far more difficult to develop than expected, while others will likely be developed faster. That means AWS development won’t be a leap from conventional weapons systems to full autonomy, but will instead make incremental steps as new autonomous capabilities are developed. This could lead to a slippery slope where it’s unclear if a line has been crossed from acceptable use of technology to unacceptable. Perhaps the solution is to look at specific robotic and autonomous technologies as they’re developed and ask ourselves whether society would want a weapons system with this capability, or if action should be taken to prevent that from happening.
</p><div class="ieee-sidebar-large">
<h3>What Do You Think?</h3>
<p>
	We want your feedback! To help bring clarity to these AWS discussions, the 
		<a href="https://standards.ieee.org/" rel="noopener noreferrer" target="_blank">IEEE Standards Association</a> convened an expert group in 2020, to consider the ethical and technical challenges of translating AWS principles into practice and what that might mean for future development and governance. Last year, the expert group published its findings in a report entitled “<a href="https://standards.ieee.org/wp-content/uploads/import/documents/other/ethical-technical-challenges-autonomous-weapons-systems.pdf" rel="noopener noreferrer" target="_blank">Ethical and Technical Challenges in the Development, Use, and Governance of Autonomous Weapons Systems</a>.” Many of the AWS challenges are similar to those arising in other fields that are developing autonomous systems. We expect and hope that IEEE members and readers of <em>IEEE Spectrum</em> will have insights from their own fields that can inform the discussion around AWS technologies.
	</p>
We’ve put together a 
	<a href="https://saforms.ieee.org/autonomous-weapons-participation/" rel="noopener noreferrer" target="_blank">series of questions</a> in <a href="https://standards.ieee.org/wp-content/uploads/import/documents/other/ethical-technical-challenges-autonomous-weapons-systems.pdf" rel="noopener noreferrer" target="_blank">the Challenges document</a> that we hope you’ll answer, to help us better understand how people in other fields are addressing these issues. Autonomous capabilities will increasingly be applied to weapons systems, much as they are being applied in other technical realms, and we hope that by looking at the challenges in more detail, we can help establish effective technical solutions, while contributing to discussions about what can and should be legally acceptable. Your feedback will help us move toward this ultimate goal. <a href="https://saforms.ieee.org/autonomous-weapons-participation/" rel="noopener noreferrer" target="_blank">Public comments will be open through 7 December 2022</a>.
</div>]]></description><pubDate>Thu, 03 Nov 2022 13:16:52 +0000</pubDate><guid>https://spectrum.ieee.org/autonomous-weapons-trust</guid><category>Autonomous weapons</category><category>Aws</category><category>Robotics</category><dc:creator>Ariel Conn</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/abstract-illustration-of-a-light-shining-through-overlapping-rectangles-of-different-sizes.jpg?id=31988677&amp;width=980"></media:content></item><item><title>How Can We Talk About Autonomous Weapons?</title><link>https://spectrum.ieee.org/autonomous-weapons-challenges</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/abstract-illustration-of-overlapping-circles-and-squares.jpg?id=31988673&width=1245&height=700&coordinates=0%2C262%2C0%2C263"/><br/><br/><p>
<em>This article is part of our <a href="/collections/autonomous-weapons-challenges/" target="_blank">Autonomous Weapons Challenges</a> series. The IEEE Standards Association is <a href="https://saforms.ieee.org/autonomous-weapons-participation/" target="_blank">looking for your feedback</a> on this topic, and has invites you to answer <a href="https://saforms.ieee.org/autonomous-weapons-participation/" target="_blank">these questions</a>.</em></p><p>
	Lethal autonomous weapons systems can sound terrifying, but autonomy in weapons systems is far more nuanced and complicated than a simple debate between “good or bad” and “ethical or unethical.” In order to address the legal and ethical issues that an autonomous weapons system (AWS) can raise, it’s important to look at the many technical challenges that arise along the full spectrum of autonomy. A group of experts convened by 
	<a href="https://standards.ieee.org/" target="_blank">the IEEE Standards Association</a> is working on this, but they need your help.
</p><p>
	Weapons systems can be built with a range of autonomous capabilities. They might be self-driving tanks, surveillance drones with AI-enabled image recognition, unmanned underwater vehicles that operate in swarms, loitering munitions with advanced target recognition—the list goes on. Some autonomous capabilities are less controversial, while others trigger intense debate over the legality and ethics of the capability. Some capabilities have existed for decades, while others are still hypothetical and may never be developed.
</p><p>
	All of this can make autonomous weapons systems difficult to talk about, and doing so has proven to be incredibly challenging over the years. Answering even the most seemingly straightforward questions, such as whether an AWS is lethal or not, can get surprisingly complicated.
</p><hr/><p>
	To date, international discussions have largely focused on the legal, ethical, and moral issues that arise with the prospect of lethal AWS, with limited consideration of the technical challenges. At the United Nations, these discussions have taken place within the
	<a href="https://www.un.org/disarmament/the-convention-on-certain-conventional-weapons/background-on-laws-in-the-ccw/" rel="noopener noreferrer" target="_blank"> Convention on Certain Conventional Weapons</a>. After nearly a decade, though, the U.N. has yet to come up with a new treaty or regulations to cover AWS. In early discussions at the CCW and other international forums, participants often talked past each other: One person might consider a “fully autonomous weapons system” to include capabilities that are only slightly more advanced than today’s drones, while another might use the term as a synonym for the Terminator.
</p><p>
	Discussions advanced to the point that in 2019, member states at the CCW agreed on a set of 
	<a href="https://documents-dds-ny.un.org/doc/UNDOC/GEN/G19/343/64/PDF/G1934364.pdf?OpenElement" rel="noopener noreferrer" target="_blank">11 guiding principles regarding lethal AWS</a>. But these principles are nonbinding, and it’s unclear how the technical community can implement them. At the most recent meeting of the CCW in July, delegates repeatedly pushed for more nuanced discussions and understanding of the various technical issues that arise throughout the life cycle of an AWS.
</p><p>
	To help bring clarity to these and other discussions, the 
	<a href="https://standards.ieee.org/" rel="noopener noreferrer" target="_blank">IEEE Standards Association</a> convened an expert group in 2020, to consider the ethical and technical challenges of translating AWS principles into practice and what that might mean for future development and governance.
</p><p>
	Last year, the expert group, which I lead, published its findings in a report entitled “<a href="https://standards.ieee.org/wp-content/uploads/import/documents/other/ethical-technical-challenges-autonomous-weapons-systems.pdf" rel="noopener noreferrer" target="_blank">Ethical and Technical Challenges in the Development, Use, and Governance of Autonomous Weapons Systems</a>.” In the document, we identified over 60 challenges of autonomous weapons systems, organized into 10 categories:
</p><ol>
<li>Establishing common language</li>
<li>Enabling effective human control</li>
<li>Determining legal obligations</li>
<li>Ensuring robustness</li>
<li>Testing and evaluating</li>
<li>Assessing risk</li>
<li>Addressing operational constraints</li>
<li>Collecting and curating data</li>
<li>Aligning procurement practices</li>
<li>Addressing nonmilitary use</li>
</ol><p>
	It’s not surprising that “establishing common language” is the first category. As mentioned, when the debates around AWS first began, the focus was on 
	<em>lethal</em> autonomous weapons systems, and that’s often still where people focus. Yet determining whether or not an AWS is lethal turns out to be harder than one might expect.
</p><p>
	Consider a drone that does autonomous surveillance and carries a remote-controlled weapon. It uses artificial intelligence to navigate to and identify targets, while a human makes the final decision about whether or not to launch an attack. Just the fact that the weapon and autonomous capabilities are within the same system suggests this could be considered a lethal AWS.
</p><p>
	Additionally, a human may not be capable of monitoring all of the data the drone is collecting in real time in order to identify and verify the target, or the human may over-trust the system (a 
	<a href="https://www.bbc.com/future/article/20211019-why-we-place-too-much-trust-in-machines" rel="noopener noreferrer" target="_blank">common problem</a> when humans work with machines). Even if the human makes the decision to launch an attack against the target that the AWS has identified, it’s not clear how much “meaningful control” the human truly has. (“<a href="https://blogs.icrc.org/law-and-policy/2018/08/15/autonomous-weapons-operationalizing-meaningful-human-control/" rel="noopener noreferrer" target="_blank">Meaningful human control</a>” is another phrase that has been hotly debated.)
</p><p>
	This problem of definitions isn’t just an issue that comes up when policymakers at the U.N. discuss AWS. AI developers also have different definitions for commonly used concepts, including “bias,” “transparency,” “trust,” “autonomy,” and “artificial intelligence.” In many instances, the ultimate question may not be, Can we establish technical definitions for these terms? but rather, How do we address the fact that there may never be consistent definitions and agreement on these terms? Because, of course, one of the most important questions for all of the AWS challenges is not whether we technically 
	<em>can</em> address this, but even if there is a technical solution, <em>should</em> we build and deploy the system?
</p><p>
	Identifying the challenges was just the first stage of the work for the IEEE-SA expert group. We also concluded that there are three critical perspectives from which a new group of experts will be considering these challenges in more depth:
</p><ul>
<li><strong><a href="https://spectrum.ieee.org/how-can-we-make-sure-that-aws-are-used-responsibly" target="_blank">Assurance and safety</a></strong>, which looks at the technical challenges of ensuring the system behaves the way it’s expected to.<br/>
</li>
<li><strong><a href="https://spectrum.ieee.org/the-challenge-of-human-control-over-autonomous-weapons" target="_blank">Human–machine teaming</a></strong>, which considers how the human and the machine will interact to enable reasonable and realistic human control, responsibility, and accountability.</li>
<li><strong><a href="https://spectrum.ieee.org/autonomous-weapon-systems-and-the-laws-of-war" target="_blank">Law, policy, and ethics</a></strong>, which considers the legal, political, and ethical implications of the issues raised throughout the Challenges document.</li>
</ul><div class="ieee-sidebar-large">
<h3>What Do You Think?</h3>
<p>
	This is where we want your feedback! Many of the AWS challenges are similar to those arising in other fields that are developing autonomous systems. We expect and hope that IEEE members and readers of 
		<em>IEEE Spectrum</em> will have insights from their own fields that can inform the discussion around AWS technologies.
	</p>
<p>
	We’ve put together a 
		<a href="https://saforms.ieee.org/autonomous-weapons-participation/" rel="noopener noreferrer" target="_blank">series of questions</a> in the Challenges document that we hope you’ll answer, to help us better understand how people in other fields are addressing these issues. Autonomous capabilities will increasingly be applied to weapons systems, much as they are being applied in other realms, and we hope that by looking at the challenges in more detail, we can help establish effective technical solutions, while contributing to discussions about what can and should be legally acceptable. Your feedback will help us move toward this ultimate goal. <a href="https://saforms.ieee.org/autonomous-weapons-participation/" rel="noopener noreferrer" target="_blank">Public comments will be open through 7 December 2022</a>.
	</p>
</div><p>
<em>The independent group of experts who authored the report for the IEEE Standards Associate includes <a href="https://www.linkedin.com/in/emmanuelbloch/?locale=en_US" rel="noopener noreferrer" target="_blank">Emmanuel Bloch</a>, <a href="https://www.linkedin.com/in/aconn23/" rel="noopener noreferrer" target="_blank">Ariel Conn</a>, <a href="https://www.linkedin.com/in/denise-garcia-13054016b/" rel="noopener noreferrer" target="_blank">Denise Garcia</a>, <a href="https://www.linkedin.com/in/amandeep-gill-33052416/" rel="noopener noreferrer" target="_blank">Amandeep Gill</a>, <a href="https://www.linkedin.com/in/ashley-soulstice-llorens/" rel="noopener noreferrer" target="_blank">Ashley Llorens</a>, <a href="https://www.linkedin.com/in/martnoorma/" rel="noopener noreferrer" target="_blank">Mart Noorma</a>, and <a href="https://www.linkedin.com/in/heather-m-roff-95466713/" rel="noopener noreferrer" target="_blank">Heather Roff</a></em><em>.</em>
</p>]]></description><pubDate>Thu, 03 Nov 2022 13:10:04 +0000</pubDate><guid>https://spectrum.ieee.org/autonomous-weapons-challenges</guid><category>Autonomous weapons</category><category>Aws</category><category>Robotics</category><dc:creator>Ariel Conn</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/abstract-illustration-of-overlapping-circles-and-squares.jpg?id=31988673&amp;width=980"></media:content></item><item><title>Innovative Shins Turn Quadrupedal Robot Biped</title><link>https://spectrum.ieee.org/quadrupedal-robot-shins-turns-biped</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-video-clip-showing-a-quadrupedal-robot-with-shin-extensions-standing-up-on-its-hind-legs.gif?id=32019322&width=1245&height=700&coordinates=0%2C23%2C0%2C23"/><br/><br/><p>
	The word <em>quadruped</em> means, technically, “four feet.” Roboticists tend to apply the term to anything that uses four limbs to walk, differentiating it from bipedal robots, which walk on two limbs instead. But there’s a huge, blurry crossover there, in both robotics and biology, where you find animals (and occasionally robots) that can transition from quadruped to biped when they need to (for example) manipulate something.
</p><p>
	If you look at quadrupedal robots simply as robots with four limbs rather than robots with four feet, they start to seem much more versatile, but that transition can be a tricky one. At the <a href="https://iros2022.org/" target="_blank">2022 IEEE/RSJ International Conference on Intelligent Robots and Systems</a> (IROS 2022) in Kyoto, Japan, last week, researchers from <a href="https://www.wpi.edu/" target="_blank">Worcester Polytechnic Institute (WPI), in Massachusetts</a>, and ShanghaiTech University <a href="https://chenaah.github.io/multimodal/" target="_blank">presented</a> a generalizable method whereby an off-the-shelf quadruped robot can turn into a biped with some clever software and a tiny bit of mechanical modification.
</p><hr/><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="bb5c78fbb0670171a354b39c52329e2e" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/Xf-fKWNRLcc?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	We’ve seen robots that can transition from quadruped to biped before, but they’re almost always designed very deliberately to be able to do this, and they pay a penalty in weight, complexity, and cost. What’s unique about this research is that it’s intended to be applied to any quadruped at all—with some very minor hardware, your quadruped can become a biped, too.
</p><p>
	The mechanical side of this bipedalization is a 3D-printed stick that gets installed onto the shin of each of the quadruped’s hind legs. This provides additional support so that the robot can stand and walk robustly—without the shin attachments, the robot wouldn’t be statically stable. This is especially useful as the robot stands up, since its center of mass is fully supported during that process. The video shows this working on what looks like a <a href="https://robots.ieee.org/robots/minicheetah/" target="_blank">Mini Cheetah</a> robot, but again, the platform really doesn’t matter as long as it meets some basic requirements.
</p><p class="pull-quote">“We [seek] to reap the benefits from two worlds: stability and speed from quadrupeds, manipulability and a gain in operational height from bipeds.”<br/>—Andre Rosendo, Worcester Polytechnic Institute</p><p>
	Once the robot is upright, walking comes from a policy that’s trained first in simulation and then transferred onto the real robot. This isn’t trivial, because the controller is trying to get the robot to both walk and not fall over, which is a bit of a contradiction, but the best performing policy was able to get the robot to walk for several meters. It’s important to remember that this is a robot that was not designed to walk bipedally at all, so in some sense you’ve got software struggling to get hardware to work in a way that it isn’t supposed to and certainly isn’t optimized for. Perhaps if this kind of thing catches on, quadruped designers might be given incentives to build a little extra flexibility into their platforms to make them more adaptable.
</p><p>
	For more on this research, <em>IEEE Spectrum</em> spoke with <a href="https://www.wpi.edu/people/faculty/arosendo" rel="noopener noreferrer" target="_blank">Andre Rosendo, who is now a professor at WPI</a>.
</p><p><strong>Fundamentally is there a difference between a four-legged robot and a four-limbed robot?</strong>
</p><p>
<strong>Andre Rosendo: </strong>As seen in nature, quadruped locomotion enables higher speeds, and the robot is noticeably faster when moving with four legs. That said, the benefits related to manipulability seen in this animal transition from four to two legs (for example, <a href="https://humanorigins.si.edu/evidence/human-fossils/species/australopithecus-afarensis" target="_blank">Australopithecus</a> using their hands to bring food to their mouths) are also true to robots. We are currently developing a “variant end effector” for the forelimbs to allow this quadruped robot to become a “two-arm manipulator” when standing, handling objects and operating environments.
</p><p>
<strong>Why did you decide on this particular system to enable the bipedal transition?</strong>
</p><p>
<strong>Rosendo: </strong>We noticed that it is quite easy to adapt the hind legs of a quadruped robot with a fixed structure, with very little drop in performance. Although not as aesthetically pleasing as an active structure, advances in materials nowadays allow us to use a small carbon-fiber link protruding from the leg to mimic the same passive stability that our feet give us (known in legged locomotion as the <a href="https://en.wikipedia.org/wiki/Support_polygon" target="_blank">polygon of stability</a>). An active retractable system, on the other hand, would add a tiny motor to the leg, increasing the <a href="https://www.britannica.com/science/moment-of-inertia" target="_blank">moment of inertia</a> of that leg during locomotion, affecting performance negatively.
</p><p>
<strong>What are the limitations to the walking performance of this system?</strong>
</p><p>
<strong>Rosendo: </strong>We trained the robot in a simulated environment, and the walking gait, after being transferred to the real world, is stable, albeit slow. Bipedal robots usually have more degrees of freedom in their legs to allow a more dynamic and adaptive locomotion, but in our case, we are focusing on the multimodal aspect to reap the benefits from two worlds: stability and speed from quadrupeds, manipulability and a gain in operational height from bipeds.
</p><p>
<strong>What are you working on next?</strong>
</p><p>
<strong>Rosendo: </strong>Our next steps...will be on the development of the manipulability of this robot. More specifically, we have been asking ourselves the question: “Now that we can stand up, what can we do that other robots cannot?”, and we already have some preliminary results on climbing to places that are higher than the center of gravity of the robot itself. After mechanical changes on the forelimbs, we will better evaluate complex handling that might require both hands at the same time, which is rare in current mobile robots.
</p><div class="horizontal-rule">
</div><p><em>Multi-Modal Legged Locomotion Framework with Automated Residual Reinforcement Learning</em>, by Chen Yu and Andre Rosendo from ShanghaiTech University, was presented this week at IROS 2022 in Kyoto, Japan. More details are available <a href="https://chenaah.github.io/multimodal/" rel="noopener noreferrer" target="_blank">on Github</a>.</p>]]></description><pubDate>Wed, 02 Nov 2022 16:16:56 +0000</pubDate><guid>https://spectrum.ieee.org/quadrupedal-robot-shins-turns-biped</guid><category>Quadruped robots</category><category>Bipedal robots</category><category>Iros</category><category>Robotics</category><dc:creator>Evan Ackerman</dc:creator><media:content medium="image" type="image/gif" url="https://spectrum.ieee.org/media-library/a-video-clip-showing-a-quadrupedal-robot-with-shin-extensions-standing-up-on-its-hind-legs.gif?id=32019322&amp;width=980"></media:content></item><item><title>Celebrate the 75th Anniversary of the Transistor With IEEE</title><link>https://spectrum.ieee.org/celebrate-transistor-anniversary-with-ieee</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-group-of-women-in-colorful-clothing-are-holding-a-white-banner-and-standing-behind-a-table-with-cake-plates-and-cups-on-it.jpg?id=32028324&width=1245&height=700&coordinates=0%2C0%2C0%2C635"/><br/><br/><p>
	The transistor, the basic building block of the electronics industry, was developed in 1947 by 
	<a href="https://ethw.org/John_Bardeen" rel="noopener noreferrer" target="_blank">John Bardeen</a>, <a href="https://ethw.org/Walter_H._Brattain" rel="noopener noreferrer" target="_blank">Walter Brattain</a>, and <a href="https://ethw.org/William_Shockley" rel="noopener noreferrer" target="_blank">William Shockley</a> at <a href="https://ethw.org/Bell_Labs" rel="noopener noreferrer" target="_blank">Bell Labs</a> in Murray Hill, N.J., to replace the vacuum tube. The three honorary IEEE members received the 1956<a href="https://ethw.org/Nobel_Prize" rel="noopener noreferrer" target="_blank"> Nobel Prize</a> in physics for their work.
</p><p>
	To mark the transistor’s 75th anniversary, the 
	<a href="https://eds.ieee.org/" rel="noopener noreferrer" target="_blank">IEEE Electron Devices Society</a> is holding seminars and panel discussions at IEEE conferences, as well as presenting webinars, giving away prizes, and organizing other events from now until the end of next year.
</p><hr/><p>
	“These events are being done at each conference to remind the community how important the mighty transistor has been over the past 75 years,” says IEEE Fellow 
	<a href="https://www.linkedin.com/in/ravitodi" rel="noopener noreferrer" target="_blank">Ravi Todi</a>, the EDS president.
</p><p>
	 Todi says he is particularly excited about the celebrations scheduled for this year’s 
	<a href="https://www.ieee-iedm.org/" rel="noopener noreferrer" target="_blank">IEEE International Electron Devices Meeting</a>, the society’s flagship conference, being held from 3 to 7 December at the <a href="https://www.hilton.com/en/hotels/sfofhhh-hilton-san-francisco-union-square/" rel="noopener noreferrer" target="_blank">Hilton San Francisco Union Square</a>. The conference has been bringing together engineers from academia and industry for more than 70 years.
</p><h3>About the IEEE Electron Devices Society</h3><br/><p>EDS was established in 1952 as a committee of the <a href="https://ethw.org/IEEE_Communications_Society_History#IRE_.28Institute_of_Radio_Engineers.29" target="_blank">Institute of Radio Engineers</a>, one of IEEE’s predecessor societies. After IRE’s merger in 1963 with the <a href="https://ethw.org/IEEE_Communications_Society_History#AIEE_.28American_Institute_of_Electrical_Engineers.29" target="_blank">American Institute of Electrical Engineers</a>—which formed IEEE—EDS was an IEEE technical group. It became a society 13 years later.</p><p>EDS now has about 9,000 members and 250 chapters worldwide.</p><p>“We pride ourselves on the fact that a lot of the core inventions of transistors were developed by IEEE EDS members,” says Ravi Todi, the society’s president.</p><p>
	“A lot of the key inventions that have to do with the transistor have been first presented at this conference,” Todi says. “IEEE IEDM has played a key role in driving the progress of the transistor.”
</p><p>
	Several society members are compiling a book about the transistor and its history. In addition, technical articles are scheduled to run in 
	<a href="https://spectrum.ieee.org/" target="_self"><em>IEEE Spectrum</em></a> and in the <a href="https://eds.ieee.org/publications/eds-newsletter" rel="noopener noreferrer" target="_blank"><em>EDS Newsletter</em></a>.
</p><p>
	 “Our aim is to teach and enlighten members on the importance of the transistor,” says IEEE Senior Member 
	<a href="https://in.linkedin.com/in/manoj-saxena-a0629a14" rel="noopener noreferrer" target="_blank">Manoj Saxena</a>, who is overseeing the anniversary celebrations. “The events shall be a tribute to the men and women who have contributed to the development of the transistor, which has had a lasting impact on people's lives and has benefited mankind.”
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A group of people stand in front of a white wall while holding a white and blue banner with writing on it." class="rm-shortcode" data-rm-shortcode-id="f70332c4e5b9e61fbc2ffc3931198a83" data-rm-shortcode-name="rebelmouse-image" id="91135" loading="lazy" src="https://spectrum.ieee.org/media-library/a-group-of-people-stand-in-front-of-a-white-wall-while-holding-a-white-and-blue-banner-with-writing-on-it.jpg?id=32028351&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Members of the IEEE Electron Devices Society’s IEEE Madhya Pradesh Section celebrate the transistors 75th anniversary at the Indian Institute of Technology in Indore.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">IEEE</small></p><h2>Panels, webinars, and historical accounts</h2><p>
	EDS is collaborating with the 
	<a href="https://sscs.ieee.org/" rel="noopener noreferrer" target="_blank">IEEE Solid-State Circuits Society</a> to mark the anniversary. Since January, the two societies have been presenting a <a href="https://eds.ieee.org/education/webinars" rel="noopener noreferrer" target="_blank">webinar series</a> on the effect transistors have had on society and technological advances. Every month, a member who works in the field of electron device engineering gives a presentation on a technology impacted by the transistor. Presentations so far have covered 2D semiconductors, chemical sensors, and 5G cellular technology. The talks are <a href="https://eds.ieee.org/education/webinars" rel="noopener noreferrer" target="_blank">available on demand</a> on the EDS website.
</p><p>
	EDS is compiling a commemorative book designed to explore the development of the transistor and how it has changed in the past 75 years. Transistor pioneers including 
	<a href="https://spectrum.ieee.org/transistor-wars" target="_self">Digh Hisamoto</a>, <a href="https://spectrum.ieee.org/four-ieee-fellows-share-queen-elizabeth-prize-for-digital-cameras" target="_self">Eric R. Fossum</a>, and <a href="https://spectrum.ieee.org/ieee-medal-of-honor-goes-to-transistor-pioneer-chenming-hu" target="_self">Chenming Hu</a> are writing chapters for the book, which is scheduled to be published early next year. Hisamoto, an IEEE Fellow, developed the 3D double-gate metal-oxide-silicon field effect transistor in 1989. Three years later, Fossum, also an IEEE Fellow, invented the CMOS image sensor. An IEEE Medal of Honor recipient and IEEE Life Fellow, Hu is known as the “<a href="https://spectrum.ieee.org/how-the-father-of-finfets-helped-save-moores-law" target="_self">father of the 3D transistor</a>” for inventing the FinFET in 1999.
</p><p>
	“We want to capture historical accounts of the device’s development and growth,” Todi says.
</p><p>
	IEEE EDS chapters are holding events to celebrate the anniversary. The activities include social gatherings, technical talks, and historical seminars.
</p><p>
	To learn more about 
	<a href="https://eds.ieee.org/about-eds/75th-anniversary-of-the-transistor" rel="noopener noreferrer" target="_blank">upcoming events in your area and how to participate</a>, visit the EDS website.
</p>]]></description><pubDate>Tue, 01 Nov 2022 18:00:01 +0000</pubDate><guid>https://spectrum.ieee.org/celebrate-transistor-anniversary-with-ieee</guid><category>Ieee news</category><category>Ieee electron devices society</category><category>Transistor</category><category>Type:ti</category><dc:creator>Joanna Goodrich</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-group-of-women-in-colorful-clothing-are-holding-a-white-banner-and-standing-behind-a-table-with-cake-plates-and-cups-on-it.jpg?id=32028324&amp;width=980"></media:content></item><item><title>Eat This Drone</title><link>https://spectrum.ieee.org/edible-robots</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-drone-airplane-sits-on-a-lab-bench-it-has-a-normal-propeller-and-motor-and-tail-but-its-wings-are-made-of-rice-crackers-cover.jpg?id=32019267&width=1245&height=700&coordinates=0%2C270%2C0%2C270"/><br/><br/><p>Drones have the potential to be very useful in disaster scenarios by transporting food and water to people in need. But whenever you ask a drone to transport anything, anywhere, the bulk of what gets moved is the drone itself. Most delivery drones can carry only about 30 percent of their mass as payload, because most of their mass is both critical, like wings, and comes in the form of things that are essentially useless to the end user, like wings.</p><p>At the IEEE/RSJ International Conference on Intelligent Robots and Systems (<a href="https://iros2022.org/" target="_blank">IROS</a>) conference in Kyoto last week, researchers from the <a href="https://www.epfl.ch/labs/lis/" target="_blank">Swiss Federal Institute of Technology Lausanne (EPFL)</a> presented a paper describing a drone that can boost its payload of food from 30 percent to 50 percent of its mass. It does so with the ingenious use of wings made from rice cakes that contain the caloric equivalent of an average, if unbalanced, breakfast. For anyone interested in digesting the paper, it is titled “Towards Edible Drones for Rescue Missions: Design and Flight of Nutritional Wings,” by Bokeon Kwak, Jun Shintake, Lu Zhang, and Dario Floreano from EPFL.</p><hr/><p>The reason why this drone exists is to work toward the effective and efficient delivery of food to someone who, for whatever reason, really, really needs food and is not in a position to gain access to it in any other way. The idea is that you could fly this drone directly to them and keep them going for an extra day or two. You obviously won’t get the drone back afterward (because its wings will have been eaten off), but that’s a small price to pay for potentially keeping someone alive via the delivery of vital calories.<br/></p><p>The researchers designed the wing of this partially edible drone out of compressed puffed rice (rice cakes or rice cookies, depending on whom you ask) because of the foodstuff’s similarity to expanded polypropylene (EPP) foam. EPP foam is something that’s commonly used as wing material in drones because it’s strong and lightweight; puffed rice shares those qualities. Though it’s not quite as strong as the EPP, it’s not bad. And it’s also affordable, accessible, and easy to laser cut. The puffed rice also has a respectable calorie density—at 3,870 kilocalorie per kilogram, rice cakes aren’t as good as something like chocolate, but they’re about on par with pasta, just with a much lower density.</p><p>Out of the box, the rice cakes are round, so the first step in fabricating the wing is to laser cut them into hexagons to make them easier to stick together. The glue is just gelatin, and after it all dries, the wing is packaged in plastic and tape to make sure that it doesn’t break down in wet or humid environments. It’s a process that’s fast, simple, and cheap.</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="" class="rm-shortcode" data-rm-shortcode-id="34ff74f7de5643c072335f12a54277bf" data-rm-shortcode-name="rebelmouse-image" id="4db39" loading="lazy" src="https://spectrum.ieee.org/media-library/image.gif?id=32019265&width=980"/>
</p><p>The size of the wing is actually driven not by flight requirements but by nutrition requirements. In this case, a wingspan of about 70 centimeters results in enough rice cake and gelatin glue to deliver 300 kcal, or the equivalent of one breakfast serving, with 80 grams remaining for a payload of vitamins or water or something like that. The formula the researchers came up with to calculate the design of this avian appetite quencher assumes that the rest of the drone is not edible, because it isn’t. The structure and tail surfaces are made of carbon fiber and foam.</p><p>While this is just a prototype, the half-edible drone does actually fly, achieving speeds of about 10 meters per second with the addition of a motor, some servos to actuate the tail surfaces for control, and a small battery. The next step is to figure out a way to make as many of those nonedible pieces out of edible materials instead, as well as finding a way to carry a payload (like water) in an edible container.</p><p>For a bit more about this drone, we spoke with the lead author of the paper, Bokeon Kwak.</p><p><strong>It sounds like your selection of edible wing material was primarily optimized for its mechanical properties and low weight. Are there other options that could work if the goal was to instead optimize for calories while still maintaining functionality?</strong></p><p><strong>Bokeon Kwak: </strong>As you pointed out, achieving sufficient mechanical properties while maintaining low weight (with food materials) was the foremost design criteria in designing the edible wing. We can expand the design criteria to contain higher calories by using fat-based material—for example, edible wax; fat has higher calories per gram than proteins and carbohydrates. On the other hand, containing more calories also implies the increase of structural weight, which is a price we need to pay toward higher calories. This aspect also requires further study to find a sweet spot! </p><p><strong>What does the drone taste like?</strong></p><p><strong>Kwak:</strong> The edible wing tastes like a crunchy rice crisp cookie with a little touch of raw gelatin (which worked as an edible glue to hold the rice cookies as a flat plate shape). No artificial flavor has been added yet. </p><p><strong>Would there be any significant advantages to making the wing into a more complex shape, for example with an airfoil cross-section instead of a flat plate?</strong></p><p><strong>Kwak:</strong> Making a well-streamlined airfoil (instead of a flat plate) is actually our next goal to achieve more efficient aerodynamic properties, such as lower drag and higher lift. These advantages let an edible drone carry more payload (which is useful to carry water) and have prolonged flight time and distance. Our team is testing 3D food printing and molding to create such an edible wing, including material characterization to make sure the edible wing has sufficient mechanical properties (for example, higher Young’s modulus, low density). </p><p><strong>What else will you be working on next?</strong></p><p><strong>Kwak:</strong> Other structural components such as wing control surfaces (such as an aileron or a rudder) will be made of edible material by 3D food printing or molding. Other things that will be considered are an edible/water-resistant coating on the surface of the edible wing, and degradation testing of the edible wing upon time (and water exposure).</p><div class="horizontal-rule"></div><p>This drone is just one application of a broader European research initiative called <a href="https://www.robofood.org/" rel="noopener noreferrer" target="_blank">RoboFood</a>, which seeks to develop edible robots in a way that maximizes both performance and nutritional value. Edible sensing, actuation, and computation are all parts of this project, and the researchers (<a href="https://www.epfl.ch/labs/lis/" rel="noopener noreferrer" target="_blank">led by Dario Floreano at EPFL</a>) can now start to focus on some of those more challenging edible components. </p>]]></description><pubDate>Tue, 01 Nov 2022 16:00:01 +0000</pubDate><guid>https://spectrum.ieee.org/edible-robots</guid><category>Drones</category><category>Iros 2022</category><category>Epfl</category><category>Robotics</category><category>Edible robots</category><dc:creator>Evan Ackerman</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-drone-airplane-sits-on-a-lab-bench-it-has-a-normal-propeller-and-motor-and-tail-but-its-wings-are-made-of-rice-crackers-cover.jpg?id=32019267&amp;width=980"></media:content></item><item><title>DRAM’s Moore’s Law Is Still Going Strong</title><link>https://spectrum.ieee.org/micron-dram</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-golden-disc-etched-with-fine-lines-and-with-light-splayed-across-it-at-intersecting-angles.jpg?id=32025123&width=1245&height=700&coordinates=0%2C402%2C0%2C403"/><br/><br/><p>Memory and storage chip maker <a href="https://www.micron.com/" target="_blank">Micron Technology</a> says it is shipping samples of the most bit-dense DRAM memory chips yet. Compared with its own previous generation, the 16-gigabit DRAM chip is 15 percent more power efficient and 35 percent more dense. Notably, Micron achieved the improvement without resorting to the most advanced chip-making technology, extreme ultraviolet lithography. The features that make up DRAM cells are not nearly as tiny as those on logic chips, but this advance shows that DRAM density could still shrink further in the future.<br/></p><p>Micron says it is shipping samples of LPDDR5X chips, memory made for power-constrained systems such as smartphones. (LPDDR5X, unpacked: a revved-up twist on the low-power version of the fifth generation of the double-data-rate memory communications standard, capable of transferring 8.5 gigabits per second.) It’s the first chip made using Micron’s new manufacturing process, called 1-beta, which the company says maintains the <a href="https://investors.micron.com/news-releases/news-release-details/micron-delivers-industrys-first-1a-dram-technology" target="_blank">lead it took a year ago</a> over rivals, including <a href="https://semiconductor.samsung.com/dram/" target="_blank">Samsung</a> and <a href="https://www.skhynix.com/" target="_blank">SK Hynix</a>.</p><p>Manufacturing processes for DRAM and logic chips diverged decades ago, with logic chips shrinking transistors much more aggressively as the years went by, explains <a href="https://objective-analysis.com/jim-handy/" target="_blank">Jim Handy</a>, a memory and storage analyst at <a href="https://objective-analysis.com/" rel="noopener noreferrer" target="_blank">Objective Analysis</a>, in Los Gatos, Calif. The reason for the difference has to do with DRAM’s structure. DRAM stores a bit as charge in a capacitor. Access to each capacitor is gated by a transistor. But the transistor is an imperfect barrier, and the charge will eventually leak away. So DRAM must be periodically refreshed, restoring its bits before they drain away. In order to keep that refresh period reasonable while still increasing the density of memory, DRAM makers had to make some pretty radical changes to the makeup of the capacitor. For Micron and other major manufacturers, it now resembles a tall pillar and is made using materials not found in logic chips.</p><p>Nevertheless, memory makers have been investing in the latest key manufacturing tool that logic-chip companies have: <a href="https://spectrum.ieee.org/euv-lithography-finally-ready-for-chip-manufacturing" target="_blank">extreme ultraviolet lithography</a>. But Micron didn’t need it to achieve its latest chip. Instead, the company used a proprietary version of “multipatterning” technology while sticking with the long-established <a href="https://spectrum.ieee.org/chip-makings-wet-new-world" target="_blank">193-nanometer immersion lithography</a>. Multipatterning involves <a href="https://www.micron.com/about/blog/2021/january/inside-1a-the-worlds-most-advanced-dram-process-technology" target="_blank">cycles of projecting a pattern, etching and depositing material, then projecting another pattern</a>—done in such a way that the interaction produces finer structures than any single pattern could. This version of multipatterning was adapted from one used in Micron’s NAND flash business, according to Thy Tran, vice president of DRAM process integration at Micron. “We have taken that and extended it aggressively,” she says. “It’s extremely valuable to be able to leverage both DRAM and NAND [flash].”</p><p>Not needing to use EUV is “a real coup,” says Handy. But it follows a trend. “Micron, for over a decade, has been able to use older process technology and equipment smarter than everybody else.”</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="A gold rectangle with intersecting paralle pale lines and darker sopts." class="rm-shortcode rm-resized-image" data-rm-shortcode-id="5d2693732af78ecc2942b08030783717" data-rm-shortcode-name="rebelmouse-image" id="4873a" loading="lazy" src="https://spectrum.ieee.org/media-library/a-gold-rectangle-with-intersecting-paralle-pale-lines-and-darker-sopts.jpg?id=32025124&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">A 16-gigabit DRAM die made with Micron’s 1-beta technology.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Micron Technology</small></p><p>Tran says the company currently believes it will begin using EUV with the next process, 1-gamma.</p><p>The initial product is meant for mobile systems, and so it makes power saving a priority. Part of that comes from the use of “enhanced” dynamic voltage and frequency scaling. This is technology that allows a chip to run with a slower clock and at lower voltage to conserve energy and then ramp up to higher frequency and voltage to get more work done. Micron’s previous manufacturing technology, 1-alpha, could transfer data at 1,600 megabits per second when in the power-saving mode, explains Ross Dermott, vice president of mobile product line management. The LPDDR5X DRAM, made using the 1-beta process, can operate at 3,200 Mb/s in the low-power condition. Handset makers that “have features or applications that are running at that faster speed,  can go into [the low-power] mode and essentially further reduce the power consumption,” he says.<br/></p><p>Tran says that Micron will later use 1-beta to make other types of DRAM, including the high-bandwidth memory that powers data-center processors and AI accelerators.</p><p><em>This post was corrected on 3 November. Micron's vice president of mobile product line management is Ross Dermott, not Ross McDermott.</em></p>]]></description><pubDate>Tue, 01 Nov 2022 13:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/micron-dram</guid><category>Dram</category><category>Moore's law</category><category>Micron</category><category>Samsung</category><category>Sk hynix</category><category>Extreme ultraviolet lithography</category><category>Euv</category><category>Memory</category><dc:creator>Samuel K. Moore</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-golden-disc-etched-with-fine-lines-and-with-light-splayed-across-it-at-intersecting-angles.jpg?id=32025123&amp;width=980"></media:content></item><item><title>Video Monday: IROS 2022 Award Winners</title><link>https://spectrum.ieee.org/robot-videos-iros-award-winners</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-photo-of-a-large-conference-center-in-japan-on-a-sunny-day.jpg?id=32023205&width=1245&height=700&coordinates=0%2C60%2C0%2C60"/><br/><br/><p>IROS 2022 took place in Kyoto last week, bringing together thousands of roboticists from around the world to share all the latest awesome research they’ve been working on. We’ve got a bunch of stuff to bring you from the conference, but while we work on that (and recover from some monster jetlag), here are the presentation videos of all of the <a href="https://iros2022.org/2022/10/30/award-winners/" target="_blank">IROS 2022 award-winning papers</a>. This is the some of the best, most impactful robotics research presented this year. Congratulations to all of the winners!</p><div class="horizontal-rule">
</div><h3>IROS 2022 Best Paper Award</h3><br/><span class="rm-shortcode" data-rm-shortcode-id="22786bbd78e15453cc7a7f1e5014b8a3" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/UTMT2WAUlRw?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span><p>
	“SpeedFolding: Learning Efficient Bimanual Folding of Garment,” by Yahav Avigal, Lars Berscheid, Tamim Asfour, Torsten Kroeger, and Ken Goldberg from the University of California, Berkeley, and the Karlsruhe Institute of Technology.</p><p>
	Read more: <a href="https://events.infovaya.com/presentation?id=84508" target="_blank">https://events.infovaya.com/presentation?id=84508</a>
</p><div class="horizontal-rule">
</div><h3>IROS 2022 Best Student Paper Award – Sponsored by ABB</h3><br/><span class="rm-shortcode" data-rm-shortcode-id="f549e7147d656ae45477c82c2cbb343f" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/6M7SWa1lxcM?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span><p>
	“FAR Planner: Fast, Attemptable Route Planner Using Dynamic Visibility Update,” by Fan Yang, Chao Cao, Hongbiao Zhu, Jean Oh, and Ji Zhang from Carnegie Mellon University and the Harbin Institute of Technology.</p><p>
	Read more: <a href="https://events.infovaya.com/presentation?id=84511" rel="noopener noreferrer" target="_blank">https://events.infovaya.com/presentation?id=84511</a></p><div class="horizontal-rule">
</div><h3>IROS Best Paper Award on Cognitive Robotics – Sponsored by KROS</h3><br/><span class="rm-shortcode" data-rm-shortcode-id="14c74b9de5b2bd6358ace305fa635b8a" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/qFObMpOboCg?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span><p>
	“Gesture2Vec: Clustering Gestures Using Representation Learning Methods for Co-Speech Gesture Generation,” by Payam Jome Yazdian, Mo Chen, and Angelica Lim from Simon Fraser University.</p><p>
	Read more: <a href="https://events.infovaya.com/presentation?id=90186" target="_blank">https://events.infovaya.com/presentation?id=90186</a>
</p><div class="horizontal-rule">
</div><h3>IROS 2022 Best RoboCup Paper Award – Sponsored by RoboCup Federation</h3><br/><span class="rm-shortcode" data-rm-shortcode-id="ef451ccb67e56b4a509e175340c38ac8" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/mNy1cloWrP0?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span><p>
	“RCareWorld: A Human-Centric Simulation World for Caregiving Robots,” by Ruolin Ye, Wenqiang Xu, Haoyuan Fu, Rajat Kumar Jenamani, Vy Nguyen, Cewu Lu, Katherine Dimitropoulou, and Tapomayukh Bhattacharjee from Cornell University, Shanghai Jiao Tong University, and Columbia University.</p><p>
	Read more: <a href="https://events.infovaya.com/presentation?id=84520" target="_blank">https://events.infovaya.com/presentation?id=84520</a>
</p><div class="horizontal-rule">
</div><h3>IROS Best Paper Award on Robot Mechanisms and Design – Sponsored by ROBOTIS</h3><br/><span class="rm-shortcode" data-rm-shortcode-id="b42d5ffc61aff7acc0b3c83690b90d0d" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/F9CE_CdK3Oo?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span><p>
	“Aerial Grasping and the Velocity Sufficiency Region,” by Tony G. Chen, Kenneth Hoffmann, JunEn Low, Keiko Nagami, David Lentink, and Mark Cutkosky from Stanford University and Wageningen University & Research.</p><p>
	Read more: <a href="https://events.infovaya.com/presentation?id=85675" target="_blank">https://events.infovaya.com/presentation?id=85675</a>
</p><div class="horizontal-rule">
</div><h3>IROS Best Entertainment and Amusement Paper Award – Sponsored by JTCF</h3><br/><span class="rm-shortcode" data-rm-shortcode-id="e7819d2c975c638e060cde9242927457" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/2DCkyE0l0aI?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span><p>
	“Robot Learning to Paint From Demonstrations,” by Younghyo Park, Seunghun Jeon, and Taeyoon Lee from Seoul National University, KAIST, and Naver Labs.</p><p>
	Read more: <a href="https://events.infovaya.com/presentation?id=85681" target="_blank">https://events.infovaya.com/presentation?id=85681</a>
</p><div class="horizontal-rule">
</div><h3>IROS Best Paper Award on Safety, Security, and Rescue Robotics in memory of Motohiro Kisoi – Sponsored by IRS</h3><br/><span class="rm-shortcode" data-rm-shortcode-id="705dface359e7a7a41e6b3571d19ec77" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/O-AQHSVig7U?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span><p>
	“Power-Based Safety Layer for Aerial Vehicles in Physical Interaction Using Lyapunov Exponents,” by Eugenio Cuniato, Nicholas Lawrance, Marco Tognon, and Roland Siegwart from ETH Zurich and CSIRO.</p><p>
	Read more: <a href="https://events.infovaya.com/presentation?id=86266" target="_blank">https://events.infovaya.com/presentation?id=86266</a>
</p><div class="horizontal-rule">
</div><h3>IROS Best Paper Award on Agri-Robotics – Sponsored by YANMAR</h3><br/><span class="rm-shortcode" data-rm-shortcode-id="3b2f882c126aef5f8454837e22e0eecd" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/DfIN8Br7xFo?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span><p>
	“Explicitly Incorporating Spatial Information to Recurrent Networks for Agriculture,” by Claus Smitt, Michael Allan Halstead, Alireza Ahmadi, and Christopher Steven McCool from the University of Bonn.</p><p>
	Read more: <a href="https://events.infovaya.com/presentation?id=86839" target="_blank">https://events.infovaya.com/presentation?id=86839</a>
</p><div class="horizontal-rule">
</div><h3>IROS Best Paper Award on Mobile Manipulation – Sponsored by OMRON Sinic X Corp.</h3><br/><span class="rm-shortcode" data-rm-shortcode-id="abfeb764a51be62c241e25d02507d356" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/7KNHtBwkt64?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span><p>
	“Robot Learning of Mobile Manipulation With Reachability Behavior Priors,” by Snehal Jauhri, Jan Peters, and Georgia Chalvatzaki from TU Darmstadt.</p><p>
	Read more: <a href="https://events.infovaya.com/presentation?id=86827" target="_blank">https://events.infovaya.com/presentation?id=86827</a>
</p><div class="horizontal-rule">
</div><h3>IROS Best Application Paper Award – Sponsored by ICROS</h3><br/><span class="rm-shortcode" data-rm-shortcode-id="f57802c306105adcf47dcebd71446a84" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/YQaLRtOtQf8?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span><p>
	“Soft Tissue Characterisation Using a Novel Robotic Medical Percussion Device with Acoustic Analysis and Neural Network,” by Pilar Zhang Qiu, Yongxuan Tan, Oliver Thompson, Bennet Cobley, and Thrishantha Nanayakkara from Imperial College London.</p><p>
	Read more: <a href="https://events.infovaya.com/presentation?id=86287" target="_blank">https://events.infovaya.com/presentation?id=86287</a>
</p><div class="horizontal-rule">
</div><h3>IROS Best Paper Award for Industrial Robotics Research for Applications – Sponsored by Mujin Inc.</h3><br/><span class="rm-shortcode" data-rm-shortcode-id="9579d11d44c465ddf325e85b5e85239a" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/DGqvREtBnS4?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span><p>
	“Absolute Position Detection in 7-Phase Sensorless Electric Stepper Motor,” by Vincent Groenhuis, Gijs Rolff, Koen Bosman, Leon Abelmann, and Stefano Stramigioli from the University of Twente, IMS BV, and Eye on Air.</p><p>
	Read more: <a href="https://events.infovaya.com/presentation?id=85705" target="_blank">https://events.infovaya.com/presentation?id=85705</a>
</p><div class="horizontal-rule">
</div>]]></description><pubDate>Mon, 31 Oct 2022 20:56:31 +0000</pubDate><guid>https://spectrum.ieee.org/robot-videos-iros-award-winners</guid><category>Robotics research</category><category>Robots</category><category>Iros</category><dc:creator>Erico Guizzo</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-photo-of-a-large-conference-center-in-japan-on-a-sunny-day.jpg?id=32023205&amp;width=980"></media:content></item><item><title>NYU Biomedical Engineering Speeds Research from Lab Bench to Bedside</title><link>https://spectrum.ieee.org/nyu-biomedical-engineering</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/medical-equipment-with-two-rings-each-with-concentric-sensors-and-wires-connected-to-a-computer-nearby.jpg?id=31997012&width=1245&height=700&coordinates=0%2C0%2C0%2C2"/><br/><br/><p><em>This is a sponsored article brought to you by <a href="https://engineering.nyu.edu/?utm_source=ieee&utm_medium=email&utm_campaign=reputation-2022&utm_content=hielscher-bme" rel="noopener noreferrer" target="_blank">NYU’s Tandon School of Engineering</a>.</em></p><p>When <a href="https://engineering.nyu.edu/academics/departments/biomedical-engineering/faculty?utm_source=ieee&utm_medium=email&utm_campaign=reputation-2022&utm_content=hielscher-bme" rel="noopener noreferrer" target="_blank">Andreas H. Hielscher</a>, the chair of the <a href="https://engineering.nyu.edu/academics/departments/biomedical-engineering?utm_source=ieee&utm_medium=email&utm_campaign=reputation-2022&utm_content=hielscher-bme" rel="noopener noreferrer" target="_blank">biomedical engineering (BME) department at NYU’s Tandon School of Engineering</a>, arrived at his new position, <a href="https://engineering.nyu.edu/news/introducing-new-chair-tandons-department-biomedical-engineering?utm_source=ieee&utm_medium=email&utm_campaign=reputation-2022&utm_content=hielscher-bme" rel="noopener noreferrer" target="_blank">he</a><a href="https://engineering.nyu.edu/news/introducing-new-chair-tandons-department-biomedical-engineering?utm_source=ieee&utm_medium=email&utm_campaign=reputation-2022&utm_content=hielscher-bme" rel="noopener noreferrer" target="_blank"> saw raw </a><a href="https://engineering.nyu.edu/news/introducing-new-chair-tandons-department-biomedical-engineering?utm_source=ieee&utm_medium=email&utm_campaign=reputation-2022&utm_content=hielscher-bme" rel="noopener noreferrer" target="_blank">potential</a>. NYU Tandon had undergone a meteoric rise in its <em>U.S. News & World Report </em>graduate ranking in recent years, skyrocketing 47 spots since 2009. At the same time, the <a href="https://med.nyu.edu/our-community/about-us" rel="noopener noreferrer" target="_blank">NYU Grossman School of Medicine</a> had shot from the thirties to the #2 spot in the country for research. The two scientific powerhouses, sitting on opposite banks of the East River, offered Hielscher a unique opportunity: to work at the intersection of engineering and healthcare research, with the unmet clinical needs and clinician feedback from NYU’s world-renowned medical program directly informing new areas of development, exploration, and testing.</p><hr/><h3></h3><br/><p>“There is now an understanding that technology coming from a biomedical engineering department can play a big role for a top-tier medical school,” said Hielscher. “At some point, everybody needs to have a BME department.”</p><p>In the early days of biomedical engineering departments nationwide, there was some resistance even to the notion of biomedical engineering: either you were an electrical engineer or a mechanical engineer. “That’s no longer the case,” said Hielscher. “The combining of the biology and medical aspects with the engineering aspects has been proven to be the best approach.”</p><h3></h3><br/><span class="rm-shortcode" data-rm-shortcode-id="9b55948ce0289b58bf59d0e2b87681eb" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/IukEi7aHndI?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span><p class="caption">Dr. Andreas Hielscher, NYU Tandon Biomedical Engineering Department Chair and head of the Clinical Biophotonics Laboratory, speaks with IEEE Spectrum about his work leveraging optical tomography for early detection and treatment monitoring for breast cancer.</p><h3></h3><br/><p>The proof of this can be seen by the trend that an undergraduate biomedical degree has become one of the most desired engineering degrees, according to Hielscher. He also noted that the current Dean of NYU’s Tandon School of Engineering, <a href="https://engineering.nyu.edu/faculty/jelena-kovacevic?utm_source=ieee&utm_medium=email&utm_campaign=reputation-2022&utm_content=hielscher-bme" rel="noopener noreferrer" target="_blank">Jelena Kovačević</a>, has a biomedical engineering background, having just received the 2022 IEEE Engineering in Medicine and Biology Society <a href="https://youtu.be/v_b-ElMEUc8" rel="noopener noreferrer" target="_blank">career achievement award</a> for her pioneering research related to signal processing applications for biomedical imaging.</p><p>Mary Cowman, a pioneer in joint and cartilage regeneration, began laying the foundations for NYU Tandon’s biomedical engineering department in the 2010s. Since her retirement in 2020, Hielscher has continued to grow the department through innovative collaborations with the medical school and medical center, including the recently-announced <a href="https://engineering.nyu.edu/research-innovation/entrepreneurship/nyu-translational-healthcare-initiative?utm_source=ieee&utm_medium=email&utm_campaign=reputation-2022&utm_content=hielscher-bme" rel="noopener noreferrer" target="_blank">Translational Healthcare Initiative</a>, on which Hielscher worked closely with <a href="https://engineering.nyu.edu/faculty/daniel-sodickson?utm_source=ieee&utm_medium=email&utm_campaign=reputation-2022&utm_content=hielscher-bme" rel="noopener noreferrer" target="_blank">Daniel Sodickson</a>, the co-director of the medical school’s <a href="https://med.nyu.edu/departments-institutes/health-technology-engineering/" rel="noopener noreferrer" target="_blank">Tech4Health</a>.</p><h3></h3><br/><img alt="Man with beard and eyeglasses in a suit against a background showing bookshelves." class="rm-shortcode" data-rm-shortcode-id="d3fcdd5a36712e5fe07365db6d2f0582" data-rm-shortcode-name="rebelmouse-image" id="647ca" loading="lazy" src="https://spectrum.ieee.org/media-library/man-with-beard-and-eyeglasses-in-a-suit-against-a-background-showing-bookshelves.jpg?id=31996999&width=980"/><h3></h3><br/><p>“The fundamental idea of the Initiative is to have one physician from Langone Medical School, and one engineer at least—you could have multiple—and have them address some unmet clinical needs, some particular problem,” explained Hielscher. “In many cases they have already worked together, or researched this issue. What this initiative is about is to give these groups funding to do some experimentation to either prove that it won’t work, or demonstrate that it can and prioritize it.”<br/></p><p>With this funding of further experimentation, it becomes possible to develop the technology to a point where you could begin to bring investors in, according Hielscher. “This mitigates the risk of the technology and helps attract potential investors,” added Hielscher. “At that point, perhaps a medical device company comes in, or some angel investor, and then you can get to the next level of investment for moving the technology forward.”</p><h2>Biophotonics for Cancer Diagnosis</h2><p>Hielscher himself has been leading research on developing new technologies within the <a href="https://wp.nyu.edu/tandonschoolofengineering-cbl/" target="_blank">Clinical Biophotonics Laboratory</a>. One of the latest areas of research has been investigating the application of optical technologies to breast cancer diagnosis.</p><h3></h3><br><img alt="Six images showing tomographic cross sections of a breast with tumor" class="rm-shortcode" data-rm-shortcode-id="3c81468928b06738c7989f37ff4f9b7e" data-rm-shortcode-name="rebelmouse-image" id="95ede" loading="lazy" src="https://spectrum.ieee.org/media-library/six-images-showing-tomographic-cross-sections-of-a-breast-with-tumor.jpg?id=31996967&width=980"/><h3></h3><br/><p>Hielscher and his colleagues have built a system that shines light through both breasts at the same time. By measuring how much light is reflected back it’s possible to generate maps of locations with high levels of oxygen and total hemoglobin, which may indicate tumors.</p><p>“We look at where there’s blood in the breast,” explained Hielscher. “Because breast tumors recruit new blood vessels, or, once they grow, they generate their own vascular network requiring more oxygen, wherever there is a tumor you will see an increase in total blood volume, and you will see more oxygenated blood.”</p><p>Initially, this diagnostic tool was targeted for early detection, since mammograms can only detect calcification in lower density breast tissue of women over a certain age. But it soon became clear in collaboration with clinical partners that it was also highly effective in monitoring treatment.</p><h3></h3><br/><p>“Technology coming from a biomedical engineering department can play a big role for a top-tier medical school”<br/>—Andreas H. Hielscher, Biomedical Engineering Department Chair, NYU Tandon</p><h3></h3><br><p>This realization came in part because of a recent change in cancer treatment that has moved towards what is known as neoadjuvant chemotherapy, in which chemotherapy drugs are administered before surgical extraction of the tumor. One of the drawbacks of this approach is that only around 60 percent of patients respond favorably to the chemotherapy, resulting in a large percentage of patients suffering through a grueling six-month-long chemotherapy treatment with minimal-to-no impact on the tumor.</p><p>With the optical technique, Hielscher and his colleagues have found that if they can detect a noticeable decrease of blood in targeted areas after two weeks, it’s very likely that the patient will respond to the chemotherapy. On the other hand, if they see that the amount of blood in that area stays the same, then there’s a very high likelihood that the patient will not respond to the therapy.</p><p>This same fundamental technique can also be applied to what is known as peripheral artery disease (PAD), which affects many patients with diabetes and involves the narrowing or blockage of the vessels that carry blood from the heart to the legs. An Israel-based company called <a href="https://www.votis.net/" target="_blank">VOTIS</a> has licensed the technology for diagnosing and treating PAD.</p><h3></h3><br/><img alt="Medical equipment image with blue background showing in bright yellow a finger joint affected by lupus arthritis" class="rm-shortcode" data-rm-shortcode-id="d1930edb3000e4edfd4b2acf58f513c1" data-rm-shortcode-name="rebelmouse-image" id="586bc" loading="lazy" src="https://spectrum.ieee.org/media-library/medical-equipment-image-with-blue-background-showing-in-bright-yellow-a-finger-joint-affected-by-lupus-arthritis.jpg?id=31996972&width=980"/><h3></h3><br/><p>
	While Hielscher’s work is in biophotonics, he recognized that the department has also quickly been developing a reputation in other emerging areas, including wearables, synthetic biology, and neurorehabilitation and stroke prediction.
</p><p>
	Hielscher highlighted the recent work of <a href="https://engineering.nyu.edu/faculty/rose-faghih?utm_source=ieee&utm_medium=email&utm_campaign=reputation-2022&utm_content=hielscher-bme" target="_blank">Rose Faghih</a>, working in smart wearables and data for mental health, <a href="https://engineering.nyu.edu/faculty/jef-d-boeke?utm_source=ieee&utm_medium=email&utm_campaign=reputation-2022&utm_content=hielscher-bme" rel="noopener noreferrer" target="_blank">Jef Boeke</a>, a synthetic biology pioneer, and <a href="https://engineering.nyu.edu/faculty/s-farokh-atashzar?utm_source=ieee&utm_medium=email&utm_campaign=reputation-2022&utm_content=hielscher-bme" rel="noopener noreferrer" target="_blank">S. Farokh Atashzar</a>, doing work in neurorehabilitation and stroke prediction. Atashzar’s work was highlighted last year in the pages of <a href="https://spectrum.ieee.org/nyu-tandon-robotics" target="_self"><em>IEEE Spectrum</em></a>.
</p><p>
	“Rose Faghih is leveraging all kinds of sensors to make inferences about the mental state of patients, to determine if someone is depressed or schizophrenic, and then possibly have a feedback loop where you actually also treat them,” said Hielscher. “Jef Boeke is involved in what I term ‘wet engineering,’ and is currently involved in efforts to take cancer cells outside of the body to find a way to attack them, or reprogram them.”
</p><h2>Future Collaborations</h2><p>
	As NYU Tandon’s BME department goes forward, Hielscher’s aim is that the department becomes a trusted source for the medical school, and that partnership enables key technologies to go from an unmet clinical need or an idea in a lab to a patient’s bedside in a 3-5 year timeframe.
</p><p>
	“What I really would like, “Hielscher concluded, “is that if somebody in the medical school has a problem, the first thing they would say is, ‘Oh, I’ll call the engineering school. I bet there’s somebody there that can help me.’ We can work together to benefit patients, and we’re starting this already.”<span class="ieee-end-mark"></span>
</p></br></br>]]></description><pubDate>Mon, 31 Oct 2022 14:46:16 +0000</pubDate><guid>https://spectrum.ieee.org/nyu-biomedical-engineering</guid><category>Biomedical engineering</category><category>Nyu tandon</category><category>Cancer</category><category>Optics</category><category>Medical diagnostics</category><category>Medical imaging</category><dc:creator>Dexter Johnson</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/medical-equipment-with-two-rings-each-with-concentric-sensors-and-wires-connected-to-a-computer-nearby.jpg?id=31997012&amp;width=980"></media:content></item><item><title>AI Helps Humans Level Up</title><link>https://spectrum.ieee.org/ai-tools</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-man-in-a-white-shirt-and-blue-blazer-moves-a-chess-piece-on-a-board.png?id=31999824&width=1245&height=700&coordinates=0%2C0%2C0%2C209"/><br/><br/><p>
<strong>Back in the </strong>mid-1970s, <em>IEEE</em> <em>Spectrum</em> senior editor Phil Ross played one of the first chess programs capable of vanquishing humans. He capitulated quickly—too quickly, it turned out: Although the program that beat him was good at openings and the middle game, it was terrible at the end game. Fast forward 50 years and the highest-ranked chess players in the world are AIs, with humans trailing far behind.
</p><p>
	As Ross points out in his online piece “
	<a href="https://spectrum.ieee.org/magnus-carlsen-chess-scandal-ai" target="_self">AI’s Grandmaster Status Overshadows Chess Scandal</a>,” the recent brouhaha involving world champion Magnus Carlsen and up-and-comer Hans Niemann highlights how chess-playing AIs, referred to as “engines” by the cognoscenti, have overtaken humans in terms of raw game-playing accuracy. The scandal also shows how AIs are being used by players at all levels to get better faster, fostering a boom in the sport.
</p><hr/><p>
	If you were born a few decades ago, your best shot at playing and learning from grandmasters was either to be lucky enough to know one or to somehow qualify for the high-level tournaments in which they participated. Nowadays, chess newbies can log in and play engines that far exceed their own abilities, learning strategies and moves in days or weeks that in the past might have taken months or years. Engines can also help neophytes and grandmasters alike analyze their own games to give them an edge against human opponents. In fact, if you burrow down the rabbit holes of chess YouTube or Twitch, you’ll find grandmasters giving move-by-move analyses of games that engines played 
	<em>against each other</em>. These AI tools, along with the humans who use them, are readily accessible: You can play the cybernetic versions of super grandmasters, like the open-source, current top-ranked chess engine <a href="https://stockfishchess.org/blog/2021/stockfish-14-1/" rel="noopener noreferrer" target="_blank">Stockfish 14.1</a>, not to mention tens of millions of human opponents on sites like <a href="https://www.chess.com/" rel="noopener noreferrer" target="_blank">Chess.com</a>, a global community of some 93 million players and a central player in the cheating controversy.
</p><p>
	While it is certainly true that bad actors use AI to cheat at chess—potentially posing an existential threat to the game, as Carlsen has suggested—it is equally true that the chess world has openly embraced AI and has been thriving as a result. Similar risk/reward calculations will need to be made in other domains.
</p><p class="pull-quote">
	“The well-designed user interfaces on these tools lower the threshold of entry but do not raise the ceiling of performance. Expert programmers will still be needed to make breakthrough programming innovations, like the next generation of AI-infused tools or post-AI supertools.” —Ben Shneidermann
</p><p>
	Take software development. On page 5 in this issue, the journalist Craig S. Smith looks at the proliferation of AI-powered code-writing assistants and how they can help programmers compose better programs faster and guide nonprogrammers to instantiate their ideas in software. As for the risk of these AI assistants driving humans out of programming, the experts Smith talked to don’t believe that human coders are going to be replaced anytime soon.
</p><p>
	Instead, programmers are learning that AI can automate routine tasks such as writing unit tests that verify discrete chunks of code, which can free up a portion of a developer’s time to spend on more creative endeavors. Amazon’s CodeWhisperer, GitHub’s Copilot, and Microsoft’s TiCoder are all based on large language models trained up on massive code bases. Among other things, these coding assistants suggest auto-completions for developers as they write code and can also make suggestions for executable instructions using natural language. In the case of TiCoder, a feedback mechanism interrogates programmers to resolve ambiguities so it can generate the cleanest possible code.
</p><h3></h3><br/><div class="rblad-ieee_in_content"></div><p>
	“Like compilers for high-level languages, code-checkers, and interactive development environments, AI-infused coding tools will speed the programmer's work,” says Ben Shneiderman, IEEE Fellow and author of “
	<a href="https://global.oup.com/academic/product/human-centered-ai-9780192845290?cc=us&lang=en&" rel="noopener noreferrer" target="_blank">Human-Centered AI</a>” (Oxford University Press, 2022). “The well-designed user interfaces on these tools lower the threshold of entry but do not raise the ceiling of performance. Expert programmers will still be needed to make breakthrough programming innovations, like the next generation of AI-infused tools or post-AI supertools.”
</p><p>
	 Indeed, while some programmers might be concerned about algorithms edging them out of their jobs, they need only look to chess to see how technology can surpass human abilities and at the same time help people push past their own limits.
</p>]]></description><pubDate>Mon, 31 Oct 2022 11:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/ai-tools</guid><category>Artificial intelligence</category><category>Machine learning</category><category>Chess</category><category>Code</category><category>Developers</category><category>Programmers</category><dc:creator>Harry Goldstein</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/a-man-in-a-white-shirt-and-blue-blazer-moves-a-chess-piece-on-a-board.png?id=31999824&amp;width=980"></media:content></item><item><title>Pong Was Boring—And People Loved It</title><link>https://spectrum.ieee.org/pong-video-game</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-boxy-retro-game-console-with-a-crt-screen-with-the-word-pong-on-the-front.jpg?id=31999182&width=1245&height=700&coordinates=0%2C371%2C0%2C371"/><br/><br/><p>
<strong>November marks the</strong> 50th anniversary of 
	<em>Pong</em>. Why should we care?
</p><p>
	For starters, 
	<em>Pong</em> is the first video game that millions of people welcomed into their homes to play on their own televisions. <a href="https://spectrum.ieee.org/pong" target="_self"><em>Pong</em> kick-started a global video-game industry</a> that is now worth upwards of <a href="https://earthweb.com/how-much-is-the-gaming-industry-worth/" rel="noopener noreferrer" target="_blank">US $300 billion</a>. And <em>Pong</em> still has a place in active research, for <a href="https://spectrum.ieee.org/ai-pong-uncertainty" target="_self">training AI algorithms</a>, <a href="https://spectrum.ieee.org/pong-in-a-dish" target="_blank">strengthening neural networks</a>, and developing the <a href="https://spectrum.ieee.org/elon-musk-brain-neuralink" target="_self">brain-machine interface called Neuralink</a>, among other things.
</p><hr/><p>
	And yet as a Gen-Xer born too late to have enjoyed 
	<em>Pong</em> as a child, I have trouble fathoming how anyone could sit in front of a TV watching a square dot—not even a round ball—bounce back and forth across the dark, featureless screen. Was this really fun? To celebrate the half-century persistence of <em>Pong</em>, I set out to discover why so many people love the most boring video game of all time.
</p><h2>What is <em>Pong?</em></h2><p>
	In case you have somehow never encountered 
	<em>Pong</em>, it is simplicity itself—represented on screen as a moving dot, two vertical lines with which to hit the dot, and squared-off digits reflecting the score. The game mimics table tennis, with two people competing against each other to get the ball past their opponent. Players can move their paddle vertically (but not horizontally) to deflect the ball, and the ball can also bounce off the top and bottom of the screen. The ball speeds up the longer you rally, and each hit is accompanied by a satisfying “click” sound. If you miss, your opponent scores a point. The first player to reach 11 points wins.
</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="A bearded white man smiles as he stands in front of a retro game console." class="rm-shortcode rm-resized-image" data-rm-shortcode-id="abb9ec2384c41b0f49126f325550c83e" data-rm-shortcode-name="rebelmouse-image" id="1c49d" loading="lazy" src="https://spectrum.ieee.org/media-library/a-bearded-white-man-smiles-as-he-stands-in-front-of-a-retro-game-console.jpg?id=31999194&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">Allan Alcorn, shown here in 2011, designed Pong as an exercise, to learn about how to design video games.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;"><a href="https://commons.wikimedia.org/wiki/File:Al_Alcorn,_Pong,_CHM_2011.jpg" rel="noopener noreferrer" target="_blank">vonguard/Wikipedia</a></small>
</p><p>
	The game was actually created as part of a training exercise to get a young engineer named 
	<a href="https://archive.computerhistory.org/resources/access/text/2012/09/102658257-05-01-acc.pdf" rel="noopener noreferrer" target="_blank">Allan Alcorn</a> acquainted with video-game development. <a href="https://www.si.edu/media/NMAH/NMAH-AC1498_Transcript_NolanBushnell.pdf" rel="noopener noreferrer" target="_blank">Nolan Bushnell </a>and <a href="https://www.youtube.com/watch?v=6arAY7cUa5I&t=6s" rel="noopener noreferrer" target="_blank">Samuel “Ted” Dabney</a> had recently started <a href="https://www.computerhistory.org/revolution/computer-games/16/185" rel="noopener noreferrer" target="_blank">Atari</a>, and they hired Alcorn as their third employee. Alcorn had majored in electrical engineering and computer science at the University of California, Berkeley, but had never played a video game before. At the time, video games were mostly the domain of research laboratories and universities. Under the guise of a contract for General Electric, Bushnell asked Alcorn to design a game similar to ping-pong that could be played on a television.
</p><p>
	Because 
	<em>Pong</em> was developed before the debut of microcontrollers, the circuit board consisted of hardwired, discrete logic components. The arcade version included 66 integrated circuits, a pair of <a href="https://spectrum.ieee.org/chip-hall-of-fame-signetics-ne555" target="_self">555 timers</a>, and a few transistors. (For a deeper dive into <em>Pong</em>’s design, see Hugo Holden’s article, “<a href="https://www.worldphaco.com/uploads/LAWN_TENNIS.pdf" rel="noopener noreferrer" target="_blank">Atari Pong E Circuit Analysis & Lawn Tennis: Building a Digital Video Game with 74 Series TTL IC’s</a>,” Dan Boris’s <a href="http://atarihq.com/danb/files/PongSchematic.pdf" rel="noopener noreferrer" target="_blank">redraw of the circuit diagram</a>, and Ricardo Ramos’s account of <a href="https://ricardoramos.me/pong-board/" rel="noopener noreferrer" target="_blank">building a <em>Pong</em> clone</a>.)
</p><p class="pull-quote">
	To celebrate the half-century persistence of 
	<em>Pong</em>, I set out to discover why so many people love the most boring video game of all time.
</p><p>
	Bushnell understood the economics of pinball machines and other arcade games, and so he decided to add a coin box to the 
	<em>Pong</em> arcade console [pictured at top]. He beta tested it at Andy Capp’s Tavern in Sunnyvale, Calif. According to <em>Pong</em> lore, people lined up for their turn to stuff quarters into the machine and play. Eventually, the manager called Alcorn because the coin box was jammed to overflowing and the game had stopped working.
</p><p>
	Atari sold 
	<a href="https://www.computerhistory.org/revolution/computer-games/16/183" rel="noopener noreferrer" target="_blank">about 35,000 consoles</a> for use in arcades, bars, and restaurants, but the real game changer, as it were, didn’t happen until 1975, when Atari introduced <em>Home Pong</em>, for play on your own TV set. The controller was a brown box with a dial on each side, forcing players to play shoulder to shoulder. It was advertised as working with any television set, but you could only play the one game. Later versions had individual controllers with long cords that allowed players to sit back more comfortably. Hundreds of thousands of <em>Pong</em> sets were distributed through the department store Sears. The game was also one of the original titles included with the <a href="https://spectrum.ieee.org/the-consumer-electronics-hall-of-fame-atari-2600" target="_self">Atari 2600</a>, a video-game system introduced in 1977 that let you play different games by swapping out the game cartridge.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A game console for the video game Pong has two knobs and a digital readout that says Tele-Games." class="rm-shortcode" data-rm-shortcode-id="4def229fa141131475c291a823b38a21" data-rm-shortcode-name="rebelmouse-image" id="740c5" loading="lazy" src="https://spectrum.ieee.org/media-library/a-game-console-for-the-video-game-pong-has-two-knobs-and-a-digital-readout-that-says-tele-games.jpg?id=31999216&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Atari’s home version of Pong was released through Sears in 1975.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..."><a href="https://commons.wikimedia.org/wiki/File:TeleGames-Atari-Pong.png" rel="noopener noreferrer" target="_blank">Evan Amos/Wikipedia</a></small>
</p><p>
	Meanwhile, Atari had to contend with the problems of 
	<em>Pong</em>’s popularity. Back in 1972, Magnavox had introduced its Odyssey home video-game system, which included a game called <em>Table Tennis</em>. Two years later, it <a href="https://thinksetmag.com/insights/digital-detective-pong" rel="noopener noreferrer" target="_blank">filed a lawsuit</a> against Atari alleging copyright infringement; that suit was settled out of court. Rival companies brought out similar <em>Pong</em>-like games, as did Atari itself. Bushnell wanted a one-player <em>Pong</em>-type game, so he worked with <a href="https://www.retrogamer.net/blog_post/remembering-steve-bristow/" rel="noopener noreferrer" target="_blank">Steve Bristow</a> and <a href="https://www.thehenryford.org/explore/stories-of-innovation/visionaries/steve-wozniak/" rel="noopener noreferrer" target="_blank">Steve Wozniak</a> to develop <a href="https://spectrum.ieee.org/atari-breakout" target="_self"><em>Breakout</em></a>, a brick-breaking game that also proved wildly popular. In 2015, the <a href="https://www.museumofplay.org/" rel="noopener noreferrer" target="_blank">Strong National Museum of Play</a>, in Rochester, N.Y., inducted <em>Pong</em> into its <a href="https://www.museumofplay.org/exhibits/world-video-game-hall-of-fame/inducted-games/" rel="noopener noreferrer" target="_blank">World Video Game Hall of Fame</a>, citing its popularity and influence in launching the video-game industry.
</p><p>
<em>Pong</em> was not the first video game, nor the first table-tennis video game, nor the first home video game, and yet it was the one that introduced millions of children and their parents to the idea of video gaming.
</p><h2>But was <em>Pong</em> fun?</h2><p>
	I first encountered 
	<em>Pong</em> in the early 1980s, on an Atari 2600 at my kindergarten best friend’s house. I didn’t see the magic then, and I still don’t. How was this <em>the</em> game that launched a billion-dollar industry? I recently posed this question to friends of a certain age and was surprised to receive a resounding chorus of cheers for <em>Pong</em>. Yes, <em>Pong</em> really was fun! My curiosity piqued, I followed up with some informal interviews and an entirely unscientific sampling methodology, and I have compiled the top five reasons people seem to love <em>Pong</em>.
</p><p>
	Above all, the 
	<strong>novelty factor</strong> seems to propel 
	<em>Pong</em> to the top of the pantheon of video games. <a href="https://computerhistory.org/profile/david-brock/" rel="noopener noreferrer" target="_blank">David C. Brock</a>, director of curatorial affairs at the Computer History Museum, fondly remembered playing <em>Pong</em> at home and in a restaurant in Pennsylvania. <em>Pong</em> was so new and different that it made an impression, Brock recalls. Pinball may literally have all the bells and whistles, but it was a game from the previous century. <em>Pong</em> was the future, even if that future was a black-and-white television screen.
</p><h3></h3><br/><h2>Top 5 Reasons People Love Pong</h2><p>
1. Novelty
</p><p>
2. Sociability
</p><p>
3. Calming Effect (ASMR)
</p><p>
4. The Challenge
</p><p>
5. Nostalgia
</p><p>
	Connected to the novelty factor is that 
	<em>Pong</em> is <strong>an inherently social game</strong>. No longer were players competing one by one to get the highest score. Now you could compete directly against your friends and family. My University of South Carolina colleague <a href="https://sc.edu/study/colleges_schools/artsandsciences/history/our_people/directory/risk.php" rel="noopener noreferrer" target="_blank">James Risk</a> grew up in Mooresville, Indiana, a small town southwest of Indianapolis. His house was the first in the neighborhood to have a home version of <em>Pong</em>. James’s father would never relinquish his turn, even when he lost, so James and his siblings had to duke it out to be Player 2. Even the family cat would get into the action, chasing the moving dot on the screen.
</p><p>
	Lynn Heidelbaugh, a museum curator in Washington, D.C., remembers watching her older brothers play 
	<em>Pong</em>. She found the game mesmerizing—watching the ball move back and forth, bouncing at angles off the paddles, each hit accompanied by a satisfying chirp. It’s not a stretch to think that <em>Pong</em> induces <strong>autonomous sensory meridian response, or ASMR</strong>, the calming and euphoric feeling triggered by repetitive auditory or visual stimuli. <em>Pong</em>’s distinctive sound was Alcorn’s response to Bushnell’s request for the roar of crowds. Alcorn crafted the sound from tones that already existed in the sync generator.
</p><p>
	Despite the simplicity of the game, 
	<em>Pong</em> was surprisingly <strong>challenging to play</strong>. The controllers were not particularly sensitive, and it was easy to miss a ball you thought you had. For many players, this imperfection made the game addictive, as they kept trying to improve. Another defect (or feature, as Alcorn preferred to think of it) was that the paddles did not move fully to the top or bottom of the screen. This was due to a problem in the circuitry. Alcorn could have fixed it, but it turned out to be a happy accident. Had the paddles reached the edges, really good players could have rallied indefinitely. Instead, the game inevitably ends after a few minutes, only to be started again by gamers intent on winning.
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="18155cb8154a8536878b40915cfea3ed" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/e4VRgY3tkh0?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	A final reason people love 
	<em>Pong</em> is <strong>nostalgia</strong>, whether that’s a recalling of childhood memories or a wistfulness for a past they never had. <a href="https://www.eiu.edu/geoscience/faculty.php?id=kjlewandowski&subcat=" rel="noopener noreferrer" target="_blank">Katherine Lewandowski</a>, now a geologist at Eastern Illinois University, didn’t play much <em>Pong</em> growing up, but was reintroduced to the game as an undergraduate at Vanderbilt in the 1990s. She says she was a novice compared to her friends, who had been playing for years, but she still enjoyed a spin at the console at Obie’s Pizza while waiting for their deep dish.
</p><p>
<em>IEEE Spectrum’s </em>special projects editor <a href="https://spectrum.ieee.org/u/stephen-cass" target="_self">Stephen Cass</a> grew up in Ireland. By the time arcade games arrived there, Cass says, <em>Pong</em> was not among the offerings—manufacturers had already moved on to other titles. His strongest <em>Pong</em> memory is thus not of the game but of the 1994 Frank Black song “Whatever Happened to Pong?,” which leveraged the ’90s trend of ’70s nostalgia. A few years later, <em>Pong</em> made a <a href="https://70s-show-diary.tumblr.com/post/118881047312/one-of-the-great-somewhat-reoccurring-plots-of" rel="noopener noreferrer" target="_blank">recurring appearance</a> on season 1 of <em>That ’70s Show</em>, thereby cementing the nostalgia.
</p><p>
	For me, though, 
	<em>Pong</em> doesn’t hold a candle to <em>Frogger</em>. My father, who was an early adopter of almost all things tech, never saw value in video games, so I had to run down the street to my friend’s house to play <em>Frogger</em> on her Atari. I love that pixelated frog and the many ways to die—I was not particularly good at the game and died frequently. To this day, references to <em>Frogger</em> inevitably surface whenever I make ill-timed attempts at jaywalking in the city.
</p><p>
I suspect that every generation of the past half century has its game, the one that introduced them to video games, that triggers waves of nostalgia and fond memories, the old favorite that people are excited to see when they serendipitously encounter it in a pizza parlor or on an emulator in a museum. It could be 
	<em>Pong</em> or <em>Frogger</em> or <em>Pac-Man</em> or <em>Tetris</em> or <em>Super Mario Bros.</em> or <em>Space Invaders</em>—or the next big thing! What game do you think we will be talking about in 50 years?
</p><p>
<em>Part of a </em><a href="https://spectrum.ieee.org/collections/past-forward/" target="_self"><em>continuing series</em></a> <em>looking at photographs of historical artifacts that embrace the boundless potential of technology.</em>
</p><p>
<em>An abridged version of this article appears in the November 2022 print issue as “In Praise of Pong.” </em>
</p>]]></description><pubDate>Sun, 30 Oct 2022 15:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/pong-video-game</guid><category>Al alcorn</category><category>Nolan bushnell</category><category>Past forward</category><category>Pong</category><category>Type:departments</category><category>Video games</category><dc:creator>Allison Marsh</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-boxy-retro-game-console-with-a-crt-screen-with-the-word-pong-on-the-front.jpg?id=31999182&amp;width=980"></media:content></item><item><title>This Implant Turns Brain Waves Into Words</title><link>https://spectrum.ieee.org/brain-computer-interface-speech</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-man-using-an-interface-looking-at-a-screen-with-words-on-it.png?id=32005613&width=1245&height=700&coordinates=156%2C0%2C157%2C0"/><br/><br/><p>
<strong>A computer screen </strong>shows the question “Would you like some water?” Underneath, three dots blink, followed by words that appear, one at a time: “No I am not thirsty.”
</p><p>
	It was brain activity that made those words materialize—the brain of a man who has not spoken for more than 15 years, ever since a stroke damaged the connection between his brain and the rest of his body, leaving him mostly paralyzed. He has used many other technologies to communicate; most recently, he used a pointer attached to his baseball cap to tap out words on a touchscreen, a method that was effective but slow. He volunteered for 
	<a href="https://changlab.ucsf.edu/" target="_blank">my research group</a>’s clinical trial at the <a href="https://www.ucsf.edu/" target="_blank">University of California, San Francisco</a> in hopes of pioneering a faster method. So far, he has used the brain-to-text system only during research sessions, but he wants to help develop the technology into something that people like himself could use in their everyday lives.
</p><hr/><p>
	In 
	<a href="https://www.nejm.org/doi/full/10.1056/NEJMoa2027540" target="_blank">our pilot study</a>, we draped a thin, flexible electrode array over the surface of the volunteer’s brain. The electrodes recorded neural signals and sent them to a speech decoder, which translated the signals into the words the man intended to say. It was the first time a paralyzed person who couldn’t speak had used neurotechnology to broadcast whole words—not just letters—from the brain.
</p><p>
	That trial was the culmination of more than a decade of research on the underlying brain mechanisms that govern speech, and we’re enormously proud of what we’ve accomplished so far. But we’re just getting started. 
	<a href="http://changlab.ucsf.edu/" target="_blank">My lab at UCSF</a> is working with colleagues around the world to make this technology safe, stable, and reliable enough for everyday use at home. We’re also working to improve the system’s performance so it will be worth the effort.
</p><h2>How neuroprosthetics work</h2><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="A series of three photographs shows the back of a man\u2019s head that has a device and a wire attached to the skull. A screen in front of the man shows three questions and responses, including \u201cWould you like some water?\u201d and \u201cNo I am not thirsty.\u201d" class="rm-shortcode rm-resized-image" data-rm-shortcode-id="9c244396817e338bf94872811b9ab530" data-rm-shortcode-name="rebelmouse-image" id="dfde7" loading="lazy" src="https://spectrum.ieee.org/media-library/a-series-of-three-photographs-shows-the-back-of-a-man-u2019s-head-that-has-a-device-and-a-wire-attached-to-the-skull-a-screen-i.png?id=32001054&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">The first version of the brain-computer interface gave the volunteer a vocabulary of 50 practical words. </small>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">University of California, San Francisco</small></p><p>
	Neuroprosthetics have come a long way in the past two decades. Prosthetic implants for hearing have advanced the furthest, with designs that interface with the 
	<a href="https://en.wikipedia.org/wiki/Cochlear_implant" target="_blank">cochlear nerve</a> of the inner ear or directly into the <a href="https://www.mayoclinic.org/tests-procedures/auditory-brainstem-implant/about/pac-20384649" target="_blank">auditory brain stem</a>. There’s also considerable research on <a href="https://spectrum.ieee.org/french-regulators-approve-human-trial-of-a-bionic-eye" target="_self">retinal</a> and <a href="https://spectrum.ieee.org/progress-toward-a-brain-implant-for-the-blind" target="_self">brain implants</a> for vision, as well as efforts to give people with prosthetic hands <a href="https://spectrum.ieee.org/bionic-hands-let-amputees-feel-and-grip" target="_self">a sense of touch</a>. All of these sensory prosthetics take information from the outside world and convert it into electrical signals that feed into the brain’s processing centers.
</p><p>
	The opposite kind of neuroprosthetic records the electrical activity of the brain and converts it into signals that control something in the outside world, such as a 
	<a href="https://spectrum.ieee.org/a-better-way-for-brains-to-control-robotic-arms" target="_self">robotic arm</a>, a <a href="https://spectrum.ieee.org/quadriplegic-pilots-race-for-gold-in-cybathlon-brain-race" target="_self">video-game controller</a>, or a <a href="https://spectrum.ieee.org/new-record-for-typing-by-brain-paralyzed-man-uses-brain-implant-to-type-8-words-per-minute" target="_self">cursor</a> on a computer screen. That last control modality has been used by groups such as the <a href="https://www.braingate.org/" target="_blank">BrainGate consortium</a> to enable paralyzed people to <a href="https://spectrum.ieee.org/people-paralysis-command-computers-wirelessly" target="_self">type words</a>—sometimes one letter at a time, sometimes using an autocomplete function to speed up the process.
</p><p>
	For that typing-by-brain function, an implant is typically placed in the motor cortex, the part of the brain that controls movement. Then the user imagines certain physical actions to control a cursor that moves over a virtual keyboard. Another approach, pioneered by some of my collaborators in a 
	<a href="https://www.nature.com/articles/s41586-021-03506-2.epdf?sharing_token=DsRZtRuFdieF5Yaw89p1GdRgN0jAjWel9jnR3ZoTv0No0Kktd9EUuDWeYWONAJ_7c9Vh-4dwWbu73lNBtR0SQIf6IATlhZ46V90CDPbxsChH4nCQRro7BGVvoyq7J1WjdI7xyn5OqpUdtG17JjQqmnEw1vWhwTgHs15FL3T_UOt48eOhTNkpTZ2H2CIz3RxMjlpXcdtJjKbGa1Ecdmbxh24IPyhEhFKcZ3e6zl5sF2YI4i7R7KRSp2iyVjkqgbvGAaTaWTrtF-jHWV5gqTXgF8vPfRjc-_gqpuUB58vK_mTr00cZmL7718XZrMGfw2ow&tracking_referrer=spectrum.ieee.org" target="_self">2021 paper</a>, had one user imagine that he was holding a pen to paper and was writing letters, creating signals in the motor cortex that were translated into text. That approach <a href="https://spectrum.ieee.org/braincomputer-interface-smashes-previous-record-for-typing-speed" target="_self">set a new record for speed</a>, enabling the volunteer to write about 18 words per minute.
</p><p>
	In my lab’s research, we’ve taken a more ambitious approach. Instead of decoding a user’s intent to move a cursor or a pen, we decode the intent to control the vocal tract, comprising dozens of muscles governing the larynx (commonly called the voice box), the tongue, and the lips.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A photo taken from above shows a room full of computers and other equipment with a man in a wheelchair in the center, facing a screen. " class="rm-shortcode" data-rm-shortcode-id="a99c14fa3abed1263e636f1a3c75356d" data-rm-shortcode-name="rebelmouse-image" id="76dc9" loading="lazy" src="https://spectrum.ieee.org/media-library/a-photo-taken-from-above-shows-a-room-full-of-computers-and-other-equipment-with-a-man-in-a-wheelchair-in-the-center-facing-a-s.png?id=32001098&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">The seemingly simple conversational setup for the paralyzed man [in pink shirt] is enabled by both sophisticated neurotech hardware and machine-learning systems that decode his brain signals. </small>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">University of California, San Francisco</small></p><p>
	I began working in this area more than 10 years ago. As a neurosurgeon, I would often see patients with severe injuries that left them unable to speak. To my surprise, in many cases the locations of brain injuries didn’t match up with the syndromes I learned about in medical school, and I realized that we still have a lot to learn about how language is processed in the brain. I decided to study the underlying neurobiology of language and, if possible, to develop a brain-machine interface (BMI) to restore communication for people who have lost it. In addition to my neurosurgical background, my team has expertise in linguistics, electrical engineering, computer science, bioengineering, and medicine. Our ongoing clinical trial is testing both hardware and software to explore the limits of our BMI and determine what kind of speech we can restore to people.
</p><h2>The muscles involved in speech</h2><p>
	Speech is one of the behaviors that 
	<a href="https://bmcbiol.biomedcentral.com/articles/10.1186/s12915-017-0405-3" target="_blank">sets humans apart</a>. Plenty of other species vocalize, but only humans combine a set of sounds in myriad different ways to represent the world around them. It’s also an extraordinarily complicated motor act—some experts believe it’s the most complex motor action that people perform. Speaking is a product of modulated air flow through the vocal tract; with every utterance we shape the breath by creating audible vibrations in our laryngeal vocal folds and changing the shape of the lips, jaw, and tongue.
</p><p>
	Many of the muscles of the vocal tract are quite unlike the joint-based muscles such as those in the arms and legs, which can move in only a few prescribed ways. For example, the muscle that controls the lips is a sphincter, while the muscles that make up the tongue are governed more by hydraulics—the tongue is largely composed of a fixed volume of muscular tissue, so moving one part of the tongue changes its shape elsewhere. The physics governing the movements of such muscles is totally different from that of the biceps or hamstrings.
</p><p>
	Because there are so many muscles involved and they each have so many degrees of freedom, there’s essentially an infinite number of possible configurations. But when people speak, it turns out they use a relatively small set of core movements (which differ somewhat in different languages). For example, when English speakers make the “d” sound, they put their tongues behind their teeth; when they make the “k” sound, the backs of their tongues go up to touch the ceiling of the back of the mouth. Few people are conscious of the precise, complex, and coordinated muscle actions required to say the simplest word.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A man looks at two large display screens; one is covered in squiggly lines, the other shows text.\u00a0" class="rm-shortcode" data-rm-shortcode-id="cff1723a7666c03132d4cc94943f4e92" data-rm-shortcode-name="rebelmouse-image" id="ef273" loading="lazy" src="https://spectrum.ieee.org/media-library/a-man-looks-at-two-large-display-screens-one-is-covered-in-squiggly-lines-the-other-shows-text-u00a0.png?id=32001182&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Team member David Moses looks at a readout of the patient’s brain waves [left screen] and a display of the decoding system’s activity [right screen].</small>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">University of California, San Francisco</small></p><p>
	My research group focuses on the parts of the brain’s motor cortex that send movement commands to the muscles of the face, throat, mouth, and tongue. Those brain regions are multitaskers: They manage muscle movements that produce speech and also the movements of those same muscles for swallowing, smiling, and kissing.
</p><p>
	Studying the neural activity of those regions in a useful way requires both spatial resolution on the scale of millimeters and temporal resolution on the scale of milliseconds. Historically, noninvasive imaging systems have been able to provide one or the other, but not both. When we started this research, we found remarkably little data on how brain activity patterns were associated with even the simplest components of speech: phonemes and syllables.
</p><p>
	Here we owe a debt of gratitude to our volunteers. At the UCSF epilepsy center, patients preparing for surgery typically have electrodes surgically placed over the surfaces of their brains for several days so we can map the regions involved when they have seizures. During those few days of wired-up downtime, many patients volunteer for neurological research experiments that make use of the electrode recordings from their brains. My group asked patients to let us study their patterns of neural activity while they spoke words.
</p><p>
	The hardware involved is called 
	<a href="https://en.wikipedia.org/wiki/Electrocorticography" target="_blank">electrocorticography</a> (ECoG). The electrodes in an ECoG system don’t penetrate the brain but lie on the surface of it. Our arrays can contain several hundred electrode sensors, each of which records from thousands of neurons. So far, we’ve used an array with 256 channels. Our goal in those early studies was to discover the patterns of cortical activity when people speak simple syllables. We asked volunteers to say specific sounds and words while we recorded their neural patterns and tracked the movements of their tongues and mouths. Sometimes we did so by having them wear colored face paint and using a computer-vision system to extract the kinematic gestures; other times we used an ultrasound machine positioned under the patients’ jaws to image their moving tongues.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A diagram shows a man in a wheelchair facing a screen that displays two lines of dialogue: \u201cHow are you today?\u201d and \u201cI am very good.\u201d Wires connect a piece of hardware on top of the man\u2019s head to a computer system, and also connect the computer system to the display screen. A close-up of the man\u2019s head shows a strip of electrodes on his brain." class="rm-shortcode" data-rm-shortcode-id="d278d132f84073773405b4318e67f3ed" data-rm-shortcode-name="rebelmouse-image" id="3689a" loading="lazy" src="https://spectrum.ieee.org/media-library/a-diagram-shows-a-man-in-a-wheelchair-facing-a-screen-that-displays-two-lines-of-dialogue-u201chow-are-you-today-u201d-and-u.png?id=32001118&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">The system starts with a flexible electrode array that’s draped over the patient’s brain to pick up signals from the motor cortex. The array specifically captures movement commands intended for the patient’s vocal tract. A port affixed to the skull guides the wires that go to the computer system, which decodes the brain signals and translates them into the words that the patient wants to say. His answers then appear on the display screen.</small>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Chris Philpot</small></p><p>
	We used these systems to match neural patterns to movements of the vocal tract. At first we had a lot of questions about the neural code. One possibility was that neural activity encoded directions for particular muscles, and the brain essentially turned these muscles on and off as if pressing keys on a keyboard. Another idea was that the code determined the velocity of the muscle contractions. Yet another was that neural activity corresponded with coordinated patterns of muscle contractions used to produce a certain sound. (For example, to make the “aaah” sound, both the tongue and the jaw need to drop.) What we discovered was that there is a map of representations that controls different parts of the vocal tract, and that together the different brain areas combine in a coordinated manner to give rise to fluent speech.
</p><h2>The role of AI in today’s neurotech</h2><p>
	Our work depends on the advances in artificial intelligence over the past decade. We can feed the data we collected about both neural activity and the kinematics of speech into a neural network, then let the machine-learning algorithm find patterns in the associations between the two data sets. It was possible to make connections between neural activity and produced speech, and to use this model to produce computer-generated speech or text. But this technique couldn’t train an algorithm for paralyzed people because we’d lack half of the data: We’d have the neural patterns, but nothing about the corresponding muscle movements.
</p><p>
	The smarter way to use machine learning, we realized, was to break the problem into two steps. First, the decoder translates signals from the brain into intended movements of muscles in the vocal tract, then it translates those intended movements into synthesized speech or text.
</p><p>
	We call this a biomimetic approach because it copies biology; in the human body, neural activity is directly responsible for the vocal tract’s movements and is only indirectly responsible for the sounds produced. A big advantage of this approach comes in the training of the decoder for that second step of translating muscle movements into sounds. Because those relationships between vocal tract movements and sound are fairly universal, we were able to train the decoder on large data sets derived from people who weren’t paralyzed.
</p><h2>A clinical trial to test our speech neuroprosthetic</h2><p>
	The next big challenge was to bring the technology to the people who could really benefit from it.
</p><p>
	The National Institutes of Health (NIH) is funding 
	<a href="https://reporter.nih.gov/search/kfhqyDGlEk-BF89Lei3E3w/project-details/10113331" target="_blank">our pilot trial</a>, which began in 2021. We already have two paralyzed volunteers with implanted ECoG arrays, and we hope to enroll more in the coming years. The primary goal is to improve their communication, and we’re measuring performance in terms of words per minute. An average adult typing on a full keyboard can type 40 words per minute, with the fastest typists reaching speeds of more than 80 words per minute.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A man in surgical scrubs and wearing a magnifying lens on his glasses looks at a screen showing images of a brain.\u00a0" class="rm-shortcode" data-rm-shortcode-id="84af6259f58fc96ea52ef1660e994a3c" data-rm-shortcode-name="rebelmouse-image" id="37775" loading="lazy" src="https://spectrum.ieee.org/media-library/a-man-in-surgical-scrubs-and-wearing-a-magnifying-lens-on-his-glasses-looks-at-a-screen-showing-images-of-a-brain-u00a0.png?id=32001077&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Edward Chang was inspired to develop a brain-to-speech system by the patients he encountered in his neurosurgery practice. </small>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Barbara Ries</small></p><p>
	We think that tapping into the speech system can provide even better results. Human speech is much faster than typing: An English speaker can easily say 150 words in a minute. We’d like to enable paralyzed people to communicate at a rate of 100 words per minute. We have a lot of work to do to reach that goal, but we think our approach makes it a feasible target.
</p><p>
	The implant procedure is routine. First the surgeon removes a small portion of the skull; next, the flexible ECoG array is gently placed across the surface of the cortex. Then a small port is fixed to the skull bone and exits through a separate opening in the scalp. We currently need that port, which attaches to external wires to transmit data from the electrodes, but we hope to make the system wireless in the future.
</p><p>
	We’ve considered using penetrating microelectrodes, because they can record from smaller neural populations and may therefore provide more detail about neural activity. But the current hardware isn’t as robust and safe as ECoG for clinical applications, especially over many years.
</p><p>
	Another consideration is that penetrating electrodes typically require daily recalibration to turn the neural signals into clear commands, and research on neural devices has shown that speed of setup and performance reliability are key to getting people to use the technology. That’s why we’ve prioritized stability in 
	<a href="https://www.nature.com/articles/s41587-020-0662-5" rel="noopener noreferrer" target="_blank">creating a “plug and play” system</a> for long-term use. We conducted a study looking at the variability of a volunteer’s neural signals over time and found that the decoder performed better if it used data patterns across multiple sessions and multiple days. In machine-learning terms, we say that the decoder’s “weights” carried over, creating consolidated neural signals.
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="d4c17701550c87cb08654854e1344975" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/AfX-fH3A6Bs?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">University of California, San Francisco</small></p><p>
	Because our paralyzed volunteers can’t speak while we watch their brain patterns, we asked our first volunteer to try two different approaches. He started with a list of 50 words that are handy for daily life, such as “hungry,” “thirsty,” “please,” “help,” and “computer.” During 48 sessions over several months, we sometimes asked him to just imagine saying each of the words on the list, and sometimes asked him to overtly 
	<em>try</em> to say them. We found that attempts to speak generated clearer brain signals and were sufficient to train the decoding algorithm. Then the volunteer could use those words from the list to generate sentences of his own choosing, such as “No I am not thirsty.”
</p><p>
	We’re now pushing to expand to a broader vocabulary. To make that work, we need to continue to improve the current algorithms and interfaces, but I am confident those improvements will happen in the coming months and years. Now that the proof of principle has been established, the goal is optimization. We can focus on making our system faster, more accurate, and—most important— safer and more reliable. Things should move quickly now.
</p><p>
	Probably the biggest breakthroughs will come if we can get a better understanding of the brain systems we’re trying to decode, and how paralysis alters their activity. We’ve come to realize that the neural patterns of a paralyzed person who can’t send commands to the muscles of their vocal tract are very different from those of an epilepsy patient who can. We’re attempting an ambitious feat of BMI engineering while there is still lots to learn about the underlying neuroscience. We believe it will all come together to give our patients their voices back. <span class="ieee-end-mark"></span>
</p>]]></description><pubDate>Sat, 29 Oct 2022 15:00:03 +0000</pubDate><guid>https://spectrum.ieee.org/brain-computer-interface-speech</guid><category>Neuroprosthetics</category><category>Brain-machine interface</category><category>Neurotechnology</category><category>Brain implants</category><category>Speech</category><dc:creator>Edward Chang</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/a-man-using-an-interface-looking-at-a-screen-with-words-on-it.png?id=32005613&amp;width=980"></media:content></item><item><title>How This Startup Cut Production Costs of Millimeter Wave Power Amplifiers</title><link>https://spectrum.ieee.org/millimeter-wave-power-amplifier-startup</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/group-of-people-standing-outside-under-a-tree-smiling-at-camera.jpg?id=32001512&width=1245&height=700&coordinates=0%2C0%2C0%2C469"/><br/><br/><p><a href="https://elvespeed.com/founder" rel="noopener noreferrer" target="_blank">Diana Gamzina</a> is on a mission to drastically reduce the price of millimeter-wave power amplifiers. The vacuum-electronics devices are used for communication with distant space probes and for other applications that need the highest data rates available.</p><p>The amplifiers can cost as much as US $1 million apiece because they’re made using costly, high-precision manufacturing and manual assembly. Gamzina’s startup,<a href="https://elvespeed.com/" rel="noopener noreferrer" target="_blank"> Elve</a>, is using advanced materials and new manufacturing technologies to lower the unit price.</p><hr/><p>It can take up to a year to produce one of the amplifiers using conventional manufacturing processes, but Elve is already making about one per week, Gamzina says. Elve’s process enables sales at about 10 percent of the usual price, making large-volume markets more accessible.</p><p>Launched in June 2020, the startup produces affordable systems for wireless connections that deliver optical fiber quality, or what Gamzina calls <em>elvespeed </em><em>connectivity</em>. The company’s name, she says, refers to atmospheric emission of light and very low frequency perturbations due to electromagnetic pulse sources. Elves can be seen as a flat ring glowing in Earth’s upper atmosphere. They appear for just a few milliseconds and can grow to be up to 320 kilometers wide.</p><p>Based in Davis, Calif., Elve employs 12 people as well as a handful of consultants and advisors.</p><p>For her work with amplifiers, Gamzina, an IEEE senior member, was recognized with this year’s<a href="https://vacuumelectronics.org/veysaward.html" rel="noopener noreferrer" target="_blank"> Vacuum Electronics Young Scientist Award</a> from the<a href="https://eds.ieee.org/" rel="noopener noreferrer" target="_blank"> IEEE Electron Devices Society</a>. She received the award in April at the<a href="https://www.ieeeivec.org/index.html" rel="noopener noreferrer" target="_blank"> IEEE International Vacuum Electronics Conference</a>, in Monterey, Calif.</p><p>“Dr. Gamzina’s innovation and contributions to the industry are remarkable,” IEEE Member <a href="https://ieeexplore.ieee.org/author/37272687400" target="_blank">Jack Tucek</a>, general chair of the conference, said in a<a href="https://www.businesswire.com/news/home/20220510005453/en/Elve-CEO-Recognized-with-IEEE-Vacuum-Electronics-Young-Scientist-Award" rel="noopener noreferrer" target="_blank"> news release</a> about the award.</p><h2>Interactions between millimeter-wave signals and electrons</h2><p>In addition to running her company, Ganzima works as a staff scientist at the <a href="https://www6.slac.stanford.edu/" rel="noopener noreferrer" target="_blank">SLAC National Accelerator Laboratory</a>, in Menlo Park, Calif.—a U.S. <a href="https://www.energy.gov/national-laboratories" rel="noopener noreferrer" target="_blank">Department of Energy national lab</a> operated by Stanford University. It was in this role that she began speaking to industry representatives about how to expand the market for high-performance millimeter-wave amplifiers.</p><p>From those discussions, she found that lowering the price was key.</p><p>“Customers are paying between $250,000 and $1 million for each individual device,” she says. “When you hear these numbers, you realize why people don’t think this price is anywhere close to affordable for growing the market.”</p><p>Elve’s millimeter-wave power amplifier system weighs 4 kilograms, measures 23 centimeters by 15 cm by 8 cm, and is powered by a 28-volt DC input bus. The heart of the amplifier system is a set of traveling-wave tubes. TWTs are a subset of vacuum electronics that can amplify electromagnetic signals by more than a hundredfold over a wide bandwidth of frequencies. They’re commonly used for communications and radar imaging applications, Gamzina says.</p><p class="pull-quote">“Our ultimate goal is to be part of the terrestrial market such that the amplifiers are installed on cellphone towers, enabling high-data-rate communication in remote and rural locations all over the world.”</p><p>The TWT takes a millimeter-wave signal and makes it “interact with a high-energy electron beam so that the signal steals the energy from the electron beam. That’s how it gets amplified,” Gamzina says. “In some ways, it’s a simple concept, but it requires excellence in RF design, manufacturing, vacuum science, electron emission, and thermal management to make it all work.”</p><p>Elve’s millimeter-wave amplifiers enable communication with satellite networks and create long-distance ground-to-ground links.</p><p>“This allows enormous amounts of data to be sent, leading to higher data rates that are comparable to fiber or laser-type communication devices,” Gamzina says. “Another advantage is that the amplifier can operate in most inclement weather.”</p><p>The initial market for Elve’s amplifiers is the communications field, she says, especially for backhaul: the connections between base stations and the core network infrastructure.</p><p>“Our ultimate goal is to be part of the terrestrial market such that the amplifiers are installed on cellphone towers,” Gamzina says, “enabling high-data-rate communication in remote and rural locations all over the world.”</p><p>The company plans to develop millimeter-wave amplifiers for imaging and radar applications as well.</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="a red device on a white background" class="rm-shortcode" data-rm-shortcode-id="2636656805d5139450a4e44b2384684e" data-rm-shortcode-name="rebelmouse-image" id="40950" loading="lazy" src="https://spectrum.ieee.org/media-library/a-red-device-on-a-white-background.jpg?id=32001526&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Elve’s power amplifier containers house a traveling-wave tube, electronic power conditioner, and a cooling system.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Elve</small></p><h2>Building on research done at the SLAC Lab</h2><p>Gamzina’s work at Elve is related to research she is conducting at the SLAC lab as well as previous work she did as a development engineer with the millimeter-wave research group at the<a href="https://www.ucdavis.edu/" rel="noopener noreferrer" target="_blank"> University of California, Davis</a>. There she studied vacuum devices that operate at terahertz frequencies for use as power sources in particle accelerators, broadcast transmitters, industrial heating, radar systems, and communication satellites. She also led research programs in additive manufacturing techniques. She holds three U.S. patents related to the manufacture of vacuum-electronics devices.</p><p>To learn how to launch a startup, she took a class at<a href="https://www.stanford.edu/" rel="noopener noreferrer" target="_blank"> Stanford</a> on the subject and she attended a 10-week training course for founders offered by <a href="https://4thly.com/" rel="noopener noreferrer" target="_blank">4thly</a>, a global startup accelerator. She also says she learned a few things as a youngster while helping out at her father’s hydraulic equipment manufacturing company.</p><p>Raising funds to keep one’s business afloat is difficult for many startups, but it wasn’t the case for Elve, Gamzina says.</p><p>“We have good market traction, a good product, and a good team,” she says. “There’s been a lot of interest in what we’re doing.”</p><p>What has been a concern, she says, is that the company is growing so rapidly, it might be difficult to scale up production.</p><p>“Making hundreds or thousands of these has been a challenge to even comprehend,” she says. “But we are ready to go from the state we’re in to the next big jump.”</p>]]></description><pubDate>Thu, 27 Oct 2022 18:00:01 +0000</pubDate><guid>https://spectrum.ieee.org/millimeter-wave-power-amplifier-startup</guid><category>Ieee member news</category><category>Telecommunications</category><category>Career</category><category>Startups</category><category>Communications</category><category>Vacuum electronics</category><category>Millimeter wave power amplifiers</category><category>Type:ti</category><dc:creator>Kathy Pretz</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/group-of-people-standing-outside-under-a-tree-smiling-at-camera.jpg?id=32001512&amp;width=980"></media:content></item><item><title>Why Is Hydroelectricity So Green, and Yet Unfashionable?</title><link>https://spectrum.ieee.org/hydroelectric-power</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/an-aerial-view-of-a-long-concrete-dam-the-farther-end-of-yangtze-river-which-rests-on-the-bank-of-a-river-at-which-point-the.jpg?id=31978226&width=1245&height=700&coordinates=0%2C294%2C0%2C294"/><br/><br/><p>
<strong>I live in Manitoba</strong>, a province of Canada where all but a tiny fraction of electricity is generated from the potential energy of water. Unlike in British Columbia and Quebec, where generation relies on huge dams, our dams on the Nelson River are low, with hydraulic heads of no more than 30 meters, which creates only small reservoirs. Of course, the potential is the product of mass, the gravitational constant, and height, but the dams’ modest height is readily compensated for by a large mass, as the mighty river flowing out of Lake Winnipeg continues its course to Hudson Bay.
</p>
<p>
	You would think this is about as “green” as it can get, but in 2022 that would be a mistake. There is no end of gushing about China’s cheap solar panels—but when was the last time you saw a paean to hydroelectricity?
</p>
<hr/>
<p>
	Construction of large dams began before World War II. The United States got the Grand Coulee on the Columbia River, the Hoover Dam on the Colorado, and the dams of the Tennessee Valley Authority. After the war, construction of large dams moved to the Soviet Union, Africa, South America (Brazil’s Itaipu, at its completion in 1984 the world’s largest dam, with 14 gigawatts capacity), and Asia, where it culminated in China’s unprecedented effort. China now has three of the world’s six largest hydroelectric stations: Three Gorges, 22.5 GW (the largest in the world); Xiluodu, 13.86 GW; and Wudongde, 10.2 GW. Baihetan on the Jinsha River should soon begin full-scale operation and become the world’s second-largest station (16 GW).
</p>
<p>
	But China’s outsize drive for hydroelectricity is unique. By the 1990s, large hydro stations had lost their green halo in the West and come to be seen as environmentally undesirable. They are blamed for displacing populations, disrupting the flow of sediments and the migration of fish, destroying natural habitat and biodiversity, degrading water quality, and for the decay of submerged vegetation and the consequent release of methane, a greenhouse gas. There is thus no longer a place for Big Hydro in the pantheon of electric greenery. Instead, that pure status is now reserved above all for wind and solar. This ennoblement is strange, given that wind projects require enormous quantities of embodied energy in the form of steel for towers, plastics for blades, and concrete for foundations. The manufacture of solar panels involves the environmental costs from mining, waste disposal, and carbon emissions.
</p>
<p class="pull-quote">
	In 2020 the world’s hydro stations produced 75 percent more electricity than wind and solar combined and accounted for 16 percent of all global generation
</p>
<p>
	And hydro still matters more than any other form of renewable generation. In 2020, the world’s hydro stations produced 75 percent more electricity than wind and solar combined (4,297 versus 2,447 terawatt-hours) and accounted for 16 percent of all global generation (compared with nuclear electricity’s 10 percent). The share rises to about 60 percent in Canada and 97 percent in Manitoba. And some less affluent countries in Africa and Asia are still determined to build more such stations. The largest projects now under construction outside China are the 
	<a href="https://spectrum.ieee.org/squabbling-over-the-waters-of-the-river-nile" target="_self">Grand Ethiopian Renaissance Dam</a> on the White Nile (6.55 GW) and Pakistan’s Diamer-Bhasha (4.5 GW) and Dasu (4.3 GW) on the Indus.
</p>
<p>
	I never understood why dams have suffered such a reversal of fortune. There is no need to build megastructures, with their inevitable undesirable effects. And everywhere in the world there are still plenty of opportunities to develop modest projects whose combined capacities could provide not only excellent sources of clean electricity but also serve as long-term 
	<a href="https://spectrum.ieee.org/4-new-ways-to-store-renewable-energy-with-water" target="_self">stores of energy</a>, as reservoirs for drinking water and irrigation, and for recreation and aquaculture.
</p>
<p>
	I am glad to live in a place that is reliably supplied by electricity generated by low-head turbines powered by flowing water. Manitoba’s six stations on the Nelson River have a combined capacity slightly above 4 GW. Just try to get the equivalent here from solar in January, when the snow is falling and the sun barely rises above the horizon!
</p>
<div class="flourish-embed flourish-chart" data-src="visualisation/11327493?672664"><script src="https://public.flourish.studio/resources/embed.js"></script></div>
<p>
<em>This article appears in the November 2022 print issue as “Hydropower, the Forgotten Renewable.”</em>
</p>]]></description><pubDate>Thu, 27 Oct 2022 15:00:01 +0000</pubDate><guid>https://spectrum.ieee.org/hydroelectric-power</guid><category>Renewable energy</category><category>Hydroelectric power</category><category>Hydroelectric dam</category><dc:creator>Vaclav Smil</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/an-aerial-view-of-a-long-concrete-dam-the-farther-end-of-yangtze-river-which-rests-on-the-bank-of-a-river-at-which-point-the.jpg?id=31978226&amp;width=980"></media:content></item><item><title>Sophia Muirhead Is IEEE’s Next Executive Director</title><link>https://spectrum.ieee.org/ieee-executive-director-sophia-muirhead</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/woman-wearing-glasses-and-green-sweater-smiling-against-a-white-background.jpg?id=31995470&width=1245&height=700&coordinates=0%2C107%2C0%2C638"/><br/><br/><p>Sophia “Sophie” Muirhead is the IEEE’s next executive director and chief operating officer. She is the first woman to hold the position. She is set to start her new job on 1 January, succeeding <a href="https://spectrum.ieee.org/stephen-welby-profile" target="_self"><u>Stephen Welby</u></a>. </p><p>Muirhead has been the IEEE’s general counsel and chief compliance officer since 2019. During that time, she improved the company’s standards, finance, and publishing protocols; oversaw the evolution of the organization’s contracting process; and deftly navigated export controls, sanctions, and other compliance regulations. </p><hr/><p>“Sophia has a proven track record of working in collaboration with the Board of Directors and a deep understanding of our organization,” <a href="https://spectrum.ieee.org/ieee-presidents-note-a-promise-is-a-promise" target="_self"><u>K.J. Ray Liu</u></a>, IEEE president and CEO, said in a <a href="https://www.prnewswire.com/il/news-releases/ieee-announces-selection-of-sophia-muirhead-as-next-executive-director-and-chief-operating-officer-301630395.html" rel="noopener noreferrer" target="_blank"><u>news release</u></a> announcing her appointment. “Her breadth of experience will enhance the Board’s continuing work to provide a professional home for our members, and to serve the engineering and technology community worldwide.”</p><p>Muirhead is quoted in the release as saying she was “attracted to IEEE in 2019 because it is a mission-based organization designed to benefit the public interest [and] its members, and serve a greater purpose.”</p><p>“I am truly honored and excited to be selected as IEEE’s next executive director and COO,” she said. “I am looking forward to collaborating with IEEE’s leadership, members, and staff to further IEEE’s mission around the world.” </p><p>Prior to joining IEEE, she was senior vice president, chief legal officer, and corporate secretary at <a href="https://www.conference-board.org/us" rel="noopener noreferrer" target="_blank"><u>The Conference Board</u></a>, a global business membership organization.</p><p>Muirhead earned a juris doctor degree from <a href="https://www.harvard.edu/" rel="noopener noreferrer" target="_blank"><u>Harvard Law School</u></a> and holds a bachelor’s degree in political science from <a href="https://hunter.cuny.edu/" rel="noopener noreferrer" target="_blank"><u>Hunter College</u></a>, in New York City.</p>]]></description><pubDate>Tue, 25 Oct 2022 18:00:01 +0000</pubDate><guid>https://spectrum.ieee.org/ieee-executive-director-sophia-muirhead</guid><category>Ieee news</category><category>Ieee executive director</category><category>Sophia “sophie” muirhead</category><category>Stephen welby</category><category>K.j. ray liu</category><category>Type:ti</category><dc:creator>Kathy Pretz</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/woman-wearing-glasses-and-green-sweater-smiling-against-a-white-background.jpg?id=31995470&amp;width=980"></media:content></item><item><title>Build a Passive Radar With Software-Defined Radio</title><link>https://spectrum.ieee.org/passive-radar-with-sdr</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-flat-building-rooftop-with-a-chimney-stack-in-the-distance-through-the-tops-of-some-trees-the-top-of-the-empire-state-buildi.png?id=31994527&width=1245&height=700&coordinates=0%2C286%2C0%2C286"/><br/><br/><p>
<strong>Normally, when it </strong>comes to radio-related projects, my home of New York City is a <em>terrible</em> place to be. If we could see and hear radio waves, it would make an <a href="https://youtu.be/hvIg3PTJWxs?t=312" rel="noopener noreferrer" target="_blank">EDM rave</a> feel like a sensory deprivation tank. Radio interference plagues the metropolis. But for once, I realized I could use this kaleidoscope of electromagnetism to my advantage—with a passive radar station.
</p><p>
	Unlike conventional radar, 
	<a href="https://en.wikipedia.org/wiki/Passive_radar" rel="noopener noreferrer" target="_blank">passive radar</a> doesn’t send out pulses of its own and watch for reflections. Instead, it uses ambient signals. A reference antenna picks up a signal from, say, a cell tower, while a surveillance antenna is tuned to the same frequency. The reference and surveillance signals are compared. If a reflection from an object is detected, then the time it took to arrive at the surveillance antenna gives a range. Frequency shifts indicate the object’s speed via the <a href="https://en.wikipedia.org/wiki/Doppler_effect" rel="noopener noreferrer" target="_blank">Doppler effect</a>.
</p><hr/><p>
	I was interested in passive radar because I wanted to put a new software-defined radio (SDR) through its paces. I’ve checked in with amateur SDR developments for 
	<em>IEEE Spectrum</em> since 2006, when <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1705782" rel="noopener noreferrer" target="_blank">SDR became something remotely within a maker’s budget</a>. The biggest leap forward happened in 2012 when it was discovered that USB stick TV tuners using the <a href="https://www.realtek.com/en/products/communications-network-ics/item/rtl2832u" rel="noopener noreferrer" target="_blank">RTL2832U demodulator</a> chip could be tapped <a href="https://spectrum.ieee.org/a-40-softwaredefined-radio" target="_self">to make very cheap but effective SDR receivers</a>. An explosion of interest in SDRs followed. Building off the demand stimulated by this activity, a number of manufacturers have started making premium, but still relatively cheap, SDRs. This includes RTLx-based USB sticks built with better supporting components and designs versus the original TV tuners, and completely new receivers such as the <a href="https://www.sdrplay.com/rspdx/" rel="noopener noreferrer" target="_blank">RSPDx</a>. Some of these new SDRs can transmit as well as receive, such as the <a href="https://greatscottgadgets.com/hackrf/one/" rel="noopener noreferrer" target="_blank">HackRF One</a> or<a href="https://www.crowdsupply.com/lime-micro/limesdr-mini-2" rel="noopener noreferrer" target="_blank"> Lime Mini</a>.
</p><p>
	I was researching diving back into SDR with one of these devices when I spotted the CrowdSupply campaign for the US $399
	<a href="https://www.krakenrf.com/" rel="noopener noreferrer" target="_blank"> KrakenSDR</a>. It’s receive only, but it boasts not one or two tuners, but five! The tuners are based on the <a href="https://www.rtl-sdr.com/tag/r820t2/" rel="noopener noreferrer" target="_blank">RTL R820T2/R860 chip</a>, and they are combined with hardware that can automatically do coherence synchronization among them.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="The Kraken RF is a rectangular box with a cooling fan. The Pi 4 is single board computer whose width and height is that of a credit card. The battery pack is a large portable unit with a handle. The TV antennas have a long pole with a receiving element about a third of the way along. It sits in front of a longer reflecting element and behind a series of nine smaller elements." class="rm-shortcode" data-rm-shortcode-id="0633199b94e1c2b4ced6bb5e8637b16e" data-rm-shortcode-name="rebelmouse-image" id="8de7f" loading="lazy" src="https://spectrum.ieee.org/media-library/the-kraken-rf-is-a-rectangular-box-with-a-cooling-fan-the-pi-4-is-single-board-computer-whose-width-and-height-is-that-of-a-cre.png?id=31994356&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Both the KrakenRF SDR and the Raspberry Pi 4 [middle bottom] require a fair amount of power via USB C cables, so a battery pack [top middle] is needed for mobile operation. The Pi is connected to the SDR via a data link, and in turn the SDR is connected via coaxial cables to two directional TV antennas [right and left].</small>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">James Provost</small>
</p><p>
	What that means is that, for example, you can arrange five omnidirectional antennas in a circle, and do radio direction finding by looking at when a transmission arrives at each antenna. Normally, 
	<a href="https://spectrum.ieee.org/chasing-weather-balloons-with-sdr" target="_self">an amateur looking to do direction finding</a> would have to wave around a directional antenna, something difficult to do while, for example, driving a car.
</p><p>
	But it was the KrakenSDR’s ability to do passive radar that really caught my eye as a new capability in lowish-cost radio tech , so I plonked down the money. The next step was to get suitable antennas. The radio’s manufacturer, KrakenRF, recommends directional Yagi TV antennas for two reasons. First, while the KrakenSDR can work with many signals including FM radio or cell-tower transmissions, digital TV signals are best to work with because they are fairly evenly distributed across the channel’s broadcast band, unlike the narrower and more variable signals from an FM station. (KrakenRF notes that if you 
	<em>must</em> use an FM signal, pick a heavy-metal station “since heavy metal is closer to white noise.”) The second reason is that pointing a directional antenna away from the reference source means that it’s less likely to be swamped by the reference signal.
</p><p>
	I ordered two small and light
	<a href="https://www.homedepot.com/p/Outdoor-TV-Antenna-Digital-UHF-with-100-Miles-Range-744841871152/316916810" rel="noopener noreferrer" target="_blank"> $19 TV antennas</a>. Portability was important because I needed to carry my entire setup to and from my apartment building’s roof, where my particular location in an outer borough of the city provided more advantages. First, the sky above has a regular supply of aircraft landing and taking off from NYC’s airports—and large metal assemblies moving against an empty background are perfect radar test objects. Second, my roof has a line of sight to the Empire State Building, giving me the ability to choose as a reference signal any one of<a href="https://www.necrat.us/empiretv.html" rel="noopener noreferrer" target="_blank"> more than half a dozen</a> TV channels transmitted from its spire.
</p><p>
	I deployed my rig: a heavy-duty battery pack, the KrakenSDR, cables and antennas, along with a Raspberry Pi 4 to process data from the SDR. KrakenRF offers 
	<a href="https://github.com/krakenrf/krakensdr_doa/releases/" rel="noopener noreferrer" target="_blank">an SD card image</a> for the Pi that bundles an operating system configured to work with its preinstalled open-source software. It also sets up the Pi as a Wi-Fi access point with a Web interface. I really wish more companies would adopt this approach, as installing open-source software is often a frustrating exercise in trying to replicate the precise system environment it was developed in. Even if you want to ultimately install the KrakenSDR software somewhere other than a Pi, having a known-good setup is useful as a reference, and allows you to test the hardware.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="An illustration of an antenna on a building with arrows pointing from the Empire State Building to the antenna to an airplane.  " class="rm-shortcode" data-rm-shortcode-id="6ca27cc2d5229db1b7127f92884d2378" data-rm-shortcode-name="rebelmouse-image" id="6362b" loading="lazy" src="https://spectrum.ieee.org/media-library/an-illustration-of-an-antenna-on-a-building-with-arrows-pointing-from-the-empire-state-building-to-the-antenna-to-an-airplane.png?id=31994357&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..."> Comparing the time between the arrival of a signal from a broadcast transmitter and the arrival of a reflection of that signal lets you detect objects such as airplanes and estimate their range. Frequency shifts between the two signals allow you to plot the speed of the object away or toward the antennas along with the range. The trace on the right shows a plane moving away as it increases its speed.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">James Provost</small>
</p><p>
	I pointed the reference antenna toward the Empire State Building and retreated with the surveillance antenna behind the superstructure of my building’s stairwell. This was in a bid to shield the antenna from the reference signal and myself from the wind. Checking the feed from the antennas using the Web interface’s built-in spectrum analyzer, I discovered I was almost 
	<em>too</em> successful in choosing the Empire State’s transmitter tower as a source of radio illumination: The reference signal was saturating the receiver with the default gain setting of 27 <a href="https://en.wikipedia.org/wiki/Decibel" rel="noopener noreferrer" target="_blank">decibels</a>, so I dropped it down to just 2.7 dB.
</p><p>
	But intense illumination means bright reflections. With one hand I pointed the surveillance antenna at the overcast skies and held my phone in the other. Gratifyingly, I almost instantly started seeing a blip on the speed-versus-range radar plot, matched a few moments later by the rumble of an approaching jet. (The plot updates about once every 3 seconds.) Because of the strength of the echoes, I was able to raise the signal-cutoff threshold significantly, giving me radar returns uncluttered with noise, and often with multiple aircraft. A win for SDR!
</p><p>
	Admittedly, my passive radar setup doesn’t have much everyday value. But as a demonstration of how far and fast inexpensive SDR technology is advancing, it’s a clear signal.
</p><p>
<em>This article appears in the November 2022 print issue as “Passive Radar With the KrakenSDR.”</em>
</p>]]></description><pubDate>Tue, 25 Oct 2022 15:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/passive-radar-with-sdr</guid><category>Diy</category><category>Sdr</category><category>Software defined radio</category><category>Type:departments</category><dc:creator>Stephen Cass</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/a-flat-building-rooftop-with-a-chimney-stack-in-the-distance-through-the-tops-of-some-trees-the-top-of-the-empire-state-buildi.png?id=31994527&amp;width=980"></media:content></item><item><title>Can Alt-Fuel Credits Accelerate EV Adoption?</title><link>https://spectrum.ieee.org/evs-boost-renewables-program</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/car-sitting-in-a-parking-lot.jpg?id=31999276&width=1245&height=700&coordinates=0%2C234%2C0%2C235"/><br/><br/><p>The United States is home to the world’s largest biofuel program. For the past decade and a half, the U.S. government has mandated that the country’s government-operated planes, trains, and automobiles run on a fuel blend partly made from corn- and soybean-based biofuels.<br/></p><p>It’s a program with decidedly mixed results. Now, it might get a breath of new life. </p><p>Earlier this month, Reuters <a href="https://www.reuters.com/business/autos-transportation/exclusive-us-epa-expected-propose-electric-cars-be-eligible-renewable-fuel-2022-10-05/" rel="noopener noreferrer" target="_blank">reported</a> that the program could be expanded to provide power for charging electric vehicles. It would be the biggest change in the history of a program that has, in part, failed to live up to its designers’ ambitious dreams.</p><p>In one way, the program in question—the Renewable Fuel Standard (RFS)—is a relic from a bygone era. U.S. lawmakers established the RFS in 2005 and expanded it in 2007, well before solar panels, wind turbines, and electric vehicles became the stalwarts of decarbonization they are today.</p><p>The RFS, in essence, mandated that the blend powering engines in the nation’s official service vehicles run on a certain amount of renewable fuel. Petroleum refiners have to put a certain amount of renewable fuel—such as ethanol derived from corn or cellulose—into the U.S. supply. If a refiner couldn’t manage it, it could buy credits, called Renewable Identification Numbers (RINs), from a supplier that did.</p><p>From 2006, the RFS set a schedule of yearly obligations through 2022, with annually rising RIN targets. The long-term targets were more ambitious than the actual amount of biofuel the U.S. ever actually produced. (It didn’t help that fossil fuel producers <a href="https://www.reuters.com/markets/commodities/what-is-stake-us-biofuel-blending-law-2022-beyond-2022-01-11/" rel="noopener noreferrer" target="_blank">fought</a> tooth and nail to reduce their obligations. Meanwhile, agriculture-industry lobbyists <a href="https://www.reuters.com/article/us-usa-biofuels/trump-epa-finalizes-2020-biofuel-rule-corn-lobby-objects-idUSKBN1YN20M" rel="noopener noreferrer" target="_blank">fought</a> just as hard against those reductions.) </p><p>By the mid-2010s, the U.S. Environmental Protection Agency, which stewards the RFS, had repeatedly downsized the targets by nearly 25 percent. In 2016, a U.S. government report <a href="https://www.gao.gov/assets/gao-17-94.pdf" rel="noopener noreferrer" target="_blank">stated</a>, quite bluntly, that “it is unlikely that the goals of the RFS will be met as envisioned.” A more recent study <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8892349/" rel="noopener noreferrer" target="_blank">found</a> that, since the program coaxed farmers into using more land for corn cultivation, RFS biofuel wasn’t actually any less carbon-intensive than gasoline.</p><p>Now, it is 2022. Amidst a backdrop of rising fuel prices, the Biden administration might bring the RFS its greatest shakeup yet.</p><p>The proposed changes aren’t set in stone. The EPA is under orders to propose a 2023 mandate by 16 November. Any electric vehicle add-on would likely debut by then. Reuters <a href="https://www.reuters.com/business/environment/biden-administration-leans-tesla-guidance-renewable-fuel-policy-reform-2022-06-23/" rel="noopener noreferrer" target="_blank">previously reported</a> that the Biden administration has reached out to electric vehicle maker Tesla to collaborate on crafting the mandates.</p><p>The changes might bolster the RFS with a new type of credit, an “e-RIN,” which would mark an amount of energy used for charging electric vehicles. The changes might nudge the RFS away from corn and oil: Car-charging companies and power-plant biogas suppliers might become eligible, too.</p><p>It wouldn’t be this administration’s first attempt at boosting electric vehicles. While California <a href="https://www.automotiveworld.com/articles/what-does-californias-ice-ban-mean-for-the-us/" rel="noopener noreferrer" target="_blank">leads</a> state governments in slating a 2035 target for ending most internal combustion vehicle sales, the federal government’s ambitious Inflation Reduction Act allocated funds for tax credits on electric vehicles. That plan, however, has proven contentious due to an asterisk: A $7,500-per-vehicle credit would apply only to cars for which most battery material and components come from North America.</p><p>Many analysts <a href="https://www.csis.org/analysis/ira-and-ev-tax-credits%E2%80%94can-we-kill-multiple-birds-one-stone-0" rel="noopener noreferrer" target="_blank">believe</a> that the plan could actually slow electric vehicle take-up rather than accelerate it. And although the plan seeks to reduce U.S. electric-vehicle supply chains’ reliance on Chinese rare earths and battery components, U.S.-friendly governments in Europe, Japan, and South Korea <a href="https://www.csis.org/analysis/ira-and-ev-tax-credits%E2%80%94can-we-kill-multiple-birds-one-stone-0" rel="noopener noreferrer" target="_blank">have criticized</a> the plan for purportedly discriminating against non-U.S. vehicles, potentially breaching World Trade Organisation rules.</p><p>Nunes says it’s currently unclear whether federal government action via a fuel standard would be more effective than direct investment. It’s not the only question with an answer that is still in flux. </p><p>“How much cleaner are electric vehicles relative to internal combustion engines that are powered by fuels that fall under the RFS?” says Nunes. “Because that’s really the comparison that you care about.”</p><p>What that means is that any electric vehicle standard will only be as carbon-free as the supply chains that go into making the vehicles and the electrical grid from which they draw power; and that puts the pressure on governments, electricity providers, and consumers alike to decarbonize the grid.</p><p>Meanwhile, in a future U.S. where electric vehicles come to dominate the roads, sidelining internal combustion engines and liquefied fuels for good, do biofuels and the RFS’s original purpose still have a place?</p><p>Nunes believes so. “There are certainly areas of the economy where electrification does not make a lot of sense,” he says.</p><p>In the world of aviation, for instance, battery tech hasn’t quite advanced to a point that would make electric flights feasible. “That’s where, I think, using things like sustainable aviation fuels and biofuels, et cetera, makes a lot more sense,” Nunes says.</p>]]></description><pubDate>Tue, 25 Oct 2022 14:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/evs-boost-renewables-program</guid><category>Electric vehicles</category><category>Alternative fuels</category><category>Alternative energy</category><category>Biofuels</category><category>Renewable energy</category><dc:creator>Rahul Rao</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/car-sitting-in-a-parking-lot.jpg?id=31999276&amp;width=980"></media:content></item><item><title>A Rich Harvest in the Desert</title><link>https://spectrum.ieee.org/lithium-mining</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/image.jpg?id=31994788&width=980"/><br/><br/><p>The Big Picture features technology through the lens of photographers.</p><p>Every month, <em>IEEE Spectrum</em> selects the most stunning technology images recently captured by photographers around the world. We choose images that reflect an important advance, or a trend, or that are just mesmerizing to look at. We feature all images on our site, and one also appears on our monthly print edition.</p><p>Enjoy the latest images, and if you have suggestions, leave a comment below.</p><h3>Lithium Mining: A Harvest in the Desert</h3><br/><img alt="Giant evaporation pools in the Atacama Desert yield lithium and salt." class="rm-shortcode" data-rm-shortcode-id="b1f26c7fc61483bcf25e3c4b201b429c" data-rm-shortcode-name="rebelmouse-image" id="f47d9" loading="lazy" src="https://spectrum.ieee.org/media-library/giant-evaporation-pools-in-the-atacama-desert-yield-lithium-and-salt.jpg?id=31959150&width=980"/><p>
	The rise in global demand for lithium—spurred mainly by the manufacturing of lithium-ion batteries for electric vehicles—has triggered expanded mining of the world’s known major caches of lithium. One of the richest deposits of lithium ore is in Chile’s Atacama Desert. The area, said to be the driest region on earth, has long been a premier source of sodium nitrate, or saltpeter. To extract lithium, miners pump a naturally occurring brine from beneath Atacama’s salt flats into giant evaporation pools on the surface. Initially, the liquid is a dark blue color. But over the course of 18 months, the sun and the arid desert air strip away a great deal of the brine’s moisture. Left behind are pools of a bright yellow slurry that has a roughly 6-percent lithium content. A nearby chemical plant processes the slurry into lithium carbonate powder suitable for making batteries for automotive propulsion.
</p><p class="caption">
John Moore/Getty Images
</p><h3>James Webb's Far-Fetched Scenerey</h3><br/><img alt="James Webb\u2019s view into deep space has yielded a view of a distant constellation that is reminiscent of a mountain range under a starry night sky." class="rm-shortcode" data-rm-shortcode-id="3749d32dc9f8904f48d90ab87856371e" data-rm-shortcode-name="rebelmouse-image" id="c282b" loading="lazy" src="https://spectrum.ieee.org/media-library/james-webb-u2019s-view-into-deep-space-has-yielded-a-view-of-a-distant-constellation-that-is-reminiscent-of-a-mountain-range-und.jpg?id=31959187&width=980"/><p>
	Objects in the James Webb Space Telescope’s imager are farther away than they appear. The level of detail in this image is startling, considering that it would take 9,100 years of travel at the speed of light to reach this gas cloud from Earth. That’s a long commute. Captured in the picture are the so-called Cosmic Cliffs. What appears at first glance to be a mountain range is actually a sprawling pocket of gas inside a star cluster known as NGC 3324. The Hubble Telescope has already captured images of the constellation, so it was only natural to produce an updated set of glamour shots using James Webb’s more sensitive imaging equipment.
</p><p class="caption">
	ESA/CSA/STSCI/NASA
</p><h3>Alternative Fuel From Artificial Leaf</h3><br/><img alt="A green-and-blue device laminated in clear plastic floats on a river, with Cambridge University\u2019s Kings College in the background." class="rm-shortcode" data-rm-shortcode-id="68a00ac546c8a6048416ad6c2324f6c2" data-rm-shortcode-name="rebelmouse-image" id="ff495" loading="lazy" src="https://spectrum.ieee.org/media-library/a-green-and-blue-device-laminated-in-clear-plastic-floats-on-a-river-with-cambridge-university-u2019s-kings-college-in-the-back.jpg?id=31959214&width=980"/><p>For eons, leaves have been creating the fuel that plants run on virtually out of thin air. A little sun, plus some oxygen and water, is enough to keep a plant going strong. Now, scientists are making strides in the ongoing effort to reverse engineer the leaf. The aim is to pull the carbon dioxide from the air and, using a catalyst, generate a cleaner alternative to the fossil fuels on which we so greatly depend. Researchers will continue trying to improve the yield and efficiency of photovoltaic devices designed to produce hydrogen fuel by splitting the water covering about three-fourths of the Earth’s surface. But a team at the University of Cambridge has demonstrated an artificial leaf that sits on the surface of a body of water and does a fairly good job of producing syngas from carbon dioxide and water. The version shown here (floating on the River Cam near the Cambridge campus) has a surface area of only 100 square centimeters. The Cambridge team already has its sights set on bigger models that would each be capable of powering dozens of homes in carbon-neutral fashion.<br/></p><p class="caption">
	VIRGIL ANDREI
</p><h3>Better Bot Battles</h3><br><img alt="Sparks fly in the BattleBots arena as a green robot attempts to saw open its red-and-black opponent like a can of tuna." class="rm-shortcode" data-rm-shortcode-id="245dcffa7b23c43b5901f4b684719188" data-rm-shortcode-name="rebelmouse-image" id="32d71" loading="lazy" src="https://spectrum.ieee.org/media-library/sparks-fly-in-the-battlebots-arena-as-a-green-robot-attempts-to-saw-open-its-red-and-black-opponent-like-a-can-of-tuna.jpg?id=31959216&width=980"/><p>For 22 years, roboticists have been dreaming up automaton warriors and sending their mechanical creations into battle. Sparks fly when these mean machines try to smash, saw, or burn their opponents into submission. The BattleBots competition is so thrilling that it continues to be televised despite a surfeit of viewing options competing for eyeballs. The latest season of the show began filming this month. When it airs, expect to see bots that have better batteries, more-refined control systems, and real-time telemetry that lets the human operators know when to, say, power down a weapon to keep it from being destroyed by overheating.<br/></p><p class="caption">
	DISCOVERY CHANNEL
</p></br>]]></description><pubDate>Mon, 24 Oct 2022 15:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/lithium-mining</guid><category>The big picture</category><category>James webb space telescope</category><category>Photosynthesis</category><category>Battlebots</category><category>Automatons</category><category>Renewable energy</category><category>Astronomy</category><dc:creator>Willie Jones</dc:creator><media:content medium="image" type="image/jpeg" url="https://assets.rbl.ms/31994788/origin.jpg"></media:content></item><item><title>Why Functional Programming Should Be the Future of Software Development</title><link>https://spectrum.ieee.org/functional-programming</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/strings-of-code-of-varying-colors-jumbling-and-tumbling-in-space-on-a-gradient-purple-backdrop.gif?id=31947274&width=1245&height=700&coordinates=0%2C380%2C0%2C381"/><br/><br/><p>
<strong>You’d expect</strong> <span style="background-color: initial;">the longest and most costly phase in the life</span> <span style="background-color: initial;">cycle of a software product to be the initial development of the system, when all those great features are first imagined and then created. In fact, the hardest part comes later, during the maintenance phase. That’s when programmers pay the price for the shortcuts they took during development.</span>
</p><p>
	So why did they take shortcuts? Maybe they didn’t realize that they were cutting any corners. Only when their code was deployed and exercised by a lot of users did its hidden flaws come to light. And maybe the developers were rushed. Time-to-market pressures would almost guarantee that their software will contain more bugs than it would otherwise.
</p><hr/><p>
	The struggle that most companies have maintaining code causes a second problem: fragility. Every new feature that gets added to the code increases its complexity, which then increases the chance that something will break. It’s common for software to grow so complex that the developers avoid changing it more than is absolutely necessary for fear of breaking something. In many companies, whole teams of developers are employed not to develop anything new but just to keep existing systems going. You might say that they run a software version of the 
	<a href="https://en.wikipedia.org/wiki/Red_Queen%27s_race" target="_blank">Red Queen’s race</a>, running as fast as they can just to stay in the same place.
</p><p>
	It’s a sorry situation. Yet the current trajectory of the software industry is toward increasing complexity, longer product-development times, and greater fragility of production systems. To address such issues, companies usually just throw more people at the problem: more developers, more testers, and more technicians who intervene when systems fail.
</p><p>
	Surely there must be a better way. I’m part of a growing group of developers who think the answer could be functional programming. Here I describe what functional programming is, why using it helps, and why I’m so enthusiastic about it.
</p><h2>With functional programming, less is more</h2><p>
	A good way to understand 
	<span style="background-color: initial;">the rationale for functional programming is by considering something that happened more than a half century ago. In the late 1960s, a programming paradigm emerged that aimed to improve the quality of code while reducing the development time needed. It was called </span><a href="https://en.wikipedia.org/wiki/Structured_programming" rel="noopener noreferrer" target="_blank">structured programming</a><span style="background-color: initial;">.</span>
</p><p>
	Various languages emerged to foster structured programming, and some existing languages were modified to better support it. One of the most notable features of these structured-programming languages was not a feature at all: It was the absence of something that had been around a long time—
	<a href="https://en.wikipedia.org/wiki/Goto" rel="noopener noreferrer" target="_blank">the GOTO statement</a>.
</p><p>
	The GOTO statement is used to redirect program execution. Instead of carrying out the next statement in sequence, the flow of the program is redirected to some other statement, the one specified in the GOTO line, typically when some condition is met.
</p><p>
<span></span>The elimination of the GOTO was based on what programmers had learned from using it—that it made the program very hard to understand. Programs with GOTOs were often referred to as spaghetti code because the sequence of instructions that got executed could be as hard to follow as a single strand in a bowl of spaghetti.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt='A plate of spaghetti made from code with a single strand of "spaghetti code" being pulled from the top of the frame in a neverending loop on a blue gradient background.' class="rm-shortcode" data-rm-shortcode-id="43605a2429891dad9116625a44962284" data-rm-shortcode-name="rebelmouse-image" id="12a26" loading="lazy" src="https://spectrum.ieee.org/media-library/a-plate-of-spaghetti-made-from-code-with-a-single-strand-of-spaghetti-code-being-pulled-from-the-top-of-the-frame-in-a-neveren.gif?id=31954498&width=980"/>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Shira Inbar</small>
</p><p>
	The inability of these developers to understand how their code worked, or why it sometimes didn’t work, was a complexity problem. Software experts of that era believed that those GOTO statements 
	<a href="https://dl.acm.org/doi/10.1145/362929.362947" target="_blank">were creating unnecessary complexity</a> and that the GOTO had to, well, go.
</p><p>
	Back then, this was a radical idea, and many programmers resisted the loss of a statement that they had grown to rely on. The debate went on for more than a decade, but in the end, the GOTO went extinct, and no one today would argue for its return. That’s because its elimination from higher-level programming languages greatly reduced complexity and boosted the reliability of the software being produced. It did this by limiting what programmers could do, which ended up making it easier for them to reason about the code they were writing.
</p><p>
	Although the software industry has eliminated GOTO from modern higher-level languages, software nevertheless continues to grow in complexity and fragility. Looking for how else such programming languages could be modified to avoid some common pitfalls, software designers can find inspiration, curiously enough, from their counterparts on the hardware side.
</p><h2>Nullifying problems with null references</h2><p>
	In designing hardware 
	<span style="background-color: initial;">for a computer, you can’t</span> have a resistor shared by<span style="background-color: initial;">, say,</span> both the keyboard and the monitor’s circuitry<span style="background-color: initial;">. But programmers do this kind of sharing all the time in their software. It’s called shared global state: Variables are owned by no one process but can be changed by any number of processes, even simultaneously.</span><br/>
</p><p>
	Now, imagine that every time you ran your microwave, your dishwasher’s settings changed from Normal Cycle to Pots and Pans. That, of course, doesn’t happen in the real world, but in software, this kind of thing goes on all the time. Programmers write code that calls a function, expecting it to perform a single task. But many functions have side effects that change the shared global state, 
	<a href="https://softwareengineering.stackexchange.com/questions/148108/why-is-global-state-so-evil" target="_blank">giving rise to unexpected consequences</a>.
</p><p>
	In hardware, that doesn’t happen because the laws of physics curtail what’s possible. Of course, hardware engineers can mess up, but not like you can with software, where just too many things are possible, for better or worse.
</p><p>
	Another complexity monster lurking in the software quagmire is called a 
	<a href="https://en.wikipedia.org/wiki/Null_pointer" target="_blank">null reference</a>, meaning that a reference to a place in memory points to nothing at all. If you try to use this reference, an error ensues. So programmers have to remember to check whether something is null before trying to read or change what it references.
</p><p>
	Nearly every popular language today has this flaw. The pioneering computer scientist 
	<a href="http://www.cs.ox.ac.uk/people/tony.hoare/" target="_blank">Tony Hoare</a> introduced null references in the <a href="https://en.wikipedia.org/wiki/ALGOL" target="_blank">ALGOL</a> language back in 1965, and it was later incorporated into numerous other languages. Hoare explained that he did this “simply because it was so easy to implement,” but today he considers it to be a “billion-dollar mistake.” That’s because it has caused countless bugs when a reference that the programmer expects to be valid is really a null reference.
</p><p>
	Software developers need to be extremely disciplined to avoid such pitfalls, and sometimes they don’t take adequate precautions. The architects of structured programming knew this to be true for GOTO statements and left developers no escape hatch. To guarantee the improvements in clarity that GOTO-free code promised, they knew that they’d have to eliminate it entirely from their structured-programming languages.
</p><p>
	History is proof that removing a dangerous feature can greatly improve the quality of code. Today, we have a slew of dangerous practices that compromise the robustness and maintainability of software. Nearly all modern programming languages have some form of null references, shared global state, and functions with side effects—things that are far worse than the GOTO ever was.
</p><p>
	How can those flaws be eliminated? It turns out that the answer 
	<a href="https://www.math-cs.gordon.edu/courses/cps323/LISP/lisp.html" target="_blank">has been around for decades</a>: purely functional programming languages.
</p><h3></h3><br/><div class="flourish-embed flourish-chart" data-src="visualisation/11061373?820658"><script src="https://public.flourish.studio/resources/embed.js"></script></div><p class="caption">Of the top dozen functional-programming languages, Haskell is by far the most popular, judging by the number of GitHub repositories that use these languages.</p><p>
	The first purely functional language to become popular, called 
	<a href="https://www.haskell.org/" target="_blank">Haskell</a>, was created in 1990. So by the mid-1990s, the world of software development really had the solution to the vexing problems it still faces. Sadly, the hardware of the time often wasn’t powerful enough to make use of the solution. But today’s processors can easily manage the demands of Haskell and other purely functional languages.
</p><p>
	Indeed, software based on pure functions is particularly well suited to modern 
	<a href="https://spectrum.ieee.org/the-trouble-with-multicore" target="_self">multicore CPUs</a>. That’s because pure functions operate only on their input parameters, making it impossible to have any interactions between different functions. This allows the compiler to be optimized to produce code that runs on multiple cores efficiently and easily.
</p><p>
	As the name suggests, with purely functional programming, the developer can write only pure functions, which, by definition, cannot have side effects. With this one restriction, you increase stability, open the door to compiler optimizations, and end up with code that’s far easier to reason about.
</p><p>
	But what if a function needs to know or needs to manipulate the state of the system? In that case, the state is passed through a long chain of what are called composed functions—functions that pass their outputs to the inputs of the next function in the chain. By passing the state from function to function, each function has access to it and there’s no chance of another concurrent programming thread modifying that state—another common and costly fragility found in far too many programs.
</p><div class="ieee-sidebar-large">
<strong><strong>
<h3>Avoiding Null-Reference Surprises </h3>
<p style="color: #666666">
<strong>A comparison of Javascript and Purescript shows how the latter can help programmers avoid bugs.</strong>
</p>
<p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="" class="rm-shortcode" data-rm-shortcode-id="8b2fdb5e9a5c50eb2754245cf72e3a76" data-rm-shortcode-name="rebelmouse-image" id="aaaf5" loading="lazy" src="https://spectrum.ieee.org/media-library/image.png?id=31996907&width=980"/>
</p>
</strong></strong>
</div><p>
	Functional programming also has a solution to Hoare’s “billion-dollar mistake,” null references. It addresses that problem by disallowing nulls. Instead, there is a construct usually called 
	<em>Maybe</em> (or <em>Option</em> in some languages). A <em>Maybe</em> can be <em>Nothing</em> or <em>Just</em> some value. Working with <em>Maybe</em><em>s</em> forces developers to always consider both cases. They have no choice in the matter. They must handle the <em>Nothing</em> case every single time they encounter a <em>Maybe</em>. Doing so eliminates the many bugs that null references can spawn.
</p><p>
	Functional programming also requires that data be immutable, meaning that once you set a variable to some value, it is forever that value. Variables are more like variables in math. For example, to compute a formula, 
	<em>y</em> = <em>x</em><sup>2</sup> + 2<em>x</em> – 11, you pick a value for <em>x</em> and at no time during the computation of <em>y</em> does <em>x</em> take on a different value. So, the same value for <em>x</em> is used when computing <em>x</em><sup>2</sup> as is used when computing 2<em>x</em>. In most programming languages, there is no such restriction. You can compute <em>x</em><sup>2</sup> with one value, then change the value of <em>x</em> before computing 2<em>x</em>. By disallowing developers from changing (mutating) values, they can use the same reasoning they did in middle-school algebra class.
</p><p>
	Unlike most languages, functional programming languages are deeply rooted in mathematics. It’s this lineage in the highly disciplined field of mathematics that gives functional languages their biggest advantages.
</p><p>
	Why is that? It’s because people have been working on mathematics for thousands of years. It’s pretty solid. Most programming paradigms, such as object-oriented programming, have at most half a dozen decades of work behind them. They are crude and immature by comparison.
</p><p class="pull-quote" style="">
	Imagine if every time you ran your microwave, your dishwasher’s settings changed from Normal Cycle to Pots and Pans. In software, this kind of thing goes on all the time.
</p><p>
	Let me share an example of how programming is sloppy compared with mathematics. We typically teach new programmers to forget what they learned in math class when they first encounter the statement 
	<em>x = x + 1</em>. In math, this equation has zero solutions. But in most of today’s programming languages, <em>x = x + 1 </em>is not an equation. It is a <em>statement</em> that commands the computer to take the value of <em>x</em>, add one to it, and put it back into a variable called <em>x</em>.
</p><p>
	In functional programming, there are no statements, only 
	<em>expressions</em>. Mathematical thinking that we learned in middle school can now be employed when writing code in a functional language.
</p><p style="">
	Thanks to functional purity, you can reason about code using algebraic substitution to help reduce code complexity in the same way you reduced the complexity of equations back in algebra class. In non-functional languages (imperative languages), there is no equivalent mechanism for reasoning about how the code works.
</p><h2>Functional programming has a steep learning curve</h2><p>
	Pure functional programming solves many of our industry’s biggest problems by removing dangerous features from the language, making it harder for developers to shoot themselves in the foot. At first, these limitations may seem drastic, as I’m sure the 1960s developers felt regarding the removal of GOTO. But the fact of the matter is that it’s both liberating and empowering to work in these languages—so much so that nearly all of today’s most popular languages have incorporated functional features, although they remain fundamentally imperative languages.
</p><p>
	The biggest problem with this hybrid approach is that it still allows developers to ignore the functional aspects of the language. Had we left GOTO as an option 50 years ago, we might still be struggling with spaghetti code today.
</p><p>
	To reap the full benefits of pure functional programming languages, you can’t compromise. You need to use languages that were designed with these principles from the start. Only by adopting them will you get the many benefits that I’ve outlined here.
</p><p>
	But functional programming isn’t a bed of roses. It comes at a cost. Learning to program according to this functional paradigm is almost like learning to program again from the beginning. In many cases, developers must familiarize themselves with math that they didn’t learn in school. The required math isn’t difficult—it’s just new and, to the math phobic, scary.
</p><p>
	More important, developers need to learn a new way of thinking. At first this will be a burden, because they are not used to it. But with time, this new way of thinking becomes second nature and ends up reducing cognitive overhead compared with the old ways of thinking. The result is a massive gain in efficiency.
</p><p>
	But making the transition to functional programming can be difficult. My own journey doing so a few years back is illustrative.
</p><p>
	I decided to learn Haskell—and needed to do that on a business timeline. This was the most difficult learning experience of my 40-year career, in large part because there was no definitive source for helping developers make the transition to functional programming. Indeed, no one had written anything very comprehensive about functional programming in the prior three decades.
</p><p class="pull-quote" style="">
	To reap the full benefits of pure functional programming languages, you can’t compromise. You need to use languages that were designed with these principles from the start.
</p><p>
	I was left to pick up bits and pieces from here, there, and everywhere. And I can attest to the gross inefficiencies of that process. It took me three months of days, nights, and weekends living and breathing Haskell. But finally, I got to the point that I could write better code with it than with anything else.
</p><p>
	When I decided that our company should switch to using functional languages, I didn’t want to put my developers through the same nightmare. So, I started building a curriculum for them to use, which became the basis for a book intended to help developers transition into functional programmers. In 
	<a href="https://leanpub.com/fp-made-easier" target="_blank">my book</a>, I provide guidance for obtaining proficiency in a functional language called <a href="https://www.purescript.org/" target="_blank">PureScript</a>, which stole all the great aspects of Haskell and improved on many of its shortcomings. In addition, it’s able to operate in both the browser and in a back-end server, making it a great solution for many of today’s software demands.
</p><p>
	While such learning resources can only help, for this transition to take place broadly, software-based businesses must invest more in their biggest asset: their developers. At my company, 
	<a href="http://www.panosoft.com/" target="_blank">Panoramic Software</a>, where I’m the chief technical officer, we’ve made this investment, and all new work is being done in either PureScript or Haskell.
</p><p>
	We started down the road of adopting functional languages three years ago, beginning with another pure functional language called 
	<a href="https://elm-lang.org/" target="_blank">Elm</a> because it is a simpler language. (Little did we know we would eventually outgrow it.) It took us about a year to start reaping the benefits. But since we got over the hump, it’s been wonderful. We have had no production runtime bugs, which were so common in what we were formerly using, <a href="https://en.wikipedia.org/wiki/JavaScript" target="_blank">JavaScript</a> on the front end and Java on the back. This improvement allowed the team to spend far more time adding new features to the system. Now, we spend almost no time debugging production issues.
</p><p>
	But there are still challenges when working with a language that relatively few others use—in particular, the lack of online help, documentation, and example code. And it’s hard to hire developers with experience in these languages. Because of that, my company uses recruiters who specialize in finding functional programmers. And when we hire someone with no background in functional programming, we put them through a training process for the first few months to bring them up to speed.
</p><h2>Functional programming’s future</h2><p>
	My company is small. It delivers software to governmental agencies to enable them to help veterans receive benefits from the 
	<a href="https://www.va.gov/" target="_blank">U.S. Department of Veteran’s Affairs</a>. It’s extremely rewarding work, but it’s not a lucrative field. With razor-slim margins, we must use every tool available to us to do more with fewer developers. And for that, functional programming is just the ticket.
</p><p>
	It’s very common for unglamorous businesses like ours to have difficulty attracting developers. But we are now able to hire top-tier people because they want to work on a functional codebase. Being ahead of the curve on this trend, we can get talent that most companies our size could only dream of.
</p><p>
	I anticipate that the adoption of pure functional languages will improve the quality and robustness of the whole software industry while greatly reducing time wasted on bugs that are simply impossible to generate with functional programming. It’s not magic, but sometimes it feels like that, and I’m reminded of how good I have it every time I’m forced to work with a non-functional codebase.
</p><p>
	One sign that the software industry is preparing for a paradigm shift is that functional features are showing up in more and more mainstream languages. It will take much more work for the industry to make the transition fully, but the benefits of doing so are clear, and that is no doubt where things are headed.
</p>]]></description><pubDate>Sun, 23 Oct 2022 15:00:01 +0000</pubDate><guid>https://spectrum.ieee.org/functional-programming</guid><category>Computer programming</category><category>Software development</category><dc:creator>Charles Scalfani</dc:creator><media:content medium="image" type="image/gif" url="https://spectrum.ieee.org/media-library/strings-of-code-of-varying-colors-jumbling-and-tumbling-in-space-on-a-gradient-purple-backdrop.gif?id=31947274&amp;width=980"></media:content></item><item><title>Pong-in-a-Dish</title><link>https://spectrum.ieee.org/pong-in-a-dish</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-grid-with-colorful-blocks-that-look-like-lights-and-columns-a-box-shows-control-panels-and-another-shows-a-visualizer-of-the.jpg?id=31994756&width=1245&height=700&coordinates=0%2C61%2C0%2C62"/><br/><br/><p>Ever hear of the <a href="https://spectrum.ieee.org/untold-history-of-ai-charles-babbage-and-the-turk" target="_self">Turk</a>—the 19th<sup></sup>-century mechanism topped by a turbaned head that played chess against all comers? In fact, hidden inside was a diminutive chessmaster, one you might imagine deadpanning, “<a href="https://www.youtube.com/watch?v=pyxJ7GKGFG0" rel="noopener noreferrer" target="_blank">Eh, It’s a living.</a>”</p><p>Then there’s its namesake, the <a href="https://spectrum.ieee.org/untold-history-of-ai-mechanical-turk-revisited-tktkt" target="_self">Mechanical Turk</a>—a 21st<sup></sup>-century service offered by Amazon to mark up images on the Web with the help of crowdsourced freelancers. They, too, might intone, glassy-eyed, “It’s a living.”</p><p>Now we have a kind of Biological Turk. A mass of neurons act as a computer that mimics a human being playing the classic computer game Pong. The neurons, some taken from mouse embryos, others grown from human precursor cells, spread out into a one-layer, 800,000-cell mesh called a biological neural network, which lives in a giant petri dish called the DishBrain. There it interfaces with arrays of electrodes that form an interface to silicon hardware. Software mounted on that hardware provides stimulation and feedback, and the minibrain learns how to control a paddle on a simulated ping-pong table.</p><p>The work was described recently in the journal <a href="https://www.cell.com/neuron/fulltext/S0896-6273(22)00806-6#%20" rel="noopener noreferrer" target="_blank"><em>Neuron</em></a> by Brett Kagan, the chief scientific officer of <a href="https://corticallabs.com/" rel="noopener noreferrer" target="_blank">Cortical Labs</a>, a startup in Melbourne, Australia, and nine colleagues at that company.</p><p>The authors talk hopefully about the emergence of sentience, a notion that other brain-in-a-dish researchers have also <a href="https://www.nature.com/articles/d41586-020-02986-y" rel="noopener noreferrer" target="_blank">recently floated</a>. But they seem to stand on solid ground when they say their method will help to advance brain science, on the one hand, and computer science, on the other. A bio-neuro-network might model the effects of drugs on the brain in ways that single-cell neurons can’t. Also, neurons may show themselves to be more than just protoplasmic logic switches but more like entire computers.</p><p>The question before us, though, is how does the thing play Pong?</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="0d57b1d55d16d51cf831540a01c2f883" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/GJaXiR_uvVI?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Pong-in-a-Dish</small>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">
<a href="https://youtu.be/GJaXiR_uvVI" target="_blank">youtu.be</a>
</small>
</p><p>First, the electronic scaffolding hits the minibrain with electrical signals that represent the position and movement of the virtual ball. It’s rather like the action potential that a firing neuron would use to convey, say, a sensory signal from the eye to the brain. Because the electrodes are placed at different points in the cell network, the system physically represents the different possible locations. Further information comes from the frequency of the signals, which varies with the distance of the ball to the virtual paddle.<br/></p><p>The network responds to these stimuli like a motor neuron, sending out a signal that moves the virtual paddle. If the resulting movement causes the ball to bounce, the neural network gets a “reward.” Failure results in a signal that has the opposite effect.<span></span></p><p>“Reward” is put in sneer quotes because these cells don’t have feelings. They can’t experience the joy of victory, the agony of defeat. There’s no dopamine, no salted popcorn. Instead, the researchers say, the network is working to minimize unpredictability. In this view, the so-called reward is a predictable signal, the anti-reward is an unpredictable one.</p><p>Kagan tells <em>IEEE Spectrum</em> that the system as a whole then reorganizes to become better at playing the game. The most marked improvement came in the first five minutes of play.</p><p>It seems amazing that a mere 800,000 neurons can model the world, even a simplified world. But, Kagan says, such feats are seen in nature. "Flies have even fewer neurons but must be able to do some modeling—although perhaps not in a way a human may—to navigate a complex and changing 3D world," he says.</p><p>As he and his colleagues point out in their report the ability of neurons to adapt to external stimuli is well established <em>in vivo</em>; it forms the basis for all animal learning. But theirs, they say, is the first <em>in vitro </em>demonstration involving a goal-directed behavior. </p><p>The current version of Pong is forgiving. The paddle is broad, the volley slow, the ball unspinning. Even a neophyte would crush DishBrain. Then again, the same was true of all of AI’s early assays in game playing.</p><p>The early chess machines would sometimes senselessly give up first a pawn, then a piece, then the queen—all because they were attempting to put off a disagreeable action to a point beyond the built-in <a href="https://spectrum.ieee.org/cars-may-think-but-will-they-achieve-artificial-stupidity" target="_self">planning horizon</a>. Poker-playing programs <a href="https://spectrum.ieee.org/poker-pros-battle-artificial-intelligence-to-statistical-draw" target="_self">got good</a> pretty fast, but the early ones sometimes played too well—that is, too cautiously—against weak human opponents, which reduced their winnings. Car navigation programs would send you into a vacant lot.</p><p>You might think that just getting a machine to play a decent game is the hard part, and that further improving it to perfection ought to be a snap. Edgar Allan Poe made that judgement when he called the Turk a fraud because it occasionally erred. His conclusion was correct but his reasoning was faulty.</p><p>It’s not easy turning a barely there machine into a world champion at chess or <a href="https://spectrum.ieee.org/alphago-wins-match-against-top-go-player" target="_self">Go</a>. And yet it has been done.</p>]]></description><pubDate>Sat, 22 Oct 2022 14:09:13 +0000</pubDate><guid>https://spectrum.ieee.org/pong-in-a-dish</guid><category>Neuroscience</category><category>Brain computer interface</category><category>Video games</category><category>Pong</category><category>Neurons</category><category>Biological neural networks</category><dc:creator>Philip E. Ross</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-grid-with-colorful-blocks-that-look-like-lights-and-columns-a-box-shows-control-panels-and-another-shows-a-visualizer-of-the.jpg?id=31994756&amp;width=980"></media:content></item><item><title>Why Cybersecurity Is Key to IoT Sensors</title><link>https://spectrum.ieee.org/sensor-cybersecurity-standards</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/overhead-view-of-vehicle-traffic-on-highway-with-widgets-in-foreground-info-for-editor-if-needed.jpg?id=31963264&width=1245&height=700&coordinates=0%2C234%2C0%2C235"/><br/><br/><p>Sensors enabled by the Internet of Things are network-connected smart devices that collect and transmit real-time data about their environment. The data they provide lets people make better-informed decisions.</p><p>The use of IoT sensors has grown explosively in recent years because their increasing functionality, small size, and low power consumption allow designers to deploy them in new applications to increase productivity and lower costs. The sensors are being used in new ways to maximize the capabilities of network-connected systems and infrastructure.</p><hr/><p>The sensors are poised for mass-scale adoption in fields including automotive, health care, industrial automation, energy, and smart cities. But the lack of standardization in IoT sensors, coupled with interoperability challenges, has made them vulnerable to cyberattacks—which creates barriers for their ubiquitous use. Hackers are targeting IoT sensors in greater numbers, in more industries, and with increased sophistication.</p>
<h2>Consequences of poor cybersecurity</h2><p>A cyberattack can lead to financial penalties and legal issues if it renders a business or organization unable to fulfill its contractual obligations. An attack could harm the corporate brand and generate user mistrust of affected systems. It is costly and time-consuming to repair damage caused by an attack.</p><p>More concerning is the inability to collect and transmit uncorrupted data in real time from critical applications such as with network-connected medical devices. The growing use of such medical devices to monitor and treat diabetes and other conditions depends on sensor cybersecurity.</p><p class="pull-quote">Cyberattackers are targeting IoT sensors in greater numbers, in more industries, and with increased sophistication. Interoperability challenges reduce the ability to access the sensors and the data they publish.</p><p>Another area where sensor cybersecurity is essential is smart cities, a recently growing market. Smart cities use networks that rely on systems of IoT-enabled sensors to gather data to improve local services, allocate resources more effectively, and manage traffic signals and other infrastructure. If compromised sensors fail to send timely, accurate information, then safety issues might arise. People and property could be in danger if warnings about fires, chemical spills, or other emergency situations fail to reach public safety officials in time.</p>
<h2>Sensors can improve operational efficiency</h2><p>The power and energy sector could significantly benefit from sensor cybersecurity and interoperability to help determine when and where to efficiently distribute power.</p><p>Consider the U.S. grid, which comprises about 7,300 power plants; 160,000 miles of high-voltage power lines; and millions of miles of low-voltage power lines and distribution transformers, according to the U.S. <a href="https://www.eia.gov/todayinenergy/detail.php?id=27152" rel="noopener noreferrer" target="_blank">Energy Information Administration</a>. The grid consists of equipment of varying vintages and different technologies, is operated by many companies, and is serviced by vendors with differing cybersecurity practices. Achieving adequate cybersecurity in such a large, disjointed system is a monumental challenge, but it could have a great impact on autonomous control of power systems.</p><p>In industry, automated systems are critical to improving operational efficiency and precision manufacturing, helping to make up for worker shortages. But when the IoT sensors in automated or semiautomated systems fail to operate, workers could be injured or exposed to toxic substances, and operations could be disrupted.</p>
<h2>IEEE standards on sensors </h2><p>The <a href="https://standards.ieee.org/" rel="noopener noreferrer" target="_blank">IEEE Standards Association</a> has a growing portfolio of standards and projects that address different aspects of IoT sensors. Here are a few of them:</p><p><a href="https://standards.ieee.org/search/?q=1451&type=%7CProject%7CStandard" rel="noopener noreferrer" target="_blank">IEEE 1451</a> is a series of standards and projects that describe a set of open, common, network-independent communication interfaces for connecting sensors or actuators to microprocessors, instrumentation systems, and control/field networks. The goal of the series is to allow access of sensor/actuator data through a common set of interfaces, whether they are connected to systems or networks by wired or wireless means.</p><p>The <a href="https://standards.ieee.org/ieee/2700/6770/?utm_source=ieeeorg&utm_medium=the-institute&utm_campaign=cybersecurity-2022" rel="noopener noreferrer" target="_blank">IEEE 2700 Standard for Sensor Performance Parameter Definitions</a> provides a common framework for performance specification terminology, units, conditions, and limits. The standard addresses accelerometers, magnetometers, gyrometers/gyroscopes, accelerometer/magnetometer/gyroscope combination sensors, barometer/pressure sensors, hygrometer/humidity sensors, temperature sensors, light sensors, and proximity sensors.</p><p><a href="https://standards.ieee.org/search/?q=p2888&type=%7CProject" rel="noopener noreferrer" target="_blank">IEEE P2888</a> is a series of standards projects that address a multitude of areas for virtual reality and augmented reality, including sensor interfaces.</p><p>The <a href="https://beyondstandards.ieee.org/addressing-the-need-for-protecting-cybersecurity-in-connected-diabetes-devices/?utm_source=ieeeorg&utm_medium=the-institute&utm_campaign=cybersecurity-2022" rel="noopener noreferrer" target="_blank">IEEE 2621</a> series of standards defines the concept of cybersecurity assurance for wireless diabetes devices, and it specifies security requirements. It also provides instructions on how to achieve assurance.</p>]]></description><pubDate>Fri, 21 Oct 2022 18:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/sensor-cybersecurity-standards</guid><category>Ieee products services</category><category>Standards</category><category>Sensors</category><category>Iot</category><category>Cybersecurity</category><category>Ieee standards association</category><category>Type:ti</category><dc:creator>The IEEE Standards Association</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/overhead-view-of-vehicle-traffic-on-highway-with-widgets-in-foreground-info-for-editor-if-needed.jpg?id=31963264&amp;width=980"></media:content></item><item><title>Video Friday: Swarm Control</title><link>https://spectrum.ieee.org/video-friday-swarm-control</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-photo-from-a-low-angle-looking-upwards-of-a-human-in-a-futuristic-suit-gesturing-at-night-while-hundreds-of-illuminated-drones.png?id=31976554&width=1245&height=700&coordinates=240%2C0%2C240%2C0"/><br/><br/><p>Video Friday is your weekly selection of awesome robotics videos, collected by your friends at <em>IEEE Spectrum</em> robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please <a href="mailto:automaton@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday">send us your events</a> for inclusion.<br/></p><h5><a href="https://iros2022.org/">IROS 2022</a>: 23–27 October 2022, KYOTO, JAPAN</h5><h5><a href="https://www.xprize.org/prizes/avatar/finals-testing">ANA Avatar XPRIZE Finals</a>: 4–5 November 2022, LOS ANGELES</h5><h5><a href="https://corl2022.org/">CoRL 2022</a>: 14–18 December 2022, AUCKLAND, NEW ZEALAND</h5><p>Enjoy today’s videos!</p><hr/><div style="page-break-after: always"><span style="display:none"> </span></div><blockquote><em>Imagine being able to control a swarm of drones with just your hands or gestures. In this video, we explore a future concept-of-operations for swarm management and how large groups of robots and drones will be able to interact and work together. </em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="32da93d8ff33eccbec7194d4c1ef8f57" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/plrS-dbwXr8?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.dronisos.com/">Dronisos</a> ]</p><div class="horizontal-rule"></div><p>There’s a new Mini Pupper on Kickstarter, now with ROS 2!</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="c0e7e0ddbf2343b068514e940f473cb3" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/LeT10vdBSJM?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.kickstarter.com/projects/336477435/mini-pupper-2-open-source-ros2-robot-kit-for-dreamers/">Kickstarter</a> ]</p><div class="horizontal-rule"></div><blockquote><em>Researchers created a method for magnetically programming materials to make cubes that are very picky about who they connect with, enabling more scalable self-assembly.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="6a270915c408d3a1d2bd724c9a8d1825" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/tWDWDf08XhE?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>Paper at IROS next week!</p><p>[ <a href="https://hcie.csail.mit.edu/research/selective/selective.html">MIT CSAIL</a> ]</p><p>Thanks, Rachel!</p><div class="horizontal-rule"></div><blockquote><em>This summer, we held a contest seeking ideas robots inspired by nature, that could help the world. And then we made the winning idea into a real working prototype! The winner this year was “Gillbert” by Eleanor Mackinstosh, a robotic fish that filters microplastics using its gills.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="f8a711c63e56cb5c4640dc5491b57ad4" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/ld15OYvvgfk?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.naturalroboticscontest.com/">Natural Robotics Contest</a> ]</p><p>Thanks, Rob!</p><div class="horizontal-rule"></div><p>I’ve never seen a real centaur climb up onto a block while carrying a payload, but I bet it would look almost exactly like Centauro doing it.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="d934152faa811b0cb06f52798f4f7927" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/YaPbRVBxj8o?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://arxiv.org/abs/2210.06803">Paper</a> ]</p><p>Thanks, Ioannis!</p><div class="horizontal-rule"></div><blockquote><em>Enjoy our favorite obstacle avoidance highlights from the Skydio community! They make showcasing the intellect of our software sublimely easy. The power of autonomous cinematography is displayed best by our incredible Skydians!</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="febb4121fcb663ff8fd4110e4c620ece" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/Ui89kE0p5a8?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>That last clip is especially impressive, since if you look closely, you can see the drone avoiding a wire while flying directly toward the setting sun.</p><p>[ <a href="https://www.skydio.com/">Skydio</a> ]</p><div class="horizontal-rule"></div><p>Somehow I missed this adorable little robot of questionable usefulness from Sony.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="1171a34937c4e7a7af3d3b799988e859" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/1GMRq-tOHLo?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><blockquote><em>Meet poiq, your future buddy robot. Its AI gets smarter and more individualized through questions and conversations with users. Sony is reimagining communication and connection, and developing one-of-a-kind friendships between humans and robots in the process.</em></blockquote><p>[ <a href="https://poiq.sony.jp/">Sony</a> ]</p><div class="horizontal-rule"></div><blockquote><em>Spot’s got permission to dance! Check out this dance created for the “BTS Yet To Come in BUSAN” concert.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="50b1b79cf11030b381893621d202d37f" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/yEW-9SbXahI?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.bostondynamics.com/">Boston Dynamics</a> ]</p><div class="horizontal-rule"></div><p>Awawa, awawa...</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="98d08b1e0b15b0fcee0d0a3f889d2666" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/xesJ7O2g3H8?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.icd.cs.tut.ac.jp/index.php/portfolio/pokebo-cube/">ICD Lab</a> ]</p><div class="horizontal-rule"></div><p>Ascento, on patrol.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="c74fc6685349250d2a7db4d3d0be190d" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/POUaZlzlBAE?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.ascento.ch/">Ascento Robotics</a> ]</p><div class="horizontal-rule"></div><p>Here’s what happens if you grab a Wing delivery drone’s cable and start running.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="c7e8ec910fa0d51672d7a031635e624b" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/snLOdFxrvUo?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://wing.com/">Wing</a> ]</p><div class="horizontal-rule"></div><blockquote><em>Detecting an overheating motor can be the difference between a $1,000 repair or a $50,000 replacement. As a result, routine thermal inspections are a major part of predictive maintenance operations, but collecting this valuable information frequently is still a challenge in many facilities. Agile mobile robots like Spot are transforming condition monitoring with dynamic sensing, so industrial teams can make the most of their predictive maintenance programs. </em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="b53a09248dba05c2cb79d42d4d5a63b2" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/-XUVulOQX9c?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.bostondynamics.com/solutions/inspection/thermal">Boston Dynamics</a> ]</p><div class="horizontal-rule"></div><blockquote><em>Robotnik is specialized in the development of industrial robotic applications based in mobile robots and mobile manipulators. Here [are] some AMR developed and manufactured by us.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="9ed311af8c3039b3803cd5d4b6d056d5" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/X9qRKnegxUY?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://robotnik.eu/">Robotnik</a> ]</p><div class="horizontal-rule"></div><p>How many robot dogs does it take to explore a football field? Fewer than it would if they weren’t working together, that’s for sure.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="bd9b5b95ba8c99274bd077bb17daefcd" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/CNh8KziM_XA?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.deeprobotics.cn/en/">Deep Robotics</a> ]</p><div class="horizontal-rule"></div><blockquote><em>During Summer 2022 our group demoed ANYmal and Spot carrying out in the context of construction progress monitoring at Costain’s Gatwick Airport Train Station site. This was the final demo of the MEMMO Horizon Europe Project.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="3cbba96397f55dab6bc5b7618543b668" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/-zfE7Dyon8o?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://ori.ox.ac.uk/labs/drs/">Oxford</a> ]</p><div class="horizontal-rule"></div><p>Lex Fridman interviews Kate Darling.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="e6a299b3fa628663276f9d9bdb0e13ed" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/ZFntEFXKDHM?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://lexfridman.com/kate-darling-2/">Lex Fridman</a> ]</p><div class="horizontal-rule"></div><p>In this week’s CMU RI Seminar, Nidhi Kalra from The RAND Corporation answers the question, “What (else) can you do with a robotics degree?”</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="4b0d770924db9968db842768145830be" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/9dyfUAJyxPI?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.ri.cmu.edu/event/ri-seminar-nidhi-kalra-rand-corporation-senior-information-scientist-2022-10-14/">CMU RI</a> ]</p><div class="horizontal-rule"></div>]]></description><pubDate>Fri, 21 Oct 2022 15:06:26 +0000</pubDate><guid>https://spectrum.ieee.org/video-friday-swarm-control</guid><category>Video friday</category><category>Robotics</category><dc:creator>Evan Ackerman</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/a-photo-from-a-low-angle-looking-upwards-of-a-human-in-a-futuristic-suit-gesturing-at-night-while-hundreds-of-illuminated-drones.png?id=31976554&amp;width=980"></media:content></item><item><title>Craig Partridge Is Still Working to Improve Internet Traffic</title><link>https://spectrum.ieee.org/craig-partridge-profile</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-smiling-bearded-man-in-a-blue-turtleneck-stands-in-front-of-a-building.png?id=31965104&width=1245&height=700&coordinates=0%2C20%2C0%2C350"/><br/><br/><p><strong>For computer pioneer </strong><a href="https://www.linkedin.com/in/craig-partridge-3aa6602/" rel="noopener noreferrer" target="_blank">Craig Partridge</a>, pushing the envelope on interesting challenges has been his modus operandi. It’s what led him to make key contributions to the early Internet. Today he’s working on getting a better understanding of traffic flows in <a href="https://en.wikipedia.org/wiki/Virtual_private_network" rel="noopener noreferrer" target="_blank">VPNs</a>. In addition, he is trying to figure out what causes high rates of packet errors, which often result in bad file transfers, and how to fix them. </p><p>Although his bachelor’s degree from <a href="http://harvard.edu/" rel="noopener noreferrer" target="_blank">Harvard</a> was in history, the computer electives he took convinced him he had both the aptitude and interest for a career in computing. And he was right. He focused on networking concepts and technologies while working at the ARPAnet and Internet pioneering firm Bolt Beranek & Newman (BBN), now <a href="https://www.raytheonintelligenceandspace.com/what-we-do/bbn" rel="noopener noreferrer" target="_blank">Raytheon BBN Technologies</a>.</p><hr/><p>There, the IEEE Fellow collaborated with TCP/IP developer <a href="https://www.linkedin.com/in/philkarn" rel="noopener noreferrer" target="_blank">Phil Karn</a> on TCP message round-trip time estimation. Partridge also led the team that designed and built the world’s first high-speed router.</p><p>“It was 1983, and BBN needed an additional person who could write TCP/IP code for the BSD [Berkeley Software Distribution] Unix operating system,” he says. “BBN had folks who wrote most of the original code, and they taught me what networking and TCP/IP was. That was something no grad school at the time could teach me.</p><p>“A lot of what I worked on 30 or more years ago is still used today,” Partridge notes proudly. “More generally, a lot of the Internet tech used today is entirely or almost entirely recognizable from what was created back then. That’s a very pleasant feeling.”</p><p>Partridge is an <a href="https://www.internethalloffame.org/inductees/craig-partridge" rel="noopener noreferrer" target="_blank">Internet Hall of Fame</a> inductee for his work in designing how email is routed using domain names.</p><p>While working at BBN, he went on to earn a master’s and Ph.D. in computer science, both from Harvard. By 2018 Partridge was chief scientist for networking research at BBN, but he decided he didn’t want his tenure there to be the last chapter of his career. Later that year he joined <a href="https://www.colostate.edu" rel="noopener noreferrer" target="_blank">Colorado State University</a> in Fort Collins, as chair of its department of computer science.</p><p>Partridge says it’s not uncommon for engineers to move from industry to academia later in their careers. They often want to “give back” by becoming professors, deans, or department chairs.</p><p>One of his assignments at CSU was to reexamine the school’s computer science curriculum. Among other improvements, the faculty now teaches courses in machine learning earlier. Virtual reality and virtual spaces courses were created, and the math requirements were revamped.</p><h2>Efforts to increase the number of women in computing</h2><p>Partridge says during the mid-1980s he became aware of some of the challenges that women face in computing careers. “At BBN, at one point, my project leader, my boss, and my boss’s vice president were all women,” he recalls. “A number of women in computing at the time commented to me how rare that was. It opened my eyes.</p><p>“So in 2017 when I was looking to shift from industry to academia, I made it clear in my interviews at various universities that one of my goals was to look for ways to leverage what I had seen and learned, to help bring more women into computing.”</p><p>“Our focus at Colorado State is on the students already on campus. We encourage them to apply to computer science. We are also improving our curriculum by following the <a href="https://anitab.org/braid" rel="noopener noreferrer" target="_blank">BRAID</a> (Building, Recruiting, and Inclusion for Diversity) principles.”</p><p>He says this effort has included figuring out how to make the introductory courses speak to a wider variety of students by covering the same material but using different examples and different presentations that relate to how they could use computing to solve problems that interest them. </p><p>“We are trying to get them to understand that just because you can create something new doesn’t mean that society wants it,” he says. “You must create it in a way that is more likely to benefit society.”</p><p>In the four years since introducing these changes, Partridge says about half the students taking the introductory courses are women, and the number of women majoring in computing has nearly doubled, from 12 percent to more than 20 percent.</p><h2>Conducting research in an academic setting</h2><p>Partridge also has continued to conduct research, which he finds very different from how he did research at BBN.</p><p>“You are trying to do research while trying to teach the students,” he says. “You could often do the research faster yourself, but then the student wouldn’t learn. By comparison, in industry I knew I had a team I could give a broad description to and get the right thing the first time. With students, by the time they get good at it, they are graduating.”</p><p> In 2011, in response to a decade-old request from <em>IEEE Spectrum</em> columnist and IEEE Fellow <a href="https://spectrum.ieee.org/bob-lucky-obituary" target="_self">Robert Lucky </a>for a list of open research questions in networking, Partridge published “Forty Data Communications Research Questions” in <a href="https://www.acm.org/" target="_blank">ACM</a>’s <em><a href="https://www.sigcomm.org/publications/computer-communication-review" target="_blank">Computer Communication Review</a></em>.</p><p>Many of these questions remain as concerns. For example, one of his graduate students studied packet errors and discovered that checksums, which are used to detect corruption in the header of IPv4 packets, are bad at detecting certain kinds of errors that are likely to occur over asynchronous transfer mode. </p><p>“We are using that kind of experience again today to find the next generation of packet errors in data centers,” Partridge says.</p><p>He would like today’s engineers to pay more attention to edge computing, smart home infrastructure, vehicle safety systems, and walled-garden social networks, such as Instagram and Twitter. He says walled gardens are “social networks where people don't want to connect to people who don't want to connect with those who think differently.”</p><p>One challenge in preparing students for the world they will be working in, he says, “is getting them to understand that just because you can create something new doesn’t mean that society wants it.”</p><p><em>This article appears in the November 2022 print issue as “Craig Partridge.”</em></p><p><em>This article was updated from a previous version.</em></p>]]></description><pubDate>Fri, 21 Oct 2022 15:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/craig-partridge-profile</guid><category>Careers</category><category>Computer science</category><category>Type:departments</category><category>Vpn</category><dc:creator>Daniel P. Dern</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/a-smiling-bearded-man-in-a-blue-turtleneck-stands-in-front-of-a-building.png?id=31965104&amp;width=980"></media:content></item><item><title>Andrew Ng: Unbiggen AI</title><link>https://spectrum.ieee.org/andrew-ng-data-centric-ai</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/andrew-ng-listens-during-the-power-of-data-sooner-than-you-think-global-technology-conference-in-brooklyn-new-york-on-wednes.jpg?id=29206806&width=1245&height=700&coordinates=0%2C0%2C0%2C474"/><br/><br/><p><strong><a href="https://en.wikipedia.org/wiki/Andrew_Ng" rel="noopener noreferrer nofollow" target="_blank">Andrew Ng</a> has serious street cred</strong> in artificial intelligence. He pioneered the use of graphics processing units (GPUs) to train deep learning models in the late 2000s with his students at <a href="https://stanfordmlgroup.github.io/" rel="noopener noreferrer nofollow" target="_blank">Stanford University</a>, cofounded <a href="https://research.google/teams/brain/" rel="noopener noreferrer nofollow" target="_blank">Google Brain</a> in 2011, and then served for three years as chief scientist for <a href="https://ir.baidu.com/" rel="noopener noreferrer nofollow" target="_blank">Baidu</a>, where he helped build the Chinese tech giant’s AI group. So when he says he has identified the next big shift in artificial intelligence, people listen. And that’s what he told <em>IEEE Spectrum</em> in an exclusive Q&A.</p><hr/><p>
	Ng’s current efforts are focused on his company 
	<a href="https://landing.ai/about/" rel="noopener noreferrer nofollow" target="_blank">Landing AI</a>, which built a platform called LandingLens to help manufacturers improve visual inspection with computer vision. <a name="top"></a>He has also become something of an evangelist for what he calls the <a href="https://www.youtube.com/watch?v=06-AZXmwHjo" target="_blank">data-centric AI movement</a>, which he says can yield “small data” solutions to big issues in AI, including model efficiency, accuracy, and bias.
</p><p>
	Andrew Ng on...
</p><ul>
<li><a href="#big">What’s next for really big models</a></li>
<li><a href="#career">The career advice he didn’t listen to</a></li>
<li><a href="#defining">Defining the data-centric AI movement</a></li>
<li><a href="#synthetic">Synthetic data</a></li>
<li><a href="#work">Why Landing AI asks its customers to do the work</a></li>
</ul><p>
<a name="big"></a><strong>The great advances in deep learning over the past decade or so have been powered by ever-bigger models crunching ever-bigger amounts of data. Some people argue that that’s an <a href="https://spectrum.ieee.org/deep-learning-computational-cost" target="_self">unsustainable trajectory</a>. Do you agree that it can’t go on that way?</strong>
</p><p>
<strong>Andrew Ng: </strong>This is a big question. We’ve seen foundation models in NLP [natural language processing]. I’m excited about NLP models getting even bigger, and also about the potential of building foundation models in computer vision. I think there’s lots of signal to still be exploited in video: We have not been able to build foundation models yet for video because of compute bandwidth and the cost of processing video, as opposed to tokenized text. So I think that this engine of scaling up deep learning algorithms, which has been running for something like 15 years now, still has steam in it. Having said that, it only applies to certain problems, and there’s a set of other problems that need small data solutions.
</p><p>
<strong>When you say you want a foundation model for computer vision, what do you mean by that?</strong>
</p><p>
<strong>Ng:</strong> This is a term coined by <a href="https://cs.stanford.edu/~pliang/" rel="noopener noreferrer" target="_blank">Percy Liang</a> and <a href="https://crfm.stanford.edu/" rel="noopener noreferrer" target="_blank">some of my friends at Stanford</a> to refer to very large models, trained on very large data sets, that can be tuned for specific applications. For example, <a href="https://spectrum.ieee.org/open-ais-powerful-text-generating-tool-is-ready-for-business" target="_self">GPT-3</a> is an example of a foundation model [for NLP]. Foundation models offer a lot of promise as a new paradigm in developing machine learning applications, but also challenges in terms of making sure that they’re reasonably fair and free from bias, especially if many of us will be building on top of them.
</p><p>
<strong>What needs to happen for someone to build a foundation model for video?</strong>
</p><p>
<strong>Ng:</strong> I think there is a scalability problem. The compute power needed to process the large volume of images for video is significant, and I think that’s why foundation models have arisen first in NLP. Many researchers are working on this, and I think we’re seeing early signs of such models being developed in computer vision. But I’m confident that if a semiconductor maker gave us 10 times more processor power, we could easily find 10 times more video to build such models for vision.
</p><p>
	Having said that, a lot of what’s happened over the past decade is that deep learning has happened in consumer-facing companies that have large user bases, sometimes billions of users, and therefore very large data sets. While that paradigm of machine learning has driven a lot of economic value in consumer software, I find that that recipe of scale doesn’t work for other industries.
</p><p>
<a href="#top">Back to top</a><a name="career"></a>
</p><p>
<strong>It’s funny to hear you say that, because your early work was at a consumer-facing company with millions of users.</strong>
</p><p>
<strong>Ng: </strong>Over a decade ago, when I proposed starting the <a href="https://research.google/teams/brain/" rel="noopener noreferrer" target="_blank">Google Brain</a> project to use Google’s compute infrastructure to build very large neural networks, it was a controversial step. One very senior person pulled me aside and warned me that starting Google Brain would be bad for my career. I think he felt that the action couldn’t just be in scaling up, and that I should instead focus on architecture innovation.
</p><p class="pull-quote">
	“In many industries where giant data sets simply don’t exist, I think the focus has to shift from big data to good data. Having 50 thoughtfully engineered examples can be sufficient to explain to the neural network what you want it to learn.”<br/>
	—Andrew Ng, CEO & Founder, Landing AI
</p><p>
	I remember when my students and I published the first 
	<a href="https://nips.cc/" rel="noopener noreferrer nofollow" target="_blank">NeurIPS</a> workshop paper advocating using <a href="https://developer.nvidia.com/cuda-zone" rel="noopener noreferrer" target="_blank">CUDA</a>, a platform for processing on GPUs, for deep learning—a different senior person in AI sat me down and said, “CUDA is really complicated to program. As a programming paradigm, this seems like too much work.” I did manage to convince him; the other person I did not convince.
</p><p>
<strong>I expect they’re both convinced now.</strong>
</p><p>
<strong>Ng:</strong> I think so, yes.
</p><p>
	Over the past year as I’ve been speaking to people about the data-centric AI movement, I’ve been getting flashbacks to when I was speaking to people about deep learning and scalability 10 or 15 years ago. In the past year, I’ve been getting the same mix of “there’s nothing new here” and “this seems like the wrong direction.”
</p><p>
<a href="#top">Back to top</a><a name="defining"></a>
</p><p>
<strong>How do you define data-centric AI, and why do you consider it a movement?</strong>
</p><p>
<strong>Ng:</strong> Data-centric AI is the discipline of systematically engineering the data needed to successfully build an AI system. For an AI system, you have to implement some algorithm, say a neural network, in code and then train it on your data set. The dominant paradigm over the last decade was to download the data set while you focus on improving the code. Thanks to that paradigm, over the last decade deep learning networks have improved significantly, to the point where for a lot of applications the code—the neural network architecture—is basically a solved problem. So for many practical applications, it’s now more productive to hold the neural network architecture fixed, and instead find ways to improve the data.
</p><p>
	When I started speaking about this, there were many practitioners who, completely appropriately, raised their hands and said, “Yes, we’ve been doing this for 20 years.” This is the time to take the things that some individuals have been doing intuitively and make it a systematic engineering discipline.
</p><p>
	The data-centric AI movement is much bigger than one company or group of researchers. My collaborators and I organized a 
	<a href="https://neurips.cc/virtual/2021/workshop/21860" rel="noopener noreferrer" target="_blank">data-centric AI workshop at NeurIPS</a>, and I was really delighted at the number of authors and presenters that showed up.
</p><p>
<strong>You often talk about companies or institutions that have only a small amount of data to work with. How can data-centric AI help them?</strong>
</p><p>
<strong>Ng: </strong>You hear a lot about vision systems built with millions of images—I once built a face recognition system using 350 million images. Architectures built for hundreds of millions of images don’t work with only 50 images. But it turns out, if you have 50 really good examples, you can build something valuable, like a defect-inspection system. In many industries where giant data sets simply don’t exist, I think the focus has to shift from big data to good data. Having 50 thoughtfully engineered examples can be sufficient to explain to the neural network what you want it to learn.
</p><p>
<strong>When you talk about training a model with just 50 images, does that really mean you’re taking an existing model that was trained on a very large data set and fine-tuning it? Or do you mean a brand new model that’s designed to learn only from that small data set?</strong>
</p><p>
<strong>Ng: </strong>Let me describe what Landing AI does. When doing visual inspection for manufacturers, we often use our own flavor of <a href="https://developers.arcgis.com/python/guide/how-retinanet-works/" rel="noopener noreferrer" target="_blank">RetinaNet</a>. It is a pretrained model. Having said that, the pretraining is a small piece of the puzzle. What’s a bigger piece of the puzzle is providing tools that enable the manufacturer to pick the right set of images [to use for fine-tuning] and label them in a consistent way. There’s a very practical problem we’ve seen spanning vision, NLP, and speech, where even human annotators don’t agree on the appropriate label. For big data applications, the common response has been: If the data is noisy, let’s just get a lot of data and the algorithm will average over it. But if you can develop tools that flag where the data’s inconsistent and give you a very targeted way to improve the consistency of the data, that turns out to be a more efficient way to get a high-performing system.
</p><p class="pull-quote">
	“Collecting more data often helps, but if you try to collect more data for everything, that can be a very expensive activity.”<br/>
	—Andrew Ng
</p><p>
	For example, if you have 10,000 images where 30 images are of one class, and those 30 images are labeled inconsistently, one of the things we do is build tools to draw your attention to the subset of data that’s inconsistent. So you can very quickly relabel those images to be more consistent, and this leads to improvement in performance.
</p><p>
<strong>Could this focus on high-quality data help with bias in data sets? If you’re able to curate the data more before training?</strong>
</p><p>
<strong>Ng:</strong> Very much so. Many researchers have pointed out that biased data is one factor among many leading to biased systems. There have been many thoughtful efforts to engineer the data. At the NeurIPS workshop, <a href="https://www.cs.princeton.edu/~olgarus/" rel="noopener noreferrer" target="_blank">Olga Russakovsky</a> gave a really nice talk on this. At the main NeurIPS conference, I also really enjoyed <a href="https://neurips.cc/virtual/2021/invited-talk/22281" rel="noopener noreferrer" target="_blank">Mary Gray’s presentation,</a> which touched on how data-centric AI is one piece of the solution, but not the entire solution. New tools like <a href="https://www.microsoft.com/en-us/research/project/datasheets-for-datasets/" rel="noopener noreferrer nofollow" target="_blank">Datasheets for Datasets</a> also seem like an important piece of the puzzle.
</p><p>
	One of the powerful tools that data-centric AI gives us is the ability to engineer a subset of the data. Imagine training a machine-learning system and finding that its performance is okay for most of the data set, but its performance is biased for just a subset of the data. If you try to change the whole neural network architecture to improve the performance on just that subset, it’s quite difficult. But if you can engineer a subset of the data you can address the problem in a much more targeted way.
</p><p>
<strong>When you talk about engineering the data, what do you mean exactly?</strong>
</p><p>
<strong>Ng: </strong>In AI, data cleaning is important, but the way the data has been cleaned has often been in very manual ways. In computer vision, someone may visualize images through a <a href="https://jupyter.org/" rel="noopener noreferrer" target="_blank">Jupyter notebook</a> and maybe spot the problem, and maybe fix it. But I’m excited about tools that allow you to have a very large data set, tools that draw your attention quickly and efficiently to the subset of data where, say, the labels are noisy. Or to quickly bring your attention to the one class among 100 classes where it would benefit you to collect more data. Collecting more data often helps, but if you try to collect more data for everything, that can be a very expensive activity.
</p><p>
	For example, I once figured out that a speech-recognition system was performing poorly when there was car noise in the background. Knowing that allowed me to collect more data with car noise in the background, rather than trying to collect more data for everything, which would have been expensive and slow.
</p><p>
<a href="#top">Back to top</a><a name="synthetic"></a>
</p><p>
<strong>What about using synthetic data, is that often a good solution?</strong>
</p><p>
<strong>Ng: </strong>I think synthetic data is an important tool in the tool chest of data-centric AI. At the NeurIPS workshop, <a href="http://tensorlab.cms.caltech.edu/users/anima/" rel="noopener noreferrer" target="_blank">Anima Anandkumar</a> gave a great talk that touched on synthetic data. I think there are important uses of synthetic data that go beyond just being a preprocessing step for increasing the data set for a learning algorithm. I’d love to see more tools to let developers use synthetic data generation as part of the closed loop of iterative machine learning development.
</p><p>
<strong>Do you mean that synthetic data would allow you to try the model on more data sets?</strong>
</p><p>
<strong>Ng: </strong>Not really. Here’s an example. Let’s say you’re trying to detect defects in a smartphone casing. There are many different types of defects on smartphones. It could be a scratch, a dent, pit marks, discoloration of the material, other types of blemishes. If you train the model and then find through error analysis that it’s doing well overall but it’s performing poorly on pit marks, then synthetic data generation allows you to address the problem in a more targeted way. You could generate more data just for the pit-mark category.
</p><p class="pull-quote">
	“In the consumer software Internet, we could train a handful of machine-learning models to serve a billion users. In manufacturing, you might have 10,000 manufacturers building 10,000 custom AI models.”<br/>
	—Andrew Ng
</p><p>
	Synthetic data generation is a very powerful tool, but there are many simpler tools that I will often try first. Such as data augmentation, improving labeling consistency, or just asking a factory to collect more data.
</p><p>
<a href="#top">Back to top</a><a name="work"></a>
</p><p>
<strong>To make these issues more concrete, can you walk me through an example? When a company approaches <a href="https://landing.ai/" rel="noopener noreferrer nofollow" target="_blank">Landing AI</a> and says it has a problem with visual inspection, how do you onboard them and work toward deployment?</strong>
</p><p>
<strong>Ng: </strong>When a customer approaches us we usually have a conversation about their inspection problem and look at a few images to verify that the problem is feasible with computer vision. Assuming it is, we ask them to upload the data to the <a href="https://landing.ai/platform/" rel="noopener noreferrer nofollow" target="_blank">LandingLens</a> platform. We often advise them on the methodology of data-centric AI and help them label the data.
</p><p>
	One of the foci of Landing AI is to empower manufacturing companies to do the machine learning work themselves. A lot of our work is making sure the software is fast and easy to use. Through the iterative process of machine learning development, we advise customers on things like how to train models on the platform, when and how to improve the labeling of data so the performance of the model improves. Our training and software supports them all the way through deploying the trained model to an edge device in the factory.
</p><p>
<strong>How do you deal with changing needs? If products change or lighting conditions change in the factory, can the model keep up?</strong>
</p><p>
<strong>Ng:</strong> It varies by manufacturer. There is data drift in many contexts. But there are some manufacturers that have been running the same manufacturing line for 20 years now with few changes, so they don’t expect changes in the next five years. Those stable environments make things easier. For other manufacturers, we provide tools to flag when there’s a significant data-drift issue. I find it really important to empower manufacturing customers to correct data, retrain, and update the model. Because if something changes and it’s 3 a.m. in the United States, I want them to be able to adapt their learning algorithm right away to maintain operations.
</p><p>
	In the consumer software Internet, we could train a handful of machine-learning models to serve a billion users. In manufacturing, you might have 10,000 manufacturers building 10,000 custom AI models. The challenge is, how do you do that without Landing AI having to hire 10,000 machine learning specialists?
</p><p>
<strong>So you’re saying that to make it scale, you have to empower customers to do a lot of the training and other work.</strong>
</p><p>
<strong>Ng: </strong>Yes, exactly! This is an industry-wide problem in AI, not just in manufacturing. Look at health care. Every hospital has its own slightly different format for electronic health records. How can every hospital train its own custom AI model? Expecting every hospital’s IT personnel to invent new neural-network architectures is unrealistic. The only way out of this dilemma is to build tools that empower the customers to build their own models by giving them tools to engineer the data and express their domain knowledge. That’s what Landing AI is executing in computer vision, and the field of AI needs other teams to execute this in other domains.
</p><p>
<strong>Is there anything else you think it’s important for people to understand about the work you’re doing or the data-centric AI movement?</strong>
</p><p>
<strong>Ng: </strong>In the last decade, the biggest shift in AI was a shift to deep learning. I think it’s quite possible that in this decade the biggest shift will be to data-centric AI. With the maturity of today’s neural network architectures, I think for a lot of the practical applications the bottleneck will be whether we can efficiently get the data we need to develop systems that work well. The data-centric AI movement has tremendous energy and momentum across the whole community. I hope more researchers and developers will jump in and work on it.
</p><p>
<a href="#top">Back to top</a>
</p><p><em>This article appears in the April 2022 print issue as “Andrew Ng, AI Minimalist</em><em>.”</em></p>]]></description><pubDate>Wed, 09 Feb 2022 15:31:12 +0000</pubDate><guid>https://spectrum.ieee.org/andrew-ng-data-centric-ai</guid><category>Andrew ng</category><category>Artificial intelligence</category><category>Deep learning</category><category>Type:cover</category><dc:creator>Eliza Strickland</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/andrew-ng-listens-during-the-power-of-data-sooner-than-you-think-global-technology-conference-in-brooklyn-new-york-on-wednes.jpg?id=29206806&amp;width=980"></media:content></item><item><title>How AI Will Change Chip Design</title><link>https://spectrum.ieee.org/ai-chip-design-matlab</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/layered-rendering-of-colorful-semiconductor-wafers-with-a-bright-white-light-sitting-on-one.jpg?id=29285079&width=1245&height=700&coordinates=0%2C156%2C0%2C156"/><br/><br/><p>The end of <a href="https://spectrum.ieee.org/on-beyond-moores-law-4-new-laws-of-computing" target="_self">Moore’s Law</a> is looming. Engineers and designers can do only so much to <a href="https://spectrum.ieee.org/ibm-introduces-the-worlds-first-2nm-node-chip" target="_self">miniaturize transistors</a> and <a href="https://spectrum.ieee.org/cerebras-giant-ai-chip-now-has-a-trillions-more-transistors" target="_self">pack as many of them as possible into chips</a>. So they’re turning to other approaches to chip design, incorporating technologies like AI into the process.</p><p>Samsung, for instance, is <a href="https://spectrum.ieee.org/processing-in-dram-accelerates-ai" target="_self">adding AI to its memory chips</a> to enable processing in memory, thereby saving energy and speeding up machine learning. Speaking of speed, Google’s TPU V4 AI chip has <a href="https://spectrum.ieee.org/heres-how-googles-tpu-v4-ai-chip-stacked-up-in-training-tests" target="_self">doubled its processing power</a> compared with that of  its previous version.</p><p>But AI holds still more promise and potential for the semiconductor industry. To better understand how AI is set to revolutionize chip design, we spoke with <a href="https://www.linkedin.com/in/heather-gorr-phd" rel="noopener noreferrer" target="_blank">Heather Gorr</a>, senior product manager for <a href="https://www.mathworks.com/" rel="noopener noreferrer" target="_blank">MathWorks</a>’ MATLAB platform.</p><p><strong>How is AI currently being used to design the next generation of chips?</strong></p><p><strong>Heather Gorr:</strong> AI is such an important technology because it’s involved in most parts of the cycle, including the design and manufacturing process. There’s a lot of important applications here, even in the general process engineering where we want to optimize things. I think defect detection is a big one at all phases of the process, especially in manufacturing. But even thinking ahead in the design process, [AI now plays a significant role] when you’re designing the light and the sensors and all the different components. There’s a lot of anomaly detection and fault mitigation that you really want to consider.</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="Portrait of a woman with blonde-red hair smiling at the camera" class="rm-shortcode rm-resized-image" data-rm-shortcode-id="1f18a02ccaf51f5c766af2ebc4af18e1" data-rm-shortcode-name="rebelmouse-image" id="2dc00" loading="lazy" src="https://spectrum.ieee.org/media-library/portrait-of-a-woman-with-blonde-red-hair-smiling-at-the-camera.jpg?id=29288554&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">Heather Gorr</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">MathWorks</small></p><p>Then, thinking about the logistical modeling that you see in any industry, there is always planned downtime that you want to mitigate; but you also end up having unplanned downtime. So, looking back at that historical data of when you’ve had those moments where maybe it took a bit longer than expected to manufacture something, you can take a look at all of that data and use AI to try to identify the proximate cause or to see  something that might jump out even in the processing and design phases. We think of AI oftentimes as a predictive tool, or as a robot doing something, but a lot of times you get a lot of insight from the data through AI.</p><p><strong>What are the benefits of using AI for chip design?</strong></p><p><strong>Gorr:</strong> Historically, we’ve seen a lot of physics-based modeling, which is a very intensive process. We want to do a <a href="https://en.wikipedia.org/wiki/Model_order_reduction" rel="noopener noreferrer" target="_blank">reduced order model</a>, where instead of solving such a computationally expensive and extensive model, we can do something a little cheaper. You could create a surrogate model, so to speak, of that physics-based model, use the data, and then do your <a href="https://institutefordiseasemodeling.github.io/idmtools/parameter-sweeps.html" rel="noopener noreferrer" target="_blank">parameter sweeps</a>, your optimizations, your <a href="https://www.ibm.com/cloud/learn/monte-carlo-simulation" rel="noopener noreferrer" target="_blank">Monte Carlo simulations</a> using the surrogate model. That takes a lot less time computationally than solving the physics-based equations directly. So, we’re seeing that benefit in many ways, including the efficiency and economy that are the results of iterating quickly on the experiments and the simulations that will really help in the design.</p><p><strong>So it’s like having a digital twin in a sense?</strong></p><p><strong>Gorr:</strong> Exactly. That’s pretty much what people are doing, where you have the physical system model and the experimental data. Then, in conjunction, you have this other model that you could tweak and tune and try different parameters and experiments that let sweep through all of those different situations and come up with a better design in the end.</p><p><strong>So, it’s going to be more efficient and, as you said, cheaper?</strong></p><p><strong>Gorr:</strong> Yeah, definitely. Especially in the experimentation and design phases, where you’re trying different things. That’s obviously going to yield dramatic cost savings if you’re actually manufacturing and producing [the chips]. You want to simulate, test, experiment as much as possible without making something using the actual process engineering.</p><p><strong>We’ve talked about the benefits. How about the drawbacks?</strong></p><p><strong>Gorr: </strong>The [AI-based experimental models] tend to not be as accurate as physics-based models. Of course, that’s why you do many simulations and parameter sweeps. But that’s also the benefit of having that digital twin, where you can keep that in mind—it's not going to be as accurate as that precise model that we’ve developed over the years.</p><p>Both chip design and manufacturing are system intensive; you have to consider every little part. And that can be really challenging. It's a case where you might have models to predict something and different parts of it, but you still need to bring it all together.</p><p>One of the other things to think about too is that you need the data to build the models. You have to incorporate data from all sorts of different sensors and different sorts of teams, and so that heightens the challenge.</p><p><strong>How can engineers use AI to better prepare and extract insights from hardware or sensor data?</strong></p><p><strong>Gorr: </strong>We always think about using AI to predict something or do some robot task, but you can use AI to come up with patterns and pick out things you might not have noticed before on your own. People will use AI when they have high-frequency data coming from many different sensors, and a lot of times it’s useful to explore the frequency domain and things like data synchronization or resampling. Those can be really challenging if you’re not sure where to start.</p><p>One of the things I would say is, use the tools that are available. There’s a vast community of people working on these things, and you can find lots of examples [of applications and techniques] on <a href="https://github.com/" rel="noopener noreferrer" target="_blank">GitHub</a> or <a href="https://www.mathworks.com/matlabcentral/" rel="noopener noreferrer" target="_blank">MATLAB Central</a>, where people have shared nice examples, even little apps they’ve created. I think many of us are buried in data and just not sure what to do with it, so definitely take advantage of what’s already out there in the community. You can explore and see what makes sense to you, and bring in that balance of domain knowledge and the insight you get from the tools and AI.</p><p><strong>What should engineers and designers consider wh</strong><strong>en using AI for chip design?</strong></p><p><strong>Gorr:</strong> Think through what problems you’re trying to solve or what insights you might hope to find, and try to be clear about that. Consider all of the different components, and document and test each of those different parts. Consider all of the people involved, and explain and hand off in a way that is sensible for the whole team.</p><p><strong>How do you think AI will affect chip designers’ jobs?</strong></p><p><strong>Gorr:</strong> It’s going to free up a lot of human capital for more advanced tasks. We can use AI to reduce waste, to optimize the materials, to optimize the design, but then you still have that human involved whenever it comes to decision-making. I think it’s a great example of people and technology working hand in hand. It’s also an industry where all people involved—even on the manufacturing floor—need to have some level of understanding of what’s happening, so this is a great industry for advancing AI because of how we test things and how we think about them before we put them on the chip.</p><p><strong>How do you envision the future of AI and chip design?</strong></p><p><strong>Gorr</strong><strong>:</strong> It's very much dependent on that human element—involving people in the process and having that interpretable model. We can do many things with the mathematical minutiae of modeling, but it comes down to how people are using it, how everybody in the process is understanding and applying it. Communication and involvement of people of all skill levels in the process are going to be really important. We’re going to see less of those superprecise predictions and more transparency of information, sharing, and that digital twin—not only using AI but also using our human knowledge and all of the work that many people have done over the years.</p>]]></description><pubDate>Tue, 08 Feb 2022 14:00:01 +0000</pubDate><guid>https://spectrum.ieee.org/ai-chip-design-matlab</guid><category>Chip fabrication</category><category>Moore’s law</category><category>Chip design</category><category>Ai</category><category>Matlab</category><category>Digital twins</category><dc:creator>Rina Diane Caballar</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/layered-rendering-of-colorful-semiconductor-wafers-with-a-bright-white-light-sitting-on-one.jpg?id=29285079&amp;width=980"></media:content></item><item><title>Atomically Thin Materials Significantly Shrink Qubits</title><link>https://spectrum.ieee.org/2d-hbn-qubit</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-golden-square-package-holds-a-small-processor-sitting-on-top-is-a-metal-square-with-mit-etched-into-it.jpg?id=29281587&width=1245&height=700&coordinates=0%2C156%2C0%2C156"/><br/><br/><p>Quantum computing is a devilishly complex technology, with many technical hurdles impacting its development. Of these challenges two critical issues stand out: miniaturization and qubit quality.</p><p>IBM has adopted the superconducting qubit road map of <a href="https://spectrum.ieee.org/ibms-envisons-the-road-to-quantum-computing-like-an-apollo-mission" target="_self">reaching a 1,121-qubit processor by 2023</a>, leading to the expectation that 1,000 qubits with today’s qubit form factor is feasible. However, current approaches will require very large chips (50 millimeters on a side, or larger) at the scale of small wafers, or the use of chiplets on multichip modules. While this approach will work, the aim is to attain a better path toward scalability.</p><p>Now researchers at <a href="https://www.nature.com/articles/s41563-021-01187-w" rel="noopener noreferrer" target="_blank">MIT have been able to both reduce the size of the qubits</a> and done so in a way that reduces the interference that occurs between neighboring qubits. The MIT researchers have increased the number of superconducting qubits that can be added onto a device by a factor of 100.</p><p>“We are addressing both qubit miniaturization and quality,” said <a href="https://equs.mit.edu/william-d-oliver/" rel="noopener noreferrer" target="_blank">William Oliver</a>, the director for the <a href="https://cqe.mit.edu/" target="_blank">Center for Quantum Engineering</a> at MIT. “Unlike conventional transistor scaling, where only the number really matters, for qubits, large numbers are not sufficient, they must also be high-performance. Sacrificing performance for qubit number is not a useful trade in quantum computing. They must go hand in hand.”</p><p>The key to this big increase in qubit density and reduction of interference comes down to the use of two-dimensional materials, in particular the 2D insulator hexagonal boron nitride (hBN). The MIT researchers demonstrated that a few atomic monolayers of hBN can be stacked to form the insulator in the capacitors of a superconducting qubit.</p><p>Just like other capacitors, the capacitors in these superconducting circuits take the form of a sandwich in which an insulator material is sandwiched between two metal plates. The big difference for these capacitors is that the superconducting circuits can operate only at extremely low temperatures—less than 0.02 degrees above absolute zero (-273.15 °C).</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="Golden dilution refrigerator hanging vertically" class="rm-shortcode rm-resized-image" data-rm-shortcode-id="694399af8a1c345e51a695ff73909eda" data-rm-shortcode-name="rebelmouse-image" id="6c615" loading="lazy" src="https://spectrum.ieee.org/media-library/golden-dilution-refrigerator-hanging-vertically.jpg?id=29281593&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">Superconducting qubits are measured at temperatures as low as 20 millikelvin in a dilution refrigerator.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">Nathan Fiske/MIT</small></p><p>In that environment, insulating materials that are available for the job, such as PE-CVD silicon oxide or silicon nitride, have quite a few defects that are too lossy for quantum computing applications. To get around these material shortcomings, most superconducting circuits use what are called coplanar capacitors. In these capacitors, the plates are positioned laterally to one another, rather than on top of one another.</p><p>As a result, the intrinsic silicon substrate below the plates and to a smaller degree the vacuum above the plates serve as the capacitor dielectric. Intrinsic silicon is chemically pure and therefore has few defects, and the large size dilutes the electric field at the plate interfaces, all of which leads to a low-loss capacitor. The lateral size of each plate in this open-face design ends up being quite large (typically 100 by 100 micrometers) in order to achieve the required capacitance.</p><p>In an effort to move away from the large lateral configuration, the MIT researchers embarked on a search for an insulator that has very few defects and is compatible with superconducting capacitor plates.</p><p>“We chose to study hBN because it is the most widely used insulator in 2D material research due to its cleanliness and chemical inertness,” said colead author <a href="https://equs.mit.edu/joel-wang/" rel="noopener noreferrer" target="_blank">Joel Wang</a>, a research scientist in the Engineering Quantum Systems group of the MIT Research Laboratory for Electronics. </p><p>On either side of the hBN, the MIT researchers used the 2D superconducting material, niobium diselenide. One of the trickiest aspects of fabricating the capacitors was working with the niobium diselenide, which oxidizes in seconds when exposed to air, according to Wang. This necessitates that the assembly of the capacitor occur in a glove box filled with argon gas.</p><p>While this would seemingly complicate the scaling up of the production of these capacitors, Wang doesn’t regard this as a limiting factor.</p><p>“What determines the quality factor of the capacitor are the two interfaces between the two materials,” said Wang. “Once the sandwich is made, the two interfaces are “sealed” and we don’t see any noticeable degradation over time when exposed to the atmosphere.”</p><p>This lack of degradation is because around 90 percent of the electric field is contained within the sandwich structure, so the oxidation of the outer surface of the niobium diselenide does not play a significant role anymore. This ultimately makes the capacitor footprint much smaller, and it accounts for the reduction in cross talk between the neighboring qubits.</p><p>“The main challenge for scaling up the fabrication will be the wafer-scale growth of hBN and 2D superconductors like [niobium diselenide], and how one can do wafer-scale stacking of these films,” added Wang.</p><p>Wang believes that this research has shown 2D hBN to be a good insulator candidate for superconducting qubits. He says that the groundwork the MIT team has done will serve as a road map for using other hybrid 2D materials to build superconducting circuits.</p>]]></description><pubDate>Mon, 07 Feb 2022 16:12:05 +0000</pubDate><guid>https://spectrum.ieee.org/2d-hbn-qubit</guid><category>Qubits</category><category>Mit</category><category>Ibm</category><category>Superconducting qubits</category><category>Hexagonal boron nitride</category><category>2d materials</category><category>Quantum computing</category><dc:creator>Dexter Johnson</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-golden-square-package-holds-a-small-processor-sitting-on-top-is-a-metal-square-with-mit-etched-into-it.jpg?id=29281587&amp;width=980"></media:content></item></channel></rss>