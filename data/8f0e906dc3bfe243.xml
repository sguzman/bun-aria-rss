<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	>

<channel>
	<title>Machine Learning, Maths and Physics</title>
	<atom:link href="https://mlopezm.wordpress.com/feed/" rel="self" type="application/rss+xml" />
	<link>https://mlopezm.wordpress.com</link>
	<description>                  Thoughts, ideas &#38; opinions</description>
	<lastBuildDate>Thu, 15 Nov 2018 19:11:47 +0000</lastBuildDate>
	<language>es</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
<cloud domain='mlopezm.wordpress.com' port='80' path='/?rsscloud=notify' registerProcedure='' protocol='http-post' />
<image>
		<url>https://s0.wp.com/i/buttonw-com.png</url>
		<title>Machine Learning, Maths and Physics</title>
		<link>https://mlopezm.wordpress.com</link>
	</image>
	<atom:link rel="search" type="application/opensearchdescription+xml" href="https://mlopezm.wordpress.com/osd.xml" title="Machine Learning, Maths and Physics" />
	<atom:link rel='hub' href='https://mlopezm.wordpress.com/?pushpress=hub'/>
	<item>
		<title>Codigo: Introducción Deep Learning-UPSA</title>
		<link>https://mlopezm.wordpress.com/2018/11/15/codigo-introduccion-deep-learning-upsa/</link>
					<comments>https://mlopezm.wordpress.com/2018/11/15/codigo-introduccion-deep-learning-upsa/#respond</comments>
		
		<dc:creator><![CDATA[mlopezm]]></dc:creator>
		<pubDate>Thu, 15 Nov 2018 19:11:47 +0000</pubDate>
				<category><![CDATA[Sin categoría]]></category>
		<guid isPermaLink="false">http://mlopezm.wordpress.com/?p=1010</guid>

					<description><![CDATA[Codigo Introduccion Deep Learning-UPSA https://drive.google.com/open?id=1-Avc2xqiYpM7G1CZqXhgYpTZEud_kbbD … &#160;]]></description>
										<content:encoded><![CDATA[<p>Codigo Introduccion Deep Learning-UPSA</p>
<p><a class="twitter-timeline-link" title="https://drive.google.com/open?id=1-Avc2xqiYpM7G1CZqXhgYpTZEud_kbbD" href="https://t.co/odQRNCai2A" target="_blank" rel="nofollow noopener"><span class="invisible">https://</span><span class="js-display-url">drive.google.com/open?id=1-Avc2</span><span class="invisible">xqiYpM7G1CZqXhgYpTZEud_kbbD</span><span class="tco-ellipsis"><span class="invisible"> </span>…</span></a></p>
<p>&nbsp;</p>
]]></content:encoded>
					
					<wfw:commentRss>https://mlopezm.wordpress.com/2018/11/15/codigo-introduccion-deep-learning-upsa/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/f6cc6c716c79438c53742113a478799b?s=96&#38;d=identicon&#38;r=G" medium="image">
			<media:title type="html">mlopezm</media:title>
		</media:content>
	</item>
		<item>
		<title>What is a Tensor</title>
		<link>https://mlopezm.wordpress.com/2016/10/19/what-is-a-tensor/</link>
					<comments>https://mlopezm.wordpress.com/2016/10/19/what-is-a-tensor/#respond</comments>
		
		<dc:creator><![CDATA[mlopezm]]></dc:creator>
		<pubDate>Wed, 19 Oct 2016 12:13:10 +0000</pubDate>
				<category><![CDATA[Sin categoría]]></category>
		<category><![CDATA[tensor]]></category>
		<guid isPermaLink="false">http://mlopezm.wordpress.com/?p=1003</guid>

					<description><![CDATA[This Youtube reproduction list is a fantastic place to understand what is a Tensor. It goes all the way to explain how tensors are defined. It starts from vectors, linear maps, dual spaces, tensor product and finally they arrive to the definition of tensors. Very clear!!]]></description>
										<content:encoded><![CDATA[<p>This Youtube reproduction list is a fantastic place to understand what is a Tensor.</p>
<p>It goes all the way to explain how tensors are defined. It starts from vectors, linear maps, dual spaces, tensor product and finally they arrive to the definition of tensors.</p>
<p>Very clear!!</p>
<div class="jetpack-video-wrapper"><iframe class="youtube-player" width="730" height="411" src="https://www.youtube.com/embed/_pKxbNyjNe8?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=es&#038;autohide=2&#038;wmode=transparent&#038;listType=playlist&#038;list=PLRlVmXqzHjUQARA37r4Qw3SHPqVXgqO6c" allowfullscreen="true" style="border:0;" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation"></iframe></div>
]]></content:encoded>
					
					<wfw:commentRss>https://mlopezm.wordpress.com/2016/10/19/what-is-a-tensor/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/f6cc6c716c79438c53742113a478799b?s=96&#38;d=identicon&#38;r=G" medium="image">
			<media:title type="html">mlopezm</media:title>
		</media:content>
	</item>
		<item>
		<title>Linear regression can be understood in many ways (optimization, probabilistic, bayesian)</title>
		<link>https://mlopezm.wordpress.com/2016/07/20/linear-regression-can-be-understood-in-many-ways-optimization-probabilistic-bayesian/</link>
					<comments>https://mlopezm.wordpress.com/2016/07/20/linear-regression-can-be-understood-in-many-ways-optimization-probabilistic-bayesian/#comments</comments>
		
		<dc:creator><![CDATA[mlopezm]]></dc:creator>
		<pubDate>Wed, 20 Jul 2016 09:29:14 +0000</pubDate>
				<category><![CDATA[Sin categoría]]></category>
		<guid isPermaLink="false">http://mlopezm.wordpress.com/?p=858</guid>

					<description><![CDATA[It is very interesting to see the connection between the different approaches to solve the linear regression problem. Optimization Optimization of deterministic cost function The firs point of view is simply as an optimization problem. The normal approach is to try to minimize the residual (errors) sum of squares (RSS). In that way we arrive [&#8230;]]]></description>
										<content:encoded><![CDATA[<p>It is very interesting to see the connection between the different approaches to solve the linear regression problem.</p>
<h1><strong>Optimization</strong></h1>
<h3><em>Optimization of deterministic cost function</em></h3>
<p>The firs point of view is simply as an optimization problem. The normal approach is to try to minimize the residual (errors) sum of squares (RSS). In that way we arrive to the normal equations that solve the problem (<a href="https://en.wikipedia.org/wiki/Linear_least_squares_(mathematics)">least squares</a>)(<a href="https://en.wikipedia.org/wiki/Linear_regression">linear regression</a>)(<a href="https://en.wikipedia.org/wiki/Ordinary_least_squares">OLS</a>). To minimize the expression we equal the gradient of the RSS to zero and solve it to obtain the parameters that achieves the minimum. If instead of the RSS we use the absolute value of the residuals we arrive to the <a href="https://en.wikipedia.org/wiki/Least_absolute_deviations">LAD</a> (least absolute deviation) solution. For this solution there is not a close form solution, instead you have to use a linear programming apprach to solve it. Both LAD or RSS use the minimization of a cost function.</p>
<p>When using a cost functions you can be lucky and have a close form solution (RSS case) or, more often, you arrive to a solution that involves an iterative process, either using gradient descent, stochastic gradient descent (using the gradient of the cost function) or some other method that uses both the gradient and the hessian of the cost fucntion (Newton method, quasi-Newton, &#8230;) (<a href="http://gradient descent">gradient descent</a>)(<a href="http://www.scipy-lectures.org/advanced/mathematical_optimization/">finding minima</a>)(<a href="http://www.analyzemath.com/calculus/Differentiation/absolute_value.html">gradient LAD</a>)</p>
<p>&nbsp;</p>
<h1>Probabilistic</h1>
<h3><em>Maximum likelihood of posterior distribution</em></h3>
<p>Same results obtained by the optimization approach can be produced by other different means.</p>
<p>The predictor variable can be seen as a random variable produced by a function of the independent variables (deterministic) plus an error term (another random variable).</p>
<p>The pdf (probability density function) of the predictor random variable (the posterior distribution) will be similar to the pdf of the error term, except by a difference in the expected value that is provided by the linear relationship established on the independent variables.</p>
<p>If we assume that the error term has a gaussian pdf with zero mean and we try to determine the parameters of the linear function that maximize the log likelihood (MLE, maximum likelihood estimator) of the predicted values, conditioned on the independent data, we arrive to exactly the same solution that when optimizing the RSS cost function (<strong>see chapter 3 of the book: The Elements of Statistical Analysis, Hastie, Tibshirani</strong>)</p>
<p>The interesting point is that changing the pdf of the error term we arrive to different solutions.</p>
<p>If the error term has a Laplacian pdf and we use the same MLE strategy to estimate the parameters, we arrive to the same solution as when optimizing the least absolute value as a cost function.</p>
<p>When using different pdf distributions for the error term we arrive to different solutions.</p>
<p>To use a Laplacian pdf or any other long tail distribution provides a robust linear regression. The reason for that is that the error term distributon is able to incorporate outliers whithout having to change drastically its location (mean) parameters. When using a gaussian pdf this is not possible as the gaussian tolerates outliers badly (<strong>see chapter 7 of K.P. Murphy book: Machine Learning a probabilistic perspective</strong>).</p>
<h3><em>Optimization of stochastic cost function</em></h3>
<p>When using the probabilistic perspective to the cost function minimization approach we have to minimize the expected value of the cost function as now both X and Y (independent and dependent variables) are random variables. Following this path we obtain that the best solution, in case of the square loss function,  is the expected value of Y conditioned on X [<img src="https://s0.wp.com/latex.php?latex=%5Clarge+E%28Y%2FX%29&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="&#92;large E(Y/X)" class="latex" />], and, in the case of the LAD cost function,  is the median of Y conditioned on X [<img src="https://s0.wp.com/latex.php?latex=%5Clarge+Median%28Y%2FX%29&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="&#92;large Median(Y/X)" class="latex" />]. A nice generalization of the latter is <a href="https://en.wikipedia.org/wiki/Quantile_regression">Quantile Regression</a> where we extend the median results (quantile 0.5) to any other quantile of the <img src="https://s0.wp.com/latex.php?latex=%5Clarge+Prob%28Y%2FX%29&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="&#92;large Prob(Y/X)" class="latex" /> distribution.</p>
<p>&nbsp;</p>
<h1>Bayesian</h1>
<p>The bayesian solution gives the most insigth to the different elements that can take part in a linear regression.</p>
<p>To tell it shortly, the bayesian&#8217;s <strong>prior distribution</strong>  takes care of the regularization term and the bayesian&#8217;s <strong>likelihood distribution</strong> handles the errors and outliers.</p>
<p>Regularization is a term used in machine learning for different techniques to manage the overfitting problem. This is done, for example,  adding a additional (regularization) term to a cost function in such a way that increasing the values of the parameters of the model gives a penalty to the cost function.</p>
<p>Why reducing the values of the parameters of a model is good to control overfitting?</p>
<p>Consider a model with a lot of parameters (so, prone to overfit) but with all the parameters equal to zero (regularization term dissapears!). Well, this model is complex (lots of parameters) but as all the parameters are zero it is actually not complex anymore, that is actually as not having any model at all.</p>
<p>This is the way that the regularization term controls overfitting of a complex model.</p>
<p>How the prior of a bayesian model is related to regularization? Well, if we assume that our parameters prior is a gaussian with zero means and a very small variance, we are telling that we are pretty sure that the parameters will be close to zero. So, we are actually adding a regularization term.</p>
<p>Otherwise, if we impose the prior of the parameters distribution to be a uniform distribution, we don&#8217;t add any regularization term.  A uniform prior on a bayesian model will arrive to the same OLS results obtained by the Normal Equations, without regularization (<strong>Table 7.1 of Murphy&#8217;s book)</strong> .</p>
<p>If we do the maths (<strong>chapter 7.5.1 of Murphy&#8217;s book)</strong> the result of adding a gaussian prior is equivalent to adding a ridge regression term to our linear regression model. If, instead we add a laplacian prior we have a different kind of regularization that is called <a href="https://en.wikipedia.org/wiki/Lasso_(statistics)">lasso</a>.</p>
<p>The likelihood distribution in the bayesian model has the same effect that the error term pdf in the probabilistic approach. Using diferent likelihoods distributions we can control our model robustness to outliers.</p>
<p>&nbsp;</p>
<h1>R code sources</h1>
<p>Great R code repository for statistical modeling, with code for all the linear models variants: <a href="http://www.uni-kiel.de/psychologie/rexrepos/posts/regression.html" rel="nofollow">http://www.uni-kiel.de/psychologie/rexrepos/posts/regression.html</a></p>
<p>&nbsp;</p>
]]></content:encoded>
					
					<wfw:commentRss>https://mlopezm.wordpress.com/2016/07/20/linear-regression-can-be-understood-in-many-ways-optimization-probabilistic-bayesian/feed/</wfw:commentRss>
			<slash:comments>1</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/f6cc6c716c79438c53742113a478799b?s=96&#38;d=identicon&#38;r=G" medium="image">
			<media:title type="html">mlopezm</media:title>
		</media:content>
	</item>
		<item>
		<title>Best introduction to NoSQL</title>
		<link>https://mlopezm.wordpress.com/2016/05/25/best-introduction-to-nosql/</link>
					<comments>https://mlopezm.wordpress.com/2016/05/25/best-introduction-to-nosql/#respond</comments>
		
		<dc:creator><![CDATA[mlopezm]]></dc:creator>
		<pubDate>Wed, 25 May 2016 11:40:52 +0000</pubDate>
				<category><![CDATA[NoSQL]]></category>
		<category><![CDATA[Sin categoría]]></category>
		<guid isPermaLink="false">http://mlopezm.wordpress.com/?p=852</guid>

					<description><![CDATA[This is the best introduction to NoSQL, I have seen]]></description>
										<content:encoded><![CDATA[<p>This is the best introduction to NoSQL, I have seen </p>
<div class="jetpack-video-wrapper"><iframe class="youtube-player" width="730" height="411" src="https://www.youtube.com/embed/qI_g07C_Q5I?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=es&#038;autohide=2&#038;wmode=transparent" allowfullscreen="true" style="border:0;" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation"></iframe></div>
]]></content:encoded>
					
					<wfw:commentRss>https://mlopezm.wordpress.com/2016/05/25/best-introduction-to-nosql/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/f6cc6c716c79438c53742113a478799b?s=96&#38;d=identicon&#38;r=G" medium="image">
			<media:title type="html">mlopezm</media:title>
		</media:content>
	</item>
		<item>
		<title>Maximum Likelihood estimates follow a normal distribution</title>
		<link>https://mlopezm.wordpress.com/2016/05/24/maximum-likelihood-estimates-follow-a-normal-distribution/</link>
					<comments>https://mlopezm.wordpress.com/2016/05/24/maximum-likelihood-estimates-follow-a-normal-distribution/#comments</comments>
		
		<dc:creator><![CDATA[mlopezm]]></dc:creator>
		<pubDate>Tue, 24 May 2016 21:45:59 +0000</pubDate>
				<category><![CDATA[Fisher Information]]></category>
		<category><![CDATA[Maximum Likelihood]]></category>
		<category><![CDATA[Sin categoría]]></category>
		<guid isPermaLink="false">http://mlopezm.wordpress.com/?p=832</guid>

					<description><![CDATA[I was quite surprised when I learnt that a maximum likelihood estimate follows asymptotically a normal distribution with the mean being the estimated value and the variance being the inverse of the Fisher Information multiplied by the number of observations. Asymptotically means when you have a big number of samples to calculate your ML estimate. [&#8230;]]]></description>
										<content:encoded><![CDATA[<p>I was quite surprised when I learnt that a maximum likelihood estimate follows asymptotically a normal distribution with the mean being the estimated value and the variance being the inverse of the Fisher Information multiplied by the number of observations.</p>
<p>Asymptotically means when you have a big number of samples to calculate your ML estimate.</p>
<p>This is quite a remarkable fact. Some information:</p>
<p><a href="http://math.arizona.edu/~jwatkins/o-mle.pdf" rel="nofollow">http://math.arizona.edu/~jwatkins/o-mle.pdf</a> (page 10)</p>
<p><a href="http://people.missouristate.edu/songfengzheng/Teaching/MTH541/Lecture%20notes/Fisher_info.pdf" target="_blank" rel="noopener noreferrer nofollow">Haz clic para acceder a Fisher_info.pdf</a></p>
<p><a href="http://stats.stackexchange.com/questions/88481/what-is-meant-by-the-standard-error-of-a-maximum-likelihood-estimate" rel="nofollow">http://stats.stackexchange.com/questions/88481/what-is-meant-by-the-standard-error-of-a-maximum-likelihood-estimate</a></p>
<p><a href="http://stats.stackexchange.com/questions/113860/how-to-compute-or-numerically-estimate-the-standard-error-of-the-mle?rq=1" rel="nofollow">http://stats.stackexchange.com/questions/113860/how-to-compute-or-numerically-estimate-the-standard-error-of-the-mle?rq=1</a></p>
]]></content:encoded>
					
					<wfw:commentRss>https://mlopezm.wordpress.com/2016/05/24/maximum-likelihood-estimates-follow-a-normal-distribution/feed/</wfw:commentRss>
			<slash:comments>1</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/f6cc6c716c79438c53742113a478799b?s=96&#38;d=identicon&#38;r=G" medium="image">
			<media:title type="html">mlopezm</media:title>
		</media:content>
	</item>
		<item>
		<title>P-hacking</title>
		<link>https://mlopezm.wordpress.com/2016/03/11/p-hacking/</link>
					<comments>https://mlopezm.wordpress.com/2016/03/11/p-hacking/#respond</comments>
		
		<dc:creator><![CDATA[mlopezm]]></dc:creator>
		<pubDate>Fri, 11 Mar 2016 13:38:39 +0000</pubDate>
				<category><![CDATA[p-value]]></category>
		<category><![CDATA[Sin categoría]]></category>
		<category><![CDATA[p-hacking]]></category>
		<guid isPermaLink="false">http://mlopezm.wordpress.com/?p=828</guid>

					<description><![CDATA[This entry on p-hacking and the issues with the use of p-values is very good: http://stats.stackexchange.com/questions/200745/how-much-do-we-know-about-p-hacking-in-the-wild]]></description>
										<content:encoded><![CDATA[<p>This entry on p-hacking and the issues with the use of p-values is very good:</p>
<p><a href="http://stats.stackexchange.com/questions/200745/how-much-do-we-know-about-p-hacking-in-the-wild" rel="nofollow">http://stats.stackexchange.com/questions/200745/how-much-do-we-know-about-p-hacking-in-the-wild</a></p>
]]></content:encoded>
					
					<wfw:commentRss>https://mlopezm.wordpress.com/2016/03/11/p-hacking/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/f6cc6c716c79438c53742113a478799b?s=96&#38;d=identicon&#38;r=G" medium="image">
			<media:title type="html">mlopezm</media:title>
		</media:content>
	</item>
		<item>
		<title>Missing values imputation</title>
		<link>https://mlopezm.wordpress.com/2016/03/07/missing-values-imputation/</link>
					<comments>https://mlopezm.wordpress.com/2016/03/07/missing-values-imputation/#respond</comments>
		
		<dc:creator><![CDATA[mlopezm]]></dc:creator>
		<pubDate>Mon, 07 Mar 2016 13:00:06 +0000</pubDate>
				<category><![CDATA[missing values]]></category>
		<category><![CDATA[Sin categoría]]></category>
		<category><![CDATA[imputation]]></category>
		<category><![CDATA[R]]></category>
		<guid isPermaLink="false">http://mlopezm.wordpress.com/?p=824</guid>

					<description><![CDATA[This blog is great as a reference to missing value imputation in R Tutorial on 5 Powerful R Packages used for imputing missing values]]></description>
										<content:encoded><![CDATA[<p>This blog is great as a reference to missing value imputation in R</p>
<div class="embed-analyticsvidhya">
<blockquote class="wp-embedded-content"><p><a href="http://www.analyticsvidhya.com/blog/2016/03/tutorial-powerful-packages-imputing-missing-values/">Tutorial on 5 Powerful R Packages used for imputing missing values</a></p></blockquote>
<p><script type='text/javascript'><!--//--><![CDATA[//><!--		!function(a,b){"use strict";function c(){if(!e){e=!0;var a,c,d,f,g=-1!==navigator.appVersion.indexOf("MSIE 10"),h=!!navigator.userAgent.match(/Trident.*rv:11./),i=b.querySelectorAll("iframe.wp-embedded-content"),j=b.querySelectorAll("blockquote.wp-embedded-content");for(c=0;c<j.length;c++)j[c].style.display="none";for(c=0;c<i.length;c++)if(d=i[c],d.style.display="",!d.getAttribute("data-secret")){if(f=Math.random().toString(36).substr(2,10),d.src+="#?secret="+f,d.setAttribute("data-secret",f),g||h)a=d.cloneNode(!0),a.removeAttribute("security"),d.parentNode.replaceChild(a,d)}else;}}var d=!1,e=!1;if(b.querySelector)if(a.addEventListener)d=!0;if(a.wp=a.wp||{},!a.wp.receiveEmbedMessage)if(a.wp.receiveEmbedMessage=function(c){var d=c.data;if(d.secret||d.message||d.value)if(!/[^a-zA-Z0-9]/.test(d.secret)){var e,f,g,h,i,j=b.querySelectorAll('iframe[data-secret="'+d.secret+'"]'),k=b.querySelectorAll('blockquote[data-secret="'+d.secret+'"]');for(e=0;e<k.length;e++)k[e].style.display="none";for(e=0;e<j.length;e++)if(f=j[e],c.source===f.contentWindow){if(f.style.display="","height"===d.message){if(g=parseInt(d.value,10),g>1e3)g=1e3;else if(200>~~g)g=200;f.height=g}if("link"===d.message)if(h=b.createElement("a"),i=b.createElement("a"),h.href=f.getAttribute("src"),i.href=d.value,i.host===h.host)if(b.activeElement===f)a.top.location.href=d.value}else;}},d)a.addEventListener("message",a.wp.receiveEmbedMessage,!1),b.addEventListener("DOMContentLoaded",c,!1),a.addEventListener("load",c,!1)}(window,document);//--><!]]&gt;</script><iframe sandbox="allow-scripts" security="restricted" src="http://www.analyticsvidhya.com/blog/2016/03/tutorial-powerful-packages-imputing-missing-values/embed/" width="600" height="338" title="Embedded WordPress Post" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" class="wp-embedded-content"></iframe></div>
]]></content:encoded>
					
					<wfw:commentRss>https://mlopezm.wordpress.com/2016/03/07/missing-values-imputation/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/f6cc6c716c79438c53742113a478799b?s=96&#38;d=identicon&#38;r=G" medium="image">
			<media:title type="html">mlopezm</media:title>
		</media:content>
	</item>
		<item>
		<title>Deep learning courses</title>
		<link>https://mlopezm.wordpress.com/2016/03/07/deep-learning-courses/</link>
					<comments>https://mlopezm.wordpress.com/2016/03/07/deep-learning-courses/#respond</comments>
		
		<dc:creator><![CDATA[mlopezm]]></dc:creator>
		<pubDate>Mon, 07 Mar 2016 12:57:59 +0000</pubDate>
				<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Sin categoría]]></category>
		<category><![CDATA[Course]]></category>
		<category><![CDATA[Deep learning]]></category>
		<category><![CDATA[Online course]]></category>
		<guid isPermaLink="false">http://mlopezm.wordpress.com/?p=820</guid>

					<description><![CDATA[Excellent http://info.usherbrooke.ca/hlarochelle/neural_networks/content.html Very good https://www.coursera.org/course/neuralnets Fantastic (made by Google) https://www.udacity.com/course/deep-learning&#8211;ud730 Splendid (more specialized on NLP) http://cs224d.stanford.edu/syllabus.html And take a look to this online resources: http://www.deeplearningbook.org/ http://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial http://deeplearning.stanford.edu/tutorial/ http://deeplearning.net/tutorial/ http://deeplearning.net/reading-list/tutorials/]]></description>
										<content:encoded><![CDATA[<p>Excellent </p>
<p><a href="http://info.usherbrooke.ca/hlarochelle/neural_networks/content.html" rel="nofollow">http://info.usherbrooke.ca/hlarochelle/neural_networks/content.html</a></p>
<p>Very good</p>
<p><a href="https://www.coursera.org/course/neuralnets" rel="nofollow">https://www.coursera.org/course/neuralnets</a></p>
<p>Fantastic (made by Google)</p>
<p><a href="https://www.udacity.com/course/deep-learning&#8211;ud730" rel="nofollow">https://www.udacity.com/course/deep-learning&#8211;ud730</a></p>
<p>Splendid (more specialized on NLP)<br />
<a href="http://cs224d.stanford.edu/syllabus.html" rel="nofollow">http://cs224d.stanford.edu/syllabus.html</a></p>
<p>And take a look to this online resources:<br />
<a href="http://www.deeplearningbook.org/" rel="nofollow">http://www.deeplearningbook.org/</a><br />
<a href="http://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial" rel="nofollow">http://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial</a><br />
<a href="http://deeplearning.stanford.edu/tutorial/" rel="nofollow">http://deeplearning.stanford.edu/tutorial/</a><br />
<a href="http://deeplearning.net/tutorial/" rel="nofollow">http://deeplearning.net/tutorial/</a><br />
<a href="http://deeplearning.net/reading-list/tutorials/" rel="nofollow">http://deeplearning.net/reading-list/tutorials/</a></p>
]]></content:encoded>
					
					<wfw:commentRss>https://mlopezm.wordpress.com/2016/03/07/deep-learning-courses/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/f6cc6c716c79438c53742113a478799b?s=96&#38;d=identicon&#38;r=G" medium="image">
			<media:title type="html">mlopezm</media:title>
		</media:content>
	</item>
		<item>
		<title>Econometrics</title>
		<link>https://mlopezm.wordpress.com/2016/03/01/econometrics/</link>
					<comments>https://mlopezm.wordpress.com/2016/03/01/econometrics/#respond</comments>
		
		<dc:creator><![CDATA[mlopezm]]></dc:creator>
		<pubDate>Tue, 01 Mar 2016 12:17:20 +0000</pubDate>
				<category><![CDATA[Sin categoría]]></category>
		<guid isPermaLink="false">http://mlopezm.wordpress.com/?p=815</guid>

					<description><![CDATA[These tutorials are fantastic for econometrics, at all levels: undergraduate: graduate level:]]></description>
										<content:encoded><![CDATA[<p>These tutorials are fantastic for econometrics, at all levels:<br />
undergraduate:</p>
<div class="jetpack-video-wrapper"><iframe class="youtube-player" width="730" height="411" src="https://www.youtube.com/embed/M_5SLG7sUa0?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=es&#038;autohide=2&#038;wmode=transparent&#038;listType=playlist&#038;list=PLwJRxp3blEvZyQBTTOMFRP_TDaSdly3gU" allowfullscreen="true" style="border:0;" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation"></iframe></div>
<div class="jetpack-video-wrapper"><iframe class="youtube-player" width="730" height="411" src="https://www.youtube.com/embed/Ky0YCQlQ5ZI?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=es&#038;autohide=2&#038;wmode=transparent&#038;listType=playlist&#038;list=PLwJRxp3blEvb7P-7po9AxuBwquPv75LjU" allowfullscreen="true" style="border:0;" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation"></iframe></div>
<p>graduate level:</p>
<div class="jetpack-video-wrapper"><iframe class="youtube-player" width="730" height="411" src="https://www.youtube.com/embed/GMVh02WGhoc?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=es&#038;autohide=2&#038;wmode=transparent&#038;listType=playlist&#038;list=PLwJRxp3blEvaxmHgI2iOzNP6KGLSyd4dz" allowfullscreen="true" style="border:0;" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation"></iframe></div>
]]></content:encoded>
					
					<wfw:commentRss>https://mlopezm.wordpress.com/2016/03/01/econometrics/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/f6cc6c716c79438c53742113a478799b?s=96&#38;d=identicon&#38;r=G" medium="image">
			<media:title type="html">mlopezm</media:title>
		</media:content>
	</item>
		<item>
		<title>Factor analysis</title>
		<link>https://mlopezm.wordpress.com/2016/03/01/factor-analysis/</link>
					<comments>https://mlopezm.wordpress.com/2016/03/01/factor-analysis/#respond</comments>
		
		<dc:creator><![CDATA[mlopezm]]></dc:creator>
		<pubDate>Tue, 01 Mar 2016 12:07:55 +0000</pubDate>
				<category><![CDATA[factor analysis]]></category>
		<category><![CDATA[Sin categoría]]></category>
		<guid isPermaLink="false">http://mlopezm.wordpress.com/?p=811</guid>

					<description><![CDATA[Great places for Exploratory Factor Analysis (conceptual):]]></description>
										<content:encoded><![CDATA[<p>Great places for Exploratory Factor Analysis (conceptual):</p>
<div class="jetpack-video-wrapper"><iframe class="youtube-player" width="730" height="411" src="https://www.youtube.com/embed/WV_jcaDBZ2I?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=es&#038;autohide=2&#038;wmode=transparent&#038;listType=playlist&#038;list=PLwJRxp3blEvaOTZfSKXysxRmi6gXJf5gP" allowfullscreen="true" style="border:0;" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation"></iframe></div>
<div class="jetpack-video-wrapper"><iframe class="youtube-player" width="730" height="411" src="https://www.youtube.com/embed/Q2JBLuQDUvI?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=es&#038;autohide=2&#038;wmode=transparent&#038;listType=playlist&#038;list=PLB2mx3gLXExM-uTqupofV-viBuA5MF-bc" allowfullscreen="true" style="border:0;" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation"></iframe></div>
<div class="jetpack-video-wrapper"><iframe class="youtube-player" width="730" height="411" src="https://www.youtube.com/embed/UspPltPmzOY?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=es&#038;autohide=2&#038;wmode=transparent" allowfullscreen="true" style="border:0;" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation"></iframe></div>
]]></content:encoded>
					
					<wfw:commentRss>https://mlopezm.wordpress.com/2016/03/01/factor-analysis/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/f6cc6c716c79438c53742113a478799b?s=96&#38;d=identicon&#38;r=G" medium="image">
			<media:title type="html">mlopezm</media:title>
		</media:content>
	</item>
	</channel>
</rss>
