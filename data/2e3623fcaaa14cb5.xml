<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns:yt="http://www.youtube.com/xml/schemas/2015" xmlns:media="http://search.yahoo.com/mrss/" xmlns="http://www.w3.org/2005/Atom">
 <link rel="self" href="http://www.youtube.com/feeds/videos.xml?channel_id=UCBa5G_ESCn8Yd4vw5U-gIcg"/>
 <id>yt:channel:UCBa5G_ESCn8Yd4vw5U-gIcg</id>
 <yt:channelId>UCBa5G_ESCn8Yd4vw5U-gIcg</yt:channelId>
 <title>Stanford Online</title>
 <link rel="alternate" href="https://www.youtube.com/channel/UCBa5G_ESCn8Yd4vw5U-gIcg"/>
 <author>
  <name>Stanford Online</name>
  <uri>https://www.youtube.com/channel/UCBa5G_ESCn8Yd4vw5U-gIcg</uri>
 </author>
 <published>2009-03-10T01:51:58+00:00</published>
 <entry>
  <id>yt:video:htjpbbvHJQo</id>
  <yt:videoId>htjpbbvHJQo</yt:videoId>
  <yt:channelId>UCBa5G_ESCn8Yd4vw5U-gIcg</yt:channelId>
  <title>Stanford Seminar - ML Explainability Part 4 I Evaluating Model Interpretations/Explanations</title>
  <link rel="alternate" href="https://www.youtube.com/watch?v=htjpbbvHJQo"/>
  <author>
   <name>Stanford Online</name>
   <uri>https://www.youtube.com/channel/UCBa5G_ESCn8Yd4vw5U-gIcg</uri>
  </author>
  <published>2022-11-05T16:30:05+00:00</published>
  <updated>2022-11-05T16:30:05+00:00</updated>
  <media:group>
   <media:title>Stanford Seminar - ML Explainability Part 4 I Evaluating Model Interpretations/Explanations</media:title>
   <media:content url="https://www.youtube.com/v/htjpbbvHJQo?version=3" type="application/x-shockwave-flash" width="640" height="390"/>
   <media:thumbnail url="https://i1.ytimg.com/vi/htjpbbvHJQo/hqdefault.jpg" width="480" height="360"/>
   <media:description>Professor Hima Lakkaraju describes how explanation methods can be compared and evaluated. Interpretability evaluation techniques range from the highly quantitative, where interpretability is replaced with a metric such as the number of rules or parameters, to qualitative where humans are asked to rate the interpretation.

View the full playlist: https://www.youtube.com/playlist?list=PLoROMvodv4rPh6wa6PGcHH6vMG9sEIPxL</media:description>
   <media:community>
    <media:starRating count="7" average="5.00" min="1" max="5"/>
    <media:statistics views="97"/>
   </media:community>
  </media:group>
 </entry>
 <entry>
  <id>yt:video:VowMdZYvh0Q</id>
  <yt:videoId>VowMdZYvh0Q</yt:videoId>
  <yt:channelId>UCBa5G_ESCn8Yd4vw5U-gIcg</yt:channelId>
  <title>Stanford Seminar - Making Teamwork an Objective Discipline - Sid Sijbrandij CEO &amp; Chairman of GitLab</title>
  <link rel="alternate" href="https://www.youtube.com/watch?v=VowMdZYvh0Q"/>
  <author>
   <name>Stanford Online</name>
   <uri>https://www.youtube.com/channel/UCBa5G_ESCn8Yd4vw5U-gIcg</uri>
  </author>
  <published>2022-11-04T18:15:01+00:00</published>
  <updated>2022-11-05T08:05:58+00:00</updated>
  <media:group>
   <media:title>Stanford Seminar - Making Teamwork an Objective Discipline - Sid Sijbrandij CEO &amp; Chairman of GitLab</media:title>
   <media:content url="https://www.youtube.com/v/VowMdZYvh0Q?version=3" type="application/x-shockwave-flash" width="640" height="390"/>
   <media:thumbnail url="https://i3.ytimg.com/vi/VowMdZYvh0Q/hqdefault.jpg" width="480" height="360"/>
   <media:description>Sid Sijbrandij is the Co-founder, CEO and Chairman of GitLab, Inc
October 25, 2022

More about the speaker: https://about.gitlab.com/handbook/ceo/

#gitlab</media:description>
   <media:community>
    <media:starRating count="5" average="5.00" min="1" max="5"/>
    <media:statistics views="364"/>
   </media:community>
  </media:group>
 </entry>
 <entry>
  <id>yt:video:3rTUouPByNw</id>
  <yt:videoId>3rTUouPByNw</yt:videoId>
  <yt:channelId>UCBa5G_ESCn8Yd4vw5U-gIcg</yt:channelId>
  <title>Stanford Webinar - CRISPR - 10 Years of Genome Editing and More</title>
  <link rel="alternate" href="https://www.youtube.com/watch?v=3rTUouPByNw"/>
  <author>
   <name>Stanford Online</name>
   <uri>https://www.youtube.com/channel/UCBa5G_ESCn8Yd4vw5U-gIcg</uri>
  </author>
  <published>2022-11-04T17:57:55+00:00</published>
  <updated>2022-11-04T20:29:19+00:00</updated>
  <media:group>
   <media:title>Stanford Webinar - CRISPR - 10 Years of Genome Editing and More</media:title>
   <media:content url="https://www.youtube.com/v/3rTUouPByNw?version=3" type="application/x-shockwave-flash" width="640" height="390"/>
   <media:thumbnail url="https://i4.ytimg.com/vi/3rTUouPByNw/hqdefault.jpg" width="480" height="360"/>
   <media:description>Learn more: https://stanford.io/3Ci61RY

Stanford Professor of Genetics Michael Snyder, PHD explains how CRISPR has impacted the areas of health, medicine, and agriculture over the last 10 years. He provides examples of how it's been used to innovate in the past, and explores the possibilities for expanded usage in the future.

You Will:
- Understand the basic biology of the CRISPR/Cas9 system as an efficient, accurate genome editing tool
- Discover the various applications of the CRISPR system including its use in cancer and gene therapy
- Learn how CRISPR has been used in the past and explore new directions in the field for the future

#crispr</media:description>
   <media:community>
    <media:starRating count="7" average="5.00" min="1" max="5"/>
    <media:statistics views="352"/>
   </media:community>
  </media:group>
 </entry>
 <entry>
  <id>yt:video:_5jtMZXqP5k</id>
  <yt:videoId>_5jtMZXqP5k</yt:videoId>
  <yt:channelId>UCBa5G_ESCn8Yd4vw5U-gIcg</yt:channelId>
  <title>Kratika Gupta talks about Stanford's Product Management Program</title>
  <link rel="alternate" href="https://www.youtube.com/watch?v=_5jtMZXqP5k"/>
  <author>
   <name>Stanford Online</name>
   <uri>https://www.youtube.com/channel/UCBa5G_ESCn8Yd4vw5U-gIcg</uri>
  </author>
  <published>2022-11-04T17:47:03+00:00</published>
  <updated>2022-11-04T21:30:35+00:00</updated>
  <media:group>
   <media:title>Kratika Gupta talks about Stanford's Product Management Program</media:title>
   <media:content url="https://www.youtube.com/v/_5jtMZXqP5k?version=3" type="application/x-shockwave-flash" width="640" height="390"/>
   <media:thumbnail url="https://i4.ytimg.com/vi/_5jtMZXqP5k/hqdefault.jpg" width="480" height="360"/>
   <media:description>Learn more: https://stanford.io/3fDhm6L

The Product Management Program will equip you with the skills you need to be a great product manager. We’ve distilled the lessons learned from decades of product management career experience and leading research to give you exactly what you need to be effective in the role. Spanning topics in product creation, marketing, team operations, product costing, and more, this program will teach you the skills you need to build a career toolkit that is complete and well-rounded.

#productmanagement</media:description>
   <media:community>
    <media:starRating count="5" average="5.00" min="1" max="5"/>
    <media:statistics views="369"/>
   </media:community>
  </media:group>
 </entry>
 <entry>
  <id>yt:video:_6n8r523QP8</id>
  <yt:videoId>_6n8r523QP8</yt:videoId>
  <yt:channelId>UCBa5G_ESCn8Yd4vw5U-gIcg</yt:channelId>
  <title>Stanford Seminar - ML Explainability Part 3 I Post hoc Explanation Methods</title>
  <link rel="alternate" href="https://www.youtube.com/watch?v=_6n8r523QP8"/>
  <author>
   <name>Stanford Online</name>
   <uri>https://www.youtube.com/channel/UCBa5G_ESCn8Yd4vw5U-gIcg</uri>
  </author>
  <published>2022-11-04T16:30:04+00:00</published>
  <updated>2022-11-04T16:30:04+00:00</updated>
  <media:group>
   <media:title>Stanford Seminar - ML Explainability Part 3 I Post hoc Explanation Methods</media:title>
   <media:content url="https://www.youtube.com/v/_6n8r523QP8?version=3" type="application/x-shockwave-flash" width="640" height="390"/>
   <media:thumbnail url="https://i4.ytimg.com/vi/_6n8r523QP8/hqdefault.jpg" width="480" height="360"/>
   <media:description>Professor Hima Lakkaraju presents some of the latest advancements in post hoc explanations for black-box machine learning models such as neural networks. Post hoc explanation techniques include local explanations such as feature importance, saliency maps, and counterfactuals as well as global explanations such as collections of local explanations, model distillation and summaries of counterfactuals.

View the full playlist: https://www.youtube.com/playlist?list=PLoROMvodv4rPh6wa6PGcHH6vMG9sEIPxL</media:description>
   <media:community>
    <media:starRating count="25" average="5.00" min="1" max="5"/>
    <media:statistics views="857"/>
   </media:community>
  </media:group>
 </entry>
 <entry>
  <id>yt:video:9Th86_3Ea7o</id>
  <yt:videoId>9Th86_3Ea7o</yt:videoId>
  <yt:channelId>UCBa5G_ESCn8Yd4vw5U-gIcg</yt:channelId>
  <title>Stanford Seminar - ML Explainability Part 2 I Inherently Interpretable Models</title>
  <link rel="alternate" href="https://www.youtube.com/watch?v=9Th86_3Ea7o"/>
  <author>
   <name>Stanford Online</name>
   <uri>https://www.youtube.com/channel/UCBa5G_ESCn8Yd4vw5U-gIcg</uri>
  </author>
  <published>2022-11-03T16:30:01+00:00</published>
  <updated>2022-11-04T00:25:01+00:00</updated>
  <media:group>
   <media:title>Stanford Seminar - ML Explainability Part 2 I Inherently Interpretable Models</media:title>
   <media:content url="https://www.youtube.com/v/9Th86_3Ea7o?version=3" type="application/x-shockwave-flash" width="640" height="390"/>
   <media:thumbnail url="https://i2.ytimg.com/vi/9Th86_3Ea7o/hqdefault.jpg" width="480" height="360"/>
   <media:description>Professor Hima Lakkaraju presents some of the latest advancements in machine learning models that are inherently interpretable such as rule-based models, risk scores, generalized additive models and prototype based models.

View the full playlist: https://www.youtube.com/playlist?list=PLoROMvodv4rPh6wa6PGcHH6vMG9sEIPxL</media:description>
   <media:community>
    <media:starRating count="36" average="5.00" min="1" max="5"/>
    <media:statistics views="1126"/>
   </media:community>
  </media:group>
 </entry>
 <entry>
  <id>yt:video:_DYQdP_F-LA</id>
  <yt:videoId>_DYQdP_F-LA</yt:videoId>
  <yt:channelId>UCBa5G_ESCn8Yd4vw5U-gIcg</yt:channelId>
  <title>Stanford Seminar - ML Explainability Part 1 I Overview and Motivation for Explainability</title>
  <link rel="alternate" href="https://www.youtube.com/watch?v=_DYQdP_F-LA"/>
  <author>
   <name>Stanford Online</name>
   <uri>https://www.youtube.com/channel/UCBa5G_ESCn8Yd4vw5U-gIcg</uri>
  </author>
  <published>2022-11-02T23:51:00+00:00</published>
  <updated>2022-11-02T23:58:42+00:00</updated>
  <media:group>
   <media:title>Stanford Seminar - ML Explainability Part 1 I Overview and Motivation for Explainability</media:title>
   <media:content url="https://www.youtube.com/v/_DYQdP_F-LA?version=3" type="application/x-shockwave-flash" width="640" height="390"/>
   <media:thumbnail url="https://i4.ytimg.com/vi/_DYQdP_F-LA/hqdefault.jpg" width="480" height="360"/>
   <media:description>In the first segment of the workshop, Professor Hima Lakkaraju motivates the need for interpretable machine learning in order to diagnose and build trust in autonomous systems.

Professor Hima Lakkaraju's day-long workshop at Stanford covered modern techniques for interpretable machine learning. 

About the speaker: Himabindu (Hima) Lakkaraju is an assistant professor at Harvard University focusing on explainability, fairness, and robustness of machine learning models. She has also been working with various domain experts in policy and healthcare to understand the real-world implications of explainable and fair ML. Hima has been named as one of the world’s top innovators under 35 by both MIT Tech Review and Vanity Fair. Her research has also received best paper awards at SIAM International Conference on Data Mining (SDM) and INFORMS, and grants from NSF, Google, Amazon, and Bayer. Hima has given keynote talks at various top ML conferences and workshops including CIKM, ICML, NeurIPS, AAAI, and CVPR, and her research has also been showcased by popular media outlets including the New York Times, MIT Tech Review, TIME magazine, and Forbes. More recently, she co-founded the Trustworthy ML Initiative to enable easy access to resources on trustworthy ML and to build a community of researchers/practitioners working on the topic. Learn more on her website: https://himalakkaraju.github.io/

View the full playlist: https://www.youtube.com/playlist?list=PLoROMvodv4rPh6wa6PGcHH6vMG9sEIPxL</media:description>
   <media:community>
    <media:starRating count="111" average="5.00" min="1" max="5"/>
    <media:statistics views="3350"/>
   </media:community>
  </media:group>
 </entry>
 <entry>
  <id>yt:video:M8LAIiU7VPA</id>
  <yt:videoId>M8LAIiU7VPA</yt:videoId>
  <yt:channelId>UCBa5G_ESCn8Yd4vw5U-gIcg</yt:channelId>
  <title>Stanford Seminar -  Towards Generalizable Autonomy: Duality of Discovery &amp; Bias</title>
  <link rel="alternate" href="https://www.youtube.com/watch?v=M8LAIiU7VPA"/>
  <author>
   <name>Stanford Online</name>
   <uri>https://www.youtube.com/channel/UCBa5G_ESCn8Yd4vw5U-gIcg</uri>
  </author>
  <published>2022-11-02T16:30:02+00:00</published>
  <updated>2022-11-02T16:30:03+00:00</updated>
  <media:group>
   <media:title>Stanford Seminar -  Towards Generalizable Autonomy: Duality of Discovery &amp; Bias</media:title>
   <media:content url="https://www.youtube.com/v/M8LAIiU7VPA?version=3" type="application/x-shockwave-flash" width="640" height="390"/>
   <media:thumbnail url="https://i2.ytimg.com/vi/M8LAIiU7VPA/hqdefault.jpg" width="480" height="360"/>
   <media:description>Towards Generalizable Autonomy: Duality of Discovery &amp; Bias
Animesh Garg of Georgia Tech/NVIDIA
October 21, 2022

Generalization in embodied intelligence, such as in robotics, requires interactive learning across families of tasks is essential for discovering efficient representation and inference mechanisms. Concurrent systems need a lot of hand-holding to even learn a single cognitive concept or a dexterous skill, say “open a door”, let alone generalizing to new windows and cupboards! This is far from our vision of everyday robots! would require a broader concept of generalization and continual update of representations. This study of the science of embodied AI opens three key questions: (a) Representational biases &amp; Causal inference for interactive decision making, (b) Perceptual representations learned by and for interaction, (c) Systems and abstractions for scalable learning. This talk will focus on decision-making uncovering the many facets of inductive biases in off-policy reinforcement learning in robotics. I will introduce C-Learning to trade off-speed and reliability instead of vanilla Q-Learning. Then I will talk about the discovery of latent causal structure to improve sample efficiency. Moving on from skills, we will describe task graphs for hierarchically structured tasks for manipulation. I will present how to scale structured learning in robot manipulation with Roboturk, and finally, prescribe a practical algorithm for deployment with safety constraints. Taking a step back, I will end with notions of structure in Embodied AI for both perception and decision making.

About the speaker:

Animesh Garg is an Assistant Professor of Computer Science at University of Toronto and a Faculty Member at the Vector Institute. He directs the UofT People, AI and Robotics (PAIR) group.  He is affiliated with Mechanical and Industrial Engineering (courtesy) and UofT Robotics Institute and also a Sr. Research Scientist at Nvidia. Learn more on his website: https://animesh.garg.tech/</media:description>
   <media:community>
    <media:starRating count="43" average="5.00" min="1" max="5"/>
    <media:statistics views="1398"/>
   </media:community>
  </media:group>
 </entry>
 <entry>
  <id>yt:video:xbES1armuJY</id>
  <yt:videoId>xbES1armuJY</yt:videoId>
  <yt:channelId>UCBa5G_ESCn8Yd4vw5U-gIcg</yt:channelId>
  <title>Stanford Seminar - Ethics Governance-in-the-Making: Bridging Ethics Work &amp; Governance Menlo Report</title>
  <link rel="alternate" href="https://www.youtube.com/watch?v=xbES1armuJY"/>
  <author>
   <name>Stanford Online</name>
   <uri>https://www.youtube.com/channel/UCBa5G_ESCn8Yd4vw5U-gIcg</uri>
  </author>
  <published>2022-11-01T21:45:51+00:00</published>
  <updated>2022-11-02T09:19:35+00:00</updated>
  <media:group>
   <media:title>Stanford Seminar - Ethics Governance-in-the-Making: Bridging Ethics Work &amp; Governance Menlo Report</media:title>
   <media:content url="https://www.youtube.com/v/xbES1armuJY?version=3" type="application/x-shockwave-flash" width="640" height="390"/>
   <media:thumbnail url="https://i1.ytimg.com/vi/xbES1armuJY/hqdefault.jpg" width="480" height="360"/>
   <media:description>Ethics Governance-in-the-Making: Bridging Ethics Work and Governance in the Menlo Report
Megan Finn of University of Washington
Katie Shilton of University of Maryland, College Park

October 21, 2022

The 2012 Menlo Report was an effort in which a group of computer scientists, US government funders, and lawyers produced ethics guidelines for research in information and communications technology. Using Menlo as a case study, we find that ethics governance-in-the-making is composed of processes that combine controversies of ethics and justice in the co-production of technoscience and society, the everyday ethics work of actors engaged in scientific and technical practice, and ethics as a form of governance in scientific and technical communities. Interviews with authors and analysis of the Menlo Report documents reveal a story of marshalling human and financial resources from a particular subfield within ICT research    network measurement and network security research    to close controversies worrying both researchers and funders. To create the Menlo Report, authors and funders relied on bricolage work with existing, available resources, significantly shaping both the report  s contents and impacts. Report authors were motivated by both forward- and backward-looking goals: enabling new data sharing as well as addressing the status of a research record thrown into ethical question by controversial studies. Authors also grappled with uncertainty about when ethical frameworks were appropriate and made the decision to classify much network data as human subjects data. Finally, the Menlo Report authors attempted to enroll multiple networks in governance through appeals to local research communities as well as taking steps towards federal rulemaking. This analysis of the Menlo Report suggests analytical lenses for study of current and future ethics governance in computer science. The Menlo Report serves as a case study in how to study ethics governance-in-the-making: with attention to resources, adaptation and bricolage, and with a focus on both the uncertainties the process tries to repair, as well as the new uncertainties the process uncovers, which will become the site of future ethics work.

About the speakers: 
Megan Finn is an Associate Professor at the Information School at University of Washington where she teaches about information policy and ethics. For this academic year Finn is a Visiting Scholar in the Department of Communication at the School of Humanities and Sciences at Stanford University. She is the author of Documenting Aftermath: Information Infrastructures in the Wake of Disasters and a co-author of a new report from the National Academies called &quot;Fostering Responsible Computing Research: Foundations and Practices.&quot;

Katie Shilton is an associate professor in the College of Information Studies at the University of Maryland, College Park. Her research focuses on technology and data ethics. She is the PI of the PERVADE project, a multi-campus collaboration focused on big data research ethics. Other projects include improving online content moderation with human-in-the-loop machine learning techniques; analyzing values in audiology technologies and treatment models; and designing experiential data ethics education. She is the founding co-director of the University of Maryland  s undergraduate major in social data science.</media:description>
   <media:community>
    <media:starRating count="11" average="5.00" min="1" max="5"/>
    <media:statistics views="598"/>
   </media:community>
  </media:group>
 </entry>
 <entry>
  <id>yt:video:-DhXHubx6YM</id>
  <yt:videoId>-DhXHubx6YM</yt:videoId>
  <yt:channelId>UCBa5G_ESCn8Yd4vw5U-gIcg</yt:channelId>
  <title>Stanford Webinar - Web3 Considered: Possible Futures for Decentralization and Digital Ownership</title>
  <link rel="alternate" href="https://www.youtube.com/watch?v=-DhXHubx6YM"/>
  <author>
   <name>Stanford Online</name>
   <uri>https://www.youtube.com/channel/UCBa5G_ESCn8Yd4vw5U-gIcg</uri>
  </author>
  <published>2022-10-31T20:00:00+00:00</published>
  <updated>2022-11-02T20:51:08+00:00</updated>
  <media:group>
   <media:title>Stanford Webinar - Web3 Considered: Possible Futures for Decentralization and Digital Ownership</media:title>
   <media:content url="https://www.youtube.com/v/-DhXHubx6YM?version=3" type="application/x-shockwave-flash" width="640" height="390"/>
   <media:thumbnail url="https://i2.ytimg.com/vi/-DhXHubx6YM/hqdefault.jpg" width="480" height="360"/>
   <media:description>Web3 is a broad concept encompassing blockchain technology and new models of exchange on the World Wide Web. Web3 will provide security and privacy through decentralization, digital ownership, digital markets and tokens. But what actually is Web3? What is the connection to blockchain, and how does it affect data security and privacy in the new era of the internet?

In this video, Stanford Professor John Mitchell and entrepreneur and investor, Mark Mitchell give us an insider's perspective on emerging developments in the blockchain-based web and discuss:
- A breakdown of Web3
- Decentralization and digital ownership
- What the evolution of the World Wide Web can mean for you and your sector

#cybersecurity #web3 #cryptocurrency 

Learn more about Stanford's Advanced Cybersecurity Program: https://stanford.io/3SUviH5

Learn more about Stanford's Cryptocurrencies and Blockchain Technologies course:
https://stanford.io/3DOv4wW</media:description>
   <media:community>
    <media:starRating count="64" average="5.00" min="1" max="5"/>
    <media:statistics views="2193"/>
   </media:community>
  </media:group>
 </entry>
 <entry>
  <id>yt:video:9iyx-sk1Q28</id>
  <yt:videoId>9iyx-sk1Q28</yt:videoId>
  <yt:channelId>UCBa5G_ESCn8Yd4vw5U-gIcg</yt:channelId>
  <title>Stanford CS330: Deep Multi-Task &amp; Meta Learning I 2021 I Lecture 18</title>
  <link rel="alternate" href="https://www.youtube.com/watch?v=9iyx-sk1Q28"/>
  <author>
   <name>Stanford Online</name>
   <uri>https://www.youtube.com/channel/UCBa5G_ESCn8Yd4vw5U-gIcg</uri>
  </author>
  <published>2022-10-29T17:45:00+00:00</published>
  <updated>2022-10-29T17:45:01+00:00</updated>
  <media:group>
   <media:title>Stanford CS330: Deep Multi-Task &amp; Meta Learning I 2021 I Lecture 18</media:title>
   <media:content url="https://www.youtube.com/v/9iyx-sk1Q28?version=3" type="application/x-shockwave-flash" width="640" height="390"/>
   <media:thumbnail url="https://i2.ytimg.com/vi/9iyx-sk1Q28/hqdefault.jpg" width="480" height="360"/>
   <media:description>For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai

To follow along with the course, visit: 
http://cs330.stanford.edu/fall2021/index.html

To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​

Chelsea Finn
Computer Science, PhD

Karol Hausman
Computer Science, PhD</media:description>
   <media:community>
    <media:starRating count="43" average="5.00" min="1" max="5"/>
    <media:statistics views="1388"/>
   </media:community>
  </media:group>
 </entry>
 <entry>
  <id>yt:video:VCK0BkfH35c</id>
  <yt:videoId>VCK0BkfH35c</yt:videoId>
  <yt:channelId>UCBa5G_ESCn8Yd4vw5U-gIcg</yt:channelId>
  <title>Stanford CS330: Deep Multi-Task &amp; Meta Learning I 2021 I Lecture 17</title>
  <link rel="alternate" href="https://www.youtube.com/watch?v=VCK0BkfH35c"/>
  <author>
   <name>Stanford Online</name>
   <uri>https://www.youtube.com/channel/UCBa5G_ESCn8Yd4vw5U-gIcg</uri>
  </author>
  <published>2022-10-28T15:15:03+00:00</published>
  <updated>2022-10-28T15:15:03+00:00</updated>
  <media:group>
   <media:title>Stanford CS330: Deep Multi-Task &amp; Meta Learning I 2021 I Lecture 17</media:title>
   <media:content url="https://www.youtube.com/v/VCK0BkfH35c?version=3" type="application/x-shockwave-flash" width="640" height="390"/>
   <media:thumbnail url="https://i3.ytimg.com/vi/VCK0BkfH35c/hqdefault.jpg" width="480" height="360"/>
   <media:description>For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai

To follow along with the course, visit: 
http://cs330.stanford.edu/fall2021/index.html

To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​

Chelsea Finn
Computer Science, PhD

Karol Hausman
Computer Science, PhD</media:description>
   <media:community>
    <media:starRating count="29" average="5.00" min="1" max="5"/>
    <media:statistics views="1331"/>
   </media:community>
  </media:group>
 </entry>
 <entry>
  <id>yt:video:r1E5OEu0uYM</id>
  <yt:videoId>r1E5OEu0uYM</yt:videoId>
  <yt:channelId>UCBa5G_ESCn8Yd4vw5U-gIcg</yt:channelId>
  <title>Stanford Seminar - Toward Better Human-AI Group Decisions</title>
  <link rel="alternate" href="https://www.youtube.com/watch?v=r1E5OEu0uYM"/>
  <author>
   <name>Stanford Online</name>
   <uri>https://www.youtube.com/channel/UCBa5G_ESCn8Yd4vw5U-gIcg</uri>
  </author>
  <published>2022-10-27T20:15:02+00:00</published>
  <updated>2022-11-02T18:45:04+00:00</updated>
  <media:group>
   <media:title>Stanford Seminar - Toward Better Human-AI Group Decisions</media:title>
   <media:content url="https://www.youtube.com/v/r1E5OEu0uYM?version=3" type="application/x-shockwave-flash" width="640" height="390"/>
   <media:thumbnail url="https://i3.ytimg.com/vi/r1E5OEu0uYM/hqdefault.jpg" width="480" height="360"/>
   <media:description>October 14, 2022
Alex 'Sandy' Pentland of MIT Media Lab

There is an immense literature about human group decision making and computer tools to help, but much of it is limited or even contradictory. In contrast, there are powerful mathematical results concerning combining evidence to achieve optimal, minimum regret decision making. The minimum-regret mathematical framework can be extended to closely approximate human group decision making, and so may provide a good framework for better human-AI group decision making.


About the speaker: 
Professor Alex 'Sandy' Pentland directs MIT Connection Science, an MIT-wide initiative, and previously helped create and direct the MIT Media Lab and the Media Lab Asia in India. A member of the U.S. National Academy of Engineering, advisor to the UN Secretary Generals' office and the OECD, and serial entrepreneur, with an h-index of 147. He has received numerous awards and prizes such as the McKinsey Award from Harvard Business Review, the 40th Anniversary of the Internet from DARPA, and the Brandeis Award for work in privacy.</media:description>
   <media:community>
    <media:starRating count="21" average="5.00" min="1" max="5"/>
    <media:statistics views="1032"/>
   </media:community>
  </media:group>
 </entry>
 <entry>
  <id>yt:video:i-wi7kwBzHU</id>
  <yt:videoId>i-wi7kwBzHU</yt:videoId>
  <yt:channelId>UCBa5G_ESCn8Yd4vw5U-gIcg</yt:channelId>
  <title>Stanford CS330: Deep Multi-task &amp; Meta Learning I 2021 I Lecture 16</title>
  <link rel="alternate" href="https://www.youtube.com/watch?v=i-wi7kwBzHU"/>
  <author>
   <name>Stanford Online</name>
   <uri>https://www.youtube.com/channel/UCBa5G_ESCn8Yd4vw5U-gIcg</uri>
  </author>
  <published>2022-10-27T16:45:02+00:00</published>
  <updated>2022-10-27T16:45:02+00:00</updated>
  <media:group>
   <media:title>Stanford CS330: Deep Multi-task &amp; Meta Learning I 2021 I Lecture 16</media:title>
   <media:content url="https://www.youtube.com/v/i-wi7kwBzHU?version=3" type="application/x-shockwave-flash" width="640" height="390"/>
   <media:thumbnail url="https://i2.ytimg.com/vi/i-wi7kwBzHU/hqdefault.jpg" width="480" height="360"/>
   <media:description>For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai

To follow along with the course, visit: 
http://cs330.stanford.edu/fall2021/index.html

To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​

Chelsea Finn
Computer Science, PhD

Karol Hausman
Computer Science, PhD</media:description>
   <media:community>
    <media:starRating count="21" average="5.00" min="1" max="5"/>
    <media:statistics views="917"/>
   </media:community>
  </media:group>
 </entry>
 <entry>
  <id>yt:video:6b5M0kx_MKg</id>
  <yt:videoId>6b5M0kx_MKg</yt:videoId>
  <yt:channelId>UCBa5G_ESCn8Yd4vw5U-gIcg</yt:channelId>
  <title>Stanford Seminar - Multi-Sensory Neural Objects: Modeling, Inference, and Applications in Robotics</title>
  <link rel="alternate" href="https://www.youtube.com/watch?v=6b5M0kx_MKg"/>
  <author>
   <name>Stanford Online</name>
   <uri>https://www.youtube.com/channel/UCBa5G_ESCn8Yd4vw5U-gIcg</uri>
  </author>
  <published>2022-10-26T19:45:00+00:00</published>
  <updated>2022-10-27T15:37:51+00:00</updated>
  <media:group>
   <media:title>Stanford Seminar - Multi-Sensory Neural Objects: Modeling, Inference, and Applications in Robotics</media:title>
   <media:content url="https://www.youtube.com/v/6b5M0kx_MKg?version=3" type="application/x-shockwave-flash" width="640" height="390"/>
   <media:thumbnail url="https://i3.ytimg.com/vi/6b5M0kx_MKg/hqdefault.jpg" width="480" height="360"/>
   <media:description>October 14, 2022
Jiajun Wu of Stanford University

In the past two years, neural representations for objects and scenes have demonstrated impressive performance on graphics and vision tasks, particularly on novel view synthesis, and have gradually gained attention from the robotics community due to their potential robotic applications. In this talk, I'll present our recent efforts in building neural representations that are object-centric and multi-sensory---two properties that are essential for flexible, efficient, and generalizable robot manipulation. I'll focus on four aspects: technical innovations in building such representations, advances in scaling them up in the form of a multi-sensory neural object dataset, methods for inferring category-agnostic neural object representations and their parameters (SysID) from unlabeled visual data, and systems that adopt these representations for robotic manipulation.

About the speaker: 
I am an Assistant Professor of Computer Science at Stanford University, affiliated with the Stanford Vision and Learning Lab (SVL) and the Stanford AI Lab (SAIL). I study machine perception, reasoning, and interaction with the physical world, drawing inspiration from human cognition. Here is some information for prospective students and visitors.

Before joining Stanford, I was a Visiting Faculty Researcher at Google Research, New York City, working with Noah Snavely. I finished my PhD at MIT, advised by Bill Freeman and Josh Tenenbaum, and my undergraduate degrees at Tsinghua University, working with Zhuowen Tu.

https://jiajunwu.com/

#robotics</media:description>
   <media:community>
    <media:starRating count="42" average="5.00" min="1" max="5"/>
    <media:statistics views="2041"/>
   </media:community>
  </media:group>
 </entry>
</feed>
