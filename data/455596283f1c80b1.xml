<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
     xmlns:content="http://purl.org/rss/1.0/modules/content/"
     xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
     xmlns:atom="http://www.w3.org/2005/Atom"
     xmlns:dc="http://purl.org/dc/elements/1.1/"
     xmlns:wfw="http://wellformedweb.org/CommentAPI/"
     >
  <channel>
    <title>brain of mat kelcey...</title>
    <link>http://matpalm.com/blog</link>
    <description>thoughts from a data scientist wannabe</description>
    <generator>Blogofile</generator>
    <sy:updatePeriod>hourly</sy:updatePeriod>
    <sy:updateFrequency>1</sy:updateFrequency>
    <item>
      <title>high performance ML with JAX</title>
      <link>http://matpalm.com/blog/pycon_jax_talk</link>
      <category><![CDATA[jax]]></category>
      <category><![CDATA[talk]]></category>
      <guid>http://matpalm.com/blog/pycon_jax_talk</guid>
      <description>high performance ML with JAX</description>
      <content:encoded><![CDATA[<p>last friday i did a talk at <a href="https://2021.pycon.org.au/">pycon</a> on jax
</p>
<p>here's a recording; check it out!
</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/cqbBjM4_yGw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<p>and here's <a href="https://drive.google.com/file/d/1_5nYIvXm_TWGYwmCwcDQtn1YI62pxz3P/view?usp=sharing">a pdf of the slides</a>
</p>]]></content:encoded>
    </item>
    <item>
      <title>evolved channel selection</title>
      <link>http://matpalm.com/blog/evolved_channel_selection</link>
      <category><![CDATA[projects]]></category>
      <category><![CDATA[ga]]></category>
      <category><![CDATA[jax]]></category>
      <guid>http://matpalm.com/blog/evolved_channel_selection</guid>
      <description>evolved channel selection</description>
      <content:encoded><![CDATA[<h1>multi spectral channel data</h1>
<p><a href="https://github.com/phelber/eurosat">eurosat/all</a> is a dataset
   of 27,000 64x64 satellite images taken with 13 spectral bands.
   each image is labelled one of ten classes.
</p>
<p>for the purpose of classification these 13 aren't equally useful, and the
   information in them varies across resolutions. if we were designing a sensor
   we might choose to use different channels in different resolutions.
</p>
<p>how can we explore the trade off between mixed resolutions and whether to use
   a channel at all?
</p>

<h1>a simple baseline model</h1>
<p>let's start with a simple baseline to see what performance we get. we won't
   spend too much time on this model, we just want something that we can
   iterate on quickly.
</p>
<p>the simple model shown below trained on a 'training' split with adam hits 0.942
   top 1 accuracy on a 2nd 'validation' split in 5 epochs. that'll do for a start.
</p>
<img src="/blog/imgs/2021/ecs/single.svg.png"/>


<h1>what is the benefit of each channel?</h1>
<p>let's check the effect of including different combos of input channels. we'll do so by
   introducing a channel mask.
</p>
<p>a mask of all ones denotes using all channels and gives our baseline performance
</p>
<table class='data'>
<tr><td>mask</td><td>validation accuracy</td></tr>
<tr><td>[1,1,1,1,1,1,1,1,1,1,1,1,1]</td><td>0.942</td></tr>
</table>

<p>a mask of all zeros denotes using <em>no</em> channels and acts as a sanity check; it
   gives the performance of random chance which is in line with what we expect
   give the balanced training set. (note: we standardise the input data so that
   it has zero mean per channel (with the mean, standard deviation parameters
   fit against training data only) so we can get this effect)
</p>
<table class='data'>
<tr><td>mask</td><td>validation accuracy</td></tr>
<tr><td>[1,1,1,1,1,1,1,1,1,1,1,1,1]</td><td>0.942</td></tr>
<tr><td>[0,0,0,0,0,0,0,0,0,0,0,0,0]</td><td>0.113</td></tr>
</table>

<p>but what about if we drop just one channel? i.e. a mask of all ones except for a single zero.
</p>
<table class='data'>
<tr><td>channel to drop</td><td>validation accuracy</td></tr>
<tr><td>0</td><td>0.735</td></tr>
<tr><td>1</td><td>0.528</td></tr>
<tr><td>2</td><td>0.661</td></tr>
<tr><td>3</td><td>0.675</td></tr>
<tr><td>4</td><td>0.809</td></tr>
<tr><td>5</td><td>0.724</td></tr>
<tr><td>6</td><td>0.749</td></tr>
<tr><td>7</td><td>0.634</td></tr>
<tr><td>8</td><td>0.874</td></tr>
<tr><td>9</td><td>0.934</td></tr>
<tr><td>10</td><td>0.593</td></tr>
<tr><td>11</td><td>0.339</td></tr>
<tr><td>12</td><td>0.896</td></tr>
</table>

<p>from this we can see that the performance hit we get from losing a single channel
   is not always the same. in particular consider channel 11; if we drop that channel we
   get a huge hit! does that mean that if we keep <em>only</em> 11 that should give reasonable
   performance?
</p>
<table class='data'>
<tr><td>mask</td><td>validation accuracy</td></tr>
<tr><td>[1,1,1,1,1,1,1,1,1,1,1,1,1]</td><td>0.942</td></tr>
<tr><td>[0,0,0,0,0,0,0,0,0,0,0,0,0]</td><td>0.113</td></tr>
<tr><td>[0,0,0,0,0,0,0,0,0,0,0,1,0] (keep just 11)</td><td>0.260</td></tr>
</table>

<p>bbbzzzttt (or other appropriate annoying buzzer noise). channel 11
   is contributing to the classification but it's not being used independently.
   in general this is exactly the behaviour we want from a neural network
   but what should we do to explore the effect of not having this dependence?
</p>

<h1>dropping out channels</h1>
<p>consider using a dropout idea, just with input channels instead of intermediate nodes.
</p>
<p>what behaviour do we get if we drop channels out during
   training? i.e. with 50% probability we replace an entire input channel with 0s?
</p>
<p>things take longer to train and we get a slight hit in accuracy...
</p>
<table class='data'>
<tr><td>dropout?</td><td>validation accuracy</td></tr>
<tr><td>no</td><td>0.942</td></tr>
<tr><td>yes</td><td>0.934</td></tr>
</table>

<p>...but now when we mask out one channel at a time we don't get a big hit for losing any
   particular one.
</p>
<table class='data'>
<tr><td>channel to drop</td><td colspan=2>validation accuracy</td></tr>
<tr><td></td><td>no dropout</td><td>with dropout</td></tr>
<tr><td>0</td><td>0.735</td><td>0.931</td></tr>
<tr><td>1</td><td>0.528</td><td>0.931</td></tr>
<tr><td>2</td><td>0.661</td><td>0.936</td></tr>
<tr><td>3</td><td>0.675</td><td>0.935</td></tr>
<tr><td>4</td><td>0.809</td><td>0.937</td></tr>
<tr><td>5</td><td>0.724</td><td>0.934</td></tr>
<tr><td>6</td><td>0.749</td><td>0.931</td></tr>
<tr><td>7</td><td>0.634</td><td>0.927</td></tr>
<tr><td>8</td><td>0.874</td><td>0.927</td></tr>
<tr><td>9</td><td>0.934</td><td>0.927</td></tr>
<tr><td>10</td><td>0.593</td><td>0.927</td></tr>
<tr><td>11</td><td>0.339</td><td>0.933</td></tr>
<tr><td>12</td><td>0.896</td><td>0.937</td></tr>
</table>


<h1>evolving the channel selection</h1>
<p>now that we have a model that is robust to any combo of channels what do we see
   if we use a simple genetic algorithm (GA) to evolve the channel mask to use
   with this pre trained network?
   a mask that represents using all channels will be the best right? right?
</p>
<p>we'll evolve the GA using the network trained above but based on it's performance
   on a 3rd "ga_train" split using the inverse loss as a fitness function.
</p>
<p>amusingly the GA finds that a mask of <code>[1,1,0,1,0,0,0,1,1,0,1,0,1]</code>
   does better marginally better than all channels, but only uses 1/2 of them!
</p>
<table class='data'>
<tr><td>mask</td><td>split</td><td>accuracy</td></tr>
<tr><td>[1,1,1,1,1,1,1,1,1,1,1,1,1] (all)</td><td>ga_validate</td><td>0.934</td></tr>
<tr><td>[1,1,0,1,0,0,0,1,1,0,1,0,1] (ga)</td><td>ga_validate</td><td>0.936</td></tr>
</table>

<p>important note: we can imagine the best performance overall would be to have the GA
   evolve not the channels to use from this model, but the channels to use when training <em>from
scratch</em>. this would though require a lot more model training, basically a full training
   cycle per fitness evaluation :( in the approach we describe here
   we only have to train a single model and then have the GA just run inference.
</p>

<h1>what about different resolutions?</h1>
<p>taking the idea of channel selection a step further, what if we got the GA to not
   only decide whether to use a channel or not, but also <em>what resolution</em> it should be
   in?
</p>
<p>consider some example images across resolutions....
</p>
<table class='data'>
<tr><td colspan=4>example images (just RGB channels shown)</td></tr>
<tr><td>orig x64</td><td>x32</td><td>x16</td><td>x8</td>
<tr>
<td><img src="/blog/imgs/2021/ecs/i05_x64.png" width='128'/></td>
<td><img src="/blog/imgs/2021/ecs/i05_x32.png" width='128'/></td>
<td><img src="/blog/imgs/2021/ecs/i05_x16.png" width='128'/></td>
<td><img src="/blog/imgs/2021/ecs/i05_x08.png" width='128'/></td>
</tr>
<tr>
<td><img src="/blog/imgs/2021/ecs/i06_x64.png" width='128'/></td>
<td><img src="/blog/imgs/2021/ecs/i06_x32.png" width='128'/></td>
<td><img src="/blog/imgs/2021/ecs/i06_x16.png" width='128'/></td>
<td><img src="/blog/imgs/2021/ecs/i06_x08.png" width='128'/></td>
</tr>
<tr>
<td><img src="/blog/imgs/2021/ecs/i08_x64.png" width='128'/></td>
<td><img src="/blog/imgs/2021/ecs/i08_x32.png" width='128'/></td>
<td><img src="/blog/imgs/2021/ecs/i08_x16.png" width='128'/></td>
<td><img src="/blog/imgs/2021/ecs/i08_x08.png" width='128'/></td>
</tr>
</table>

<p>we could then weight the use of a channel based on resolution; the higher the resolution
   the more the channel "costs" to use, with not using the channel at all being "free".
</p>
<p>to support this we can change the GA to represent members not as a string of {0, 1}s
   but instead a sequence of {0, x8, x16, x32, x64} values per channel where these represent...
</p>
<table class='data'>
<tr><td><b>resolution</b></td><td><b>description</b></td><td><b>channel cost<b/></td></tr>
<tr><td>x64</td><td>use original (64, 64) version of input</td><td>0.8</td></tr>
<tr><td>x32</td><td>use a 1/2 res (32, 32) version of input</td><td>0.4</td></tr>
<tr><td>x16</td><td>use a 1/4 res (16, 16) version of input</td><td>0.2</td></tr>
<tr><td>x8</td><td>use a 1/8 res (8, 8) version of input</td><td>0.1</td></tr>
<tr><td>0</td><td>don't use channel</td><td>0</td></tr>
</table>

<p>the change in the encoding of our GA is trivial, just 5 values per channel instead of 2,
   but before we look at that; how do we change our network?
</p>
<p>we can do it without having to add too many extra parameters by using the magic of fully
   convolutional networks :)
</p>
<p>notice how the main trunk of our first network was a series of 2d convolutions
   with a global spatial mean. this network will simply take as input all the
   resolutions we need! we can simply reuse it multiple times!
</p>
<p>so we can have our network...
</p>
<ol>
 <li>
     take the original x64 input
 </li>

 <li>
     downsample it multiple times to x32, x16 and x8
 </li>

 <li>
     mask out the channels so that each channel is only represented in one of the resolutions (or not
     represented at all if we want to ignore that channel)
 </li>

 <li>
     run the main trunk network with shared parameters on each of the masked resolutions
 </li>

 <li>
     combine the outputs with a simple channel concatenation
 </li>

 <li>
     do one more non linear mixing (because, why not..)
 </li>

 <li>
     finish with the logits
 </li>
</ol>
<img src="/blog/imgs/2021/ecs/multi_res.svg.png"/>

<p>note: try as i might i can't get steps 2 to 4 to run parallelised in a pmap.
   <a href="https://github.com/google/jax/discussions/5895">asked on github about it</a>
   and looks to be something you can't do at the moment.
</p>

<h1>the channel cost vs loss pareto front</h1>
<p>when we consider channel cost vs loss there is no single best solution, it's a classic
   example of a
   <a href="https://en.wikipedia.org/wiki/Pareto_efficiency">pareto front</a>
   where we see a tradeoff between the channel_cost and loss.
</p>
<p>consider this sampling of 1,000 random channel masks...
</p>
<img src="/blog/imgs/2021/ecs/pareto_front.just_random.png"/>


<h1>rerunning the GA</h1>
<p>the GA needs to operate with a fitness that's a single scalar; for now we just use
   a simple combo of <code>(1.0 / loss) - channel_cost</code>
</p>
<p>running with this fitness function we evolve the solution
   <code>[x16, x64, x64, x16, x32, ignore, x8, x64, x8, ignore, x8, ignore, x32]</code>
</p>
<p>it's on the pareto front, as we'd hope, and it's interesting that it includes
   a mix of resolutions including ignoring 3 channels completely :)
</p>
<img src="/blog/imgs/2021/ecs/pareto_front.with_ga.png"/>

<p>different mixings of loss and channel_cost would result in different GA solutions along the front
</p>

<h1>code</h1>
<p>all the code is <a href="https://github.com/matpalm/evolved_channel_selection">on github</a>
</p>]]></content:encoded>
    </item>
    <item>
      <title>crazy large batch sizes</title>
      <link>http://matpalm.com/blog/crazy_large_batch_sizes</link>
      <category><![CDATA[quick_hack]]></category>
      <category><![CDATA[tpu]]></category>
      <category><![CDATA[jax]]></category>
      <guid>http://matpalm.com/blog/crazy_large_batch_sizes</guid>
      <description>crazy large batch sizes</description>
      <content:encoded><![CDATA[<h1>"use a bigger batch"</h1>
<p>a classic piece of advice i hear for people using tpus is that
   they should "try using a bigger batch size".
</p>
<p>this got me thinking;
   i wonder how big a batch size i could reasonably use?
   how would the optimisation go?
   how fast could i get things?
</p>

<h1>dataset</h1>
<p>let's train a model on the
   <a href="https://github.com/phelber/eurosat">eurosat/rgb dataset</a>.
   it's a 10 way classification problem on 64x64 images
</p>
<img src="/blog/imgs/2020/en/sample_images.png"/>

<p>with a training split of 80% we have 21,600 training examples.
   we'll use another 10% for validation (2,700 images)
   and just not use the final 10% test split #hack
</p>

<h1>model</h1>
<p>for the
   <a href="https://github.com/matpalm/large_batch/blob/master/model.py">model</a>
   we'll use a simple stack of convolutions with
   channel sizes 32, 64, 128 and 256, a stride of 2 for spatial reduction
   all with gelu activation. after the convolutions we'll do a simple global
   spatial pooling, a single 128d dense layer with gelu and then a
   10d logit output. a pretty vanilla architecture of ~400K params. nothing fancy.
</p>

<h1>splitting up the data</h1>
<p>a v3-32 tpu pod slice is 4 hosts, each with 8 tpu devices.
</p>
<p>21,600 training examples total =&gt; 5,400 examples per host =&gt; 675 examples per device.
</p>
<p>this number of images easily fits on a device. great.
</p>

<h2>augmentation</h2>
<p>now usually augmentation is something we do randomly per batch, but for this hack
   we're interested in seeing how big a batch we can run. so why not
   fill out the dataset a bit by just running a stack of augmentations before training?
</p>
<p>for each image we'll do 90, 180 and 270 deg rotations along with left/right flips
   for a total of 8 augmented images for each original image. e.g.....
</p>
<img src="/blog/imgs/2021/lb/augmentations.png"/>

<p>this gives us now 172,800 images total =&gt; 43,200 per host =&gt; 5,400 per tpu device.
   which stills fits no problem.
</p>
<p>side note: turns out doing this augmentation was one of the most fun parts of
   this hack :)
   see <a href="https://twitter.com/mat_kelcey/status/1360555768630501377">this tweet thread</a>
   for some more info on how i used nested pmaps and vmaps to do it!
</p>

<h2>optimisers?</h2>
<p>one motivation i had for this hack was to compare adam to lamb. i'd
   seen lamb referred to in the past, would it perform better for this model/dataset size?
   turns out it does! a simple sweep comparing lamb, adam and sgd shows lamb consistently
   doing the best. definitely one to add to the tuning mix from now on.
</p>

<h2>data / model / optimiser state placement</h2>
<p>not only does the augmented data fit sharded across devices but we can replicate
   both the model parameters and the optimiser state as well. this is important
   for speed since the main training loop doesn't have to do any host/device
   communication. taking a data parallel approach means the only cross device
   comms is a gradient psum.
</p>

<h1>results</h1>
<p>for training we run an inner loop just pumping the <code>param = update(params)</code> step.
</p>
<p>an outer loop runs the inner loop 100 times before doing a validation accuracy check.
</p>
<p>the inner loop runs at 1.5s for the 100 iterations and since each iteration is a
   forward &amp; backwards pass for all 172,800 images across all hosts that's 11M images
   processed per second. ðŸ”¥ðŸ”¥ðŸ”¥
</p>
<p>at this speed the best result of 0.95 on validation takes 13 outer loops;
   i.e. all done in under 20s. o_O !!
</p>
<p>when reviewing runs i did laugh to see sgd with momentum make a top 10 entry.
</p>
<p><em>new t-shirt slogan: "sgd with momentum; always worth a try"</em>
</p>
<img src="/blog/imgs/2021/lb/top_10.png"/>


<h1>code</h1>
<p>all the code in hacktastic undocumented form
   <a href="https://github.com/matpalm/large_batch">on github</a>
</p>]]></content:encoded>
    </item>
    <item>
      <title>solving y=mx+b... with jax on a tpu pod slice</title>
      <link>http://matpalm.com/blog/ymxb_pod_slice</link>
      <category><![CDATA[tpu]]></category>
      <category><![CDATA[ensemble_nets]]></category>
      <category><![CDATA[jax]]></category>
      <category><![CDATA[projects]]></category>
      <category><![CDATA[haiku]]></category>
      <guid>http://matpalm.com/blog/ymxb_pod_slice</guid>
      <description>solving y=mx+b... with jax on a tpu pod slice</description>
      <content:encoded><![CDATA[<h1>from jax fundamentals to running on a tpu pod slice</h1>
<p>this 4 (and a bit) part tute series starts with
   <a href="https://jax.readthedocs.io/en/latest/">jax</a>
   fundamentals, builds up to describing a data parallel approach to training on a
   <a href="https://cloud.google.com/tpu">cloud tpu pod slice</a>, and
   finishes with a tpu pod slice implementation of
   <a href="http://matpalm.com/blog/ensemble_nets">ensemble nets</a>....
   all with the goal of solving 1d <code>y=mx+b</code>
</p>
<p>and though it may seem like a bit of overkill it turns out it's a good example
   to work through so that we can focus on the library support without having
   to worry about the modelling.
</p>

<h2>part 1: some jax basics</h2>
<p>in this first section we introduce some jax fundamentals;
   e.g. make_jaxpr, grad, jit, vmap &amp; pmap.
</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/W1vfBDFLm7Q" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<p>colab: <a href="https://colab.research.google.com/drive/1vmEckPE6o9pDJF1tctPW5yyuAIVZ817B">01 pmap jit vmap oh my.ipynb</a>
</p>

<h2>part 2: solving y=mx+b</h2>
<p>in part 2 we use the techniques from part 1 to solve <code>y=mx+b</code> in pure jax. we'll also
   introduce
   <a href="https://jax.readthedocs.io/en/latest/pytrees.html">pytrees</a>
   and various
   <a href="https://jax.readthedocs.io/en/latest/jax.tree_util.html">tree_utils</a>
   for manipulating them.
</p>
<p>we run first on a single device and work up to using
   <a href="https://jax.readthedocs.io/en/latest/jax.html#jax.pmap">pmap</a> to demonstrate a
   simple data parallelism approach. along the way we'll do a small detour to a tpu pod slice
   to illustrate the difference in a multi host setup.
</p>
<p>( note: the experience as described here for a pod slice isn't publically available yet; but sign up via the
   <a href="http://goo.gle/jax-tpu-signup">JAX on Cloud TPU Interest Form</a> to get more info. see also this
   <a href="https://docs.google.com/presentation/d/1eBfNKT3D3lEWtcn4mkgvvitKZTU7HSn4f3fGHqYgpeA/">JAX on Cloud TPUs (NeurIPS 2020)</a> talk )
</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/XXsSZlHzHcw" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<p>colab: <a href="https://colab.research.google.com/drive/1qkjyNtmPzQIYHAY7l1YqvMgTOfbQAfgA?usp=sharing">02 y mx b on a tpu.ipynb</a>
</p>

<h2>part 3: introducing haiku and optax</h2>
<p>next we introduce <a href="https://github.com/deepmind/dm-haiku">haiku</a> as a way
   of defining our model and <a href="https://github.com/deepmind/dm-haiku">optax</a> as a
   library to provide standard optimisers. to illustrate there use we'll do a minimal
   port of our model and training loop to use them.
</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/8-N1-7lPWOs" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<p>colab: <a href="https://colab.research.google.com/drive/1_MDXCnwmLTPPm4qJnpHYj-X0E-a-ev6H?usp=sharing">03 y mx b in haiku.ipynb</a>
</p>

<h2>part 4: ensemble nets</h2>
<p>in part 4 we'll reimplement <a href="http://matpalm.com/blog/ensemble_nets">ensemble nets</a>
   for this trivial model, continuing to do things in a way that supports
   running on a tpu pod slice.
</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/_-ftTbABKuk" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<p>colab: <a href="https://colab.research.google.com/drive/1uWZQQfp5T4nKRdg_kgV93tIzkA316f2O?usp=sharing">04 y mx b haiku ensemble.ipynb</a>
</p>

<h2>part 5: some sanity</h2>
<p>to wrap up we acknowledge that though tpu pod slices and data parallel
   approaches <em>are</em> fun we could have just solved this in a single
   calculation using the normal equation... :D
</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/5hKse-PUo0k" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<p>colab: <a href="https://colab.research.google.com/drive/1sX5plJfIesT-5IUU-d9U-mimXtWOXEE3?usp=sharing">05 booooooooooooooooring.ipynb</a>
</p>

<h1>what a way to solve <code>y=mx+b</code> !!!</h1>
<img src="/blog/imgs/2021/ymxb/tenor.gif"/>]]></content:encoded>
    </item>
    <item>
      <title>develomentor.com podcast interview</title>
      <link>http://matpalm.com/blog/develomentor_podcast</link>
      <category><![CDATA[talk]]></category>
      <guid>http://matpalm.com/blog/develomentor_podcast</guid>
      <description>develomentor.com podcast interview</description>
      <content:encoded><![CDATA[<p>was <a href="https://develomentor.com/2020/12/07/matthew-kelcey-former-research-engineer-at-google-brain-114/">a guest on the develomentor podcast</a> talking about random parts of my career. i always enjoy chatting to grant, hope you get to have a listen!
</p>]]></content:encoded>
    </item>
    <item>
      <title>out of distribution detection using focal loss</title>
      <link>http://matpalm.com/blog/ood_using_focal_loss</link>
      <category><![CDATA[objax]]></category>
      <category><![CDATA[jax]]></category>
      <category><![CDATA[projects]]></category>
      <guid>http://matpalm.com/blog/ood_using_focal_loss</guid>
      <description>out of distribution detection using focal loss</description>
      <content:encoded><![CDATA[<h1>out of distribution (OOD) detection</h1>
<p>OOD detection is an often overlooked part of many projects.
   it's super important to understand when your model is operating on
   data it's not familiar with!
</p>
<p>though this can be framed as the problem of detecting that an input
   differs from training data, discussed more in this
   <a href="http://matpalm.com/blog/de_concerns_for_ml/">'data engineering concerns for machine learning products' talk</a>,
   for this post we're going to look at the
   problem more from the angle of the model needing to be able to express it's
   own lack of confidence.
</p>
<p>a core theme of this post is treating OOD as a function of
   the output of the model. when i've played with this idea before the
   best result i've had is from using entropy as a proxy of confidence.
   in the past it's not be in the context of OOD directly but instead
   as a way of prioritising data annotation (i.e. uncertainty sampling for active learning)
</p>
<p>using entropy as a stable measure of confidence though requires a model be
   well calibrated.
</p>

<h1>neural net calibration</h1>
<p>neural nets are sadly notorious for not being well calibrated under the default ways
   we train these days.
</p>
<p>the first approach i was shown to calibrate a neural net is to train your model
   as normal, then finetune <em>just</em> the final classifier layer using a held out set;
   the general idea being that this is effectively just a logistic regression on
   features learnt by the
   rest of the network and so should come with the better guarantees that logistic
   regression provides us regarding calibration
   (see this <a href="https://scikit-learn.org/stable/modules/calibration.html">great overview</a>
   from sklearn)
   whereas this has always worked well for me it does require two steps to training,
   as well as the need for a seperate held out set to be managed.
</p>
<p>another apporach to calibration i revisited this week comes from
   this great overview paper from 2017
   <a href="https://arxiv.org/abs/1706.04599">on calibration of modern neural networks</a>.
   it touches on platt scaling, which i've also used in the past successfully, but includes
   something even simpler i didn't really notice until it was pointed out to me by a
   colleague; don't bother tuning the entire last layer, just fit a single temperature
   rescaling of the output logits i.e. what can be thought of as a single parameter
   version of platt scaling. what a great simple idea!
</p>
<p>the main purpose of this post though is to reproduce some ideas around out of distribution
   and calibration when you use focal loss. this was described in this great piece of work
   <a href="https://torrvision.github.io/focal_calibration/">calibrating deep neural networks using focal loss</a>. we'll implement that, but the other two as well for comparison.
</p>
<p>( as an aside, another great simple way of determining confidence is using an ensemble!
   i explore this idea a bit in my post on <a href="http://matpalm.com/blog/ensemble_nets/">ensemble nets</a>
   where i train ensembles as a single model using jax vmap )
</p>

<h1>experiments</h1>

<h2>dataset</h2>
<p>let's start with
   <a href="https://www.cs.toronto.edu/~kriz/cifar.html">cifar10</a>
   and split the data up a bit differently than the standard splits...
</p>
<ol>
 <li>
     we'll union the standard cifar10 <code>train</code> and <code>test</code> splits.
 </li>

 <li>
     hold out instances labelled <code>automobile</code> &amp; <code>cat</code> as a "hard" <code>ood</code> set ( i've intentionally chosen these two since i know they cause model confusion as observed in this <a href="https://keras.io/examples/vision/metric_learning/">keras.io tute on metric learning</a> ).
 </li>

 <li>
     split the remaining 8 classes into 0.7 <code>train</code> and 0.1 for each of <code>validation</code>, <code>calibration</code> and <code>test</code>.
 </li>
</ol>
<p>we'll additionally generate an "easy" <code>ood</code> set which will just be random images.
</p>
<p>in terms of how we'll use these splits...
</p>
<ul>
 <li>
     <code>train</code> will be the main dataset to train against.
 </li>

 <li>
     <code>validation</code> will be used during training for learning rate stepping, early stopping, etc.
 </li>

 <li>
     <code>calibration</code> will be used when we want to tune a model in some way for calibration.
 </li>

 <li>
     <code>test</code> will be used for final held out analysis.
 </li>

 <li>
     <code>ood_hard</code> and <code>ood_easy</code> will be used as the representative out of distribution sets.
 </li>
</ul>
<table class='data'>
<tr><td colspan="2">examples images</td></tr>
<tr><td>in distribution</td>
<td><img src="/blog/imgs/2020/ood/eg.in_distribution.png" /></td></tr>
<tr><td>out of distribution (hard)</td>
<td><img src="/blog/imgs/2020/ood/eg.ood_hard.png" /></td></tr>
<tr><td>out of distribution (easy)</td>
<td><img src="/blog/imgs/2020/ood/eg.ood_easy.png" /></td></tr>
</table>


<h2>method overview</h2>
<ol>
 <li>
     train a base model, <code>model_1</code>, on <code>train</code>, tuning against <code>validate</code>.
 </li>

 <li>
     create <code>model_2</code> by finetuning the last layer of <code>model_1</code> against <code>calibrate</code>; i.e. the logisitic regression approach to calibration.
 </li>

 <li>
     create <code>model_3</code> by just fitting a scalar temperture of last layer of <code>model_1</code>; i.e. the temperture scaling approach.
 </li>

 <li>
     train some focal loss models from scratch against <code>train</code> using <code>validate</code> for tuning.
 </li>
</ol>
<p>at each step we'll check the entropy distributions across the various splits, including the easy and hard
   ood sets
</p>

<h1>model_1: baseline</h1>
<p>for a baseline we'll use a
   <a href="https://objax.readthedocs.io/en/latest/objax/zoo.html#objax.zoo.resnet_v2.ResNet18">resnet18 objax model</a>
   trained against <code>train</code> using <code>validate</code> for simple early stopping.
</p>
<pre class="prettyprint">
# define model
model = objax.zoo.resnet_v2.ResNet18(in_channels=3, num_classes=10)
# train against all model vars
trainable_vars = model.vars()
</pre>

<pre class="prettyprint">
$ python3 <a href="https://github.com/matpalm/ood_focal_loss/blob/master/train_basic_model.py">train_basic_model.py</a> --loss-fn cross_entropy --model-dir m1

learning_rate 0.001 validation accuracy 0.372 entropy min/mean/max 0.0005 0.8479 1.9910
learning_rate 0.001 validation accuracy 0.449 entropy min/mean/max 0.0000 0.4402 1.9456
learning_rate 0.001 validation accuracy 0.517 entropy min/mean/max 0.0000 0.3566 1.8013
learning_rate 0.001 validation accuracy 0.578 entropy min/mean/max 0.0000 0.3070 1.7604
learning_rate 0.001 validation accuracy 0.680 entropy min/mean/max 0.0000 0.3666 1.7490
learning_rate 0.001 validation accuracy 0.705 entropy min/mean/max 0.0000 0.3487 1.6987
learning_rate 0.001 validation accuracy 0.671 entropy min/mean/max 0.0000 0.3784 1.8276
learning_rate 0.0001 validation accuracy 0.805 entropy min/mean/max 0.0000 0.3252 1.8990
learning_rate 0.0001 validation accuracy 0.818 entropy min/mean/max 0.0000 0.3224 1.7743
learning_rate 0.0001 validation accuracy 0.807 entropy min/mean/max 0.0000 0.3037 1.8580
</pre>

<p>we can check the distribution of entropy values of this trained model against our various
   splits
</p>
<img src="/blog/imgs/2020/ood/m1.entropy.png" />

<p>some observations...
</p>
<ul>
 <li>
     highest confidence, i.e. lowest entropy, on training set.
 </li>

 <li>
     equiv values on validation and test; both &lt; training.
 </li>

 <li>
     ood lowest of all; this is a good thing!
 </li>

 <li>
     mean value of entropy on the validation set slowly drops over training.
 </li>
</ul>
<p>recall that a higher entropy =&gt; a more uniform distribution =&gt; less prediction confidence.
</p>
<p>as such our goal is to have the entropy of the <code>ood</code> set as high as possible without
   dropping the <code>test</code> set too much.
</p>

<h1>model_2: fine tuning entire classifier layer</h1>
<p>as mentioned before the main approach i'm familiar with is retraining the classifier
   layer. we'll start with <code>model_1</code> and use the so-far-unseen <code>calibration</code> set for fine tuning it.
</p>
<pre class="prettyprint">
# define model
model = objax.zoo.resnet_v2.ResNet18(in_channels=3, num_classes=10)
# restore from model_1
objax.io.load_var_collection('m1/weights.npz', model.vars())
# train against last layer only
classifier_layer = model[-1]
trainable_vars = classifier_layer.vars()
</pre>

<pre class="prettyprint">
$ python3 <a href="https://github.com/matpalm/ood_focal_loss/blob/master/train_model_2.py">train_model_2.py</a> --input-model-dir m1 --output-model-dir m2

learning_rate 0.001 calibration accuracy 0.808 entropy min/mean/max 0.0000 0.4449 1.8722
learning_rate 0.001 calibration accuracy 0.809 entropy min/mean/max 0.0002 0.5236 1.9026
learning_rate 0.001 calibration accuracy 0.808 entropy min/mean/max 0.0004 0.5468 1.9079
learning_rate 0.001 calibration accuracy 0.811 entropy min/mean/max 0.0005 0.5496 1.9214
learning_rate 0.001 calibration accuracy 0.812 entropy min/mean/max 0.0001 0.5416 1.9020
learning_rate 0.001 calibration accuracy 0.812 entropy min/mean/max 0.0002 0.5413 1.9044
learning_rate 0.001 calibration accuracy 0.811 entropy min/mean/max 0.0003 0.5458 1.8990
learning_rate 0.001 calibration accuracy 0.814 entropy min/mean/max 0.0001 0.5472 1.9002
learning_rate 0.001 calibration accuracy 0.812 entropy min/mean/max 0.0001 0.5455 1.9124
learning_rate 0.001 calibration accuracy 0.813 entropy min/mean/max 0.0001 0.5503 1.9092
learning_rate 0.0001 calibration accuracy 0.813 entropy min/mean/max 0.0001 0.5485 1.9011
learning_rate 0.0001 calibration accuracy 0.814 entropy min/mean/max 0.0001 0.5476 1.9036
learning_rate 0.0001 calibration accuracy 0.813 entropy min/mean/max 0.0001 0.5483 1.9051
learning_rate 0.0001 calibration accuracy 0.813 entropy min/mean/max 0.0000 0.5479 1.9038
learning_rate 0.0001 calibration accuracy 0.813 entropy min/mean/max 0.0000 0.5482 1.9043
learning_rate 0.0001 calibration accuracy 0.813 entropy min/mean/max 0.0000 0.5482 1.9059
learning_rate 0.0001 calibration accuracy 0.813 entropy min/mean/max 0.0000 0.5479 1.9068
learning_rate 0.0001 calibration accuracy 0.813 entropy min/mean/max 0.0000 0.5480 1.9058
learning_rate 0.0001 calibration accuracy 0.812 entropy min/mean/max 0.0000 0.5488 1.9040
learning_rate 0.0001 calibration accuracy 0.814 entropy min/mean/max 0.0000 0.5477 1.9051
</pre>

<p>if we compare (top1) accuracy of <code>model_1</code> vs <code>model_2</code> we see a slight improvement
   in <code>model_2</code>, attributable to the slight extra training i suppose (?)
</p>
<pre class="prettyprint">
$ python3 calculate_metrics.py --model m1/weights.npz

train accuracy      0.970
validate accuracy   0.807
calibrate accuracy  0.799
test accuracy       0.803

$ python3 calculate_metrics.py --model m2/weights.npz

train accuracy      0.980
validate accuracy   0.816
calibrate accuracy  0.814
test accuracy       0.810

</pre>

<p>more importantly though; how do the entropy distributions looks?
</p>
<img src="/blog/imgs/2020/ood/m2.entropy.png" />

<p>some observations...
</p>
<ul>
 <li>
     calibration accuracy and entropy during training shifted a bit, but not a lot.
 </li>

 <li>
     again highest confidence on training.
 </li>

 <li>
     again equiv values on validation and test; both &lt; training.
 </li>

 <li>
     ood difference a bit more distinct now.
 </li>
</ul>

<h1>model_3: fit a scalar temperature value to logits</h1>
<p>in <code>model_3</code> we create a simple single parameter layer that represents rescaling,
   append it to the pretrained resnet and train just that layer.
</p>
<pre class="prettyprint">
class Temperature(objax.module.Module):
    def __init__(self):
        super().__init__()
        self.temperature = objax.variable.TrainVar(jnp.array([1.0]))
    def __call__(self, x):
        return x / self.temperature.value

# define model
model = objax.zoo.resnet_v2.ResNet18(in_channels=3, num_classes=10)
# restore from model_1
objax.io.load_var_collection('m1/weights.npz', model.vars())
# add a temp rescaling layer
temperature_layer = layers.Temperature()
model.append(temperature_layer)
# train against just this layer
trainable_vars = temperature_layer.vars()
</pre>

<pre class="prettyprint">
$ python3 <a href="https://github.com/matpalm/ood_focal_loss/blob/master/train_model_3.py">train_model_3.py</a> --input-model-dir m1 --output-model-dir m3

learning_rate 0.01 temp 1.4730 calibration accuracy 0.799 entropy min/mean/max 0.0006 0.5215 1.9348
learning_rate 0.01 temp 1.5955 calibration accuracy 0.799 entropy min/mean/max 0.0013 0.5828 1.9611
learning_rate 0.01 temp 1.6118 calibration accuracy 0.799 entropy min/mean/max 0.0014 0.5911 1.9643
learning_rate 0.01 temp 1.6162 calibration accuracy 0.799 entropy min/mean/max 0.0014 0.5933 1.9652
learning_rate 0.01 temp 1.6118 calibration accuracy 0.799 entropy min/mean/max 0.0014 0.5911 1.9643
learning_rate 0.01 temp 1.6181 calibration accuracy 0.799 entropy min/mean/max 0.0015 0.5943 1.9655
learning_rate 0.01 temp 1.6126 calibration accuracy 0.799 entropy min/mean/max 0.0014 0.5915 1.9645
learning_rate 0.01 temp 1.6344 calibration accuracy 0.799 entropy min/mean/max 0.0016 0.6025 1.9687
learning_rate 0.01 temp 1.6338 calibration accuracy 0.799 entropy min/mean/max 0.0016 0.6022 1.9686
learning_rate 0.01 temp 1.6018 calibration accuracy 0.799 entropy min/mean/max 0.0013 0.5860 1.9623
learning_rate 0.001 temp 1.6054 calibration accuracy 0.799 entropy min/mean/max 0.0014 0.5878 1.9630
learning_rate 0.001 temp 1.6047 calibration accuracy 0.799 entropy min/mean/max 0.0013 0.5874 1.9629
learning_rate 0.001 temp 1.6102 calibration accuracy 0.799 entropy min/mean/max 0.0014 0.5903 1.9640
learning_rate 0.001 temp 1.6095 calibration accuracy 0.799 entropy min/mean/max 0.0014 0.5899 1.9638
learning_rate 0.001 temp 1.6112 calibration accuracy 0.799 entropy min/mean/max 0.0014 0.5908 1.9642
learning_rate 0.001 temp 1.6145 calibration accuracy 0.799 entropy min/mean/max 0.0014 0.5924 1.9648
learning_rate 0.001 temp 1.6134 calibration accuracy 0.799 entropy min/mean/max 0.0014 0.5919 1.9646
learning_rate 0.001 temp 1.6144 calibration accuracy 0.799 entropy min/mean/max 0.0014 0.5924 1.9648
learning_rate 0.001 temp 1.6105 calibration accuracy 0.799 entropy min/mean/max 0.0014 0.5904 1.9640
learning_rate 0.001 temp 1.6151 calibration accuracy 0.799 entropy min/mean/max 0.0014 0.5927 1.9650

</pre>

<img src="/blog/imgs/2020/ood/m3.entropy.png" />

<p>some observations...
</p>
<ul>
 <li>
     we're fitting a single value, so very fast!
 </li>

 <li>
     as expected we don't see <em>any</em> change in accuracy during training compared to <code>model_1</code>. this is since the temp rescaling never changes the ordering of predictions. ( just this property alone could be enough reason to use this approach in some cases! )
 </li>

 <li>
     slightly better than <code>model_2</code> in terms of raising the entropy of <code>ood</code>!
 </li>
</ul>
<p>not bad for fitting <em>a single</em> parameter :D
</p>

<h1>model_4: use focal loss</h1>
<p>as a final experiment we'll use focal loss for training instead of cross entropy
</p>
<p>note: i rolled my own focal loss function which is always dangerous! it's not
   impossible (i.e. it's likely) i've done something wrong in terms of numerical
   stability; please let me know if you see a dumb bug :)
</p>
<pre class="prettyprint"><code class="language-python">def focal_loss_sparse(logits, y_true, gamma=1.0):
    log_probs = jax.nn.log_softmax(logits, axis=-1)
    log_probs = log_probs[jnp.arange(len(log_probs)), y_true]
    probs = jnp.exp(log_probs)
    elementwise_loss = -1 * ((1 - probs)**gamma) * log_probs
    return elementwise_loss
</code></pre>

<p>note that focal loss includes a gamma. we'll trial 1.0, 2.0 and 3.0 in line with
   the experiments mentioned in
   <a href="https://torrvision.github.io/focal_calibration/">calibrating deep neural networks using focal loss</a>
</p>

<h2>gamma 1.0</h2>
<pre class="prettyprint">
$ python3 train_basic_model.py --loss-fn focal_loss --gamma 1.0 --model-dir m4_1

learning_rate 0.001 validation accuracy 0.314 entropy min/mean/max 0.0193 1.0373 2.0371
learning_rate 0.001 validation accuracy 0.326 entropy min/mean/max 0.0000 0.5101 1.8546
learning_rate 0.001 validation accuracy 0.615 entropy min/mean/max 0.0000 0.6680 1.8593
learning_rate 0.001 validation accuracy 0.604 entropy min/mean/max 0.0000 0.5211 1.8107
learning_rate 0.001 validation accuracy 0.555 entropy min/mean/max 0.0001 0.5127 1.7489
learning_rate 0.0001 validation accuracy 0.758 entropy min/mean/max 0.0028 0.6008 1.8130
learning_rate 0.0001 validation accuracy 0.780 entropy min/mean/max 0.0065 0.6590 1.9305
learning_rate 0.0001 validation accuracy 0.796 entropy min/mean/max 0.0052 0.6390 1.8589
learning_rate 0.0001 validation accuracy 0.790 entropy min/mean/max 0.0029 0.5886 1.9151
</pre>

<img src="/blog/imgs/2020/ood/m4_1.entropy.png" />


<h2>gamma 2.0</h2>
<pre class="prettyprint">
$ python3 train_basic_model.py --loss-fn focal_loss --gamma 2.0 --model-dir m4_2

learning_rate 0.001 validation accuracy 0.136 entropy min/mean/max 0.0001 0.2410 1.7906
learning_rate 0.001 validation accuracy 0.530 entropy min/mean/max 0.0000 0.8216 2.0485
learning_rate 0.001 validation accuracy 0.495 entropy min/mean/max 0.0003 0.7524 1.9086
learning_rate 0.001 validation accuracy 0.507 entropy min/mean/max 0.0000 0.6308 1.8460
learning_rate 0.001 validation accuracy 0.540 entropy min/mean/max 0.0000 0.5899 2.0196
learning_rate 0.001 validation accuracy 0.684 entropy min/mean/max 0.0001 0.6189 1.9029
learning_rate 0.001 validation accuracy 0.716 entropy min/mean/max 0.0000 0.6509 1.8822
learning_rate 0.001 validation accuracy 0.606 entropy min/mean/max 0.0001 0.7096 1.8312
learning_rate 0.0001 validation accuracy 0.810 entropy min/mean/max 0.0002 0.6208 1.9447
learning_rate 0.0001 validation accuracy 0.807 entropy min/mean/max 0.0004 0.6034 1.8420
</pre>

<img src="/blog/imgs/2020/ood/m4_2.entropy.png" />


<h2>gamma 3.0</h2>
<pre class="prettyprint">
$ python3 train_basic_model.py --loss-fn focal_loss --gamma 3.0 --model-dir m4_3

learning_rate 0.001 validation accuracy 0.288 entropy min/mean/max 0.0185 1.1661 1.9976
learning_rate 0.001 validation accuracy 0.319 entropy min/mean/max 0.0170 1.0925 2.1027
learning_rate 0.001 validation accuracy 0.401 entropy min/mean/max 0.0396 0.9795 2.0361
learning_rate 0.001 validation accuracy 0.528 entropy min/mean/max 0.0001 0.8534 1.9115
learning_rate 0.001 validation accuracy 0.528 entropy min/mean/max 0.0002 0.7405 1.8509
learning_rate 0.0001 validation accuracy 0.748 entropy min/mean/max 0.0380 0.9204 1.9697
learning_rate 0.0001 validation accuracy 0.760 entropy min/mean/max 0.0764 0.9807 1.9861
learning_rate 0.0001 validation accuracy 0.772 entropy min/mean/max 0.0906 0.9646 2.0556
learning_rate 0.0001 validation accuracy 0.781 entropy min/mean/max 0.0768 0.8973 1.9264
learning_rate 0.0001 validation accuracy 0.780 entropy min/mean/max 0.0871 0.8212 1.9814
</pre>

<img src="/blog/imgs/2020/ood/m4_3.entropy.png" />

<p>some observations...
</p>
<ul>
 <li>
     can see that immediately the model behaves more like <code>model_2</code> than <code>model_1</code> in terms of calibration.
 </li>

 <li>
     increasing gamma really starts to seperate the <code>ood</code> data, but with a drop in overall <code>validation</code> accuracy. maybe the seperation isn't as much as i'd like, but happy to experiment more given this result.
 </li>
</ul>
<p>interesting!! focal loss is definitely going in my "calibration toolkit" :D
</p>

<h1>doing the <em>actual</em> detection though</h1>
<p>recall the distribution plot of entropies from the <code>model_3</code> experiment
</p>
<img src="/blog/imgs/2020/ood/m3.entropy.png" />

<p>we can eyeball that the entropies values are low for in-distribution sets (train, validate,
   calibrate and test) and high for the two ood sets (easy and hard)
   but how do we <em>actually</em> turn this difference into a in-distribution classifier?
   ( recall this is not a standard supervised learning problem, we only have the in-distribution
   instances to train against and nothing from the ood sets at training time )
</p>
<p>amusingly i spent a bunch of time hacking around with all sorts of density estimation
   techniques but it turned out the simplest baseline i started with was the best :/
</p>
<p>the baseline works on a strong, but valid enough, assumption of the entropies; the entropies
   for in-distribution instances will be lower than the entropies for OOD instances.
   given this assumption we can just do the simplest "classifier" of all; thresholding
</p>
<p>to get a sense of the precision/recall tradeoff of approach we can do the following...
</p>
<ul>
 <li>
     fit a MinMax scaler to the <code>train</code> set; this scales all train values to (0, 1)
 </li>

 <li>
     use this scaler to transform <code>test</code>, <code>ood_easy</code> and <code>ood_hard</code>; including (0, 1) clipping
 </li>

 <li>
     "flip" all the values, <code>x = 1.0 - x</code>, since we want a low entropy to denote a positive instance of in-distribution
 </li>

 <li>
     compare the performance of this "classifier" by considering <code>test</code> instances as positives and <code>ood</code> instances
as negatives.
 </li>
</ul>
<p>with this simple approach we get the following ROC, P/R and reliability plots.
</p>
<img src="/blog/imgs/2020/ood/m3.roc_pr_reliability.thresholding.png" />

<p>i've included the attempts i made to use a kernel density estimator in <code>fit_kernel_density_to_entropy.py</code>
   so if you manage to get a result better than this trivial baseline please
   let me know :)
</p>

<h1>other options for detecting OOD</h1>
<p>though this post is about using entropy as a way of measuring OOD there are other ways too.
</p>
<p>the simplest way i've handled OOD detection in the past is to include an
   explicit OTHER label. this has worked in cases where
   i've had data available to train against, e.g. data that was (somehow) sampled
   but wasn't of interest for actual supervised problem at hand. but i can see
   in lots of situations this wouldn't always be possible.
</p>
<p>another approach i'm still hacking on is doing density estimation on the inputs; this
   related very closely to the diversity sampling ideas for active learning.
</p>

<h1>code</h1>
<p>these experiments were written in
   <a href="https://objax.readthedocs.io/">objax</a>, an object oriented wrapper for
   <a href="https://jax.readthedocs.io/">jax</a>
</p>
<p><a href="https://github.com/matpalm/ood_focal_loss">code available on github</a>
</p>]]></content:encoded>
    </item>
    <item>
      <title>my updated list of cool machine learning books</title>
      <link>http://matpalm.com/blog/cool_machine_learning_books</link>
      <category><![CDATA[Uncategorized]]></category>
      <guid>http://matpalm.com/blog/cool_machine_learning_books</guid>
      <description>my updated list of cool machine learning books</description>
      <content:encoded><![CDATA[<p>awhile ago i posted
   <a href="/blog/2010/08/06/my-list-of-cool-machine-learning-books/">my list of cool machine learning books</a>,
   but it's been awhile so it's probably time to update it...
</p>
<p><hr>
   <img src="/blog/imgs/2020/mlb/mml.jpg"/>
</p>
<p><b><a href="https://mml-book.github.io">Mathematics for Machine Learning</a>
   by Marc Peter Deisenroth, A. Aldo Faisal &amp; Cheng Soon Ong.</b>
</p>
<p>this is my personal favorite book on the general math required for machine learning,
   the way things are described really resonate with me.
   available as a free pdf but i got a paper copy to support the authors after reading the
   first half.
</p>
<p><hr>
   <img src="/blog/imgs/2020/mlb/laalfd.jpg" />
</p>
<p><b><a href="http://math.mit.edu/~gs/learningfromdata/">Linear Algebra and Learning from Data</a>
   by Gilbert Strang.</b>
</p>
<p>this is gilbert's most recent work. it's really great, he's such a good teacher, and
   <a href="https://ocw.mit.edu/courses/mathematics/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/">his freely available lectures</a>
   are even better. it's a shorter text than his other classic intro below with
   more of a focus on how things are connected to modern machine learning techniques.
</p>
<p><hr>
   <img src="/blog/imgs/2020/mlb/itla.jpg" />
</p>
<p><b><a href="https://math.mit.edu/~gs/linearalgebra/">Introduction to Linear Algebra</a>
   by Gilbert Strang.</b>
</p>
<p>this was my favorite linear algebra book for a long time before his 'learning from
   data' came out. this is a larger book with a more comprehensive view of linear algebra.
</p>
<p><hr>
   <img src="/blog/imgs/2020/mlb/ts.jpg" />
</p>
<p><b><a href="https://greenteapress.com/wp/think-stats-2e/">Think Stats: Probability and Statistics for Programmers</a> by Allen Downey.</b>
</p>
<p>this book focuses on practical computation methods for probability and statistics.
   i got a lot out of working through this one.
   it's all in python and available for free.
   ( exciting update! as part of writing this post i've discovered there's a new edition
   to read!)
</p>
<p><hr>
   <img src="/blog/imgs/2020/mlb/dbda.jpg" />
</p>
<p><b><a href="https://sites.google.com/site/doingbayesiandataanalysis/">Doing Bayesian Data Analysis</a>
   by John Kruscgke</b>
</p>
<p>on the bayesian side of things this is the book i've most enjoyed working through.
   i've only got the first edition which was R and
   <a href="https://en.wikipedia.org/wiki/OpenBUGS">BUGS</a> but i see
   the second edition is R,
   <a href="http://mcmc-jags.sourceforge.net/">JAGS</a> and
   <a href="https://mc-stan.org/">Stan</a>.
   it'd be fun i'm sure to work through it doing
   everything in <a href="https://github.com/pyro-ppl/numpyro">numpyro</a>. i might do that in all
   my free time. haha. "free time" hahaha. sob.
</p>
<p><hr>
   <img src="/blog/imgs/2020/mlb/eosl.jpg" />
</p>
<p><b><a href="https://web.stanford.edu/~hastie/ElemStatLearn/">The Elements of Statistical Learning</a>
   by Hastie, Tibshirani and Friedman</b>
</p>
<p>this is still one of the most amazing fundamental machine learning books i've ever had.
   in fact i've purchased this book <em>twice</em> and given it away both times :/ i might buy another
   copy some time soon, even though it's been freely available to download for ages. an
   amazing piece of work.
</p>
<p><hr>
   <img src="/blog/imgs/2020/mlb/pgm.jpg" />
</p>
<p><b>
   <a href="https://mitpress.mit.edu/books/probabilistic-graphical-models">Probabilistic Graphical Models</a>
   by Daphne Koller &amp; Nir Friedman</b>
</p>
<p>this is an epic textbook that i'd love to understand better. i've read a couple of sections in
   detail but not the entire tome yet.
</p>
<p><hr>
   <img src="/blog/imgs/2020/mlb/praml.jpg" />
</p>
<p><b>
   <a href="https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/">Pattern Recognition and Machine Learning</a>
   by Christopher Bishop</b>
</p>
<p>this is probably the best overall machine learning text book i've ever read. such a beautiful book
   and <a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">the pdf is FREE FOR DOWNLOAD!!!</a>
</p>
<p><hr>
   <img src="/blog/imgs/2020/mlb/mlapp.jpg" />
</p>
<p><b><a href="https://mitpress.mit.edu/books/machine-learning-1">Machine Learning: A Probabilistic Perspective</a> by Kevin Murphy</b>
</p>
<p>this is my second favorite general theory text on machine learning.
   i got kevin to sign my copy when he was passing my desk once but
   someone borrowed it and never gave it back :(
   so if you see a copy with my name on the spine let me know!
</p>
<p><hr>
   <img src="/blog/imgs/2020/mlb/homl.jpg" />
</p>
<p><b><a href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/">Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow</a> by AurÃ©lien GÃ©ron</b>
</p>
<p>this is the book i point most people to when they are interested in getting up
   to speed with modern applied machine learning without too much concern for the
   theory. it's very up to date (as much as a book can be) with the latest libraries
   and, most importantly, provides a good overview of not just neural stuff but fundamental
   <a href="https://scikit-learn.org/stable/">scikit-learn</a> as well.
</p>
<p><hr>
   <img src="/blog/imgs/2020/mlb/mle.jpg" />
</p>
<p><b><a href="http://www.mlebook.com/wiki/doku.php">Machine Learning Engineering</a> by Andriy Burkov</b>
</p>
<p>a great book focussing on the operations side of running a machine learning system. i'm a bit
   under half way through the free online version and very likely to buy a physical copy to finish
   it and support the author. great stuff and, in many ways, a more impactful book than any of
   the theory books here.
</p>
<p><hr>
   <img src="/blog/imgs/2020/mlb/itdm.jpg" />
</p>
<p><b><a href="https://www-users.cs.umn.edu/~kumar001/dmbook/index.php">Introduction to Data Mining</a>
   by Pang-Ning Tan, Michael Steinbach &amp; Vipin Kumar</b>
</p>
<p>this is another one that was also on my list from ten years ago and though it's section
   on neural networks is a bit of chuckle these days there is still a bunch of really
   great fundamental stuff in this book. very practical and easy to digest. i also see there's
   a second edition now. i reckon this would compliment the "hands on" book above very well.
</p>
<p><hr>
   <img src="/blog/imgs/2020/mlb/salp.jpg" />
</p>
<p><b><a href="https://web.stanford.edu/~jurafsky/slp3/">Speech and Language Processing</a>
   by Dan Jurafsky &amp; James Martin</b>
</p>
<p>still the best overview of NLP there is (IMHO). can't wait to read the 3rd edition which
   apparently will cover more modern stuff (e.g. transformers) but until then, for the
   love of god though, please don't be one of those "this entire book is
   irrelevant now! just fine tune BERT" people :/
</p>
<p><hr>
   <img src="/blog/imgs/2020/mlb/no.jpg" />
</p>
<p><b><a href="https://link.springer.com/book/10.1007/978-0-387-40065-5">Numerical Optimization</a>
   by Jorge NocedalStephen J. Wright</b>
</p>
<p>this book is super hard core and maybe more an operations
   research book than machine learning. though i've not read it cover to cover the
   couple of bits i've worked through really taught me a lot. i'd love to understand
   the stuff in this text better; it's so so fundamental to machine learning (and more)
</p>
<p><hr>
   <img src="/blog/imgs/2020/mlb/dl.jpg" />
</p>
<p><b><a href="https://www.deeplearningbook.org/">Deep Learning</a>
   by Ian Goodfellow</b>
</p>
<p>writing a book specifically on deep learning is very dangerous since things move so fast but
   if anyone can do it, ian can... i think ian's approach to explaining neural networks
   from the ground up is one of my favorites. i got the first edition hardback but it's free to
   download from the website.
</p>
<p><hr>
   <img src="/blog/imgs/2020/mlb/pr.jpg" />
</p>
<p><b><a href="https://mitpress.mit.edu/books/probabilistic-robotics">Probabilistic Robotics</a>
   by Sebastian Thrun, Wolfram Burgard and Dieter Fox</b>
</p>
<p>when i first joined a robotics group i bought a stack of ML/robotics books and this
   was by far the best. it's good intro stuff, and maybe already dated in places given
   it's age (the 2006 edition i have) but i still got a bunch from it.
</p>
<p><hr>
   <img src="/blog/imgs/2020/mlb/tml.jpg" />
</p>
<p><b><a href="https://www.oreilly.com/library/view/tinyml/9781492052036/">TinyML</a>
   by Pete Warden &amp; Daniel Situnayake</b>
</p>
<p>this was a super super fun book to tech review! neural networks on microcontrollers?!?
   yes please!
</p>
<p><hr>
   <img src="/blog/imgs/2020/mlb/ec.jpg" />
</p>
<p><b><a href="https://www.wiley.com/en-us/Evolutionary+Computation%3A+Toward+a+New+Philosophy+of+Machine+Intelligence%2C+3rd+Edition-p-9780471669517">Evolutionary Computation</a> by David Fogel</b>
</p>
<p>this is still by favorite book on evolutionary algorithms; i've had this for a loooong
   time now. i still feel like evolutionary approaches are due for a big big comeback
   any time soon....
</p>
<hr>


<h2>in the mail...</h2>
<p>the good thing about writing a list is you get people telling you cool ones you've missed :)
</p>
<p>the top three i've chosen (that are in the mail) are...
</p>
<p><hr>
   <img src="/blog/imgs/2020/mlb/ciis.jpg" />
</p>
<p><b><a href="http://bayes.cs.ucla.edu/PRIMER/">Causal Inference in Statistics</a> by
   Judea Pearl, Madelyn Glymour &amp; Nicholas P. Jewell</b>
</p>
<p>recommended by <a href="https://twitter.com/animesh_garg">animesh</a> who quite rightly points out
   the lack of causality in machine learning books in the books above.
</p>
<p><hr>
   <img src="/blog/imgs/2020/mlb/itiala.jpg" />
</p>
<p><b><a href="https://www.cambridge.org/au/academic/subjects/computer-science/pattern-recognition-and-machine-learning/information-theory-inference-and-learning-algorithms?format=HB&amp;isbn=9780521642989">Information Theory, Inference and Learning Algorithms</a> by David MacKay</b>
</p>
<p>i've seen this book mentioned a number of times and was most recently recommended by
   my colleague <a href="https://twitter.com/danesherbs">dane</a> so it's time to get it.
</p>]]></content:encoded>
    </item>
    <item>
      <title>dithernet very slow movie player</title>
      <link>http://matpalm.com/blog/dithernet_vsmp</link>
      <category><![CDATA[gan]]></category>
      <category><![CDATA[jax]]></category>
      <category><![CDATA[projects]]></category>
      <category><![CDATA[objax]]></category>
      <guid>http://matpalm.com/blog/dithernet_vsmp</guid>
      <description>dithernet very slow movie player</description>
      <content:encoded><![CDATA[<h1>very slow movie player</h1>
<p>it's been about two years since i first saw the awesome
   <a href="https://medium.com/s/story/very-slow-movie-player-499f76c48b62">very slow movie player</a>
   project by bryan boyer. i thought it was such an excellent idea but never got around
   to buying the hardware to make one. more recently though i've seen a couple of references
   to the project so i decided it was finally time to make one.
</p>
<p>one interesting concern about an eink very slow movie player is the screen refresh. simpler
   eink screens refresh by doing a full cycle of a screen of white or black before displaying
   the new image. i hated the idea of an ambient slow player doing this every few minutes
   as it switched frames, so i wanted to make sure i got a piece of hardware that could do
   incremental update.
</p>
<p>after a bit of shopping around i settled on a
   <a href="https://www.waveshare.com/6inch-hd-e-paper-hat.htm">6 inch HD screen from waveshare</a>
</p>
<p>it ticks all the boxes i wanted
</p>
<ul>
 <li>
     6 inch
 </li>

 <li>
     1448Ã—1072 high definition
 </li>

 <li>
     comes with a raspberry pi HAT
 </li>

 <li>
     and, most importantly, support partial refresh
 </li>
</ul>
<p>this screen also supports grey scale, but only with a flashy full cycle redraw,
   so i'm going to stick to just black and white since it supports the partial redraw.
</p>
<p>note: even though the partial redraw is basically instant it does suffer from a ghosting problem;
   when you draw a white pixel over a black one things are fine, but if you draw black over
   white, in the partial redraw, you get a slight ghosting of gray that is present until a
   full redraw :/
</p>

<h1>dithering</h1>
<p>so how do you display an image when you can only show black and white?
   dithering! here's an example of a 384x288 RGB image dithered using
   <a href="https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.convert">PILS implementation of the Floyd-Steinberg algorithm</a>
</p>
<table class='data'>
<tr><td><img src="/blog/imgs/2020/dn/eg.dither.png" /></td></tr>
<tr><td>original RGB vs dithered version</td></tr>
</table>

<p>it makes intuitive sense that you could have small variations in the exact locations of the
   dots as long as you get the densities generally right. s
   so there's a reasonable question then; how do you dither in such a way that you get a
   good result, but with minimal pixel changes from a previous frame? (since we're
   motivated on these screens to change as little as possible)
</p>
<p>there are two approaches i see
</p>
<p>1) spend 30 minutes googling for a solution that no doubt someone came up with 20 years
   ago that can be implemented in 10 lines of c running at 1000fps ...
</p>
<p>2) .... or train an
   <a href="https://jax.readthedocs.io/">jax</a>
   based GAN to generate the dithers with a loss balancing a good dither vs no pixel change. :P
</p>

<h1>the data</h1>
<p>when building a very slow movie player the most critical decision is...
   what movie to play?
   i really love the 1979 classic <a href="https://www.imdb.com/title/tt0078748/">alien</a>,
   it's such a great dark movie, so i thought i'd go with it.
   the movie is 160,000 frames so at a play back rate of a frame every 200 seconds
   it'll take just over a year to finish.
</p>
<p>note that in this type of problem there is no concern around overfitting.
   we have access to all data going in and so it's fine to overfit as much as we like;
   as long as we're minimising whatever our objective is we're good to go.
</p>

<h1>v1: the vanilla unet</h1>
<p>i started with a
   <a href="https://arxiv.org/abs/1505.04597">unet</a>
   that maps 3 channel RGB images to a single channel dither.
</p>
<table class='data'>
<tr><td><img src="/blog/imgs/2020/dn/models.v1.png" /></td></tr>
<tr><td>v1 architecture</td></tr>
</table>

<p>i tinkered a bit with the architecture but didn't spend too much time tuning it.
   for the final v3 result i ended with a pretty vanilla stack of encoders &amp; decoders
   (with skip connections connecting an encoder to the decoder at the same spatial resolution)
   each encoder/decoder block uses a residual like shortcut around a couple of convolutions.
   nearest neighbour upsampling gave a nicer result than deconvolutions in the decoder
   for the v3 result.
   also, <a href="https://arxiv.org/abs/1606.08415">gelu</a> is my new favorite activation :)
</p>
<p>for v1 i used a binary cross entropy loss of P(white) per pixel
   ( since it's what worked well for my
   <a href="http://matpalm.com/blog/counting_bees/">bee counting project</a> )
</p>
<p>as always i started by overfitting to a single example to get a baseline feel for capacity required.
</p>
<table class='data'>
<tr><td>
<img src="/blog/imgs/2020/dn/overfit.png" />
</td></tr>
<tr><td>
v1 overfit result
</td></tr>
</table>

<p>when scaling up to the full dataset i switched to training on half resolution images
   against a patch size of 128. working on half resolution consistently gave a better
   result than working with the full resolution.
</p>
<p>as expected though this model gave us the classic type of problem we see with
   straight unet style image translation; we get a reasonable sense of the shapes, but no
   fine details around the dithering.
</p>
<table class='data'>
<tr><td>
<img src="/blog/imgs/2020/dn/v1.upsample.png" />
</td></tr>
<tr><td>
v1 vanilla unet with upsampling example
</td></tr>
</table>

<p>side notes:
</p>
<ul>
 <li>
     for this v1 version using deconvolutions in the decoder
     (instead of nearest neighbour upsampling) actually looked pretty good!
     nicely captured texture for a dither with a surprisingly small network.
 </li>

 <li>
     i actually did some experiments using branches in the decoder for both upsampling
     and deconvolutions but the deconvs always dominated too much. i thought that would
     allow the upsampling to work as a kind of residual to the deconv but it never happened.
 </li>
</ul>
<table class='data'>
<tr><td>
<img src="/blog/imgs/2020/dn/v1.deconv.png" />
</td></tr>
<tr><td>
v1 vanilla unet with deconvolution example
</td></tr>
</table>


<h1>v2: to the GAN...</h1>
<p>for v2 i added a GAN objective in an attempt to capture finer details
</p>
<table class='data'>
<tr><td>
<img src="/blog/imgs/2020/dn/models.v2.png" />
</td></tr>
<tr><td>
v2 architecture
</td></tr>
</table>

<p>i started with the original
   <a href="https://arxiv.org/abs/1611.07004">pix2pix</a>
   objective but reasonably quickly moved to use a
   <a href="https://arxiv.org/abs/1701.07875">wasserstein</a>
   critic style objective since i've always found it more stable.
</p>
<p>the generator (G) was the same as the unet above with the discriminator (D) running patch based.
   at this point i also changed the reconstruction loss from a binary objective to just L1.
   i ended up using batchnorm in D, but not G.
   to be honest i only did a little did of manual tuning, i'm sure there's a better result
   hidden in the hyperparameters somewhere.
</p>
<p>so, for this version, the loss for G has two components
</p>
<pre>
1. D(G(rgb))             # fool D
2. L1(G(rgb), dither)    # reconstruct the dither
</pre>

<p>very quickly (i.e. in &lt; 10mins ) we get a reasonable result that is started to
   show some more detail than just the blobby reconstruction.
</p>
<table class='data'>
<tr><td>
<img src="/blog/imgs/2020/dn/v2.eg.png" />
</td></tr>
<tr><td>
v2 partial trained eg
</td></tr>
</table>

<p>note: if the loss weight of 2) is 0 we degenerate to v1
   (which proved a useful intermediate debugging step).
   at this point i didn't want to tune to much since the final v3 is coming...
</p>

<h1>v3: a loss related to change from last frame</h1>
<p>for v3 we finally introduce a loss relating the previous frame
   (which was one of the main intentions of the project in the first place)
</p>
<p>now G takes not just the RGB image, but the dither of the previous frame.
</p>
<table class='data'>
<tr><td>
<img src="/blog/imgs/2020/dn/models.v3.png" />
</td></tr>
<tr><td>
v3 architecture
</td></tr>
</table>

<p>the loss for G now has three parts
</p>
<pre>
1. D(G(rgb_t1)) => real      # fool D
2. L1(G(rgb_t1), dither_t1)  # reconstruct the dither
3. L1(G(rgb_t1), dither_t0)  # don't change too much from the last frame
</pre>

<p>normally with a network that takes as input the same thing it's outputting
   we have to be careful to include things like teacher forcing.
   but since we don't intend to use this network for any kind of rollouts
   we can just always feed the "true" dithers in where required.
   having said that, rolling out the dithers from this network would be interesting :D
</p>

<h1>digression; the troublesome scene changes</h1>
<p>the third loss objective, not changing too many pixels from the last frame,
   works well for generally stationary shots
   but is disastrous for scene changes :/
</p>
<p>consider the following graph for a sequence of frames showing the pixel difference
   between frames.
</p>
<img src="/blog/imgs/2020/dn/pixel_diff_between_scenes.png" />

<p>when there is a scene change we observe a clear "spike" in pixel diff. my first thought
   was to look for these and do a full redraw for them. it's very straightforward to
   find them (using a simple z-score based anomaly detector on a sliding window) but
   the problem is that it doesn't pick up the troublesome case of a panning shot where we don't
   have a scene change exactly. in these cases there is no abrupt scene change, but there
   are a lot of pixels changing so we end up seeing a lot of ghosting.
</p>
<p>i spent ages tinkering with the best way to approach this before deciding that a simple
   approach of <code>num_pixels_changed_since_last_redraw &gt; threshold</code> was good enough to decide
   if a full redraw was required (with a cooldown to ensure we not redrawing all the time)
</p>

<h1>... and back to v3</h1>
<p>the v3 network gets a very good result <em>very</em> quickly; unsurprisingly since the dither at time
   t0 provided to G is a pretty good estimate of the dither at t1 :)
   i.e. G can get a good result simply by copying it!
</p>
<p>the following scenario shows this effect...
</p>
<p>consider three sequential frames, the middle one being a scene change.
</p>
<img src="/blog/imgs/2020/dn/v3.scene_eg.1.png" />

<p>at the very start of training the reconstruction loss is dominant and
   we get blobby outlines of the frame.
</p>
<img src="/blog/imgs/2020/dn/v3.scene_eg.2.png" />

<p>but as the contribution from the dither at time t0 kicks it things look good in general but
   the frames at the scene change end up being a ghosted mix attempt to copy through the old
   frame along with dithering the new one.
   (depending on the relative strength of the loss terms of G).
</p>
<img src="/blog/imgs/2020/dn/v3.scene_eg.3.png" />


<h1>the final result</h1>
<p>so the v3 version generally works and i'm sure with some more tuning i could get a better result
   but, as luck would have it, i actually find the results from v2 more appealing when testing
   on the actual eink screen. so even though the intention was do something like v3 i'm going to end
   up running something more like v2 (as shown in these couple of examples (though the resolution
   does it no justice (not to mention the fact the player will run about 5000 times slower than these
   gifs)))
</p>
<table class='data'><tr>
<td><img src="/blog/imgs/2020/dn/f_00048000.gif"/></td>
<td><img src="/blog/imgs/2020/dn/f_00067400.gif"/></td>
</tr><tr>
<td><img src="/blog/imgs/2020/dn/f_00082400.gif"/></td>
<td><img src="/blog/imgs/2020/dn/f_00107045.gif"/></td>
</tr><tr>
<td><img src="/blog/imgs/2020/dn/f_00120600.gif"/></td>
<td><img src="/blog/imgs/2020/dn/f_00145400.gif"/></td>
</tr></table>


<h1>player case</h1>
<p>i ran for a few weeks with a prototype that lived balanced precariously on a piece of foam
   below it's younger sibling pi zero eink screen running game of life. eventually i cut up some
   pieces of an old couch and made a simple wooden frame. a carpenter, i am not :/
</p>
<table class='data'>
<tr>
<td><img src="/blog/imgs/2020/dn/prototype.png" /></td>
<td><img src="/blog/imgs/2020/dn/prototype.2.png" /></td>
</tr>
<tr>
<td>prototype</td>
<td>frame</td>
</tr>
</table>


<h1>todos</h1>
<ul>
 <li>
     for reconstruction and frame change i used L1 loss, but that's not exactly what we
     want. since we want to avoid the ghosting (white changing to black resulting in grey)
     we should try to avoid white to black but ignore black to white.
 </li>

 <li>
     we might be able to better handle scene changes by also including a
     loss component around the <em>next</em> frame.
 </li>

 <li>
     there's a padding issue where i train G on patches but when it's run on the full res
     version we get an edge artefact the size of the original patch (see image below).
     as a hacky fix i just padded the RGB image before passing it to G in the final run, but this problem could
     be fixed by changing the padding schema during training.
 </li>
</ul>
<img src="/blog/imgs/2020/dn/todo_padding.png"/>


<h1>code</h1>
<p><a href="https://github.com/matpalm/dither_net">all on github</a>
</p>]]></content:encoded>
    </item>
    <item>
      <title>ensemble networks</title>
      <link>http://matpalm.com/blog/ensemble_nets</link>
      <category><![CDATA[objax]]></category>
      <category><![CDATA[projects]]></category>
      <category><![CDATA[ensemble_nets]]></category>
      <category><![CDATA[jax]]></category>
      <guid>http://matpalm.com/blog/ensemble_nets</guid>
      <description>ensemble networks</description>
      <content:encoded><![CDATA[<h1>overview</h1>
<p>ensemble nets are a method of representing an ensemble of
   models as one single logical model. we use
   <a href="https://github.com/google/jax">jax's</a>
   <a href="https://github.com/google/jax#auto-vectorization-with-vmap">vmap</a>
   operation to batch over not
   just the inputs but additionally sets of model parameters.
   we propose some
   approaches for training ensemble nets and introduce logit dropout as
   a way to improve ensemble generalisation as well as provide
   a method of calculating model confidence.
</p>
<p><b>update: though i originally developed this project using vmap, which
   is how things are described below, the latest version of the
   <a href="https://github.com/matpalm/ensemble_net">code</a>
   is a port to use <a href="https://github.com/google/jax#spmd-programming-with-pmap">pmap</a>
   so we can run on a tpu pod slice, not just one machine</b>
</p>

<h1>background</h1>
<p>as part of my "embedding the chickens" project i wanted to use
   random projection embedding networks to generate pairs of similar
   images for weak labelling. since this technique works really well
   in an ensemble i did some playing around and got the ensemble
   running pretty fast in jax. i wrote it up in
   <a href="/blog/jax_random_embedding_ensemble_network/">this blog post</a>.
   since doing that i've been wondering how to not just run an
   ensemble net forward pass but how you might <em>train</em> one too...
</p>

<h1>dataset &amp; problem</h1>
<p>for this problem we'll work with the
   <a href="https://github.com/phelber/eurosat">eurosat/rgb dataset</a>.
   eurosat/rgb is a 10 way classification task across 27,000 64x64 RGB images
</p>
<p>here's a sample image from each of the ten classes...
</p>
<img src="/blog/imgs/2020/en/sample_images.png" />


<h1>base line model</h1>

<h2>architecture</h2>
<p>as a baseline we'll start with a simple non ensemble network. it'll consist of
   a pretty vanilla convolutional stack, global spatial pooling, one dense layer
   and a final 10 way classification layer.
</p>
<table class='data'>
<tr><td><b>layer</b></td><td><b>shape</b></td></tr>
<tr><td>input</td><td>(B, 64, 64, 3)</td></tr>
<tr><td>conv2d</td><td>(B, 31, 31, 32)</td></tr>
<tr><td>conv2d</td><td>(B, 15, 15, 64)</td></tr>
<tr><td>conv2d</td><td>(B, 7, 7, 96)</td></tr>
<tr><td>conv2d</td><td>(B, 3, 3, 96)</td></tr>
<tr><td>global spatial pooling</td><td>(B, 96)</td></tr>
<tr><td>dense</td><td>(B, 96)</td></tr>
<tr><td>logits (i.e. dense with no activation)</td><td>(B, 10)</td></tr>
</table>

<p>all convolutions use 3x3 kernels with a stride of 2. the conv layers and the single
   dense layer use a gelu activation. batch size is represented by <code>B</code>.
</p>
<p>we use no batch norm, no residual connections, nothing fancy at all.
   we're more interested in the training than getting the absolute best value.
   this network is small enough that we can train it fast but it still gives
   reasonable results. residual connections would be trivial to add but batch
   norm would be a bit more tricky given how we'll build the ensemble net later.
</p>
<p>we'll use <a href="https://github.com/google/objax">objax</a> to manage the model params
   and orchestrate the training loops.
</p>

<h2>training setup</h2>
<p>training for the baseline will be pretty standard but let's walk through it
   so we can call out a couple of specific things for comparison with an ensemble
   net later...
</p>
<p>( we'll use 2 classes in these diagrams for ease of reading though
   the eurosat problem has 10 classes. )
</p>
<img src="/blog/imgs/2020/en/d.non_ensemble.png" />

<p>walking through left to right...
</p>
<ol>
 <li>
     input is a batch of images; <code>(B, H, W, 3)</code>
 </li>

 <li>
     the output of the first convolutional layers with stride=2 &amp; 32 filters will be <code>(B, H/2, W/2, 32)</code>
 </li>

 <li>
     the network output for an example two class problem are logits shaped <code>(B, 2)</code>
 </li>

 <li>
     for prediction probabilities we apply a softmax to the logits
 </li>

 <li>
     for training we use cross entropy, take the mean loss and apply backprop
 </li>
</ol>
<p>we'll train on 80% of the data, do hyperparam tuning on 10% (validation set) and
   report final results on the remaining 10% (test set)
</p>
<p>for hyperparam tuning we'll use <a href="https://ax.dev/">ax</a> on very short runs of 30min
   for all trials.
   for experiment tracking we'll use <a href="https://www.wandb.com/">wandb</a>
</p>
<p>the hyperparams we'll tune for the baseline will be...
</p>
<table class='data'>
<tr><td><b>param</b></td><td><b>description</b></td></tr>
<tr>
<td>max_conv_size</td>
<td>conv layers with be sized as [32, 64, 128, 256]</br>
up to a max size of max_conv_size.</br>
i.e. a max_conv_size of 75 would imply sizes [32, 64, 75, 75]</br></td>
</tr>
<tr>
<td>dense_kernel_size</td>
<td>how many units in the dense layer before the logits</td>
</tr>
<tr>
<td>learning_rate</td>
<td>learning rate for optimiser</td>
</tr>
</table>

<p>we'd usually make choices like the conv sizes being powers of 2 instead of a smooth value but
   i was curious about the behaviour of ax for tuning.
   also we didn't bother with a learning rate schedule; we just use simple early
   stopping (against the validation set)
</p>
<img src="/blog/imgs/2020/en/single_model.all_runs.png" />

<p>the best model of this group gets an accuracy of 0.913 on the validation set
   and 0.903 on the test set. ( usually not a fan of accuracy but the classes
   are pretty balanced so accuracy isn't a terrible thing to report. )
</p>
<table class='data'>
<tr><td><b>model</td><td><b>validation</b></td><td><b>test</b></td></tr>
<tr><td>baseline</td><td>0.913</td><td>0.903</td></tr>
</table>


<h1>ensemble net model</h1>
<p>so what then is an ensemble net?
</p>
<p>logically we can think about our models as being functions that take two
   things 1) the parameters of the model and 2) an input example. from
   these they return an output.
</p>
<pre class="prettyprint"><code class="language-python"># pseudo code
model(params, input) -> output
</code></pre>

<p>we pretty much always though run a batch of <code>B</code> inputs at once.
   this can be easily represented as a leading axis on the input and allows us to
   make better use of accelerated hardware as well as providing some benefits regarding
   learning w.r.t gradient variance.
</p>
<p>jax's
   <a href="https://github.com/google/jax#auto-vectorization-with-vmap">vmap</a>
   function makes this trivial to implement by vectorising a call to the model
   across a vector of inputs to return a vector of outputs.
</p>
<pre class="prettyprint"><code class="language-python"># pseudo code
vmap(partial(model, params))(b_inputs) -> b_outputs
</code></pre>

<p>interestingly we can use this same functionality to batch not across independent
   inputs but instead batch across independent sets of <code>M</code> model params. this effectively
   means we run the <code>M</code> models in parallel. we'll call these <code>M</code> models sub models
   from now on.
</p>
<pre class="prettyprint"><code class="language-python"># pseudo code
vmap(partial(model, input))(m_params) -> m_outputs
</code></pre>

<p>and there's no reason why we can't do both batching across both a set of inputs
   as well as a set of model params at the same time.
</p>
<pre class="prettyprint"><code class="language-python"># pseudo code
vmap(partial(model)(b_inputs, m_params) -> b_m_outputs
</code></pre>

<p>for a lot more specifics on how i use jax's vmap to support this
   see my prior post on
   <a href="http://matpalm.com/blog/jax_random_embedding_ensemble_network/">jax random embedding ensemble nets</a>.
</p>
<p>and did somebody say TPUs? turns out we can make ensemble nets run
   super fast on TPUs by simply swapping the vmap calls for pmap ones!
   using pmap on a TPU will have each ensemble net run in parallel! see
   <a href="https://colab.research.google.com/drive/1ijI77AlYqGOEXm5BqtnNomtUPnFr26o0?usp=sharing">this colab</a>
   for example code running pmap on TPUs
</p>

<h2>single_input ensemble</h2>
<p>let's walk through this in a bit more detail with an ensemble net with two sub models.
</p>
<img src="/blog/imgs/2020/en/d.single_input.png" />

<ol>
 <li>
     our input is the same as for the baseline; a batch of images <code>(B, H, W, 3)</code>
 </li>

 <li>
     the output of the first conv layer now though has an additional <code>M</code> axis to
represent the outputs from the <code>M</code> models and results in <code>(M, B, H/2, W/2, 32)</code>
 </li>

 <li>
     this additional <code>M</code> axis is carried all the way through to the logits <code>(M, B, 2)</code>
 </li>

 <li>
     at this point we have <code>(M, B, 2)</code> logits but we need <code>(B, 2)</code>
to compare against <code>(B,)</code> labels. with logits this reduction is
very simple; just sum over the <code>M</code> axis!
 </li>

 <li>
     for prediction probabilities we again apply a softmax
 </li>

 <li>
     for training we again use cross entropy to calculate the mean loss and apply backprop
 </li>
</ol>
<p>this gives us a way to train the sub models to act as a single ensemble unit as well as
   a way to run inference on the ensemble net in a single forward pass.
</p>
<p>we'll refer to this approach as <strong>single_input</strong> since we are starting with a single
   image for all sub models.
</p>

<h2>multi_input ensemble</h2>
<p>an alternative approach to training is to provide a separate image per sub model.
   how would things differ if we did that?
</p>
<img src="/blog/imgs/2020/en/d.multi_input.png" />

<ol>
 <li>
     now our input has an additional <code>M</code> axis since it's a different batch per sub model.
<code>(M, B, H, W, 3)</code>
 </li>

 <li>
     the output of the first conv layers carries this <code>M</code> axis through <code>(M, B, H/2, W/2, 32)</code>
 </li>

 <li>
     which is carried to the logits <code>(M, B, 2)</code>
 </li>

 <li>
     in this case though we have <code>M</code> seperate labels for the <code>M</code> inputs so we don't have to combine the logits at all, we can just calculate the mean loss across the <code>(M, B)</code> training instances.
 </li>
</ol>
<p>we'll call this approach <strong>multi_input</strong>.
   note that this way of feeding separate images only really applies to training;
   for inference if we want the representation of the ensemble it only really makes
   sense to send a batch of <code>(B)</code> images, not <code>(M, B)</code>.
</p>

<h2>training the ensemble net</h2>
<p>let's do some tuning as before but with a couple of additional hyper
   parameters that this time we'll sweep across.
</p>
<p>we'll do each of the six combos of <code>[(single, 2), (single, 4), (single, 8),
(multi, 2), (multi, 4), (multi, 8)]</code> and tune for 30 min for each.
</p>
<img src="/blog/imgs/2020/en/single_multi_sweeps.png" />

<p>when we poke around and facet by the various params there's only one that makes
   a difference; single_input mode consistently does better than multi_input.
</p>
<p>in hindsight this is not surprising i suppose since single_input mode
   is effectively training one network with xM parameters
   (with an odd summing-of-logits kind of bottleneck)
</p>
<img src="/blog/imgs/2020/en/validation_boxplot.png" />


<h1>confusion matrix per sub model</h1>

<h2>single_input ensemble</h2>
<p>when we check the best single_input 4 sub model ensemble net we get an accuracy of 0.920
   against the validation set and 0.901 against the test set
</p>
<table class='data'>
<tr><td><b>model</td><td><b>validation</b></td><td><b>test</b></td></tr>
<tr><td>baseline</td><td>0.913</td><td>0.903</td></tr>
<tr><td>single_input</td><td>0.920</td><td>0.901</td></tr>
</table>

<p>looking at the confusion matrix the only
   really thing to note is the slight confusion between 'Permanent Crop' and
   'Herbaceous Vegetation' which is reasonable given the similarity in RGB.
</p>
<img src="/blog/imgs/2020/en/cm.simo.ensemble.png" />

<p>we can also review the confusion matrices of each of the 4 sub models run as
   individuals; i.e. not working as an ensemble. we observe the quality of each isn't
   great with accuracies of [0.111, 0.634, 0.157, 0.686].
   again makes sense since they had been trained only to work together. that first model
   really loves 'Forests', but don't we all...
</p>
<table>
<tr>
<td><img src="/blog/imgs/2020/en/cm.simo.model_0.png" /></td>
<td><img src="/blog/imgs/2020/en/cm.simo.model_1.png" /></td>
</tr>
<tr>
<td><img src="/blog/imgs/2020/en/cm.simo.model_2.png" /></td>
<td><img src="/blog/imgs/2020/en/cm.simo.model_3.png" /></td>
</tr>
</table>


<h2>multi_input ensemble</h2>
<p>the performance of the multi_input ensemble isn't quite as good with
   a validation accuracy of 0.902 and test accuracy of 0.896.
   the confusion matrix looks similar to the single_input mode version.
</p>
<table class='data'>
<tr><td><b>model</td><td><b>validation</b></td><td><b>test</b></td></tr>
<tr><td>baseline</td><td>0.913</td><td>0.903</td></tr>
<tr><td>single_input</td><td>0.920</td><td>0.901</td></tr>
<tr><td>multi_input</td><td>0.902</td><td>0.896</td></tr>
</table>

<img src="/blog/imgs/2020/en/cm.mimo.ensemble.png" />

<p>this time though the output of each of the 4 sub models individually is much stronger
   with accuracies of [0.842, 0.85, 0.84, 0.83, 0.86]. this makes sense
   since they were trained to not predict as one model.
   it is nice to see at least that the ensemble result is higher than any one model.
   and reviewing their confusion matrices they seem to specialise in different
   aspects with differing pairs of confused classes.
</p>
<table>
<tr>
<td><img src="/blog/imgs/2020/en/cm.mimo.model_0.png" /></td>
<td><img src="/blog/imgs/2020/en/cm.mimo.model_1.png" /></td>
</tr>
<tr>
<td><img src="/blog/imgs/2020/en/cm.mimo.model_2.png" /></td>
<td><img src="/blog/imgs/2020/en/cm.mimo.model_3.png" /></td>
</tr>
</table>


<h1>dropping logits</h1>
<p>the main failing of the single_input approach is that the sub models
   are trained to always operate together; that breaks some of
   the core ideas of why we do ensembles in the first place. as i was
   thinking about this i was reminded that the core idea of dropout is
   quite similar; when nodes in a dense layer are running together
   we can drop some out to ensure other nodes don't overfit to
   expecting them to always behave in a particular way.
</p>
<p>so let's do the same with the sub models of the ensemble. my first thought
   around this was that the most logical place would be at the logits. we can
   zero out the logits of a random half of the models during training and,
   given the ensembling is implemented by summing the logits, this effectively
   removes those models from the ensemble.
   the biggest con though is the waste of the forward pass of running those sub
   models in the first place.
   during inference we don't have to do anything in terms of masking &amp; there's no
   need to do any rescaling ( that i can think of ).
</p>
<p>so how does it do? accuracy is 0.914 against the validation set and 0.911 against
   the test set; the best result so far! TBH though, these numbers are pretty
   close anyways so maybe we were just lucky ;)
</p>
<table class='data'>
<tr><td><b>model</td><td><b>validation</b></td><td><b>test</b></td></tr>
<tr><td>baseline</td><td>0.913</td><td>0.903</td></tr>
<tr><td>single_input</td><td>0.920</td><td>0.901</td></tr>
<tr><td>multi_input</td><td>0.902</td><td>0.896</td></tr>
<tr><td>logit drop</td><td>0.914</td><td>0.911</td></tr>
</table>

<img src="/blog/imgs/2020/en/cm.simo_ld.ensemble.png" />

<p>the sub models are all now doing OK with accuracies of
   [0.764, 0.827, 0.772, 0.710]. though the sub models aren't as
   strong as the sub models of the multi_input mode, the overall
   performance is the best. great! seems like a nice compromise
   between the two!
</p>
<table>
<tr>
<td><img src="/blog/imgs/2020/en/cm.simo_ld.model_0.png" /></td>
<td><img src="/blog/imgs/2020/en/cm.simo_ld.model_1.png" /></td>
</tr>
<tr>
<td><img src="/blog/imgs/2020/en/cm.simo_ld.model_2.png" /></td>
<td><img src="/blog/imgs/2020/en/cm.simo_ld.model_3.png" /></td>
</tr>
</table>


<h1>wait! don't drop logits, drop models instead!</h1>
<p>the main problem i had with dropping logits is that there
   is a wasted forward pass for half the sub models. then i realised
   why run the models at all? instead of dropping logits we can just
   choose, through advanced indexing, a random half of the models to
   run a forward pass through. this has the same effect of running a random
   half of the models at a time but only requires half the forward pass
   compute. this approach of dropping models is what the code currently does.
   (though the dropping of logits is in the git history)
</p>

<h1>using the sub models to measure confidence</h1>
<p>ensembles also provide a clean way of measuring confidence of
   a prediction. if the variance of predictions across sub models is low
   it implies the ensemble as a whole is confident.
   alternatively if the variance is high it implies the ensemble is not confident.
</p>
<p>with the ensemble model that has been trained with logit dropout we can
   get an idea of this variance by considering the ensemble in a hold-one-out
   fashion; we can obtain <code>M</code> different predictions from the ensemble
   by running it as if each of the <code>M</code> sub models was not present (using the same
   idea as the logit dropout).
</p>
<p>consider a class that the ensemble is very good at; e.g. 'Sea &amp; Lake'.
   given a batch of 8 of these images across an ensemble net with 4 sub
   models we get the following prediction mean and stddevs.
</p>
<table class='data'>
<tr><td><b>idx</b></td>
<td><b>y_pred</b></td>
<td><b>mean(P(class))</b></td>
<td><b>std(P(class))</td></b></tr>
<tr><td>0</td><td>Sea & Lake</td><td>0.927</td><td>0.068</td></tr>
<tr><td>1</td><td>Sea & Lake</td><td>1.000</td><td>0.000</td></tr>
<tr><td>2</td><td>Sea & Lake</td><td>1.000</td><td>0.000</td></tr>
<tr><td>3</td><td>Sea & Lake</td><td>0.999</td><td>0.001</td></tr>
<tr><td>4</td><td>Sea & Lake</td><td>0.989</td><td>0.019</td></tr>
<tr><td>5</td><td>Sea & Lake</td><td>1.000</td><td>0.000</td></tr>
<tr><td>6</td><td>Sea & Lake</td><td>1.000</td><td>0.000</td></tr>
<tr><td>7</td><td>Sea & Lake</td><td>1.000</td><td>0.000</td></tr>
</table>

<p>whereas when we look at a class the model is not so sure of,
   e.g. 'Permanent Crop', we can see that for the lower probability cases
   have a higher variance across the models.
</p>
<table class='data'>
<tr><td><b>idx</b></td>
<td><b>y_pred</b></td>
<td><b>mean(P(class))</b></td>
<td><b>std(P(class))</td></b></tr>
<tr><td>0</td><td>Industrial Buildings</td><td>0.508</td><td>0.282</td></tr>
<tr><td>1</td><td>Permanent Crop</td><td>0.979</td><td>0.021</td></tr>
<tr><td>2</td><td>Permanent Crop</td><td>0.703</td><td>0.167</td></tr>
<tr><td>3</td><td>Herbaceous Vegetation</td><td>0.808</td><td>0.231</td></tr>
<tr><td>4</td><td>Permanent Crop</td><td>0.941</td><td>0.076</td></tr>
<tr><td>5</td><td>Permanent Crop</td><td>0.979</td><td>0.014</td></tr>
<tr><td>6</td><td>Permanent Crop</td><td>0.833</td><td>0.155</td></tr>
<tr><td>7</td><td>Permanent Crop</td><td>0.968</td><td>0.025</td></tr>
</table>


<h1>conclusions</h1>
<ul>
 <li>
     jax vmap provides a great way to represent an ensemble in a single ensemble net.
 </li>

 <li>
     we have a couple of options on how to train an ensemble net.
 </li>

 <li>
     the single_input approach gives a good result, but each sub model is poor by itself.
 </li>

 <li>
     multi_input trains each model to predict well, and the ensemble gets a bump.
 </li>

 <li>
     logit dropout gives a way to stop the single_input ensemble from overfitting by
     preventing sub models from specialising.
 </li>

 <li>
     variance across the sub models predictions gives a hint of prediction confidence.
 </li>
</ul>

<h1>TODOs</h1>
<ul>
 <li>
     compare the performance of single_input mode vs multi_input mode normalising for
     the number of effective parameters ( recall; single_input mode, without logit dropout,
     is basically training a single xM param large model )
 </li>

 <li>
     what is the effect of sharing an optimiser? would it be better to train each with
     seperate optimisers? can't see why; but might be missing something..
 </li>
</ul>

<h1>code</h1>
<p><a href="https://github.com/matpalm/ensemble_net">all on github</a>
</p>]]></content:encoded>
    </item>
    <item>
      <title>metric learning for image similarity search in objax</title>
      <link>http://matpalm.com/blog/objax_metric_learning</link>
      <category><![CDATA[objax]]></category>
      <category><![CDATA[jax]]></category>
      <guid>http://matpalm.com/blog/objax_metric_learning</guid>
      <description>metric learning for image similarity search in objax</description>
      <content:encoded><![CDATA[<p>i recently ported by
   <a href="https://keras.io/">keras.io</a>
   <a href="https://keras.io/examples/vision/metric_learning/">metric learning for image similarity search</a>
   tutorial to use
   <a href="https://objax.readthedocs.io/en/latest/">objax</a>.
</p>
<p>check it out in
   <a href="https://github.com/google/objax/blob/master/examples/tutorials/metric-learning.ipynb">this tutorial</a>
   and let me know what you think!
</p>]]></content:encoded>
    </item>
  </channel>
</rss>
