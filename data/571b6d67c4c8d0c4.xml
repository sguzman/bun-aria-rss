<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Stories by Ahmed El Deeb on Medium]]></title>
        <description><![CDATA[Stories by Ahmed El Deeb on Medium]]></description>
        <link>https://medium.com/@D33B?source=rss-55388a733bf9------2</link>
        <image>
            <url>https://cdn-images-1.medium.com/fit/c/150/150/1*SMqClMGFLKu-CDKZZ7s2GA@2x.jpeg</url>
            <title>Stories by Ahmed El Deeb on Medium</title>
            <link>https://medium.com/@D33B?source=rss-55388a733bf9------2</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Sat, 05 Nov 2022 16:33:57 GMT</lastBuildDate>
        <atom:link href="https://medium.com/@D33B/feed" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[So what is Machine Learning good for anyway?]]></title>
            <link>https://medium.com/rants-on-machine-learning/so-what-is-machine-learning-good-for-anyway-3cb38621383f?source=rss-55388a733bf9------2</link>
            <guid isPermaLink="false">https://medium.com/p/3cb38621383f</guid>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[artificial-intelligence]]></category>
            <dc:creator><![CDATA[Ahmed El Deeb]]></dc:creator>
            <pubDate>Mon, 20 Mar 2017 00:03:22 GMT</pubDate>
            <atom:updated>2017-03-31T20:53:30.012Z</atom:updated>
            <content:encoded><![CDATA[<p>A few weeks ago, a friend and colleague (Alex G.) asked me this question. He was aware of recent breakthroughs in the world of Machine Learning, esp. where it came to computer vision. Popular media picked up these relatively recent successes and brought them to the center of attention and then started projecting a future where machines are going to render humans obsolete. So he asked me: <strong>“Other than the recent breakthroughs in computer vision, what are the other success stories for machine learning?”</strong></p><p>At first I was quite surprised by the question. I have been using ML techniques for several years now and was thoroughly convinced of their usefulness and feasibility, I thought: surely, everyone must see it the same way! But as I started formulating my answer, I was struck by how hard I found it to come up with a satisfying answer. I though it must be one of these brain-freeze moments where you can’t seem to capture the simplest of words or ideas, but at the moment, my own answer sounded unconvincing to my own ears. Being a generous man, he didn’t call me out on my terrible answer, but went on to ask: <strong>“… and what is it that makes a particular problem more or less amenable to ML techniques?”</strong></p><h3>The State of Machine Learning</h3><p>So what <em>is</em> machine learning good for? What problems has it solved for us? In what fields has it proven itself worth the investment and the hype? And should we expect it to succeed in the future at solving more problems in other domains, or is this just naive optimism? Where are we in the <a href="https://en.wikipedia.org/wiki/Hype_cycle">hype cycle</a> of “Deep Learning”? Are we heading towards a new AI winter?</p><p>I’ll try to make the case that machine learning has already paid off, and that optimism in its future is well warranted (even if it is currently somewhat overhyped).</p><h4>Spam or ham?</h4><p>One of the early successes of machine learning was in the domain of text “understanding”, or more precisely, document classification. The simple task of filtering out spam email, without which email would not really be viable and the world as we know today would not exist. Most of the techniques used for document classification don’t actually rely on any kind of language understanding, but rely instead on the occurrence of various words and phrases.</p><h4>Are we feeling lucky?</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/812/1*tJCL_QTXjEs8X-aeY5j-ZQ.png" /></figure><p>Robust and adaptive spam filters allowed us to take email back from the spammers, but an arguably more important success was the use of Machine Learning in search engines. Early search engine ranking systems relied on hand-crafted formulas, but very quickly machine learning and statistical models became the most important building blocks of search engines, from the filtering of spammy pages, to spell-correcting user queries and augmenting them with extra keywords, to the actual ranking of web results, to extracting relevant parts of page contents for snippets. Without machine learning, Google would not be the search engine we know today, and reaching information about anything would be at least twice as hard. It’s hard to overstate how big an impact this had on our modern world.</p><h4>You may also like …</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ErvMprB4ZYLP0PeDt4ho9w.png" /><figcaption>Recommendations based on previous behavior and relatedness as a way for exploration of contents.</figcaption></figure><p>Both the previous cases had mostly to do with text documents. Understanding language was still beyond reach, but using statistical characteristics of language and documents proved very effective in a number of tasks. The first notable non-text success to my mind was collaborative filtering, also known as recommender systems, notably pioneered by Amazon and Netflix. The idea that web services can use information about some users to infer what <em>other</em> users may be interested in had some very lucrative applications and provided a way to deal with heterogenous objects (movies, songs, items sold in an online store, travel destinations) potentially without needing a good way to represent them. Statistics about which users liked or bought a particular object replaced statistics about words it contained or genres it belonged to. What would our modern world be if online stores were not as effective as they are today? Where would the music industry be if streaming service as we know them today did not exist?</p><h4>The damn ads!</h4><p>We all hate ads, but there had to be a way for companies like Google to make money. And make money they do! Once the internet became more efficient than old-fashioned media at targeting specific user groups, say, based on interest in some specific type of content, it started to really pay of. And that was just the beginning, with Machine Learning based ad targeting and selection systems, and platforms that, like Facebook, Google, and Amazon that had information-rich user profiles, ad targeting became more and more efficient. In fact, in most major internet companies, you’ll find a significant portion of the Machine Learning talent syphoned into the teams building the ad targeting and optimization platforms. Which can be seen as unfortunate, but ads are less annoying when they are actually relevant, so let’s hope for better, privacy-preserving ad-targeting systems that enable a better balance of content to ads and make the web a better place for everyone.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*SoIa3L4GYQGs81kQU82Lng.png" /><figcaption>A tad too much?</figcaption></figure><h4>Now you see me …</h4><p>More recently and maybe more famously, computers became good at seeing things and “understanding” them. Some applications of turning written numbers or words into digital form have been around for decades and quite useful in their own right, but with recent advances is Machine Learning algorithms, computers started becoming good at detecting objects in images. Detecting faces in an image and where they are, detecting smiles and frowns, detecting cats and dogs and cars, detecting several objects from a single image and characterizing the spatial relationships between them. These algorithms had immediate applications for social networks and camera software, but the more momentous consequence was enabling advances in autonomous vehicles. Autonomous vehicles require more than just seeing and understanding images, and is not yet quite a success. But it’s only a matter of time, and the impact it would have on our world is going to be massive, even by conservative estimates. The effects on transportation is going to be significant, but the effect on jobs and employment is going to be more disruptive and more dangerous. It’s hard to estimate how quickly our societies are going to cope with such a big and quick change.</p><p>In a parallel vein, similar algorithms and techniques were used to make computers hear the spoken word and convert speech into readable, digitized text. Converting speech to text is not quite as good as actually understanding the content of the text though.</p><h3>What does the future hold?</h3><p>General AI is still out of our reach. There are maybe three to five major steps that AI needs to make before general AI becomes within reach. The first such step or building block is <strong>language understanding</strong>. By language understanding I mean the ability to read a paragraph of text and build a representation of its meaning in an actionable way that is comparable to how humans understand language. It is interesting to note that we have been relatively more successful in understanding image than understanding natural language. I attribute that to the fact that images as a representation usually have more redundancy, while language is usually constructed with just enough redundancy to make it discernable to an intelligent human.</p><p>Another step or building block is <strong>long-term strategic planning</strong>. We have a form of that for playing games like chess and go. But achieving the same for general situations where the rules of the game are more ambiguous is still out of reach. Recent breakthroughs have reinforcement learning algorithms play video games without knowing the rules a priori , but just by looking at the screen’s visual information like humans learn to play such games. True, these games are usually tactical and short-term, but I can’t help but feel that we’re getting close to achieving this one.</p><p><strong>Mass coordination and hive intelligence</strong> is not something we often think about when we think about AI. But the potential for machines coordinating on a much grater scale than humans is one of machines’ few edges on humans, and may lead to fundamentally different ways of solving problems than traditional human intelligence would.</p><p>One of the final steps is perhaps developing a <strong>unified system of rewards and incentives</strong>. Or even a system of laws and bounds. This aspect has been in the realm of science fiction for decades and may be the aspect we should be focusing on the most. It’s not a requirement of AI as much as a requirement for avoiding that apocalyptic future where machines take over the world.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=3cb38621383f" width="1" height="1" alt=""><hr><p><a href="https://medium.com/rants-on-machine-learning/so-what-is-machine-learning-good-for-anyway-3cb38621383f">So what is Machine Learning good for anyway?</a> was originally published in <a href="https://medium.com/rants-on-machine-learning">Rants on Machine Learning</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[What to do with “small” data?]]></title>
            <link>https://medium.com/rants-on-machine-learning/what-to-do-with-small-data-d253254d1a89?source=rss-55388a733bf9------2</link>
            <guid isPermaLink="false">https://medium.com/p/d253254d1a89</guid>
            <category><![CDATA[data-science]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[statistics]]></category>
            <dc:creator><![CDATA[Ahmed El Deeb]]></dc:creator>
            <pubDate>Tue, 06 Oct 2015 05:18:39 GMT</pubDate>
            <atom:updated>2015-10-14T12:42:46.153Z</atom:updated>
            <content:encoded><![CDATA[<p>By <a href="https://medium.com/@D33B"><strong>Ahmed El Deeb</strong></a></p><p>Many technology companies now have teams of smart data-scientists, versed in big-data infrastructure tools and machine learning algorithms, but every now and then, a data set with very few data points turns up and none of these algorithms seem to be working properly anymore. What the hell is happening? What can you do about it?</p><h4>Where do small data come from?</h4><p>Most data science, relevance, and machine learning activities in technology companies have been focused around “Big Data” and scenarios with huge data sets. Sets where the rows represent documents, users, files, queries, songs, images, etc. Things that are in the thousands, hundreds of thousands, millions or even billions. The infrastructure, tools, and algorithms to deal with these kinds of data sets have been evolving very quickly and improving continuously during the last decade or so. And most data scientists and machine learning practitioners have gained experience is such situations, have grown accustomed to the appropriate algorithms, and gained good intuitions about the usual trade-offs (bias-variance, flexibility-stability, hand-crafted features vs. feature learning, etc.). But small data sets still arise in the wild every now and then, and often, they are trickier to handle, require a different set of algorithms and a different set of skills. Small data sets arise is several situations:</p><ul><li><strong>Enterprise Solutions:</strong> when you try to make a solution for an enterprise of a relatively limited members instead of a single solution for thousands of users, or if you are making a solution which companies instead of individuals are the focus of the experience</li><li><strong>Time Series:</strong> Time is in short supply! Esp. in comparison with users, queries, sessions, documents, etc. This obviously depends on the time unit or sampling rate, but it’s not always easy to increase the sampling rate effectively, and if your ground truth is a daily number, then you have one data point for each day.</li><li><strong>Aggregate modeling</strong> of states, countries, sports teams, or any situation where the population itself is limited (or sampling is really expensive).</li><li>Modeling of <strong>rare phenomena</strong> of any kind: Earthquakes, floods, etc.</li></ul><h4>Small Data problems</h4><p>Problems of small-data are numerous, but mainly revolve around high variance:</p><ul><li><strong>Over-fitting</strong> becomes much harder to avoid</li><li>You don’t only over-fit to your training data, but sometimes you over-fit to your validation set as well.</li><li><strong>Outliers</strong> become much more dangerous.</li><li>Noise in general becomes a real issue, be it in your target variable or in some of the features.</li></ul><h4>So what to do in these situation?</h4><p><strong>1- Hire a statistician</strong></p><p>I’m not kidding! Statisticians are the original data scientists. The field of statistics was developed when data was much harder to come by, and as such was very aware of small-sample problems. Statistical tests, parametric models, bootstrapping, and other useful mathematical tools are the domain of classical statistics, not modern machine learning. Lacking a good general-purpose statistician, get a marine-biologist, a zoologist, a psychologist, or anyone who was trained in a domain that deals with small sample experiments. The closer to your domain the better. If you don’t want to hire a statistician full time on your team, make it a temporary consultation. But hiring a classically trained statistician could be a very good investment.</p><p><strong>2- Stick to simple models</strong></p><p>More precisely: stick to a limited set of hypotheses. One way to look at predictive modeling is as a search problem. From an initial set of possible models, which is the most appropriate model to fit our data? In a way, each data point we use for fitting down-votes all models that make it unlikely, or up-vote models that agree with it. When you have heaps of data, you can afford to explore huge sets of models/hypotheses effectively and end up with one that is suitable. When you don’t have so many data points to begin with, you need to start from a fairly small set of possible hypotheses (e.g. the set of all linear models with 3 non-zero weights, the set of decision trees with depth &lt;= 4, the set of histograms with 10 equally-spaced bins). This means that you rule out complex hypotheses like those that deal with non-linearity or feature interactions. This also means that you can’t afford to fit models with too many degrees of freedom (too many weights or parameters). Whenever appropriate, use strong assumptions (e.g. no negative weights, no interaction between features, specific distributions, etc.) to restrict the space of possible hypotheses.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ydhgwh2s1gEiSirOxxM4vw.png" /><figcaption>Any crazy model can fit this single data point (drawn from distribution around yellow curve)</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*9chiBwNP9IwtP5Wf12nDTA.png" /><figcaption>As we have more points, less and less models can reasonably explain them together.</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*pxh3I4KVBFMNy5_r-LaJ6w.png" /><figcaption>The figures were taken from Chris Bishop’s book <a href="http://research.microsoft.com/en-us/um/people/cmbishop/prml/webfigs.htm">Pattern Recognition and Machine Learning</a></figcaption></figure><p><strong>3- Pool data when possible</strong></p><p>Are you building a personalized spam filter? Try building it on top of a universal model trained for all users. Are you modeling GDP for a specific country? Try fitting your models on GDP for all countries for which you can get data, maybe using importance sampling to emphasize the country you’re interested in. Are you trying to predict the eruptions of a specific volcano? … you get the idea.</p><p><strong>4- Limit Experimentation</strong></p><p>Don’t over-use your validation set. If you try too many different techniques, and use a hold-out set to compare between them, be aware of the statistical power of the results you are getting, and be aware that the performance you are getting on this set is not a good estimator for out of sample performance.</p><p><strong>5- Do clean up your data</strong></p><p>With small data sets, noise and outliers are especially troublesome. Cleaning up your data could be crucial here to get sensible models. Alternatively you can restrict your modeling to techniques especially designed to be robust to outliers. (e.g. <a href="https://en.wikipedia.org/wiki/Quantile_regression">Quantile Regression</a>)</p><p><strong>6- Do perform feature selection</strong></p><p>I am not a big fan of explicit feature selection. I typically go for regularization and model averaging (next two points) to avoid over-fitting. But if the data is truly limiting, sometimes explicit feature selection is essential. Wherever possible, use domain expertise to do feature selection or elimination, as brute force approaches (e.g. all subsets or greedy forward selection) are as likely to cause over-fitting as including all features.</p><p><strong>7- Do use Regularization</strong></p><p>Regularization is an almost-magical solution that constraints model fitting and reduces the effective degrees of freedom without reducing the actual number of parameters in the model. <strong>L1 regularization</strong> produces models with fewer non-zero parameters, effectively performing implicit feature selection, which could be desirable for explainability of performance in production, while <strong>L2 regularization</strong> produces models with more conservative (closer to zero) parameters and is effectively similar to having strong zero-centered priors for the parameters (in the Bayesian world). L2 is usually better for prediction accuracy than L1.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/616/1*CwnSVcr9rZa3pllDrRP71Q.png" /><figcaption>L1 Regularization can push most model parameters to zero</figcaption></figure><p><strong>8- Do use Model Averaging</strong></p><p>Model averaging has similar effects to regularization is that it reduces variance and enhances generalization, but it is a generic technique that can be used with any type of models or even with heterogeneous sets of models. The downside here is that you end up with huge collections of models, which could be slow to evaluate or awkward to deploy to a production system. Two very reasonable forms of model averaging are Bagging and Bayesian model averaging.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*LQYmYT1t4QqGGnho-zVe0g.png" /><figcaption>Each of the red curves is a model fitted on a few data points</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*-RadM0WAtEHd0HIAxQAawQ.png" /><figcaption>But averaging all these high variance models gets us a smooth output that is remarkably close to the original distribution <a href="http://research.microsoft.com/en-us/um/people/cmbishop/prml/webfigs.htm">Pattern Recognition and Machine Learning</a></figcaption></figure><p><strong>9- Try Bayesian Modeling and Model Averaging</strong></p><p>Again, not a favorite technique of mine, but Bayesian inference may be well suited for dealing with smaller data sets, especially if you can use domain expertise to construct sensible priors.</p><p><strong>10- Prefer Confidence Intervals to Point Estimates</strong></p><p>It is usually a good idea to get an estimate of confidence in your prediction in addition to producing the prediction itself. For regression analysis this usually takes the form of predicting a range of values that is calibrated to cover the true value 95% of the time or in the case of classification it could be just a matter of producing class probabilities. This becomes more crucial with small data sets as it becomes more likely that certain regions in your feature space are less represented than others. Model averaging as referred to in the previous two points allows us to do that pretty easily in a generic way for regression, classification and density estimation. It is also useful to do that when evaluating your models. Producing confidence intervals on the metrics you are using to compare model performance is likely to save you from jumping to many wrong conclusions.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*WoZICSUtmuWhblYksKnoqg.png" /><figcaption>Parts of the feature space are likely to be less covered by your data and prediction confidence within these regions should reflect that</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/949/1*vBLM1uU-N8HTi2X0GURoVw.gif" /><figcaption>Bootstrapped performance charts from <a href="http://rocr.bioinf.mpi-sb.mpg.de/">ROCR</a></figcaption></figure><h4>Summary</h4><p>This could be a somewhat long list of things to do or try, but they all revolve around three main themes: constrained modeling, smoothing and quantification of uncertainty.</p><p>Most figures used in this post were taken from the book <a href="http://research.microsoft.com/en-us/um/people/cmbishop/prml/">“Pattern Recognition and Machine Learning”</a> by <a href="http://research.microsoft.com/en-us/um/people/cmbishop/index.htm">Christopher Bishop</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=d253254d1a89" width="1" height="1" alt=""><hr><p><a href="https://medium.com/rants-on-machine-learning/what-to-do-with-small-data-d253254d1a89">What to do with “small” data?</a> was originally published in <a href="https://medium.com/rants-on-machine-learning">Rants on Machine Learning</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Interview Questions for Data Scientist Positions (Part II)]]></title>
            <link>https://medium.com/rants-on-machine-learning/interview-questions-for-data-scientist-positions-part-ii-ac294c2c7241?source=rss-55388a733bf9------2</link>
            <guid isPermaLink="false">https://medium.com/p/ac294c2c7241</guid>
            <category><![CDATA[data-science]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[statistics]]></category>
            <dc:creator><![CDATA[Ahmed El Deeb]]></dc:creator>
            <pubDate>Tue, 30 Jun 2015 19:50:39 GMT</pubDate>
            <atom:updated>2015-06-30T19:50:39.662Z</atom:updated>
            <content:encoded><![CDATA[<p>In a previous <a href="https://medium.com/p/interview-questions-for-data-scientist-positions-5ad3c5d5b8bd">post</a>, I argued for the need of a different kind of interview questions for data science and machine learning engineers. I then listed some questions I thought good for gauging data science knowledge and cleverness. I extend the list here with some more question:</p><p><strong>On some online stores, you notice reviews for multiple-installment novels follow a peculiar trend that goes slightly up for each installment, even though the number of reviews goes down. What do you think is happening here?</strong></p><p>Selection bias. Plain and simple. Mostly those who liked the first installment (or who are hard-core fans of the author) go on to read subsequent installments, and hence are more likely to have a favorable opinion of the book and the writer in general.</p><p><strong>OK, that makes sense. Now what can we do to de-bias these ratings and get a score on which we can compare novels on an equal footing.</strong></p><p>This could be solved by stratified re-sampling, or by creating a new scoring system that combines average rating with number of reviews into a single score.</p><p><strong>You are chief scientist for the air forces in WW II and you are tasked with making air strikes safer for fighter pilots (i.e. you want more of them to come back). You personally inspect damaged planes after coming back from battle (say 70% of the planes make it back on average, and 20% are damaged). You find that bullet damage is distributed in a highly non-uniform way (e.g. way more bullets in the wings region than is merited by their area). What could be the reason for this? What would you do to make planes less prone.</strong></p><p>This actually happened during WW II and the protagonist was Abraham Wald:</p><p><a href="https://en.wikipedia.org/wiki/Abraham_Wald">Abraham Wald - Wikipedia, the free encyclopedia</a></p><p><strong>The </strong><a href="https://en.wikipedia.org/wiki/United_States_Chess_Federation"><strong>United States Chess Federation</strong></a><strong> (USCF) invites you to devise their new ranking system that will replace </strong><a href="https://en.wikipedia.org/wiki/Elo_rating_system"><strong>Elo</strong></a><strong>. You are free to devise enhancements to the current system or propose a completely new ranking algorithm.</strong></p><p>Many possible ways to do this. makes for a good discussion question. Also whether the candidate decides to extend Elo or to start from scratch tells something about her/his character.</p><p><a href="https://en.wikipedia.org/wiki/Elo_rating_system">Elo rating system - Wikipedia</a></p><p><strong>How would you go about building an ensemble of hundreds of highly diverse models? (resulting from different algorithms and different parameters)</strong></p><p>This opens the room for the candidate to show off knowledge about bagging and boosting and the benefits of each, but for that scale of diverse models to be beneficial, stacking is a natural choice. Essential here is the awareness that stacking requires an extra hold-out set, and to show resourcefulness in adapting the stacking scheme if that holdout set is fairly small.</p><p><strong>How would you sample uniformly from a continuous stream of data?</strong></p><p><a href="https://en.wikipedia.org/wiki/Reservoir_sampling">Reservoir Sampling</a>.</p><p><strong>Assume you already have a classification model with great ROC curve, but the model produces arbitrary scores that do not map to probability estimates, how would you go about calibrating the scores into probabilities?</strong></p><p>There are a few <a href="https://en.wikipedia.org/wiki/Calibration_(statistics)#In_classification">methods</a> for doing that. Interesting to see if the candidate understand that the calibration process biases some metrics on the calibration data set.</p><p><a href="https://en.wikipedia.org/wiki/Isotonic_regression">Isotonic regression - Wikipedia, the free encyclopedia</a></p><p><strong>Explain the bootstrap sampling method and when it can be useful.</strong></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=ac294c2c7241" width="1" height="1" alt=""><hr><p><a href="https://medium.com/rants-on-machine-learning/interview-questions-for-data-scientist-positions-part-ii-ac294c2c7241">Interview Questions for Data Scientist Positions (Part II)</a> was originally published in <a href="https://medium.com/rants-on-machine-learning">Rants on Machine Learning</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Smarter Parameter Sweeps (or Why Grid Search Is Plain Stupid)]]></title>
            <link>https://medium.com/rants-on-machine-learning/smarter-parameter-sweeps-or-why-grid-search-is-plain-stupid-c17d97a0e881?source=rss-55388a733bf9------2</link>
            <guid isPermaLink="false">https://medium.com/p/c17d97a0e881</guid>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[statistics]]></category>
            <category><![CDATA[data-science]]></category>
            <dc:creator><![CDATA[Ahmed El Deeb]]></dc:creator>
            <pubDate>Mon, 22 Jun 2015 08:32:10 GMT</pubDate>
            <atom:updated>2015-06-23T00:17:35.849Z</atom:updated>
            <content:encoded><![CDATA[<p>Anyone that ever had to train a machine learning model had to go through some parameter sweeping (a.k.a. <a href="https://en.wikipedia.org/wiki/Hyperparameter_optimization">hyper-parameter optimization</a>) to find a sweet spot for algorithm parameters. For random forests the parameters in need of optimization could be the number of trees in the model and the number of features considered at each split, for a neural network, there is the learning rate, the number of hidden layers, the number of hidden units in each layer, and several other parameters.</p><p>Hyper-parameter optimization requires the use (and maybe the abuse) of a validation set on which you can’t trust your performance metrics anymore. In this sense it is like a second phase of learning, or an extension to the learning algorithm itself. The performance metric (or the objective function) can be visualized as a heat-map in the n-dimensional parameter-space or as a surface in an n+1-dimensional space (the dimension n+1 being the value of that objective function). The bumpier this surface is (the more local minima and saddle points it has), the harder it becomes to optimize these parameters. Here are a couple of illustrations for two such surfaces defined by two parameters, the first one is mostly well behaved:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/368/1*fOCJ_mAi_-MNb7dfrXjW5A.gif" /></figure><p>While the second one is more bumpy and riddled with several local minima:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/588/1*v-LBGd9GV0wZBJI3Vyqddw.jpeg" /></figure><p>The most common method at selecting algorithm parameters is by far the ubiquitous grid-search. In fact, the word “parameter sweep” actually refers to performing a grid search but has also become synonymous with performing parameter optimization. Grid-search is performed by simply picking a list of values for each parameter, and trying out all possible combinations of these values. This might look methodical and exhaustive. But in truth <strong>even a random search of the parameter space can be MUCH more effective than a grid search!</strong></p><p>This amazing <a href="http://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf">paper</a> by Bergstra et al. claims that a random search of the parameter space is guaranteed to be more effective than grid search (and quite competitive in comparison with more sophisticated techniques).</p><p>Surprising, ha? Why should random search be better than the much more robust-looking grid-search? Here is why:</p><p>The idea is that in most cases the bumpy surface of the objective function is not as bumpy in all dimensions. Some parameters have much less effect on the cost function than others, if the importance of each parameter is known, this can be encoded in the number of values picked for each parameter in the grid-search. But that’s not typically the case, and anyway, just using random search allows the exploration of more values for each parameter, given the same amount of trials:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ZTlQm_WRcrNqL-nLnx6GJA.png" /></figure><p>(The beautiful illustration is taken from the same <a href="http://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf">paper</a> referenced above)</p><p><strong>More elaborate ways of optimizing algorithm hyper-parameters exist, in fact whole start-ups have been built around the idea (</strong><a href="http://techcrunch.com/2015/06/17/twitter-acquires-machine-learning-startup-whetlab/"><strong>one of them recently acquired by twitter</strong></a><strong>). A </strong><a href="https://github.com/HIPS/Spearmint"><strong>couple</strong></a><strong> of </strong><a href="https://github.com/hyperopt/hyperopt"><strong>libraries</strong></a><strong> and </strong><a href="http://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf"><strong>several</strong></a><strong> </strong><a href="http://www.cs.ubc.ca/labs/beta/Projects/SMAC/papers/11-LION5-SMAC.pdf"><strong>research</strong></a><strong> </strong><a href="http://www.ressources-actuarielles.net/EXT/ISFA/1226.nsf/0/f84f7ac703bf5862c12576d8002f5259/$FILE/Jones98.pdf"><strong>papers</strong></a><strong> tackle the problem, but for me, random sweeps are good enough for now.</strong></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c17d97a0e881" width="1" height="1" alt=""><hr><p><a href="https://medium.com/rants-on-machine-learning/smarter-parameter-sweeps-or-why-grid-search-is-plain-stupid-c17d97a0e881">Smarter Parameter Sweeps (or Why Grid Search Is Plain Stupid)</a> was originally published in <a href="https://medium.com/rants-on-machine-learning">Rants on Machine Learning</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Unreasonable Effectiveness of Random Forests]]></title>
            <link>https://medium.com/rants-on-machine-learning/the-unreasonable-effectiveness-of-random-forests-f33c3ce28883?source=rss-55388a733bf9------2</link>
            <guid isPermaLink="false">https://medium.com/p/f33c3ce28883</guid>
            <category><![CDATA[data-science]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[statistics]]></category>
            <dc:creator><![CDATA[Ahmed El Deeb]]></dc:creator>
            <pubDate>Thu, 18 Jun 2015 01:12:08 GMT</pubDate>
            <atom:updated>2015-06-18T01:12:08.490Z</atom:updated>
            <content:encoded><![CDATA[<p>It’s very common for machine learning practitioners to have favorite algorithms. It’s a bit irrational, since no algorithm strictly dominates in all applications, the performance of ML algorithms varies wildly depending on the application and the dimensionality of the dataset. And even for a given problem and a given dataset, any single model will likely be beaten by an ensemble of diverse models trained by diverse algorithms anyway. But people have favorites nevertheless. Some like SVMs for the elegance of their formulation or the quality of the available implementations, some like decision rules for their simplicity and interpretability, and some are crazy about neural networks for their flexibility.</p><p>My favorite out-of-the-box algorithm is (as you might have guessed) the Random Forest, and it’s the second modeling technique I typically try on any given data set (after a linear model).</p><h4>Here’s why:</h4><ul><li><strong>Random Forests require almost no input preparation.</strong> They can handle binary features, categorical features, numerical features without any need for scaling.</li><li><strong>Random Forests perform implicit feature selection</strong> and provide a pretty good indicator of feature importance.</li><li><strong>Random Forests are very quick to train.</strong> It’s a stroke of brilliance when a performance optimization happens to enhance model precision, or vice versa. The random feature sub-setting that aims at diversifying individual trees, is at the same time a great performance optimization! Tuning down the fraction of features that is considered at any given node can let you easily work on datasets with thousands of features. (The same is applicable for row sampling if your dataset has lots of rows)</li><li><strong>Random Forests are pretty tough to beat.</strong> Although you can typically find a model that beats RFs for any given dataset (typically a neural net or some boosting algorithm), it’s never by much, and it usually takes much longer to build and tune said model than it took to build the Random Forest. This is why they make for excellent benchmark models.</li><li><strong>It’s really hard to build a bad Random Forest!</strong> Since random forests are not very sensitive to the specific hyper-parameters used, they don’t require a lot of tweaking and fiddling to get a decent model, just use a large number of trees and things won’t go terribly awry. Most Random Forest implementations have sensible defaults for the rest of the parameters.</li><li><strong>Versatility.</strong> Random Forest are applicable to a wide variety of modeling tasks, they work well for regression tasks, work very well for classification taks(and even produce decently calibrated probability scores), and even though I’ve never tried it myself, they can be used for <a href="http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#cluster">cluster</a> <a href="http://labs.genetics.ucla.edu/horvath/RFclustering/RFclustering/RandomForestHorvath.pdf">analysis</a>.</li><li><strong>Simplicity.</strong> If not of the resulting model, then of the learning algorithm itself. The basic RF learning algorithm can be written in a few lines of code. There’s a certain irony about that. But a sense of elegance as well.</li><li><strong>Lots of excellent, free, and open-source implementations.</strong> You can find a good implementation in almost all major ML libraries and toolkits. <a href="http://cran.r-project.org/web/packages/randomForest/index.html">R</a>, <a href="http://scikit-learn.org/stable/modules/ensemble.html#random-forests">scikit-learn</a> and <a href="http://www.cs.waikato.ac.nz/ml/weka/">Weka</a> jump to mind for having exceptionally good implementations.</li><li>As if all of that is not enough, <strong>Random Forests can be easily grown in parallel.</strong> The same cannot be said about boosted models or large neural networks.</li></ul><p>This beautiful visualization from <a href="http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_iris.html"><strong>scikit-learn</strong></a> illustrates the modelling capacity of a decision forest:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*I8K0qBQ9QeppYjrFgbcoCw.png" /><figcaption>Visualization from <a href="http://scikit-learn.org/">scikit-learn.org</a> illustrating decision boundaries and modeling capacity of a single decision tree, a random forest and some other techniques.</figcaption></figure><h4>Drawbacks?</h4><ul><li>The main drawback of Random Forests is the model size. You could easily end up with a forest that takes hundreds of megabytes of memory and is slow to evaluate.</li><li>Another point that some might find a concern is that random forest models are black boxes that are very hard to interpret.</li></ul><h4>Some References:</h4><p>Here’s a <a href="http://oz.berkeley.edu/~breiman/randomforest2001.pdf">paper</a> by <strong>Leo Breiman</strong>, the inventor of the algorithms describing random forests.</p><p>Here’s another amazing <a href="http://lowrank.net/nikos/pubs/empirical.pdf">paper</a> by <strong>Rich Caruana</strong> et al. evaluating several supervised learning algorithms on many different datasets.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=f33c3ce28883" width="1" height="1" alt=""><hr><p><a href="https://medium.com/rants-on-machine-learning/the-unreasonable-effectiveness-of-random-forests-f33c3ce28883">The Unreasonable Effectiveness of Random Forests</a> was originally published in <a href="https://medium.com/rants-on-machine-learning">Rants on Machine Learning</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Interview Questions for Data Scientist Positions]]></title>
            <link>https://medium.com/rants-on-machine-learning/interview-questions-for-data-scientist-positions-5ad3c5d5b8bd?source=rss-55388a733bf9------2</link>
            <guid isPermaLink="false">https://medium.com/p/5ad3c5d5b8bd</guid>
            <category><![CDATA[data-science]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[statistics]]></category>
            <dc:creator><![CDATA[Ahmed El Deeb]]></dc:creator>
            <pubDate>Mon, 08 Jun 2015 20:02:09 GMT</pubDate>
            <atom:updated>2015-08-03T23:00:22.603Z</atom:updated>
            <content:encoded><![CDATA[<p>There are loads of books on “cracking” the programming interview, and every computer scientist or software engineer has spent some time hunting down and trying to solve interesting interview problems. But the typical interview problems are not any good for assessing the aptitude of a data scientist. I&#39;ve personally seen brilliant programmers and software engineers struggle for years with wrapping their minds around machine learning concepts and statistical analysis techniques. It is clear then that the job interview for a data scientist needs to have questions and problems specifically designed to gauge these abilities.</p><p>These are some questions I came up with when I was asked to conduct interviews for “Research Engineer” positions, please feel free to give feedback and send your own questions to augment this list.</p><h4>Modeling:</h4><p><strong>What is the simplest possible classification model you can learn from data?</strong></p><p>I&#39;ve seen time and again that some ML practitioners are used to using sophisticated algorithms (e.g. SVMs, Gradient Boosted Trees, etc.) and have very tenuous grasp of simpler modeling techniques. I believe this is a critical blind spot. Simple modeling techniques serve as good, solid baselines, are less prone to overfitting and are easier to implement on a large to huge scale in online environments. The simplest classification model that can be learned from data is a simple threshold on a single feature. The next step in complication is a linear model linking the target variable to multiple predictors or a single decision tree. A candidate should be able to write the algorithm to tune any of these models in 10 minutes or so.</p><p><strong>What are your favorite Machine Learning algorithms and why?</strong></p><p>This is an inherently biased question, since every machine learning practitioner has his own set of algorithms and if the candidate’s picks match those of the interviewer, he’ll definitely get his sympathy. But the goal of the question is really the “why” part. Whatever the candidates’ favorite algorithms are, they should be able to justify their choices convincingly. This question can also allow the candidate to show actual passion and enthusiasm about the field, something I believe crucial for the successful data scientist.</p><p><strong>Why is feature selection an important step in modeling and what’s your favorite method of doing it?</strong></p><p>This is kind of a trick question (at least coming from me) since I don’t really believe that feature selection is all that important. Not in most cases anyway. But it’s treated heavily in literature, and I would love to see that the candidate is not just doing things in a certain way because it’s how other people usually do it. Anyway, even if the candidate does believe in the importance of feature selection, the way he would go about it and whether he understands it’s costs would tell a lot about his caliber.</p><p><strong>How do you go about tuning algorithm specific hyper-parameters?</strong></p><p>What I’m looking for here is basically any method smarter than the <a href="https://medium.com/p/smarter-parameter-sweeps-or-why-grid-search-is-plain-stupid-c17d97a0e881">mindless grid-search</a>.</p><p><strong>How do you know that your model is over-fitting and what do you do about it?</strong></p><p>Simple. Straight-forward. Still an essential question.</p><h4>Metrics and experimentation:</h4><p><strong>You inherited a patch of land from your uncle. The first year under your management, land yield goes down to half what it was the prior year, you investigate and find out that you uncle had a secret recipe that he didn’t pass on. There are three possible types of seeds, four types of fertilizers, and two types of pesticide. How would you go about re-discovering you late uncle’s formula?</strong></p><p>Well, … randomized experiments with small land patches assigned randomly to treatments is a good start, including treatments that the lack pesticide and fertilizer, assessing main effects and interactions, getting confidence intervals and possibly comparing finalist treatments in a subsequent round (depending on statistical significance of results), … something along these lines.</p><p><strong>What kind of metrics would you track for you music streaming website?</strong></p><p>No single good answer to this question of course but I’d be looking to assess candidate’s grasp on metrics and their importance and the fact that most metrics have blind spots and how to combine several metrics into one “success” metric and the drawbacks of doing that, and why it might be a good idea to change that metric every now and then, and so on and so forth.</p><p><strong>If you were training a classifier, which metrics would you use for model selection and why?</strong></p><p>How many time have I seen slides filled with precision/recall numbers that were completely useless for comparing models?! For this question I expect either a metric that compares classifier efficacy along the whole score range like area under ROC curve, or at least comparing recall at a preset precision point or something equally sensible.</p><p><strong>You get a weekly spam message predicting the outcome of one football game each week, the spammer claims he has insider information and will let you in on it for a significant fee. You ignore it of course, but you keep getting the weekly message and it keeps guessing the game outcome correctly for 10 weeks in a row, should you pay him? What’s going on here?</strong></p><p>…</p><p><strong>This list is by no means exhaustive, in fact I left whole areas and skills totally un-covered (esp. if I believe the typical programming interview covers it). So I’d love to hear some suggestions to expand this list and make it more rounded.</strong></p><h4><strong><em>(read the </em></strong><a href="https://medium.com/p/interview-questions-for-data-scientist-positions-part-ii-ac294c2c7241"><strong><em>second part</em></strong></a><strong><em> of this article)</em></strong></h4><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=5ad3c5d5b8bd" width="1" height="1" alt=""><hr><p><a href="https://medium.com/rants-on-machine-learning/interview-questions-for-data-scientist-positions-5ad3c5d5b8bd">Interview Questions for Data Scientist Positions</a> was originally published in <a href="https://medium.com/rants-on-machine-learning">Rants on Machine Learning</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[7 Ways to Improve your Predictive Models]]></title>
            <link>https://medium.com/rants-on-machine-learning/7-ways-to-improve-your-predictive-models-753705eba3d6?source=rss-55388a733bf9------2</link>
            <guid isPermaLink="false">https://medium.com/p/753705eba3d6</guid>
            <category><![CDATA[predictive-modeling]]></category>
            <category><![CDATA[data-science]]></category>
            <category><![CDATA[machine-learning]]></category>
            <dc:creator><![CDATA[Ahmed El Deeb]]></dc:creator>
            <pubDate>Thu, 28 May 2015 18:09:59 GMT</pubDate>
            <atom:updated>2015-05-28T18:09:59.005Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*-bdiOLbUBkrbCTIYWilv-g.png" /></figure><p>This is a figure I dug up from an old slide deck I prepared years ago for a workshop on predictive modeling. it illustrates what I think of as the “war horse” of model tuning (‘cause you know, it kind of looks like a horse, with an extra spear). It also is a kind of map for navigating the Bias-Variance space.</p><p><strong>Bias </strong>and <strong>variance </strong>are the two components of imprecision in predictive models, and in general there is a trade-off between them, so normally reducing one tends to increase the other. <strong>Bias</strong> in predictive models is a measure of <strong>model rigidity</strong> and<strong>inflexibility</strong>, and means that your model is not capturing all the signal it could from the data. Bias is also known as <strong>under-fitting</strong>. <strong>Variance</strong> on the other hand is a measure of <strong>model inconsistency</strong>, high variance models tend to perform very well on some data points and really bad on others. This is also known as <strong>over-fitting</strong>and means that your model is <strong>too flexible</strong> for the amount of training data you have and ends up picking up noise in addition to the signal, learning random patterns that happen by chance and do not generalize beyond your training data.</p><p>The simplest way to determine if your model is suffering more from bias or from variance is the following rule of thumb:</p><p><strong>If your model is performing really well on the training set, but much poorer on the hold-out set, then it’s suffering from high variance. On the other hand if your model is performing poorly on both training and test data sets, it is suffering from high bias.</strong></p><p>Depending on the performance of your current model and whether it is suffering more from high bias or high variance, you can resort to one or more of these seven techniques to bring your model where you want it to be:</p><ol><li><strong>Add More Data!</strong> Of course! This is almost always a good idea if you can afford it. It drives variance down (without a trade-off in bias) and allows you to use more flexible models.</li><li><strong>Add More Features!</strong> This is almost always a good idea too. Again, if you can afford it. Adding new features increases model flexibility and <strong>decreases bias</strong>(on the expense of variance). The only time when it’s not a good idea to add new features is when your data set is small in terms of data points and you can’t invest in #1 above.</li><li><strong>Do Feature Selection.</strong> Well, … only do it if you have a lot of features and not enough data points. Feature selection is almost the inverse of #2 above, and pulls your model in the opposite direction (decreasing variance on the expense of some bias) but the trade-off can be good if you do the feature selection methodically and only remove noisy and in-informative features. If you have enough data, most models can automatically handle noisy and uninformative features and you don’t need to do explicit feature selection. In this day and age of “Big Data” the need for explicit feature selection rarely arises. It is also worth noting that proper feature selection is non-trivial and computationally intensive.</li><li><strong>Use Regularization.</strong> This is the neater version of #3 and amounts to implicit feature selection. The specifics are beyond the scope for this post, but regularization tells your algorithm to try to use as few features as possible, or to not trust any single feature too much. Regularization relies on smart implementations of training algorithms and is usually the much preferred version of feature selection.</li><li><a href="http://en.wikipedia.org/wiki/Bootstrap_aggregating">Bagging</a> is short for Bootstrap Aggregation. It uses several versions of the same model trained on slightly different samples of the training data to reduce variance without any noticeable effect on bias. Bagging could be computationally intensive esp. in terms of memory.</li><li><a href="http://en.wikipedia.org/wiki/Boosting_(machine_learning)">Boosting</a> is a slightly more complicated concept and relies on training several models successively each trying to learn from the errors of the models preceding it. Boosting decreases bias and hardly affects variance (unless you are very sloppy). Again the price is computation time and memory size.</li><li><strong>Use a more different class of models!</strong> Of course you don’t have to do all the above if there is another type of models that is more suitable to your data set out-of-the-box. Changing the model class (e.g. from linear model to neural network) moves you to a different point in the space above. Some algorithms are just better suited to some data sets than others. Identifying the right type of models could be really tricky though!</li></ol><p><strong>It should be noted though that model accuracy (being as far to the bottom left as possible) is not the only objective. Some highly accurate models could be very hard to deploy in production environments and are usually black boxes that are very hard to interpret or debug, so many production systems opt for simpler, less accurate model that are less resource-intensive, easier to deploy and debug.</strong></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=753705eba3d6" width="1" height="1" alt=""><hr><p><a href="https://medium.com/rants-on-machine-learning/7-ways-to-improve-your-predictive-models-753705eba3d6">7 Ways to Improve your Predictive Models</a> was originally published in <a href="https://medium.com/rants-on-machine-learning">Rants on Machine Learning</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>