<?xml version='1.0' encoding='UTF-8'?><?xml-stylesheet href="http://www.blogger.com/styles/atom.css" type="text/css"?><feed xmlns='http://www.w3.org/2005/Atom' xmlns:openSearch='http://a9.com/-/spec/opensearchrss/1.0/' xmlns:blogger='http://schemas.google.com/blogger/2008' xmlns:georss='http://www.georss.org/georss' xmlns:gd="http://schemas.google.com/g/2005" xmlns:thr='http://purl.org/syndication/thread/1.0'><id>tag:blogger.com,1999:blog-5825758052688213474</id><updated>2022-11-04T23:36:13.855-07:00</updated><category term="Data science"/><title type='text'>The Unofficial Google Data Science Blog</title><subtitle type='html'></subtitle><link rel='http://schemas.google.com/g/2005#feed' type='application/atom+xml' href='https://www.unofficialgoogledatascience.com/feeds/posts/default'/><link rel='self' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default'/><link rel='alternate' type='text/html' href='https://www.unofficialgoogledatascience.com/'/><link rel='hub' href='http://pubsubhubbub.appspot.com/'/><link rel='next' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default?start-index=26&amp;max-results=25'/><author><name>Amir Najmi</name><uri>http://www.blogger.com/profile/18174523203317227640</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><generator version='7.00' uri='http://www.blogger.com'>Blogger</generator><openSearch:totalResults>33</openSearch:totalResults><openSearch:startIndex>1</openSearch:startIndex><openSearch:itemsPerPage>25</openSearch:itemsPerPage><entry><id>tag:blogger.com,1999:blog-5825758052688213474.post-1836316619446915186</id><published>2021-12-14T22:17:00.000-08:00</published><updated>2021-12-14T22:17:13.243-08:00</updated><title type='text'>Uncertainties: Statistical, Representational, Interventional</title><content type='html'>&lt;p&gt;&amp;nbsp;by AMIR NAJMI &amp;amp; MUKUND SUNDARARAJAN&lt;/p&gt;&lt;br /&gt;&lt;i&gt;Data science is about decision making under uncertainty. Some of that uncertainty is the result of statistical inference, i.e., using a finite sample of observations for estimation. But there are other kinds of uncertainty, at least as important, that are not statistical in nature. This blog post introduces the notions of &lt;b&gt;&lt;u&gt;representational uncertainty&lt;/u&gt;&lt;/b&gt; and &lt;b&gt;&lt;u&gt;interventional uncertainty&lt;/u&gt;&lt;/b&gt; to paint a fuller picture of what the practicing data scientist is up against.&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;Data science and uncertainty&lt;/h2&gt;&lt;div&gt;Data Science (DS) deals with &lt;b&gt;data-driven decision making under uncertainty&lt;/b&gt;. The decisions themselves may range from &quot;how much data center capacity should we build for two years hence?&quot; or &quot;does this product change benefit users?&quot; to the very granular &quot;what content should we recommend to this user at this moment?&quot;&lt;br /&gt;&lt;br /&gt;This kind of decision making must address particular kinds of uncertainty. Wrestling with uncertainty characterizes the unique experience of doing DS. It explains why answering an innocuous question (&quot;Which content verticals have low quality?&quot;) takes longer than what the asking executive thinks it should take (&quot;... and can we have the numbers by next Monday?&quot;).&lt;br /&gt;&lt;br /&gt;In this post, we discuss three types of uncertainty:&lt;br /&gt;&lt;ol style=&quot;text-align: left;&quot;&gt;&lt;li&gt;&lt;b&gt;Statistical uncertainty&lt;/b&gt;: the gap between the &lt;b&gt;estimand&lt;/b&gt;, the unobserved property of the population we wish to measure, and an &lt;b&gt;estimate&lt;/b&gt; of it from observed data.&lt;/li&gt;&lt;li&gt;&lt;b&gt;Representational uncertainty&lt;/b&gt;: the gap between the &lt;b&gt;desired meaning&lt;/b&gt; of some measure and its &lt;b&gt;actual meaning&lt;/b&gt;.&lt;/li&gt;&lt;li&gt;&lt;b&gt;Interventional uncertainty&lt;/b&gt;: the gap between the &lt;b&gt;true benefit&lt;/b&gt; of an intervention arising from a decision, and an&lt;b&gt; evaluation of that benefit&lt;/b&gt;.&lt;/li&gt;&lt;/ol&gt;Among these, only statistical uncertainty has formal recognition. Thus, there is a tendency for data scientists and executives to fixate on it at the expense of the other two. We introduce these other uncertainties to express aspects of the job that go beyond the abstractions of mathematics or the certainties of computation. They involve the judgment to interpret data analysis in the context of the real world. Not being mathematizable, these uncertainties are harder to articulate. And yet it is only in connecting data to meaning and to purpose that we create value. Without new vocabulary, we cannot talk about this crucial element of DS, we cannot teach it, reward it, or hire those who aspire to it.&lt;br /&gt;&lt;br /&gt;This blog post does not discuss objectives. What is the purpose of the organization? What information is meaningful towards that purpose? We assume that answers to these questions are given. Nevertheless, it is our hope that wider use of the terms &quot;representational uncertainty&quot; and &quot;interventional uncertainty&quot; will help you better understand, exercise and express how DS creates value, and recognize when a problem is not a DS problem.&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;Vignette: Data Science at fluff.ai&lt;/h2&gt;The purpose of this post is to dissect and examine uncertainty in order to address it. But it might help the reader see first how the different types of uncertainty we just named may play out in everyday experience. To that end, we present a vignette that is lighthearted but sufficiently realistic as to allow for the requisite reasoning. Thus, we imagine the activities of data scientists at fluff.ai (pronounced &quot;fluff-ay&quot;). The product&#39;s mission is to deliver 20 second videos of &lt;i&gt;&lt;b&gt;the cuddliest critters of all creation™&lt;/b&gt;&lt;/i&gt;. When users watch a  video, they can express their approval by clicking the Like button. There is no &quot;Dislike&quot; button, but users may abandon videos without watching all the way (a harsh judgment on those little tykes). Finally, videos are categorized for easy browsing, such as &quot;Animal Apparel&quot; (animals attired), &quot;Bon Appetit&quot; (animals at mealtime) and &quot;Sweet Dreams&quot; (animals asleep). Recently, the product added the hot new &quot;Just Arrived&quot; category for videos of newborn animals.&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/a/AVvXsEj5OTGM330S3G1dMzFXMLbNu2pBZV3_IPE8KiI1a-cZES94D2z_-VUuIBIdCvHTQeT2tju0aPvoG3n5U04Kr6D0Q9BR1oTHD0KzfkcohrLGItTlBeBdufGDbL0GWert7q4URa8i_chcNQhp_xxlggXMy06OJUhHH8T-VM2HJHTH4PhFbmU6wbUsD5vATA=s500&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;375&quot; data-original-width=&quot;500&quot; height=&quot;480&quot; src=&quot;https://blogger.googleusercontent.com/img/a/AVvXsEj5OTGM330S3G1dMzFXMLbNu2pBZV3_IPE8KiI1a-cZES94D2z_-VUuIBIdCvHTQeT2tju0aPvoG3n5U04Kr6D0Q9BR1oTHD0KzfkcohrLGItTlBeBdufGDbL0GWert7q4URa8i_chcNQhp_xxlggXMy06OJUhHH8T-VM2HJHTH4PhFbmU6wbUsD5vATA=w640-h480&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Figure 1: A video from fluff.ai&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Uncertainty makes data science different from the mere reporting of facts. For instance, the Like Rate of a video is the probability that a viewer will click the Like button for  that video. If three users saw a video and all three liked it, then it is valid to report an observed Like Rate of 100%. But any student of &lt;b&gt;statistical uncertainty&lt;/b&gt; would consider a Like Rate of 100% to be a poor estimate of the true Like Rate.&lt;br /&gt;&lt;br /&gt;&lt;h3 style=&quot;text-align: left;&quot;&gt;A measure of video quality&lt;/h3&gt;Early on in the life of the product, the data scientists at fluff.ai are asked to assess the content quality of their videos. Even the least-experienced among them can intuit that, properly estimated, Like Rate might be a proxy for quality. But they also know that the (true) Like Rate for a video does not equate to the quality of that video. This gap between the quality of a video and its measurement, Like Rate, is &lt;b&gt;representational uncertainty&lt;/b&gt;. In the past, the data scientists had debated this very issue. They went through the many reasons why the Like Rate for a video may be an incomplete measure of the video&#39;s quality. For instance, users who proactively express their appreciation of videos may not be representative of all users. Or perhaps cognitive dissonance prevents users from &quot;liking&quot; a high quality video if its content is sad.&lt;br /&gt;&lt;br /&gt;The DS team needed an explicit definition of video quality. They agreed to base their definition on the evaluation of trained raters. Since Content Quality cannot be defined by formula, raters were given carefully chosen guidelines designed to evoke their individual judgment. For each video, raters were asked to rate how much they agreed with the statement &quot;This video made me feel warm and fuzzy all over&quot; on a five point &lt;a href=&quot;https://en.wikipedia.org/wiki/Likert_scale&quot;&gt;Likert scale&lt;/a&gt;. As per guidelines, this question is meant to capture both the cuteness of the video&#39;s subject, as well as production quality. Instead of trying to convert the categorical rating into a score, they defined a video&#39;s Content Quality as the probability that a trained rater would respond &quot;Agree&quot; or &quot;Strongly Agree&quot; with the rating question. They collected ratings on a broad set of videos and set it up as their gold standard. &lt;br /&gt;&lt;br /&gt;Using human raters is a valid solution to the problem, but there may still be a gap between the full meaning of Content Quality and how each rater interprets the rating question. Moreover, an individual rater&#39;s interpretation may drift over time. But given the simplicity of the rating task and strong inter-rater reliability, the DS team felt they could assume the raters had the right interpretation, more or less. Perhaps they would check for rater drift at some future date.&lt;br /&gt;&lt;br /&gt;Next, they determined the relationship between Like Rate and Content Quality by a whole lot of analysis of different videos in varying contexts. They knew this effort would be worthwhile because Content Quality was likely to be a factor in many important decisions. The legitimacy to read the estimated Like Rate as Content Quality (the desired meaning) was the result of debate and consensus. Most agreed that Like Rate of a video on the site was in fact a reasonable proxy for this definition of video content quality. Their chain of reasoning can be written out as follows:&lt;br /&gt;&lt;ol style=&quot;text-align: left;&quot;&gt;&lt;li&gt;We wish to measure Content Quality.&lt;/li&gt;&lt;li&gt;We start proxying this measurement via a rating task.&lt;/li&gt;&lt;li&gt;We satisfy ourselves that each rater follows the guidelines and uses her judgment appropriately.&lt;/li&gt;&lt;li&gt;We correlate rater judgments with Like Rate, allowing us to use Like Rate at scale.&lt;/li&gt;&lt;/ol&gt;There was one final argument against the need for Like Rate made by a data scientist who argued that one could use the complement of Abandonment Rate as a measure of quality. Abandonment Rate is the number of users who abandon a video divided by the number of users who started to watch the video. And indeed, there is significant (negative) correlation between estimated Like Rate and estimated Abandonment Rate over the population of videos. However, the correlation is weak when restricted to the subset of videos that human raters consider high quality.&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/a/AVvXsEg-cvJqDP0gJ_RNaL1yNjfGOwNDX00Ht1zv6-5a82LLCXirwraNiPMrbi_zlrT1IE2J-Wxzaq1xBFlwugkiGhSlUrjkvWGXlQqut-roEEhhU9Xzau5e17APrRpM068IOcr1BBsjO0B88JWXE_susHUyKNbyy1frxl1pJ-IY_YgsYufCOkdFWXhzNs6TeA=s1950&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1200&quot; data-original-width=&quot;1950&quot; height=&quot;394&quot; src=&quot;https://blogger.googleusercontent.com/img/a/AVvXsEg-cvJqDP0gJ_RNaL1yNjfGOwNDX00Ht1zv6-5a82LLCXirwraNiPMrbi_zlrT1IE2J-Wxzaq1xBFlwugkiGhSlUrjkvWGXlQqut-roEEhhU9Xzau5e17APrRpM068IOcr1BBsjO0B88JWXE_susHUyKNbyy1frxl1pJ-IY_YgsYufCOkdFWXhzNs6TeA=w640-h394&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Figure 2: The relationship between Abandonment Rate and Like Rate&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;It seems Abandonment Rate is sensitive to low quality, while Like Rate is sensitive to high quality. With these caveats in mind, Like Rate became well-established as a measure of Content Quality. Indeed, they had been using Like Rate in fluff.ai&#39;s recommendation system so that it only recommended videos having an estimated Like Rate of at least x%.&lt;br /&gt;&lt;br /&gt;&lt;h3 style=&quot;text-align: left;&quot;&gt;Contemplating a change to the product&lt;/h3&gt;Then one day, the Chief Product Officer tells the DS team that she has an intuition that market reception might benefit from drastically fewer video recommendations (not your typical CPO). She asks the DS team for help to make the decision. After some discussion and initial analysis, they determine that this calls for raising the threshold on Like Rate to $y$%. Their job is now to determine the impact of launching this change. They know they cannot do this perfectly because there will always be something approximate, something speculative about the answer. This gap between the &#39;true&#39; impact of the intervention and its estimated impact is &lt;b&gt;interventional uncertainty&lt;/b&gt;.&lt;br /&gt;&lt;br /&gt;There are several ways to measure interventional uncertainty but data scientists don&#39;t always consider their choices carefully. Tunnel vision may stem from inertia (&quot;We only believe in live experiments&quot;), tool preferences (&quot;I am deeply in love with machine learning&quot;), indolence (&quot;Running a survey is a lot of work. Let&#39;s just use what&#39;s in the logs.&quot;), impatience (&quot;We should have already made  this decision last quarter.&quot;), least resistance (&quot;Easier to do the analysis than explain why it is irrelevant.&quot;) or just plain lack of imagination (&quot;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&quot;). &lt;br /&gt;&lt;br /&gt;Fortunately, the data scientists at fluff.ai are a diligent lot, critical thinkers even.  It turns out, they have some mathematical models for how this might play out, so they could do this in silico. But it is easy enough to run a live experiment for two weeks, so they do that instead. They see an increase in recommendation Uptake Rate (defined as the fraction of recommendations that are accepted; as many as four videos may be recommended at a time), along with an immediate drop in engagement (defined as the number of videos watched per person per day). The increase in Uptake Rate makes sense because they were now recommending fewer videos with higher Like Rate. The drop in engagement also makes sense because there was no relevant video to recommend in many situations.&lt;br /&gt;&lt;br /&gt;Now comes the hard part —&amp;nbsp;interpreting what the results of a brief experiment mean for a decision focused on the long term. They have a model based on past experiments which suggests that an improvement in recommendation Uptake Rate leads to greater engagement in the long term. This is caused by increased user confidence in recommendations, but manifests over a period of several months. According to the model, a fractional change of $\epsilon$ in recommendation Uptake Rate translates into a long-term fractional change in engagement of $\lambda \epsilon$. This is great.&lt;br /&gt;&lt;br /&gt;But wait. The model is based on data from experiments that change the Uptake Rate by small amounts. The change in Uptake Rate of this experiment is larger than any they have tried before. The model may not hold well for such extrapolation. &lt;br /&gt;&lt;br /&gt;However, suppose they were to claim that the product change should not be made because the short-term loss in engagement cannot be made up by the long-term gain. Perhaps they have reason to believe that the effect (i.e., change in long-term engagement due to change in Uptake rate) is sublinear. In this case the model can be interpreted as an upper bound on long-term engagement. This might be good enough to justify the decision. They take the inference back to the CPO that the long-term gains in engagement don&#39;t offset the short-term hit to engagement.&lt;br /&gt;&lt;br /&gt;Moreover, they believe that the quality of recommended videos went down for the Just Arrived category. They explain to her that Uptake Rate in that category has dropped because the system now recommends newborn pandas more broadly, even though only panda enthusiasts (a.k.a  &quot;the Pandanista&quot;) find them cute. &lt;a href=&quot;https://twitter.com/zoobeauval/status/1426202287777763330&quot;&gt;These videos&lt;/a&gt; are almost exclusively watched by the Pandanista who always &quot;like&quot; them. Thus, newborn pandas dominate the high Like Rate subset of this relatively small video category.&lt;br /&gt;&lt;br /&gt;The CPO accepts the analysis but notes that they haven&#39;t taken into account the long-term impact on the brand. Her hypothesis was not about Uptake Rate, but about UI. With very few recommendations, fluff.ai would be a radical departure from the clutter of other similar sites. fluff.ai would attract users from competitors by separating from the pack. But all this may take several months to play out. The data scientists acknowledge that they have no good way to assess this hypothesis. It was now a business decision to decide whether to pursue this radical strategy given the projected hit to metrics.&lt;br /&gt;&lt;br /&gt;&lt;h3 style=&quot;text-align: left;&quot;&gt;Caveat: a more complex reality&lt;/h3&gt;We have painted a picture of the fictitious fluff.ai only in enough detail to motivate the types of uncertainty that are central to this article. For the sake of brevity and focus, there is much that we omit. But the state of the online services industry is such that we feel we must at least acknowledge what our upbeat description of fluff.ai leaves out.&lt;br /&gt;&lt;br /&gt;Online services, even those that deliver short animal videos, may entail unforeseen effects. As a service scales, so too its potential problems. Its very success may attract bad actors who exploit the platform for profit or mischief. For instance, how should fluff.ai handle a cute animal video that prominently displays a commercial brand, making it something of an ad? What if a video for the Bon Appetit category appears to show dogs being fed chocolate or sugar-free gum (i.e., foods harmful to dogs), misinforming the naive? Even without bad actors, a widely-used service may give rise to bad behavior. Thus, &lt;a href=&quot;https://tenor.com/view/dog-sleepy-haggard-gif-5522505&quot;&gt;sleepy&lt;/a&gt; &lt;a href=&quot;https://gifer.com/en/y9O&quot;&gt;animals&lt;/a&gt; may be the cutest (hence fluff.ai&#39;s Sweet Dreams category) but are some pets being chemically sedated or having their &lt;a href=&quot;https://www.google.com/search?q=drowsy+dog&quot;&gt;symptoms of disease&lt;/a&gt; ignored? Finally, runaway success may itself become a societal problem. Apropos, the team at fluff.ai may work hard to deliver the cutest videos, but what if some kids watch them way past their bedtime?&lt;br /&gt;&lt;br /&gt;This is the difficult reality of online services. Anyone involved in building a service needs to pay attention to its effects, both expected and unexpected. Concerned with assessing impact, DS has a particularly important role to play here in defining success and choosing metrics. While this blog post does not address these complexities, no responsible data scientist can afford to ignore them.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;Types of uncertainty&lt;/h2&gt;We now discuss the uncertainties named in the vignette more formally. But first, what is uncertainty? By &quot;uncertainty&quot; we mean absence of a knowable truth, and hence absence of a perfect answer. Even so, there are better and worse answers, and criteria for evaluating their quality. Judiciously, we narrow and manage the gap between the truth and our estimate but we never entirely eliminate it.&lt;br /&gt;&lt;br /&gt;The uncertainties we identify below differ in the nature of the gap, the criteria for evaluating answers, and the strategies a data scientist uses to mitigate the gap.&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3 style=&quot;text-align: left;&quot;&gt;Statistical uncertainty (or the problem of measurement)&lt;/h3&gt;Our definition of uncertainty quite obviously fits with statistical uncertainty where the estimand (population parameter) is unknown and unknowable, and must be estimated from random observation $X$ by an estimator $\hat{\theta}(X)$. The task is to choose from one of many estimators, none of which can guarantee that $\hat{\theta}(X) = \theta$. We want $\theta$ but must settle for $\hat{\theta}(X)$ at the cost of some accuracy:&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/a/AVvXsEj4ytIz_vT_0yDLqqSIzt-GEdQgar41zVnXyzPsNIRWHcuB3bmHpU38dQ7_UMZoc5xv3yhU6qiSnx10qpBVJvhCav9ssBg7VFMcgWyUZsnNIpKVVBNSLuniXMsl47b-0IN2U4MjsX3eu6MSf0PvJrGnM4EA4gF5_wzU3tGzr8xIhB4aGsHZYYoYfmhrBg=s1260&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;416&quot; data-original-width=&quot;1260&quot; height=&quot;66&quot; src=&quot;https://blogger.googleusercontent.com/img/a/AVvXsEj4ytIz_vT_0yDLqqSIzt-GEdQgar41zVnXyzPsNIRWHcuB3bmHpU38dQ7_UMZoc5xv3yhU6qiSnx10qpBVJvhCav9ssBg7VFMcgWyUZsnNIpKVVBNSLuniXMsl47b-0IN2U4MjsX3eu6MSf0PvJrGnM4EA4gF5_wzU3tGzr8xIhB4aGsHZYYoYfmhrBg=w200-h66&quot; width=&quot;200&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;Example: we want the &quot;Like Rate&quot; for any video:&lt;/div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;ul style=&quot;text-align: left;&quot;&gt;&lt;li&gt;Estimand: Like Rate, i.e., probability that a viewer will &quot;like&quot; the video&lt;/li&gt;&lt;li&gt;Estimator: (# of likes + $\alpha$)/(# of viewers + $\beta$), Bayesian for prior Beta($\alpha$, $\beta$)&lt;/li&gt;&lt;/ul&gt;Other estimators are also possible, e.g., Bayesian with a non-parametric prior, and even the Maximum Likelihood estimate, disfavored for its poor small-sample behavior. What is special about statistical uncertainty is that this choice and many criteria for comparison (e.g. &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_squared_error#Estimator&quot;&gt;mean squared error&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Consistency_(statistics)&quot;&gt;consistency&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Efficiency_(statistics)&quot;&gt;efficiency&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Admissible_decision_rule&quot;&gt;admissibility&lt;/a&gt;) have long been formalized and mathematized, much by Fisher himself (e.g. &lt;a href=&quot;https://royalsocietypublishing.org/doi/pdf/10.1098/rsta.1922.0009&quot;&gt;Royal Society 1922&lt;/a&gt;). Should we use a t-test or a sign test? How to compute confidence intervals? The data scientist may choose from the literature, or occasionally extend theory to fashion a custom estimation procedure. The best way to address statistical uncertainty is to study the body of statistical literature. There is plenty out there, so we need say no more.&lt;br /&gt;&lt;br /&gt;Our purpose is to contrast statistical uncertainty with the other forms of uncertainty we introduce. Statistical uncertainty may involve matters of bias and variance in estimating a quantity. But representational uncertainty asks whether we measured the right thing. To what extent does the thing measured mean what we take it to mean? Finally, interventional uncertainty is about evaluating the consequences of a hypothetical intervention. In assessing impact, what cost and benefits do we measure? And how do we conjure the counterfactual scenarios we need for these measurements?&lt;/div&gt;&lt;br /&gt;&lt;h3 style=&quot;text-align: left;&quot;&gt;Representational uncertainty (or the problem of representation)&lt;/h3&gt;Representational uncertainty permeates any effort to represent the state of the world in terms of concepts supported by  data. Recall the vignette: we want a measure of the content quality for any video:&lt;br /&gt;&lt;ul style=&quot;text-align: left;&quot;&gt;&lt;li&gt;Concept: Content Quality of video&lt;/li&gt;&lt;li&gt;Estimand: video&#39;s true Like Rate&lt;/li&gt;&lt;li&gt;Estimator: Bayes estimate of Like Rate, using prior Beta($\alpha$, $\beta$)&lt;/li&gt;&lt;/ul&gt;Another example: we want to represent the country of the user:&lt;br /&gt;&lt;ul style=&quot;text-align: left;&quot;&gt;&lt;li&gt;Concept: country of residence of the user&lt;/li&gt;&lt;li&gt;Estimand: country from which the user is most likely to use fluff.ai &lt;/li&gt;&lt;li&gt;Estimator: country with the most distinct days of activity from the user in the past 28 days&lt;/li&gt;&lt;/ul&gt;We choose a concept because it possesses meanings we desire, and we choose a measure (i.e., estimator for an estimand) because we believe it represents something of those desired meanings. But the concept itself is in our head, not in the data. There is a gap between the messy reality of the data beneath the measure and the idealized meanings we may read into it. (DS is usually involved in identifying the relevant concept, not just how to make it measurable. But here we take the concept as given).&lt;br /&gt;&lt;br /&gt;The relationship between concept and estimand is analogous to the relationship between estimand and estimator — in either case, we seek the former but must settle for the latter. It is convention in statistics to use a Latin symbol for an observed quantity and a Greek symbol for an unobserved estimand, perhaps because the Greek alphabet was an ancestor of the Latin. Given that the Greek alphabet itself descended from Egyptian hieroglyphics, we extend this notion by using an Egyptian hieroglyph to represent the concept. We can write the relationship between all three as follows:&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/a/AVvXsEjN4wJdbfWoVmkOy3G8e_nqnlaWsRbneM8FOYagHK2pjH468Z_H8T49W2zoQZCb-gNYNkU2mB0H0aUOJwPAyVfBPpIO73aXYspsv17FPvYdn_zt22sFADlCFFwKRLvBZdziVgPmj9ivrFf4jQmaLBsLaLVFt1-zM1_H-xTOkHElSzrwUbCPrUytq49kYA=s2320&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;416&quot; data-original-width=&quot;2320&quot; height=&quot;71&quot; src=&quot;https://blogger.googleusercontent.com/img/a/AVvXsEjN4wJdbfWoVmkOy3G8e_nqnlaWsRbneM8FOYagHK2pjH468Z_H8T49W2zoQZCb-gNYNkU2mB0H0aUOJwPAyVfBPpIO73aXYspsv17FPvYdn_zt22sFADlCFFwKRLvBZdziVgPmj9ivrFf4jQmaLBsLaLVFt1-zM1_H-xTOkHElSzrwUbCPrUytq49kYA=w400-h71&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;where 𓂀 (Unicode 13080, &lt;a href=&quot;https://en.wikipedia.org/wiki/Eye_of_Horus&quot;&gt;eye of Horus&lt;/a&gt;) represents the concept. In going from 𓂀 to $\theta$ we lose meaning, while loss in accuracy is incurred in going from $\theta$ to $\hat{\theta}(X)$.&lt;br /&gt;&lt;br /&gt;Perhaps more fundamental than the idea of representational uncertainty is the notion of &lt;b&gt;separating&lt;/b&gt; the estimand, the quantity we seek to estimate from the data, from the concept, the thing in our heads that we would like the estimand to represent.&lt;br /&gt;&lt;br /&gt;As a precedent, consider statistical uncertainty. It is only possible to discuss statistical uncertainty once there is a separation between estimator and estimand. Making this distinction is one of Fisher&#39;s great contributions. Now, we can argue about the &lt;b&gt;accuracy&lt;/b&gt; of the estimator as the (probabilistic) gap between estimate and estimand. Non-statisticians often fail to see the distinction between estimator and estimand and therefore unknowingly fall prey to &quot;noise&quot;. &lt;br /&gt;&lt;br /&gt;With representational uncertainty we make a further distinction between concept and measure (the estimator for an estimand). Now, we can argue about the &lt;b&gt;meaningfulness&lt;/b&gt; of a measure as the (semantic) gap between measure and concept. If we fail to make this distinction, we will unknowingly fall prey to &quot;semantic noise&quot;. Going back to fluff.ai, it would not be possible to discuss the flaws of Like Rate as a measure without articulating what we would want Like Rate to represent (Content Quality).&lt;br /&gt;&lt;br /&gt;Aside from statistical uncertainty, the primary reason why a measure may not have all the meanings we desire from the concept is lack of direct measurement. That is, the relationship between the measure and the concept is indirect and not entirely causal. All else being equal, one might be convinced that true Like Rate is tantamount to Content Quality, but all will rarely be equal for two different videos. &lt;br /&gt;&lt;br /&gt;The observed relationship between measure and concept may only hold in particular contexts, and may not generalize — if we measure the relationship in one data slice it may not extrapolate to others. For instance, videos with higher content quality tend to garner a higher Like Rate, but this isn&#39;t always so. It may also depend on the topic of the video, the subpopulation of its viewers, even differences in user interface between computer and phone (e.g., how prominent is the Like button).  In our second example, there is no direct mechanism to determine a user&#39;s residence country. One resorts to a reasonable heuristic, even though it will be wrong some of the time (e.g., for those who reside in France but commute daily to Switzerland).&lt;/div&gt;&lt;div&gt;&lt;br /&gt;An added challenge is that the world in which the DS team operates is ever in flux. For fluff.ai, the product is changing, how users interact with the product is changing, the needs of the organization are changing, new users are being added, tastes are changing (RIP &lt;a href=&quot;https://en.wikipedia.org/wiki/Grumpy_Cat&quot;&gt;Grumpy Cat&lt;/a&gt;). Many relationships we depend on are not entirely causal, and such changes can disrupt them. For instance, fluff.ai data scientists had found Like Rate to be a good proxy for Content Quality. But that was before the hot new category of Just Arrived videos was added, where this relationship is not as strong (thank you Pandanistas).&lt;br /&gt;&lt;br /&gt;To identify the scope of validity of any concept-measure relationship, we bring all our knowledge of the domain to bear on the analysis. In practice, this means cross-checking against other relevant measures for consistency and monotonicity. For example, fluff.ai primarily compared Like Rate to content quality ratings but they also compared it to Abandonment Rate, a related measure. The relationship between Like Rate and Abandonment Rate was not entirely expected, but it made sense given what these measures are supposed to mean. If Like Rate were not monotone with respect to Abandonment Rate, the team would have been skeptical that Like Rate could represent the concept of Content Quality. We require concepts to be mutually reinforcing. Even so, we can never fully corroborate the relationship between concept and measure. Therefore we will close the gap between them, but never eliminate it.&lt;/div&gt;&lt;/div&gt;&lt;br /&gt;The semantics of any concept we have worked hard to establish are tentative in a world of correlations and change. Thus our approach needs to be defensive —&amp;nbsp;each new inference is partial reconfirmation of what (we think) we already know. It&#39;s a lot to ask, and can only thrive in an environment that values skepticism, in a culture of integrity that doesn&#39;t brush away inconvenient findings. At every step, the data must be able to surprise, to alter beliefs. For example, while evaluating the experiment on raising the Like Rate threshold for recommendations, fluff.ai&#39;s data scientists found the anomalous behavior in the Just Arrived category. This led them to identify the problematic subcategory of newborn panda videos. They could go further and try to define a new concept for the breadth of appeal of a video. This concept might be based on the number of unique video categories visited by those who &quot;liked&quot; the video, and employed in recommendation decisions.&lt;br /&gt;&lt;br /&gt;Thus far, for reasons of clarity, we have limited our treatment of representational uncertainty to a single concept. But in reality, a DS team works towards the creation of a constellation of concepts that relate to one another. This ontology is a quantified representation of the world within which the organization exists and which it seeks to influence. This is why we call it representational uncertainty. How the ontology is managed is beyond the scope of this post, but a possible subject for a future one.&lt;br /&gt;&lt;br /&gt;&lt;h3 style=&quot;text-align: left;&quot;&gt;Interventional uncertainty (or the problem of evaluating an intervention)&lt;/h3&gt;Organizations exist to intervene in the world, and well-run organizations make informed decisions on what they do. Interventions and decisions come in several flavors: they may be bespoke — the typical strategic decision made by executives, or they may be one-of-many — for instance, when we use an experiment framework to enable a class of decisions. In all cases, in deciding whether or not to make a given intervention, the organization&#39;s data scientists evaluate and compare two alternative futures.&lt;br /&gt;&lt;br /&gt;Example from the vignette: Our recommender system currently only recommends videos with a Like Rate hihger than x%. Should we raise this threshold from $x$% to $y$%?&lt;br /&gt;&lt;ul style=&quot;text-align: left;&quot;&gt;&lt;li&gt;Actual net value:&lt;/li&gt;&lt;ul&gt;&lt;li&gt;An immediate improvement in overall recommendation quality, though worse for the Just Arrived category&lt;/li&gt;&lt;li&gt;Long-term increase in user confidence of our recommendations&lt;/li&gt;&lt;li&gt;Long-term change in engagement&lt;/li&gt;&lt;li&gt;Long-term change in brand perception &lt;/li&gt;&lt;/ul&gt;&lt;li&gt;Possible estimate:&lt;/li&gt;&lt;ul&gt;&lt;li&gt;Determine impact from randomized experiment by objective:$$\Delta \mathrm{Short\ term \ Engagement} + \lambda \Delta \mathrm{recommendation \ Uptake \ Rate}$$&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;There are alternate futures depending on whether or not we intervene. Interventional uncertainty is not the standard error of an experiment used to estimate the impact of intervening. Rather, it derives from incompleteness of the evaluation criteria and infidelity of the proxies we use for the alternate futures. Since only one future will be realized, the difference between them is unknowable, its ramifications never to be measured directly. Yet we may still estimate this difference based on the requirements of decision makers and reasonable modeling assumptions. We can think of this as a two step process.&lt;br /&gt;&lt;br /&gt;The first step is to &lt;b&gt;identify measures&lt;/b&gt; that largely capture the value to the organization from the intervention. We must narrow down the all-inclusive notion of organizational value for the specific decision at hand in order to make it measurable. This is how we frame the decision, i.e., identify the logical hypothesis behind it in order to evaluate the decision. For instance, fluff.ai data scientists framed the decision to raise the Like Rate recommendation threshold as a hypothesized long-term increase in engagement, which they can infer in the near term by a change in engagement and a change in recommendation Uptake Rate. Also note that they did not include long-term brand perception, which was the CPO&#39;s motivation.&lt;br /&gt;&lt;br /&gt;The second step is to &lt;b&gt;choose two counterfactual scenarios&lt;/b&gt; that are good proxies for launch versus status quo wherein we can evaluate the difference in these measures we identified in the first step. The choice of measures used to quantify the value of the decision drives the choice of comparison points wherein these can be evaluated. For instance, only after they have a hypothesis for increased long-term engagement based on near term Uptake Rate does it make sense for fluff.ai to run a brief experiment to test the hypothesis. The points of comparison go by various names: &quot;potential outcomes&quot; in statistics, &quot;counterfactuals&quot; in philosophy, &quot;benchmarks&quot; in marketing. DS teams invest in building out methodology to evaluate a variety of counterfactuals, their tools include observational causality, mathematical modeling, simulation, and evermore-sophisticated experimentation.&lt;br /&gt;&lt;br /&gt;Recall how fluff.ai&#39;s DS team was unable to measure the effects of an intervention on long-term brand perception. It might behoove them to build out their capabilities in this area. For instance, they might use experiments on user panels to develop models for long-term behavior based on short-term metrics. Or they might develop the ability to extrapolate from long running experiments in certain small countries (e.g., Finland). Every method for evaluating counterfactuals has limitations, so it is important to develop a range of methods for evaluating different types of interventions. It is the mark of a mature DS team.&lt;br /&gt;&lt;br /&gt;The launch scenario is never perfectly modeled because it is never fully-realized in the data.  Even randomized user experiments are imperfect. They make simplifying assumptions: e.g., the assignment of one user does not affect the behavior of another (&lt;a href=&quot;https://en.wikipedia.org/wiki/Rubin_causal_model#Stable_unit_treatment_value_assumption_(SUTVA)&quot;&gt;SUTVA&lt;/a&gt;); or that the treatment being tested is not affected by seasonality (no one at fluff.ai would expect treatment effects in the Animal Apparel category to generalize if the experiment were run near Halloween). There is always residual interventional uncertainty, i.e., some gap between the true net value of the launch and our estimate; we can close this gap, but not eliminate it. For instance, if fluff.ai launches their product change, it will be harder for quirky videos to find their audience, and thus limit product growth beyond users with mainstream tastes. The framing ignores this since it is not captured in experiments by near-term engagement and recommendation Uptake Rate.&lt;br /&gt;&lt;br /&gt;There is an interaction between the selection of measures, and the selection of counterfactuals. Our choice of counterfactuals can identify effects that were not initially hypothesized. For instance, we can slice the results of a live experiment to discover unanticipated issues (e.g., lower Uptake Rate of the Just Arrived category). Equally, our choice of counterfactuals could constrain the type of effects we can measure. As you saw, fluff.ai was unable to assess the value of long-term brand perception within a two-week randomized experiment. For the same intervention (reducing the number of recommendations by raising the quality bar) it may also be hard to identify the reaction of content creators —&amp;nbsp;do they respond by creating higher quality videos or do they churn?  In general, the net value of all ramifications is hard to estimate.&lt;br /&gt;&lt;br /&gt;Finally, organizational values and ethics address hard-to-estimate ramifications. Often ramifications have far reaching effects beyond the organization, to society at large. Because this kind of impact takes place beyond the product itself, it is harder to represent and to estimate in a controlled manner. Data scientists must not let the difficulty of definitive measurements stand in the way of sensible actions. Yes, we work for the organization, but we also hold values as members of society.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;Summary &lt;/h2&gt;Good DS only happens when we approach analyses with the right sort of skepticism, and this goes beyond just the statistical variety.  &lt;br /&gt;&lt;br /&gt;&lt;b&gt;Representational uncertainty&lt;/b&gt; is about the gap between the desired meaning (e.g. video quality) of a measure (e.g. Like Rate) and its actual meaning. This gap may arise for a variety of reasons such as unstable correlations or feedback loops. Identifying the source of the gap isn&#39;t a mathematical problem —&amp;nbsp;it is about wrestling with the mess of the real world. Because these effects vary over time, taming representational uncertainty is an ongoing process.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Interventional uncertainty&lt;/b&gt; is about the gap between the measured value to the organization of an intervention and its actual value in the real world. This gap arises because no set of measures fully captures the change in organizational value, and because in any measurement of value, one of the two outcomes (the world with and without the intervention) is never realized fully.&lt;br /&gt;&lt;br /&gt;Representation and intervention &lt;a href=&quot;https://www.amazon.com/Representing-Intervening-Introductory-Philosophy-Natural/dp/0521282462&quot;&gt;are of a pair in science&lt;/a&gt;. Perhaps they are more so in DS because the organization that employs DS usually seeks to both understand and change its environment. Thus, its ability to represent and to intervene are coevolving, an aspect of &lt;a href=&quot;https://youtu.be/t8T8bStabq0?t=95&quot;&gt;laying down tracks of representation while deciding where to drive the train of intervention&lt;/a&gt;. What we believe to be the state of the world depends on what we can measure, and what we measure depends on what we believe to be relevant to the state of the world.  The purpose of self-skepticism is to mitigate this circularity.  Business cliches such as the draconian &quot;&lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/S0263237398000292?via%3Dihub&quot;&gt;You are what you measure&lt;/a&gt;&quot; and the &lt;a href=&quot;https://www.linkedin.com/pulse/why-you-cant-manage-what-measure-bad-advice-don-peppers/&quot;&gt;misattributed&lt;/a&gt; &quot;If you can&#39;t measure it, you can&#39;t improve it&quot; don&#39;t give enough credit to the two-way dynamic between representation and intervention in DS. Good representations don&#39;t just &lt;b&gt;enable&lt;/b&gt; desired interventions —&amp;nbsp;they often &lt;b&gt;suggest&lt;/b&gt; how one might intervene. Conversely, interventions may themselves affect the quality of representations by altering relationships on which the representations depend. In the case of objectives, this general phenomenon may show up as Goodhart-Strathern&#39;s Law (&quot;When a measure becomes a target, it ceases to be a good measure&quot;).&lt;br /&gt;&lt;br /&gt;Of course, the work of the data scientist involves more than addressing uncertainties. Nor are we making an exhaustive claim as to the types of uncertainties in DS. Nevertheless, we hope that our description of representational and interventional uncertainty rings true to your DS  experience. We also hope that our formalization helps foster professional awareness, and stops data scientists from unduly focussing on statistical uncertainty.&lt;br /&gt;&lt;br /&gt;Did these new concepts resonate with you? We would love to hear from you in the comment section. &lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;</content><link rel='replies' type='application/atom+xml' href='https://www.unofficialgoogledatascience.com/feeds/1836316619446915186/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='https://www.unofficialgoogledatascience.com/2021/12/uncertainties-statistical.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/1836316619446915186'/><link rel='self' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/1836316619446915186'/><link rel='alternate' type='text/html' href='https://www.unofficialgoogledatascience.com/2021/12/uncertainties-statistical.html' title='Uncertainties: Statistical, Representational, Interventional'/><author><name>Amir Najmi</name><uri>http://www.blogger.com/profile/18174523203317227640</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://blogger.googleusercontent.com/img/a/AVvXsEj5OTGM330S3G1dMzFXMLbNu2pBZV3_IPE8KiI1a-cZES94D2z_-VUuIBIdCvHTQeT2tju0aPvoG3n5U04Kr6D0Q9BR1oTHD0KzfkcohrLGItTlBeBdufGDbL0GWert7q4URa8i_chcNQhp_xxlggXMy06OJUhHH8T-VM2HJHTH4PhFbmU6wbUsD5vATA=s72-w640-h480-c" height="72" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-5825758052688213474.post-6560793314987327113</id><published>2021-04-19T18:46:00.003-07:00</published><updated>2021-04-20T10:23:33.331-07:00</updated><title type='text'>Why model calibration matters and how to achieve it</title><content type='html'>by LEE RICHARDSON &amp;amp; TAYLOR POSPISIL&lt;br /&gt;&lt;br /&gt;&lt;i&gt;Calibrated models make probabilistic predictions that match real world probabilities.  This post explains why calibration matters, and how to achieve it. It discusses practical issues that calibrated predictions solve and presents a flexible framework to calibrate any classifier. Calibration applies in many applications, and hence the practicing data scientist must understand this useful tool.&lt;br /&gt;&lt;/i&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;br /&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;What is calibration?&lt;/h2&gt;At Google we make predictions for a large number of binary events such as “will a user click this ad” or “is this email spam”.  In addition to the raw classification of $Y = 0$/&#39;NotSpam&#39; or $Y = 1$/&#39;Spam&#39; we are also interested in predicting the probability of the binary event $\Pr(Y = 1 | X)$ for some covariates $X$.  One useful property of these predictions is calibration.  To explain, let’s borrow a quote from Nate Silver’s &lt;u&gt;The Signal and the Noise&lt;/u&gt;:&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;blockquote style=&quot;border: none; margin: 0px 0px 0px 40px; padding: 0px;&quot;&gt;&lt;div style=&quot;text-align: left;&quot;&gt;One of the most important tests of a forecast — I would argue that it is the single most important one — is called calibration. Out of all the times you said there was a 40 percent chance of rain, how often did rain actually occur? If, over the long run, it really did rain about 40 percent of the time, that means your forecasts were well calibrated. If it wound up raining just 20 percent of the time instead, or 60 percent of the time, they weren’t.&lt;/div&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;p&gt;Mathematically we can define this as $\hat{p} = \Pr(Y | \hat{p})$, where $\hat{p}$ is the predicted probability given by our classifier.&lt;br /&gt;&lt;br /&gt;While calibration seems like a straightforward and perhaps trivial property, miscalibrated models are actually quite common. For example, some complex models are miscalibrated out-of-the-box, such as &lt;a href=&quot;https://scikit-learn.org/stable/modules/calibration.html&quot;&gt;Random Forests, SVMs, Naive Bayes&lt;/a&gt;, and (modern) &lt;a href=&quot;https://arxiv.org/abs/1706.04599&quot;&gt;neural networks&lt;/a&gt;.  Simpler models, such as logistic regression, will be miscalibrated if the conditional probability doesn’t follow the specified functional form (e.g. a sigmoid). And even if your simpler model is correctly specified, you still might suffer from the curse of dimensionality (see figure 3 in &lt;a href=&quot;https://www.pnas.org/content/116/29/14516&quot;&gt;Candes and Sur&lt;/a&gt;). Things get even more complicated when you bring in regularization, complex reweighting schemes, or &lt;a href=&quot;https://www.cs.cornell.edu/~caruana/niculescu.scldbst.crc.rev4.pdf&quot;&gt;boosting&lt;/a&gt;. Given all of the potential causes of miscalibration, you shouldn’t assume your model will be calibrated.&lt;/p&gt;&lt;br /&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;Why calibration matters&lt;/h2&gt;What are the consequences of miscalibrated models?  Intuitively, you want to have calibration so that you can interpret your estimated probabilities as long-run frequencies. In that sense, the question could be “why wouldn’t you want to be calibrated?”.  But let’s go further and point out some practical reasons why we want our model to be calibrated:&lt;/div&gt;&lt;div&gt;&lt;h3 style=&quot;text-align: left;&quot;&gt;Practical Reason #1: Estimated probabilities allow flexibility&lt;/h3&gt;If you are predicting whether a user will click on an ad with a classifier, it’s useful to be able to rank ads by their probability of being clicked.  This does not require calibration.  But if you want to calculate the expected number of clicks then you need calibrated probabilities.  This expected value can be helpful for simulating the impact of an experiment (does this increase expected clicks enough to merit running an actual experiment?) or may be used directly by not serving ads which don’t have expected revenue greater than their cost.&lt;/div&gt;&lt;div&gt;&lt;h3 style=&quot;text-align: left;&quot;&gt;Practical Reason #2: Model Modularity&lt;/h3&gt;In complex machine learning systems, models depend on each other.  Single classifiers are often inputs into larger systems that make the final decisions.&lt;br /&gt;&lt;br /&gt;For these ML systems, calibration simplifies interaction. Calibration allows each model to focus on estimating its particular probabilities as well as possible. And since the interpretation is stable, other system components don’t need to shift whenever models change.&lt;br /&gt;&lt;br /&gt;For example, let’s say you quantify the importance of an email using a $\Pr(\mbox{Important})$ model.  This is then an input to a $\Pr(\mbox{Spam})$ model, and the $\Pr(\mbox{Spam})$ model decides which emails get flagged as spam. Now the $\Pr(\mbox{Important})$ model becomes miscalibrated and starts assigning too high of  probabilities for emails being important.  In this case, you can just change the threshold for $\Pr(\mbox{Important})$ and the system seems to be back to normal.  However, downstream your $\Pr(\mbox{Spam})$ model just sees the shift and starts under-predicting spam because the upstream importance signal is telling it that they’re likely to be important. The numerical value of the signal became decoupled from the event it was measuring even as the ordinal value remained unchanged. And users may start receiving a lot more spam!&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;Calibration and other considerations&lt;/h2&gt;Calibration is a desirable property, but it is not the only important metric.  Indeed, merely having calibration may not even be helpful for the task at hand.  Consider predicting whether an email is spam.  Assuming 10% of messages are spam, we predict $\Pr(\mathrm{Spam}) = 0.1$ for each individual email.  This is well-calibrated but it doesn’t do anything to help your inbox.&lt;br /&gt;&lt;br /&gt;The problem with the previous example is when we predicted $\Pr(\mathrm{Spam}) = 0.1$&amp;nbsp;&amp;nbsp;we didn’t condition on any covariates.  A model that considers whether the email came from a known-address and predicts $\Pr(\mathrm{Spam}|\mathrm{Known\ Sender}) = 0.01$ and $\Pr(\mathrm{Spam}|\mathrm{Unknown\ Sender}) = 0.4$ could be perfectly calibrated and also more useful.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;To examine the difference between these two models, let’s consider the expected quadratic loss function, which we can decompose as$$&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;E[(\hat{p}-Y)^2] = E[(\hat{p} - \pi(\hat{p}))^2] - E[(\pi(\hat{p}) - \bar{\pi})^2] +&lt;br /&gt; \bar{\pi} (1 -  \bar{\pi})&lt;br /&gt;&lt;br /&gt;$$where $\pi(\hat{p})$ is $\Pr(Y | \hat{p})$ and $\bar{\pi}$ is $\Pr(Y)$.&lt;br /&gt;&lt;br /&gt;Let’s examine each of these terms in the decomposition:&lt;br /&gt;&lt;ul style=&quot;text-align: left;&quot;&gt;&lt;li&gt;$E[(\hat{p} - \pi(\hat{p}))^2]$: This term is &lt;b&gt;calibration&lt;/b&gt;.  If you have a perfectly calibrated classifier this will be zero and deviations from calibration will hurt the loss.&lt;/li&gt;&lt;li&gt;$- E[(\pi(\hat{p}) - \bar{\pi})^2]$: This term is &lt;b&gt;sharpness&lt;/b&gt;.  The further your predictions are from the global average the more you improve the loss.&lt;/li&gt;&lt;li&gt;$\bar{\pi} (1 - \bar{\pi})$: This is the irreducible loss due to uncertainty.&lt;/li&gt;&lt;/ul&gt;This shows why $\Pr(\mathrm{Spam}) = 0.1$&amp;nbsp;isn’t good enough: it optimizes the calibration term, but pays the price in sharpness. And if our model has useful features (known senders are less likely to send spam), the global average model ($\Pr(\mathrm{Spam}) = 0.1$) should have a worse quadratic loss than our model.&lt;br /&gt;&lt;br /&gt;The relationship between calibration and sharpness is complicated.  For instance, we can always coarsen prediction to improve calibration: indeed we can coarsen all the way to the global average and achieve perfect calibration. But is there some intrinsic tradeoff between the two — will calibration always decrease sharpness?&lt;/div&gt;&lt;div&gt;&lt;br /&gt;This depends on the nature of the miscalibration, i.e., whether the model is over or under confident.  Overconfidence occurs when the model is too close to the extremes — it predicts something with 99% when it actually happens at 80%.  This is symmetric — it is also over confident when it predicts something at 1% when it actually happens at 20%.  The ultimate overconfident model would just predict 0s or 1s as probabilities.  The opposite problem occurs when the model is underconfident: the ultimate underconfident model might just predict 0.5 (or the global average) for each observation.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;If the model is overconfident and too far into the tails, we lose sharpness to improve calibration. If models are under confident and not far enough into the tails, we can improve both calibration and sharpness.  In principle, this means you can end up with either a lower or higher quadratic loss (or other loss functions) for finite samples after implementing the calibration methods we discuss below. In practice, we haven’t observed worse performance, in either the quadratic loss or log loss.&lt;br /&gt;&lt;br /&gt;Other important losses we consider are accuracy (the proportion of correct classifications)  and discrimination based metrics like AUC.  These are less affected by calibration because they are only functions of the &lt;a href=&quot;https://blog.revolutionanalytics.com/2016/08/roc-curves-in-two-lines-of-code.html&quot;&gt;ordered probabilities and their labels&lt;/a&gt; (assuming you change your threshold for accuracy appropriately).  We discuss below that we can choose calibration functions which keep accuracy and AUC unchanged.&lt;br /&gt;&lt;br /&gt;This implies that if we care about AUC, but calibration also matters for our application, we can take the shortcut of just picking the best model according to AUC and applying a calibration fix on top of it. In fact, this is exactly our situation in the notifications case-study described in a later section.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;How should practitioners integrate calibration into their workflow? First, you should decide if calibration matters for your application. If calibration matters, our recommendation is to follow the paradigm proposed by &lt;a href=&quot;https://sites.stat.washington.edu/raftery/Research/PDF/Gneiting2007jrssb.pdf&quot;&gt;Gneiting (2007)&lt;/a&gt;: pick the best performing model amongst models that are approximately calibrated, where &quot;approximately calibrated&quot; is discussed in the next section.&lt;/div&gt;&lt;br /&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;How to be calibrated&lt;/h2&gt;&lt;div&gt;At this point, you may be convinced that you want your model to be calibrated.  The question then becomes, how can you achieve calibration? The natural first step is checking whether you have already achieved calibration.  Practically speaking, we are interested in whether your model is calibrated enough.  We can check this by plotting your predicted probability against your empirical probability for some quantile buckets of your data.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-d5IfW5Jill8/YH33ycIJpNI/AAAAAAAAfNw/d5ztLjvAiQQIKHlXdDRH3SFZG5as8qwcwCPcBGAYYCw/s742/blog%2B1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;737&quot; height=&quot;640&quot; src=&quot;https://1.bp.blogspot.com/-d5IfW5Jill8/YH33ycIJpNI/AAAAAAAAfNw/d5ztLjvAiQQIKHlXdDRH3SFZG5as8qwcwCPcBGAYYCw/w636-h640/blog%2B1.png&quot; width=&quot;636&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;div&gt;Miscalibration will be recognizable as a deviation from the diagonal line that represents perfect calibration. Usually an eye-test suffices to diagnose problems (see above) although you could check more formally with hypothesis testing or thresholding calibration specific metrics.&lt;br /&gt;&lt;br /&gt;If you discover that your classifier is miscalibrated, you might want to start fixing the classifier itself. We suggest a different approach: view your classifier as a black box and learn a&amp;nbsp;&lt;b&gt;calibration function&lt;/b&gt;&amp;nbsp;which transforms your output to be calibrated. The reason you don’t want to adjust your classifier directly is it requires adaptation to your specific method. For instance, a random forest classifier will have different problems than a neural network.&lt;br /&gt;&lt;br /&gt;The model-as-black-box perspective assumes that fixing the model is intractable analytically. Instead, we just ignore the model’s internal structure and fix things with a method-agnostic approach. This is the same fruitful perspective taken by the jackknife,&amp;nbsp;&lt;a href=&quot;https://jmlr.csail.mit.edu/papers/volume9/shafer08a/shafer08a.pdf&quot;&gt;conformal prediction&lt;/a&gt;, and&amp;nbsp;&lt;a href=&quot;https://www.unofficialgoogledatascience.com/2015/11/using-empirical-bayes-to-approximate.html&quot;&gt;second-order calibration&lt;/a&gt;. This gives us the advantage that we can rapidly iterate on the model&#39;s structure and features and not have to worry about calibration every time. Of course it comes at the cost of maintaining a separate step, but we’ll show you that calibration functions are not particularly complicated to add/maintain.&lt;br /&gt;&lt;br /&gt;Finally, a frequent discussion topic is the relationship between calibration and slices. Currently we’re only talking about global calibration: $\hat{p} = \Pr(Y | \hat{p})$. You can also have calibration for a particular slice: $\hat{p} = \Pr(Y | \hat{p}, Z)$ for some covariates $Z$. Calibration for large $Z$ is unlikely, as it’s an even bigger ask than global calibration. In fact, the only model that’s perfectly calibrated across all slices is the true model. However, you might have good reasons for wanting calibration on a few select slices.&lt;br /&gt;&lt;br /&gt;Like with global calibration, you can calibrate your model on slices/subsets of data. But if you calibrate across too many slices, things can become as complicated as the original model. To keep things manageable, our recommendation is to calibrate globally, and to calibrate a small number of slices that affect important decisions as needed.&lt;/div&gt;&lt;div&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;How calibration functions work&lt;/h2&gt;A calibration function takes as input the predicted probability $\hat{p}$ and outputs a calibrated probability $p$.  In this way, you can view it as a single input probabilistic classifier: given $\hat{p}$ as the sole covariate, can you predict the true probability $p$?&lt;br /&gt;&lt;br /&gt;Viewed this way, we can start imposing some conditions that will determine how our calibration function is constructed:&lt;br /&gt;&lt;ul style=&quot;text-align: left;&quot;&gt;&lt;li&gt;&lt;b&gt;The calibration function should minimize a strictly &lt;a href=&quot;https://sites.stat.washington.edu/raftery/Research/PDF/Gneiting2007jasa.pdf&quot;&gt;proper scoring rule&lt;/a&gt;&lt;/b&gt;.  Strictly proper scoring rules are loss functions such that the unique minimizer is the true probability distribution.  Log-loss and quadratic loss are two such examples.  This ensures that with enough data our calibration function converges to the true probabilities: $\hat{p} = \Pr(Y | \hat{p})$.  &lt;/li&gt;&lt;li&gt;&lt;b&gt;The calibration function should be strictly monotonic&lt;/b&gt;. It doesn’t feel intuitive to flip predictions if the model suggests one is more likely. Additionally, a monotonic calibration function preserves the ranking of predictions: this means that AUC isn’t affected (indeed you can estimate AUC and train your calibration function on the same data: more on that later).&lt;/li&gt;&lt;li&gt;&lt;b&gt;The calibration function should be flexible&lt;/b&gt;. Miscalibration may not fit a specific parametric form so we need a non-parametric model.&lt;/li&gt;&lt;li&gt;&lt;b&gt;The calibration function needs to be trained on independent data&lt;/b&gt;. Otherwise you might be vulnerable to extreme overfitting: your model could be too confident predicting close to zero and one and then your calibration function makes it even more overconfident.&lt;/li&gt;&lt;li&gt;&lt;b&gt;Calibration functions should be implemented in Tensorflow&lt;/b&gt;. This is an extra Google requirement because many of our important models are implemented in Tensorflow.  Keeping tooling the same allows easy collaboration and coordination.  Also, implementing a calibration function into the same graph as the original model simplifies the training process: you can use &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/stop_gradient&quot;&gt;stop-gradients&lt;/a&gt; to only train the calibration function in stage two.  Similarly it simplifies the serving process as it’s only another layer in the original graph.&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;br /&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;Calibration Methods&lt;/h2&gt;With these requirements in mind, let’s consider some traditional calibration methods.&lt;br /&gt;&lt;br /&gt;The first method is Platt’s scaling which uses a logistic regression as the calibration function.  This is easy to fit, but it violates our requirement for flexibility.  As a parametric function it doesn’t flexibly adapt to more complicated calibration curves.&lt;br /&gt;&lt;br /&gt;Isotonic regression solves this problem by switching from logistic regression to fully nonparametric regression.  This is almost everything we would want from a calibration method.  However, it does have two downsides.  First, it’s not strictly monotonic: the model will output piecewise continuous functions that lead to ties and thus affects AUC.  Second, isotonic regression is hard to fit into Tensorflow.&lt;br /&gt;&lt;br /&gt;With both Platt’s scaling and isotonic regression failing to satisfy all our requirements, we need another method. Taking a step back, it’s clear we simply need a strictly monotonic regression function that is easy to fit in Tensorflow. This gives us two potential candidates: &lt;a href=&quot;https://www.unofficialgoogledatascience.com/2020/11/adding-common-sense-to-machine-learning.html&quot;&gt;Tensorflow Lattice&lt;/a&gt; and &lt;a href=&quot;https://rpubs.com/deleeuw/268327&quot;&gt;I-Splines (ISplines)&lt;/a&gt; (monotonic neural nets are another option, but they have not worked as well in our experience) . Tensorflow Lattice is described in the previous &lt;a href=&quot;https://www.unofficialgoogledatascience.com/2020/11/adding-common-sense-to-machine-learning.html&quot;&gt;blog post&lt;/a&gt; so we will focus on I-Splines here.&lt;div&gt;&lt;br /&gt;&lt;h3 style=&quot;text-align: left;&quot;&gt;I-Spline Calibration&lt;/h3&gt;Splines are piecewise polynomial functions which, amongst other applications, are used to learn nonparametric regression functions. Splines can be constructed as a linear combination of basis functions:$$&lt;br /&gt;&lt;br /&gt;\Pr(y | x) = \sum_{i = 1}^R \beta_i \phi_i(x)&lt;br /&gt;&lt;br /&gt;$$ where the basis functions $\phi_i(x)$ are pre-specified. To fit splines, we learn the weights $\beta_i$.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;The most common spline functions are B-Splines. B-splines are popular for computational purposes, since they can be defined recursively as:$$&lt;br /&gt;&lt;br /&gt;B_{i,1}(x) =&lt;br /&gt;\begin{cases}&lt;br /&gt;1 &amp;amp; t_i \leq x &amp;lt; t_{i+1} \\&lt;br /&gt;0 &amp;amp; \mbox{otherwise}&lt;br /&gt;\end{cases}&lt;br /&gt;\\&lt;br /&gt;B_{i,k+1}(x) = &lt;br /&gt;\frac{x-t_i}{t_{i+k} - t_i} B_{i,k}(x) + &lt;br /&gt;\frac{t_{i+1+k}-x}{t_{i+1+k} - t_{i+1}} B_{i+1,k}(x)&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;$$where $k$ represents the order of the polynomial function, and $T = [t_1, t_2, …, t_k]$ is a set of non-decreasing knots where the piecewise polynomials connect.&lt;div&gt;&lt;br /&gt;B-Splines don&#39;t quite work for calibration functions, since they’re not monotonic. Fortunately, you can make splines monotonic with Integrated Splines, or &lt;a href=&quot;https://en.wikipedia.org/wiki/I-spline&quot;&gt;I-Splines&lt;/a&gt;. The idea behind I-Splines is that, since properly normalized B-Splines only take positive values, the integral of a B-Spline is always increasing. And if we pair increasing functions with positive weights, we achieve monotonicity.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;Less well known, though pointed out by &lt;a href=&quot;https://rpubs.com/deleeuw/268327&quot;&gt;Jan de Leeuw&lt;/a&gt; in an excellent article, is that I-Splines are also easy to compute. De Leeuw shows (section 4.1.2) that you can compute I-Splines directly from B-Splines:$$&lt;br /&gt;&lt;br /&gt;I_{j,m} = \sum_{l=j}^R B_{l,m+1}(x)&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;$$where $R$ is the number of spline functions. This formula shows that we can compute I-Splines using reversed cumulative sums of B-Splines.&lt;br /&gt;&lt;br /&gt;Putting it all together:&lt;br /&gt;&lt;div&gt;&lt;ol style=&quot;text-align: left;&quot;&gt;&lt;li&gt;We can evaluate the basis functions using the closed form expressions for I-splines given in this section.&lt;/li&gt;&lt;li&gt;We achieve monotonicity by restricting our weights to be positive. In practice, we enforce this by optimizing over $\log(\beta_i)$.&lt;/li&gt;&lt;li&gt;We can optimize the weights using a proper scoring rule, such as log loss, or MSE.&lt;/li&gt;&lt;/ol&gt;All of this can be achieved in Tensorflow. And since I-Splines are flexible non-parametric functions, I-Spline calibration meets all of our requirements for a calibration function.&lt;br /&gt;&lt;br /&gt;Now that we understand how calibration functions work, and a few of our options, let’s look at a practical example.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;Case Study:  Notifications Opt Out Model&lt;/h2&gt;At Google, one way we interact with users is through notifications. We want to send users notifications they find valuable, but we don’t want to annoy users with too many notifications.&lt;br /&gt;&lt;br /&gt;To determine which notifications to send, we use models to predict whether or not a user will dismiss, opt out, or otherwise react to a notification in a negative way. Then, we use the predictions to decide whether or not to send the notification by assuring that the probability of negative reactions is minimal. This is sometimes done using a combination of ML model outputs, with coefficients that have been tuned through offline simulations.&lt;br /&gt;&lt;br /&gt;One model that’s particularly challenging is the $\Pr(\mbox{Opt Out})$ model. The $\Pr(\mbox{Opt Out})$&amp;nbsp;model predicts whether a user will opt out of receiving notifications if a particular notification is sent. Since opt outs can be a clear negative signal from a user, large $\Pr(\mbox{Opt Out})$&amp;nbsp;predictions prevent us from sending annoying notifications.&lt;br /&gt;&lt;br /&gt;A difficulty with predicting opt outs is that they’re rare, so the model suffers from class imbalance issues. To improve model performance in class imbalanced problems, a standard trick is re-weighting the data to emphasize examples from the minority class. But re-weighting based on the class changes the distribution of the predicted probabilities, which leads to miscalibration. In this case, calibration functions automatically compensate for scaling issues. An added bonus is that calibration functions work for any re-weighting method, so engineers can quickly iterate on new methods.&lt;br /&gt;&lt;br /&gt;Let’s start by looking at the reliability diagram for various calibration methods. The calibration methods we’ll try here are Platt Scaling, Isotonic Regression, and I-Splines. Also, since this model iteration re-weights the Opt Outs based on a set of features, our baseline method is to invert the weights computed from the features, which we include here as well.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-g7MF60ii-K4/YH33yWCN2PI/AAAAAAAAfN4/vLvEjw3ghtcPWRFIwxaDFhb5WjsS66ppwCPcBGAYYCw/s904/blog%2B2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;904&quot; data-original-width=&quot;862&quot; height=&quot;640&quot; src=&quot;https://1.bp.blogspot.com/-g7MF60ii-K4/YH33yWCN2PI/AAAAAAAAfN4/vLvEjw3ghtcPWRFIwxaDFhb5WjsS66ppwCPcBGAYYCw/w610-h640/blog%2B2.png&quot; width=&quot;610&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;div&gt;This plot shows the flaw in Platt Scaling. In this case, the empirical calibration curve was more complex than a sigmoid, and Platt scaling wasn’t flexible enough to adapt, and converged to essentially a constant function. We also observe that simply inverting the weights leads to miscalibrated predictions. In this case, the smaller predictions under predict, and the larger predictions over-predict. Fortunately, both I-Splines and Isotonic Regression are flexible enough to learn the more complex calibration curve, which leads to calibrated predictions.&lt;br /&gt;&lt;br /&gt;Let’s take a closer look at the calibration functions, which reveals a more about the differences between Isotonic Regression and I-Splines:&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-9mQYhl4KR_M/YH33yQHvphI/AAAAAAAAfN0/qmhRa1HUtvYZqmIQmodNyia54X5piNtaACPcBGAYYCw/s904/blog%2B3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;904&quot; data-original-width=&quot;862&quot; height=&quot;640&quot; src=&quot;https://1.bp.blogspot.com/-9mQYhl4KR_M/YH33yQHvphI/AAAAAAAAfN0/qmhRa1HUtvYZqmIQmodNyia54X5piNtaACPcBGAYYCw/w610-h640/blog%2B3.png&quot; width=&quot;610&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;div&gt;You can see that I-Splines and Isotonic Regression learn essentially the same calibration function. The main difference is that I-Splines are smooth, and Isotonic Regression is piecewise constant.&lt;br /&gt;&lt;br /&gt;Finally, we mentioned earlier that when calibration functions are strictly monotonic, applying them leaves AUC unchanged. We can confirm this observation from the following table, which shows that AUC for Platt Scaling and I-Splines are identical up to 8 decimal places. In this case, Isotonic regression isn’t quite the same due to ties:&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-3MB3iNolhz0/YH8MML1yotI/AAAAAAAAfOA/K933xxQyIL0rg0_9h-vQGmki7_j3VazLwCNcBGAsYHQ/s596/blog%2B4.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;175&quot; data-original-width=&quot;596&quot; src=&quot;https://1.bp.blogspot.com/-3MB3iNolhz0/YH8MML1yotI/AAAAAAAAfOA/K933xxQyIL0rg0_9h-vQGmki7_j3VazLwCNcBGAsYHQ/s16000/blog%2B4.png&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;span style=&quot;font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span id=&quot;docs-internal-guid-873c2650-7fff-4c04-2017-25f4f1847545&quot;&gt;&lt;span style=&quot;font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline;&quot;&gt;In sum, this case study shows how you can use a calibration function to calibrate a tricky real-world problem. It also shows how flexible calibration functions are, since they work for arbitrary re-weighting schemes, extreme scales, and complicated shapes.&lt;br /&gt;&lt;br /&gt;We hope this example has convinced you that when your model gets too complicated, and you want to iterate on methods quickly, it’s ok to stop worrying and use a calibration function.&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;span style=&quot;font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/span&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;&lt;span&gt;&lt;span style=&quot;font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline;&quot;&gt;Conclusion&lt;/span&gt;&lt;/span&gt;&lt;/h2&gt;&lt;span&gt;&lt;span style=&quot;font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline;&quot;&gt;Calibration is an intuitively appealing property, and also has many practical benefits in complex ML systems and real-world environments. Despite its importance, there are many ways in which a model can end up not being calibrated.  Instead of focusing on fixing the model, we can treat the model as a blackbox and achieve calibration with a calibration function.  We can use any 1D monotonic regression function, but two that we’ve found to work well are I-Splines and piecewise linear functions from TF-Lattice.&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;</content><link rel='replies' type='application/atom+xml' href='https://www.unofficialgoogledatascience.com/feeds/6560793314987327113/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='https://www.unofficialgoogledatascience.com/2021/04/why-model-calibration-matters-and-how.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/6560793314987327113'/><link rel='self' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/6560793314987327113'/><link rel='alternate' type='text/html' href='https://www.unofficialgoogledatascience.com/2021/04/why-model-calibration-matters-and-how.html' title='Why model calibration matters and how to achieve it'/><author><name>Amir Najmi</name><uri>http://www.blogger.com/profile/18174523203317227640</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-d5IfW5Jill8/YH33ycIJpNI/AAAAAAAAfNw/d5ztLjvAiQQIKHlXdDRH3SFZG5as8qwcwCPcBGAYYCw/s72-w636-h640-c/blog%2B1.png" height="72" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-5825758052688213474.post-7366705871442221344</id><published>2020-11-17T15:16:00.012-08:00</published><updated>2021-04-19T18:44:09.439-07:00</updated><title type='text'>Adding common sense to machine learning with TensorFlow Lattice</title><content type='html'>by TAMAN NARAYAN &amp;amp; SEN ZHAO&lt;br /&gt;&lt;div&gt;&lt;i&gt;&lt;br /&gt;&lt;/i&gt;&lt;/div&gt;&lt;div&gt;&lt;i&gt;A data scientist is often in possession of domain knowledge which she cannot easily apply to the structure of the model. On the one hand, basic statistical models (e.g. linear regression, trees) can be too rigid in their functional forms. On the other hand, sophisticated machine learning models are flexible in their form but not easy to control. This blog post motivates this problem more fully, and discusses monotonic splines and lattices as a solution. &lt;/i&gt;&lt;i&gt;While the discussion is about methods and applications, the blog also contains pointers to research papers and to the TensorFlow Lattice package that provides an implementation of these solutions.&amp;nbsp;&lt;/i&gt;&lt;i&gt;Authors of this post are part of the team at Google that builds TensorFlow Lattice.&lt;/i&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-kGrxT6z-jtQ/X7Rc3oMiH2I/AAAAAAAAerc/AK1Oc7cI5zMNiLvOg_JWvAYGkcORWJ8JwCNcBGAsYHQ/s754/thumbnail.png&quot; style=&quot;display: none; margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;754&quot; data-original-width=&quot;752&quot; height=&quot;320&quot; src=&quot;https://1.bp.blogspot.com/-kGrxT6z-jtQ/X7Rc3oMiH2I/AAAAAAAAerc/AK1Oc7cI5zMNiLvOg_JWvAYGkcORWJ8JwCNcBGAsYHQ/s320/thumbnail.png&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;&lt;span id=&quot;docs-internal-guid-3532450b-7fff-809f-7df7-c66fc3ec8737&quot;&gt;Introduction&lt;/span&gt;&lt;/h2&gt;&lt;span id=&quot;docs-internal-guid-3532450b-7fff-809f-7df7-c66fc3ec8737&quot;&gt;Machine learning models often behave unpredictably, as data scientists would be the first to tell you. For example, consider the following simple example —&amp;nbsp;fitting a two-dimensional function to predict if someone will pass the bar exam based just on their GPA (grades) and LSAT (a standardized test) using the public dataset (Wightman, 1998).&lt;/span&gt;&lt;/div&gt;&lt;br /&gt;Here is what happened after training a neural network and gradient boosted trees model (Wang and Gupta, 2020):&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-Hk4NmKQI2Wg/X7Wc3kYDGCI/AAAAAAAAers/U_r3Vd-oowQg7LR3mYEwi8UeJJWTNfvCACNcBGAsYHQ/s976/Fig%2BA.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;385&quot; data-original-width=&quot;976&quot; height=&quot;252&quot; src=&quot;https://1.bp.blogspot.com/-Hk4NmKQI2Wg/X7Wc3kYDGCI/AAAAAAAAers/U_r3Vd-oowQg7LR3mYEwi8UeJJWTNfvCACNcBGAsYHQ/w640-h252/Fig%2BA.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;Both models end up penalizing higher grades and higher LSAT in some parts of the input space! That seems wrong, and a quick look at the data distribution explains why we are seeing these erratic results —&amp;nbsp;there just aren’t many data samples outside of the high-LSAT high-GPA quadrant.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-IX9yzPlSt8g/X7RWN2AJTnI/AAAAAAAAerE/YoYSEp8n77Ay4vfMtyHMcfcqy-bMDQWuwCPcBGAYYCw/s640/image%2B2.png&quot; style=&quot;margin-left: 1em; margin-right: 1em; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;331&quot; data-original-width=&quot;640&quot; height=&quot;333&quot; src=&quot;https://1.bp.blogspot.com/-IX9yzPlSt8g/X7RWN2AJTnI/AAAAAAAAerE/YoYSEp8n77Ay4vfMtyHMcfcqy-bMDQWuwCPcBGAYYCw/w640-h333/image%2B2.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;There are a number of reasons why a model like this can, and does, get us in trouble:&lt;br /&gt;&lt;ol style=&quot;text-align: left;&quot;&gt;&lt;li&gt;&lt;span id=&quot;docs-internal-guid-3532450b-7fff-809f-7df7-c66fc3ec8737&quot;&gt;&lt;b&gt;Training-serving skew&lt;/b&gt;: The offline numbers may look great, but what if your model will be evaluated on a different or broader set of examples than those found in the training set? This phenomenon, more generally referred to as “dataset shift” or “distribution shift”, happens all the time in real-world situations. Models are trained on a curated set of examples, or clicks on top-ranked recommendations, or a specific geographical region, and then applied to every user or use case. Curiosities and anomalies in your training and testing data become genuine and sustained loss patterns.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span id=&quot;docs-internal-guid-3532450b-7fff-809f-7df7-c66fc3ec8737&quot;&gt;&lt;b&gt;Bad individual errors&lt;/b&gt;: Models are often judged by their worst behavior —&amp;nbsp;a single egregious outcome can damage the faith that important stakeholders have in the model and even cause serious reputational harm to your business or institution. They can also defy explanation in that there may be no feature value to blame, and therefore no obvious way to fix the problem.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span id=&quot;docs-internal-guid-3532450b-7fff-809f-7df7-c66fc3ec8737&quot;&gt;&lt;b&gt;Violating policy goals&lt;/b&gt;: It is often important for deployed models to uphold certain policy goals in addition to overall performance, such as fairness, ethics, and safety. Policy may require that certain inputs may only ever positively influence the output score. Any model that is unable to make such a guarantee may be rejected for such policy reasons, regardless of its overall accuracy.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;When these concerns loom too large to ignore, data scientists and practitioners will generally adopt one of a few suboptimal strategies. First, they might throw up their hands and pursue a non-machine-learned solution (say, some trusty if-statements). Second, they could fit a simple model such as linear regression that may not perform very well but does behave predictably and understandably. Alternatively, they may enter a long cycle of iteration —&amp;nbsp;collecting more data, cleaning that data, fitting more models, analyzing the results —&amp;nbsp;hoping to end up with a complex model that nonetheless passes a battery of reliability checks.&lt;br /&gt;&lt;br /&gt;We were unsatisfied with this state of affairs and wanted to build on the following insight:&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-pt73bd-LO64/X7RWOa8NCBI/AAAAAAAAerE/5GW-hC24EnoGaQ4NZOBFZeBHhGsua6W1ACPcBGAYYCw/s640/image%2B3.png&quot; style=&quot;margin-left: 1em; margin-right: 1em; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;124&quot; data-original-width=&quot;640&quot; height=&quot;125&quot; src=&quot;https://1.bp.blogspot.com/-pt73bd-LO64/X7RWOa8NCBI/AAAAAAAAerE/5GW-hC24EnoGaQ4NZOBFZeBHhGsua6W1ACPcBGAYYCw/w640-h125/image%2B3.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;span&gt;&lt;br /&gt;&lt;br /&gt;For instance, the problem with the law school example above can be reduced to the requirement that both features be monotonic with respect to the model output. Monotonicity is a very powerful idea and can apply to a huge number of ML contexts and all types of features —&amp;nbsp;be they boolean, categorical (especially ordered categoricals, such as Likert-style responses), and continuous.&lt;br /&gt;&lt;br /&gt;We also realized that many other types of domain knowledge and common sense could be expressed mathematically. For example, consider trying to rate coffee shops (in order to recommend some of them to users) based on how far away they are, how many reviews they have received, and what their average review score is. The strength of the recommendation should decrease monotonically with distance, and increase monotonically with average review score (and maybe also with number of reviews). But we also know more. For example, distance should probably obey a diminishing returns relationship, where each additional kilometer hurts less than the last. We should also trust, or rely on, the average review score more if it is backed by a high number of reviews.&lt;br /&gt;&lt;br /&gt;In this blog post, we describe how we impose common-sense “shape constraints” on complex models. We call these “semantic regularizers”, because they serve the same purpose as traditional regularization strategies —&amp;nbsp;guiding your model towards learning real generalizable correlations instead of noise —&amp;nbsp;but do so in a way that directly takes advantage of your domain knowledge. They give you the right kind of control over your model, leaving it flexible enough to learn while ensuring it behaves in accordance with known facts about the phenomenon.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-lHGYTMRHaj0/X7RWO5dCOgI/AAAAAAAAerQ/EocSxgeETaoHxN3dQJ7ch0EBGh3FUGIRwCPcBGAYYCw/s606/image%2B4.png&quot; style=&quot;margin-left: 1em; margin-right: 1em; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;312&quot; data-original-width=&quot;606&quot; height=&quot;330&quot; src=&quot;https://1.bp.blogspot.com/-lHGYTMRHaj0/X7RWO5dCOgI/AAAAAAAAerQ/EocSxgeETaoHxN3dQJ7ch0EBGh3FUGIRwCPcBGAYYCw/w640-h330/image%2B4.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;br /&gt;Adding such constraints to a model will regularize the model and produce guaranteed model behavior for reliability or policy needs. These constraints also make it easier to summarize and explain the model.  For the law school example, before we even train the model with shape constraints,  we can already tell end-users, “The learned function never penalizes higher GPA and higher LSAT scores.”&amp;nbsp;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-P9TDQoyKKd0/X7Wc30r38SI/AAAAAAAAerw/5HTUugaa548QyFwB1GSuCosKiw82-QJEQCNcBGAsYHQ/s1331/Fig%2BB.png&quot; style=&quot;margin-left: 1em; margin-right: 1em; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;416&quot; data-original-width=&quot;1331&quot; height=&quot;200&quot; src=&quot;https://1.bp.blogspot.com/-P9TDQoyKKd0/X7Wc30r38SI/AAAAAAAAerw/5HTUugaa548QyFwB1GSuCosKiw82-QJEQCNcBGAsYHQ/w640-h200/Fig%2BB.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;We have packaged the solutions we describe in this blog post in the &lt;a href=&quot;https://www.tensorflow.org/lattice/overview&quot;&gt;TensorFlow Lattice&lt;/a&gt; library. TF Lattice offers semantic regularizers that can be applied to models of varying complexity, from simple Generalized Additive Models, to flexible fully interacting models called lattices, to deep models that mix in arbitrary TF and Keras layers.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-U3vSJUVNvrY/X7RWP-0UYbI/AAAAAAAAerU/jo8LHAKWBwAE6Aj5fTR2q2cnjWLDhKcrwCPcBGAYYCw/s640/image%2B6.png&quot; style=&quot;margin-left: 1em; margin-right: 1em; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;265&quot; data-original-width=&quot;640&quot; height=&quot;264&quot; src=&quot;https://1.bp.blogspot.com/-U3vSJUVNvrY/X7RWP-0UYbI/AAAAAAAAerU/jo8LHAKWBwAE6Aj5fTR2q2cnjWLDhKcrwCPcBGAYYCw/w640-h264/image%2B6.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;In the remainder of this blog post, we’ll walk you through how we do shape constrained machine learning, starting from linear functions and moving through more flexible modeling. We end with links to code tutorials and papers with more technical detail.&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;&lt;span&gt;Generalized Additive Models&lt;/span&gt;&lt;/h2&gt;&lt;span&gt;Linear models are a common first step when building models, and have the nice property that they will automatically obey some of the common-sense properties discussed above, such as monotonicity. The assumption of linearity is often too strong, though, leading practitioners to explore transforming their features in various ways. For example, one could apply logarithms or add polynomial terms for certain features.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;A more general approach is to learn a Generalized Additive Model (GAM). GAMs are popular among data science and machine learning applications for their simplicity and interpretability. Formally, GAMs are generalized linear models, in which the model output depends linearly on learned transformations of features, denoted by $c$:$$&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;\hat{y} = \alpha + c_1(x_1) + c_2(x_2) + c_3(x_3) + \cdots + c_k(x_k)&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;$$The transformations of features usually take the form of smoothing splines or local smoothers, which are fit using back-fitting algorithms. When the data is noisy or sparse, however, there is no guarantee that the learned transformations will align with domain knowledge and common sense.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;For example, after we train a one-feature GAM model to recommend coffee shops based on how far they are, we observe the following trend. The model fits training data well, and in feature regions with many training examples (e.g., coffee shops $5$ to $20$ km away), the model performs sensibly. However, the model is questionable when rating coffee shops that are more than $20$ km away, as coffee shops $30$ km away are predicted to be more preferable than ones only $20$ km away. This phenomenon is likely due to noise in the training data at the corresponding feature region. If we deploy this model online, it will likely direct users to farther away coffee shops, hurting user experience.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-9OcRxX8eyhU/X7RWQaGU4TI/AAAAAAAAerQ/0EabtDm8f0M6igpOg5XoOFDNKiQmT-RKQCPcBGAYYCw/s348/image%2B7.png&quot; style=&quot;margin-left: 1em; margin-right: 1em; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;278&quot; data-original-width=&quot;348&quot; height=&quot;320&quot; src=&quot;https://1.bp.blogspot.com/-9OcRxX8eyhU/X7RWQaGU4TI/AAAAAAAAerQ/0EabtDm8f0M6igpOg5XoOFDNKiQmT-RKQCPcBGAYYCw/w400-h320/image%2B7.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;/div&gt;&lt;br /&gt;One may try to apply regularization to smooth the curve, but the questionable performance on far away coffee shops still persists.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-_v-0oAglHTs/X7RWQ89FUvI/AAAAAAAAerM/61dob_pexHcSHhHq4SHhy0_8coCcDl72gCPcBGAYYCw/s348/image%2B8.png&quot; style=&quot;margin-left: 1em; margin-right: 1em; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;279&quot; data-original-width=&quot;348&quot; height=&quot;321&quot; src=&quot;https://1.bp.blogspot.com/-_v-0oAglHTs/X7RWQ89FUvI/AAAAAAAAerM/61dob_pexHcSHhHq4SHhy0_8coCcDl72gCPcBGAYYCw/w400-h321/image%2B8.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;With TF Lattice, we can learn GAMs in which the feature transformations are guaranteed to behave sensibly in ways that align with domain knowledge and common sense. The most prominent of these is monotonic GAMs, in which we constrain the feature transformations to be monotonically increasing or decreasing without otherwise limiting their flexibility. &lt;br /&gt;&lt;br /&gt;The figure below shows the learned trend of a monotonic GAM. This model fits well in regions with sufficient training data. In regions with less training data, the model still performs sensibly, which ensures robust online performance.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-Jiay66NG6uM/X7RWRdghV8I/AAAAAAAAerU/p--9GsX0M-cMRuB2JEFMPXdtpIDOqT8FwCPcBGAYYCw/s348/image%2B9.png&quot; style=&quot;margin-left: 1em; margin-right: 1em; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;278&quot; data-original-width=&quot;348&quot; height=&quot;320&quot; src=&quot;https://1.bp.blogspot.com/-Jiay66NG6uM/X7RWRdghV8I/AAAAAAAAerU/p--9GsX0M-cMRuB2JEFMPXdtpIDOqT8FwCPcBGAYYCw/w400-h320/image%2B9.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;The secret to achieving this guarantee comes in how we pose the problem. TF Lattice uses piecewise-linear functions (PLFs) in its feature transformations. PLFs have two useful properties that we take advantage of. The first is that they are straightforward to optimize using traditional gradient-based optimizers as long as we pre-specify the placement of the knots. If we parameterize the PLF by the&amp;nbsp;values $\theta$ it takes at each of its knots $\kappa$, then$$&lt;br /&gt;&lt;br /&gt;c(x) = \sum_{j=1}^K (\alpha \theta_j + (1-\alpha)\theta_{j+1})&lt;br /&gt;\left[ x \in (\kappa_j, \kappa_{j+1}] \right]&lt;br /&gt;\quad \alpha = \frac{x-\kappa_j}{\kappa_{j+1} - \kappa_j}&lt;br /&gt;$$ where square brackets represents the truth value 0 or 1 of the expression contained.&lt;br /&gt;&lt;br /&gt;If we observe label vector y and feature vectors $x_1, \cdots, x_d$ we can write the differentiable empirical risk minimization problem with a squared loss as$$&lt;br /&gt;&lt;br /&gt;\min_\theta \left\| y - \sum_{j=1}^d c_j(x_j) \right\|^2&lt;br /&gt;&lt;br /&gt;$$Note that we use squared loss for the simplicity of presentation; one can use any differentiable loss in their application.&lt;br /&gt;&lt;br /&gt;Secondly, PLFs have the property that many types of constraints can be written as simple linear inequalities on their parameters. For example, a monotonicity constraint can be succinctly expressed by the constraint set $\{ \theta_i &amp;lt; \theta_{i+1} \}$ while diminishing returns are captured by $\{ \theta_{i+1} - \theta_i &amp;gt; \theta_{i+2} - \theta_{i+1} \}$. There is no convenient way to do this with other popular function classes such as polynomials, particularly in a way that does not overconstrain and rule out viable monotonic functions. For categorical features, unique categories are assigned their own knots and then we learn a “PLF” (really, a 1-d embedding since every value is exactly at a knot). This formulation allows the user to specify any monotonic orderings they want between the categories, or even a total ordering (as with, say, a Likert-scale).&lt;br /&gt;&lt;br /&gt;There is a robust set of tools for working with these kinds of constrained optimization problems. TF Lattice uses a projected stochastic gradient descent (SGD) algorithm in which each gradient step on the objective function is followed by a projection onto the constraints. The projection is performed with &lt;a href=&quot;https://en.wikipedia.org/wiki/Dykstra%27s_projection_algorithm&quot;&gt;Dykstra’s algorithm&lt;/a&gt;, which involves sequential exact projections on subsets of the constraints. This is where the linearity of the constraints is important, as these exact projections are straightforward.&lt;br /&gt;&lt;br /&gt;It is true that PLFs are sometimes viewed as rigid or choppy, with the additional undesirable property of having to preselect good knots. In our case, it turns out that the monotonicity regularizer allows us to increase the number of knots without incurring much risk of overfitting —&amp;nbsp;there are fewer ways for the model to go wrong. More knots make the learned feature transformation smoother and more capable of approximating any monotonic function. As a result, selecting knots according to the quantiles of the input data (or even linearly across the domain), and then steadily increasing their number as&amp;nbsp;long as the metrics improve works well in practice.&lt;br /&gt;&lt;br /&gt;With PLFs, we can even create unique and interesting types of regularization. We can make a learned transformation flatter, making a feature less influential in the final model output, by adding a regularizer on the magnitude of the first-order differences $ \theta_{i+1} - \theta_i $. We can make the transformation more linear, by regularizing the magnitude of the second-order differences $ (\theta_{i+2} - \theta_{i+1}) - (\theta_{i+1} - \theta_i)$. We can even make the learned function smoother, by regularizing the magnitude of the third-order differences! (Can you work out how to express that in terms of $\theta$?)&lt;br /&gt;&lt;br /&gt;The drawback of GAMs is that they do not allow feature interactions. In the next section, we describe lattice models, which allow feature interactions that are guaranteed to align with common sense.&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;Flexible Lattice&amp;nbsp;&lt;/h2&gt;To build shape-constrained models which allow interactions among features, we utilize &lt;a href=&quot;https://jmlr.org/papers/volume17/15-243/15-243.pdf&quot;&gt;lattice models&lt;/a&gt;. Similar to PLF-based GAMs, a lattice model also learns feature transformations that can be constrained to align with common sense (e.g., monotonicity, diminishing returns). Unlike GAMs, which sum up the transformed feature values, a lattice model uses a learned lattice function $l$ to fuse the features that may themselves be transformed by PLFs $c_k$.$$&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;\hat{y} = \alpha + l(c_1(x_1), c_2(x_2), \cdots, c_k(x_k))&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;$$The following image shows the response surface of an example lattice function $l$ with two inputs, RATER CONFIDENCE and RATING. This lattice function is parametrized by four parameters $\theta_1, \cdots, \theta_4$, which define the lattice function output at extreme input values; at other input values, we linearly interpolate from the vertices. As can be seen from the image, the lattice response surface is not a plane, which indicates that unlike GAMs the lattice function can model interactions between features.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-ZlyxMmTn0ZY/X7RWMQPMqjI/AAAAAAAAerA/iffwgRe9BNsz6j4M45ZDOAi272N9ih0swCPcBGAYYCw/s400/image%2B10.png&quot; style=&quot;margin-left: 1em; margin-right: 1em; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;243&quot; data-original-width=&quot;400&quot; height=&quot;243&quot; src=&quot;https://1.bp.blogspot.com/-ZlyxMmTn0ZY/X7RWMQPMqjI/AAAAAAAAerA/iffwgRe9BNsz6j4M45ZDOAi272N9ih0swCPcBGAYYCw/w400-h243/image%2B10.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;We can therefore write a very similar empirical risk minimization problem as in the case of 1-dimensional PLFs; indeed, an astute reader will note that lattices are in some sense a multi-dimensional extension of 1-d PLFs. For simplicity, we stick to the two-feature model described above, where x1 is the rating feature and x2 is the rater confidence feature and we have two knots per dimension set at their min and max values. Assuming that both features are scaled to lie in [0,1], this setup gives rise to the lattice function$$&lt;br /&gt;&lt;br /&gt;l(x_1, x_2) = (1-x_1)(1-x_2)\theta_1 + (1-x_1)x_2\theta_2 + x_1(1-x_2)\theta_3 + x_1 x_2 \theta_4&lt;br /&gt;&lt;br /&gt;$$We similarly write out the lattice function for more features. The empirical risk minimization problem with a squared loss hence becomes$$&lt;br /&gt;&lt;br /&gt;\min_\theta \| y - l(x_1, x_2, \cdots, x_d) \|^2&lt;br /&gt;&lt;br /&gt;$$Note that as we have mentioned in the previous section, one can use any differentiable loss in their application.&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;As with PLFs, the parameterization of lattices allows us to easily constrain their shape to make them align with common sense. For example, to make the RATING feature monotonic, we just need to constrain the model parameters such that$$&lt;br /&gt;&lt;br /&gt;\theta_4 &amp;gt; \theta_2, \theta_3 &amp;gt; \theta_1&lt;br /&gt;&lt;br /&gt;$$We should pause here to note just how powerful these two constraints are. They guarantee that no matter what value RATER CONFIDENCE takes, we remain monotonic as we move along the RATING dimension. This is a substantially more complex task than maintaining monotonicity in the absence of feature interactions, but one that is made possible thanks to our parameterization.&lt;br /&gt;&lt;br /&gt;In addition to the monotonicity constraints, lattice models also allow us to constrain how two features interact. Consider the intuition described above: We want good ratings to “help” more (and bad ratings to “hurt” more) when backed by high rater confidence. We don’t want our model, in contrast, to reward or penalize ratings as strongly when our confidence in them is low. We call this idea trust constraints -- we trust RATING more if RATER CONFIDENCE is higher. Mathematically, it implies that$$&lt;br /&gt;&lt;br /&gt;\theta_4 &amp;gt; \theta_3 &amp;gt; \theta_1 &amp;gt; \theta_2&lt;br /&gt;&lt;br /&gt;$$There are various other pairwise constraints, from Edgeworth complementarity to feature dominance to joint unimodality, that we have &lt;a href=&quot;https://proceedings.icml.cc/static/paper_files/icml/2020/1458-Paper.pdf&quot;&gt;explored&lt;/a&gt; and that can be imposed on lattice models —&amp;nbsp;there simply is not enough room in this post to describe them all. As with monotonicity, all these cases are implemented in the TF Lattice package in such a way that the pairwise interaction is constrained for all possible values of the other features. And as in the case of PLFs, we can impose custom forms of regularization on the model parameters that can make the learned lattice function less reliant on specific features, more linear, or more flat overall.&lt;br /&gt;&lt;br /&gt;The smoothness of the lattice surface may make it seem inflexible —&amp;nbsp;it is not. As with piecewise linear feature transformations, we can add additional knots to increase expressibility. Indeed, lattices are as expressive as deep neural networks (both are universal function approximators). We can also compose PLFs with lattices by first feeding features through one-dimensional feature transformations. In practice, even a low-dimensional lattice model combined with learned feature transformations can achieve high performance. The figure below shows the output surface of a real feature-transformed lattice model used within Google. One of the features is constrained to be monotonic (can you identify it?), and the other unconstrained. The model is smooth, yet flexible. It also aligns with our common sense and hence is more robust.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-vIe5hOPIpr0/X7RWMQct5ZI/AAAAAAAAeq8/ArVt-TE7FHUtcJN53o2Wim-eVpQ34uFTwCPcBGAYYCw/s400/image%2B11.png&quot; style=&quot;margin-left: 1em; margin-right: 1em; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;245&quot; data-original-width=&quot;400&quot; height=&quot;245&quot; src=&quot;https://1.bp.blogspot.com/-vIe5hOPIpr0/X7RWMQct5ZI/AAAAAAAAeq8/ArVt-TE7FHUtcJN53o2Wim-eVpQ34uFTwCPcBGAYYCw/w400-h245/image%2B11.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;At the same time, the richness of the lattice’s expressibility means that we have an exponential number of parameters relative to the number of features. In practice, we adjust for that by fitting ensembles of smaller lattices when working with higher numbers of features. TF Lattice has various mechanisms to help with that process, such as intelligently grouping features with high interactions into a single lattice.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;span&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;Monotonic Deep Lattice Networks&lt;/h2&gt;Deep learning is a powerful tool when we have an abundance of data to learn from. However, it can suffer from the same problems highlighted at the beginning of this post as any other unconstrained model - brittleness in the presence of training-serving skew, the potential of strange predictions in some parts of the feature space, and a general lack of guardrails and guarantees about its behavior.&lt;br /&gt;&lt;br /&gt;In this section, we extend the ideas of building monotonic GAMs and lattice models to construct monotonic deep learning models. Mathematically, the key idea is that PLFs and lattices can be viewed as generic transformations capable of propagating monotonicity inside deeper modeling structures. A monotonic deep learning model is well-regularized, easy to understand, and aligns with common sense. It takes advantage of your domain knowledge to keep the model on track, while maintaining the advantages of modern deep learning, namely, scalability and performance.&lt;br /&gt;&lt;br /&gt;Deep learning models are usually composed of several functions, stacked together. For example, a two layer fully-connected deep learning model is of the form$$&lt;br /&gt;&lt;br /&gt;\hat{y} = f(g(h(x)))&lt;br /&gt;&lt;br /&gt;$$where $f$ and $h$ are fully connected layers, and $g$ is the activation function. Other deep learning models can also be written in this form.&lt;br /&gt;&lt;br /&gt;A sufficient (but not necessary) condition for a deep learning model to be monotonic is that all of its layers are monotonic. One can achieve this by, say, using monotonic activations and constraining all the coefficients in all fully-connected layers to be non-negative. As a result, even if we only want to constrain a subset of features to be monotonic, a monotonic deep learning model with fully-connected layers still needs to constrain all hidden layer weights to be non-negative. As a result, such deep learning models are inflexible, losing much of the benefit of using a deep learning model.&lt;br /&gt;&lt;br /&gt;To address this issue, we propose using monotonic piecewise linear and lattice functions as core components to build monotonic deep learning models. (We will also use “monotonic linear layers”, or linear layers with some coefficients constrained to be non-negative, as a supporting tool.) As we’ve said, both monotonic piecewise linear and lattice functions with enough keypoints are universal approximators of monotonic functions of their respective dimensions, and hence composing them together will produce highly flexible monotonic deep learning models. For lattice functions, we can constrain a subset of its inputs to be monotonic, with the others left unconstrained; we call such a lattice a “partially monotonic lattice”.&lt;br /&gt;&lt;br /&gt;Piecewise-linear and lattice layers can coexist with traditional deep learning structures, as we will show. While lattices are universal function approximators, they are not always the best modeling choice to use with arbitrary data. As mentioned above, lattice layers have an exponential number of parameters in their input dimension, meaning that we will generally use ensembles of lattices in applications. But these can be cumbersome in the presence of high-dimensional sparse features or images for which effective modeling structures already exist and where granular element-wise shape constraints are not appropriate. The key is thinking through composability. When monotonicity is required, we can enforce and propagate it through a model with our layers. When it isn’t, piecewise-linear and lattice layers may still be effective tools, but other layers and approaches can be used as well.&lt;br /&gt;&lt;br /&gt;Viewed through the lens of composability, we can express the GAMs and Lattice models described in the earlier sections as special cases of monotonic modeling. GAMs feed element-wise piecewise-linear layers into a monotonic linear layer, while Lattice models combine piecewise-linear layers with lattice layers. Lattice ensembles, meanwhile, start with piecewise-linear layers and end with lattice layers, but have an intermediate layer to assign inputs to their respective lattices (or optionally, do a “soft” assignment with learned monotonic linear layers).&lt;br /&gt;&lt;br /&gt;In the following sections, we describe several deep learning applications that can be improved by using lattice layers. The main benefit is the ability to employ monotonic and partially-monotonic structures. We also present some examples that take advantage of multi-dimensional shape constraints. We recommend caution when using multi-dimensional constraints in arbitrary deep models, as it can be difficult to think through how such constraints compose end-to-end —&amp;nbsp;on the other hand, monotonicity is much more straightforward. How best to use lattices in deep learning applications is still an open question and an active area of research. The following examples are by no means the only way to utilize TF Lattice layers.&lt;br /&gt;&lt;br /&gt;&lt;/span&gt;&lt;h3 style=&quot;text-align: left;&quot;&gt;Combining Embeddings with TF Lattice Layers&lt;/h3&gt;Let us dig into the case of embedding layers. Sparse categorical features, such as keywords or location ids, are often used in modern machine learning applications, and have proven to be powerful improvements to model performance. For example, if we want to predict the box-office performance of a movie, in addition to dense numeric features, such as the budget of the movie (which likely is a positive signal to the box-office revenue), sparse features such as the names of the director and leading actors can also be useful.&lt;br /&gt;&lt;br /&gt;These sparse features are often handled through high-dimensional embeddings and specialized processing blocks such as fully connected layers, residual networks, or transformers. These offer a huge amount of flexibility and processing power, but also make it easy for dense features to get “lost” in the mix, learning strange correlations with rare sparse features. Recall that lattice layers can be set to make only certain inputs monotonic. These offer a way to keep the dense features monotonic, no matter what values the sparse features take, without having to constrain the relationships learned within the embedding block.&lt;br /&gt;&lt;br /&gt;In addition, the embeddings themselves can be prone to excessive memorization as they are not robust to distribution changes in input features. This can make the resulting models hard to understand, and when they make unreasonable predictions, difficult to pinpoint the cause of the bug. As a fix, we can impose monotonicity on the final outputs of an embedding block as an additional regularization technique —&amp;nbsp;if the resulting outputs of the embedding block are unusually high, we can point to the sparse features as the “reason” for our model output being high.&lt;br /&gt;&lt;br /&gt;The following figure shows an example model structure that we may use for the movie box-office performance problem. It is an example of a model with sparse features, non-monotonic dense features, and monotonic dense features.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-ZsS4fHTH7l0/X7RWMn2-GOI/AAAAAAAAerA/rI-3xqyfWnYnWzM5xNhUGOS9crE_hqm2wCPcBGAYYCw/s640/image%2B12.png&quot; style=&quot;margin-left: 1em; margin-right: 1em; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;397&quot; data-original-width=&quot;640&quot; height=&quot;398&quot; src=&quot;https://1.bp.blogspot.com/-ZsS4fHTH7l0/X7RWMn2-GOI/AAAAAAAAerA/rI-3xqyfWnYnWzM5xNhUGOS9crE_hqm2wCPcBGAYYCw/w640-h398/image%2B12.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;We use an embedding structure to embed the names of the director and leading actors into a small number of real-valued outputs. We also use non-monotonic structures (e.g., fully connected layers) to fuse non-monotonic features (such as length of the movie, season of the premiere) into a few outputs. For those monotonic features (such as the budget of the movie), we fuse them with non-monotonic features using a lattice structure. Finally, the outputs from embedding, non-monotonic and monotonic blocks are monotonically fused in the final monotonic block.&lt;br /&gt;&lt;br /&gt;Note that there is flexibility in how you create those monotonic blocks and structures. Our monotonic linear, piecewise-linear, lattice, and lattice-ensemble layers can be combined with standard monotonic functions like sigmoids in as simple or complex of a manner as you might want. We often build around the lattice-ensemble layer, which can flexibly accept large numbers of inputs. These inputs can be processed with individual PLFs or even just a monotonic linear layer and sigmoid (recall that lattice inputs must be bounded). These blocks can then be stacked. Alternatively, there might be times when a monotonic linear layer, despite its lack of expressiveness, is enough for one part of your model.&lt;br /&gt;&lt;br /&gt;The green paths in the diagram denote monotonic functions, and as a result, the output (yellow circle) is monotonic with respect to those monotonic dense features (green circle), but is not monotonic with respect to other features. By constraining some of the inputs to be monotonic, such a model balances flexibility with regularization. Furthermore, this model is easy to debug. For example, when the prediction of an example is unexpectedly large we can trace back the prediction through visualization of the final block and identify its problematic, unexpectedly large input. This can help us pinpoint the issue in the model or feature.&lt;br /&gt;&lt;br /&gt;A similar idea can be used when combining dense features with other types of input data, such as images or videos. Existing convolutional or residual structures can be used to process the sparse data into a manageable number of real outputs, while lattice layers can be used towards the end of the model to combine with dense features in an interpretable way.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h3 style=&quot;text-align: left;&quot;&gt;Controllable Deep Learning with Spatiotemporal Data&lt;/h3&gt;Spatiotemporal data are often used in forecasting models. We are provided with historical data from the past several time periods, for example, and want to predict outcomes in the following time period. A common approach is to fit a univariate time series model, but there has recently been a focus on building larger machine-learned models that can generalize across time series and take in other high-dimensional inputs.&lt;br /&gt;&lt;br /&gt;Incorporating lattice layers into deep learning models offer a way to build models like this while embedding in them relevant domain knowledge. For example, dominance constraints (which we have not discussed in detail —&amp;nbsp;see paper &lt;a href=&quot;https://proceedings.icml.cc/static/paper_files/icml/2020/1458-Paper.pdf&quot;&gt;Multidimensional Shape Constraints&lt;/a&gt;) can be used to ensure that no matter where we are in the feature space, changing the value of a dominant feature will influence the model more than changing the value of a non-dominant feature. Using these, we can require more recent data to be more influential in our forecast, matching the behavior of common univariate techniques such as exponential smoothing.&lt;br /&gt;&lt;br /&gt;We depict below a simple model that takes in historical revenue data alongside a sparse embedding feature to predict future revenue. Eschewing potential complications such as seasonality or bounce-back effects, we assume that each historical data point is monotonic with respect to our forecast and that recent data should be more important to our model relative to older data. Our resulting model may be easier to explain —&amp;nbsp;and generalize better —&amp;nbsp;than one which simply threw in all of the inputs into a neural network.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-LsSUi_uMX0w/X7RWMzpQaPI/AAAAAAAAerE/zHdSJxlPbhYdggcSIIjbPkRhKZcz2xHfACPcBGAYYCw/s640/image%2B13.png&quot; style=&quot;margin-left: 1em; margin-right: 1em; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;368&quot; data-original-width=&quot;640&quot; height=&quot;368&quot; src=&quot;https://1.bp.blogspot.com/-LsSUi_uMX0w/X7RWMzpQaPI/AAAAAAAAerE/zHdSJxlPbhYdggcSIIjbPkRhKZcz2xHfACPcBGAYYCw/w640-h368/image%2B13.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h3 style=&quot;text-align: left;&quot;&gt;Controllable Deep Learning with a Set of Features&lt;/h3&gt;Consider the task of predicting the quality of a restaurant using user review ratings for that given restaurant. For each user review rating, we also have information regarding the corresponding reviewer, such as the number of reviews given and average rating. This is a non-classical machine learning task in the sense that the model features (user review ratings, number of reviews by each reviewer, average rating by reviewer) are of variable length —&amp;nbsp;for different restaurants, we will have different number of reviews. &lt;br /&gt;&lt;br /&gt;A simple approach might be to take a weighted mean of some sort. We could average reviewer scores while giving greater weight to experienced reviewers (who may be more trustworthy) and adjust for reviewers who are prone to giving high or low scores across the board. We could tweak these weights and adjustments to better correlate with some “golden” quality label.&lt;br /&gt;&lt;br /&gt;But what if we want a machine learning model that directly took in these variable length features? What if we still wished to maintain key properties of a weighted mean such as monotonicity on individual review scores, and to accord greater trust to the scores of experienced reviewers?&lt;br /&gt;&lt;br /&gt;As introduced in Zaheer et al. (2017), we could use the following model to learn a “set function”, i.e., a function that acts on an unordered set of fixed-dimensional inputs in a permutation invariant way$$&lt;br /&gt;&lt;br /&gt;\hat{y} = \rho\left( \sum_{m=1}^M \phi(x_m) \right)&lt;br /&gt;&lt;br /&gt;$$where $x_m$are the features for the $m$-th review, while $\phi$ and $\rho$ are generic deep learning models ($\phi$ may have multiple outputs).  We extend this model with the following 6-layer structure with monotonicity constraints (Cotter et al., 2019). Absent monotonicity, this structure is proven to be flexible enough to learn any function on the variable length features. Here, our structure additionally constrains the user rating feature to be monotonic —&amp;nbsp;if one user increases his/her rating on the restaurant, our predicted restaurant quality score will only increase. We can optionally add trust constraints to make sure that user ratings from experienced reviewers are more influential in the final model output. For more details, see &lt;a href=&quot;http://proceedings.mlr.press/v97/cotter19a/cotter19a.pdf&quot;&gt;Shape Constraints for Set Functions&lt;/a&gt; (Cotter et al., 2019).&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-JYU4lyDsKrw/X7RWNWZRv4I/AAAAAAAAerQ/fgS6wxb0ylI-AdhEVNiVXLKMU7eTMHHPQCPcBGAYYCw/s640/image%2B14.png&quot; style=&quot;margin-left: 1em; margin-right: 1em; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;360&quot; data-original-width=&quot;640&quot; height=&quot;360&quot; src=&quot;https://1.bp.blogspot.com/-JYU4lyDsKrw/X7RWNWZRv4I/AAAAAAAAerQ/fgS6wxb0ylI-AdhEVNiVXLKMU7eTMHHPQCPcBGAYYCw/w640-h360/image%2B14.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;Discussion&lt;/h2&gt;As data scientists, we’ve all been there —&amp;nbsp;scratching our head at unexpected model outputs, or witnessing a seemingly promising model struggle when deployed in the real world. We believe TensorFlow Lattice takes a big step forward in building high-performance models that align with common sense, satisfy policy constraints, and ultimately deliver peace-of-mind. We are particularly excited about the broad applicability of shape-constrained models because principles such as monotonicity are intuitive and pop up repeatedly across domains and use cases.&lt;br /&gt;&lt;br /&gt;This blog post is only a primer of the problems we encountered and the solutions at which we have arrived. &lt;a href=&quot;https://www.tensorflow.org/lattice&quot;&gt;TF Lattice&lt;/a&gt; includes many more constraints, regularizers, and techniques to solve other problems that a practitioner may face, and we continue to improve it. We encourage you to take a look at our tutorials and colabs on TF Lattice &lt;a href=&quot;https://www.tensorflow.org/lattice/tutorials/keras_layers&quot;&gt;Keras layers&lt;/a&gt;, &lt;a href=&quot;https://www.tensorflow.org/lattice/tutorials/premade_models&quot;&gt;Keras premade models&lt;/a&gt;, &lt;a href=&quot;https://www.tensorflow.org/lattice/tutorials/canned_estimators&quot;&gt;canned estimators&lt;/a&gt;, &lt;a href=&quot;https://www.tensorflow.org/lattice/tutorials/custom_estimators&quot;&gt;custom estimators&lt;/a&gt;,&amp;nbsp;&lt;a href=&quot;https://www.tensorflow.org/lattice/tutorials/shape_constraints&quot;&gt;shape constraints&lt;/a&gt;, &lt;a href=&quot;https://www.tensorflow.org/lattice/tutorials/shape_constraints_for_ethics&quot;&gt;ethical constraints&lt;/a&gt; and &lt;a href=&quot;https://www.tensorflow.org/lattice/tutorials/aggregate_function_models&quot;&gt;set functions&lt;/a&gt;. We hope TF Lattice will prove useful to you in your real-world applications.&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;References&lt;/h2&gt;&lt;br /&gt;Wightman, L. LSAC National Longitudinal Bar Passage Study. Law School Admission Council, 1998.&lt;br /&gt;&lt;br /&gt;Garcia, E. and Gupta, M. (2009). Lattice Regression. In Advances in Neural Information Processing Systems 22, pages 594-602.&lt;br /&gt;&lt;br /&gt;Gupta, M., Cotter, A., Pfeifer, J., Voevodski, K., Canini, K., Mangylov, A., Moczydlowski, W. and van Esbroeck, A. (2016). Monotonic Calibrated Interpolated Look-Up Tables. Journal of Machine Learning Research, 17: 1-47.&lt;br /&gt;&lt;br /&gt;Milani Fard, M., Canini, K., Cotter, A., Pfeifer, J. and Gupta, M. (2016). Fast and Flexible Monotonic Functions with Ensembles of Lattices. In Advances in Neural Information Processing Systems 29, pages 2919-2927.&lt;br /&gt;&lt;br /&gt;You, S., Ding, D., Canini, K., Pfeifer, J. and Gupta, M. (2017). Deep Lattice Networks and Partial Monotonic Functions. In Advances in Neural Information Processing Systems 30, pages 2981-2989.&lt;br /&gt;&lt;br /&gt;Zaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B., Salakhutdinov, R. R. and Smola, A. J. (2017). Deep Sets. In Advances in Neural Information Processing Systems 30, pages 3391-3401.&lt;br /&gt;&lt;br /&gt;Cotter, A., Gupta, M., Jiang, H., Louidor, E., Muller, J., Narayan, T., Wang, S. and Zhu, T. (2019). Shape Constraints for Set Functions. In Proceedings of the 36th International Conference on Machine Learning, pages 1388-1396.&lt;br /&gt;&lt;br /&gt;Gupta, M. R., Louidor, E., Mangylov, O., Morioka, N., Narayan, T. and Zhao, S. (2020). Multidimensional Shape Constraints. In Proceedings of the 37th International Conference on Machine Learning, to appear.&lt;br /&gt;&lt;br /&gt;Wang, S., Gupta, M. (2020). Deontological Ethics by Monotonicity Shape Constraints. In Proceedings of the 23rd International Conference on Artificial Intelligence and Statistics, pages 2043-2054.</content><link rel='replies' type='application/atom+xml' href='https://www.unofficialgoogledatascience.com/feeds/7366705871442221344/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='https://www.unofficialgoogledatascience.com/2020/11/adding-common-sense-to-machine-learning.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/7366705871442221344'/><link rel='self' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/7366705871442221344'/><link rel='alternate' type='text/html' href='https://www.unofficialgoogledatascience.com/2020/11/adding-common-sense-to-machine-learning.html' title='Adding common sense to machine learning with TensorFlow Lattice'/><author><name>Amir Najmi</name><uri>http://www.blogger.com/profile/18174523203317227640</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-kGrxT6z-jtQ/X7Rc3oMiH2I/AAAAAAAAerc/AK1Oc7cI5zMNiLvOg_JWvAYGkcORWJ8JwCNcBGAsYHQ/s72-c/thumbnail.png" height="72" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-5825758052688213474.post-7354704051404876351</id><published>2020-07-22T18:46:00.003-07:00</published><updated>2020-10-09T10:57:59.913-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Data science"/><title type='text'>Changing assignment weights with time-based confounders</title><content type='html'>by ALEXANDER WAKIM&lt;br /&gt; &lt;br /&gt;&lt;i&gt; Ramp-up and multi-armed bandits (MAB) are common strategies in online controlled experiments (OCE). These strategies involve changing assignment weights during an experiment. However, if one changes assignment weights when there are time-based confounders, then ignoring this complexity can lead to biased inference in an OCE. In the case of MABs, ignoring this complexity can also lead to poor total reward,&lt;/i&gt;&lt;i&gt;&amp;nbsp;making it counterproductive towards its intended purpose. In this post we discuss the problem, a solution, and practical considerations.&lt;br /&gt;&lt;/i&gt; &lt;br /&gt; &lt;br /&gt; &lt;br /&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt; Background &lt;/h2&gt;&lt;h3 style=&quot;text-align: left;&quot;&gt; Online controlled experiments&lt;/h3&gt;&lt;div&gt;An online controlled experiment (OCE) randomly assigns different versions of a website or app to different users in order to see which version causes more of some desired action. In this post, these “versions” are called &lt;b&gt;arms&lt;/b&gt; and the desired action is called the &lt;b&gt;reward&lt;/b&gt; (arms are often called “treatments”&amp;nbsp;and reward is often called the “dependent variable” in other contexts). Examples of rewards for an OCE include the amount of product bought, the number of people who sign up for a newsletter, the number of people who start an account, etc. This post considers a common design for an OCE where a user may be randomly assigned an arm on their first visit during the experiment, with &lt;b&gt;assignment weights&lt;/b&gt; referring to the proportion that are randomly assigned to each arm.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt; There are two common reasons assignment weights may change during an OCE. The first is a strategy called &lt;b&gt;ramp-up&lt;/b&gt; and is advised by many experts in the field [1]. The second common reason is multi-armed bandit algorithms (MAB) that maximize reward by assigning more users to a winning arm sooner in order to take advantage of it sooner.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h3 style=&quot;text-align: left;&quot;&gt; Ramp-up&lt;/h3&gt;&lt;/div&gt;&lt;div&gt;Ramp-up is when the experiment initially gives a small assignment weight to the new arm and, as the experiment continues, increases the new arm&#39;s assignment weight. One reason to do ramp-up is to mitigate the risk of never before seen arms. For example, imagine a fantasy football site is considering displaying advanced player statistics. A ramp-up strategy may mitigate the risk of upsetting the site’s loyal users who perhaps have strong preferences for the current statistics that are shown. Another reason to use ramp-up is to test if a website&#39;s infrastructure can handle deploying a new arm to all of its users. For example, consider a smaller website that is considering adding a video hosting feature to increase engagement on the site. The website wants to make sure they have the infrastructure to handle the feature while testing if engagement increases enough to justify the infrastructure.&lt;/div&gt;&lt;div&gt; &lt;br /&gt;Although there are many ways to do ramp-up, we assume that once a user is assigned an arm, they will stay assigned to this arm for the duration of the experiment. When assignment weights change in a ramp-up experiment, there are periods of constant assignment weights that we define as epochs. We consider the assignment weights for each epoch to be the proportion of previously unassigned users that are assigned to each arm upon first visit during that epoch. The assignment weights within epochs may or may not add up to 100%. If they do not add up to 100%, we call this a partial traffic ramp-up and a user who visits during that epoch may be unassigned and could be assigned on a future visit in a later epoch. When they add up to 100%, we call this a full traffic ramp-up.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;span id=&quot;docs-internal-guid-6838f6c1-7fff-01b1-f326-075af7e61222&quot;&gt;&lt;br /&gt;&lt;div align=&quot;center&quot; dir=&quot;ltr&quot; style=&quot;margin-left: 0pt;&quot;&gt;&lt;table style=&quot;border-collapse: collapse; border: none;&quot;&gt;&lt;colgroup&gt;&lt;col width=&quot;107&quot;&gt;&lt;/col&gt;&lt;col width=&quot;100&quot;&gt;&lt;/col&gt;&lt;col width=&quot;81&quot;&gt;&lt;/col&gt;&lt;/colgroup&gt;&lt;tbody&gt;&lt;tr style=&quot;height: 22.5pt;&quot;&gt;&lt;td rowspan=&quot;2&quot; style=&quot;border-bottom: solid #9e9e9e 1pt; border-left: solid #9e9e9e 1pt; border-right: solid #9e9e9e 1pt; border-top: solid #9e9e9e 1pt; overflow-wrap: break-word; overflow: hidden; padding: 7pt 7pt 7pt 7pt; vertical-align: middle;&quot;&gt;&lt;p dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: Arial; font-size: 11pt; font-variant-east-asian: normal; font-variant-numeric: normal; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;Assignment&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td colspan=&quot;2&quot; style=&quot;border-bottom: solid #9e9e9e 1pt; border-left: solid #9e9e9e 1pt; border-right: solid #9e9e9e 1pt; border-top: solid #9e9e9e 1pt; overflow-wrap: break-word; overflow: hidden; padding: 7pt 7pt 7pt 7pt; vertical-align: top;&quot;&gt;&lt;p dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: Arial; font-size: 11pt; font-variant-east-asian: normal; font-variant-numeric: normal; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;Weights&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 21.75pt;&quot;&gt;&lt;td style=&quot;border-bottom: solid #9e9e9e 1pt; border-left: solid #9e9e9e 1pt; border-right: solid #9e9e9e 1pt; border-top: solid #9e9e9e 1pt; overflow-wrap: break-word; overflow: hidden; padding: 7pt 7pt 7pt 7pt; vertical-align: top;&quot;&gt;&lt;p dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: Arial; font-size: 11pt; font-variant-east-asian: normal; font-variant-numeric: normal; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;Epoch 1&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #9e9e9e 1pt; border-left: solid #9e9e9e 1pt; border-right: solid #9e9e9e 1pt; border-top: solid #9e9e9e 1pt; overflow-wrap: break-word; overflow: hidden; padding: 7pt 7pt 7pt 7pt; vertical-align: top;&quot;&gt;&lt;p dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: Arial; font-size: 11pt; font-variant-east-asian: normal; font-variant-numeric: normal; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;Epoch 2&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 4.5pt;&quot;&gt;&lt;td style=&quot;border-bottom: solid #9e9e9e 1pt; border-left: solid #9e9e9e 1pt; border-right: solid #9e9e9e 1pt; border-top: solid #9e9e9e 1pt; overflow-wrap: break-word; overflow: hidden; padding: 7pt 7pt 7pt 7pt; vertical-align: top;&quot;&gt;&lt;p dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: Arial; font-size: 11pt; font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;Current Arm&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #9e9e9e 1pt; border-left: solid #9e9e9e 1pt; border-right: solid #9e9e9e 1pt; border-top: solid #9e9e9e 1pt; overflow-wrap: break-word; overflow: hidden; padding: 7pt 7pt 7pt 7pt; vertical-align: top;&quot;&gt;&lt;p dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: Arial; font-size: 11pt; font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;90%&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #9e9e9e 1pt; border-left: solid #9e9e9e 1pt; border-right: solid #9e9e9e 1pt; border-top: solid #9e9e9e 1pt; overflow-wrap: break-word; overflow: hidden; padding: 7pt 7pt 7pt 7pt; vertical-align: top;&quot;&gt;&lt;p dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: Arial; font-size: 11pt; font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;50%&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0pt;&quot;&gt;&lt;td style=&quot;border-bottom: solid #9e9e9e 1pt; border-left: solid #9e9e9e 1pt; border-right: solid #9e9e9e 1pt; border-top: solid #9e9e9e 1pt; overflow-wrap: break-word; overflow: hidden; padding: 7pt 7pt 7pt 7pt; vertical-align: top;&quot;&gt;&lt;p dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: Arial; font-size: 11pt; font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;New Arm&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #9e9e9e 1pt; border-left: solid #9e9e9e 1pt; border-right: solid #9e9e9e 1pt; border-top: solid #9e9e9e 1pt; overflow-wrap: break-word; overflow: hidden; padding: 7pt 7pt 7pt 7pt; vertical-align: top;&quot;&gt;&lt;p dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: Arial; font-size: 11pt; font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;10%&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #9e9e9e 1pt; border-left: solid #9e9e9e 1pt; border-right: solid #9e9e9e 1pt; border-top: solid #9e9e9e 1pt; overflow-wrap: break-word; overflow: hidden; padding: 7pt 7pt 7pt 7pt; vertical-align: top;&quot;&gt;&lt;p dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: Arial; font-size: 11pt; font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;50%&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;&lt;/span&gt;&lt;table cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;br /&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;&lt;font size=&quot;2&quot;&gt;Fig 1. A simple example of a full traffic ramp-up strategy where 90% of users are assigned the current arm and 10% are assigned the new arm in epoch 1. In epoch 2, 50% of previously unassigned users are assigned each the current and new arm.&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;div style=&quot;text-align: center;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;br /&gt;There is a lot of consideration that ought go into the design and analysis of a specific ramp-up strategy. Describing the precise decision criteria for different ramp-up designs is out of scope for this post. Instead, we focus on the case where an experimenter has decided to run a full traffic ramp-up experiment and wants to use the data from all of the epochs in the analysis. The fantasy football and video hosting examples, which we will discuss in more detail later, highlight situations where this design might be considered, despite potential complexity in the analysis. &lt;br /&gt;&lt;br /&gt; Complexity arises in the presence of time-based confounders, when taking an unweighted average across data that is aggregated across epochs can lead to biased estimates (whether it’s a partial or full traffic ramp-up strategy). At a high level, this is because changing the assignment weights causes the distribution of the time-based confounder in each experiment arm to be different than the overall population. Using the fantasy football example, we will simulate a situation where the bias is so strong that an inferior decision on which arm to deploy is made. We will offer a weighted average solution that allows an experimenter to do full traffic ramp-up and aggregate data across epochs to get an unbiased estimate of the effect of each arm. &lt;br /&gt;&lt;br /&gt;&lt;h3&gt; Multi-armed bandits &lt;/h3&gt;Multi-armed bandits (MAB) are a class of algorithms that maximize reward by assigning more users to better performing arms sooner in order to take advantage of them sooner. MAB algorithms are popular across many of the large web companies. Companies like Google [2], Amazon [3], and Microsoft [4] have all published scholarly articles on this topic.  &lt;br /&gt;&lt;br /&gt; Sometimes a MAB is run to maximize reward during an OCE with the goal of eventually ending the MAB, determining which arm is better, and deploying the better arm. Other times a MAB is run perpetually to maximize reward and not to necessarily find a winning arm. Just as in ramp-up, making inferences while ignoring the complexity of time-based confounders that are present can lead to biased estimates. Moreover, certain MAB algorithms can be counterproductive towards maximizing reward if assumptions aren&#39;t met. This post will discuss how to use data from a MAB to get unbiased estimates. We will offer strategies, considerations, and things to think about when running a MAB to maximize reward.&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&amp;nbsp; &lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;img height=&quot;350&quot; src=&quot;https://lh6.googleusercontent.com/-QMXYSr8a1tjVujRX36SXEIc3OxmgRATBFknwHUAMW2mnpsxn6Gv5MbruqWC8ikA1VSfzGPYifsCVj7mYMtyCJVXIytqNTjVh7SwWbY-PPZ3WkkzcxBBEF9z-9zhMUdZUxyxzICd7A=w524-h350&quot; style=&quot;margin-left: auto; margin-right: auto;&quot; width=&quot;524&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;&lt;span style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;font size=&quot;2&quot;&gt;F&lt;/font&gt;&lt;/span&gt;&lt;font size=&quot;2&quot;&gt;ig 2. Multi-armed bandit algorithms are often thought of by imagining a gambler sitting at several slot machines, deciding which machine to play in order to maximize winnings.&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;  &lt;br /&gt;&lt;h2&gt;Naive aggregation of data in ramp-up can lead to biased estimates &lt;/h2&gt;In an OCE, the average reward for each arm is often estimated by the average of the observed outcomes in that arm. In an OCE with constant assignment weights and a representative sample, this is an unbiased estimator. &lt;br /&gt;&lt;br /&gt; However, when there are changing assignment weights, then an unweighted average of data across the epochs can be a biased estimate. The reason is that changing assignment weights introduces a dependence between time of first visit and probability of being assigned to an arm. If time of first visit is also correlated with reward, this introduces bias. Time of first visit and reward may be correlated due to time-based confounders, which we define as a variable that is correlated with assignment time and reward. To summarize, changing assignment weights causes the distribution of the time-based confounder for those assigned to each arm to be unrepresentative of the overall population of users who visit during the experiment. Possible time-based confounders include day-of-week, time-of-day, or frequent vs. infrequent users. &lt;br /&gt;&lt;br /&gt; When there are changing assignment weights and time-based confounders, this complication must be considered either in the analysis or the experimental design. An option that can sometimes work is to use only the data from users assigned in the final epoch. This leads to valid inference so long as the final epoch is run long enough to be representative with respect to the time-based confounder. If the time-based confounder is something like day-of-week, then making sure the final epoch is run for at least a full 7-day cycle can do the trick. However, if the confounder is frequent/infrequent user, this option may not work if one is unwilling to reassign users as they ramp-up (there may be statistical or product reasons for not wanting to do this). This is because users assigned in the final epoch are less likely to be frequent users than the users who visit at any point in the experiment. In the case of full traffic ramp-up, users assigned in the final epoch must not have visited in earlier epochs and hence are less likely to be frequent users. In the case of partial traffic ramp-up, a proportion of users who visited during earlier epochs were assigned in those earlier epochs. If the proportion is large, users assigned in the final epoch are a lot less likely to have visited in earlier epochs. If the proportion is small, users assigned in the final epoch are only slightly less likely to have visited in earlier epochs and the bias may have little practical significance.  &lt;br /&gt;&lt;br /&gt; Using data from the final epoch is just one way to analyze experiments that use ramp-up strategies in the presence of time-based confounders — other experimental designs or ways to analyze the data may also provide valid inference. With that being said, full traffic ramp-up and combining data across epochs would sometimes be attractive, provided one could get unbiased estimates. We offer two examples where this may be the case.  &lt;br /&gt;&lt;br /&gt; For the first example, consider a small website that is a platform for content on personal finance. Imagine you run this site and want to test a new video hosting feature to hopefully increase engagement on your site. How willing your users are to engage with personal finance content depends on whether or not it’s the weekend. Here, day-of-week is a time-based confounder.  &lt;br /&gt;&lt;br /&gt; Although increasing engagement is the goal of this new video hosting feature, you also want to make sure your small website can handle this. So you initially give this new feature an assignment weight of 25%, then if all goes well 50%, then eventually 75%. Since so many users were given the feature in earlier epochs, there’s a lot of information being left on the table if you were to ignore these epochs. So combining data from epochs is attractive.  &lt;br /&gt;&lt;br /&gt; For the second example, we will go into more detail in the next section. &lt;br /&gt;&lt;br /&gt;&lt;h3&gt; Example: fantasy football website &lt;/h3&gt;Fantasy football is a virtual game where users pick professional football players to play on their team at the beginning of the season. Before each week&#39;s games, a user chooses a subset of these players to “play” that week. The better the picked football players perform in a real football game, the more points the user gets.  &lt;br /&gt;&lt;br /&gt; Imagine you are experimenting on a fantasy football site and you want to show some advanced player statistics in order to aid users in picking the players to play that week. This change gives users a teaser of what they would get if they signed up for a premium account. The goal of this change is to increase the number of non-premium users who sign up for a premium account. You decide to use a ramp-up strategy to mitigate the risk of your non-premium users having strong preferences for the traditional player statistics that are currently shown. However, ramp-up adds complications that you ought to consider. For example, some of your users visit frequently to tinker with their lineups, check scores, check rankings, etc. and other users visit infrequently. Since the frequent users visit more often, it’s more likely for their first visit to be earlier in the experiment. The frequent vs. infrequent user variable is a time-based confounder if, in addition to being correlated with time of first visit, it has an effect on the probability of signing up for a premium account. &lt;br /&gt;&lt;br /&gt; Fantasy football follows a weekly schedule based on the real football games that are played on  Thursday, Sunday, and Monday. We call a Tuesday through Monday cycle a “football week” to distinguish it from an arbitrary 7 day period that could span two football weeks. Due to the weekly schedule, both frequent and infrequent users will likely visit at least once during a football week, with each football week&#39;s visitors being representative of the overall population of users the site is interested in. So when running an experiment on a fantasy football site, it’s a good idea to consider data from a complete football week in order to have a representative sample of the population.  &lt;br /&gt;&lt;br /&gt; If you wanted to mitigate risk and do ramp-up, using full traffic ramp-up and combining data across epochs is an attractive option in part due to the weekly schedule of fantasy football. With this option, you could do ramp-up and run the experiment for just one football week. If instead you modified the ramp-up design and considered only the final epoch, earlier epochs would need to be planned such that the final epoch ran for a full football week. Although not insurmountable, this does add some complexity in the experimental design and could cause the experiment to take meaningfully longer than a week to finish.  &lt;br /&gt;&lt;br /&gt; We simulate data using this fantasy football example to illustrate how naive aggregation of data in ramp-up can lead to biased estimates. We do the exact same simulation twice, but in one simulation the hypothetical experimenter changes the assignment weights part way through and in the second they don’t. We will see that the bias caused by naive aggregation with changing assignment weights can be so strong that it changes the outcome of the experiment.&lt;br /&gt;&lt;br /&gt; In this simulation, all users who visit during the experiment are randomly assigned an arm on their first visit and stay assigned to that arm for the duration of the experiment. The experiment will run for a full football week in order to get a representative sample and to make a decision before next week&#39;s games. We simulate 2,000,000 users who visit during the experiment. The number of simulated users is large enough so that the simulation gives nearly identical answers each time it&#39;s run. For this reason we don’t report uncertainty measures or statistical significance in the results of the simulation. Time of first visit and time of assignment for user i are represented by the same variable $Z_i$ and is simulated by a uniform distribution $$&lt;br /&gt;&lt;br /&gt; Z_i \sim \mathrm{Uniform}(0, 1) &lt;br /&gt;&lt;br /&gt;$$ where experiment duration is normalized to be between $0$ and $1$.  &lt;br /&gt;&lt;br /&gt; Let $A_i =1$ if user $i$ is a “frequent user” and $A_i =0$ otherwise. A “frequent user” is someone who frequently visits the website, so their first visit is more likely to come earlier within the football week than “infrequent users”. Hence, the frequent users are more likely to be assigned earlier. The relationship between these variables is simulated by $$&lt;br /&gt;&lt;br /&gt; A_i \sim \mathrm{Bernoulli}(1- Z_i) &lt;br /&gt;&lt;br /&gt;$$A user can sign up for a premium account only once. Whether or not user $i$ converts during the experiment is simulated by the probabilities in Fig 3.  &lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-fDGlvcjdGT4/XxeYCtsk7uI/AAAAAAAAeeQ/5PV9xqn6QpwLZhw33B0sAU1WP9M3wPFkQCNcBGAsYHQ/s552/Fig3.png&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;216&quot; data-original-width=&quot;552&quot; src=&quot;https://1.bp.blogspot.com/-fDGlvcjdGT4/XxeYCtsk7uI/AAAAAAAAeeQ/5PV9xqn6QpwLZhw33B0sAU1WP9M3wPFkQCNcBGAsYHQ/s320/Fig3.png&quot; width=&quot;320&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;&lt;font size=&quot;2&quot;&gt;Fig 3. Conversion rates used in the simulation for frequent and infrequent users for the current and new arm.&amp;nbsp;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;Notice that the new arm performs better on frequent users and the current arm performs better on infrequent users. This is not an example of Simpson’s paradox since one arm doesn&#39;t uniformly perform better on both frequent and infrequent users. &lt;br /&gt;&lt;br /&gt; The experimenter in this simulation wants to make sure the new arm isn’t going to alienate users who perhaps have strong preferences for the current player statistics that are shown. In order to mitigate risk, the experimenter decides to “play it safe” and not assign too many users to the new arm. So the experimenter decides to initially assign 90% of users to the current arm and 10% to the new arm.  &lt;br /&gt;&lt;br /&gt; Half-way through the experiment, early data suggests that the new arm isn’t a complete disaster. Consider two hypothetical experimenters: one that ramps-up the assignment weights and one that does not. These two experimenters will use the simulated data as described above. &lt;br /&gt;&lt;br /&gt;&lt;h3&gt; Experimenter 1: No ramp-up &lt;/h3&gt;Experimenter 1 keeps the assignment weights constant throughout the duration of the experiment. This is simulated by $$&lt;br /&gt;T_i \sim \mathrm{Bernoulli}(0.1)\quad\mbox{for all $Z_i$}&lt;br /&gt;$$ where $T_i=0$ means user i is assigned to the current arm and $T_i=1$ if assigned to the new arm. In this scenario, the following are sample conversion rates at the end of the experiment:&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;div style=&quot;text-align: center;&quot;&gt;Current Arm:&amp;nbsp; 0.0390&amp;nbsp;&lt;/div&gt;&lt;div style=&quot;text-align: center;&quot;&gt;New Arm:&amp;nbsp; 0.0371&amp;nbsp;&lt;/div&gt;&lt;div style=&quot;text-align: center;&quot;&gt;&lt;br /&gt;&lt;/div&gt; Here the experimenter decides to keep the current arm. &lt;br /&gt;&lt;br /&gt;&lt;h3&gt; Experimenter 2: Ramp-up &lt;/h3&gt;Experimenter&amp;nbsp;2 decides to ramp-up the assignment weights to the new arm and assigns 50% of users to the current arm and 50% to the new arm in the second epoch. This is simulated by $$&lt;br /&gt;&lt;br /&gt;T_i \sim \mathrm{Bernoulli}(0.1)\quad\mbox{if $Z_i&amp;lt;0.5$}\\&lt;br /&gt;T_i \sim \mathrm{Bernoulli}(0.5)\quad\mbox{if $Z_i&amp;gt;0.5$} &lt;br /&gt;&lt;br /&gt;$$ where $T_i=0$ means user $i$ is assigned to the current arm and $T_i=1$ if assigned to the new arm. Notice that there is a dependence between which arm user $i$ is assigned to and when they are assigned. However, when they are assigned is also correlated with which arm they prefer. Thus the resulting sample conversion rates will be biased. Indeed, in our simulations we get the following sample conversion rates at the end of the experiment: &lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;div style=&quot;text-align: center;&quot;&gt;Current Arm:&amp;nbsp; 0.0362&amp;nbsp;&lt;/div&gt;&lt;div style=&quot;text-align: center;&quot;&gt;New Arm:&amp;nbsp; 0.0395&amp;nbsp;&lt;/div&gt;&lt;div style=&quot;text-align: center;&quot;&gt;&lt;br /&gt;&lt;/div&gt; Here, the experimenter ends up deploying an inferior arm. The bias caused by changing assignment weights and not considering this in the analysis changes the experimenter’s decision.  &lt;br /&gt;&lt;br /&gt;&lt;h3&gt; Ramp-up solution: measure epoch and condition on its effect  &lt;/h3&gt;If one wants to do full traffic ramp-up and use data from all epochs, they must use an adjusted estimator to get an unbiased estimate of the average reward in each arm. By conditioning on a user’s epoch, we can get an unbiased estimate without needing to know and observe all possible time-based confounders. Let’s formally define a user’s epoch.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;b&gt;Epoch&lt;/b&gt;: If assignment weights are changed at times $Z^*_1, ..., Z^*_J$ then the assignment weights are constant during $[Z^*_j, Z^*_{j+1})$. The period of constant assignment weights $[Z^*_j, Z^*_{j+1})$ will be called &lt;b&gt;epoch&lt;/b&gt; $j$. The epoch of user $i$ is determined by their assignment time and will be denoted by $E_i$. &lt;br /&gt;&lt;br /&gt; To get an unbiased estimate for the average reward in arm $t$, we can infer the arm’s reward in each epoch and take a weighted average across the epochs with equal weights in the arms $$&lt;br /&gt;&lt;br /&gt;\hat{\theta}_{t, \mathrm{adjusted}} = \sum_j \hat{E}[Y_i | T_i=t, E_i=j] P(E_i=j) &lt;br /&gt;&lt;br /&gt;$$Here $Y_i$ is the observed reward for user $i$ and $\hat{E}$, $\hat{P}$ indicate estimates of the corresponding expected value and probability. Within an epoch, assignment weights are constant and so a sample mean using data from the appropriate epoch and arm is an unbiased estimate of $E[Y_i | T_i=t, E_i=j]$. Notice that the weights $P(E_i=j)$ do not depend on the assigned arm, so should be estimated from all users regardless of arm assignment. Intuitively the adjusted estimator reweights each epoch estimate in proportion to the percentage of the overall population that arrived in that epoch. Contrast this with an unadjusted estimate where each arm’s sample mean is only representative of the population of users assigned to the given arm. These populations are not the same across arms when there are changing assignment weights and time-based confounders.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt; Indeed, when we use this solution in the simulation, we get back the same result as if we hadn&#39;t changed assignment weights. To use the solution, let’s first look at the observed conversion rates in each&amp;nbsp;epoch&lt;/div&gt;&lt;div&gt;&lt;br /&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-TJAaJmesV4Q/XxiqbsJDPtI/AAAAAAAAeeg/n6DDWdkAeZsSENf_lFKUBtt65Jm3Fj7nwCNcBGAsYHQ/s552/Fig4.png&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;216&quot; data-original-width=&quot;552&quot; src=&quot;https://1.bp.blogspot.com/-TJAaJmesV4Q/XxiqbsJDPtI/AAAAAAAAeeg/n6DDWdkAeZsSENf_lFKUBtt65Jm3Fj7nwCNcBGAsYHQ/s320/Fig4.png&quot; width=&quot;320&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;&lt;font size=&quot;2&quot;&gt;Fig 4. Observed conversion rates for each epoch/arm combination.&amp;nbsp;&lt;br style=&quot;text-align: left;&quot; /&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Next notice that 50% of all users in the experiment were assigned in Epoch 1 and 50% were assigned in Epoch 2 so $\hat{P}(E_i=1 )=0.5$ and $\hat{P}(E_i=2 )=0.5$ (first arrival time is uniform and the epochs are equal lengths). Combining all these together as described above, &lt;br /&gt;&lt;br /&gt;&lt;div style=&quot;text-align: center;&quot;&gt;Current Arm: $\hat{\theta}_{0, \mathrm{adjusted}} = 0.0295 \times 0.5 + 0.0483 \times 0.5 =0.0389$&lt;/div&gt;&lt;div style=&quot;text-align: center;&quot;&gt;New Arm: $\hat{\theta}_{1, \mathrm{adjusted}} = 0.0340 \times 0.5 + 0.0406 \times 0.5=0.0373$&lt;/div&gt;&lt;div style=&quot;text-align: center;&quot;&gt;&lt;br /&gt;&lt;/div&gt; We see with this solution that the changing assignment weights problem disappears. An experimenter who changes assignment weights gets the same answer as the experimenter who doesn’t change assignment weights (modulo some rounding errors) so long as they use the adjusted estimator. &lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt; Using sample proportions, as we do here, is a simple way to estimate $\hat{E}[Y_i | T_i=1,E_i=j]$ and $\hat{P}(E_i=j )$. In practice, one may want to use more complex models to make these estimates. For example, one may want to use a model that can pool the epoch estimates with each other via hierarchical modeling (a.k.a. partial pooling) if one has many short epochs.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h3&gt; Justification of the solution &lt;/h3&gt;In the above simulation we see that when a hypothetical experimenter changes assignment weights and uses the proposed solution, they get the same answer as if they hadn’t changed assignment weights. Here we formally justify that when assignment weights are changed in an experiment, this solution is an unbiased estimator of what is called the average treatment effect (ATE). To formally define ATE, we first introduce potential outcomes, &lt;br /&gt;&lt;br /&gt;&lt;b&gt; Potential outcome&lt;/b&gt;: Let $Y_i (t)$ denote the realized reward for user $i$ if they had been given arm $t$. Potential outcomes are counterfactual conditional statements about what would have happened if the user was assigned a particular arm. &lt;br /&gt;&lt;br /&gt; The average treatment effect (ATE) is formally defined as the expected difference in potential outcomes between users assigned to different arms $$&lt;br /&gt;&lt;br /&gt; ATE =E[Y_i(1)] - E[Y_i(0)] &lt;br /&gt;&lt;br /&gt;$$ATE is important in experiments because it tells us about the causal effect of each arm. In a changing assignment weights scenario, we must rely on two assumptions to show that our adjusted estimate is unbiased for ATE [5]. The first is called consistency, &lt;br /&gt;&lt;br /&gt;&lt;b&gt; Consistency&lt;/b&gt;: the observed outcome if assigned to arm $t$ is the same as the potential outcome if assigned to arm $t$, i.e $Y_i=Y_i(t)$ if $T_i=t$. &lt;br /&gt;&lt;br /&gt; Consistency generally means that users assigned to an arm will experience the assigned arm and no other arm. In the simulation we are assuming that users can only see the version of the website to which they are assigned - hence, consistency. The second assumption is called conditional ignorability&lt;br /&gt;&lt;br /&gt;&lt;b&gt; Conditional ignorability:&lt;/b&gt;&amp;nbsp;The potential outcomes of user $i$ if they had been assigned to arm $t$ are conditionally independent of which arm is actually assigned given some other variable, i.e $Y_i(t)$ is conditionally independent of $T_i$ given some other variable.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;In the simulation, the potential outcome is conditionally independent of arm assignment given epoch, since assignment weights are constant within an epoch. Thus we have conditional ignorability no&amp;nbsp;matter how many time-based confounders are present. With both consistency and conditional ignorability&lt;br /&gt;\begin{align*}&lt;br /&gt;&lt;br /&gt;E[Y_i(t)] &amp;amp;= \sum_j E[Y_i(t) | E_i=j] P(E_i=j) \quad \mbox{(law of total probability)}\\&lt;br /&gt;&amp;amp;= \sum_j E[Y_i(t)| E_i=j] P(E_i=j)\quad (Y \perp T | E) \\&lt;br /&gt;&amp;amp;= \sum_j E[Y_i| E_i=j, T_i=t]P(E_i=j)\quad\mbox{(consistency)}&lt;br /&gt;&lt;br /&gt;\end{align*} So to get an unbiased estimate of the expected potential outcome one can combine unbiased estimates for $E[Y_i | T_i=t, E_i=j]$ and $P(E_i=j)$ as we do in $\hat{\theta}_{t, \mathrm{adjusted}}$. From a Bayesian perspective, one can combine joint posterior samples for $E[Y_i | T_i=t, E_i=j]$ and $P(E_i=j)$, which provides a measure of uncertainty around the estimate.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt; This post discusses the solution in the context of an experiment where random assignment and reward both happen at the user level. The solution generalizes trivially if both random assignment and reward happen at some other unit of analysis. In a more complicated experiment where random assignment is at the user level and reward can happen several times for a user, the solution also generalizes.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;h2&gt; Naive implementation of multi-armed bandits can lead to biased estimates and inferior reward &lt;/h2&gt;In addition to ramp-up, multi-armed bandits (MABs) are a common reason for changing assignment weights. MABs are a class of algorithms that maximize reward (conversion rate, sales, etc) by assigning more users to better performing arms sooner in order to take advantage of them sooner. One such MAB algorithm is Thompson sampling which assigns a user to an arm according to the probability that the arm is best given the data seen thus far [6], [7].  &lt;br /&gt;&lt;br /&gt;We can formally define Thompson sampling in the case of a two-arm experiment. Let $T_i=1$ if user $i$ is assigned to arm 1 and $T_i=0$ if assigned to arm 0. With Thompson sampling, $T_i$ is determined by $$&lt;br /&gt;&lt;br /&gt; T_i \sim \mathrm{Bernoulli}(\Phi_i)\\&lt;br /&gt;\Phi_i = P(\theta_1&amp;gt; \theta_0 | Y_1 \cdots Y_{i-1}) &lt;br /&gt;&lt;br /&gt;$$where $\theta_0$ and $\theta_1$ are the average rewards for arm 0 and arm 1 respectively and $\Phi_i$ is the posterior probability that arm 1 has better reward than arm 0 given all the data observed thus far. Notice that this probability will change with each additional sample, so assignment weights will change many times throughout the MAB. &lt;br /&gt;&lt;br /&gt; Although there are a wide variety of MAB algorithms, many MAB algorithms are like Thompson sampling and require models to produce probabilities of each arm being the best. These MAB algorithms are great at maximizing reward when the models are perfectly specified and probabilities are accurate. However, these probabilities can be sensitive to model misspecification. Inaccurate probabilities can cause the MAB to do a poor job of maximizing future reward. This is because the MAB maximizes future reward by using previous users to infer the reward of future users as if they were assigned to each arm. If there are time-based confounders, then users who visit earlier will not be representative of later users. If the model is missing these time-based confounders then the inference will be poor and the MAB will do a poor job of maximizing future reward. &lt;br /&gt;&lt;br /&gt; Often a MAB will be run to maximize reward during an experiment. Here the MAB eventually ends, a winner is declared, and the winning arm is deployed. If one does inference to determine the winning arm while ignoring the complexity of time-based confounders that are present, then the inference will be biased. The reason is much the same as in ramp-up. Changing assignment weights causes the distribution of time-based confounders to be systematically different across arms. Since time-based confounders affect reward, not considering time-based confounders that are present will produce biased results on the effect of each arm. &lt;br /&gt;&lt;br /&gt; We continue with the fantasy football example and see that if a model omits a time-based confounder then MAB algorithms like Thompson sampling can be counterproductive towards maximizing reward. We will also use the example to illustrate how the naive model can lead to inferior decisions on which arm to deploy at the end of the MAB.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h3&gt; Example: fantasy football website &lt;/h3&gt;Let’s go back to the fantasy football example and use the same simulation as described above. But now consider one hypothetical experimenter who runs a MAB to maximize reward during the experiment and the other who runs a traditional A/B experiment with constant and equal assignment weights. For the experimenter who runs a MAB, there’s more sampling variation than in the simulation for ramp-up because of the path dependence between what happens in the beginning and end of the experiment. For this reason, we run the simulation with 2,000,000 users 300 times.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&amp;nbsp; &lt;br /&gt;The MAB experimenter will use Thompson sampling. Since Thompson sampling requires Bayesian posteriors, the hypothetical experimenters will infer the conversion rate for each arm j with the following simple Bayesian model\begin{align*}&lt;br /&gt;&lt;br /&gt; Y_i &amp;amp;\sim \mathrm{Bernoulli}(\theta_j)\\&lt;br /&gt;\theta_j &amp;amp;\sim \mathrm{Beta}(1, 1) &lt;br /&gt;&lt;br /&gt;\end{align*}Although the model is perhaps overly simple in the real world, remember that the data generating process in the simulation is also quite simple. The data generating process differs from the model only by the omission of a single variable (frequent vs. infrequent user) that is correlated with both assignment time and reward. Omitting confounding variables isn’t usually a concern in a traditional A/B test because the law of large numbers and random assignment often make these variables equally distributed among the arms. As we&#39;ll see in the results, this intuition doesn&#39;t apply with time-based confounders in MABs, since the assignment depends on time.&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt; Just as in the previous ramp-up simulation, changing assignment weights causes some serious issues. Fig 5 shows the estimated ATE for each of the 300 simulations. We see that the MAB experimenter incorrectly picks the new arm as the winner in 267 out of 300 simulations. In fact, even when the MAB experimenter correctly picks the current arm as the winner, it dramatically misestimates the ATE relative to the experimenter who runs a traditional A/B test.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;      &lt;a href=&quot;https://1.bp.blogspot.com/-GVas48gMjqY/XxjIFTSeSsI/AAAAAAAAees/4690LVddxxI8_Dj267gTQXkZubGS3ifVgCNcBGAsYHQ/s533/Fig5.png&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;400&quot; data-original-width=&quot;533&quot; height=&quot;469&quot; src=&quot;https://1.bp.blogspot.com/-GVas48gMjqY/XxjIFTSeSsI/AAAAAAAAees/4690LVddxxI8_Dj267gTQXkZubGS3ifVgCNcBGAsYHQ/w625-h469/Fig5.png&quot; width=&quot;625&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;&lt;font size=&quot;2&quot;&gt;Fig 5. Comparison of estimated Average Treatment Effect (ATE) for each of the 300 simulations from a MAB and against a traditional A/B test with constant assignment weights. Here we see how poor the estimates from a MAB can be.*&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt; Not only does the MAB experimenter usually pick the wrong arm, we see in Fig 6 that the MAB experimenter also has fewer total conversions than the traditional A/B experimenter in 267 out of 300 simulations.&lt;br /&gt; &lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;  &lt;a href=&quot;https://1.bp.blogspot.com/-rMEPI7GYnAQ/XxjIKJrnexI/AAAAAAAAeew/yoj6V4N4RYke9tAn8A8tGf4wKLPcVyjEwCNcBGAsYHQ/s533/Fig6.png&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;400&quot; data-original-width=&quot;533&quot; height=&quot;469&quot; src=&quot;https://1.bp.blogspot.com/-rMEPI7GYnAQ/XxjIKJrnexI/AAAAAAAAeew/yoj6V4N4RYke9tAn8A8tGf4wKLPcVyjEwCNcBGAsYHQ/w625-h469/Fig6.png&quot; width=&quot;625&quot; /&gt;  &lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;&lt;font size=&quot;2&quot;&gt;Fig 6. Comparison of total reward (total conversions regardless of arm) during the experiment for each of the 300 simulations from a MAB against a traditional A/B test with constant assignment weights. Here we see that MAB leads to inferior reward&amp;nbsp;most of the time.*&amp;nbsp;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;  &lt;br /&gt;*The bimodality in Fig 5 and Fig 6 is caused by whether or not the MAB overconfidently goes into full “exploit” mode by sending nearly all users to the new arm (the preferred arm among frequent users). Occasionally the MAB “explores” just enough to send users to the current arm and eventually becomes confident that it is the winning arm (the preferred arm among infrequent users).&amp;nbsp;&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;In this simulation, changing assignment weights with a MAB caused biased results and usually changed the outcome of the experiment. Additionally, the MAB experimenter usually had inferior reward than the traditional A/B experimenter who wasn’t even concerned about maximizing reward during the experiment!&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;h3&gt; MAB solution: measure assignment time and condition on its effect&lt;/h3&gt; If one wants valid inference after using a MAB, one could use a similar strategy as the one described for ramp-up. Just as in the case of ramp-up, one can get valid inference without needing to know and observe all possible time-based confounders by conditioning on continuous assignment time and using a weighted average over assignment time as follows$$&lt;br /&gt;&lt;br /&gt; P(Y_i(t) | \theta )= \int P(Y_i | Z_i=z, T_i=t, \theta) P(Z_i=z | \theta) dz  &lt;br /&gt;&lt;br /&gt;$$where are model parameters. Conditioning on continuous assignment time Zi, rather than discrete assignment epoch, is necessary because the assignment weights can change with each additional sample in a MAB. Notice that this means a model that conditions on continuous assignment time is required. Conditioning on continuous assignment time is much more difficult in practice than conditioning on discrete epoch. For example, what happens if one doesn’t have a good understanding of how continuous assignment time impacts reward? Will the inference still be solid?  &lt;br /&gt;&lt;br /&gt; This solves the bias problem but doesn’t fix the problem of maximizing reward during the experiment when the model omits time-based confounders. Since maximizing reward is the reason to use a MAB, fixing the bias problem isn’t reason enough to run a MAB if it can’t also maximize reward. For maximizing reward during the experiment when there are time-based confounders, one should consider a class of MABs called restless multi-armed bandits which don’t assume reward is constant over time [8].  However, restless multi-armed bandits add new assumptions about how reward changes over time. Considering how counterproductive things can be when assumptions are not met in Thompson sampling, one should thoroughly understand and test these assumptions before implementing. &lt;br /&gt;&lt;br /&gt; When using MABs, an experimenter needs to critically assess whether or not the model is a good fit to the data at hand. Often simple models won’t work. Even when the model is mostly correct, but is missing just one time-based confounder, the MAB can lead to both bad inference and inferior reward. Moreover, if a MAB ends and one of the arms in consideration is deployed, then the MAB is being used like an A/B test. Given all this, perhaps one should focus on valid inference rather than taking on the complexities required to also maximize reward. &lt;br /&gt;&lt;br /&gt;&lt;h2&gt; Conclusion &lt;/h2&gt;Changing assignment weights during an experiment is a common practice. Experts in the field often recommend ramping up assignment to never-before-seen arms [1] and MAB algorithms are popular in industry [2], [3], [4]. This post shows that naive implementations of ramp-up and MABs can break the inferential promises of randomized experiments and, in the case of MABs, can lead to inferior reward relative to a simple A/B testing design with constant assignment weights. &lt;br /&gt;&lt;br /&gt; We offer a solution for an unbiased estimator for the average treatment effect (ATE) when an experimenter does full traffic ramp-up and aggregates data across epochs in the presence of changing assignment weights and time-based confounders. The solution is a straightforward weighted average of reward during epochs that allows the experimenter to use data across the entire experiment. For MABs, the strategy of conditioning on assignment time is a bit trickier and doesn’t fix the problem of maximizing reward. With this in mind, one needs to consider why they are using a MAB. If the goal is really to maximize reward, one should thoroughly understand and critically test the assumptions of the MAB algorithm and any models the algorithm may require before implementing. If the goal is to eventually deploy one of the arms in consideration, perhaps one shouldn’t be taking on the complexities required for using a MAB. &lt;br /&gt;&lt;br /&gt; Although this blog post makes some specific points about changing assignment weights in an A/B experiment, there is a more general takeaway as well. A/B testing isn’t simple just because data is big — the law of large numbers doesn’t take care of everything! Even with big data, A/B tests require thinking deeply and critically about whether or not the assumptions made match the data. Simply trusting methods that have been used in the past, either by the experimenter or the industry, can often lead experimenters astray.&amp;nbsp;&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;br /&gt;&lt;h3 style=&quot;text-align: left;&quot;&gt; References&lt;/h3&gt; [1] Kohavi, Ron, Randal M. Henne, and Dan Sommerfield. &quot;Practical guide to controlled experiments on the web: listen to your customers not to the hippo.&quot; Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining. 2007. &lt;br /&gt;&lt;br /&gt; [2] Scott, Steven L. &quot;Multi‐armed bandit experiments in the online service economy.&quot; Applied Stochastic Models in Business and Industry 31.1 (2015): 37-45. &lt;br /&gt;&lt;br /&gt; [3] Hill, Daniel N., et al. &quot;An efficient bandit algorithm for realtime multivariate optimization.&quot; Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2017. &lt;br /&gt;&lt;br /&gt; [4] Audibert, Jean-Yves, and Sébastien Bubeck. &quot;Best arm identification in multi-armed bandits.&quot; 2010. &lt;br /&gt;&lt;br /&gt; [5] Imbens, Guido W., and Donald B. Rubin. Causal inference in statistics, social, and biomedical sciences. Cambridge University Press, 2015. &lt;br /&gt;&lt;br /&gt; [6] Thompson, William R. &quot;On the likelihood that one unknown probability exceeds another in view of the evidence of two samples.&quot; Biometrika 25.3/4 (1933): 285-294. &lt;br /&gt;&lt;br /&gt; [7] Thompson, William R. &quot;On the theory of apportionment.&quot; American Journal of Mathematics 57.2 (1935): 450-456. &lt;br /&gt;&lt;br /&gt; [8] Whittle, P. “Restless Bandits: Activity Allocation in a Changing World.” Journal of Applied Probability, vol. 25, no. A, 1988, pp. 287–298., doi:10.2307/3214163.&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;br /&gt;&lt;/div&gt;</content><link rel='replies' type='application/atom+xml' href='https://www.unofficialgoogledatascience.com/feeds/7354704051404876351/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='https://www.unofficialgoogledatascience.com/2020/07/changing-assignment-weights-with-time.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/7354704051404876351'/><link rel='self' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/7354704051404876351'/><link rel='alternate' type='text/html' href='https://www.unofficialgoogledatascience.com/2020/07/changing-assignment-weights-with-time.html' title='Changing assignment weights with time-based confounders'/><author><name>Amir Najmi</name><uri>http://www.blogger.com/profile/18174523203317227640</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://lh6.googleusercontent.com/-QMXYSr8a1tjVujRX36SXEIc3OxmgRATBFknwHUAMW2mnpsxn6Gv5MbruqWC8ikA1VSfzGPYifsCVj7mYMtyCJVXIytqNTjVh7SwWbY-PPZ3WkkzcxBBEF9z-9zhMUdZUxyxzICd7A=s72-w524-h350-c" height="72" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-5825758052688213474.post-1477753203544658965</id><published>2019-12-04T10:36:00.000-08:00</published><updated>2019-12-04T11:39:07.442-08:00</updated><title type='text'>Humans-in-the-loop forecasting: integrating data science and business planning</title><content type='html'>by THOMAS OLAVSON&lt;br /&gt;&lt;br /&gt;&lt;i&gt;Thomas leads a team at Google called &quot;Operations Data Science&quot; that helps Google scale its infrastructure capacity optimally.  ln this post he describes where and how having “humans in the loop” in forecasting makes sense, and reflects on past failures and successes that have led him to this perspective.&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;Our team does a lot of forecasting. It also owns Google’s internal time series forecasting platform described in an &lt;a href=&quot;http://www.unofficialgoogledatascience.com/2017/04/our-quest-for-robust-time-series.html&quot;&gt;earlier blog post&lt;/a&gt;. I am sometimes asked whether there should be any role at all for &quot;humans-in-the-loop” in forecasting. For high stakes, strategic forecasts, my answer is: yes! But this doesn&#39;t have to be an either-or choice, as I explain below.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Forecasting at the “push of a button”?&lt;/h2&gt;In &lt;a href=&quot;https://foresight.forecasters.org/2019-conference/&quot;&gt;conferences&lt;/a&gt; and research publications, there is a lot of excitement these days about machine learning methods and forecast automation that can scale across many time series.  My team and I are excited by this too (see [1] for reflections on the recent M4 forecasting competition by my colleagues).  But looking through the blogosphere, some go further and posit that “platformization” of forecasting and “forecasting as a service” &lt;a href=&quot;https://venturebeat.com/2019/07/30/how-uber-is-turning-everyone-in-the-company-into-a-data-scientist/&quot;&gt;can turn anyone into a data scientist&lt;/a&gt; at the push of a button.  Others argue that there will still be a unique &lt;a href=&quot;https://towardsdatascience.com/is-the-data-science-profession-at-risk-of-automation-ae162b5f052f&quot;&gt;role for the data scientist&lt;/a&gt; to deal with ambiguous objectives, messy data, and knowing the limits of any given model.  These perspectives can be useful if seen as part of a spectrum of forecasting problems, each calling for different approaches.  But what is missing from this discussion is that the range of the role of humans in the loop is wider than just that of the data scientist.  There are some problems where not only should the data scientist be heavily involved, but the data scientist should also involve non-data scientist stakeholders in the forecasting process.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Tactical vs strategic forecasts&lt;/h2&gt;Forecasting problems may be usefully characterized on a continuum between &lt;b&gt;tactical&lt;/b&gt; on the one hand, and &lt;b&gt;strategic&lt;/b&gt; on the other. This classification is based on the purpose, horizon, update frequency and uncertainty of the forecast. These characteristics of the problem drive the forecasting approaches.&lt;br /&gt;&lt;br /&gt;The table below summarizes different forecasting problems as tactical and strategic:&lt;br /&gt;&lt;br class=&quot;Apple-interchange-newline&quot; /&gt;&lt;table class=&quot;c15&quot; style=&quot;border-collapse: collapse; border-spacing: 0px; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr class=&quot;c11&quot; style=&quot;height: 0pt;&quot;&gt;&lt;td class=&quot;c10&quot; colspan=&quot;1&quot; rowspan=&quot;1&quot; style=&quot;border-color: rgb(0, 0, 0); border-style: solid; border-width: 1pt; padding: 5pt; vertical-align: top; width: 85.5pt;&quot;&gt;&lt;div class=&quot;c1 c12&quot; style=&quot;font-family: Arial; font-size: 11pt; height: 11pt; line-height: 1; padding-bottom: 0pt; padding-top: 0pt;&quot;&gt;&lt;span class=&quot;c7 c3&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; vertical-align: baseline;&quot;&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class=&quot;c4&quot; colspan=&quot;1&quot; rowspan=&quot;1&quot; style=&quot;border-color: rgb(0, 0, 0); border-style: solid; border-width: 1pt; padding: 5pt; vertical-align: top; width: 187.5pt;&quot;&gt;&lt;div class=&quot;c1&quot; style=&quot;font-family: Arial; font-size: 11pt; line-height: 1; padding-bottom: 0pt; padding-top: 0pt;&quot;&gt;&lt;span class=&quot;c3 c8&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; font-weight: 700; vertical-align: baseline;&quot;&gt;Strategic Forecasts&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class=&quot;c9&quot; colspan=&quot;1&quot; rowspan=&quot;1&quot; style=&quot;border-color: rgb(0, 0, 0); border-style: solid; border-width: 1pt; padding: 5pt; vertical-align: top; width: 195pt;&quot;&gt;&lt;div class=&quot;c1&quot; style=&quot;font-family: Arial; font-size: 11pt; line-height: 1; padding-bottom: 0pt; padding-top: 0pt;&quot;&gt;&lt;span class=&quot;c8 c3&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; font-weight: 700; vertical-align: baseline;&quot;&gt;Tactical Forecasts&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;c11&quot; style=&quot;height: 0pt;&quot;&gt;&lt;td class=&quot;c10&quot; colspan=&quot;1&quot; rowspan=&quot;1&quot; style=&quot;border-color: rgb(0, 0, 0); border-style: solid; border-width: 1pt; padding: 5pt; vertical-align: top; width: 85.5pt;&quot;&gt;&lt;div class=&quot;c1&quot; style=&quot;font-family: Arial; font-size: 11pt; line-height: 1; padding-bottom: 0pt; padding-top: 0pt;&quot;&gt;&lt;span class=&quot;c8 c3&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; font-weight: 700; vertical-align: baseline;&quot;&gt;Problem Characteristics&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class=&quot;c4&quot; colspan=&quot;1&quot; rowspan=&quot;1&quot; style=&quot;border-color: rgb(0, 0, 0); border-style: solid; border-width: 1pt; padding: 5pt; vertical-align: top; width: 187.5pt;&quot;&gt;&lt;ul class=&quot;c6 lst-kix_1lbgwlh84ai5-0 start&quot; style=&quot;list-style-type: none; margin: 0px; padding: 0px;&quot;&gt;&lt;li class=&quot;c0&quot; style=&quot;font-family: Arial; font-size: 11pt; line-height: 1; margin-left: 18pt; padding-bottom: 10pt; padding-top: 0pt; text-align: left;&quot;&gt;&lt;span class=&quot;c2&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; font-style: italic; font-weight: 700;&quot;&gt;Purpose&lt;/span&gt;&lt;span class=&quot;c3 c5&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; font-style: italic;&quot;&gt;:&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;c7 c3&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; vertical-align: baseline;&quot;&gt;an input to medium- to long-term planning in order to guide product, investment, or high stakes capacity planning decisions&lt;/span&gt;&lt;/li&gt;&lt;li class=&quot;c0&quot; style=&quot;font-family: Arial; font-size: 11pt; line-height: 1; margin-left: 18pt; padding-bottom: 10pt; padding-top: 0pt; text-align: left;&quot;&gt;&lt;span class=&quot;c2&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; font-style: italic; font-weight: 700;&quot;&gt;Horizon&lt;/span&gt;&lt;span class=&quot;c7 c3&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; vertical-align: baseline;&quot;&gt;: months to years&lt;/span&gt;&lt;/li&gt;&lt;li class=&quot;c0&quot; style=&quot;font-family: Arial; font-size: 11pt; line-height: 1; margin-left: 18pt; padding-bottom: 10pt; padding-top: 0pt; text-align: left;&quot;&gt;&lt;span class=&quot;c2&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; font-style: italic; font-weight: 700;&quot;&gt;Update frequency&lt;/span&gt;&lt;span class=&quot;c7 c3&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; vertical-align: baseline;&quot;&gt;: monthly or less&lt;/span&gt;&lt;/li&gt;&lt;li class=&quot;c0&quot; style=&quot;font-family: Arial; font-size: 11pt; line-height: 1; margin-left: 18pt; padding-bottom: 10pt; padding-top: 0pt; text-align: left;&quot;&gt;&lt;span class=&quot;c2&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; font-style: italic; font-weight: 700;&quot;&gt;Uncertainty&lt;/span&gt;&lt;span class=&quot;c3 c5&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; font-style: italic;&quot;&gt;:&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;c7 c3&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; vertical-align: baseline;&quot;&gt;difficult to quantify solely based on historical data due to long horizon, non-stationarity and possibly censored data.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;&lt;td class=&quot;c9&quot; colspan=&quot;1&quot; rowspan=&quot;1&quot; style=&quot;border-color: rgb(0, 0, 0); border-style: solid; border-width: 1pt; padding: 5pt; vertical-align: top; width: 195pt;&quot;&gt;&lt;ul class=&quot;c6 lst-kix_j2i7qdqqmwmo-0 start&quot; style=&quot;list-style-type: none; margin: 0px; padding: 0px;&quot;&gt;&lt;li class=&quot;c0&quot; style=&quot;font-family: Arial; font-size: 11pt; line-height: 1; margin-left: 18pt; padding-bottom: 10pt; padding-top: 0pt; text-align: left;&quot;&gt;&lt;span class=&quot;c2&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; font-style: italic; font-weight: 700;&quot;&gt;Purpose&lt;/span&gt;&lt;span class=&quot;c3 c5&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; font-style: italic;&quot;&gt;:&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;c7 c3&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; vertical-align: baseline;&quot;&gt;an input to short-term and mostly automated planning processes like inventory replenishment, workforce planning, production scheduling, etc.&lt;/span&gt;&lt;/li&gt;&lt;li class=&quot;c0&quot; style=&quot;font-family: Arial; font-size: 11pt; line-height: 1; margin-left: 18pt; padding-bottom: 10pt; padding-top: 0pt; text-align: left;&quot;&gt;&lt;span class=&quot;c2&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; font-style: italic; font-weight: 700;&quot;&gt;Horizon&lt;/span&gt;&lt;span class=&quot;c3 c5&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; font-style: italic;&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;c7 c3&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; vertical-align: baseline;&quot;&gt;&amp;nbsp;days to weeks&lt;/span&gt;&lt;/li&gt;&lt;li class=&quot;c0&quot; style=&quot;font-family: Arial; font-size: 11pt; line-height: 1; margin-left: 18pt; padding-bottom: 10pt; padding-top: 0pt; text-align: left;&quot;&gt;&lt;span class=&quot;c2&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; font-style: italic; font-weight: 700;&quot;&gt;Update frequency&lt;/span&gt;&lt;span class=&quot;c3 c5&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; font-style: italic;&quot;&gt;:&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;c7 c3&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; vertical-align: baseline;&quot;&gt;weekly or more&lt;/span&gt;&lt;/li&gt;&lt;li class=&quot;c0&quot; style=&quot;font-family: Arial; font-size: 11pt; line-height: 1; margin-left: 18pt; padding-bottom: 10pt; padding-top: 0pt; text-align: left;&quot;&gt;&lt;span class=&quot;c2&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; font-style: italic; font-weight: 700;&quot;&gt;Uncertainty:&lt;/span&gt;&lt;span class=&quot;c3 c5&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; font-style: italic;&quot;&gt;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;c7 c3&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; vertical-align: baseline;&quot;&gt;quantifiable through model fitting or backtesting on historical data&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;c11&quot; style=&quot;height: 0pt;&quot;&gt;&lt;td class=&quot;c10&quot; colspan=&quot;1&quot; rowspan=&quot;1&quot; style=&quot;border-color: rgb(0, 0, 0); border-style: solid; border-width: 1pt; padding: 5pt; vertical-align: top; width: 85.5pt;&quot;&gt;&lt;div class=&quot;c1&quot; style=&quot;font-family: Arial; font-size: 11pt; line-height: 1; padding-bottom: 0pt; padding-top: 0pt;&quot;&gt;&lt;span class=&quot;c8 c3&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; font-weight: 700; vertical-align: baseline;&quot;&gt;Forecasting Approaches&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class=&quot;c4&quot; colspan=&quot;1&quot; rowspan=&quot;1&quot; style=&quot;border-color: rgb(0, 0, 0); border-style: solid; border-width: 1pt; padding: 5pt; vertical-align: top; width: 187.5pt;&quot;&gt;&lt;ul class=&quot;c6 lst-kix_1lbgwlh84ai5-0&quot; style=&quot;list-style-type: none; margin: 0px; padding: 0px;&quot;&gt;&lt;li class=&quot;c0&quot; style=&quot;font-family: Arial; font-size: 11pt; line-height: 1; margin-left: 18pt; padding-bottom: 10pt; padding-top: 0pt; text-align: left;&quot;&gt;&lt;span class=&quot;c2&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; font-style: italic; font-weight: 700;&quot;&gt;Methods&lt;/span&gt;&lt;span class=&quot;c3 c5&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; font-style: italic;&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;c7 c3&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; vertical-align: baseline;&quot;&gt;&amp;nbsp;triangulation between alternate modeling methods and what-if analysis&lt;/span&gt;&lt;/li&gt;&lt;li class=&quot;c0&quot; style=&quot;font-family: Arial; font-size: 11pt; line-height: 1; margin-left: 18pt; padding-bottom: 10pt; padding-top: 0pt; text-align: left;&quot;&gt;&lt;span class=&quot;c2&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; font-style: italic; font-weight: 700;&quot;&gt;Key metrics&lt;/span&gt;&lt;span class=&quot;c3 c5&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; font-style: italic;&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;c7 c3&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; vertical-align: baseline;&quot;&gt;&amp;nbsp;summaries of forecast changes and drivers; thresholds to flag significant gaps between alternate forecasts&lt;/span&gt;&lt;/li&gt;&lt;li class=&quot;c0&quot; style=&quot;font-family: Arial; font-size: 11pt; line-height: 1; margin-left: 18pt; padding-bottom: 10pt; padding-top: 0pt; text-align: left;&quot;&gt;&lt;span class=&quot;c2&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; font-style: italic; font-weight: 700;&quot;&gt;Humans in the loop&lt;/span&gt;&lt;span class=&quot;c3 c5&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; font-style: italic;&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;c3&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt;&quot;&gt;&amp;nbsp;data scientist to suggest different forecast methods and generate or collect those forecasts;&lt;/span&gt;&lt;span class=&quot;c13&quot; style=&quot;font-size: 10pt;&quot;&gt;&amp;nbsp;stakeholders to review differences and approve a “consensus” forecast&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;&lt;td class=&quot;c9&quot; colspan=&quot;1&quot; rowspan=&quot;1&quot; style=&quot;border-color: rgb(0, 0, 0); border-style: solid; border-width: 1pt; padding: 5pt; vertical-align: top; width: 195pt;&quot;&gt;&lt;ul class=&quot;c6 lst-kix_1lbgwlh84ai5-0&quot; style=&quot;list-style-type: none; margin: 0px; padding: 0px;&quot;&gt;&lt;li class=&quot;c0&quot; style=&quot;font-family: Arial; font-size: 11pt; line-height: 1; margin-left: 18pt; padding-bottom: 10pt; padding-top: 0pt; text-align: left;&quot;&gt;&lt;span class=&quot;c2&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; font-style: italic; font-weight: 700;&quot;&gt;Methods&lt;/span&gt;&lt;span class=&quot;c3 c5&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; font-style: italic;&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;c7 c3&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; vertical-align: baseline;&quot;&gt;&amp;nbsp;automated pipeline of time series forecasts; if many related series are available then global, ML and/or hierarchical models may be appropriate&lt;/span&gt;&lt;/li&gt;&lt;li class=&quot;c0&quot; style=&quot;font-family: Arial; font-size: 11pt; line-height: 1; margin-left: 18pt; padding-bottom: 10pt; padding-top: 0pt; text-align: left;&quot;&gt;&lt;span class=&quot;c2&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; font-style: italic; font-weight: 700;&quot;&gt;Key metrics:&lt;/span&gt;&lt;span class=&quot;c3 c7&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; vertical-align: baseline;&quot;&gt;&amp;nbsp;point forecast and prediction interval accuracy metrics for model evaluation&lt;/span&gt;&lt;/li&gt;&lt;li class=&quot;c0&quot; style=&quot;font-family: Arial; font-size: 11pt; line-height: 1; margin-left: 18pt; padding-bottom: 10pt; padding-top: 0pt; text-align: left;&quot;&gt;&lt;span class=&quot;c2&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; font-style: italic; font-weight: 700;&quot;&gt;Humans in the loop&lt;/span&gt;&lt;span class=&quot;c3 c5&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; font-style: italic;&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;c3&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt;&quot;&gt;&amp;nbsp;data&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;c3&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt;&quot;&gt;scientist&lt;/span&gt;&lt;span class=&quot;c7 c3&quot; style=&quot;background-color: white; color: #222222; font-size: 10pt; vertical-align: baseline;&quot;&gt;&amp;nbsp;to build and maintain models; may include judgmental adjustments on model output, but used sparingly&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;div style=&quot;text-align: center;&quot;&gt;Table 1: Strategic and tactical forecasts.&lt;/div&gt;&lt;br /&gt;Some also distinguish further between tactical and&amp;nbsp;&lt;b&gt;operational forecasts&lt;/b&gt;&amp;nbsp;[2]. The latter involve updates at least daily. In this case there is no time at all for human review, and forecast automation is essential.&lt;br /&gt;&lt;br /&gt;In choosing the appropriate method, a key distinction lies in the business stakes associated with a given forecast publication cycle. Based on the decisions being made and how quickly plans can adjust to new forecast updates, what is the cost of forecasting too high or too low? If the costs of prediction error are asymmetric (e.g. predicting too low is more costly than predicting too high), decisions should plan to a certain&amp;nbsp;&lt;a href=&quot;https://robjhyndman.com/hyndsight/quantile-forecasts-in-r/&quot;&gt;quantile forecast&lt;/a&gt;&amp;nbsp;(e.g. 95th percentile). This may be true for both strategic and tactical forecasts. For example, long-term capacity or short-term inventory may be planned to a high quantile forecast, if the cost of a shortage is much greater than the cost of holding excess.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;The ROI of human involvement&lt;/h2&gt;When it comes to human involvement, the key difference is in the magnitude of costs associated with any one forecast cycle.  What is the reduction in cost of a forecast that was improved by human intervention? This defines the ROI on the investment of human time.&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Tactical forecasts have a higher frequency of updates and a shorter forecast horizon. Thus, there is both less time to make adjustments and less return on human time in doing so. If the number of forecasts is not too unwieldy and the forecasts not too frequent, there may be some room for what Hyndman calls “judgmental adjustments” to the model output, as a sort of light-weight version of strategic forecasting. Hyndman cautions that adjustments should be made sparingly using a structured and systematic approach and are “most effective when there is significant additional information at hand or strong evidence of the need for an adjustment.” [3].&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;In contrast, strategic forecasts benefit from a higher level of human review and a more formal process for triangulating between different forecast methods, some of which may rely primarily on judgment and forward-looking information.&lt;br /&gt;&lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-E0pXVxSJ1x0/XefnvYCMtWI/AAAAAAAAeIg/NoFLZ-l_lu8g7Pa2kjqbC3sjrDKQNFIpACEwYBhgL/s1600/Blue%2BLights%2Bin%2BServer%2BRow.jpg&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1067&quot; data-original-width=&quot;1600&quot; height=&quot;426&quot; src=&quot;https://1.bp.blogspot.com/-E0pXVxSJ1x0/XefnvYCMtWI/AAAAAAAAeIg/NoFLZ-l_lu8g7Pa2kjqbC3sjrDKQNFIpACEwYBhgL/s640/Blue%2BLights%2Bin%2BServer%2BRow.jpg&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot;&gt;Figure 1: A Google data center&lt;br /&gt;&lt;br /&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;As an example, consider Google’s forecasting and planning for data center capacity.  This capacity is planned years in advance due to long lead times for land, utility infrastructure, and construction of physical buildings with cooling infrastructure.  Once built, the data centers can be populated with compute and storage servers at much shorter lead times. Future demand for servers is uncertain, but the cost of empty data center space is much less than the shortage cost of not being able to deploy compute and storage when needed to support product growth. We therefore plan capacity to a high quantile demand forecast.&lt;br /&gt;&lt;br /&gt;Prediction intervals are critical for our quantile forecast. But unlike in the tactical case, we have a limited time series history available for backtesting. Nor can we learn prediction intervals across a large set of parallel time series, since we are trying to generate intervals for a single global time series.  With those stakes and the long forecast horizon, we do not rely on a single statistical model based on historical trends.&lt;br /&gt;&lt;br /&gt;I sometimes see the erroneous application of a tactical approach to strategic forecasting problems.  Strategic forecasts drive high stakes decisions at longer horizons, so they should not be approached simply as a black box forecasting service, divorced from decision-making.  Done right, strategic forecasts can provide insights to decision makers on trends, incorporate forward-looking knowledge of product plans and technology roadmaps when relevant, expose the risks and biases of relying on any one forecasting methodology, and invite input from stakeholders on the uncertainty ranges.  In this case, there is a high return on investment (ROI) of human time to triangulate among different forecasts to arrive at a consensus forecast.&lt;br /&gt;&lt;br /&gt;This focus on the ROI of human time turns on its head the conventional wisdom from 50 years ago, where the essence of how to choose the right forecasting technique was how much &lt;b&gt;computational time&lt;/b&gt; to invest to arrive at “good enough” forecast accuracy [4].  Today, as computation has become cheap, the key tradeoff is between the &lt;b&gt;human time&lt;/b&gt; invested vs “good enough” forecast accuracy.  For tactical forecasts of many parallel time series, computational time may still be a consideration. But even here, a greater concern is the time invested by data scientists (mostly at the development stage) in data analysis and cleaning, feature engineering, and model development.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;A forecast triangulation framework&lt;/h2&gt;As stated earlier, strategic forecasts should triangulate between a variety of methodologies.  But this does not mean simply presenting a menu of forecasts from which decision makers can choose.  Consider again the example of long-term forecasts for data center capacity planning. We might generate at least three types of forecasts with fundamentally different world views on which factors really drive growth: an “&lt;b&gt;affordability&lt;/b&gt;” forecast  based on forecasted revenue growth and the relationship between data center capacity and revenue growth; a “&lt;b&gt;resource momentum&lt;/b&gt;” forecast based on historical trends for compute and storage usage translated into data center capacity needs using technology roadmaps; or a “&lt;b&gt;power momentum&lt;/b&gt;” time series forecast based on historical consumption of usable data center capacity (measured in watts of usable capacity).  Each has some merit, but simply presenting all three as a choice shirks responsibility for actually arriving at the “best” forecast.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;The data scientist could try to build a single model that integrates all the signals together, but doing so typically relies on historical data to determine which features have the most predictive value.  Boiling all the information down to a single model does not help us challenge to what degree we think the future will differ from the past. A single model may also not shed light on the uncertainty range we actually face.  For example, we may prefer one model to generate a range, but use a second scenario-based model to “stress test” the range. If the alternate model is plausible with a small probability, then we’d like to see that the “stress test” forecast scenario still falls inside the prediction interval generated from our preferred model.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Rather than providing a menu of models, or a single model, the data scientist needs to play a bigger role in reviewing and evaluating forecasts. In particular, the data scientist must take responsibility for stakeholders approving the “best” forecast from all available information sources.  By “best forecast”, we mean the most accurate forecasts and prediction intervals. Using multiple forecasts forces a conversation about the drivers, a revisitation of the input assumptions. It provides the occasion for deeper exploration of which inputs that can be influenced and which risks can be proactively managed.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Over the life of the forecast, the data scientist will publish historical accuracy metrics. But due to the long time lag between forecasts and actuals, these metrics alone are insufficient.  The data scientist will conduct post-mortem analyses and adjustments when actual demand deviates significantly from the forecast. Every forecast update will include metrics to provide insight on change drivers, and will flag significant gaps between different model forecasts.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Note that this approach assumes that the forecasts directly drive high stakes decisions, and are not judged subsequently.  If forecasts are simply used as a baseline to detect trend changes, then other approaches and less investment may be appropriate.  But even in those cases, it may be worth the data scientist’s time to understand which decisions are in fact being made based on the trends detected by the forecast. (If the forecast cannot influence decisions, it does not merit a data scientist).  Finally, data scientists must recognize that their forecasts may be used more broadly than first anticipated, and broad communications may have more value than first realized. It is therefore good discipline to provide forecast release notes explaining key risks and drivers of change.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;The diagram below shows how we approach strategic forecasting for high stakes infrastructure capacity planning decisions at Google.  The data scientist surfaces differences between a &lt;b&gt;proposed&lt;/b&gt; forecast and one or more &lt;b&gt;benchmark&lt;/b&gt; forecasts.  The proposed forecast is the forecast believed to require the fewest subsequent adjustments.  This proposed forecast may still have other shortcomings, such as being prone to biases of human judgment, or lacking a robust prediction interval. The benchmarks forecasts are used as cross-checks, and to gain insight into how the future may differ from the past.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;The data scientist advocates for methods to include as benchmarks, as well as the method used as the proposed forecast.  Some but not necessarily all of these forecasts may be generated by the data scientist directly. For example, proposed forecasts may come from customers, if their forecasts are based on forward-looking information about product and technology plans that would be difficult for the data scientist to extract as inputs into a predictive model.  At least one of the forecast methods will have a quantitative prediction interval generated by the data scientist, so that other forecasts can be considered in the context of this range.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-QB9UL80N6Ig/XeftX_jEWcI/AAAAAAAAeIs/bDNP3yBC5_8jbR43l8_3x8oBZ59bPMVAgCNcBGAsYHQ/s1600/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;373&quot; data-original-width=&quot;611&quot; height=&quot;390&quot; src=&quot;https://1.bp.blogspot.com/-QB9UL80N6Ig/XeftX_jEWcI/AAAAAAAAeIs/bDNP3yBC5_8jbR43l8_3x8oBZ59bPMVAgCNcBGAsYHQ/s640/image2.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot;&gt;Figure 2: Forecast triangulation&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Integrating customer forecasts with statistical forecasts&lt;/h2&gt;&lt;div&gt;In strategic forecasting, the proposed forecast may rely partially on forecasts or assumptions not owned by the data scientist.  In the supply chain context, forecast and information sharing between buyers and suppliers is called “&lt;b&gt;collaborative forecasting and planning&lt;/b&gt;.”  This collaboration might also be between internal customers and an internal supplier.  Using customer forecasts as the proposed forecast can capture valuable information about future inorganic growth events or trends that are difficult to extract as features for a predictive model. On the other hand, these customer forecasts can be aspirational and often lack high quality prediction intervals.  Customer forecasts may further suffer from what Kahneman calls the “&lt;a href=&quot;https://www.mckinsey.com/business-functions/strategy-and-corporate-finance/our-insights/daniel-kahneman-beware-the-inside-view&quot;&gt;inside view&lt;/a&gt;”, where a forecaster (a customer in this case) may extrapolate from a narrow set of personal experiences and specific circumstances without the benefit of an “outside view” that can learn from a much larger set of analogous experiences.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;So what to do?  An operations team in need of a forecast to plan against may poorly frame this as an either-or proposition — either they accept the customer forecast (perhaps interpreting it as a high quantile forecast scenario), or discard it in favor of a statistical time series forecast with a quantitative prediction interval.  The alternative we use is the forecast triangulation framework described above. We collect base and high scenario customer forecasts and generate statistical forecasts, and we build a process to approve a “consensus” forecast using as inputs the proposed customer forecasts and one or more benchmark time series forecasts.  This allows us to capture forward-looking information signaled in the customer forecast, while checking for bias and adding prediction intervals from the time series forecasts. A variant of this method would be to provide the customer with a baseline statistical forecast and allow them to make adjustments to it.  Either can work well, as long as the difference between the statistical forecast and approved consensus forecast is reviewed, understood and approved by a set of decision makers who are accountable for the costs of both under-forecasting and over-forecasting. Where there are significant gaps between a customer forecast and a statistical forecasts, the process requires a good story to explain the gap that everyone understands before it is approved as the consensus forecast.  It may take multiple forecasting cycles to resolve, but typically we see the following:&lt;/div&gt;&lt;div&gt;&lt;ul&gt;&lt;li&gt;an approval by all parties that the gap is legitimate due to forward-looking factors&lt;/li&gt;&lt;li&gt;removal of outlier events from history or model adjustments to improve the accuracy of statistical forecasts&lt;/li&gt;&lt;li&gt;convergence between the customer and statistical forecast&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;div&gt;In an internal customer-supplier setting, we have found it useful to require “consensus” to mean alignment between customers and other stakeholders, since the customers are the ones who most acutely feel the pain of shortages due to under-forecasting.  Also included in the approver group is Finance (who are particularly concerned about the costs of excess capacity) and operations teams (who are responsible for executing on the plans and help mediate between customers and Finance to drive forecast alignment).  &lt;br /&gt;&lt;br /&gt;There are more sophisticated alternatives, such as contracts and risk-sharing agreements between customers and suppliers.  In fact, there is a body of literature on optimal contracting structures between buyers and suppliers [5]. Unfortunately, formal risk-sharing agreements can be cumbersome and difficult to put in place.  This is particularly true in operational planning domains where contracting around risk is not common, as it is in financial markets. We have found that a simple but effective approach is to help customers forecast by providing useful statistical benchmark forecasts, while also inviting their input on what “inorganic” events may require adjustment of the statistical forecasts.  Not only does this improve forecast quality and build a common understanding of forecast drivers, it also &lt;b&gt;creates a shared fate&lt;/b&gt;.  As the old adage goes, all forecasts are wrong.  It is easy in hindsight for stakeholders to second-guess the data scientist’s statistical forecast if the data scientist did not make concerted efforts to consult them about forward-looking information they may have had. &lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Case study: machines demand planning&lt;/h2&gt;Below is an example of the evolution of an important strategic forecast process at Google. It illustrates the benefits and pitfalls of automation, and follows the thesis-antithesis-synthesis narrative.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;Original process — customer-driven forecast&lt;/h3&gt;&lt;div&gt;In our supply chain planning for new machines (storage and compute servers), we would stock inventory of component parts based on forecasts so that we could quickly build and deliver machines to fulfill demand from internal customers such as Search, Ads, Cloud, and Youtube.  The operations teams would plan with only high level “machine count” forecast based on input from our internal customers. There was no accounting for error in the customer forecasts and no credible benchmark time series forecast. There was little internal alignment between product and finance functions on the machine count forecasts — it was not uncommon to see a 2x difference in the machine count forecasts during annual planning discussions.  These would actually be competing forecasts, and there was no clear process for reconciling them nor documenting a single plan of record consensus forecast visible to all.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;This disconnect between customer and finance forecasts often was not resolved outside of supply chain lead time (over 6 months for some components). The teams planning component inventory were therefore left having to judge how much to discount the customer forecast (and risk being responsible for a shortage) or uplift the finance guidance (and risk being responsible for excess inventory).  Even if the operations team brought materials in early as a safety stock hedge, the forecasted mix of components would often be wrong. The outcome was both poor on-time delivery due to shortages on specific components and excess inventory due to overall machine count forecasts being too high. In the face of poor on-time delivery and long lead times for new machines, internal customers needed to hold large reserves to buffer against unpredictable machine deliveries.  We had the worst possible outcomes — high supply chain inventory and a poor customer experience that led to high idle deployed inventory in the fleet.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;First attempt — Stats forecast&lt;/h3&gt;&lt;div&gt;Our first attempt at fixing this problem was to remove all humans from the forecasting loop — we placed all our bets on the statistical forecasts for new component demand.  Our data scientists developed statistical forecasts for each machine component category and determined the forecast quantile we needed to plan against for each component type to meet on-time delivery targets for machines.  We invested several quarters in building and tuning the forecasting models to reduce the error as much as possible. But this was never implemented because the predictions required us to triple our safety stock inventory in order to meet our on-time delivery goals.  The forecast ranges were too wide and so the solution was just too costly. Component inventory is planned to high quantile forecasts, and &lt;b&gt;the high quantile forecast was driven by outlier behavior in the past&lt;/b&gt;, such as inorganic jumps in demand due to new product launches or specific machine configuration changes our customers requested.  Our customers often knew when those changes were coming. Trying to forecast based on history&amp;nbsp;alone, our fully-automated approach was ignoring this forward-looking information.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;Forecast triangulation&lt;/h3&gt;&lt;div&gt;We had to find a more efficient way to buffer for customer demand uncertainty that did not allow for unbounded customer forecast error based on past inorganic events or machine configuration changes.  We therefore invested in processes and tools that would consider customer forecasts as proposed forecasts, compare with benchmark forecasts, and arrive at consensus high scenario forecasts that supply chain teams could plan materials against.  This required investment from software engineering teams to capture forecasts from our customers in machine-readable form and convert those forecasts into units relevant to component capacity planning. It required investments from our data science team to re-think our statistical forecasting approach to make it easier to compare against customer forecasts.  Instead of forecasting machine components as we had first tried, &lt;b&gt;we forecasted closer to the true source of demand&lt;/b&gt; — customer compute and storage load at the data center level.  We forecasted load at a finer granularity that allowed us to compare customer and statistical capacity forecasts directly, netting load growth versus existing capacity already deployed in the fleet. Our prediction intervals were also shared with our customers as a tool to rightsize the amount of deployed inventory they needed to hold in the fleet. With this change in focus, while leaving the exact mix of machine configs to those customers who required specific machine types, our statistical forecasts were more stable with narrower ranges and a much more credible benchmark.&lt;br /&gt;&lt;br /&gt;The data science team also defined metrics to drive forecast accountability for both the operations teams and customer teams. Any significant shortage or excess in the consensus forecast could be traced back to its root cause based on the consensus forecast of record at lead time.  The operations team facilitated a monthly process to drive a rolling 4-quarter alignment between customers and Finance on the consensus “base” and “high” forecasts so that all downstream teams could confidently execute according to the high scenario forecast. &lt;br /&gt;&lt;br /&gt;This consensus building process helped us shape the future in a way that neither the customer-driven nor stats forecast alone could do.  By combining data science with process rigor, we could expose key risks and better manage them, expose disagreements between stakeholders and negotiate to resolve them.  This process reduced mean percent error (forecast bias) of the consensus forecast. We also learned that most of our customers most of the time were not actually asking us to cover nearly as large of an uncertainty range as estimated by our first attempt at an automated component forecast. The results are summarized in the table below.&lt;/div&gt;&lt;div&gt;&lt;br class=&quot;Apple-interchange-newline&quot; /&gt;&lt;div class=&quot;c2 c11&quot; style=&quot;font-family: Arial; font-size: 11pt; height: 11pt; line-height: 1.15; padding-bottom: 0pt; padding-top: 0pt;&quot;&gt;&lt;span class=&quot;c8&quot; style=&quot;background-color: white; color: #222222; font-size: 11pt; vertical-align: baseline;&quot;&gt;&lt;/span&gt;&lt;/div&gt;&lt;a href=&quot;https://www.blogger.com/null&quot; id=&quot;t.4d2543c5363de2bfc215e6a9846d9eb65d2df827&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://www.blogger.com/null&quot; id=&quot;t.1&quot;&gt;&lt;/a&gt;&lt;br /&gt;&lt;table class=&quot;c13&quot; style=&quot;border-collapse: collapse; border-spacing: 0px; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr class=&quot;c1&quot; style=&quot;height: 0pt;&quot;&gt;&lt;td class=&quot;c22&quot; colspan=&quot;1&quot; rowspan=&quot;1&quot; style=&quot;border-color: rgb(0, 0, 0); border-style: solid; border-width: 1pt; padding: 5pt; vertical-align: top; width: 133.5pt;&quot;&gt;&lt;div class=&quot;c9 c11&quot; style=&quot;font-family: Arial; font-size: 11pt; height: 11pt; line-height: 1; padding-bottom: 0pt; padding-top: 0pt;&quot;&gt;&lt;span class=&quot;c8&quot; style=&quot;background-color: white; color: #222222; font-size: 11pt; vertical-align: baseline;&quot;&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class=&quot;c19&quot; colspan=&quot;1&quot; rowspan=&quot;1&quot; style=&quot;border-color: rgb(0, 0, 0); border-style: solid; border-width: 1pt; padding: 5pt; vertical-align: top; width: 121.5pt;&quot;&gt;&lt;div class=&quot;c9&quot; style=&quot;font-family: Arial; font-size: 11pt; line-height: 1; padding-bottom: 0pt; padding-top: 0pt;&quot;&gt;&lt;span class=&quot;c0&quot; style=&quot;background-color: white; color: #222222; font-weight: 700;&quot;&gt;Customer-driven&lt;/span&gt;&lt;span class=&quot;c20&quot; style=&quot;background-color: white; color: #222222;&quot;&gt;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;c0&quot; style=&quot;background-color: white; color: #222222; font-weight: 700;&quot;&gt;forecast&lt;/span&gt;&lt;span class=&quot;c8&quot; style=&quot;background-color: white; color: #222222; font-size: 11pt; vertical-align: baseline;&quot;&gt;&amp;nbsp;with added safety stock&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class=&quot;c15&quot; colspan=&quot;1&quot; rowspan=&quot;1&quot; style=&quot;border-color: rgb(0, 0, 0); border-style: solid; border-width: 1pt; padding: 5pt; vertical-align: top; width: 102pt;&quot;&gt;&lt;div class=&quot;c9&quot; style=&quot;font-family: Arial; font-size: 11pt; line-height: 1; padding-bottom: 0pt; padding-top: 0pt;&quot;&gt;&lt;span class=&quot;c0&quot; style=&quot;background-color: white; color: #222222; font-weight: 700;&quot;&gt;Stats forecast&lt;/span&gt;&lt;span class=&quot;c8&quot; style=&quot;background-color: white; color: #222222; font-size: 11pt; vertical-align: baseline;&quot;&gt;&amp;nbsp;with prediction interval&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class=&quot;c12&quot; colspan=&quot;1&quot; rowspan=&quot;1&quot; style=&quot;border-color: rgb(0, 0, 0); border-style: solid; border-width: 1pt; padding: 5pt; vertical-align: top; width: 111pt;&quot;&gt;&lt;div class=&quot;c9&quot; style=&quot;font-family: Arial; font-size: 11pt; line-height: 1; padding-bottom: 0pt; padding-top: 0pt;&quot;&gt;&lt;span class=&quot;c17 c0 c23&quot; style=&quot;background-color: white; color: #222222; font-size: 11pt; font-weight: 700; vertical-align: baseline;&quot;&gt;Forecast triangulation&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;c9 c11&quot; style=&quot;font-family: Arial; font-size: 11pt; height: 11pt; line-height: 1; padding-bottom: 0pt; padding-top: 0pt;&quot;&gt;&lt;span class=&quot;c8&quot; style=&quot;background-color: white; color: #222222; font-size: 11pt; vertical-align: baseline;&quot;&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;c1&quot; style=&quot;height: 0pt;&quot;&gt;&lt;td class=&quot;c22&quot; colspan=&quot;1&quot; rowspan=&quot;1&quot; style=&quot;border-color: rgb(0, 0, 0); border-style: solid; border-width: 1pt; padding: 5pt; vertical-align: top; width: 133.5pt;&quot;&gt;&lt;div class=&quot;c9&quot; style=&quot;font-family: Arial; font-size: 11pt; line-height: 1; padding-bottom: 0pt; padding-top: 0pt;&quot;&gt;&lt;span class=&quot;c8&quot; style=&quot;background-color: white; color: #222222; font-size: 11pt; vertical-align: baseline;&quot;&gt;Component safety stock inventory levels&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class=&quot;c19&quot; colspan=&quot;1&quot; rowspan=&quot;1&quot; style=&quot;border-color: rgb(0, 0, 0); border-style: solid; border-width: 1pt; padding: 5pt; vertical-align: top; width: 121.5pt;&quot;&gt;&lt;div class=&quot;c9&quot; style=&quot;font-family: Arial; font-size: 11pt; line-height: 1; padding-bottom: 0pt; padding-top: 0pt;&quot;&gt;&lt;span class=&quot;c8&quot; style=&quot;background-color: white; color: #222222; font-size: 11pt; vertical-align: baseline;&quot;&gt;1x&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class=&quot;c15&quot; colspan=&quot;1&quot; rowspan=&quot;1&quot; style=&quot;border-color: rgb(0, 0, 0); border-style: solid; border-width: 1pt; padding: 5pt; vertical-align: top; width: 102pt;&quot;&gt;&lt;div class=&quot;c9&quot; style=&quot;font-family: Arial; font-size: 11pt; line-height: 1; padding-bottom: 0pt; padding-top: 0pt;&quot;&gt;&lt;span class=&quot;c8&quot; style=&quot;background-color: white; color: #222222; font-size: 11pt; vertical-align: baseline;&quot;&gt;3x&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class=&quot;c12&quot; colspan=&quot;1&quot; rowspan=&quot;1&quot; style=&quot;border-color: rgb(0, 0, 0); border-style: solid; border-width: 1pt; padding: 5pt; vertical-align: top; width: 111pt;&quot;&gt;&lt;div class=&quot;c9&quot; style=&quot;font-family: Arial; font-size: 11pt; line-height: 1; padding-bottom: 0pt; padding-top: 0pt;&quot;&gt;&lt;span class=&quot;c8&quot; style=&quot;background-color: white; color: #222222; font-size: 11pt; vertical-align: baseline;&quot;&gt;0.6x&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;c1&quot; style=&quot;height: 0pt;&quot;&gt;&lt;td class=&quot;c22&quot; colspan=&quot;1&quot; rowspan=&quot;1&quot; style=&quot;border-color: rgb(0, 0, 0); border-style: solid; border-width: 1pt; padding: 5pt; vertical-align: top; width: 133.5pt;&quot;&gt;&lt;div class=&quot;c9&quot; style=&quot;font-family: Arial; font-size: 11pt; line-height: 1; padding-bottom: 0pt; padding-top: 0pt;&quot;&gt;&lt;span class=&quot;c8&quot; style=&quot;background-color: white; color: #222222; font-size: 11pt; vertical-align: baseline;&quot;&gt;Excess component inventory due to forecast bias&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class=&quot;c19&quot; colspan=&quot;1&quot; rowspan=&quot;1&quot; style=&quot;border-color: rgb(0, 0, 0); border-style: solid; border-width: 1pt; padding: 5pt; vertical-align: top; width: 121.5pt;&quot;&gt;&lt;div class=&quot;c9&quot; style=&quot;font-family: Arial; font-size: 11pt; line-height: 1; padding-bottom: 0pt; padding-top: 0pt;&quot;&gt;&lt;span class=&quot;c8&quot; style=&quot;background-color: white; color: #222222; font-size: 11pt; vertical-align: baseline;&quot;&gt;~30% of forecast&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class=&quot;c15&quot; colspan=&quot;1&quot; rowspan=&quot;1&quot; style=&quot;border-color: rgb(0, 0, 0); border-style: solid; border-width: 1pt; padding: 5pt; vertical-align: top; width: 102pt;&quot;&gt;&lt;div class=&quot;c9&quot; style=&quot;font-family: Arial; font-size: 11pt; line-height: 1; padding-bottom: 0pt; padding-top: 0pt;&quot;&gt;&lt;span class=&quot;c8&quot; style=&quot;background-color: white; color: #222222; font-size: 11pt; vertical-align: baseline;&quot;&gt;0&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;c9&quot; style=&quot;font-family: Arial; font-size: 11pt; line-height: 1; padding-bottom: 0pt; padding-top: 0pt;&quot;&gt;&lt;span class=&quot;c8&quot; style=&quot;background-color: white; color: #222222; font-size: 11pt; vertical-align: baseline;&quot;&gt;(hypothetically)&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class=&quot;c12&quot; colspan=&quot;1&quot; rowspan=&quot;1&quot; style=&quot;border-color: rgb(0, 0, 0); border-style: solid; border-width: 1pt; padding: 5pt; vertical-align: top; width: 111pt;&quot;&gt;&lt;div class=&quot;c9&quot; style=&quot;font-family: Arial; font-size: 11pt; line-height: 1; padding-bottom: 0pt; padding-top: 0pt;&quot;&gt;&lt;span class=&quot;c8&quot; style=&quot;background-color: white; color: #222222; font-size: 11pt; vertical-align: baseline;&quot;&gt;0&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;c1&quot; style=&quot;height: 0pt;&quot;&gt;&lt;td class=&quot;c22&quot; colspan=&quot;1&quot; rowspan=&quot;1&quot; style=&quot;border-color: rgb(0, 0, 0); border-style: solid; border-width: 1pt; padding: 5pt; vertical-align: top; width: 133.5pt;&quot;&gt;&lt;div class=&quot;c9&quot; style=&quot;font-family: Arial; font-size: 11pt; line-height: 1; padding-bottom: 0pt; padding-top: 0pt;&quot;&gt;&lt;span class=&quot;c8&quot; style=&quot;background-color: white; color: #222222; font-size: 11pt; vertical-align: baseline;&quot;&gt;On-time delivery and fleet deployed inventory&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class=&quot;c19&quot; colspan=&quot;1&quot; rowspan=&quot;1&quot; style=&quot;border-color: rgb(0, 0, 0); border-style: solid; border-width: 1pt; padding: 5pt; vertical-align: top; width: 121.5pt;&quot;&gt;&lt;div class=&quot;c9&quot; style=&quot;font-family: Arial; font-size: 11pt; line-height: 1; padding-bottom: 0pt; padding-top: 0pt;&quot;&gt;&lt;span class=&quot;c8&quot; style=&quot;background-color: white; color: #222222; font-size: 11pt; vertical-align: baseline;&quot;&gt;Poor on-time delivery&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;c9&quot; style=&quot;font-family: Arial; font-size: 11pt; line-height: 1; padding-bottom: 0pt; padding-top: 0pt;&quot;&gt;&lt;span class=&quot;c8&quot; style=&quot;background-color: white; color: #222222; font-size: 11pt; vertical-align: baseline;&quot;&gt;→ high fleet inventory&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class=&quot;c15&quot; colspan=&quot;1&quot; rowspan=&quot;1&quot; style=&quot;border-color: rgb(0, 0, 0); border-style: solid; border-width: 1pt; padding: 5pt; vertical-align: top; width: 102pt;&quot;&gt;&lt;div class=&quot;c9&quot; style=&quot;font-family: Arial; font-size: 11pt; line-height: 1; padding-bottom: 0pt; padding-top: 0pt;&quot;&gt;&lt;span class=&quot;c8&quot; style=&quot;background-color: white; color: #222222; font-size: 11pt; vertical-align: baseline;&quot;&gt;Good (hypothetically)&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class=&quot;c12&quot; colspan=&quot;1&quot; rowspan=&quot;1&quot; style=&quot;border-color: rgb(0, 0, 0); border-style: solid; border-width: 1pt; padding: 5pt; vertical-align: top; width: 111pt;&quot;&gt;&lt;div class=&quot;c9&quot; style=&quot;font-family: Arial; font-size: 11pt; line-height: 1; padding-bottom: 0pt; padding-top: 0pt;&quot;&gt;&lt;span class=&quot;c8&quot; style=&quot;background-color: white; color: #222222; font-size: 11pt; vertical-align: baseline;&quot;&gt;Good&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;div style=&quot;text-align: center;&quot;&gt;Table 2: Forecast methods compared.&lt;/div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;h2&gt;Conclusion&lt;/h2&gt;&lt;/div&gt;&lt;div&gt;Compared with a purely algorithmic forecast, including humans in the loop certainly adds ambiguity, complexity and effort. This can be uncomfortable for data scientists, and can make us vulnerable to feeling insufficiently technical or scientific in our approach. To the extent possible, we all want to take technically rigorous approaches that are free from human bias. We dream of the one model to rule them all that has access to the perfect set of useful features with a long data history from which to learn. But in strategic forecasting, the available time series is short relative to the forecast horizon, and the time series is likely to be non-stationary. Perfection is not possible. &lt;b&gt;Ambiguity already exists in the business problem and in the variety of information one can bring to bear to solve it&lt;/b&gt;. Models that ignore key business drivers or uncertainties due to lack of hard data bring their own type of bias. It is the data scientist’s job to grapple with the ambiguity, frame the analytical problem, and establish a process in which decision makers make good decisions based on all the relevant information at hand. We believe this applies as much to forecasting as any other kind of data science. With oversight from good data scientists, there is much value in having humans in the loop of strategic forecasts. &lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;h3&gt;References&lt;/h3&gt;[1] C.Fry and M.Brundage, The M4 forecasting competition – A practitioner’s view. International Journal of Forecasting (2019),&amp;nbsp;&lt;a href=&quot;https://doi.org/10.1016/j.ijforecast.2019.02.013&quot;&gt;https://doi.org/10.1016/j.ijforecast.2019.02.013&lt;/a&gt;.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;[2] Tim Januschowski &amp;amp; Stephan Kolassa, 2019. &quot;&lt;a href=&quot;https://ideas.repec.org/a/for/ijafaa/y2019i52p36-43.html&quot;&gt;A Classification of Business Forecasting Problems&lt;/a&gt;,&quot; Foresight: The International Journal of Applied Forecasting, International Institute of Forecasters, issue 52, pages 36-43, Winter.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;[3] Hyndman, R.J., &amp;amp; Athanasopoulos, G. (2018) Forecasting: principles and practice, 2nd edition, OTexts: Melbourne, Australia. &lt;a href=&quot;http://otexts.com/fpp2&quot;&gt;OTexts.com/fpp2&lt;/a&gt;. Accessed on 2019-08-01.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;[4] Chambers, J., S. Mullick and D. Smith, “How to choose the right forecasting technique,” Harvard Business Review, July 1971, &lt;a href=&quot;https://hbr.org/1971/07/how-to-choose-the-right-forecasting-technique&quot;&gt;https://hbr.org/1971/07/how-to-choose-the-right-forecasting-technique&lt;/a&gt;.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;[5] Graves, S.C. and A.G. de Kok.  Supply Chain Management: Design, Coordination and Operation, 2003.  See the chapters “Supply Chain Coordination with Contracts” from G. Cachon and “Information Sharing and Supply Chain Coordination” by F. Chen.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;</content><link rel='replies' type='application/atom+xml' href='https://www.unofficialgoogledatascience.com/feeds/1477753203544658965/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='https://www.unofficialgoogledatascience.com/2019/12/humans-in-loop-forecasting-integrating.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/1477753203544658965'/><link rel='self' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/1477753203544658965'/><link rel='alternate' type='text/html' href='https://www.unofficialgoogledatascience.com/2019/12/humans-in-loop-forecasting-integrating.html' title='Humans-in-the-loop forecasting: integrating data science and business planning'/><author><name>Amir Najmi</name><uri>http://www.blogger.com/profile/18174523203317227640</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-E0pXVxSJ1x0/XefnvYCMtWI/AAAAAAAAeIg/NoFLZ-l_lu8g7Pa2kjqbC3sjrDKQNFIpACEwYBhgL/s72-c/Blue%2BLights%2Bin%2BServer%2BRow.jpg" height="72" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-5825758052688213474.post-6508474691442909455</id><published>2019-08-27T09:37:00.000-07:00</published><updated>2019-12-05T21:40:11.753-08:00</updated><title type='text'>Estimating the prevalence of rare events — theory and practice</title><content type='html'>by YI LIU&lt;br /&gt;&lt;br /&gt;&lt;i&gt;Importance sampling is used to improve precision in estimating the prevalence of some rare event in a population. In this post, we explain how we use variants of importance sampling to estimate the prevalence of videos that violate community standards on YouTube. We also cover many practical challenges encountered in implementation when the requirement is to produce fresh and regular estimates of prevalence.&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Background&lt;/h2&gt;Every day, millions of videos are uploaded to YouTube. While most of these videos are safe for everyone to enjoy, some videos violate the community guidelines of YouTube and should be removed from the platform. There is a wide range of &lt;a href=&quot;https://www.youtube.com/yt/about/policies/#community-guidelines&quot;&gt;policy violations&lt;/a&gt;, from spammy videos, to videos containing nudity, to those with harassing language. We want to estimate the prevalence of violation of each individual policy category (we call them &lt;i&gt;policy verticals&lt;/i&gt;) by sampling the videos and manually reviewing those sampled videos.&lt;br /&gt;&lt;br /&gt;Naturally, we get an unbiased estimate of the overall prevalence of violation if we sample the videos uniformly from the population and have them reviewed by human raters to estimate the proportion of violating videos. We also get an unbiased estimate of the violation rate in each policy vertical. But given the low probability of violation and wanting to use our rater capacity wisely, this is not an adequate solution — we typically have too few positive labels in uniform samples to achieve an accurate estimate of the prevalence, especially for those sensitive policy verticals. To obtain a relative error of no more than 20%, we need roughly 100 positive labels, and more often than not, we have zero violation videos in the uniform samples for rarer policies.&lt;br /&gt;&lt;br /&gt;Our goal is a better sampling design to improve the precision of the prevalence estimates. This problem can be phrased as an optimization problem — given some fixed review capacity&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;how should we sample videos?&lt;/li&gt;&lt;li&gt;how do we calculate the prevalence rate from these samples (with confidence intervals)?&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;Of course, any mistakes by the reviewers would propagate to the accuracy of the metrics, and the metrics calculation should take into account human errors. Furthermore, the metrics of interest could involve the prevalence of more than one policy vertical, and the choice of the metrics would affect how we design our sampling and objective function. We will defer rater accuracy and multiple policy verticals to future posts. In this post, we will assume that the raters always give the correct answer, and we only consider the binary label case.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Importance sampling&lt;/h2&gt;In the binary label problem, a video will be either labeled good or bad. Usually, bad videos only make up a tiny proportion of the population, so the two labels are highly imbalanced. Our goal here is to have an accurate and precise estimate of the proportion of bad videos in the population by sampling.&lt;br /&gt;&lt;br /&gt;As we note, uniform sampling is unlikely to get enough positive samples to draw inference about the proportion. But&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/Importance_sampling&quot;&gt;importance sampling&lt;/a&gt;&amp;nbsp;in statistics is a variance reduction technique to improve the inference of the rate of rare events, and it seems natural to apply it to our prevalence estimation problem. We refer interested readers to [1] (cited here with the author&#39;s permission) on the details of importance sampling, and will follow the notation from [1] in what follows.&lt;br /&gt;&lt;br /&gt;Suppose there are $N$ videos in the population&amp;nbsp;$V = \{v_1, \ldots, v_N\}$, and $p(v)$ is the probability mass function (PMF) of the video $v$ in $V$, such that $p(v) &amp;gt; 0$, $\sum_i p(v_i) = 1$. The PMF depends on how prevalence is defined. For example, if we care about prevalence in uploaded videos, each video has equal weight $p(v) = \frac{1}{N}$, whereas for prevalence in views, the weight of a video is its share of views during some given period.&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Furthermore, let $B$ be the set of all bad videos. Define $f(v) = [v \in B]$ where $[\cdot]$ maps the boolean value of the expression contained within to $0$ or $1$. Then the proportion of bad videos is $$&lt;br /&gt;\mu = P_p(B) = \sum_{i = 1} ^N f(v_i) p(v_i) = \mathbb{E}_{p}[f(V)]&lt;br /&gt;$$If  $q$ is another PMF defined on the videos such that $q(v_i) &amp;gt; 0$,  $\sum_{i=1} ^N q(v_i) = 1$, then we could also write the proportion as$$&lt;br /&gt;\mu = \sum_{i=1}^N f(v_i) q(v_i) \cdot \frac{p(v_i)}{q(v_i)} = \mathbb{E}_q\left[ f(V) \frac{p(V)}{q(V)}\right]$$&lt;/div&gt;&lt;div&gt;Instead of uniformly sampling from the target distribution $p$, importance sampling allows us to sample $n$ items $v_1, \ldots, v_n$ from the population &lt;b&gt;with replacement&lt;/b&gt;&amp;nbsp;(more on this later) using the importance distribution $q$, and to correct the sampling bias by inverse-weighting the sampling ratio $q / p$ in the estimator:$$&lt;br /&gt;&lt;br /&gt;\hat{\mu}_q =\frac{1}{n} \sum_{i = 1} ^ n f(v_i) \cdot \frac{p(v_i)}{q(v_i)}&lt;br /&gt;&lt;br /&gt;$$It is easy to show that the importance sampling estimator is unbiased for $\mu$. The variance of&amp;nbsp;$\hat{\mu}_q$ is\begin{align*}&lt;br /&gt;&lt;br /&gt;\texttt{Var}(\hat{\mu}_q) &amp;amp;= &lt;br /&gt;\frac{1}{N}\sum_{i=1}^N \frac{\left(f(v_i)p(v_i) \right)^2}{q(v_i)} -\mu^2 \\&lt;br /&gt;&amp;amp;= \frac{1}{N}\sum_{i=1}^N \frac{\left(f(v_i)p(v_i) - \mu q(v_i)\right)^2}{q(v_i)}&lt;br /&gt;&lt;br /&gt;\end{align*}The variance of our estimator depends on the proposed importance distribution $q$, and with appropriate choice of $q$, we can have a better estimator than by uniform sampling.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;From the equation above, the variance is minimized when $$q \propto f \cdot p$$that is, when $q$ is proportional to the target distribution $p$ on the set $B$, and 0 outside the set $B$. Unfortunately, this perfect importance distribution requires us to know $f$. If we could separate bad videos from good videos perfectly, we could simply calculate the metrics directly without sampling. But in a world without perfect knowledge of $f$, the heuristic still holds — if we have an approximate rule to separate bad videos from the good ones, we could use it to design our importance distribution $q$. A machine learning classifier serves this task perfectly.&lt;br /&gt;&lt;br /&gt;Formally, let $S(v)$ be the real-valued classifier score for video $v$ (if the score is categorical, we assume it has some number of distinct levels). The classifier score $S(v)$ contains whatever information we have about the chances that the video is bad — if two videos have the same score $S(v_1) = S(v_2)$, then the ratio in the importance distribution $q$ should be the same as the ratio in the target distribution $p$, i.e. $$&lt;br /&gt;S(v_1) = S(v_2) \implies \frac{q(v_1)}{p(v_1)} = \frac{q(v_2)}{p(v_2)}$$The ratio between the importance distribution and target distribution is thus a function of $S(v)$:$$ &lt;br /&gt;&lt;br /&gt;\frac{q(v)}{p(v)} = \frac{\tilde{q}(S(v))}{\tilde{p}(S(v))}&lt;br /&gt;&lt;br /&gt;$$where $\tilde{p}$ and $\tilde{q}$ are PMFs of $S(v)$ under the target distribution and importance distribution respectively.&lt;/div&gt;&lt;br /&gt;As noted, we do not know $f$ precisely but have a good estimate of it in the form of a score $S$. For now, assume that we have a well-calibrated function $g$ such that $$&lt;br /&gt;g(S(v)) = \mathbb{E}_p\left[f(V)|S(V)=S(v)\right]&lt;br /&gt;$$That is, $g$ maps the score to the fraction of videos having that score that are bad. To choose $\tilde{q}$, we first observe that when $f$ is binary, the formula for $\texttt{Var}(\hat{\mu}_q)$ given earlier is minimized by minimizing$$&lt;br /&gt;\sum_{i=1}^N \frac{f(v_i) p(v_i)^2}{q(v_i)}&lt;br /&gt;$$While we cannot minimize this quantity, we can minimize our best guess of it. Thus, optimal q is given by$$&lt;br /&gt;&lt;br /&gt;q^* = \arg \min_{q} \sum_{i=1}^N \frac{g(S(v_i))p(v_i)^2}{q(v_i)}&lt;br /&gt;\mbox{ such that } \sum_{i=1}^N q(v_i)=1&lt;br /&gt;&lt;br /&gt;$$Applying Lagrange multipliers, this constrained optimization leads to the choice$$&lt;br /&gt;&lt;br /&gt;q^*(v) \propto p(v)\sqrt{g(S(v))}&lt;br /&gt;&lt;br /&gt;$$Thus, the importance distribution that minimizes the infinite population variance is\begin{align*}&lt;br /&gt;&lt;br /&gt;\frac{q(v)}{p(v)} &amp;amp;\propto \sqrt{g(S(v))}\\&lt;br /&gt;&amp;amp;= \sqrt{ \mathbb{E}_p\left[f(V)|S(V)=S(v)\right]}&lt;br /&gt;&lt;br /&gt;\end{align*}where $g(v)=\mathbb{E}_p\left[f(V)|S(V)=S(v)\right]$ is the conditional prevalence given the classifier score.&lt;br /&gt;&lt;br /&gt;&lt;div&gt;&lt;h2&gt;Post-stratification&lt;/h2&gt;When the probability of sampling video $v$ depends only on its score $S(v)$, importance sampling is the same as random sampling within the same score category. We can leverage the score&amp;nbsp;information and use post-stratification to&amp;nbsp; reduce the strata variability introduced by sampling:$$&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;\hat{\mu}_{q, PS} =&lt;br /&gt;\sum_s \tilde{p}(s) &lt;br /&gt;\frac{\sum_{i=1}^N f(v_i) [S(v_i) = s]}{\sum_{i=1}^n [S(v_i) = s]}&lt;br /&gt;&lt;br /&gt;$$To see the connection between $\hat{\mu}_{q, PS}$ and $\hat{\mu}_q$, we can rewrite $\hat{\mu}_q$:$$&lt;br /&gt;&lt;br /&gt;\hat{\mu}_q = \frac{1}{n}\sum_{i=1}^n f(v_i)\frac{\tilde{p}(S(v_i))}{\tilde{q}(S(v_i))} = \sum_{s}\tilde{p}(s)\frac{\sum_{i=1}^n f(v_i) [S(v_i) = s] }{n\tilde{q}(s)}&lt;br /&gt;&lt;br /&gt;$$This shows that $\hat{\mu}_q$ calculates the per stratum prevalence rate using the expected sample size $n\tilde{q}(s)$ in the denominator, whereas $\hat{\mu}_{q,PS}$ divides by the actual number of samples in each stratum.&lt;br /&gt;&lt;br /&gt;&lt;div&gt;With the estimator $\hat{\mu}_{q, PS}$, the optimal sampling weight is proportional to the stratum standard error (see [7]):\begin{align*}&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;\frac{q(v)}{p(v)}&lt;br /&gt;&amp;amp;\propto&lt;br /&gt;\sqrt{g(S(v)) \left(1 - g(S(v)) \right) } \\&lt;br /&gt;&amp;amp;=&lt;br /&gt;\sqrt{ \mathbb{E}_p\left[f(V)|S(V)=S(v)\right] \left( 1 - \mathbb{E}_p\left[f(V)|S(V)=S(v)\right]\right) }&lt;br /&gt;&lt;br /&gt;\end{align*}The biggest difference between post-stratification and importance sampling is that the optimal sampling weight for post-stratification peaks at the conditional prevalence rate of $\frac{1}{2}$, while the optimal weight for importance sampling is monotone increasing as conditional prevalence rate increases. In our case when the events are rare and the probability of high conditional prevalence rate is small under the target distribution, the difference between the methods is minor. But in other applications, using post-stratification might further reduce the variance to a significant degree.&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;We discuss other practical benefits of post-stratification estimator $\mu_{q,PS}$ in the next section.&lt;/div&gt;&lt;/div&gt;&lt;br /&gt;&lt;h2&gt;Complications of implementation&lt;/h2&gt;So far, we have seen how importance sampling and post-stratification can provide an unbiased, more precise estimate of prevalence of bad videos. In reality, there are many factors that might affect the unbiasedness and precision of the estimator. In this section, we focus on the practical issues of sampling and estimation, especially how different conditions might affect our estimator. We also discuss how to choose $q$ with respect to the conditional prevalence rate $g(S(v))=\mathbb{E}_p\left[f(V)|S(V)=S(v)\right]$.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Sampling algorithm&lt;/h3&gt;If the sample size $n$ is fixed, sampling algorithms could be classified as sampling with replacement or sampling without replacement. The difference between the two is whether the same item can appear in the sample multiple times. Our estimators $\mu_{q}$ and $\mu_{q, PS}$ rely on the assumption of sampling with replacement. The measurement may be biased if our samples are generated from a procedure that samples without replacement, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Reservoir_sampling&quot;&gt;reservoir sampling&lt;/a&gt;, especially if some items have disproportionate weight, i.e., $q(v_i) \cdot n$ is large. For videos and other web content, it is common for the most popular items to account for a significant fraction of views.&lt;br /&gt;&lt;br /&gt;To see how much of a difference this makes, we can compare the number of unique items in the two sampling schemes. It is clear that there are $n$ unique items in a sampling without replacement. If the unique items in samples with replacement is close to $n$, the influence of individual items on the metric might be small, and we might use sampling without replacement to approximate the procedure of sampling with replacement.&lt;br /&gt;&lt;br /&gt;&lt;div&gt;The number of unique items in sampling with replacement is\begin{align*}&lt;br /&gt;&lt;br /&gt;\sum_{i=1}^N\left[1 - (1 - q(v_i))^n\right] &lt;br /&gt;&amp;amp;\approx&lt;br /&gt;\sum_{i=1}^N\left[  n q(v_i) - \frac{n^2}{2} q(v_i)^2 \right] \\&lt;br /&gt;&amp;amp;= n - \frac{n^2}{2} \sum_{i=1}^N q(v_i)^2&lt;br /&gt;&lt;br /&gt;\end{align*}So the expected number of unique items is controlled by $n ^ 2 \sum_{i=1}^N q(v_i)^2$, and we should be able to use sampling without replacement if $q(v_i)^2$ is small.&lt;br /&gt;&lt;br /&gt;With this insight, we could artificially split all videos to $k$ independent copies $(v_i, q_i) \rightarrow (v_i, q_i/k), \ldots, (v_i, q_i/k)$, so each item might appear in the sample at most $k$ times. The sum of weights squared after splitting is $\sum_{i=1}^N q_i^2 / k$, and thus mitigates the problem of skewness in the weights.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;Alternatively, if we allow the sampling size to vary, we could make use of Poisson sampling. Poisson sampling takes a similar idea from the&amp;nbsp;&lt;a href=&quot;http://www.unofficialgoogledatascience.com/2015/08/an-introduction-to-poisson-bootstrap26.html&quot;&gt;Poisson bootstrap&lt;/a&gt;&amp;nbsp;— sampling with replacement is sampling from a multinomial distribution $\texttt{Multinom}(n, (q_1,\ldots, q_N))$. With fixed size $n$, the number of times an individual video $v_i$ occurs in the sample is $\texttt{Binom}(n, q_i)$ marginally, and this can be approximated by the Poisson distribution $\texttt{Pois}(n q_i)$ when $nq_i$ is not large.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;Missing reviews&lt;/h3&gt;One challenge we have with human eval data is that we don&#39;t have individual verdict $f(v_i)$ immediately after sampling, and it takes time before we can use it to calculate the metric. Quite often, when we calculate the prevalence metrics, the number of samples with a verdict $n^*$ is smaller than the total sample size. The missing verdicts create two problems.&lt;br /&gt;&lt;br /&gt;First, we cannot assume the metric we calculate is a good approximation of the &quot;true metric&quot;. If missingness of the verdict is correlated with the label, ignoring missing items might bias the metric. For example, if positive items take longer time to verify because of additional steps, then ignoring those items might underestimate the prevalence metric. We cannot correct such biases in our estimators, and to limit the impact of the missing verdicts, we should wait to&amp;nbsp;compute the metrics until the missing verdict rate is small.&lt;br /&gt;&lt;br /&gt;Second, even if the missing verdict rate is small, we might still need to modify our estimators to adjust for it. For the importance sampling estimator $\hat{\mu}_q$, we divide the weighted sum by the number of samples $n$, and with missing verdicts, it seems natural to calculate the metrics using only reviewed items and divide the weighted sum by the number of reviewed samples $n^*$. However, if the missing verdict rate depends on the classifier score, then we may have $$&lt;br /&gt;\mathbb{E}_q \left[\frac{p(V)}{q(V)} |V \text{ is reviewed} \right] \neq 1&lt;br /&gt;$$Because our derivation of importance sampling required that $\mathbb{E}_q \left[\frac{p(V)}{q(V)} \right] = 1$, using $n^*$ as the denominator may be biased. But this can happen easily in practice. For example, if we want to remove bad content from the platform as soon as possible, we may prioritize reviewing the content with the highest classifier scores, and the ones missing review are more likely to be those videos with low classifier scores.&lt;br /&gt;&lt;div&gt;&lt;br /&gt;Suppose we accept the missing at random (MAR) assumption, that the probability the video is missing a verdict is independent of its label given the classifier score. In this case, we can remove bias by using the post-stratification estimator $\mu_{q, PS}$. Under the MAR assumption, both the strata weights in the target distribution $p$ and the conditional prevalence rate is unbiased. Even if this MAR assumption is not true, post-stratification seems a more robust estimator than the importance sampling estimator.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;Choosing the importance distribution&lt;/h3&gt;We have that seen the optimal importance distribution $q*$ depends on the conditional prevalence rate and the target distribution (it also depends whether we use post-stratification or not), but the conditional prevalence rate is unknown and needs to be estimated. There are many strategies we can use to estimate this quantity, and we will discuss each option in detail.&lt;br /&gt;&lt;br /&gt;First, if the classifier prediction is a bona fide probability of the video being bad, we can use the score directly. This is straightforward and easy to implement. But we find that the predicted score is often calibrated to the training data. When training a classifier with few positives in the population, one common strategy is to over sample items with positive labels, and/or down sample items with negative labels. The ratio between positive and negative samples in the training data directly affects the probability score of the classifier, so the classifier score usually is not calibrated to the probability.&lt;br /&gt;&lt;br /&gt;In this situation, we may use historical data to estimate the conditional prevalence rate. Say we observe $(S(v_i), f(v_i))$ pair from past samples, and we would like to calculate $g(s) = \mathbb{E}_p\left[f(V)|S(V)= s \right]$ from the observations. With finite observations on a continuous classifier score $S(v_i)$, the sample conditional prevalence in the stratum would make for a volatile estimation. There are two ideas to improve the estimates:&lt;br /&gt;&lt;ol&gt;&lt;li&gt;We might smooth the estimation by fitting a regression on the given data. For example, we could fit a logistic regression on a smooth transform of $(S(v_i)$ (e.g. polynomial or spline) to estimate the conditional prevalence. This reduces the number of parameters to estimate.&lt;/li&gt;&lt;li&gt;We may also reduce the number of parameters we need to estimate, by bucketing the $S(v_i)$ into a few discrete buckets and estimating the prevalence rate within each bucket. By reducing the parameters we want to estimate, the sparse observations become dense, and we will have a better estimation of each classifier bucket. &lt;/li&gt;&lt;/ol&gt;It is worth comparing the two strategies —&amp;nbsp;both estimate the conditional prevalence with fewer parameters. Regression implicitly uses information (borrows strength) from other scores to estimate the results of a given score, while discretizing and merging buckets does this in a more explicit manner while only using local information. The bucketing method also changes the importance sampling to a stratified sampling setting, and allows us to use binomial confidence intervals to estimate the uncertainty of our estimate (more on that later). But the sampling rate is discontinuous at the score bucket boundary and not smooth as with the regression method. &lt;br /&gt;&lt;br /&gt;Whether or not we borrow strength from other scores also impacts the estimation. In our case when the population prevalence rate is low, it is quite common to have few positive items in historical samples for when the classifier score is low. The regression method extrapolates the prevalence of high score regions and extends the estimate to low score regions. On the other hand, the bucketing method might put 0 as the prevalence rate if no positive items occur in the bucket, and the importance sampling distribution using this prevalence rate would not sample any new items from the region (we could adjust the bucket boundary to avoid that).&lt;br /&gt;&lt;br /&gt;When we fit the historical data, it is important to be aware of the following caveats:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;The estimates depend on the distribution of historical samples, which need not be identical to the target distribution. If we use importance sampling on historical samples, we would inversely weigh each observation in the sample and fit a weighted regression or weighted average to form a better estimate.&lt;/li&gt;&lt;li&gt;In regression, the scale of the classifier score $S(v_i)$ matters. Because binary classifiers often use softmax to normalize the predicted score, it makes sense to reverse the predicted score $S(v_i)$ with the &lt;a href=&quot;https://en.wikipedia.org/wiki/Logit&quot;&gt;logit function&lt;/a&gt; and fit the regression on the real line.&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;All methods discussed in this section rely on past data (either from training data or historical samples). However, the distribution of bad videos might not remain constant over time due to the adversarial nature of the problem. For example, if the video violation is correlated with classifier score, bad actors might upload more videos that are harder to detect by the classifiers, resulting in a shifting of conditional prevalence over time. To mitigate this issue, we may limit to only using recent data, or adding a time decay function to down-weight items further in the past. &lt;br /&gt;&lt;br /&gt;More generally, we can apply defensive importance sampling [8] in which the importance sampling distribution is a mixture of our target distribution $p$ and the estimated importance distribution $\hat{q}$ to deal with shifts in either conditional prevalence or the region where the prevalence is most uncertain:$$&lt;br /&gt;&lt;br /&gt;{\hat{q}_{\lambda}(v)} = \lambda p(v)+ (1 - \lambda){\hat{q}(v)}&lt;br /&gt;&lt;br /&gt;$$With the mixture distribution, we can guaranteed that inverse sampling ratio $$&lt;br /&gt;&lt;br /&gt;\frac{p(v)}{\hat{q}_{\lambda}(v)} \leq 1 / \lambda&lt;br /&gt;&lt;br /&gt;$$ is bounded above, and we can allocate enough samples even when $\hat{q}$ is very small in the regions with few positives (or few negatives using optimal weight from post-stratification). The choice of $\lambda$ would be a small value between 0.1 and 0.5 as estimated from the data (details in [8]).&lt;/div&gt;&lt;div&gt;&lt;span id=&quot;docs-internal-guid-8b0027ae-7fff-15d9-d3c0-fee64bec9dfc&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;h2&gt;Confidence Intervals&lt;/h2&gt;Besides the point estimate of the prevalence metric, we are also interested in constructing a confidence interval to indicate the accuracy of the estimate. Typically, we have the variance formula for $\hat{\mu}_q$ or $\hat{\mu}_{q, PS}$, and we could use a z-score to calculate the confidence interval of the metric.&lt;br /&gt;&lt;br /&gt;However, in the case of binomial proportions far from $\frac{1}{2}$, estimates of $\hat{\mu}_q$ and $\hat{\mu}_{q, PS}$ on finite samples converge poorly to the true uncertainty. As a result, confidence intervals from the z-distribution have poor coverage (see [2]). &lt;br /&gt;&lt;br /&gt;To demonstrate this issue, consider the following toy example with categorical scores:&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div align=&quot;left&quot; dir=&quot;ltr&quot; style=&quot;margin-left: 0pt;&quot;&gt;&lt;table style=&quot;border-collapse: collapse; border: none; table-layout: fixed; width: 468pt;&quot;&gt;&lt;colgroup&gt;&lt;col&gt;&lt;/col&gt;&lt;col&gt;&lt;/col&gt;&lt;col&gt;&lt;/col&gt;&lt;col&gt;&lt;/col&gt;&lt;/colgroup&gt;&lt;tbody&gt;&lt;tr style=&quot;height: 0pt;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; overflow-wrap: break-word; overflow: hidden; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;Score&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; overflow-wrap: break-word; overflow: hidden; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;Prevalence&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; overflow-wrap: break-word; overflow: hidden; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;Population Proportion&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; overflow-wrap: break-word; overflow: hidden; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;Sample Proportion&amp;nbsp;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0pt;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; overflow-wrap: break-word; overflow: hidden; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;Low Risk&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; overflow-wrap: break-word; overflow: hidden; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0.1%&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; overflow-wrap: break-word; overflow: hidden; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;95%&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; overflow-wrap: break-word; overflow: hidden; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;66.7%&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 23pt;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; overflow-wrap: break-word; overflow: hidden; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;High Risk&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; overflow-wrap: break-word; overflow: hidden; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;10%&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; overflow-wrap: break-word; overflow: hidden; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;5%&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; overflow-wrap: break-word; overflow: hidden; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;33.3%&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;&lt;br /&gt;The sampling proportions are chosen to be optimal for post-stratification. The following chart shows the point estimate and z-confidence interval of this toy example with 400 Monte Carlo simulations. From the chart, as the sample size increases from 1,000 to 100,000, the length of the confidence interval is more consistent, and the mis-coverage case is more uniformly distributed on both sides.&lt;br /&gt;&lt;div&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;img height=&quot;320&quot; src=&quot;https://lh5.googleusercontent.com/-MUUO0MZ0Xlt1Slj5FYF1Uow--NABGrSD_KO-j-QyzQXXT2fdCw9eec8VKo8nT-exeKAl9lDBBFgDfFGL9jrX2RobkfcE4buEXu9x1Y2jFtFEub8ND-lk1swvRPcZ1jizA_ReKMI&quot; style=&quot;margin-left: auto; margin-right: auto; margin-top: 0px;&quot; width=&quot;640&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot;&gt;Confidence intervals ordered by the point estimate. Uncovered intervals are red.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;If we zoom in to the case with 1,000 samples, we see that the CI width is highly correlated with the number of positive items in the low risk bucket. When there are no positives in that bucket, the sample standard error is on average 40% smaller than the one using theoretical standard deviation, resulting in poor coverage.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;span style=&quot;border: none; display: inline-block; height: 219px; margin-left: 1em; margin-right: 1em; overflow: hidden; width: 624px;&quot;&gt;&lt;img height=&quot;224&quot; src=&quot;https://lh5.googleusercontent.com/R-sKef0-ibzyNeV_qO_FutHYi9aG68xmSu6GBXO87xjFiGfjiMtablhN8bh6RFaqDD3WnDjGrXE3-h5V4FI0hQ2UkZa1rl76i-74yzEePsaXLwyzZgGVe_iZdzqiJesrJZ-mrFDR&quot; style=&quot;margin-left: 0px; margin-top: 0px;&quot; width=&quot;640&quot; /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span id=&quot;docs-internal-guid-f2cc569c-7fff-4394-a839-cd7bfbd0a19d&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;The situation is slightly better when we have a larger sample, but even with 10,000 samples (expected 6.7 positive in the low risk bucket), having 5 or fewer positive samples (about 1/3&amp;nbsp;of the chance) would result in the standard error being at least 9% smaller than the theoretical standard deviation.&lt;br /&gt;&lt;br /&gt;Under-estimation of standard error essentially happens when we do not have enough positives in certain regions. But here is the thing —&amp;nbsp;importance sampling makes the problem worse by down-sampling such regions, resulting in even more sparse positive items!&lt;br /&gt;&lt;br /&gt;Luckily, the binomial proportion estimation literature contains&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval&quot;&gt;several methods&lt;/a&gt; other than the normal approximation interval (the Wald interval), and [3] suggests using either the Jeffreys interval,&amp;nbsp;the Agresti-Coull interval, or the Wilson score interval to estimate the proportion. Jeffreys interval is a Bayesian posterior interval with prior $\texttt{Beta}(\frac{1}{2}, \frac{1}{2})$. The Agresti-Coull interval adds pseudo counts 2 to success and failures before using the normal approximation, and the center of the Wilson score interval may also be interpreted as adding pseudo counts.  Thus all these methods are connected to adding pseudo counts and shrinking the point estimate towards $\frac{1}{2}$. &lt;br /&gt;&lt;br /&gt;When we use post-stratification in our importance sampling, we could apply a similar idea to estimate the standard error (or posterior interval for Bayesian methods) in each stratum before combining them into an overall CI. But none of these methods yields a satisfactory result in our case because the positive events are rare, and shrinking the prevalence estimate to $\frac{1}{2}$&amp;nbsp;will overestimate the true prevalence. An alternative is to the Stratified Wilson Interval estimate [3] such the pseudo counts added to each stratum depend on the overall interval width.&lt;br /&gt;&lt;div&gt;&lt;br /&gt;Of course, which CI method works best for the problem depends on many factors and no method is universally better than others. In our problem, the stratified Wilson interval works well for our video sampling problem.&lt;br /&gt;&lt;br /&gt;To demonstrate the performance of different CI methods, we created a synthetic data with 5 strata that is similar to our video sampling problem. The following chart shows the coverage of 95% CI of different methods for each method under different sample sizes using 4000 simulations.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;img height=&quot;332&quot; src=&quot;https://lh5.googleusercontent.com/HWNyKNP9XEXwj3Iff_gJ0mlhvJ0Rn1_2wYR-d318OYQHZqJ9ZaMGTvcRZx_Xw0KXKYJoS0cuY7-UZvF6tN2TSz77DaOZ_ufQRgHiP023whq-pAZXOhfOmf5vKlo56-MWMlyrA3AT&quot; style=&quot;margin-left: auto; margin-right: auto; margin-top: 0px;&quot; width=&quot;640&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot;&gt;Coverage of 95% CIs with 95% confidence bands.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;div&gt;&lt;span id=&quot;docs-internal-guid-d5ff281e-7fff-9eb1-93c7-d708d3c94483&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;We see that confidence intervals using the normal approximation perform poorly until we have about 100,000 samples. Jeffreys CI under-covers when the number of samples is small, while the stratified Wilson CI is a little more conservative, having coverage greater than nominal.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;If we drill down into the cases when the CIs do not cover the true prevalence rate, it is clear that Jeffreys CI tends to overestimate the CI (the lower end of the CI is above the ground truth). This happens because the prior shrinks the estimate towards $\frac{1}{2}$. CI using the normal approximation has the opposite issue — because of zero counts, the point estimate is more likely to be too small than too big. In contrast, the miss-coverage of the stratified Wilson CI is more balanced when we have about 25,000 samples.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;img height=&quot;332&quot; src=&quot;https://lh5.googleusercontent.com/lCZwpnlDUL-G4BU__0rOTlWhDhCuqEPtHeThc5C-Z7ucDdq_N-STkq9q5OBJ5HVmnjuaLB56jUn_MQR4o9IZ55LLiOnULznE-30e0EoAYbnXMOSVqL1e8pVdG66dRebYBz0-ffX7&quot; style=&quot;margin-left: auto; margin-right: auto; margin-top: 0px;&quot; width=&quot;640&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot;&gt;Miss-coverage rate with 95% confidence bands.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;div&gt;&lt;span id=&quot;docs-internal-guid-9798c4fd-7fff-2b11-9a0e-da12c8e2dbc3&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;br /&gt;&lt;h2&gt;How Many Strata?&lt;/h2&gt;If post-stratification and stratified Wilson confidence interval can provide better uncertainty estimates, then it is natural to ask how many strata do we need. It really depends on the problem, but a rule of thumb is there should be enough positive and negative items in each stratum. If two strata do not have any positive items, with different sampling rates, merging them into a single strata without changing the number of samples will increase the effective sample size. &lt;br /&gt;&lt;br /&gt;There is an additional benefit of having coarse strata —&amp;nbsp;it makes it harder for videos to move into different buckets when we retrain a new version of the classifier. This makes aggregating the metrics over multiple periods more robust, as we may sample items on different days with different versions of the classifier.&lt;br /&gt;&lt;div&gt;&lt;br /&gt;Cochran recommends no more than 6 or 7 strata [4] while others [5] suggest little to gained from going beyond 5 to 10 strata. Once you fix strata size, you might consider using the Danlenius-Hodges method [6] to select strata boundary.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h2&gt;Conclusion&lt;/h2&gt;In this post, we discussed the theory of importance sampling to improve the precision estimate of the binomial proportion of rare events. We also discuss how to design a sampling scheme and choose the appropriate aggregation metric to address practical concerns. The improvement of the metric depends on the precision/recall of the classifier, as well as many other factors. In our problem, when we apply post-stratification to importance sampling, we are able to more than triple the count positive items in the sample while reducing the CI width of the estimator by more than 30%. We hope you see similar gains in your application.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;h2&gt;References&lt;/h2&gt;[1] Art Owen. Unpublished book chapter on &lt;a href=&quot;https://statweb.stanford.edu/~owen/mc/Ch-var-is.pdf&quot;&gt;importance sampling&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;[2] Lawrence Brown, Tony Cai, Anirban DasGupta (2001). Interval Estimation for a Binomial Proportion. Statistical Science. 16 (2): 101–133. &lt;br /&gt;&lt;br /&gt;[3] Xin Yan, Xiao Gang Su. &lt;a href=&quot;https://s3.amazonaws.com/academia.edu.documents/44364976/Statistical-Research-in-Biopharmaceutical-Research-2010.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&amp;amp;Expires=1533861265&amp;amp;Signature=O%2FVZS%2F9dmMJ7zzBJnUHApWXfIso%3D&amp;amp;response-content-disposition=inline%3B%20filename%3DStratified_Wilson_and_Newcombe_Confidenc.pdf&quot;&gt;Stratified Wilson and Newcombe Confidence Intervals for Multiple Binomial Proportions&lt;/a&gt;. Statistics in Biopharmaceutical Research, 2010.&lt;br /&gt;&lt;br /&gt;[4] William Cochran (1977). Sampling Techniques pp 132-134.&lt;br /&gt;&lt;br /&gt;[5] Ray Chambers, Robert Clark (2012). An Introduction to Model-Based Survey Sampling with Applications.&lt;br /&gt;&lt;br /&gt;[6] Tore Dalenius, Joseph Hodges (1959). Minimum Variance Stratification. Journal of the American Statistical Association, 54, 88-101.&lt;br /&gt;&lt;br /&gt;[7] Neyman, J. (1934). “On the Two Different Aspects of the Representative Method: The Method of Stratified Sampling and the Method of Purposive Selection”, Journal of the Royal Statistical Society, Vol. 97, 558-606.&lt;br /&gt;&lt;br /&gt;[8] Tim Hesterberg (1995). Weighted Average Importance Sampling and Defensive Mixture Distributions. Technometrics, 37(2): 185 - 192.&lt;/div&gt;</content><link rel='replies' type='application/atom+xml' href='https://www.unofficialgoogledatascience.com/feeds/6508474691442909455/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='https://www.unofficialgoogledatascience.com/2019/08/estimating-prevalence-of-rare-events.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/6508474691442909455'/><link rel='self' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/6508474691442909455'/><link rel='alternate' type='text/html' href='https://www.unofficialgoogledatascience.com/2019/08/estimating-prevalence-of-rare-events.html' title='Estimating the prevalence of rare events — theory and practice'/><author><name>Amir Najmi</name><uri>http://www.blogger.com/profile/18174523203317227640</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://lh5.googleusercontent.com/-MUUO0MZ0Xlt1Slj5FYF1Uow--NABGrSD_KO-j-QyzQXXT2fdCw9eec8VKo8nT-exeKAl9lDBBFgDfFGL9jrX2RobkfcE4buEXu9x1Y2jFtFEub8ND-lk1swvRPcZ1jizA_ReKMI=s72-c" height="72" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-5825758052688213474.post-1194836254357685717</id><published>2019-04-16T17:01:00.000-07:00</published><updated>2019-04-16T17:01:22.662-07:00</updated><title type='text'>Misadventures in experiments for growth</title><content type='html'>&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &#39;Google Sans&#39;,sans-serif; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;&lt;span id=&quot;docs-internal-guid-783ba33d-7fff-331d-e6c0-5e657e95bfe8&quot; style=&quot;font-weight: normal;&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;by MICHAEL FORTE&lt;br /&gt;&lt;br /&gt;&lt;i&gt;Large-scale live experimentation is a big part of online product development. In fact, this blog has published posts on this very topic. With the right experiment methodology, a product can make continuous improvements, as Google and others have done. But what works for established products may not work for a product that is still trying to find its audience. Many of the assumptions on which the &quot;standard&quot; experiment methodology is premised are not valid. This means a small and growing product has to use experimentation differently and very carefully. Indeed, failure to do so may cause experiments to mislead rather than guide. This blog post is about experimentation in this regime.&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Established versus fledgling products&lt;/h2&gt;For the purpose of this post, &quot;established products&quot; are products that have found viable segments of their target user populations, and have sustained retention among those segments. These established products fill a particular need for a particular set of users, and while these products would want to expand, they do not need to as a matter of existence. Product viability these days does not necessarily mean being a financially viable standalone product either. Fulfilling unmet user needs is often enough to be of value to a larger product that might someday purchase you. For established products, growth is structured as incremental rather than a search for viability, or a matter of survival.&lt;br /&gt;&lt;br /&gt;In contrast, &quot;fledgling products&quot; are products that are still trying to find their market. Now how is it possible for these fledgling products exist, do something, have enough users that one could contemplate experimentation, and yet still not have market fit? Wonders of the internet (and VC funding)! Modern products often don&#39;t start with set-in-stone business models because starting and scaling costs are low. Modern products often start with an idea, but then gather enough momentum to pivot to fill emergent needs. You do some cool stuff and then try to figure out from usage patterns what hole your product is filling in the world (so-called &quot;paving the cowpath&quot;). Instrumentation and analysis are critical to finding this unexpected use.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Why does anyone use experiments?&lt;/h2&gt;Let&#39;s revisit the various reasons for running experiments to see how relevant they are for a fledgling product:&lt;div&gt;&lt;br /&gt;&lt;h3&gt;To decide on incremental product improvements&lt;/h3&gt;This is the classic use case of experimentation. Such decisions involve an actual hypothesis test on specific metrics (e.g. version A has better task completion rates than B) that is administered by means of an experiment. Are the potential improvements realized and worthwhile? This scenario is typical for an established product. Often, an established product will have an overall evaluation criterion (OEC) that incorporates trade-offs among important metrics and between short- and long-term success. If so, decision making is further simplified.&lt;br /&gt;&lt;br /&gt;On the other hand, fledgling products often have neither the statistical power to identify the effects of small incremental changes, nor the luxury to contemplate small improvements. They are usually making big changes in an effort to provide users a reason to try and stick with their fledgling product.&lt;div&gt;&lt;br /&gt;&lt;h3&gt;To do something sizable&lt;/h3&gt;Sizable changes are the bulk of changes a fledgling product is making. But these are not usually amenable to A/B experimentation. The metrics to measure the impact of the change might not yet be established. Typically, it takes a period of back-and-forth between logging and analysis to gain the confidence that a metric is actually measuring what we designed for it to measure. Only after such validation would a product make decisions based on a metric. With major changes, the fledgling product is basically building the road as it travels on it.&lt;br /&gt;&lt;br /&gt;That said, there might still be reasons to run a long-term holdback experiment (i.e. withhold the change from a subset of users). It can provide a post hoc measure of eventual impact, and hence insight into what the product might try next. This is not the classic case of hypothesis testing via experimentation, and thus the measured effects are subject to considerations that come with the territory of &lt;a href=&quot;http://www.unofficialgoogledatascience.com/2017/10/unintentional-data.html&quot;&gt;unintentional data&lt;/a&gt;.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h3&gt;To roll out a change&lt;/h3&gt;We have a change we know we want to launch&amp;nbsp;&lt;span style=&quot;background-color: white; color: #4e4e4e; font-family: Roboto, sans-serif; font-size: 15px;&quot;&gt;—&amp;nbsp;&lt;/span&gt;we just want to make sure we didn&#39;t break anything. We might use randomization at the user level to spread the rollout. Unlike, say, picking a data center, randomization produces metrics that are immune to sampling bias and can thus detect regressions in fewer treated units. This is a good use of existing experiment infrastructure, but it is not really an experiment as in hypothesis testing. &lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;In summary, classic experimentation is applicable to fledgling products but in a much more limited way than to established products. With continual large product changes, a fledgling product&#39;s metrics may not be mature enough for decision making, let alone amenable to an OEC. &lt;a href=&quot;http://www.unofficialgoogledatascience.com/2015/10/experiment-design-and-modeling-for-long.html&quot;&gt;To focus on the long term&lt;/a&gt; is great advice&amp;nbsp;&lt;span style=&quot;background-color: white; color: #4e4e4e; font-family: Roboto, sans-serif; font-size: 15px;&quot;&gt;—&lt;/span&gt;&amp;nbsp;once immediate survival can be assumed.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;h2&gt;Your users aren&#39;t who you think they are!&lt;/h2&gt;A more fundamental problem with live experiments is that the users whose behavior they measure might not be who we imagine them to be. To illustrate the issues that arise from naively using experimentation in a fledgling product let us imagine a toy example: &lt;br /&gt;&lt;br /&gt;&lt;a href=&quot;https://2.bp.blogspot.com/-hjnw9wiiSbI/XLZdAJI0YsI/AAAAAAAAdo0/hrhpVc6d_Y4JK0fdNvf6yhHUOR6cTRIpgCEwYBhgL/s1600/guitar_amp.jpg&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1067&quot; data-original-width=&quot;1600&quot; height=&quot;426&quot; src=&quot;https://2.bp.blogspot.com/-hjnw9wiiSbI/XLZdAJI0YsI/AAAAAAAAdo0/hrhpVc6d_Y4JK0fdNvf6yhHUOR6cTRIpgCEwYBhgL/s640/guitar_amp.jpg&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;We have an MP3 music sales product that we have just launched in a &quot;beta&quot; state. Our specialty is to push users through a series of questions and then recommend, for purchase, tracks that we think they will like. We back up our belief by offering a full refund if they don&#39;t love the song. Each page-view of recommendations consists of an appealing display of tracks of which the user may click on one to purchase. The product is premised on making it a no-brainer to purchase a single song (&quot;for the price of chewing gum&quot;, according to our marketing message).&lt;br /&gt;&lt;br /&gt;We define an impression as a recommendation page-view and a sale as the purchase of a track. Of particular interest to us is the conversion rate, defined as fraction of impressions resulting in sales. To grow, we paid for a small amount of advertising and have a slow but steady stream of sales, say roughly 5,000 sales per day from about 100K impressions (5% conversion rate).&lt;br /&gt;&lt;br /&gt;The design team decides it wants to add BPM (beats per minute) to the song list page but isn&#39;t sure how to order it with the title (e.g. should it be [Artist Title BPM] or [BPM Artist Title]). So they set up an experiment to see which one our users prefer. This is expected to be a small change and does not change the sort order, just adds a little extra information.&lt;br /&gt;&lt;br /&gt;The experiment had 10,000 impressions in each arm with results as shown below with 95% confidence intervals. These are binomial confidence intervals computed naively under assumptions of impressions being independent (this is usually a poor assumption, but for now let us proceed with it):&lt;/div&gt;&lt;div&gt;&lt;b style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;div dir=&quot;ltr&quot; style=&quot;margin-left: 0pt;&quot;&gt;&lt;table style=&quot;border-collapse: collapse; border: none;&quot;&gt;&lt;colgroup&gt;&lt;col width=&quot;163&quot;&gt;&lt;/col&gt;&lt;col width=&quot;101&quot;&gt;&lt;/col&gt;&lt;col width=&quot;72&quot;&gt;&lt;/col&gt;&lt;col width=&quot;133&quot;&gt;&lt;/col&gt;&lt;col width=&quot;154&quot;&gt;&lt;/col&gt;&lt;/colgroup&gt;&lt;tbody&gt;&lt;tr style=&quot;height: 0pt;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;b&gt;Treatment&lt;/b&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;b&gt;Impressions&lt;/b&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;b&gt;Sales&lt;/b&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;b&gt;Conversion Rate&lt;/b&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;b&gt;Delta From Control&lt;/b&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0pt;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;[Artist Title] (control)&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;10000&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;400&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;4.00±0.38%&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &#39;Google Sans&#39;,sans-serif; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;-&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0pt;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;[Artist Title BPM]&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;10000&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;500&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;5.00±0.43%&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;+1.00±0.57%&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0pt;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;[BPM Artist Title]&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;10000&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;600&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;6.00±0.47%&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;+2.00±0.60%&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;&lt;br /&gt;Given just this information, it seems obvious to us that we should pick &quot;[BPM Artist Title]&quot; going forward and that we can expect an uplift of roughly 2% more of our impressions to turn into sales. Going from 4 to 6%, that seems like a big win.&lt;br /&gt;&lt;br /&gt;Unfortunately this analysis missed one subtle but very important caveat. Early in our product&#39;s life cycle we have a user population that strongly prefers EDM (electronic dance music) to the point that roughly 80% of the 5,000 songs we sell are EDM. Given this information it might seem obvious in retrospect that adding BPM to the song list would lead to more sales (BPM is an important selection parameter for EDM music).&lt;br /&gt;&lt;br /&gt;How could more sales be a problem? Putting BPM first in the song list came at the expense of putting artist first, and if we had broken out our user population by EDM listener and non-EDM listener we would have seen something very telling:&lt;br /&gt;&lt;br /&gt;EDM users (8,000 impressions):&lt;div dir=&quot;ltr&quot; style=&quot;margin-left: 0pt;&quot;&gt;&lt;table style=&quot;border-collapse: collapse; border: none;&quot;&gt;&lt;colgroup&gt;&lt;col width=&quot;158&quot;&gt;&lt;/col&gt;&lt;col width=&quot;124&quot;&gt;&lt;/col&gt;&lt;col width=&quot;62&quot;&gt;&lt;/col&gt;&lt;col width=&quot;131&quot;&gt;&lt;/col&gt;&lt;col width=&quot;148&quot;&gt;&lt;/col&gt;&lt;/colgroup&gt;&lt;tbody&gt;&lt;tr style=&quot;height: 0pt;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;b&gt;Treatment&lt;/b&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;b&gt;Impressions&lt;/b&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;b&gt;Sales&lt;/b&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline;&quot;&gt;Conve&lt;/span&gt;rsion Rate&lt;/b&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;b&gt;Delta From Control&lt;/b&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0pt;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;[Artist Title] (control)&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;8000&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;320&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;4.00±0.43%&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0pt;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;[Artist Title BPM]&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;8000&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;440&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;5.50±0.50%&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;+1.50±0.66%&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0pt;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;[BPM Artist Title]&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;8000&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;570&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;7.12±0.56%&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;+3.12±0.71%&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;Non-EDM users (2,000 impressions):&lt;div dir=&quot;ltr&quot; style=&quot;margin-left: 0pt;&quot;&gt;&lt;table style=&quot;border-collapse: collapse; border: none;&quot;&gt;&lt;colgroup&gt;&lt;col width=&quot;159&quot;&gt;&lt;/col&gt;&lt;col width=&quot;126&quot;&gt;&lt;/col&gt;&lt;col width=&quot;59&quot;&gt;&lt;/col&gt;&lt;col width=&quot;131&quot;&gt;&lt;/col&gt;&lt;col width=&quot;148&quot;&gt;&lt;/col&gt;&lt;/colgroup&gt;&lt;tbody&gt;&lt;tr style=&quot;height: 0pt;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;b&gt;Treatment&lt;/b&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;b&gt;Impressions&lt;/b&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;b&gt;Sales&lt;/b&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;b&gt;Conversion Rate&lt;/b&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;b&gt;Delta From Control&lt;/b&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0pt;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;[Artist Title] (control)&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;2000&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;80&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;4.00±0.86%&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0pt;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;[Artist Title BPM]&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;2000&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;60&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;3.00±0.75%&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;-1.00±1.14%&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0pt;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;[BPM Artist Title]&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;2000&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;30&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;1.50±0.53%&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;-2.50±1.01%&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;From this it is clear that we have sacrificed sales from non-EDM users for EDM users. This might be an acceptable trade-off if we have looked at the marketplace and decided to make a niche product for EDM users. But the charts indicate that EDM music makes up only 4% of total music sales (&lt;a href=&quot;https://www.statista.com/statistics/310746/share-music-album-sales-us-genre/&quot;&gt;source&lt;/a&gt;), which means our product might not appeal to 96% of the market. So by optimizing short-term metrics such as sales volume we might have actually hurt our long-term growth potential.&lt;br /&gt;&lt;br /&gt;The underlying principle at play is that &lt;b&gt;your current user base is different from your target user base&lt;/b&gt;. This fact will always be true, but the bias is dramatically worse for fledgling products as early growth tends to be in specific pockets of users (often due to viral effects) and not uniformly spread across the planet. Those specific pockets won&#39;t behave like the broader population along some dimension (here it is EDM vs non-EDM music preference).&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;And how to do it right (or at least better)&lt;/h2&gt;Continuing with our MP3 product, how can we undo this bias that our non-representative users are injecting?&lt;br /&gt;&lt;br /&gt;There are a few ways to de-bias the data to make the experimental results usable. The easiest approach is to identify the segments and reweight them based on the target population distribution.&lt;br /&gt;&lt;br /&gt;Since we don&#39;t particularly want to build a product optimized for EDM users, we can reweight back to the mean of the broader population. To do that we can separate the populations and then take a weighted mean of the effects to project the effects onto the target user population. &lt;br /&gt;Here the target population is 96% non-EDM, 4% EDM, so to reweight the conversion rate this amounts to: $$&lt;br /&gt;&amp;nbsp; 0.04 \times EDMrate + 0.96 \times nonEDMrate&lt;br /&gt;$$The confidence intervals must also be adjusted, as the standard errors add in quadrature: $$&lt;br /&gt;(0.04 \times EDMrateSE)^2 + (0.96 \times nonEDMrateSE)^2&lt;br /&gt;$$&lt;/div&gt;&lt;div&gt;&lt;br /&gt;Weighted average conversion rates:&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;margin-left: 0pt;&quot;&gt;&lt;table style=&quot;border-collapse: collapse; border: none;&quot;&gt;&lt;colgroup&gt;&lt;col width=&quot;159&quot;&gt;&lt;/col&gt;&lt;col width=&quot;151&quot;&gt;&lt;/col&gt;&lt;col width=&quot;198&quot;&gt;&lt;/col&gt;&lt;col width=&quot;116&quot;&gt;&lt;/col&gt;&lt;/colgroup&gt;&lt;tbody&gt;&lt;tr style=&quot;height: 0pt;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;b&gt;Treatment&lt;/b&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;b&gt;EDM Conversion Rate&lt;/b&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;b&gt;Non-EDM Conversion Rate&lt;/b&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;b&gt;Weighted Average&lt;/b&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0pt;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;[Artist Title] (control)&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;4.00±0.43%&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;4.00±0.86%&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;b&gt;4.00±0.83%&lt;/b&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0pt;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;[Artist Title BPM]&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;5.50±0.50%&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;3.00±0.75%&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;b&gt;3.10±0.72%&lt;/b&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0pt;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;[BPM Artist Title]&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;7.12±0.56%&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;1.50±0.53%&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;b&gt;1.72±0.51%&lt;/b&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;From this it becomes clear that we might not want to add BPM at all, but if we needed the change for some reason other than conversion rate, we should put it after the title.&lt;br /&gt;&lt;br /&gt;Also notice the change in confidence intervals in the weighted average versus the original conversion rates; in the original control group we had ±0.38%, now it is ±0.83%. This large increase is a result of the fact we don&#39;t have much data from the &quot;target&quot; user base and so we cannot speak very confidently about its behavior.&lt;br /&gt;&lt;br /&gt;This strategy only works if we have the ability to identify EDM users. If, for example, we were optimizing the first interaction with our product, we wouldn&#39;t know if a new user was an EDM lover or not since they would not have purchased anything yet.&lt;br /&gt;&lt;br /&gt;This early user classification problem goes hand in hand with product personalization. Luckily the user segmentation (e.g. &quot;EDM fans&quot;) that we aim to use for experiments can also be useful for personalizing our user interface. For our product this might mean simply asking the user when they sign up what their favorite song is. This can then be used for tailoring the product to the user, but also for weighting experimental analysis.&lt;br /&gt;&lt;br /&gt;This example with EDM users is clearly a cartoon. In reality, there will be more than two slices. This reweighting technique generalizes to the case when users fall into a small number of slices. But often there are multiple dimensions whose Cartesian product is large, leading to sparse observations within slices. In this case, we need a propensity-score model to provide the appropriate weight for each user.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Do you even want those users?&lt;/h2&gt;The idea that your current users aren&#39;t your target users can be taken a step further. For our music example, we imagined that EDM users don&#39;t approximate the target population for some experiments. But what if certain users didn&#39;t even represent the kind of a user we wanted (e.g. their lifetime value was negative)?&lt;br /&gt;&lt;br /&gt;One example of this for our music product could be die-hard fans of the American rock band Tool. Tool does not allow any digital sales of their albums, so users coming to our site looking for this band&#39;s music will leave with negative sentiment of our product. They may subsequently return any tracks they purchased, leading to an actual cost to our business. These users might further share their experiences with non-Tool fans on social media, causing more damage.&lt;br /&gt;&lt;br /&gt;Early in our product&#39;s lifecycle, this population of users will contribute to our active user population as they explore our product and maybe even purchase some albums. But without finding their core audio preferences they will likely churn.&lt;br /&gt;&lt;br /&gt;Gaining more of these users may increase our short-term metrics, but these users do not offer long term stable revenue and may negatively impact our ability to gain non-Tool users in the future.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;The tech-savvy users&#39; siren call&lt;/h2&gt;Hopefully it is now clear that using experiments without understanding how the existing user population differs from the target population is a dangerous exercise.&lt;br /&gt;&lt;br /&gt;On top of this idiosyncratic population bias due to uneven population growth rates, there is a more persistent early adopter bias. These early adopters tend to be much more tech-savvy than the general population, trying out new products to be on the cutting edge of technology. This tech-savvy population desires features that can be detrimental to the target population. In our music example, tech-savvy users will want to select the specific bit-rate and sampling frequency of the song they are buying, but forcing our target population through this flow would lead to confusion and decreased conversion rates.&lt;br /&gt;&lt;br /&gt;Where the average user would walk away, tech-savvy users may be more willing to see past issues to find value in your product. For example, if we add roadblocks in the purchase flow, the average user will abandon the purchase. In contrast, the tech-savvy user is capable of navigating the complicated process without dropping or even developing a negative sentiment. If we assume that these early users represent our target users we will miss the fact that our product is actually churning our target population.&lt;br /&gt;&lt;br /&gt;Unfortunately since these tech-savvy users often have a larger than average social media/megaphone presence, we need to be delicate with how we react to them. In evaluating product changes, we will rarely make trade-offs in their favor at the cost of most users. But we still want the product to work well enough for them so they don&#39;t have negative experiences. This might mean having the bit-rate setting buried in the fine print, available if absolutely needed, but not distracting to the target users.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Conversions are not independent&lt;/h2&gt;Further complicating matters, when products are small they are much more susceptible to error from individual &quot;power&quot; users. In our music product, most users will buy a single song, that one track that they heard on the radio. Indeed, that is the premise of our product, and how we built the user experience. But every once in a while there will be a user who decides to rebuy his or her entire CD collection on MP3. This wasn&#39;t what we intended and our UI doesn&#39;t make it easy, but there it is. The behavior of this single user user appears in our data as a large number of impressions with conversions.&lt;br /&gt;&lt;br /&gt;Imagine that early in our product&#39;s lifecycle we have one such user per week who buys 1,000 tracks, even though in a given week we only sell 2,000 tracks total. In other words, this single user represents half our sales. If we run a null A/B experiment where the users are randomly assigned to the A and B arms with the collection buyer in the A arm, we will have 1500 sales in A and 500 in B. This makes it look as though the A arm performs 3x better than the B arm, even though they are actually the same. As our product grows, it is less likely that a single user&#39;s behavior will affect aggregate metrics, but this example illustrates why we usually don&#39;t want to assume conversions are independent across impressions. The binomial confidence intervals we computed earlier may greatly underestimate the uncertainty in our inference. It is imperative that we use techniques such as resampling entire users to correct for this kind  of user effect. This applies to a product of any size, but is a greater concern when sample sizes are smaller.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;A word on growth hacking&lt;/h2&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Growth_hacking&quot;&gt;Growth hacking&lt;/a&gt; is an emergent field attempting to optimize product growth. It often comes up in discussions of a fledgling product&#39;s adoption rates. Unfortunately this space has mainly functioned as a way to optimize marketing spend and small product changes under the assumption that a product has already found &quot;product-market-fit&quot;. This mentality does not mesh well with our description earlier of a fledgling product. Modern software products do not come onto the market as fixed immutable &quot;things&quot; but instead iteratively (and sometimes drastically) evolve to find their niche.&lt;br /&gt;&lt;br /&gt;Of particular concern in growth hacking is the focus on influencers for pushing growth. Influencers very rarely represent your target user base and focussing your product features too much on them can lead you to have a product that influencers love but your target users don&#39;t find compelling (e.g. Twitter). This doesn&#39;t mean you shouldn&#39;t attempt to obtain them, but you should not design for them at the expense of your target user.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Conclusion&lt;/h2&gt;Creating something from nothing is the hardest thing humans do. It takes imagination, execution, and a dose of luck to build a successful product. While barriers to entry for a new product have come down, success is always elusive. This means there will be an increasing number of fledgling products out there trying to make it. In this post, we described several ways in which such products may not be able to leverage experiment methodology to the same extent as established products. Nor does growth hacking provide ready answers. But if there is one thing that I have learned from my experience working on fledgling products it is to be explicit and vigilant about the population for whom the product is built. Those with expertise in large-scale experimentation are typically mindful of evaluation metrics. My experience suggests that to a fledgling product being mindful of the target user population is just as important. Never stop asking, &quot;do the users I want, want this product?&quot;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt;</content><link rel='replies' type='application/atom+xml' href='https://www.unofficialgoogledatascience.com/feeds/1194836254357685717/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='https://www.unofficialgoogledatascience.com/2019/04/misadventures-in-experiments-for-growth.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/1194836254357685717'/><link rel='self' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/1194836254357685717'/><link rel='alternate' type='text/html' href='https://www.unofficialgoogledatascience.com/2019/04/misadventures-in-experiments-for-growth.html' title='Misadventures in experiments for growth'/><author><name>Amir Najmi</name><uri>http://www.blogger.com/profile/18174523203317227640</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://2.bp.blogspot.com/-hjnw9wiiSbI/XLZdAJI0YsI/AAAAAAAAdo0/hrhpVc6d_Y4JK0fdNvf6yhHUOR6cTRIpgCEwYBhgL/s72-c/guitar_amp.jpg" height="72" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-5825758052688213474.post-7545299884043426405</id><published>2018-07-17T09:42:00.000-07:00</published><updated>2018-07-17T11:02:56.185-07:00</updated><title type='text'>Crawling the internet: data science within a large engineering system</title><content type='html'>&lt;div&gt;by BILL RICHOUX&lt;/div&gt;&lt;div&gt;&lt;i&gt;&lt;br /&gt;&lt;/i&gt;&lt;/div&gt;&lt;i&gt;Critical decisions are being made continuously within large software systems. Often such decisions are the responsibility of a separate machine learning (ML) system. But there are instances when having a separate ML system is not ideal. In this blog post we describe one of these instances — Google search deciding when to check if web pages have changed. Through this example, we discuss some of the special considerations impacting a data scientist when designing solutions to improve decision-making deep within software infrastructure.&lt;/i&gt;&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Data scientists promote principled decision-making following several different arrangements.  In some cases, data scientists provide executive level guidance, reporting insights and trends.  Alternatively, guidance and insight may be delivered below the executive level to product managers and engineering leads, directing product feature development via metrics and A/B experiments.&lt;br /&gt;&lt;br /&gt;This post focuses on an even lower-level pattern, when data scientists are themselves implementing solutions to analytical problems within the software system codebase. Instead of closing the loop (and impacting the product) through strategic decision making, in this scenario the data scientists are directly rewiring the software stack.  This kind of action usually comes with a prelude, working though higher-level scenarios — first metrics are developed, next metrics are universally embraced, and lastly software changes are driven by the desire to improve those metrics.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;Every large-scale system has its own constraints and pressures that the data scientist will need to accommodate.  They are typically built as a software suite that has been abstracted into several interacting components, each owned by a distinct subteam of infrastructure engineers.  Most of these subteams interact with only a small subset of subteams upstream or downstream of their subsystem. Although the software components themselves may have limited cross-dependencies in the software layer, the subteams remain heavily dependent on one another to ensure the product as a whole is successful.&lt;br /&gt;&lt;br /&gt;Good data scientists typically seek to solve problems not just once, but to solve problems continuously.  Even in instances when data scientists supply metrics, insights, and trends to inform strategic decision-making, they still care to minimize future work needed to continue solving such problems. But when a data scientist is implementing solutions within software systems, it is arguably a higher form of solving problems continuously.  Such solutions are no longer necessarily just strategic and may have to address tactical issues. For instance, tactical responsiveness is critical whenever the software system interacts with a continuously changing external environment (e.g., user behaviors/interests, the internet, etc.). Moreover, there is a much higher bar on reliability, as well as a higher bar on immediately recognizing when solutions break, as your eyes no longer will be sanity-checking each solution that your algorithm emits.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h2&gt;Example: Recrawl Logic within Google search&lt;/h2&gt;Google search works because our software has previously crawled many billions of web pages, that is, scraped and snapshotted each one. These snapshots comprise what we refer to as our search index.  When queries arrive, the search system matches the inferred meaning of the query to web pages on the basis of these snapshots. The best-matching snapshots then tell the search system which web pages should appear on the search results page as an answer to the user&#39;s query.   In other words, Google search heavily relies on its own dated snapshot of the&amp;nbsp;internet — dated&amp;nbsp;to the extent that it is based on the contents that appeared on web pages when we last crawled them.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-JLB9zo8s_x0/W00zYKGsedI/AAAAAAAAdFk/67uBxOjB0NESUgz-_tVg5-ctDBlOa-ytQCLcBGAs/s1600/%255BUNSET%255D.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;323&quot; data-original-width=&quot;846&quot; height=&quot;243&quot; src=&quot;https://3.bp.blogspot.com/-JLB9zo8s_x0/W00zYKGsedI/AAAAAAAAdFk/67uBxOjB0NESUgz-_tVg5-ctDBlOa-ytQCLcBGAs/s640/%255BUNSET%255D.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;h3&gt;Freshness&lt;/h3&gt;Many web pages have their contents updated repeatedly, and it is imperative that Google search keep the snapshots as up-to-date as possible.  Whenever a snapshot’s contents match its real-world counterpart, we call that snapshot ‘fresh.’ Even if it has been a long while since that snapshot was taken, as long as the contents continue to match, the snapshot is fresh.  On the other hand, if a web page’s contents have meaningfully changed since Google search last crawled it —&amp;nbsp;regardless of how little time has passed since this last crawl — Google search’s snapshot of the web page is not acceptable, or ‘stale.’  When a user clicks through to a stale web page from a Google search results page, his or her browser loads a web page whose contents may no longer be relevant to the user’s query.  This results in a poor user experience.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;A team of data scientists defined a measure of freshness for each snapshot and built a system to estimate continuously the weighted average of the freshness metric for each snapshot in our web search index.  The weights were provided by a separate system that assigns a value to each web page, akin to &lt;a href=&quot;https://en.wikipedia.org/wiki/PageRank&quot;&gt;PageRank&lt;/a&gt;. These web pages&#39; values are used ubiquitously across Google search for prioritization. This measure of web page value is on a meaningful linear scale, such that our freshness metric (a weighted average) has an intuitive interpretation.  It was widely agreed that we should undertake effort to improve this freshness metric.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h3&gt;Crawl policy&lt;/h3&gt;&lt;div&gt;Improving this freshness metric would require changes to the logic that decides when Google search recrawls web pages. We call this logic the ‘crawl policy’, and the implementation of that policy is owned by the crawl team of infrastructure engineers. There are two sets of constraints that make crawl an interesting problem:&lt;span id=&quot;docs-internal-guid-631340e3-a4b1-4e1d-4d46-18c72a9c8e87&quot;&gt;&lt;/span&gt;&lt;br /&gt;&lt;span id=&quot;docs-internal-guid-631340e3-a4b1-4e1d-4d46-18c72a9c8e87&quot;&gt;&lt;/span&gt;&lt;br /&gt;&lt;ol&gt;&lt;span id=&quot;docs-internal-guid-631340e3-a4b1-4e1d-4d46-18c72a9c8e87&quot;&gt;&lt;li&gt;Each host (a collection of web pages sharing a common URL prefix) imposes an implicit or explicit limit on the rate of crawls Google’s web crawler can request.  Google continuously estimates this maximum crawl rate for each host (with such estimates being made available to any logic implementing how web pages should be recrawled).&lt;/li&gt;&lt;li&gt;A global constraint of how much compute and network resources Google itself is willing to dedicate to crawling web pages. Effectively, this imposes an overall limit on the rate of crawls across all web pages.&lt;/li&gt;&lt;/span&gt;&lt;/ol&gt;&lt;span id=&quot;docs-internal-guid-631340e3-a4b1-4e1d-4d46-18c72a9c8e87&quot;&gt;&lt;/span&gt;&lt;h3&gt;&lt;span id=&quot;docs-internal-guid-631340e3-a4b1-4e1d-4d46-18c72a9c8e87&quot;&gt;&lt;/span&gt;&lt;/h3&gt;&lt;span id=&quot;docs-internal-guid-631340e3-a4b1-4e1d-4d46-18c72a9c8e87&quot;&gt;&lt;/span&gt;&lt;br /&gt;&lt;h3&gt;Mathematical abstraction&lt;/h3&gt;An optimal crawl policy could be derived from this objective of maximizing average freshness, if we could abstract this problem into an optimization framework. We first made some simplifying assumptions:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;We could assume the problem was static, solve for an optimal crawl policy under such static assumptions, and then apply this policy for a brief period of time —&amp;nbsp;however long it would take to solve (again) and update the policy.&amp;nbsp;&lt;/li&gt;&lt;li&gt;Each web page will experience, in its future, meaningful changes. We model the arrival of these changes as a Poisson process.  The Poisson rate of such meaningful changes for Page $j$ on Host $i$ is denoted as $\delta_{ij}$, and assumed constant over the life of a solution.  An estimate of this change rate for each web page would be available to the recrawl logic.&lt;/li&gt;&lt;/ul&gt;The value of this Page $j$ on Host $i$ (denoted as $w_{ij}$), the maximum crawl rate on Host $i$ (denoted as $k_i$), and maximum global crawl rate (denoted as $k_0$) are all available to the recrawl logic.  Under our static assumption, all these terms are constant over the life of the solution.&lt;br /&gt;&lt;br /&gt;Putting these parameters together, we pose an optimization problem:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;At a period of time $\tau$ since the last time Page $j$ on Host $i$ is crawled, the probability that it will be fresh (not have meaningfully changed) will be $ e^{-\delta_{ij} \tau} $&lt;/li&gt;&lt;li&gt;If Page $j$ is recrawled every $\Delta_{ij}$ time units, its probability of being fresh at a time chosen uniformly over this period will be $ \frac{1}{\Delta_{ij}} \int_0^{\Delta_{ij}} e^{-\delta_{ij} \tau} d \tau $ or equivalently, $ \frac{1}{\delta_{ij} \Delta_{ij}} \left( 1 - e^{-\delta_{ij} \Delta_{ij}} \right) $.&lt;/li&gt;&lt;/ul&gt;Putting this all together, our objective of choosing recrawl periods to maximize our freshness metric maps to the following optimization problem $$&lt;br /&gt;\arg \max_{\Delta_{11}, \Delta_{12}, \ldots } \sum_{ij} \frac{w_{ij}}{\delta_{ij} \Delta_{ij}} \left( 1 - e^{-\delta_{ij} \Delta_{ij}} \right) \\&lt;br /&gt;\textrm{s.t.} \sum_j \frac{1}{\Delta_{ij}} \leq k_i \textrm{ for the }i \textrm{th host} \\&lt;br /&gt;\;\;\; \sum_{ ij } \frac{1}{\Delta_{ij}} \leq k_0 \\&lt;br /&gt;\;\;\; \Delta_{ij} &amp;gt; 0 \;\; \forall i,j&lt;br /&gt;$$&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;We can express this optimization problem in a more compact abstract form, clarifying its special structure, by&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;ul&gt;&lt;li&gt;denoting&amp;nbsp;$C_{ij}(\Delta_{ij})$  as the term in the objective function relating to Page $j$ on Host $i$&lt;/li&gt;&lt;li&gt;denoting $K_{ij}(\Delta_{ij})$ as the term in the constraints relating to Page $j$ on Host $i$&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;div&gt;Then the optimization problem can be expressed as $$&lt;br /&gt;\arg \max_{\Delta_{11}, \Delta_{12}, \ldots } \sum_{ij} C_{ij}(\Delta_{ij}) \\&lt;br /&gt;\textrm{s.t.} \sum_{ j } K_{ij}(\Delta_{ij}) \leq k_i \textrm{ for the }i \textrm{th host} \\&lt;br /&gt;\;\;\; \sum_{ ij } K_{ij}(\Delta_{ij}) \leq k_0&lt;br /&gt;$$Functions $C_{ij}$ and $K_{ij}$ are convex.  Consequently, our optimization problem is convex.  Determining optimal recrawl periods for each web page, optimal in the sense of maximizing our freshness metric, has been mapped to solving a convex optimization problem.  That makes it feasible to perform the optimization repeatedly and frequently, regularly updating each web page’s recrawl period as our estimates associated with each web page, host, and the web itself, all evolve over time.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;The Missing Link!&lt;/h3&gt;This optimization problem was the missing mathematical link between our singular intention (improve our freshness metric) and the multitude of decisions to be made in the software layer (how often to crawl each web page).  We had the mathematical formulation, and we had the technical expertise to solve it. As our optimization problem could not reasonably reside in memory on a normal computer — it has several free parameters for every page in Google’s large index of web pages — the challenges that remained would be exciting for both data scientists and infrastructure engineers.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;We envisioned two potential approaches to solving this optimization problem — employ a solver on a single machine; or employ a solver distributed across multiple machines, partitioned to guarantee that no host would be split across multiple machines.  Our preference was for the latter because 1) better scaling properties over time, and 2) more in line with Google’s usual approach to solving large computational problems — parallelize computation across machines. Moreover, intuitively, the structure of our optimization problem (diagonal Hessian matrices associated with the objective function and all constraints) lent itself to being well-conditioned under a distributed-across-machines approach.  To us, this appeared to be a near-ideal project for collaboration between data scientists and infrastructure engineers.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;Or was it?  &lt;b&gt;Hint&lt;/b&gt;: there is arguably a much simpler way to understand when web pages should be recrawled.  Do you see it? Initially, all of us were so fixated with designing a solver for our optimization problem, especially those of us with backgrounds in optimization, that none of us saw it.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;h2&gt;Pushback&lt;/h2&gt;When we discussed this proposal with the crawl team comprised of infrastructure engineers, none of its members were enthusiastic about the proposal.  Their concerns fell into four categories:&lt;/div&gt;&lt;div&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;black box solution&lt;/b&gt;: Although we data scientists felt that we could clearly explain the optimization problem and also explain the rough structure of the infrastructure to solve it, infrastructure engineers saw this proposed solution as introducing a new black box component to their crawl system. None of the infrastructure engineers on the crawl team were convinced that they could easily understand and diagnose when or why such new optimization infrastructure would fail. All of this led to significant concerns with being able to quickly debug problems — they promised us that problems would invariably occur, as was always the case when working at the scale of Google search with a continuously evolving external environment (the web), no matter how well designed the system.&lt;/li&gt;&lt;li&gt;&lt;b&gt;solving the wrong problem formulation&lt;/b&gt;: Our solution approach was to freeze time, solve a static optimization problem, and then to solve again repeatedly. However, in the software layer, the decision being made repeatedly was to determine the subset of web pages that should be crawled next, whenever some crawl bandwidth becomes available. This meant that we would need to take the solution from our solver, and merge it in with information detailing when each URL was last crawled, and only then could we determine what URLs should be crawled next. It’s arguably a minor extra step, but nevertheless requires a bridge between our problem solution and the software implementation. By not matching the problem formulation in the infrastructure, we imposed an extra layer of complexity to interpret our solution.&lt;/li&gt;&lt;li&gt;&lt;b&gt;substantial expansion in complexity&lt;/b&gt;: Although we were excited when we had a concrete proposal to solve for many billions of optimal crawl periods, our proposal was effectively&amp;nbsp;to replace an existing heuristic implemented in a few lines of code, with some new software infrastructure, a distributed solver for a very large convex optimization problem&amp;nbsp;—&amp;nbsp;likely tens of thousands of lines of new code. This would sound fun to many developers thinking only about the short-term. But for anyone with a long-term perspective, there were obvious concerns with the burden of maintaining such a large specialized system that exceeds the technical complexity that either a data scientist or infrastructure engineer could individually master. Moreover, the infrastructure engineers insisted that any new, not-battle-tested system of moderate complexity would likely fail in unintended ways. Without any indication that such proposed infrastructure would become a standard tool to solve additional problems, the crawl team was hesitant to commit to it.&lt;/li&gt;&lt;li&gt;&lt;b&gt;limitations on responsiveness&lt;/b&gt;: The structure of the proposed solution implicitly imposed a batch approach to solving the optimization problem in practice. That is, the approach was to freeze time and solve a static problem, and then repeat ad infinitum. Although we referred to the solution as optimal, in reality it would never be optimal considering that the parameters feeding into such recrawl logic (the maximum crawl rates, the value of a page, the estimated change rate of a page) were all being updated asynchronously. No matter how responsive one wanted our recrawl logic to be, the runtime of each iteration of our solver would impose a non-negotiable lower bound on such responsiveness, eroding any claim of optimality of our solution. Moreover, as our index would invariably grow over time, such runtimes (and responsiveness) potentially could worsen.&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;h2&gt;Deconstructing our mistakes&lt;/h2&gt;At a high level, our mistakes boiled down to either 1) not knowing and appreciating our context — both the people, the infrastructure engineers on the crawl team, as well as the existing software implementation, or 2) an excessive eagerness to build a new solution that aligned with our technical interests (building a large-scale distributed convex optimization solver).  In both instances, these mistakes led us to willingly embrace an expansion of complexity, rather than simpler abstractions.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h3&gt;Failing to appreciate the infrastructure engineer’s pressures and responsibilities&lt;/h3&gt;Simpler software systems make software team responsibilities easier, whether it be onboarding new engineers, maintenance, or debugging.  Especially in the case of being far upstream in a complex software system (much of Search is dependent on the crawl working intelligently), debuggability becomes critical.  When you have a large software system interacting with an external environment, it is accepted that the software team will continually encounter cases where the existing code behaves in undesirable ways.  Because the continuously running system does not just take a break when this occurs, responsiveness to fixing such issues affects the long-term performance of such a software system. Data scientists must appreciate this, and particularly appreciate the value of simpler solutions when integrating solutions into the software layer. &lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Knowing too little of the actual, existing software implementation&lt;/h3&gt;The actual problem within the software was to determine the next few web pages to crawl whenever capacity becomes available.  An extra step was required to map the problem that our optimization solved (determining web page recrawl rates) to this actual problem.  If we were better aware of the actual problem initially, we would have spent more time thinking about a solution that could rank/sort web pages, as such a formulation would align with the actual decision-making within the software layer.&lt;br /&gt;&lt;br /&gt;Furthermore, it would have benefited us to appreciate the importance of a solution being responsive to dynamic conditions.  Our batch solution assumes that we can treat short durations of time as relatively uniform in a steady-state. Obviously, this assumption is inappropriate if, over the lifetime of a solution, there are significant changes to the parameters assumed frozen in our optimization formulation (e.g., the crawl rate capacity of a host).&lt;br /&gt;&lt;br /&gt;Less obvious, this static approach also assumes sufficient ‘mixing’ to ensure that the resulting crawl rate on a host would effectively be uniform over time when an optimal solution is followed.  Because of the continuous evolution of the internet, the sudden emergence of new blocks of web pages, and the thick tail of the web consisting of a large number of moderately-sized hosts without large numbers to ensure sufficient ‘mixing’, this second assumption is especially inappropriate (this will be discussed further in the next section).  Lastly, we also had failed to be aware of an ongoing transition of the existing batch-based crawl software infrastructure effectively to a streaming system — hence adding a new batch-based component would be unwelcome. Knowing all of this would have compelled us to avoid proposing our batch solution based on static assumptions.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h3&gt;Seduction of familiarity&lt;/h3&gt;When you’re holding a hammer, every problem looks like a nail.  It is easy to get drawn into a complex solution when you’ve spent years training yourself to design complex solutions.  When trying to solve problems in software systems, however, arguably more time ought to be spent thinking of new ways to abstract the problem.  It is best to hold off on building a solution, if only a small amount of time was spent to understand the problem. &lt;br /&gt;&lt;br /&gt;In summary, we solved the wrong problem —&amp;nbsp;wrong both at the human client level and at the level of mathematical abstraction. We proposed a solution to a stand-alone static problem, whereas what was needed was a method to make good use of all available capacity of the crawl system. In other words, what the crawl system needed was &lt;i&gt;a forward-looking means of deciding which next few web pages to crawl&lt;/i&gt;.&lt;/div&gt;&lt;/div&gt;&lt;br /&gt;&lt;h2&gt;A revised proposal&lt;/h2&gt;Learning specifics about the underlying software implementation left us wishing that we could define, for each web page, a function that would tell us the value of recrawling the page at any given time since it was last crawled.  If we could sort our queues by such a function, it would be relatively easy to determine, at any time, the web pages to crawl next whenever capacity was available. But then suddenly we realized that we had precisely that, made possible by special mathematical structure in the optimization problem itself.&lt;br /&gt;&lt;br /&gt;&lt;div&gt;As before, let $i$ index hosts and $j$ index pages on each host. Thus each page is uniquely indexed by $(i,j)$. Furthermore, define crawl rate for Page $i,j$ as the reciprocal of the duration $$&lt;br /&gt;\rho_{ij}=\frac{1}{\Delta_{ij}}&lt;br /&gt;$$This is the same as $K_{ij}(\Delta_{ij})$ in the original framing but we want to think of it as a variable over which we are optimizing rather than as a function. We may now write down the contribution of the Page $i,j$ to the overall objective as $$&lt;br /&gt;C_{ij}(\rho_{ij})=\frac{w_{ij}}{\delta_{ij}}\rho_{ij}\left(1-\exp(-\frac{\delta_{ij}}{\rho_{ij}})\right)&lt;br /&gt;$$If we define $\underline{\rho}$ as the vector of all crawl rates to be chosen, the overall freshness objective to be maximized is $$&lt;br /&gt;\arg \max_{\underline{\rho}}\sum_{ij}C_{ij}(\rho_{ij})&lt;br /&gt;$$under the constraint for each host indexed by $i$ as $$&lt;br /&gt;\sum_{j}\rho_{ij}\leq k_{i}&lt;br /&gt;$$and the global crawl constraint $$&lt;br /&gt;\sum_{ij}\rho_{ij}\leq k_{0}&lt;br /&gt;$$We use Lagrange multipliers to optimize the unconstrained objective&lt;br /&gt;&lt;div&gt;\begin{align}&lt;br /&gt;J(\underline{\rho}) &amp;amp; =\left(\sum_{ij}C_{ij}(\rho_{ij})\right)+\left(\sum_{i}\lambda_{i}\sum_{j}\rho_{ij}\right)+\left(\lambda_{0}\sum_{ij}\rho_{ij}\right)\\&lt;br /&gt;&amp;amp; =\sum_{ij}C_{ij}(\rho_{ij})+(\lambda_{i}+\lambda_{0})\rho_{ij}&lt;br /&gt;\end{align}From the KKT conditions, the Lagrange multipliers $\lambda_{i}$ must all be non-negative. Setting $\partial J/\partial\rho_{ij}=0$ we get $$&lt;br /&gt;C&#39;_{ij}(\rho_{ij}^*)=\lambda_{i}+\lambda_{0}&lt;br /&gt;$$at the optimal crawl rates $\rho_{ij}^*$ where&lt;br /&gt;\begin{align}&lt;br /&gt;C&#39;(\rho) &amp;amp; =\frac{w}{\delta} \left(1-\exp(-\frac{\delta}{\rho})\right)-\frac{w}{\rho}\exp(-\frac{\delta}{\rho})&lt;br /&gt;\end{align}all variables indexed by $i,j$. Furthermore, $C&#39;(\rho)$ is monotone decreasing in $\rho$ because $C&#39;&#39;(\rho)$ is always negative$$&lt;br /&gt;C&#39;&#39;(\rho)=-\frac{w\delta\exp(-\frac{\delta}{\rho})}{\rho^{3}}&lt;br /&gt;$$For hosts whose crawl rate does not constrain the solution, $\lambda_{i}=0$.&lt;br /&gt;&lt;br /&gt;Up to this point, we have considered how to solve the static optimization problem. But the form of the solution offers us a new perspective on how to understand and implement an optimal solution, primarily in terms of functions that tell us the value of crawling any web page at any given time. To see this, first consider the function&lt;/div&gt;\begin{align}&lt;br /&gt;V(\Delta) &amp;amp;= C&#39;(1 / \Delta) \\&lt;br /&gt;&amp;amp;= \frac{w}{\delta} \left(1-\mathrm{e}^{-\delta \Delta}\right)&lt;br /&gt;- w\Delta \mathrm{e}^{-\delta\Delta}&lt;br /&gt;\end{align}The function $V(\Delta)$ is monotonically increasing, starting with $V(0) = 0$. At the optimal crawl period $\Delta_{ij}^*=1/\rho^*_{ij}$, it follows that for each Page $j$ on Host $i$ $$&lt;br /&gt;V_{ij}(\Delta_{ij}^*) =\lambda_{i}+\lambda_{0}&lt;br /&gt;$$This perspective motivates explicitly expressing this function temporally, that is, envision $\tau$ as the time that has elapsed since Page $j$ on Host $i$ was last crawled; we can then consider the&amp;nbsp;function$$&lt;br /&gt;V_{ij}(\tau) = \frac{w_{ij}}{\delta_{ij}} \left( 1 - e^{-\delta_{ij} \tau} \right ) - w_{ij} \tau e^{-\delta_{ij} \tau}&lt;br /&gt;$$which is a function of time since the web page was last crawled. We refer to $V_{ij}(\tau)$ as Page $i,j$’s &lt;b&gt;crawl value function&lt;/b&gt;.  When we temporally observe a solution to the static optimization problem being executed, we can track each Page $i,j$’s crawl value function and observe it climbing until it reaches a threshold of $\lambda_i + \lambda_0$, whereupon Page $i,j$ will be recrawled —&amp;nbsp; as the crawl value function equals  $\lambda_i + \lambda_0$ at the web page’s optimal&amp;nbsp;crawl period $\Delta^*_{ij}$. Upon recrawl, its crawl value function will reset to 0.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;As for the parameters, it turns out that it is not necessary to estimate $\lambda_i$ in order to enforce the crawl rate limits on individual hosts —&amp;nbsp;each host itself limits how fast it allows itself to be crawled. The implementation assigns a number of parallel workers, each with its own range of pages to crawl. Thus, $\lambda_i$ are maintained by the worker assigned to each host, estimated based on the web pages on that host crawled recently. In contrast, $\lambda_0$ is estimated centrally and updated periodically. Its value is then propagated to all the parallel workers. While in theory we need $\lambda_i$ in order to estimate $\lambda_0$, in practice $\lambda_0$ is relatively constant over time. Thus, its value can be updated based only on hosts which are not constrained.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;Note what this crawl value function is not —&amp;nbsp;it is not greedy. The crawl value function we derived for the static optimization problem has some properties that were seen as favorable —&amp;nbsp;at least to some people —&amp;nbsp;when compared to what results from a greedy algorithm. In some regimes (and in practice for google search), a greedy algorithm would devote more recrawl resources towards high value pages, as lower value pages would commonly starve. Many in Google search had experienced past situations where greedy algorithms accumulated serious issues over time, and hence there was a lot of institutional intuition to avoid them.&lt;br /&gt;&lt;br /&gt;As for the use of the crawl value function — at any given time, we can evaluate the crawl value function for each web page on a host. We can use this function to sort the web pages, and then determine which web pages should be scheduled for immediate crawl.  This is the substantial simplification alluded to earlier —&amp;nbsp;that for each web page, there is a function (this crawl value function) telling us the value of recrawling that page, only in terms of parameters for the given web page, at any given time since it was last crawled.  It’s precisely what one wants when one has a queue, as in the software implementation in Google search, and it dramatically simplifies how a solution to the static optimization problem can be implemented on a per-host basis.&lt;/div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;br /&gt;As this reasoning is derived directly from the KKT conditions of a solution to the static optimization problem, the reader should be convinced that a crawl policy executed by sorting web pages by their crawl value functions and recrawling them when they reach thresholds $\lambda_i+\lambda_0$ will be equivalent to a crawl policy implemented by solving the optimization problem for the optimal recrawl periods and then scheduling URLs to be recrawled at intervals according to their solved-for optimal recrawl periods.  What should be obvious is that one can apply the policy of sorting by crawl value to prioritize what to recrawl, even in situations where the available crawl rate on a host varies over time (recall that it was assumed to be fixed in the static optimization problem’s formulation). What may be less obvious, however, is that this recrawl policy can be an optimal crawl policy (optimal in the sense that it will maximize weighted freshness over time) even under some more general conditions.&lt;br /&gt;&lt;br /&gt;There are some very desirable properties of such a solution, properties which we hadn’t really focused on until learning more about the existing software implementation and the longer-term goals of the crawl team (which led us to revisit our original proposal):&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;familiar&lt;/b&gt;: the optimization problem is mapped to a solution that matches a common routine in software engineering: scan a list and find the top $N$. Minimal effort was required to convince infrastructure engineers to be open to the proposal.&lt;/li&gt;&lt;li&gt;&lt;b&gt;adaptive to heterogeneity&lt;/b&gt;: this alternative solution is not burdened by the easily-overlooked assumption in the static optimization problem that there is sufficient ‘mixing’ to ensure that the resulting crawl rate on a host will be effectively uniform over time. A consequence of such an assumption is that there’s real potential to thrash a host, i.e., exhibit a lot of volatility in the crawl demand imposed on the host. On the other hand, our alternative approach is more adaptive, and from the perspective of the host, less volatile. This difference in behavior between the two approaches is evident under a common scenario —&amp;nbsp;a new host comes online, and Google search quickly crawls all the new web pages (over a day or two). As these new web pages initially are not well-differentiated, a solution emitted by a solver to the optimization problem would suggest that all web pages be recrawled roughly $x$ days later —&amp;nbsp;meaning no recrawls are initially requested for some time, and then all the web pages on the host would be scheduled to be recrawled over a short duration. If the available crawl rate on the host is restricted when it is time to recrawl, such behavior will be suboptimal. With our new solution, if recrawls would otherwise be bunched, the threshold at which web pages will be recrawled will fall downwards until reaching $\lambda_0$. Consequently, some pages will be crawled sooner than suggested by the solution to the static optimization problem, with an active spreading of the recrawls as the crawl capacity is fully utilized much earlier.&lt;/li&gt;&lt;li&gt;&lt;b&gt;debuggable&lt;/b&gt;: one may need to debug why a web page was recrawled (or hasn’t been recrawled). Debugging of an individual web page is decoupled into two parts: 1) how the web page itself influences when it will be recrawled (its crawl value function is a function of only its own parameters), and 2) how the other web pages on a host will influence when it will be recrawled, as they collectively determine $\lambda_i$. This provides more intuition and clarity when compared to an optimization framework that merely emits optimal recrawl periods — one can understand how changes in a web page’s underlying parameters, its value (weight) and estimated change rate will tend to drive changes in its recrawl rate.&lt;/li&gt;&lt;li&gt;&lt;b&gt;continuous&lt;/b&gt;: the solution in practice much more closely mirrors a streaming solution. The only meaningful batch component is repeated scanning of lists to re-sort. This can be controlled by splitting data and parallelizing such scans, or by choosing how deep to scan — familiar workarounds to Google infrastructure engineers.&lt;/li&gt;&lt;li&gt;&lt;b&gt;responsiveness to external changes&lt;/b&gt;: the solution can be responsive to changes on a host.&amp;nbsp; As mentioned earlier, we have found that in practice the global crawl KKT multiplier $\lambda_0$ is rather stable (conceptually, it is determined by the aggregated behavior of many independent hosts). This is less the case for many host-specific KKT multipliers (in some cases this is due to recrawl demand being non-uniform, the concern highlighted above).&lt;/li&gt;&lt;li&gt;&lt;b&gt;easily generalized&lt;/b&gt;: generalizing the approach to a probabilistic setting (assuming the estimated parameters are distributions rather than point estimates) isn’t a significant increase in complexity.&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;div&gt;We’re going to sweep under the rug some additional issues that needed to be resolved, e.g., the crawl value functions are bounded, meaning the optimal solution would elect never to recrawl some pages.  However, we hopefully made our point that often much simpler solutions exist, which can also much better align with existing software system design.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Conclusions&lt;/h2&gt;In this post we described our experience of a very specific problem, but our message to data scientists is broader. Finding the right abstraction is always important in data science, but it is absolutely crucial when designing algorithms to control the behavior of complex software systems. Black-box solutions to well-understood mathematical problems often don&#39;t work well in this capacity because of the assumptions they make. This mismatch in assumptions can lead to additional system complexity around the black box, especially in addressing issues of robustness and responsiveness to other parts of the system. Worst of all, the solution may expose a poor conceptual model to the infrastructure team whose job it is to keep the system running. The people on pager duty might not appreciate the elegance of your solution when they have no easy means of debugging when things go wrong.&lt;br /&gt;&lt;br /&gt;In projects where decision-making is to be deployed to control a complex software system, we run a greater risk of incurring a &lt;a href=&quot;https://en.wikipedia.org/wiki/Type_III_error#Kimball&quot;&gt;Type III error&lt;/a&gt;&amp;nbsp;—&amp;nbsp;giving the right answer to the wrong problem. But with additional diligence and humility, it is possible to avoid such a scenario.&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content><link rel='replies' type='application/atom+xml' href='https://www.unofficialgoogledatascience.com/feeds/7545299884043426405/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='https://www.unofficialgoogledatascience.com/2018/07/by-bill-richoux-critical-decisions-are.html#comment-form' title='1 Comments'/><link rel='edit' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/7545299884043426405'/><link rel='self' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/7545299884043426405'/><link rel='alternate' type='text/html' href='https://www.unofficialgoogledatascience.com/2018/07/by-bill-richoux-critical-decisions-are.html' title='Crawling the internet: data science within a large engineering system'/><author><name>Amir Najmi</name><uri>http://www.blogger.com/profile/18174523203317227640</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://3.bp.blogspot.com/-JLB9zo8s_x0/W00zYKGsedI/AAAAAAAAdFk/67uBxOjB0NESUgz-_tVg5-ctDBlOa-ytQCLcBGAs/s72-c/%255BUNSET%255D.png" height="72" width="72"/><thr:total>1</thr:total></entry><entry><id>tag:blogger.com,1999:blog-5825758052688213474.post-6625835772947801316</id><published>2018-03-22T09:32:00.000-07:00</published><updated>2018-03-22T10:32:34.843-07:00</updated><title type='text'>Compliance bias in mobile experiments</title><content type='html'>by DANIEL PERCIVAL&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;i&gt;Randomized experiments are invaluable in making product decisions, including on mobile apps. But what if users don&#39;t immediately uptake the new experimental version? What if their uptake rate is not uniform? We&#39;d like to be able to make decisions without having to wait for the long tail of users to experience the treatment to which they have been assigned. This blog post provides details for how we can make inferences without waiting for complete uptake.&lt;/i&gt;&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;br /&gt;&lt;div&gt;&lt;h2&gt;Background&lt;/h2&gt;At Google, experimentation is an invaluable tool for making decisions and inference about new products and features. An experimenter, once their candidate product change is ready for testing, often needs only to write a few lines of configuration code to begin an experiment. Ready-made systems then perform standardized analyses on their work, giving a common and repeatable method of decision making. This process operates well under ideal conditions; in those applications where this process makes optimistic or unrealistic assumptions, data scientists must creep from the shadows and provide new approaches. In the &lt;a href=&quot;https://play.google.com/store/apps?hl=en&quot;&gt;Google Play app store&lt;/a&gt;, challenges such as these regularly occur, as the basic framework of mobile technology introduces many wrinkles in experimentation and measurement. The Play Data Science team works to develop appropriate approaches to these cases and develops reusable methodologies to broaden the capabilities of Google Play experimenters at large.&lt;br /&gt;&lt;br /&gt;When we set a treatment and a control group, we typically assume that, instantaneously, each unit within the treatment experiences the treatment condition, and each unit within the control instead gets some manner of baseline condition. This assumption of &lt;i&gt;total compliance&lt;/i&gt;&amp;nbsp;allows us to make strong inferences about the impact of the treatment condition. However, there are common cases where this assumption could be broken, most notably in mobile. Naturally, this issue is of particular concern to us in the Play Data Science team.&lt;br /&gt;&lt;br /&gt;Suppose a developer of a game wants to release a new major version of the game. They have a lot of users already playing their game, and the developer is eager to see what effect the update will have on them. They have a clever thought to run an experiment, offer some users the new version with a popup notification within the game, and don’t tell the rest about it. Then, by comparing the two groups, they can see exactly what benefits (and problems!) they get with the new version. Once they proceed with this, immediately they run into problems. First, as soon as they offer the version, not everybody agrees to upgrade. Some users immediately rush to upgrade, others are slower because they rarely play the game, or have some technical trouble installing the update until days or weeks after the offer is sent. Some users even refuse to update, being happy with the current version and adverse to change. Even worse, some users in the control group manage to get ahold of the new version. Clearly, drawing conclusions from this messy experiment will require more sophisticated analysis.&lt;br /&gt;&lt;br /&gt;Abstracting this problem a bit, the total compliance assumption breaks down here --- some units assigned the treatment do not receive it immediately. Instead, a non-random subset of units receives the treatment, where the membership of a unit within this set is a function of covariates and time. In our example, users who rarely interact with the game will likely adopt the treatment more slowly, causing them to be underrepresented in the set of treated users in comparison to the population of users. Further, units in the control group may manage to receive the treatment, despite the assignment setup. In our example, this corresponds to &lt;i&gt;sideloading&lt;/i&gt;, when a user obtains the new version of the game despite it not being offered.&lt;br /&gt;&lt;br /&gt;Given this situation, a natural response is this: why not just wait until all the users in the treatment have upgraded? Even if we ignore the sideloaders (users in the control who take the treatment), we can’t always take this path. Though sometimes we can wait until most users have completed the update to draw conclusions, we usually want to make inference quickly. For example, we might want to stop the process if we measure harmful effects early.  We also might want to use early data to make a decision whether or not to release the treatment to more users, giving us more measurement power later on. In these cases, we must address the bias on the early data, when only a fraction of users who can eventually update their game have done so, and those that have are a non-random subset of the population. For example, perhaps users who have been historically more engaged with the game will update first.&lt;br /&gt;&lt;br /&gt;To summarize the rest of this post, we first characterize the main issues with these sorts of experiments. We then build our methods from a simple baseline method: intent-to-treat analysis. Finally, we expand our methods to include notions of which users are treated, and using the control to match users with similar upgrade probabilities.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Characterizing the main issues&lt;/h2&gt;The analysis here focuses on an experiment where a set of users is randomly assigned to one of two experiment arms:treatment or control. We observe for each user a set of units with covariates or user features $X_i$ and an associated response of interest $Y_i$. In our introductory example, this response might be the number of times a user opens the game each day. User features might be the country, device quality, and a bucketed measure of how often the user has played the game in a predefined period prior to the experiment. We are interested then in measuring the effect of the treatment on our response $Y_i$. We make the simplifying assumption that the impact of the treatment is instantaneous and does not further evolve over time. From here, the situation is complicated in a few ways, which we now explore in detail.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Issue 1: the empirical mismatch between treatment assignment and experience&lt;/h3&gt;Following our running example, after the game developer begins their experimental release they immediately observe that not all users assigned the treatment (the new game version) actually experience the treatment. Further, some users in the control group manage to experience the treatment.  That is, users may have an actual experience that differs from their experiment arm assignment. To index these situations, we introduce two binary variables as follows:&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;$Q$ indexes the user’s experiment arm assignment&lt;/li&gt;&lt;ul&gt;&lt;li&gt;$Q=0$: the treatment is not assigned to the user; that is, they are the control group&lt;/li&gt;&lt;li&gt;$Q=1$: the treatment is assigned to the user&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;$U$ indexes the treatment experience:&lt;/li&gt;&lt;ul&gt;&lt;li&gt;$U=0$: user does not experience the treatment condition&lt;/li&gt;&lt;li&gt;$U=1$: user experiences the treatment&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;br /&gt;We can track the users in the treatment group $(Q=1)$ to see how many are actually experiencing the treatment ($U=1$) at any given time. We can then produce graphs of this upgrade percentage over time similar to the following:&lt;br /&gt;&lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-6Bq_8ZA0jrQ/WrM_7ptvmdI/AAAAAAAAc1g/FNF2RORH7LsfssHTQX4EnIZLQcs5ElRWgCEwYBhgL/s1600/F1__ramp_up.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;985&quot; data-original-width=&quot;1600&quot; height=&quot;392&quot; src=&quot;https://1.bp.blogspot.com/-6Bq_8ZA0jrQ/WrM_7ptvmdI/AAAAAAAAc1g/FNF2RORH7LsfssHTQX4EnIZLQcs5ElRWgCEwYBhgL/s640/F1__ramp_up.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Fig 1: Only a fraction of users in the treatment group adhere to the treatment. Some take a significant time to adhere.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;&lt;h3&gt;Issue 2: the users experiencing treatment are not a simple random subsample of the population&lt;/h3&gt;A natural question that follows from the graph of user treatment adoption: what users are actually receiving the treatment here? Typical factors that could influence update speed might be system properties like connectivity (better means faster upgrade), hardware quality (higher means faster), or core operating system software. More user-based covariates could be factors like country or current frequency of use of the product. From the point of view of making valid statistical inference, we can assess if the mix of units who have actually received the treatment reflect the overall population. Loosely put, the degree to which the treated units represent a simple random sample of the population has implications for our ability to draw generalizable conclusions from our experiment. One way to assess this is compare the set of treated units within the treatment group ($Q=1$; $U=1$) to all units assigned to the treatment ($Q=1$).  Since units satisfying $Q=1$ are a random subset of the overall population, this comparison will give us hints as to how effectively we can generalize our conclusions. &lt;br /&gt;&lt;br /&gt;To take our example, suppose we are able to measure how engaged each user has been with the game over the past month. We then can bucket these data into six groups, giving a spectrum of engagement. We can then compare the distribution over these buckets for users with the new version to the entire treatment group:&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://2.bp.blogspot.com/-nV-i__zg3ag/WrM_7httUzI/AAAAAAAAc2A/6Y5f4AHzd08gzlotk54zKpHbmkkbrEa1wCEwYBhgL/s1600/F2__user_properties.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;985&quot; data-original-width=&quot;1600&quot; height=&quot;393&quot; src=&quot;https://2.bp.blogspot.com/-nV-i__zg3ag/WrM_7httUzI/AAAAAAAAc2A/6Y5f4AHzd08gzlotk54zKpHbmkkbrEa1wCEwYBhgL/s640/F2__user_properties.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Fig 2: Treatment units experiencing treatment are rather different from the population as a whole. This is shown here for one particular dimension, usage.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;It is clear from this plot that the two distributions do not match. Further, we see some expected discrepancies: users who have been more highly engaged in the game are more likely to actually experience the treatment. Perhaps they are more enthusiastic to upgrade, or perhaps this covariate is correlated with other more impactful covariates such as the quality of the user’s hardware. In any case, it is clear that we cannot make inference without some caution in this case.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Issue 3: the need to make a timely decision&lt;/h3&gt;Following our running example, after the game developer begins their experimental release they want draw conclusions about its impact as soon as possible. As mentioned previously, we could wait to do the analysis until all the users who may eventually upgrade actually, but this is typically impractical. However the strategy of waiting until all (who will comply) have received the treatment is useful as a ground truth for evaluating our methods in the following. For each method, we can compare the conclusions we can draw at the beginning of the process to those we would get at the end. We can then choose the methods where these two conclusions match, or at least where the first is a more useful nuanced view.&lt;br /&gt;&lt;br /&gt;We adopt the following notation and assumptions to make the time component of the problem clear:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;We build our models at two time points: $t=T_{\mathrm{measure}}$, and $t=T_{\mathrm{final}}$.&amp;nbsp;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;$T_{\mathrm{measure}}$ represents a time point at an early stage, where we would do our experiment analysis in a ‘real’ situation.&lt;/li&gt;&lt;li&gt;$T_{\mathrm{final}}$&amp;nbsp;represents a later time point, where virtually all of the users who eventually may upgrade have done so, and is used to benchmark the performance of the models and estimates produced at $T_{\mathrm{measure}}$&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;We assume that the effect of the treatment does not evolve over time. This allows us to compare the two results directly. This is often an unrealistic assumption in applications. For example, users of a game will probably behave differently across days of the week, and their behavior will evolve as they learn the game through experience. We leave it aside in this post so we can clearly explain and explore the remaining issues.&lt;/li&gt;&lt;/ul&gt;We can visualize some typical values of $T_{\mathrm{measure}}$&amp;nbsp;and $T_{\mathrm{final}}$&amp;nbsp;by annotating the adoption figure given above as follows: &lt;br /&gt;&lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://4.bp.blogspot.com/-BMbGNyougzU/WrM_7tV9_NI/AAAAAAAAc1o/rkwWuEApaZAcTjXPut5UXukd_RdkggebQCEwYBhgL/s1600/F3__ramp_up_time_annotation.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;985&quot; data-original-width=&quot;1600&quot; height=&quot;392&quot; src=&quot;https://4.bp.blogspot.com/-BMbGNyougzU/WrM_7tV9_NI/AAAAAAAAc1o/rkwWuEApaZAcTjXPut5UXukd_RdkggebQCEwYBhgL/s640/F3__ramp_up_time_annotation.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Fig 3: The above figure illustrates a typical case: we can afford to wait a short amount of time for a reasonable percentage of users to adopt before doing analysis ($T_{\mathrm{measure}}$). In order to get more comprehensive adoption ($T_{\mathrm{final}}$), we would have to wait a significantly longer time.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt;&lt;h2&gt;Intent to Treat (ITT) and Treatment on the Treated (TOT) analysis&lt;/h2&gt;With the main issues characterized, we now turn to analysis methods for the application. Before we begin, we should state our overall goal estimand, which is the expected effect of the treatment on the&amp;nbsp;average single unit: $$&lt;br /&gt;\theta = E(Y|U=1) - E(Y|U=0)&lt;br /&gt;$$ In the language of counterfactuals, this gives the difference between the outcomes under the two different treatment conditions. In a simple experiment, we assume that the random assignment to experiment arms is enough to give us a reasonable estimate of this quantity from standard methods, which rely on each group being a simple random sample from the population over which we wish to make inference.&lt;br /&gt;&lt;div&gt;&lt;br /&gt;A simple baseline to analyze this kind of experiment is an Intent to Treat (ITT) approach. The intent to treat estimand measures the effect of assigning the treatment to a user:$$&lt;br /&gt;\theta_{\mathrm{ITT}} =  E(Y | Q=1) - E(Y|Q=0)&lt;br /&gt;$$&amp;nbsp;That is, we simply compare the two experiment groups on the basis of their treatment status assignment, rather than their actual experience of the treatment. To estimate this quantity, we&amp;nbsp;could compare the mean value of a metric Y between treatment and control in the standard way:$$&lt;br /&gt;\hat{\theta}_{\mathrm{ITT}} = &lt;br /&gt;\frac{1}{N_{Q=1}} \sum_i Y_i[Q_i = 1] - \frac{1}{N_{Q=0}} \sum_j Y_j[Q_j = 0]&lt;br /&gt;$$&amp;nbsp;where $Y_i$ refers to the measured outcome for a single unit, $N_{Q=1}$ and $N_{Q=0}$ are the number of units measured in the treatment and control group, respectively. $[ \cdot]$ evaluates to $0$ or $1$ depending on whether the boolean expression within evaluates to false or true (Iversonian notation).&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;This method has a few weaknesses we can anticipate from the previous general assessment of the treatment group. A basic issue is that since many units in the treatment do not actually experience the treatment, we would expect that for such units, there is no impact of the treatment. This would effectively shrink our estimates of the effect towards a null point. To refine the analysis, we could focus on estimating the effect of the Treatment on the Treated (TOT). That is, we consider only the units in the treatment group that actually received the treatment to the entire control group. We could adjust for this in a simple way by scaling our estimated effect by the fraction of impacted units as follows:$$&lt;br /&gt;\hat{\theta}^*_{\mathrm{TOT}}&lt;br /&gt;= \left( \frac{1}{N_{Q=1}} \sum_i Y_i [Q_i=1] - \frac{1}{N_{Q=0}} \sum_j Y_j [Q_j=0] &lt;br /&gt;\right) \frac{N_{Q=1}}{N_{Q=1, U=1}}&lt;br /&gt;$$&amp;nbsp;Here, we introduce the additional notation $N_{Q=1, U=1}$, which represents the number of units that received the treatment within the treatment group. This estimator adjusts for the gross fraction of users who actually receive the treatment. A more direct estimator of the TOT effect slices the treatment group:$$&lt;br /&gt;\hat{\theta}_{\mathrm{TOT}} =&lt;br /&gt;\frac{1}{N_{Q=1,U=1}} \sum_i Y_i [Q_i=1 \cap U_i=1] - \frac{1}{N_{Q=0}} \sum_j Y_j [Q_j=0]&lt;br /&gt;$$ We can now do our first evaluation of these methods, by comparing ITT and TOT estimates computed during the beginning ($T_{\mathrm{measure}}$) and end ($T_{\mathrm{final}}$) of the observation period. The following figure displays these results:&lt;br /&gt;&lt;div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-YrYPb7iBrzQ/WrM_8qQNyWI/AAAAAAAAc2M/KB9KMwyGLcI1DoQ8akTZOTw_mxB9aXu1gCEwYBhgL/s1600/F4__ITT_TOT_first_last.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;985&quot; data-original-width=&quot;1600&quot; height=&quot;392&quot; src=&quot;https://3.bp.blogspot.com/-YrYPb7iBrzQ/WrM_8qQNyWI/AAAAAAAAc2M/KB9KMwyGLcI1DoQ8akTZOTw_mxB9aXu1gCEwYBhgL/s640/F4__ITT_TOT_first_last.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Fig 4: How the ITT and TOT estimates evolve over time.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;div&gt;&lt;br /&gt;Here, we see that the ITT method performs poorly; it estimates quite a different effect at the beginning ($t=T_{\mathrm{measure}}$) than at the end ($t=T_{\mathrm{final}}$) of the period. This is likely because the number of users experiencing the treatment is increasing over time, and so the earlier estimate is shrunk more strongly towards zero. The TOT method performs somewhat better in terms of stability, but the estimate declines between the two time points. The differences between the distribution of users experiencing the treatment and the population are likely to be a key factor here.&lt;br /&gt;&lt;br /&gt;Indeed, both of these estimators do not well estimate the treatment effect for all users if this set of treated users is not a random subset of the population, and if the covariates that differ between this subset and the population are also correlated with our outcome and the treatment effect. From our earlier analysis, we can see that this is not the case by comparing the distribution over one categorical covariate. To proceed, we fully characterize the types of units that are more likely to adopt the treatment at an earlier stage in our experiment.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Compliance Bias&lt;/h2&gt;A central issue in this application is that users assigned treatment sometimes do not actually experience the treatment at $T_{\mathrm{measure}}$, and furthermore this set of users is not random. Here, we can draw a direct analogy to Compliance Bias, which is primarily described in literature on the analysis of medical studies. This type of bias can occur when users do not adhere to their assignment to an intervention plan, for example when patients with less acute disease symptoms more often refuse to take a drug they were given.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;To make this issue precise, we expand our language of potential outcomes, and introduce a set of four potential outcomes for each unit accounting for both availability of the treatment and the actual application thereof. To index these situations, we combine our two binary variables $Q$ for assignment and $U$ for treatment experience to give the following table of potential outcomes:&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;span id=&quot;docs-internal-guid-56b1ca24-46b2-68b0-6d48-c4f816314cb2&quot;&gt;&lt;/span&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;margin-left: -5.25pt;&quot;&gt;&lt;table style=&quot;border-collapse: collapse; border: none;&quot;&gt;&lt;colgroup&gt;&lt;col width=&quot;245&quot;&gt;&lt;/col&gt;&lt;col width=&quot;345&quot;&gt;&lt;/col&gt;&lt;/colgroup&gt;&lt;tbody&gt;&lt;tr style=&quot;height: 24pt;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; font-style: italic; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;Potential outcome index (Q, U)&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; font-style: italic; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;Description&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0pt;&quot;&gt;&lt;td style=&quot;border-color: rgb(0, 0, 0); border-style: solid; border-width: 1pt; padding: 5pt; text-align: center; vertical-align: top;&quot;&gt;$(0, 0)$&lt;/td&gt;&lt;td style=&quot;border-color: rgb(0, 0, 0); border-style: solid; border-width: 1pt; padding: 5pt; text-align: left; vertical-align: top;&quot;&gt;Control group user not experiencing treatment&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 25pt;&quot;&gt;&lt;td style=&quot;border-color: rgb(0, 0, 0); border-style: solid; border-width: 1pt; padding: 5pt; text-align: center; vertical-align: top;&quot;&gt;$(0, 1)$&lt;/td&gt;&lt;td style=&quot;border-color: rgb(0, 0, 0); border-style: solid; border-width: 1pt; padding: 5pt; text-align: left; vertical-align: top;&quot;&gt;Control group user experiencing the treatment&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0pt;&quot;&gt;&lt;td style=&quot;border-color: rgb(0, 0, 0); border-style: solid; border-width: 1pt; padding: 5pt; text-align: center; vertical-align: top;&quot;&gt;$(1, 0)$&lt;/td&gt;&lt;td style=&quot;border-color: rgb(0, 0, 0); border-style: solid; border-width: 1pt; padding: 5pt; text-align: left; vertical-align: top;&quot;&gt;Treatment user not experiencing treatment &lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0pt;&quot;&gt;&lt;td style=&quot;border-color: rgb(0, 0, 0); border-style: solid; border-width: 1pt; padding: 5pt; text-align: center; vertical-align: top;&quot;&gt;$(1, 1)$&lt;/td&gt;&lt;td style=&quot;border-color: rgb(0, 0, 0); border-style: solid; border-width: 1pt; padding: 5pt; text-align: left; vertical-align: top;&quot;&gt;Treatment user experiencing treatment&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;&lt;span id=&quot;docs-internal-guid-56b1ca24-46b2-68b0-6d48-c4f816314cb2&quot;&gt;&lt;/span&gt;&lt;/div&gt;&lt;br /&gt;&lt;div&gt;In the context of this application, these rows have already so far been roughly explored. Users in the control are expected to behave as $(0, 0)$ indicates (not offered the upgrade, don’t take the upgrade), and users in the treatment group may be in state $(1, 0)$ or $(1, 1)$. Users realized in the case $(0, 1)$ may seem impossible or surprising, as they represent a sort of leakage of the treatment condition into the control group. These correspond to sideloading behavior, where a user obtains the update without an offer, possibly through internet backchannels, which was discussed above.&lt;br /&gt;&lt;br /&gt;It further helps to map out the full potential outcomes for each unit. That is, for a fixed unit, what are the pair of potential outcomes we might see if we vary the group assignment Q? This gives the following table (see [1]):&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;span id=&quot;docs-internal-guid-56b1ca24-46b8-9ea6-cf22-f53fbebf7417&quot;&gt;&lt;/span&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;margin-left: -5.25pt;&quot;&gt;&lt;table style=&quot;border-collapse: collapse; border: none;&quot;&gt;&lt;colgroup&gt;&lt;col width=&quot;162&quot;&gt;&lt;/col&gt;&lt;col width=&quot;167&quot;&gt;&lt;/col&gt;&lt;col width=&quot;303&quot;&gt;&lt;/col&gt;&lt;/colgroup&gt;&lt;tbody&gt;&lt;tr style=&quot;height: 24pt;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; font-style: italic; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;Potential outcomes for unit over groups&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; font-style: italic; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;(Q, U) ; (Q’, U’)&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; font-style: italic; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;User Type&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; font-style: italic; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;Rough Description&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0pt;&quot;&gt;&lt;td style=&quot;border-color: rgb(0, 0, 0); border-style: solid; border-width: 1pt; padding: 5pt; text-align: center; vertical-align: top;&quot;&gt;$(0, 0); (1, 0)$&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;Never-taker&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;User that will never upgrade&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 25pt;&quot;&gt;&lt;td style=&quot;border-color: rgb(0, 0, 0); border-style: solid; border-width: 1pt; padding: 5pt; text-align: center; vertical-align: top;&quot;&gt;$(0, 0); (1, 1)$&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;Complier&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;User that will upgrade if offered&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0pt;&quot;&gt;&lt;td style=&quot;border-color: rgb(0, 0, 0); border-style: solid; border-width: 1pt; padding: 5pt; text-align: center; vertical-align: top;&quot;&gt;$(0, 1); (1, 0)$&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;Defier&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;User that will avoid upgrades if offered, seek them if not offered&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0pt;&quot;&gt;&lt;td style=&quot;border-color: rgb(0, 0, 0); border-style: solid; border-width: 1pt; padding: 5pt; text-align: center; vertical-align: top;&quot;&gt;$(0, 1); (1, 1)$&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;Always-taker&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1pt; border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-top: solid #000000 1pt; padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;User that will seek out the upgrade in all conditions (sideloading)&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;&lt;span id=&quot;docs-internal-guid-56b1ca24-46b8-9ea6-cf22-f53fbebf7417&quot;&gt;&lt;br /&gt;While they may exist in theory, do we have units of each of these types? In our application, never-takers (units who can never execute an update) and compliers (units who will upgrade if given the chance) seem reasonably common. Note here that we evaluate $U$ at $T_{\mathrm{measure}}$, so never-takers here are those who would not experience the treatment at $T_{\mathrm{measure}}$, regardless of assignment $Q$. &lt;br /&gt;&lt;br /&gt;We will ignore always-takers and defiers, since in our application the number of users with realized $(0, 1)$ outcomes are extremely rare ($\ll 1\%$). This implies our population consists overwhelmingly of compliers and never-takers. This greatly simplifies our situation, and makes full conditional observable or conditional compliance modeling approaches unnecessary [1]. Conditional compliance models estimate causal effects for each of the four types of users in the table. Conditional observable models try to estimate relationships between all four counterfactual quantities for each user.&lt;/span&gt;&lt;/div&gt;&lt;br /&gt;&lt;h2&gt;Propensity scoring within the treatment&lt;/h2&gt;We now explore statistical strategies for estimation that account for the difference in users who experience the treatment. A starting idea is to analyze the treatment alone as an observational study (an analysis on $Q=1$ only). Here, we ignore any control group, and analyze the treatment group as a self-contained observational study for units where $Q=1$. We attack this via propensity modeling, using $U$ as the new ‘treatment’ variable and reducing our set of potential outcomes to $\{Y(1, 1), Y(1, 0)\}$. In this case, we fit the following logistic regression:$$&lt;br /&gt;&lt;br /&gt;\mathrm{logit}( \Pr(U | X, Q=1) ) = X \beta &lt;br /&gt;&lt;br /&gt;$$We then estimate the effect using reweighting, matching or stratification methods. Before proceeding to effect estimates, it is useful to examine the output of this model. At both the start and end of the study, we can produce a histogram of the propensity scores for $Q=1$, sliced by $U$, that is, the probability that a user within the treatment group will actually compete the upgrade (receive the treatment)&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://4.bp.blogspot.com/-6iLyqSrrVno/WrM_8lwhoqI/AAAAAAAAc1w/VjkAsm1XZzoJ6Vsaf7tC2T_dNRHVc4AcACEwYBhgL/s1600/F5__propensity_histogram_first.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;985&quot; data-original-width=&quot;1600&quot; height=&quot;392&quot; src=&quot;https://4.bp.blogspot.com/-6iLyqSrrVno/WrM_8lwhoqI/AAAAAAAAc1w/VjkAsm1XZzoJ6Vsaf7tC2T_dNRHVc4AcACEwYBhgL/s640/F5__propensity_histogram_first.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Fig 5: Estimated probability of experiencing the treatment in the treatment group. Observe the subset unlikely to uptake.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;br /&gt;We see that a collection of never-takers immediately stands out with very low estimated scores, a clear conclusion even from the start of the study. These users can be safely discarded from our effect estimation analysis. Further, the existence of never-taker users calls into the question of the ITT analysis, even at $T_{\mathrm{final}}$. If there are users who will never experience the counterfactual treatment state, then ITT will never estimate the difference between these states if those users are included in the estimation. Otherwise, the model produces a decent range of $\Pr(U=1 | X)$ predictions, and we can see from the plot that the number of users with $U=1$ are more common at higher probabilities. As a classifier, the model gives merely decent performance, which is actually advantageous for propensity methods. If in this case we instead had a clean separation of the two classes for all users, this would imply that certain user factors completely determine adoption. Therefore, within the treatment group, many users with $U=0$ would be without a peer user with $U=1$, so we could not reasonably estimate our target effect. We then would not be able to generalize our conclusions to the entire user population. This problem would be somewhat mitigated if only a subset of users have estimated probability $0$ or $1$, and we were therefore able to understand clearly for which users we cannot estimate the effect / cannot find matching users.&lt;br /&gt;&lt;br /&gt;For estimation, we consider stratification or bucketing methods. That is, we take our range of estimated propensity scores, and partition them into buckets. We then perform analysis within each bucket, and collect the within-bucket estimates to form an overall estimate. Since the scores are estimated from a model depending on many covariates, this bucketing has the effect of partitioning users based on their covariates, reduced to a single univariate measure: $\Pr(U=1 | X)$. We adopt a simple form of stratification&amp;nbsp;where we partition the range $0$ to $1$ into twenty $0.05$-width strata $S_k$ for $1\leq k \leq 20$. Let $\hat{p}_i$ be the estimated propensity score for unit $i$. We can compute a propensity weighted comparison by users whose scores fall into the $k$th&amp;nbsp;stratum $S_k$ as follows: $$&lt;br /&gt;\hat{\theta}_{\mathrm{p-weight}, k} =&lt;br /&gt;\frac{\sum_i \frac{Y_i}{\hat{p}_i} [\hat{p}_i \in S_k \cap Q_i=1 \cap U_i=1] }{\sum_i \frac{1}{\hat{p}_i}[\hat{p}_i \in S_k \cap Q_i=1 \cap U_i=1]} -&lt;br /&gt;\frac{\sum_i \frac{Y_i}{1-\hat{p}_i} [\hat{p}_i \in S_k \cap Q_i=1 \cap U_i=0] }{\sum_i \frac{1}{1-\hat{p}_i}[\hat{p}_i \in S_k \cap Q_i=1 \cap U_i=0]}$$&lt;br /&gt;We can then collect these into a single estimate by taking a weighted combination of these strata comparisons, where $N_k$ is the number of users falling into the $k$th stratum:$$&lt;br /&gt;\hat{\theta}_{\mathrm{p-weight}} = &lt;br /&gt;\frac{ \sum_k N_k \hat{\theta}_{\mathrm{p-weight}, k}} {\sum_k N_k}&lt;br /&gt;$$&lt;br /&gt;It is useful to examine the results per strata, as the following plot does. Here, we compare the results on users at both $T_\mathrm{measure}$ and $T_\mathrm{final}$, where the strata and $\hat{p}$ are defined by the model fit at $T_\mathrm{measure}$ only.&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://2.bp.blogspot.com/-gk-mG5BOWBA/WrM_8okGWaI/AAAAAAAAc2E/cXiAqN9isuQXuqaamN7qJYM7cq-jXv7cQCEwYBhgL/s1600/F6__weighting_bucket_plot.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;985&quot; data-original-width=&quot;1600&quot; height=&quot;392&quot; src=&quot;https://2.bp.blogspot.com/-gk-mG5BOWBA/WrM_8okGWaI/AAAAAAAAc2E/cXiAqN9isuQXuqaamN7qJYM7cq-jXv7cQCEwYBhgL/s640/F6__weighting_bucket_plot.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Fig 6: Propensity scores estimated for each stratum. For the most part, the estimates are in good agreement, but differ significantly in the high propensity strata.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;We first see that the model performs well for the lower strata (e.g. $(0.45, 0.5]$), in that the estimate at $T_\mathrm{final}$ is close to that made at $T_\mathrm{measure}$. The approach performs worse for higher buckets, which is expected as there are fewer users with $Q=1, U=0$ here.&lt;br /&gt;&lt;br /&gt;A more striking overall point is that the effect is not uniform across the buckets. In fact, the impact of the update increases as our estimate of $\Pr(U=1 | X)$ increases. This means that for different users, the new game has a different effect. This is a valuable insight that the ITT and TOT approaches do not provide, as their estimand assumes that the treatment effect is a universal mean shift across all users. &lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Propensity score matching to the control&lt;/h2&gt;Another approach is to leverage the control group along with our propensity scores. With a control group, we have access to many users for whom we (mostly) observe $Y(0, 0)$ for the entire range of estimated propensity scores. In contrast, in our within treatment approach, there are fewer users with realized outcome $Y(1, 0)$ as the estimated propensity score increases. After fitting a propensity model to the treatment group, we can estimate the probability of each member of the control group experiencing treatment by assuming that $\Pr( U =1 | X, Q = 0) = \Pr( U =1| X, Q = 1)$ and $Y(0, 0) = Y(1, 0)$. With a $\Pr(U=1 | X)$ available in for each unit, we can now perform some form of matching, either exact or stratified, between units and take paired differences between the resulting groups. To obtain an estimate of $E(Y(1, 1) - Y(1, 0))$, we select only groups containing units receiving the treatment in the treatment group and compare them as follows:$$&lt;br /&gt;\hat{\theta}_{\mathrm{p-match}, k} =\frac{\sum_i \frac{Y_i}{\hat{p}_i} [\hat{p}_i \in S_k \cap Q_i=1 \cap U_i=1] }{\sum_i \frac{1}{\hat{p}_i}[\hat{p}_i \in S_k \cap Q_i=1 \cap U_i=1]} -&lt;br /&gt;\frac{\sum_i \frac{Y_i}{1-\hat{p}_i} [\hat{p}_i \in S_k \cap Q_i=0 \cap U_i=0] }{\sum_i \frac{1}{1-\hat{p}_i}[\hat{p}_i \in S_k \cap Q_i=0 \cap U_i=0]}&lt;br /&gt;$$&lt;br /&gt;This approach has the distinct advantage in comparison to our ‘within treatment’ stratification analysis pool of users without the treatment at all time points for all strata. Again, we can plot the performance at both the start and end of the study:&lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-dH2Wdhxxqdg/WrM_9HOlDbI/AAAAAAAAc2I/ThiZeliVaEMGNjzTfj4-Zxr_GKULkXvoACEwYBhgL/s1600/F7__matching_bucket_plot.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;985&quot; data-original-width=&quot;1600&quot; height=&quot;392&quot; src=&quot;https://3.bp.blogspot.com/-dH2Wdhxxqdg/WrM_9HOlDbI/AAAAAAAAc2I/ThiZeliVaEMGNjzTfj4-Zxr_GKULkXvoACEwYBhgL/s640/F7__matching_bucket_plot.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Fig 7: Analogous to Fig 6, but with better agreement in the high propensity strata&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;The results here are similar to the previous approach in the lower propensity score buckets. The main improvement comes at the higher buckets, where the estimates at $T_\mathrm{final}$ and $T_\mathrm{measure}$ are now as close as in the other buckets. The core reason for this improvement is that in this approach, we have many users available in the control ($Q=0$) with similar user features as those in the treatment ($Q=1$) that would produce higher propensity scores. In contrast, the $Q=0$ users have primarily $U=1$ users with high propensity scores, leading to poor estimates in these strata.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://2.bp.blogspot.com/-uKSkyqB_97w/WrM_9VMPdkI/AAAAAAAAc2A/z4z8NP38pi0MvtN88j2tms_wbWKZ5P-TwCEwYBhgL/s1600/F8__propensity_first_last.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;985&quot; data-original-width=&quot;1600&quot; height=&quot;392&quot; src=&quot;https://2.bp.blogspot.com/-uKSkyqB_97w/WrM_9VMPdkI/AAAAAAAAc2A/z4z8NP38pi0MvtN88j2tms_wbWKZ5P-TwCEwYBhgL/s640/F8__propensity_first_last.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Fig 8: As expected, propensity matching is more consistent over time than propensity weighting&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2&gt;&lt;/h2&gt;&lt;h2&gt;Conclusion&lt;/h2&gt;Experiment analysis often cannot rely on the assumption of faithful adoption of the treatment condition. Here, we’ve explored a case where many users assigned the treatment do not actually experience the treatment for a long time period after the beginning of the experiment. Moreover, waiting until a steady state of treatment adoption to draw inference is often impractical, so we have to make do with a biased early subset of users. As we’ve shown, adjustments are possible, but a litany of assumptions and concerns must be dealt with. Several complexities are left unaddressed here, such as effects that evolve over time or large volumes of users falling into ‘defier’ or ‘always-taker’ categories that require further refinements of approaches. Nonetheless, propensity based models often provide insightful refinements to the basic ITT or TOT approaches, and would form the basis for methods that would address these complexities.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;References&lt;/h3&gt;&lt;/div&gt;&lt;div&gt;[1] Have, Thomas R. Ten, et al. “Causal Models for Randomized Physician Encouragement Trials in Treating Primary Care Depression.” Journal of the American Statistical Association, vol. 99, no. 465, 2004, pp. 16–25. JSTOR, JSTOR, www.jstor.org/stable/27590349.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;</content><link rel='replies' type='application/atom+xml' href='https://www.unofficialgoogledatascience.com/feeds/6625835772947801316/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='https://www.unofficialgoogledatascience.com/2018/03/quicker-decisions-in-imperfect-mobile.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/6625835772947801316'/><link rel='self' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/6625835772947801316'/><link rel='alternate' type='text/html' href='https://www.unofficialgoogledatascience.com/2018/03/quicker-decisions-in-imperfect-mobile.html' title='Compliance bias in mobile experiments'/><author><name>Amir Najmi</name><uri>http://www.blogger.com/profile/18174523203317227640</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-6Bq_8ZA0jrQ/WrM_7ptvmdI/AAAAAAAAc1g/FNF2RORH7LsfssHTQX4EnIZLQcs5ElRWgCEwYBhgL/s72-c/F1__ramp_up.png" height="72" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-5825758052688213474.post-8391093797732166139</id><published>2018-01-16T18:04:00.002-08:00</published><updated>2022-04-06T09:07:52.749-07:00</updated><title type='text'>Designing A/B tests in a collaboration network</title><content type='html'>by SANGHO YOON&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;i&gt;In this article, we discuss an approach to the design of experiments in a network. In particular, we describe a method to prevent potential contamination (or inconsistent treatment exposure) of samples due to network effects. We present data from Google Cloud Platform (GCP) as an example of how we use A/B testing when users are connected. Our methodology can be extended to other areas where the network is observed and when avoiding contamination is of primary concern in experiment design. We first describe the unique challenges in designing experiments on developers working on GCP. We then use simulation to show how proper selection of the randomization unit can avoid estimation bias. This simulation is based on the actual user network of GCP.&lt;/i&gt;&lt;br /&gt;&lt;span id=&quot;docs-internal-guid-188b50a0-e687-23fe-abf4-60d956055494&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;h2&gt;Experimentation on networks&lt;/h2&gt;A/B testing is a standard method of measuring the effect of changes by randomizing samples into different treatment groups. Randomization is essential to A/B testing because it removes selection bias as well as the potential for confounding factors in assessing treatment effects.&lt;br /&gt;&lt;br /&gt;At Google, A/B testing plays a key role in better understanding our users and products. With A/B testing, we can validate various hypotheses and measure the impact of our product changes, allowing us to make better decisions. Of course, A/B testing is not something new in our field, as it has been adopted by many tech companies. But due to the large scale and complexity of data, each company tends to develop its own A/B test solution to solve its unique challenges. One particular area involves experiments in marketplaces or social networks where users (or randomized samples) are connected and treatment assignment of one user may influence another user&#39;s behavior.&lt;br /&gt;&lt;br /&gt;Typical A/B experiments assume that the response or behavior of an individual sample depends only on its own assignment to a treatment group. This is known as the Stable Unit Treatment Value Assumption (SUTVA). However, this assumption no longer holds when samples interact with each other, such as in a network. An example of this is when the effects of exposed users can spill over to their peers. This is the case for experiments on the Google Cloud Platform.&lt;br /&gt;&lt;br /&gt;&lt;a href=&quot;https://cloud.google.com/&quot;&gt;Google Cloud Platform&lt;/a&gt; (GCP) offers a suite of products that enable developers to work on their projects in the cloud. GCP also provides great flexibility to developers in sharing their resources and projects, with tools to protect and control their security and privacy. We find that users in GCP naturally form collaboration networks to work on shared projects and this in turn improves efficiency in managing their resources. Our goal is to leverage this network structure in designing and analyzing experiments to improve the GCP product.&lt;br /&gt;&lt;br /&gt;One salient requirement we impose on experiment design is that the experience of all users who collaborate with each other be consistent. This is critical to our highly collaborative product. For example, imagine a situation where two users collaborate on a shared project. One user sees a new feature to enable a firewall, but the other user doesn’t see the same option available. This could create confusion. Such an undesired effect is not only bad for user experience, it also hinders us measuring the &lt;b&gt;true&amp;nbsp;&lt;/b&gt;average treatment effect&amp;nbsp;of the new firewall feature. Thus, the requirement is to provide a consistent user experience for the following two types of treatment exposure:&lt;br /&gt;&lt;div&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Direct exposure&lt;/b&gt;: Every user must have a consistent experience across all GCP projects that he or she owns, manages, or collaborates in.&amp;nbsp;&lt;/li&gt;&lt;li&gt;&lt;b&gt;Indirect exposure&lt;/b&gt;: Any two users who collaborate on a project must have the same experience.&lt;/li&gt;&lt;/ul&gt;The graph of user collaboration can be separated into distinct connected components (hereafter referred to as &quot;components&quot;). In order to satisfy these consistency requirements, we choose to use the component as our randomization unit in experiment design.&lt;br /&gt;&lt;br /&gt;While the collaboration networks in Google Cloud Platform bear similarities to components of social networks (e.g. Facebook, Twitter, LinkedIn, Google+), there are significant differences. Two fundamental differences are described below:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;A few large connected networks versus many connected components&lt;/b&gt;: Users in social networks are linked to each other through their common friends. Methodologies for experimenting on users in social networks focus on ways to partition the overall graph into subgraphs, and then to run randomized experiments on these subgraphs. Severing edges is necessary because the largest connected components of the graph are typically very large. In GCP, however, we observe many small connected components because our customers want to manage their own privacy and security in their projects, and do not want to share access with third parties.&lt;/li&gt;&lt;li&gt;&lt;b&gt;Spillover effects versus contamination&lt;/b&gt;: Experiments in social networks must care about &quot;spillover&quot; or influence effects from peers. These spillover effects are a fundamental aspect of user behavior in a social network. Thus, the effect comes from both direct exposure of each treated individual, and indirect exposure from his or her peers. Such spillover effects also exist in the GCP user collaboration network but they are of secondary importance. In our case, avoiding confusion is more important than estimating indirect treatment effects. For example, imagine the confusion resulting from two users who work on a shared project but see two different versions. We need to avoid these effects rather than estimate them.&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;br /&gt;&lt;h2&gt;Structure of the user collaboration network&lt;/h2&gt;As mentioned earlier, users in GCP collaborate with other developers via shared projects. Projects are linked to a Google Cloud billing account for proper resource management and billing. Since a project can be linked to at most one billing account, the project-to-billing account relationship is nested. However, a user can work on multiple projects.The user-to-project relationship is not necessarily nested. Rather, users can have membership in multiple projects. Therefore the relationship among billing account, project and user is complex. Figure 1 illustrates these three entities.&lt;br /&gt;&lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-QfwFNR7tU5k/Wl6z1SAwnGI/AAAAAAAAcrU/fFOqv08cOv8eWSoNYlkGUSgOS724pVm5ACLcBGAs/s1600/sangho2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;275&quot; data-original-width=&quot;624&quot; height=&quot;282&quot; src=&quot;https://3.bp.blogspot.com/-QfwFNR7tU5k/Wl6z1SAwnGI/AAAAAAAAcrU/fFOqv08cOv8eWSoNYlkGUSgOS724pVm5ACLcBGAs/s640/sangho2.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot;&gt;Figure 1. Connected components: an advanced account with multiple billing accounts managing separate sets of projects (left), a developing account with multiple projects (middle), and a new account (right)&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;Since users can be connected via shared projects, we also need to track another entity, the component of the graph to which any user belongs. A user can be associated with exactly one component. Figure 2 shows that the user collaboration graph has three distinct components.&lt;br /&gt;&lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-gMnrEuuLJeE/Wl625X8TJ6I/AAAAAAAAcrg/JvEffkVnzpI9gVb1t0dU_0totrxY2lGVwCLcBGAs/s1600/Sangho3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;322&quot; data-original-width=&quot;564&quot; height=&quot;364&quot; src=&quot;https://3.bp.blogspot.com/-gMnrEuuLJeE/Wl625X8TJ6I/AAAAAAAAcrg/JvEffkVnzpI9gVb1t0dU_0totrxY2lGVwCLcBGAs/s640/Sangho3.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot;&gt;Figure 2. Hierarchy: component → account → project → user&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2&gt;&lt;br /&gt;Designing experiments on the collaboration network&lt;/h2&gt;The hierarchical structure of the collaboration network makes it clear that we must use component as the unit of randomization in our experiments. This is necessary to provide guarantees on treatment consistency. However, the downside of using a larger unit of randomization is that we lose experimental power. This comes from two factors: fewer experimental units, and greater inherent difference across experimental units.&lt;br /&gt;&lt;br /&gt;Figure 3 shows distribution of project size, as measured in # users per project, and the distribution of # of project per user (axes have been removed for confidentiality). These contribute to the structure and size of components. We see here indications of large differences in size and structure of components. These differences tend to increase the variance of our estimates and hence lose us statistical power.&lt;br /&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-9fjoPq07hOQ/Wl643XJRD7I/AAAAAAAAcr0/wA_NXaTF9ikodROngLigGNKlaXQKR6M3QCLcBGAs/s1600/Sangho3.1.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1600&quot; data-original-width=&quot;1600&quot; height=&quot;320&quot; src=&quot;https://3.bp.blogspot.com/-9fjoPq07hOQ/Wl643XJRD7I/AAAAAAAAcr0/wA_NXaTF9ikodROngLigGNKlaXQKR6M3QCLcBGAs/s320/Sangho3.1.png&quot; width=&quot;320&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-jCA2qmXd_Ag/Wl643GkY9kI/AAAAAAAAcrw/Inoij1AX0oYQOEua7bEJUSi_IGfXT6DlwCEwYBhgL/s1600/sangho3.2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1600&quot; data-original-width=&quot;1600&quot; height=&quot;320&quot; src=&quot;https://1.bp.blogspot.com/-jCA2qmXd_Ag/Wl643GkY9kI/AAAAAAAAcrw/Inoij1AX0oYQOEua7bEJUSi_IGfXT6DlwCEwYBhgL/s320/sangho3.2.png&quot; width=&quot;320&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot;&gt;&lt;br /&gt;Figure 3: # of users (per project) and # of projects (per user)&lt;br /&gt;&lt;br /&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;One way to mitigate this loss of power is to cluster samples into more homogeneous strata and sample proportionately from each stratum. We define strata based on two features: number of users and &quot;usage&quot;, a measure of the aggregate user activity in the network. These two properties were selected because they correlate strongly with experiment metrics of greatest interest.&lt;br /&gt;&lt;div&gt;&lt;div&gt;&lt;br /&gt;By drawing a fixed fraction of units from each stratum, we achieve better balance across treatment groups, and hence reduce variance in our estimates. In addition, stratified sampling helps us obtain representative samples when the sampling rate is low.&lt;br /&gt;&lt;br /&gt;The overall procedure of our methodology for stratified random sampling is described as follows:&lt;br /&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;Build user graphs&lt;/b&gt;: Find all the components in the current collaboration network.&lt;/li&gt;&lt;li&gt;&lt;b&gt;Stratify graphs by size and usage&lt;/b&gt;: Measure the size of each component by number of users and revenue and stratify graphs in number of users and revenue.&lt;/li&gt;&lt;li&gt;&lt;b&gt;Select samples and random assignment&lt;/b&gt;: Randomly sample a fraction of components in each stratum from Step 2 depending on the size of a study. Then randomly assign them to a treatment arm. For example, if we wish to run an experiment with a 5% arm for treatment and 5% for control, we first select a random 10% of components from each stratum, and subsequently assign them 50-50 to treatment and control groups. Each user, project and account inherits the experiment group from the component to which they belong.&lt;/li&gt;&lt;li&gt;&lt;b&gt;Run experiment&lt;/b&gt;: Steps 2 and 3 are repeated daily after the graph has been updated, and new components properly also randomized.&lt;/li&gt;&lt;/ol&gt;The description above assumes that while new components may emerge over the course of the experiment, the topology of existing components will remain unchanged during this time. As discussed later, this is not entirely the case.&amp;nbsp; Figure 4 is a visual representation of these steps.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgXSjtfHH6RIQ0Eayu443zQnc3ZI3OKPTvXy7_JvptBrJXGNjC4RTTnUMoEngpy3WIGp8mArARyw99MyDhRqViFn_dtbv-EE3VChfFKJGFxjxVqDd2DVJIjYCRd8O-WDfqXQ_bQcZZPWrToVrJ8hbXkNjr6CoFg0i_BjmPkHPnXjC0jufTKsHwTWmfA-g/s967/Fig%204.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;407&quot; data-original-width=&quot;967&quot; height=&quot;270&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgXSjtfHH6RIQ0Eayu443zQnc3ZI3OKPTvXy7_JvptBrJXGNjC4RTTnUMoEngpy3WIGp8mArARyw99MyDhRqViFn_dtbv-EE3VChfFKJGFxjxVqDd2DVJIjYCRd8O-WDfqXQ_bQcZZPWrToVrJ8hbXkNjr6CoFg0i_BjmPkHPnXjC0jufTKsHwTWmfA-g/w640-h270/Fig%204.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;&lt;span id=&quot;docs-internal-guid-991fe212-7fff-75fc-1f42-0487cb1d83c4&quot;&gt;&lt;p dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: #4e4e4e; font-family: Roboto, sans-serif; font-size: 11.5pt; font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;Figure 4. components and random sampling with stratification&lt;/span&gt;&lt;/p&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;div&gt;&lt;h2&gt;&lt;br /&gt;Modeling network effects&lt;/h2&gt;In order to quantify the tradeoffs involved in experiment design, we need a model of network effects to be used in subsequent simulation studies. We now describe a generative model for how effects might propagate through the network. The network topology itself is the actual collaboration network we observe for GCP.&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;font-variant-east-asian: normal; font-variant-numeric: normal; text-align: justify; vertical-align: baseline;&quot;&gt;&lt;br /&gt;Consider the case where experiment metrics are evaluated at the per-user level. Assume we have $K$ users. Let $Z_k$ denote the assignment of the $k^{th}$ user to an arm of the experiment.  Here $Z_k = 0$ means the user is assigned to control and $Z_k = 1$ for treatment. Under Stable Unit&amp;nbsp;&lt;/span&gt;Treatment Value Assumption (SUTVA), one can estimate treatment effect as follows:&lt;br /&gt;$$&lt;br /&gt;\delta = \frac{1}{N_0}\sum_k Y_k[Z_k = 1] - \frac{1}{N_1}\sum_k Y_k [Z_k=0]&amp;nbsp; &amp;nbsp; \tag{1}&lt;br /&gt;$$ where $N_0$ and $N_1$ are the number of samples assigned to treatment group and control group, respectively. This is equivalent to estimating in the following linear model:&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;$$&lt;br /&gt;y_k \sim \mu +&amp;nbsp;\tau z_k\tag{2}&lt;br /&gt;$$ where $\mu$ is an overall intercept term and $\tau$ is the effect of treatment.&lt;/div&gt;&lt;br /&gt;When users are connected in a network, their treatment assignments can generate network effects through their interactions. Our model considers two aspects of network effects:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Homophily&lt;/b&gt; or similarity within network: users collaborating in network tend to behave similarly. For example, developers working on a specific mobile app show similar behavior in usage. We use hierarchical models for this effect.&lt;/li&gt;&lt;li&gt;&lt;b&gt;Spillover&lt;/b&gt; or contamination effects: direct treatment effects can spill over through network connections. We conservatively limit the degree of spillover effects to immediate neighbors.&lt;/li&gt;&lt;/ul&gt;We model network similarity using the fully nested hierarchical structure: component → account → project. Then we use random effects to model response from this hierarchy as follows: &lt;br /&gt;$$&lt;br /&gt;y_{i,j,k} \sim c_i + a_{i,j} + p_{i,j,k}&amp;nbsp; &amp;nbsp; \tag{3}&lt;br /&gt;$$ where $c_i$ refers to the response from Component $i$, $a_{i,j}$ from Account $j$ in Component $i$, and $p_{i,j,k}$ from Project $k$ in Account $j$ in Component $i$. The random effects&lt;/div&gt;&lt;div&gt;\begin{align*}&lt;br /&gt;c_i &amp;amp;\sim N(0, \sigma_c^2)\\&lt;br /&gt;a_{i,j} &amp;amp;\sim N(0, \sigma_a^2)\\&lt;br /&gt;p_{i,j,k} &amp;amp;\sim N(0, \sigma_p^2)&lt;br /&gt;\end{align*} can model potential correlation among accounts and projects within a component.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;Spillover effects are modeled as an additional component added to to the linear model in (2):&lt;br /&gt;$$&lt;br /&gt;y_k \sim \mu + \tau z_k + \gamma \tau a_k^T \cdot Z     \tag{4}&lt;br /&gt;$$ where $Z$ is a vector representing treatment group assignment of every user, and $a_k$ is the $k^{th}$ column of adjacency matrix $A$, i.e., $m^{th}$ element of $a_k$ is $1$ if the $k^{th}$ user and $m^{th}$ user are connected. Note that we only model first order spillover effects in (4). In other words, we do not consider potential effects from neighbors’ neighbors. Thus, our model is conservative with respect to spillover effects (i.e. it limits their impact). Combining spillover effects and similarity within network, we have &lt;br /&gt;$$&lt;br /&gt;y_{i,j,k} \sim \mu + \tau z_k + \gamma \tau a_k^T \cdot Z + (c_i + a_{i,j} + p_{i,j,k})   \tag{5}&lt;br /&gt;$$&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Experimental power and unit of randomization&lt;/h2&gt;We can use the model just defined to simulate the effect of randomization unit. We consider two randomization units: project and connected component. To further illustrate the effects of stratification on experimental power, we sample components either uniformly, or by strata. In other words, for the three methods of randomization&lt;br /&gt;&lt;ul&gt;&lt;li&gt;uniform random component&lt;/li&gt;&lt;li&gt;uniform random project&lt;/li&gt;&lt;li&gt;stratified random component&lt;/li&gt;&lt;/ul&gt;we simulate confidence intervals for A/A tests, i.e. when there is no real effect.&lt;br /&gt;&lt;br /&gt;Figure 5 shows empirical 95% confidence intervals for each of these sampling methods. Since the true effect is zero in each case, we expect our confidence intervals to include zero 95% of the time. The plot sorts 1000 empirical confidence intervals by their mid point (grey dot). The vertical line segment corresponding to each interval is green if it covers zero, red otherwise. Thus, the patch of red on either side consists of about 25 cases (i.e. 2.5%).&lt;br /&gt;&lt;br /&gt;The figure shows that random sampling by component has the widest confidence interval while random sampling by project has the least. Stratified sampling by component is in between. Thus stratification recovers some of the experimental power lost when going from sampling by project to sampling by connected component.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiKOXpeoUzRJJ_yFv_p9odcncB3OsHcmuguRSga3nxvJ_4Jk1IZyrRJZRJpIlTEDJhd22VjI0OjcG8UEz8x84H2ljmc-QX-cDJK-qRKquEfurxGiWzc4ec_Ps7cNUzqeqJjjXGSSlg-uohxqqntjPIsHHbxY7K-moeZ13_FW4h0pFxvu67H_XMidrWdvA/s1200/Fig%205.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;800&quot; data-original-width=&quot;1200&quot; height=&quot;426&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiKOXpeoUzRJJ_yFv_p9odcncB3OsHcmuguRSga3nxvJ_4Jk1IZyrRJZRJpIlTEDJhd22VjI0OjcG8UEz8x84H2ljmc-QX-cDJK-qRKquEfurxGiWzc4ec_Ps7cNUzqeqJjjXGSSlg-uohxqqntjPIsHHbxY7K-moeZ13_FW4h0pFxvu67H_XMidrWdvA/w640-h426/Fig%205.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;&lt;span id=&quot;docs-internal-guid-f625688c-7fff-b885-779e-dd76ab9b2acc&quot;&gt;&lt;div align=&quot;left&quot; dir=&quot;ltr&quot; style=&quot;margin-left: 0pt;&quot;&gt;&lt;table style=&quot;border-collapse: collapse; border: none;&quot;&gt;&lt;colgroup&gt;&lt;col width=&quot;594&quot;&gt;&lt;/col&gt;&lt;/colgroup&gt;&lt;tbody&gt;&lt;tr style=&quot;height: 27pt;&quot;&gt;&lt;td style=&quot;overflow-wrap: break-word; overflow: hidden; vertical-align: top;&quot;&gt;&lt;p dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;color: #4e4e4e; font-family: Roboto, sans-serif; font-size: 11.5pt; font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;Figure 5. A/A test results: Confidence intervals of three methods: random sampling by projects, random sampling by component, and stratified sampling by component.&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;&lt;h2&gt;Estimation bias due to unit of randomization&lt;/h2&gt;Of course, running null experiments is hardly the purpose of experiment design. The reason we chose component as the unit of experimentation was that it better captures spillover effects when they are not null. Because randomized projects does not take network effects into account, we would expect to incur bias to the extent there are spillover effects.&lt;br /&gt;&lt;br /&gt;We generate simulation data using (5) on the actual GCP user network, with the following parameter values for similarity and spillover effects:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;Similarity effect parameters: $\sigma_c=2$, $\sigma_a=1$ and $\sigma_p=0.5$&lt;/li&gt;&lt;li&gt;Direct treatment effect size: $\tau= \frac{1}{8}$&lt;/li&gt;&lt;li&gt;Spillover effect parameter: $\gamma=2^{-m}$, where $m = 1, 2, 3, 4, 5, 6, 7$ or $8$&lt;/li&gt;&lt;/ul&gt;We varied $m$ to better understand the contribution of network effects under different levels of spillover effect.&lt;br /&gt;&lt;br /&gt;For each setting of the parameters, we ran three randomized experiments, once for each of the three sampling methods. Each experiment ran with 50% in treatment and 50% in control. We repeated this whole process 1,000 times to estimate a distribution of effect size estimates.&lt;br /&gt;&lt;br /&gt;Figure 6 shows the distributions of estimated effect size for the three experiment designs based on 1,000 simulation data sets for fixed values of $\tau=1/8$ and $\gamma= 2^{-4}$. While the variance of the random project design is least, it incurs significant bias. In contrast, random component and stratified component have higher variance but no observable bias.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjZSKuSiVXj-LQbwvSqikGr-OYyqoMSkaFZZ3j_PUdZzzIPsuBx8QH8MLmGNS7sSyuBwkFaDJHBjCCIr3umbp5ndNhe_kCNK42ZdZUxa2tKl9S4JVOZgcGT_qfAUoppzY4zvpHxUBZsqAYnnhUtKam2WLcHf-5WGB9dUalYrRKOMcRzAZriCRbe3b5g1g/s1200/Fig%206.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;800&quot; data-original-width=&quot;1200&quot; height=&quot;426&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjZSKuSiVXj-LQbwvSqikGr-OYyqoMSkaFZZ3j_PUdZzzIPsuBx8QH8MLmGNS7sSyuBwkFaDJHBjCCIr3umbp5ndNhe_kCNK42ZdZUxa2tKl9S4JVOZgcGT_qfAUoppzY4zvpHxUBZsqAYnnhUtKam2WLcHf-5WGB9dUalYrRKOMcRzAZriCRbe3b5g1g/w640-h426/Fig%206.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot;&gt;Figure 6. Effect size estimates for each of the three experiment designs. Dotted line shows the true effect size. Distributions estimated from 1,000 simulations, $\tau=1/8$ and $\gamma = 2^{-4}$.&lt;br /&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;span style=&quot;text-align: center;&quot;&gt;The amount bias in a random project design depends on the level of spillover effect. This is shown for different values of $m$ in Figure 7. The bias of the random project design is such that its 95% confidence intervals, estimated under independence, exclude the true effect even for small spillover effects ($m \leq 6$).&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhx-K7ak-o3iJmHJA_4rX6nToJiMNOofOy4K6bYP-cUIJJ9Ko8fmL2ZbnZAUYp83D0tGtanilw1Y_qD4c7x4WxVPLPINwei0KGius-wDQw4B-yPN44qgpVTAJEHtpCM5-iiJaM9lfnb5_7Zd8nfXsrxYspAb99y1l5B83q8oi0VeDOfkMISyWSH0gRUow/s1200/Fig%207.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;800&quot; data-original-width=&quot;1200&quot; height=&quot;426&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhx-K7ak-o3iJmHJA_4rX6nToJiMNOofOy4K6bYP-cUIJJ9Ko8fmL2ZbnZAUYp83D0tGtanilw1Y_qD4c7x4WxVPLPINwei0KGius-wDQw4B-yPN44qgpVTAJEHtpCM5-iiJaM9lfnb5_7Zd8nfXsrxYspAb99y1l5B83q8oi0VeDOfkMISyWSH0gRUow/w640-h426/Fig%207.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot;&gt;Figure 7. Degree of network effects and effect size estimation. The dotted line with “A” refers to the true average effect.&lt;br /&gt;&lt;br /&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h2&gt;Dynamic evolution of user collaboration network&lt;/h2&gt;An actual user collaboration network is not static and evolves over time as users start new projects, finish existing ones, or change their project memberships. As a result, the following four changes can happen to components:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Create&lt;/b&gt;: a new component is created.&lt;/li&gt;&lt;li&gt;&lt;b&gt;Split&lt;/b&gt;: an existing component breaks into sub-components.&lt;/li&gt;&lt;li&gt;&lt;b&gt;Remove&lt;/b&gt;: a component no longer exists.&lt;/li&gt;&lt;li&gt;&lt;b&gt;Merge&lt;/b&gt;: existing components become connected.&amp;nbsp;&lt;/li&gt;&lt;/ul&gt;The first three cases are easy: a newly created component is just randomly assigned to an arm, while no action needs to be taken for splits and removal. Difficulty only arises when connected components merge as shown in Figure 8.&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-xCgp_24E_H4/WlwFHbY-fhI/AAAAAAAAcqg/sneWUIX1Tok-FQaWPdASgoFq_FNdHnIaACLcBGAs/s1600/sangho1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;435&quot; data-original-width=&quot;737&quot; height=&quot;376&quot; src=&quot;https://1.bp.blogspot.com/-xCgp_24E_H4/WlwFHbY-fhI/AAAAAAAAcqg/sneWUIX1Tok-FQaWPdASgoFq_FNdHnIaACLcBGAs/s640/sangho1.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot;&gt;Figure 8. Addition and Merge: new entities (projects and users) added are in RED, and components merged are in BLUE. &lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;div&gt;&lt;br /&gt;At this point, it is no longer possible to guarantee consistent user treatment as defined earlier. We may discuss nuances of graph evolution in a future post, but for the most part, we are fortunate that this is a relatively rare event in our collaboration network today. Another aspect of concern is that treatment can in theory affect not just experiment metrics but also the graph topology itself. Thus graph evolution events also need to be tracked over the course of the experiment.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h2&gt;Conclusion&lt;/h2&gt;Designing randomized experiments on a network of users is more challenging because of network effects. It is often inappropriate to ignore these network effects, either because it results in poor (inconsistent) user experience or because doing so incurs bias in estimating the effect of treatment. Our simulation results based on the actual GCP user network demonstrate the potential for bias if the structure of the network is not considered when designing experiments.&lt;br /&gt;&lt;br /&gt;&lt;div&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content><link rel='replies' type='application/atom+xml' href='https://www.unofficialgoogledatascience.com/feeds/8391093797732166139/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='https://www.unofficialgoogledatascience.com/2018/01/designing-ab-tests-in-collaboration.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/8391093797732166139'/><link rel='self' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/8391093797732166139'/><link rel='alternate' type='text/html' href='https://www.unofficialgoogledatascience.com/2018/01/designing-ab-tests-in-collaboration.html' title='Designing A/B tests in a collaboration network'/><author><name>Amir Najmi</name><uri>http://www.blogger.com/profile/18174523203317227640</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://3.bp.blogspot.com/-QfwFNR7tU5k/Wl6z1SAwnGI/AAAAAAAAcrU/fFOqv08cOv8eWSoNYlkGUSgOS724pVm5ACLcBGAs/s72-c/sangho2.png" height="72" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-5825758052688213474.post-4089241891103651494</id><published>2017-10-12T18:26:00.002-07:00</published><updated>2017-10-13T09:55:50.929-07:00</updated><title type='text'>Unintentional data</title><content type='html'>by ERIC HOLLINGSWORTH&lt;br /&gt;&lt;i&gt;&lt;br /&gt;A large part of the data we data scientists are asked to analyze was not collected with any specific analysis in mind, or perhaps any particular purpose at all. This post describes the analytical issues which arise in such a setting, and what the data scientist can do about them.&lt;/i&gt;&lt;br /&gt;&lt;h2&gt;A landscape of promise and peril&lt;/h2&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;br /&gt;&lt;/div&gt;The data scientist working today lives in what Brad Efron has termed the &quot;era of scientific mass production,&quot; of which he remarks, &quot;But now the flood of data is accompanied by a deluge of questions, perhaps thousands of estimates or hypothesis tests that the statistician is charged with answering together; not at all what the classical masters had in mind. [1]&quot;&lt;br /&gt;&lt;br /&gt;Statistics, as a discipline, was largely developed in a small data world. Data was expensive to gather, and therefore decisions to collect data were generally well-considered. Implicitly, there was a prior belief about some interesting causal mechanism or an underlying hypothesis motivating the collection of the data. As computing and storage have made data collection cheaper and easier, we now gather data without this underlying motivation. There is no longer always intentionality behind the act of data collection — data are not collected in response to a hypothesis about the world, but for the same reason George Mallory climbed Everest: because it’s there.&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-VkZ6R1cwQV4/Wd_lN8T9XEI/AAAAAAAAcfk/XZ8HbK0ViYwknagXf0ZU7RrBfL2fhNd8QCEwYBhgL/s1600/1024px-Himalayas.jpg&quot; imageanchor=&quot;1&quot; style=&quot;clear: left; margin-bottom: 1em; margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;675&quot; data-original-width=&quot;1024&quot; height=&quot;262&quot; src=&quot;https://3.bp.blogspot.com/-VkZ6R1cwQV4/Wd_lN8T9XEI/AAAAAAAAcfk/XZ8HbK0ViYwknagXf0ZU7RrBfL2fhNd8QCEwYBhgL/s400/1024px-Himalayas.jpg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot;&gt;&lt;span style=&quot;font-size: 12.8px;&quot;&gt;Fig. 1: The Himalaya mountain range&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;Much of the data available to us in big data settings is large not only in the number of observations, but also in the number of features. With more features come more potential post hoc hypotheses about what is driving metrics of interest, and more opportunity for exploratory analysis. Operating successfully as a data scientist in industry in such an environment is not only a matter of mathematics, but also one of intuition and discretion. Understanding the goals of the organization as well as guiding principles for extracting value from data are both critical for success in this environment.&lt;br /&gt;&lt;br /&gt;Thankfully not only have modern data analysis tools made data collection cheap and easy, they have made the process of exploratory data analysis cheaper and easier as well. Yet when we use these tools to explore data and look for anomalies or interesting features, we are implicitly formulating and testing hypotheses after we have observed the outcomes. The ease with which we are now able to collect and explore data makes it very difficult to put into practice even basic concepts of data analysis that we have learned — things such as:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;Correlation does not imply causation.&lt;/li&gt;&lt;li&gt;When we segment our data into subpopulations by characteristics of interest, members are not randomly assigned (rather, they are chosen deliberately) and suffer from selection bias.&lt;/li&gt;&lt;li&gt;We must correct for multiple hypothesis tests.&lt;/li&gt;&lt;li&gt;We ought not dredge our data.&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;&lt;span id=&quot;docs-internal-guid-43a7cd67-1194-416c-cd54-d86815150d80&quot;&gt;All of those principles are well known to statisticians, and have been so for many decades. What is newer is just how cheap it is to posit hypotheses. For better and for worse, technology has led to a democratization of data within organizations. More people than ever are using statistical analysis packages and dashboards, explicitly or more often implicitly, to develop and test hypotheses.&lt;br /&gt;&lt;br /&gt;Although these difficulties are more pronounced when we deal with observational data, the proliferation of hypotheses and lack of intentionality in data collection can even impact designed experiments. We data scientists now have access to tools that allow us to run a large numbers of experiments, and then to slice experimental populations by any combination of dimensions collected. This leads to the proliferation of post hoc hypotheses. That is, having observed that Variant A is better than Variant B, we are induced by our tools to slice the data by covariates to try and understand &lt;b&gt;why&lt;/b&gt;, even if we had no a priori hypothesis or proposed causal mechanism.&lt;br /&gt;&lt;br /&gt;Looking at metrics of interest computed over subpopulations of large data sets, then trying to make sense of those differences, is an often recommended practice (even on this very blog). And for good reason! Every data scientist surely has a story of identifying important issues by monitoring metrics on dashboards without having any particular hypothesis about what they are looking for. &lt;b&gt;As data scientists working in big data environments, the question before us is how to explore efficiently and draw inference from data collected without clear intent&amp;nbsp;&lt;/b&gt;&lt;/span&gt;&lt;b&gt;—&amp;nbsp;where there was no prior belief that the data would be relevant or bear on any particular question.&lt;/b&gt; Our challenges are no longer purely analytical in nature: questions of human psychology and organizational dynamics arise in addition to the mathematical challenges in statistical inference.&lt;/div&gt;&lt;br /&gt;John Tukey writes in the introduction to &lt;u&gt;The Future of Data Analysis&lt;/u&gt; that although he had once believed himself to be a statistician, he realized that his real interest was in &lt;b&gt;data analysis&lt;/b&gt; — which includes “procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier… in addition to all the machinery and results of (mathematical) statistics…” Mathematics can inform us, but it alone can no longer save us, and while mathematics may inform us &lt;b&gt;how&lt;/b&gt; to do something, it cannot inform us &lt;b&gt;what&lt;/b&gt; should be done. Although this post prescribes no formula for being an effective data scientist in the world of unintentional data, it does offer guidance in confronting both the organizational and operational issues the data scientist may encounter.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Avalanche of questions: the role of the data scientist amid unintentional data&lt;/h2&gt;The internet is awash in guides and tutorials for the aspiring data scientist that focus on various facets of statistics, mathematics, programming, or other methodological concerns. Yet it is just as important to have a handle on how to reason about the mountains of observational data that can overwhelm an organization. &lt;br /&gt;&lt;br /&gt;When presented with a “finding” from the world of unintentional data, the data scientist must answer three broad questions:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;&lt;i&gt;Is it relevant to our goals?&lt;/i&gt;&lt;/b&gt; Is the effect we’ve discovered related to a topic that is of interest to the organization? Is the effect size large enough to be meaningful to the organization? In the world of big, unintentional data there are many discoveries to be had which have no bearing on the organization’s goals. Understanding what is relevant to the organization is critical to managing the deluge of questions that could be asked of all the data we now collect.&amp;nbsp;&lt;/li&gt;&lt;li&gt;&lt;b&gt;&lt;i&gt;Is it actionable?&lt;/i&gt;&lt;/b&gt; If the question is material to the organization, are there any decisions that can be made given what has been discovered? Understanding how the organization operates and what avenues are available to respond is critical in choosing how to investigate a pile of unintentional data. The data scientist in industry needs not only a way to attack the analysis problem, but a way also to attack the business problem on which their analysis may shed light.&lt;/li&gt;&lt;li&gt;&lt;b&gt;&lt;i&gt;Is it real?&lt;/i&gt;&lt;/b&gt; Is the effect being observed the result of some causal process as opposed to the kind of random variation in user or system behavior expected in steady state? This question is statistical or methodological in nature.&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;&lt;span id=&quot;docs-internal-guid-43a7cd67-11bc-ba45-0aa1-63c10d7c7a30&quot;&gt;What can the data scientist do to answer these questions?&lt;br /&gt;&lt;br /&gt;&lt;b&gt;&lt;i&gt;Know what matters.&lt;/i&gt;&lt;/b&gt; If hypothesis generation is cheap, the data scientist will soon be inundated with hypotheses from across the organization: theories to evaluate, urgent emails in the middle of the night to explain every wiggle on a time series dashboard. Only by knowing the broader goals of the organization, what data the organization has that can speak to those goals, and what analyses have been impactful in the past can the data scientist guide the organization to focus on meaningful questions.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;&lt;i&gt;Know your data.&lt;/i&gt;&lt;/b&gt; Without understanding how and why the data are generated and collected, it is impossible to have any reliable intuition about whether the result of an analysis makes sense, or whether a given hypothesis is any good. Be sure to have a deep, thorough understanding of how data under consideration was collected and what it actually means. Check any assumptions up front — nothing is worse than completing an analysis based on a faulty assumption about what a piece of data means. &lt;br /&gt;&lt;br /&gt;&lt;b&gt;&lt;i&gt;Make experimentation cheap and understand the cost of bad decisions.&lt;/i&gt;&lt;/b&gt; The gold standard of evidence is the randomized controlled experiment. If we are to succeed in a world where hypothesis generation is cheap, we must develop or acquire infrastructure and processes to ensure that it is also cheap to test them. It is part of the data scientist&#39;s role to advocate for rapid experimentation and to educate those who use it. Still, if there are too many hypotheses to test with experiments, we should only be testing those that lead to consequential decisions. We need to know whether the decision we are optimizing is significant enough to justify the time and experiment resources spent optimizing it. &lt;br /&gt;&lt;br /&gt;&lt;i&gt;&lt;b&gt;Be skeptical, intellectually honest.&lt;/b&gt;&lt;/i&gt; When an appealing conclusion presents itself, be skeptical and thorough in considering any issue with the data that may lead to incorrect conclusions. In addition to the issues above, does the conclusion pass the smell test? Is the data presented consistent with other data that you have seen?&amp;nbsp;&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;Because the issues with observational data are subtle and easily missed or ignored, the temptation to make up just-so stories to explain the observed world is sometimes overwhelming. The engineer who carelessly fails to write tests and breaks the build is caught immediately, but the data scientist who carelessly fails to conduct a thorough analysis and comes to false conclusions may not be discovered quickly, and may by then have caused significant damage to the organization and to their own reputation.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Democratization of analysis: quantity has a quality all its own&lt;/h2&gt;Just as dealing with unintentional data shapes the role of the data scientists in their organization, it also shapes the day to day practice of data analysis. We now describe some of the chief issues encountered in analysis (and especially exploratory analysis) of unintentional data.&lt;span id=&quot;docs-internal-guid-43a7cd67-11c2-4d4c-31b6-a4416d1eef1f&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;h4&gt;Selection bias and survivorship bias&lt;/h4&gt;When we slice a population by some factor or combination of factors and compute a metric over the new sub-populations, we are implicitly specifying a model (the metric is a function of the slicing factors) and testing a hypothesis. But this test is subject to &quot;selection bias&quot;, which occurs when we analyze a group that was not randomly selected. Selection bias played a notable role in the discussion of the avian influenza outbreak of 2011 during which the reported case fatality rate was as high as 80% [2]. However, the World Health Organization criteria for defining a &quot;confirmed case&quot; of avian influenza were very strict and meant that only very sick individuals were counted. Because only very sick individuals were counted, the case mortality rate was quite high (since individuals who were less sick and more likely to recover were never counted as cases at all.) Nevertheless, these estimates caused considerable fear about the ramifications of the outbreak.&lt;br /&gt;&lt;br /&gt;A related issue can occur when looking at time series data. Suppose a data scientist works at The Hill Climber, a climbing shop in the Himalayas. She wants to evaluate the performance of the Ice Axe 2000. To do this, she evaluates the condition of Ice Axes brought in for sharpening. She finds these axes to be in very good shape after controlling for age, and concludes that the Ice Axe 2000 can be recommended on the basis of its durability. She is dismayed next season at a wave of customer complaints over axes that cracked on their first use. The cause is survivorship bias, which has happened because only the most durable Ice Axes survive into old age and return to the shop for maintenance. In general, survivorship bias happens in longitudinal studies when we cannot track all members of the group through the entire period of interest.&lt;br /&gt;&lt;br /&gt;More generally, when we look at slices of data longitudinally, the individuals comprising those groups may vary over time and the distribution of characteristics of those individuals may also change. Formally, we may say that the joint distribution of the slicing variable and other variables that are correlated with the metric of interest is non-stationary.&lt;/div&gt;&lt;h4&gt;Multiple hypothesis testing and invalid inferences&lt;/h4&gt;In an effort to improve sales, suppose our data scientist slices purchase rates at the Himalayan climbing store by all manner of data available about the customers: what other mountains they have climbed, how large their climbing teams are, whether they have already made a purchase at the store, and so on, before discovering that climbers from several countries look especially interesting because of their unusually high or low conversion rates. Assuming customers from 15 different countries are represented in the data set, how large is the family of hypotheses under test? Correspondingly, how much should the p-values of each individual test be corrected? The natural answer is 15, but what about all of those other hypotheses that were rejected during the hypothesis generation phase? What about all of those hypotheses that will be tested at the end of the next quarter? What is the likelihood of a given rank ordering for arbitrary comparisons between countries?&lt;br /&gt;&lt;br /&gt;As Andrew Gelman and Eric Loken point out in their essay &quot;&lt;a href=&quot;http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf&quot;&gt;The garden of the forking paths&lt;/a&gt;&quot;, invalid inferences can occur when the opportunity to test multiple hypotheses merely exists, even if multiple tests are not actually carried out. Exercising these “researcher degrees of freedom” by choosing not to to carry out a hypothesis test upon seeing the data may not feel like a fishing expedition, but will lead to invalid inferences all the same. Of course, exploratory analysis of big unintentional data puts us squarely at risk for these types of mistakes.&lt;br /&gt;&lt;h4&gt;Regression to the mean&lt;/h4&gt;If enough slices are examined, there will certainly be some slices with extreme values on metrics of interest. But this does not mean that the slice will continue to exhibit an extreme value on this measurement in the future. This is closely related to the issues with multiple hypothesis testing — given enough observations, we expect to find some extreme values.&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h2&gt;What is to be done?&lt;/h2&gt;&lt;div&gt;&lt;h3&gt;Natural experiments, counterfactuals, synthetic controls.&lt;/h3&gt;Back at The Hill Climber, our data scientist wants to understand the impact of a more stringent permitting process on the number of climbers attempting Everest, in order to better understand potential future sales.&lt;br /&gt;&lt;br /&gt;The simplest approach is to compare data from the month preceding to the month following the change. Unfortunately this approach does nothing to control for trends or seasonality in the number of climbers nor typical year-to-year variation, and attributes all differences to the intervention (the new permitting process).&lt;br /&gt;&lt;span id=&quot;docs-internal-guid-43a7cd67-11c9-229f-ffa9-1d73504f38cb&quot;&gt;&lt;br /&gt;A slightly more sophisticated approach is to compare year-over-year Everest climbing data with the change in the number of climbers of K2 over the same time period. This may give the data scientist confidence that the change is attributable to the policy, rather than some global trend of interest in mountain climbing. This is the differences-in-differences approach, where we estimate the impact of an intervention by comparing the change in metric before and after the intervention in a group receiving that intervention with the change in a group that did not receive the intervention. This requires that both groups satisfy the parallel trends assumption, which states the groups must have similar additive trends prior to the intervention. The parallel trends assumption is most likely to be true if we have a natural experiment, that is, we believe the intervention (in this case, the permitting process) happened essentially at random to some subjects (Everest) and not others (K2).&lt;br /&gt;&lt;br /&gt;Alternatively, sometimes we can estimate what the climbing data for Everest would have been, under a counterfactual scenario to the intervention. To do this, our data scientist may use a weighted combination of climbing time series data from several different mountains into a synthetic control, where the weighted combination is designed such that the characteristics of the synthetic control match the characteristics of the Himalayas before the intervention.&lt;br /&gt;&lt;br /&gt;The process of constructing synthetic controls is made easy by the &lt;a href=&quot;https://google.github.io/CausalImpact/CausalImpact.html&quot;&gt;CausalImpact package in R&lt;/a&gt;, which uses &lt;a href=&quot;http://www.unofficialgoogledatascience.com/2017/07/fitting-bayesian-structural-time-series.html&quot;&gt;Bayesian structural time series&lt;/a&gt; to build a forecast based on a time series from a population that received the intervention with one that did not. In this way, it can be thought of as a more rigorous differences-in-differences approach.&lt;/span&gt;&lt;br /&gt;&lt;h3&gt;Develop a retrospective (better yet, prospective) cohort study, or a case-control study&lt;/h3&gt;&lt;div&gt;Tracking an identified population of individuals longitudinally can help avoid bias caused by changes in the composition of the group, and careful selection of those users can help avoid selection bias inherent in observational data. Although there are subtle differences in the design and interpretation of case-control and prospective or retrospective cohort studies, the general concept is the same. That is, by selecting groups of substantially similar users and tracking their outcomes, we can have greater confidence in our causal conclusion, even if we haven’t perfectly controlled for bias via proper randomization.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-WSMLhBm26p4/WeASyLY6hcI/AAAAAAAAcgA/YOjSbQILNNw7EoFozh423mYqdwJb-r74gCLcBGAs/s1600/faces.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;684&quot; data-original-width=&quot;1062&quot; height=&quot;256&quot; src=&quot;https://3.bp.blogspot.com/-WSMLhBm26p4/WeASyLY6hcI/AAAAAAAAcgA/YOjSbQILNNw7EoFozh423mYqdwJb-r74gCLcBGAs/s400/faces.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot;&gt;Fig 2: Features (smile color, hat wearing) are distributed differently in these two groups which could confound analysis. We can correct for this with techniques such as propensity matching, or observational experimental designs.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;h4&gt;Multiple Hypothesis Testing&lt;/h4&gt;The standard advice in multiple hypothesis testing settings (such as using a Bonferroni correction) typically involves using some method to correct (that is, inflate) p-values to account for the additional hypothesis tests that were performed. But these approaches are typically focused on controlling the &lt;b&gt;family-wise&lt;/b&gt; Type I error rates. Such procedures ensure that for a set of hypotheses being tested, the probability of getting any statistically significant results under the null should be no greater than some 𝛼. &lt;br /&gt;&lt;br /&gt;In the unintentional data setting is it is nearly impossible to define the ‘family’ for which we are controlling the familywise error rate. How do we know when two studies are sufficiently similar that they require correction for multiple testing? And given all the exploratory data analysis that has gone before, it may be hard to say how many hypotheses have been implicitly tested — how far down Gelman’s forking path have we already walked before we started the mathematically formal portion of the analysis?&lt;br /&gt;&lt;br /&gt;Controlling the Type I error necessarily comes at the expense of increasing the risk of a Type II error. In some settings, this is prudent, but without the data scientist understanding the loss function for a specific context it’s not clear that having such a high aversion to falsely rejecting the null is more costly than failing to reject the null when it is indeed false. (Again, understanding the organization and the decisions to be made is critical to producing a useful analysis.)&lt;br /&gt;&lt;b&gt;&lt;i&gt;&lt;br /&gt;Consider your loss function.&lt;/i&gt;&lt;/b&gt; How expensive is it to confirm experimentally that the null hypothesis can be rejected, and how expensive would it be if that path leads down turned out to be wrong? By contrast, how valuable are the opportunities that will be passed up if we fail to reject a truly false null?&lt;br /&gt;&lt;b&gt;&lt;i&gt;&lt;br /&gt;Control the false discovery rate.&lt;/i&gt;&lt;/b&gt; When we have many hypotheses to evaluate, it may be more important that we identify a subset that are mostly true, rather than insisting on high certainty that each hypothesis is true. This is the essence of false discovery rate procedures. Controlling the false discovery rate means that when we identify a set of &quot;discoveries&quot; (rejected nulls), no more than a specified fraction of them will be false.&lt;br /&gt;&lt;h4&gt;Regression to the mean&lt;/h4&gt;&lt;/div&gt;&lt;div&gt;Start with a hypothesis that follows from a causal mechanism. Regression to the mean occurs when we find a subpopulation that has an unusually high or low value for some metric of interest due to pure chance. We can avoid being fooled into bad assumptions about causality if we begin by exploring slices for which we have a credible causal mechanism that could be driving the differences between groups.&lt;br /&gt;&lt;span id=&quot;docs-internal-guid-43a7cd67-11db-5539-6d76-93209d54ead2&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;h2&gt;A Bradford Hill to die on&lt;/h2&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://2.bp.blogspot.com/-5xhqiJIVGGo/WeAQc93ZtAI/AAAAAAAAcf0/vM2da2hW3K0wIQkqIZheclQjPVrt48xTQCLcBGAs/s1600/mountain-goats-at-the-top.jpg&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1200&quot; data-original-width=&quot;1600&quot; height=&quot;300&quot; src=&quot;https://2.bp.blogspot.com/-5xhqiJIVGGo/WeAQc93ZtAI/AAAAAAAAcf0/vM2da2hW3K0wIQkqIZheclQjPVrt48xTQCLcBGAs/s400/mountain-goats-at-the-top.jpg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot;&gt;&lt;br /&gt;Fig 3: Pictured: Goats on a hill. (Not Bradford.)&lt;br /&gt;&lt;br /&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;Sir Austin Bradford Hill was an English epidemiologist and statistician credited both with running the first randomized clinical trial and with landmark research into the association between lung cancer and smoking (beginning with a case-control study conducted with Richard Doll and published in 1950). However, he is perhaps best remembered today for the “Bradford Hill criteria” [3], nine “aids to thought” [4] published in 1965 that can help an epidemiologist determine whether some factor is a cause (not &lt;b&gt;the&lt;/b&gt; cause) for a disease. The modern data scientist and the epidemiologist have plenty in common, as they are both often tasked with explaining the world around them without the benefit of experimental evidence. We data scientists could do much worse than to learn from Bradford Hill.&lt;br /&gt;&lt;br /&gt;First, one of the defining pieces of work in Hill’s storied career began with a case-control study. We have already emphasized cohort and case-control studies as a technique for facilitating inferences from unintentional data. The conceptual motivation behind it is that although we only have observational data, if we take care in what we observe and compare we may still be able to make causal claims. By keeping in mind the need to control for bias in observational data and the strategies employed in the design of observational trials to do so, we can check our own thinking during an exploratory analysis of observational data.&lt;br /&gt;&lt;br /&gt;Next, consider the Bradford Hill criteria used to evaluate whether observational data may serve as evidence of causality. In the world of the data scientist, this amounts to understanding whether variation in the slices of data that we are looking at is caused by the factor we used to create the slices, or merely correlated with it.&lt;br /&gt;&lt;br /&gt;Of the nine points Hill gives, four are especially relevant to unintentional data:&lt;br /&gt;&lt;b&gt;&lt;i&gt;&lt;br /&gt;Strength of effect&lt;/i&gt;&lt;/b&gt;. A larger association is more likely causal. This principle is particularly important for data exploration in the world of unintentional data, where we may have no proposed causal mechanism and consequently should have strong prior belief that there is no effect. The standard of evidence required to overcome this prior is high indeed. Any attempt at causal inference assumes that we have properly accounted for confounders, which can never be done perfectly. Small effects could be explained by flaws in this assumption, but large effects are much harder to explain away.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;&lt;i&gt;Consistency&lt;/i&gt;&lt;/b&gt;. Can we observe the effect in different places, under different circumstances or at different times? We have previously discussed the risk of confounding factors obscuring important real effects. By making sure that an effect is consistent across various sub-populations and time we can increase our confidence that we have found a real causal relationship.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;&lt;i&gt;Biological gradient (dose response)&lt;/i&gt;&lt;/b&gt;. Subjects in slices that see greater exposure to the proposed causal mechanism should (probably) exhibit a greater response. If we posit that discounts on climbing gear lead to more purchases, our data scientist should expect that climbers with coupons for 50% off will purchase more gear than those for coupons for 15% off. &lt;br /&gt;&lt;br /&gt;&lt;b&gt;&lt;i&gt;Temporality&lt;/i&gt;&lt;/b&gt;. The effect must follow the cause. As Richard Doll says, this is the only of the nine that is sine qua non for identifying causality. Are we sure those customers with 50% off coupons hadn’t started purchasing more climbing gear before our data scientist mailed the coupons?&lt;br /&gt;&lt;br /&gt;The nine guideposts of causality are the best remembered parts of Hill’s 1965 paper, but equally important is its final stanza in which Hill lays out ”The Case For Action” [5]. He argues that determinations of causality should not be made independently of the decisions that will be made based on those determinations. Although there is no scientific justification for this, Hill does not forget that as an epidemiologist his goal is to take action, and that the quality of the evidence we require to take some action must be judged in the context of the potential costs and harms of the decision to be made. In this regard, Hill’s situation was remarkably similar to the modern data scientist working in industry.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Conclusion&lt;/h2&gt;As we try to summit the challenges of living in a world with a mountain of observational data and a blizzard of hypotheses, the concepts above can help us turn out more like &lt;a href=&quot;https://en.wikipedia.org/wiki/Edmund_Hillary&quot;&gt;Edmund Hillary&lt;/a&gt; than &lt;a href=&quot;https://en.wikipedia.org/wiki/George_Mallory&quot;&gt;George Mallory&lt;/a&gt;. When data are collected without intentionality, we must approach exploratory analysis with a skeptical eye. This state of affairs may not correspond to the world as it existed in the early days of statistics, but as Bradford Hill proves there is a path forward if we maintain focus on the nature of our data and purpose of our analysis. Modern technologies that facilitate rapid experimentation and easy exploratory analysis offer tremendous opportunity for the data scientist who can focus on the key concerns of their organization and find meaningful information in the data.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;br /&gt;&lt;div&gt;&lt;h3&gt;References&lt;/h3&gt;[1] Efron, B. (2010). Large-Scale Inference: Empirical Bayes Methods for Estimation, Testing, and Prediction. New York: Cambridge University Press.&lt;br /&gt;&lt;br /&gt;[2] Palese P, Wang TT. 2012. H5N1 influenza viruses: facts, not fear. PNAS. 109:2211–2213.&lt;br /&gt;&lt;br /&gt;[3] Hill, A. B. (1965). The Environment and Disease: Association or Causation? Proceedings of the Royal Society of Medicine, 58(5), 295–300.&lt;br /&gt;&lt;br /&gt;[4] Doll R.(2002). Proof of Causality: Deduction from Epidemiological Observation. Perspect Biol Med., 45, 499–515. &lt;br /&gt;&lt;br /&gt;[5] Phillips CV, Goodman KJ. The missed lessons of Sir Austin Bradford Hill. Epidemiol Perspect Innov. 2004 Oct 4;1(1):3.&lt;/div&gt;&lt;/div&gt;</content><link rel='replies' type='application/atom+xml' href='https://www.unofficialgoogledatascience.com/feeds/4089241891103651494/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='https://www.unofficialgoogledatascience.com/2017/10/unintentional-data.html#comment-form' title='4 Comments'/><link rel='edit' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/4089241891103651494'/><link rel='self' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/4089241891103651494'/><link rel='alternate' type='text/html' href='https://www.unofficialgoogledatascience.com/2017/10/unintentional-data.html' title='Unintentional data'/><author><name>Amir Najmi</name><uri>http://www.blogger.com/profile/18174523203317227640</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://3.bp.blogspot.com/-VkZ6R1cwQV4/Wd_lN8T9XEI/AAAAAAAAcfk/XZ8HbK0ViYwknagXf0ZU7RrBfL2fhNd8QCEwYBhgL/s72-c/1024px-Himalayas.jpg" height="72" width="72"/><thr:total>4</thr:total></entry><entry><id>tag:blogger.com,1999:blog-5825758052688213474.post-6693193164123748208</id><published>2017-07-11T13:55:00.000-07:00</published><updated>2017-07-12T04:27:55.976-07:00</updated><title type='text'>Fitting Bayesian structural time series with the bsts R package</title><content type='html'>by STEVEN L. SCOTT&lt;br /&gt;&lt;br /&gt;&lt;i&gt;Time series data are everywhere, but time series modeling is a fairly specialized area within statistics and data science.  This post describes the bsts software package, which makes it easy to fit some fairly sophisticated time series models with just a few lines of R code.&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Introduction&lt;/h2&gt;Time series data appear in a surprising number of applications, ranging from business, to the physical and social sciences, to health, medicine, and engineering. Forecasting (e.g. next month&#39;s sales) is common in problems involving time series data, but explanatory models (e.g. finding drivers of sales) are also important. Time series data are having something of a moment in the tech blogs right now, with Facebook announcing their &quot;Prophet&quot; system for time series forecasting (Taylor and Letham 2017), and Google posting about its forecasting system in this blog (Tassone and Rohani 2017).&lt;br /&gt;&lt;br /&gt;This post summarizes the &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;bsts&lt;/span&gt; R package, a tool for fitting Bayesian structural time series models. These are a widely useful class of time series models, known in various literatures as &quot;structural time series,&quot; &quot;state space models,&quot; &quot;Kalman filter models,&quot; and &quot;dynamic linear models,&quot; among others. Though the models need not be fit using Bayesian methods, they have a Bayesian flavor and the &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;bsts&lt;/span&gt; package was built to use Bayesian posterior sampling.&lt;br /&gt;&lt;br /&gt;The &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;bsts&lt;/span&gt; package is open source. You can download it from CRAN with the R command &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;install.packages(&quot;bsts&quot;)&lt;/span&gt;. It shares some features with Facebook and Google systems, but it was written with different goals in mind. The other systems were written to do &quot;forecasting at scale,&quot; a phrase that means something different in time series problems than in other corners of data science. The Google and Facebook systems focus on forecasting daily data into the distant future. The &quot;scale&quot; in question comes from having many time series to forecast, not from any particular time series being extraordinarily long. The bottleneck in both cases is the lack of analyst attention, so the systems aim to automate analysis as much as possible. The Facebook system accomplishes this using regularized regression, while the Google system works by averaging a large ensemble of forecasts. Both systems focus on daily data, and derive much of their efficiency through the careful treatment of holidays.&lt;br /&gt;&lt;br /&gt;There are aspects of &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;bsts&lt;/span&gt; which can be similarly automated, and a specifically configured version of bsts is a powerful member of the Google ensemble. However, &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;bsts&lt;/span&gt; can also be configured for specific tasks by an analyst who knows whether the goal is short term or long term forecasting, whether or not the data are likely to contain one or more seasonal effects, and whether the goal is actually to fit an explanatory model, and not primarily to do forecasting at all.&lt;br /&gt;&lt;br /&gt;The workhorse behind &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;bsts&lt;/span&gt; is the structural time series model.  These models are briefly described in the section &lt;b&gt;Structural time series models&lt;/b&gt;.  Then the software is introduced through a series of extended examples that focus on a few of the more advanced features of &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;bsts&lt;/span&gt;. &lt;b&gt;Example 1: Nowcasting&lt;/b&gt; includes descriptions of the local linear trend and seasonal state models, as well as spike and slab priors for regressions with large numbers of predictors.&lt;b&gt;  Example 2: Long term forecasting &lt;/b&gt;describes a situation where the local level and local linear trend models would be inappropriate.  It offers a semilocal linear trend model as an alternative. &lt;b&gt;Example 3: Recession modeling&lt;/b&gt; describes an model where the response variable is non-Gaussian.  The goal in Example 3 is not to predict the future, but to control for serial dependence in an explanatory model that seeks to identify relevant predictor variables.  A final section concludes with a discussion of other features in the package which we won&#39;t have space (maybe &quot;time&quot; is a better word) to explore with fully fleshed out examples. &lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Structural time series models&lt;/h2&gt;A structural time series model is defined by two equations.  The &lt;i&gt;observation equation&lt;/i&gt; relates the observed data $y_t$ to a vector of latent variables $\alpha_t$ known as the &quot;state.&quot;&lt;br /&gt;$$  y_t = Z_t^T\alpha_t + \epsilon_t.$$&lt;br /&gt;The &lt;i&gt;transition equation&lt;/i&gt; describes how the latent state evolves through time.&lt;br /&gt;$$  \alpha_{t+1} = T_t \alpha_t + R_t \eta_t.$$&lt;br /&gt;The error terms $\epsilon_t$ and $\eta_t$ are Gaussian and independent of everything else.  The arrays $Z_t$, $T_t$ and $R_t$ are &lt;i&gt;structural parameters.&lt;/i&gt;  They may contain parameters in the statistical sense, but often they simply contain strategically placed 0&#39;s and 1&#39;s indicating which bits of $\alpha_t$ are relevant for a particular computation.  An example will hopefully make things clearer.&lt;br /&gt;&lt;br /&gt;The simplest useful model is the &quot;local level model,&quot; in which the vector $\alpha_t$ is just a scalar $\mu_t$.  The local level model is a random walk observed in noise.&lt;br /&gt;$$    y_t = \mu_t + \epsilon_t $$ $$    \mu_{t+1} = \mu_t + \eta_t. $$&lt;br /&gt;Here $\alpha_t = \mu_t$, and $Z_t$, $T_t$, and $R_t$ all collapse to the scalar value 1.  Similar to Bayesian hierarchical models for nested data, the local level model is a compromise between two extremes.  The compromise is determined by variances of $\epsilon_t \sim N(0, \sigma^2)$ and $\eta_t \sim N(0,  \tau^2)$.  If $\tau^2 = 0$ then $\mu_t$ is a constant, so the data are IID Gaussian noise. In that case the best estimator of $y_{t+1}$ is the mean of $y_1, \dots, y_t$. Conversely, if $\sigma^2 = 0$ then the data follow a random walk, in which case the best estimator of $y_{t+1}$ is $y_t$.  Notice that in one case the estimator depends on all past data (weighted equally) while in the other it depends only on the most recent data point, giving past data zero weight.  If both variances are positive then the optimal estimator of $y_{t+1}$ winds up being &quot;exponential smoothing,&quot; where past data are forgotten at an exponential rate determined by the ratio of the two variances.  Also notice that while the state in this model is Markov (i.e. it only depends on the previous state), the dependence among the observed data extends to the beginning of the series.&lt;br /&gt;&lt;br /&gt;Structural time series models are useful because they are flexible and modular. The analyst chooses the structure of $\alpha_t$ based on things like whether short or long term predictions are more important, whether the data contains seasonal effects, and whether and how regressors are to be included.  Many of these models are standard, and can be fit using a variety of tools, such as the &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;StructTS&lt;/span&gt; function distributed with base R or one of several R packages for fitting these models (with the &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;dlm&lt;/span&gt; package (Petris 2010, Petris, Petrone, and Campagnoli 2009) deserving special mention).  The &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;bsts&lt;/span&gt; package handles all the standard cases, but it also includes several useful extensions, described in the next few sections through a series of examples. Each example includes a mathematical description of the model and example &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;bsts&lt;/span&gt; code showing how to work with the model using the &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;bsts&lt;/span&gt; software.  To keep things short, details about prior assumptions are largely avoided.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Example 1: Nowcasting&lt;/h2&gt;Scott and Varian (2014, 2015) used structural time series models to show how Google search data can be used to improve short term forecasts (&quot;nowcasts&quot;) of economic time series.  Figure 1 shows the motivating data set from Scott and Varian (2014), which is also included with the &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;bsts&lt;/span&gt; package.  The data consist of the weekly initial claims for unemployment insurance in the US, as reported by the US Federal Reserve.  Like many official statistics they are released with delay and subject to revision. At the end of the week, the economic activity determining these numbers has taken place, but the official numbers are not published until several days later.  For economic decisions based on these and similar numbers, it would help to have an early forecast of the current week&#39;s number as of the close of the week.  Thus the output of this analysis is truly a &quot;nowcast&quot; of data that has already happened rather than a &quot;forecast&quot; of data that will happen in the future.&lt;br /&gt;&lt;br /&gt;&lt;div style=&quot;text-align: center;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;img alt=&quot;initial-claims-nsa-data.png&quot; height=&quot;504&quot; src=&quot;https://lh5.googleusercontent.com/e9pS3At33PjZ4yO5JID4JEv6bzIsdEs_XhOO2b_Cv4WrKKaZ2OrCqaoRxCLFEHJD3U-zt5X7MB-x7BNukQKPFzuYsQJYQk9wgFO52YA_2yDvgsy8iKaSPhF0d7FRKkiDa6ydj-H0&quot; style=&quot;border: none; transform: rotate(0rad);&quot; width=&quot;504&quot; /&gt;&lt;/span&gt;&lt;/div&gt;&lt;b&gt;Figure 1:&lt;/b&gt;  Weekly initial claims for unemployment in the US.&lt;b style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;There are two sources of information about the current value $y_t$ in the initial claims series: past values $y_{t - \tau}$ describing the time series behavior of the series, and contemporaneous predictors ${\bf x}_t$ from a data source which is correlated with $y_t$, but which is available without the delay exhibited by $y_t$.  The time series structure shows an obvious trend (in which the financial and housing crises in 2008 - 2009 are apparent) as well as a strong annual seasonal pattern.  The external data source explored by Scott and Varian was search data from Google trends with search queries such as &quot;how to file for unemployment&quot; having obvious relevance. &lt;br /&gt;&lt;h3&gt;State components&lt;/h3&gt;&lt;div&gt;Scott and Varian modeled the data in Figure 1 using a structural time series with three state components: a trend $\mu_t$, a seasonal pattern $\tau_t$ and a regression component $\beta^T{\bf x}_t$.  The model is&lt;br /&gt;$$      y_t = \mu_t + \tau_t + \beta^T{\bf x}_t + \epsilon_t $$ $$    \mu_{t+1} = \mu_t + \delta_t + \eta_{0t} $$ $$    \delta_{t+1} = \delta_t + \eta_{1t} $$ $$    \tau_{t+1} = -\sum_{s = 1}^{S-1}\tau_{t} + \eta_{2t}.$$ &lt;br /&gt;The trend component looks similar to the local level model above, but it has an extra term $\delta_t$.  Notice that $\delta_t$ is the amount of extra $\mu$ you can expect as $t\rightarrow t+1$, so it can be interpreted as the slope of the local linear trend.  Slopes normally multiply some $x$ variable, but in this case $x = \Delta t$, which omitted from the equation because it is always 1. The slope evolves according to a random walk, which makes the trend an &lt;i&gt;integrated&lt;/i&gt; random walk with an extra drift term.  The local linear trend is a better model than the local level model if you think the time series is trending in a particular direction and you want future forecasts to reflect a continued increase (or decrease) seen in recent observations.  Whereas the local level model bases forecasts around the average value of recent observations, the local linear trend model adds in recent upward or downward slopes as well.  As with most statistical models, the extra flexibility comes at the price of extra volatility.&lt;br /&gt;&lt;br /&gt;The best way to understand the seasonal component $\tau_t$ is in terms of a regression with seasonal dummy variables.  Suppose you had quarterly data, so that $S = 4$.  You might include the annual seasonal cycle using 3 dummy variables, with one left out as a baseline.  Alternatively, you could include all four dummy variables but constrain their coefficients to sum to zero.  The seasonal state model takes the latter approach, but the constraint is that the $S$ most recent seasonal effects must sum to zero in expectation.  This allows the seasonal pattern to slowly evolve.  Scott and Varian described the annual cycle in the weekly initial claims data using a seasonal state component with $S = 52$.  Of course weeks don&#39;t neatly divide years, but given the small number of years for which Google data are available the occasional one-period seasonal discontinuity was deemed unimportant.&lt;br /&gt;&lt;br /&gt;Let&#39;s ignore the regression component for now and fit a &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;bsts&lt;/span&gt; model with just the trend and seasonal components.&lt;b style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;library(bsts)     &lt;/span&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;# load the bsts package&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;data(iclaims)     &lt;/span&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;# bring the initial.claims data into scope&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br /&gt;&lt;/span&gt;  &lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;ss &amp;lt;- AddLocalLinearTrend(list(), initial.claims$iclaimsNSA)&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;ss &amp;lt;- AddSeasonal(ss, initial.claims$iclaimsNSA, nseasons = 52)&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;model1 &amp;lt;- bsts(initial.claims$iclaimsNSA,&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;background-color: transparent; color: black; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline;&quot;&gt;&lt;/span&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;               state.specification = ss,&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;               niter = 1000)&lt;/span&gt;&lt;b style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;The first thing to do when fitting a &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;bsts&lt;/span&gt; model is to specify the contents of the latent state vector $\alpha_t$.  The &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;bsts&lt;/span&gt; package offers a library of state models, which are included by adding them to a state specification (which is just a list with a particular format).  The call to &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;AddLocalLinearTrend&lt;/span&gt; above adds a local linear trend state component to an empty state specification (the &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;list()&lt;/span&gt; in its first argument).  The call to &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;AddSeasonal&lt;/span&gt; adds a seasonal state component with 52 seasons to the state specification created on the previous line.  The state vector $\alpha_t$ is formed by concatenating the state from each state model.  Similarly, the vector $Z_t$ is formed by concatenating the $Z$ vectors from the two state models, while the matrices $T_t$ and $R_t$ are combined in block-diagonal fashion.&lt;br /&gt;&lt;br /&gt;The state specification is passed as an argument to &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;bsts&lt;/span&gt;, along with the data and the desired number of MCMC iterations.  The model is fit using an MCMC algorithm, which in this example takes about 20 seconds to produce 1000 MCMC iterations.  The returned object is a list (with class attribute &quot;&lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;bsts&lt;/span&gt;&quot;).  You can see its contents by typing &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;names(model1)&lt;/span&gt;.  The first few elements contain the MCMC draws of the model parameters.  Most of the other elements are data structures needed by various S3 methods (&lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;plot&lt;/span&gt;&lt;span style=&quot;font-family: inherit;&quot;&gt;,&lt;/span&gt;&lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt; print&lt;/span&gt;&lt;span style=&quot;font-family: inherit;&quot;&gt;,&lt;/span&gt;&lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt; predict&lt;/span&gt;, etc.) that can be used with the returned object.  MCMC output is stored in vectors (for scalar parameters) or arrays (for vector or matrix parameters) where the first index in the array corresponds to MCMC iteration number, and the remaining indices correspond to dimension of the deviate being drawn.&lt;br /&gt;&lt;br /&gt;Most users won&#39;t need to look inside the returned &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;bsts&lt;/span&gt; object because standard tasks like plotting and prediction are available through familiar S3 methods. For example, there are several plot methods available.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;plot(model)&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;plot(model1, &quot;components&quot;) &amp;nbsp;# plot(model1, &quot;comp&quot;) works too!&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;plot(model1, &quot;help&quot;)&lt;/span&gt;&lt;/div&gt;&lt;b style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;img alt=&quot;iclaims-bsm-model.png&quot; height=&quot;360&quot; src=&quot;https://lh6.googleusercontent.com/ak8QR1ohLsUdl9ts9-gYCwp_J4N1wEbdTT4Zh-liFtoAsOdvAis9Cj9JJJGNO5Ldc1iKQzAfRu-u0PKkrxiNAByMhp60Awvjhj2xE_TQDKXib71qpH5A1vfmO82-fzwrdNiu0czJ&quot; style=&quot;border: none; transform: rotate(0rad);&quot; width=&quot;360&quot; /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;img alt=&quot;iclaims-bsm-components.png&quot; height=&quot;312&quot; src=&quot;https://lh4.googleusercontent.com/hrM-6wrKLfChwMol9sxHOqfMMk2lyOaCgwoFIc6XzV63rTIFVGEItXed2wbeEYvwzMNZ1T3RQgNv4eEs_BekAjq1dvYQGqNH8wLA9ybOz9UUS_mxKk01kvyz--UU2C3z7G-ABzad&quot; style=&quot;border: none; transform: rotate(0rad);&quot; width=&quot;624&quot; /&gt;&lt;/span&gt;&lt;/div&gt;&lt;br /&gt;&lt;b&gt;Figure 2:&lt;/b&gt; (top) Posterior distribution of model state.  Blue circles are actual data points. (bottom) Individual state components.  The plot looks fuzzy because it is showing the marginal posterior distribution at each time point.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;The default &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;plot&lt;/span&gt; method plots the posterior distribution of the conditional mean $Z_t^T\alpha_t$ given the full data ${\bf y} = y_1, \dots, y_T.$ Other plot methods can be accessed by passing a string to the plot function.  For example, to see the contributions of the individual state components, pass the string &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;&quot;components&quot;&lt;/span&gt; as a second argument, as shown above. Figure 2 shows the output of these two plotting functions. You can get a list of all available plots by passing the string &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;&quot;&lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;help&lt;/span&gt;&quot;&lt;/span&gt; as the second argument.&lt;br /&gt;&lt;br /&gt;To predict future values there is a &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;predict&lt;/span&gt; method.  For example, to predict the next 12 time points you would use the following commands.&lt;br /&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;pred1 &amp;lt;- predict(model1, horizon = 12)&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;plot(pred1, plot.original = 156)&lt;/span&gt;&lt;/div&gt;&lt;b style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;br /&gt;The output of &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;predict&lt;/span&gt; is an object of class &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;bsts.prediction&lt;/span&gt;, which has its own &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;plot&lt;/span&gt; method.  The &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;plot.original = 156&lt;/span&gt; argument says to plot the prediction along with the last 156 time points (3 years) of the original series.  The results are shown in Figure 3.&lt;br /&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;div style=&quot;text-align: center;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;img alt=&quot;iclaims-bsm-prediction.png&quot; height=&quot;312&quot; src=&quot;https://lh6.googleusercontent.com/Zo0mdpiftu-T7Rl8YOUTXhnvsiWF2363YUJWFKyi0RlE46Eobk5AGHKh3OUT58RSkTnO-vIWCVU0Ih0TU3IXhi3PlpaIuGZCaFXPsgRrGRwmSdhRlPtGI3beeUzavXe-onamcUR1&quot; style=&quot;border: none; transform: rotate(0rad);&quot; width=&quot;624&quot; /&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;b&gt;  Figure 3:&lt;/b&gt; Posterior predictive distribution for the next 12 weeks of initial claims.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h3&gt;Regression with spike and slab  priors&lt;/h3&gt;Now let&#39;s add a regression component to the model described above, so that we can use Google search data to improve the forecast.  The &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;bsts&lt;/span&gt; package only includes 10 search terms with the initial claims data set, to keep the package size small, but Scott and Varian (2014) considered examples with several hundred predictor variables.  When faced with large numbers of potential predictors it is important to have a prior distribution that induces sparsity. A spike and slab prior is a natural way to express a prior belief that most of the regression coefficients are exactly zero.&lt;br /&gt;&lt;br /&gt;A spike and slab prior is a prior on a set of regression coefficients that assigns each coefficient a positive probability of being zero.  Upon observing data, Bayes&#39; theorem updates the inclusion probability of each coefficient. When sampling from the posterior distribution of a regression model under a spike and slab prior, many of the simulated regression coefficients will be exactly zero.  This is unlike the &quot;lasso&quot; prior (the Laplace, or double-exponential distribution), which yields MAP estimates at zero but where posterior simulations will be all nonzero.  You can read about the mathematical details of spike and slab priors in Scott and Varian (2014).&lt;br /&gt;&lt;br /&gt;When fitting &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;bsts&lt;/span&gt; models that contain a regression component, extra arguments captured by &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;...&lt;/span&gt; are passed to the &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;SpikeSlabPrior&lt;/span&gt; function from the &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;BoomSpikeSlab&lt;/span&gt; package.  This allows the analyst to adjust the default prior settings for the regression component from the &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;bsts&lt;/span&gt; function call.  To include a regression component in a &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;bsts&lt;/span&gt; model, simply pass a model formula as the first argument.&lt;b style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;# Fit a bsts model with expected model size 1, the default.&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;model2 &amp;lt;- bsts(iclaimsNSA ~ .,&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;state.specification = ss,&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;niter = 1000,&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;data = initial.claims)&lt;/span&gt;&lt;/div&gt;&lt;b style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;# Fit a bsts model with expected model size 5, to include more coefficients.&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;model3 &amp;lt;- bsts(iclaimsNSA ~ .,&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;state.specification = ss,&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;niter = 1000,&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;data = initial.claims,&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;expected.model.size = 5) &amp;nbsp;# Passed to SpikeSlabPrior.&lt;/span&gt;&lt;/div&gt;&lt;b style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;br /&gt;To examine the output you can use the same plotting functions as before.  For example, to see the contribution of each state component you can type &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;plot(model2, &quot;comp&quot;)&lt;/span&gt;, producing the output in Figure 4.  The regression component is explaining a substantial amount of variation in the initial claims series.&lt;b style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;div style=&quot;text-align: center;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;img alt=&quot;iclaims-reg-ems1-components.png&quot; height=&quot;312&quot; src=&quot;https://lh5.googleusercontent.com/iphojjZ4JOTPdLRPBxNzGw69-syYSqemNwvzW3nLRYNrj-YWSXZGY4TdbVO7wi5akODJhqldVBKBsEAzZ0fO3T8R_cbvhKsO_uxYTX5G6_21CD2giEeWow4oeOFebxT5rYDc3pUF&quot; style=&quot;border: none; transform: rotate(0rad);&quot; width=&quot;624&quot; /&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;b&gt;Figure 4:&lt;/b&gt; Contribution of each state component to the initial claims data, assuming a regression component with default prior.  Compare to Figure 2. &lt;br /&gt;&lt;br /&gt;&lt;br /&gt;There are also plotting functions that you can use to visualize the regression coefficients.  The following commands&lt;br /&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;plot(model2, &quot;coef&quot;)&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;plot(model3, &quot;coef&quot;)&lt;/span&gt;&lt;/div&gt;&lt;b style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;produce the summary plots in Figure 5.  The search term &quot;unemployment office&quot; shows up with high probability in both models. Increasing the expected model size from 1 (the default) to 5 allows other variables into the model, though &quot;Idaho unemployment&quot; is the only one that shows up with high probability.&lt;b style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;margin-left: 0pt;&quot;&gt;&lt;table style=&quot;border-collapse: collapse; border: none; width: 468pt;&quot;&gt;&lt;colgroup&gt;&lt;col width=&quot;*&quot;&gt;&lt;/col&gt;&lt;col width=&quot;*&quot;&gt;&lt;/col&gt;&lt;/colgroup&gt;&lt;tbody&gt;&lt;tr style=&quot;height: 0pt;&quot;&gt;&lt;td style=&quot;padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;img alt=&quot;iclaims-reg-ems1-coef.png&quot; height=&quot;297&quot; src=&quot;https://lh6.googleusercontent.com/1qrG5JFbvHvblYkU1ztGwlCOOSMR92H-csfVQV4XgTnlIhkTMsNn3gf9BhroBu-y6jcGz2OUavq1ntToTZzls7SV_DOvgdn5Xlwfzp7NbEIaHOs8MQwIcTiv4fYFIzDSMZWvQheD&quot; style=&quot;border: none; transform: rotate(0rad);&quot; width=&quot;298&quot; /&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;img alt=&quot;iclaims-reg-ems5-coef.png&quot; height=&quot;297&quot; src=&quot;https://lh3.googleusercontent.com/krdpiG0EZUnccg9SlpD2yR4N4CjaGjWlN0KHeya7lRxlWozUi2x70dVEGqUzzwj_c0aUu57HsAwdo7UPHnbZIqapxbDON8GnXPcPdGHcqwXJRjWQ6I7lS3fj5vcbqCxCSvAivolZ&quot; style=&quot;border: none; transform: rotate(0rad);&quot; width=&quot;298&quot; /&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0pt;&quot;&gt;&lt;td style=&quot;padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;(a)&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;(b)&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;&lt;b style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;br /&gt;&lt;b&gt;Figure 5:&lt;/b&gt; Posterior inclusion probabilities for predictors in the &quot;initial claims&quot; nowcasting example assuming an expected model size of (a) 1 and (b) 5.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;Model diagnostics: Did the Google data help?&lt;/h3&gt;As part of the model fitting process, the algorithm generates the one-step-ahead prediction errors $y_t - E(y_t | Y_{t-1}, \theta)$, where $Y_{t-1} = y_1, \dots, y_{t-1}$, and the vector of model parameters $\theta$ is fixed at its current value in the MCMC algorithm.  The one-step-ahead prediction errors can be obtained from the &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;bsts&lt;/span&gt; model by calling &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;bsts.prediction.errors(model1)&lt;/span&gt;.&lt;br /&gt;&lt;div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;br /&gt;The one step prediction errors are a useful diagnostic for comparing several &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;bsts&lt;/span&gt; models that have been fit to the same data.  They are used to implement the function &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;CompareBstsModels&lt;/span&gt;, which is called as shown below.&lt;br /&gt;&lt;br /&gt;&lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; white-space: pre-wrap;&quot;&gt;CompareBstsModels(list(&quot;Model 1&quot; = model1,&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&quot;Model 2&quot; = model2,&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&quot;Model 3&quot; = model3),&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;colors = c(&quot;black&quot;, &quot;red&quot;, &quot;blue&quot;))&lt;/span&gt;&lt;/div&gt;&lt;b style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;The result of the call is the plot shown in Figure 6.  The bottom panel shows the original series.  The top panel shows the cumulative total of the mean absolute one step prediction errors for each model.  The final time point in the top plot is proportional to the mean absolute prediction error for each model, but plotting the errors as a cumulative total lets you see particular spots where each model encountered trouble, rather than just giving a single number describing each model’s predictive accuracy. Figure 6 shows that the Google data help explain the large spike near 2009, where model 1 accumulates errors at an accelerated rate, but models 2 and 3 continue accumulating errors at about the same rate they had been before.  The fact that the lines for models 2 and 3 overlap in Figure 6 means that the additional predictors allowed by the relaxed prior used to fit model 3 do not yield additional predictive accuracy.&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;div style=&quot;text-align: center;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;img alt=&quot;iclaims-compare-models.png&quot; height=&quot;437&quot; src=&quot;https://lh6.googleusercontent.com/w78qt2-5Yu7zJaHdJ6JiTq9GdfvxiKOLY7xHScy6-AJBeY3TcY8xHeeIGRo415ij34pFM9ZuBi2xRO22eH7I8MyhV8Abwbo5b2hjjDryIhnrhhsINtN4kT7bxi4_XVYL3Yb_rX4K&quot; style=&quot;border: none; transform: rotate(0rad);&quot; width=&quot;624&quot; /&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;b&gt;Figure 6:&lt;/b&gt; Comparing models 1 - 3 in terms of cumulative prediction error, for the nowcasting&amp;nbsp;application.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Example 2: Long term forecasting&lt;/h2&gt;A common question about &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;bsts&lt;/span&gt; is &quot;which trend model should I use?&quot; To answer that question it helps to know a bit about the different models that the &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;bsts&lt;/span&gt; software package provides, and what each model implies.&lt;br /&gt;In the local level model the state evolves according to a random walk:&lt;br /&gt;&lt;br /&gt;$$ \mu_{t+1} = \mu_t + \eta_t.$$&lt;br /&gt;If you place your eye at time 0 and ask what happens at time $t$, you find that $\mu_t \sim N(\mu_0, t \sigma^2_\eta)$.  The variance continues to grow with $t$, all the way to $t = \infty$.  The local linear trend is even more volatile. When forecasting far into the future the flexibility provided by these models becomes a double edged sword, as local flexibility in the near term translates into extreme variance in the long term.&lt;br /&gt;&lt;br /&gt;An alternative is to replace the random walk with a stationary AR process.  For example&lt;br /&gt;&lt;br /&gt;$$ \mu_{t+1} = \rho \mu_{t} + \eta_t,$$&lt;br /&gt;with $\eta_t \sim N(0, \sigma^2_\eta)$ and $|\rho| &amp;lt; 1$.  This model has stationary distribution&lt;br /&gt;$$\mu_\infty \sim N\left(0, \frac{\sigma^2_\eta}{1 - \rho^2}\right),$$&lt;br /&gt;which means that uncertainty grows to a finite asymptote, rather than infinity, in the distant future.  Bsts offers autoregressive state models through the functions &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;AddAr&lt;/span&gt;, when you want to specify a certain number of lags, and &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;AddAutoAr&lt;/span&gt; when you want the software to choose the important lags for you. &lt;br /&gt;&lt;br /&gt;A hybrid model modifies the local linear trend model by replacing the random walk on the slope with a stationary AR(1) process, while keeping the random walk for the level of the process.  The &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;bsts&lt;/span&gt; package refers to this is the &quot;semilocal linear trend&quot; model.&lt;br /&gt;$$    \mu_{t+1} = \mu_t + \delta_t + \eta_{0t} $$ $$    \delta_{t+1} = D + \rho (\delta_t - D) + \eta_{1t} $$ The $D$ parameter is the long run slope of the trend component, to which $\delta_t$ will eventually revert.  However $\delta_t$ can have short term autoregressive deviations from the long term trend, with memory determined by $\rho$.  Values of $\rho$ close to 1 will lead to long deviations from $D$.  To see the impact this can have on long term forecasts, consider the time series of daily closing values for the S&amp;amp;P 500 stock market index over the last 5 years, shown in Figure 7. &lt;br /&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;div style=&quot;text-align: center;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;img alt=&quot;sp500-daily-data.png&quot; height=&quot;429&quot; src=&quot;https://lh6.googleusercontent.com/6pf2Y7KZ3BsN-XnEPHaBseRalFTaASjfg-zeWzF5kijZbXDpn_NnruLbnMK4e9MxJ3jqKlzy2ea0-OFcW726xU4LWCKfxSUJ_3pXalnO0k-nhQvy45GGPHi7I0htQil4zJDIqoii&quot; style=&quot;border: none; transform: rotate(0rad);&quot; width=&quot;429&quot; /&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;b&gt;Figure 7:&lt;/b&gt; Daily closing values for the S&amp;amp;P 500 stock market index.&lt;b style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;Consider two forecasts of the daily values of this series for the next 360 days.  The first assumes the local linear trend model.  The second assumes the semilocal linear trend.&lt;br /&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;ss1 &amp;lt;- AddLocalLinearTrend(list(), sp500)&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;model1 &amp;lt;- bsts(sp500, state.specification = ss1, niter = 1000)&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;pred1 &amp;lt;- predict(model1, horizon = 360)&lt;/span&gt;&lt;/div&gt;&lt;b style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;ss2 &amp;lt;- AddSemilocalLinearTrend(list(), sp500)&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;model2 &amp;lt;- bsts(sp500, state.specification = ss2, niter = 1000)&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;pred2 &amp;lt;- predict(model2, horizon = 360)&lt;/span&gt;&lt;/div&gt;&lt;b style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;plot(pred2, plot.original = 360, ylim = range(pred1))&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;plot(pred1, plot.original = 360, ylim = range(pred1))&lt;/span&gt;&lt;/div&gt;&lt;b style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;br /&gt;The resulting forecasts are plotted in Figure 8.  The forecast expectations from the two models are quite similar, but the forecast errors from the local linear trend model are implausibly wide, including a small but nonzero probability that the S&amp;amp;P 500 index could close near zero in the next 360 days.  The error bars from the semilocal linear trend model are far more plausible, and more closely match the uncertainty observed over the life of the series thus far.&lt;b style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;margin-left: 0pt;&quot;&gt;&lt;table style=&quot;border-collapse: collapse; border: none; width: 468pt;&quot;&gt;&lt;colgroup&gt;&lt;col width=&quot;*&quot;&gt;&lt;/col&gt;&lt;col width=&quot;*&quot;&gt;&lt;/col&gt;&lt;/colgroup&gt;&lt;tbody&gt;&lt;tr style=&quot;height: 0pt;&quot;&gt;&lt;td style=&quot;padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;img alt=&quot;sp500-semilocal-linear-prediction.png&quot; height=&quot;297&quot; src=&quot;https://lh4.googleusercontent.com/bRXfuqLEonbjH3mYqphjswBj_9S1rrjbpHdNEci2c9IL0v5cK-J5z8MimP_owe7zYAKD95IDktar58J6ZMazc1AwZDDg4tGh01vWQIAJViUbmy2fB_0uSZTQmhAsMTtF2gXz3BZP&quot; style=&quot;-webkit-transform: rotate(0.00rad); border: none; transform: rotate(0.00rad);&quot; width=&quot;298&quot; /&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;img alt=&quot;sp500-local-linear-prediction.png&quot; height=&quot;297&quot; src=&quot;https://lh4.googleusercontent.com/bWZ70ULbepNj3mO70bri0vVuoTTAXWw682QkwWpgzFu81u3YEt2XmgH07V5o_8n3NYv1HVKMB0qDHn_TlBpleXDnPcly0VjHc4af2AYTUS87vR-ZHIkLU3vd6daC_DKH2l_npsqH&quot; style=&quot;-webkit-transform: rotate(0.00rad); border: none; transform: rotate(0.00rad);&quot; width=&quot;298&quot; /&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0pt;&quot;&gt;&lt;td style=&quot;padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;(a)&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;(b)&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;&lt;b style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;br /&gt;&lt;b&gt;Figure 8:&lt;/b&gt; Long term forecasts of the S&amp;amp;P 500 closing values under the (a) local linear trend and (b) semilocal linear trend state models.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;br /&gt;&lt;h2&gt; Example 3: Recession modeling using non-Gaussian data&lt;/h2&gt;Although we have largely skipped details about how the &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;bsts&lt;/span&gt; software fits models, the Gaussian error assumptions in the observation and transition equations are important for the model fitting process.  Part of that process involves running data through the Kalman filter, which assumes Gaussian errors in both the state and transition equations.  In many settings where Gaussian errors are obviously inappropriate, such as for binary or small count data, one can introduce latent variables that give the model a conditionally Gaussian representation.  Well known &quot;data augmentation&quot; methods exist for probit regression (Albert and Chib, 1993) and models with student T errors (Gelman et al. 2014).  Somewhat more complex methods exist for logistic regression (Frühwirth-Schnatter and Frühwirth 2005, Holmes and Held 2006, Gramacy and Polson 2012) and Poisson regression (Frühwirth-Schnatter et al 2008). Additional methods exist for quantile regression (Benoit and Van Den Poel 2012), support vector machines (Polson and Scott 2011), and multinomial logit regression (Frühwirth-Schnatter and Frühwirth 2010). These are not currently provided by the &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;bsts&lt;/span&gt; package, but they might be added in the future.&lt;br /&gt;&lt;br /&gt;To see how non-Gaussian errors can be useful, consider the analysis done by Berge, Sinha, and Smolyansky (2016) who used Bayesian model averaging (BMA) to investigate which of several economic indicators would best predict the presence or absence of a recession.  We will focus on their nowcasting example, which models the probability of a recession at the same time point as the predictor variables.  Berge, Sinha, and Smolyansky (2016) also analyzed the data with the predictors at several lags.&lt;b style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;div style=&quot;text-align: center;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;img alt=&quot;gdp-training-data.png&quot; height=&quot;360&quot; src=&quot;https://lh5.googleusercontent.com/qeo1_lYUUBOgIG7tCWVZGDnjdC0NujPfCOyoGAdwWyQrmKqI82GMFvLij-B0biqrh1xM3POzCvtBv2tRt7a4IegHcdbs_QeaAg8yWWt9JHB-ElDtlCILi34-k3pnvdfxoAXG8eMw&quot; style=&quot;border: none; transform: rotate(0rad);&quot; width=&quot;360&quot; /&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;b&gt;Figure 9:&lt;/b&gt; US recession indicators, as determined by NBER.&lt;b style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;br /&gt;&lt;b style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;br /&gt;The model used in Berge, Sinha, and Smolyansky (2016) was a probit regression, with Bayesian model averaging used to determine which predictors should be included.  The response variable was the the presence or absence of a recession (as &lt;a href=&quot;http://www.nber.org/cycles.html&quot; target=&quot;_blank&quot;&gt;determined by NBER&lt;/a&gt;), plotted in Figure 9. The BMA done by Berge, Sinha, and Smolyansky (2016) is essentially the same as fitting a logistic regression under a spike-and-slab prior with the prior inclusion probability of each predictor set to $1/2$.  That analysis can be run using the &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;BoomSpikeSlab&lt;/span&gt; R package (Scott 2010), which is similar to &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;bsts&lt;/span&gt;, but with only a regression component and no time series.  The marginal posterior inclusion probabilities produced by &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;BoomSpikeSlab&lt;/span&gt; are shown in Figure 10(a).  They largely replicate the findings of Berge, Sinha, and Smolyansky (2016), up to minor Monte Carlo error.&lt;br /&gt;&lt;br /&gt;The logistic regression model is highly predictive, but it ignores serial dependence in the data.  To capture serial dependence, consider the following dynamic logistic regression model with a local level trend model.&lt;br /&gt;$$    \text{logit}(p_t) = \mu_t + \beta^T{\bf x}_t $$ $$    \mu_{t+1} = \mu_t + \eta_t $$&lt;br /&gt;Here $p_t$ is the probability of a recession at time $t$, and ${\bf x}_t$ is the set of economic indicators used by Berge, Sinha, and Smolyansky (2016) in their analysis.  To fit this model, we can issue the commands shown below.&lt;b style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;## Because &#39;y&#39; is 0/1 and the state is on the logit scale the default prior&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;## assumed by AddLocalLevel won&#39;t work here, so we need to explicitly set the&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;## priors for the variance of the state innovation errors and the initial value&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;## of the state at time 0. &amp;nbsp;The &#39;SdPrior&#39; and &#39;NormalPrior&#39; functions used to&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;## define these priors are part of the Boom package. &amp;nbsp;See R help for&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;## documentation. &amp;nbsp;Note the truncated support for the standard deviation of the&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;## random walk increments in the local level model.&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;ss &amp;lt;- AddLocalLevel(list(),&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;sigma.prior = SdPrior(sigma.guess = .1,&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;sample.size = 1,&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;upper.limit = 1),&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;initial.state.prior = NormalPrior(0, 5))&lt;/span&gt;&lt;/div&gt;&lt;b style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;## Tell bsts that the observation equation should be a logistic regression by&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;## passing the &#39;family = &quot;logit&quot;&#39; argument.&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;ts.model &amp;lt;- bsts(nber ~ ., ss, data = gdp, niter = 20000,a&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;family = &quot;logit&quot;, expected.model.size = 10)&lt;/span&gt;&lt;/div&gt;&lt;b style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;br /&gt;The marginal posterior inclusion probabilities under this model are shown in Figure 10(b).  The top predictor is the same in both models, but posterior inclusion probabilities for the remaining predictors are smaller than in Figure 10(a).  To understand why, consider the distribution of $\mu_t$ shown in Figure 11. The figure shows $\mu_t$ moving to very large values during a recession, and to very small values outside of a recession.  This effect captures the strong serial dependence in the recession data.  Recessions are rare, but once they occur they tend to persist.  Assuming independent time points is therefore unrealistic, and it substantially overstates the amount of information available to identify logistic regression coefficients.  The spike and slab priors that &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;bsts&lt;/span&gt; uses to identify predictors naturally produce sparser models in the face of less information, which is why Figure 10(b) shows fewer included coefficients.&lt;b style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;margin-left: 0pt;&quot;&gt;&lt;table style=&quot;border-collapse: collapse; border: none; width: 468pt;&quot;&gt;&lt;colgroup&gt;&lt;col width=&quot;*&quot;&gt;&lt;/col&gt;&lt;col width=&quot;*&quot;&gt;&lt;/col&gt;&lt;/colgroup&gt;&lt;tbody&gt;&lt;tr style=&quot;height: 0pt;&quot;&gt;&lt;td style=&quot;padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;img alt=&quot;nber-plain-regression-coefficients.png&quot; height=&quot;297&quot; src=&quot;https://lh6.googleusercontent.com/NZgat7vf_IkPSur9xhBblId_VWtgF7p2Y2pgd5R2EsUVGWPigTE9Ni8PjwwB-d6TqS0EjDfbmV6VjN20OIvaO-i6clgFsIAaFYr1R6u1Zg0lbSXK3owZCNZqjZovbJclTUZUzdkY&quot; style=&quot;-webkit-transform: rotate(0.00rad); border: none; transform: rotate(0.00rad);&quot; width=&quot;298&quot; /&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;img alt=&quot;nber-ts-regression-coefficients.png&quot; height=&quot;297&quot; src=&quot;https://lh4.googleusercontent.com/ebS6FTc2-9rgLZAhYIXJVKPXHU7EN3eday67IyFfHXfrssxYQagwXWNGMDEOnX-a7C2VczXqR1I9GI8yRbAwSTGKjbp3CMZFcnCDAflFh6eNbOzUk60PFCW297wn-hVzoteQS6QJ&quot; style=&quot;border: none; transform: rotate(0rad);&quot; width=&quot;298&quot; /&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0pt;&quot;&gt;&lt;td style=&quot;padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;(a)&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;padding: 5pt 5pt 5pt 5pt; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;(b)&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;&lt;b style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;br /&gt;&lt;b&gt;Figure 10:&lt;/b&gt; Regression coefficients for the (a) plain logistic regression model and (b) time series logistic regression model under equivalent spike and slab priors.&lt;b style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;div style=&quot;text-align: center;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;img alt=&quot;nber-bsts-logit.png&quot; height=&quot;360&quot; src=&quot;https://lh6.googleusercontent.com/sqp4YXyWER4Dc1Z0Pv5a691sf1eFe9yqcjm9kpVWJrl-bTQMMvE95Puckx0Qljrq6GkovUeB_3QJ2fKLenGTw7l3Ig60BsFgJBX0p-1MjmrUDGgWqyPwRe36FGgjCDP0drvkF_fn&quot; style=&quot;border: none; transform: rotate(0rad);&quot; width=&quot;360&quot; /&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;b&gt;Figure 11: &lt;/b&gt;Distribution of state (on logit scale) for recession data.  Blue dots show the true presence or absence of a recession, as determined by official&amp;nbsp;statistics.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h2&gt; Conclusion&lt;/h2&gt;The preceding examples have shown that the &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;bsts&lt;/span&gt; software package can handle several nonstandard, but useful, time series applications.  These include the ability to handle large numbers of contemporaneous predictors with spike and slab priors, the presence of trend models suitable for long term forecasting, and the ability to handle non-Gaussian data.  We have run out of space, but &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;bsts&lt;/span&gt; can do much more.&lt;br /&gt;&lt;br /&gt;For starters there are other state models you can use.  Bsts has elementary support for holidays.  It knows about 18 US holidays, and has capacity to add more, including holidays that occur on the same date each year, holidays that occur on a fixed weekday of a fixed month (e.g. 3rd Tuesday in February, or last Monday in November).  The model for each holiday is a simple random walk, but look for future versions to have improved holiday support via Bayesian shrinkage.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Bsts offers support for multiple seasonalities.  For example, if you have several weeks of hourly data then you will have an hour-of-day effect as well as a day-of-week effect.  You can model these using a single seasonal effect with 168 seasons (which would allow for different hourly effects on weekends and weekdays), or you can assume additive seasonal patterns using the &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;season.duration&lt;/span&gt; argument to &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;AddSeasonal&lt;/span&gt;,&lt;b style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;ss &amp;lt;- AddSeasonal(ss, y, nseasons = 24)&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;ss &amp;lt;- AddSeasonal(ss, y, nseasons = 7, season.duration = 24)&lt;/span&gt;&lt;/div&gt;&lt;b style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;br /&gt;The latter specifies that each daily effect should remain constant for 24 hours. For modeling physical phenomena, &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;bsts&lt;/span&gt; also offers trigonometric seasonal effects, which are sine and cosine waves with time varying coefficients.  You obtain these by calling &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;AddTrig&lt;/span&gt;.  Time varying effects are available for arbitrary regressions with small numbers of predictor variables through a call to &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;AddDynamicRegression&lt;/span&gt;.&lt;br /&gt;&lt;br /&gt;In addition to the trend models discussed so far, the function &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;AddStudentLocalLinearTrend&lt;/span&gt; gives a version of the local linear trend model that assumes student $t$ errors instead of Gaussian errors.  This is a useful state model for short term predictions when the mean of the time series exhibits occasional dramatic jumps.  Student $t$ errors can be introduced into the observation equation by passing the &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;family = &quot;student&quot;&lt;/span&gt; argument to the &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;bsts&lt;/span&gt; function call.  Allowing for heavy tailed errors in the observation equation makes the model robust against individual outliers, while heavy tails in the state model provides robustness against sudden persistent shifts in level or slope.  This can lead to tighter prediction limits than Gaussian models when modeling data that have been polluted by outliers.  The observation equation can also be set to a Poisson model for small count data if desired.&lt;br /&gt;&lt;br /&gt;Finally, the most recent update to &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;bsts&lt;/span&gt; supports data with multiple observations at each time stamp.  The Gaussian version of the model is&lt;br /&gt;$$    y_{it} = \beta^T {\bf x}_{it} + Z_t^T\alpha_t + \epsilon_{it}$$ $$    \alpha_{t+1} = T_t \alpha_t + R_t \eta_t, $$&lt;br /&gt;which is best understood as a regression model with a time varying intercept.&lt;br /&gt;&lt;br /&gt;Bsts is a mature piece of software with a broad user base both inside and outside of Google.  It is the product of several years of development, and I expect to continue improving it for the foreseeable future.  I hope you find it useful. &lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;References&lt;/h2&gt;Albert, J. H. and Chib, S. (1993). Bayesian analysis of binary and polychotomous response data. &lt;i&gt;Journal of the American Statistical Association&lt;/i&gt; &lt;b&gt;88&lt;/b&gt;, 669–679.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Benoit, D. F. and Van Den Poel, D. (2012). Binary quantile regression: A Bayesian approach based on the asymmetric Laplace distribution. &lt;i&gt;Journal of Applied Econometrics&lt;/i&gt; &lt;b&gt;27&lt;/b&gt;, 1174–1188.&lt;br /&gt;&lt;br /&gt;Berge, T., Sinha, N., and Smolyansky, M. (2016). &lt;a href=&quot;https://www.federalreserve.gov/econresdata/notes/feds-notes/2016/which-market-indicators-best-forecast-recessions-20160802.html&quot; target=&quot;_blank&quot;&gt;Which market indicators best forecast recessions&lt;/a&gt;? Tech. rep., US Federal Reserve.&lt;br /&gt;&lt;br /&gt;Frühwirth-Schnatter, S. and Frühwirth, R. (2005). Auxiliary mixture sampling with applications to logistic models. Tech. rep., IFAS Research Paper Series, Department of Applied Statistics, Johannes Kepler University Linz.&lt;br /&gt;&lt;br /&gt;Frühwirth-Schnatter, S. and Frühwirth, R. (2010). &lt;a href=&quot;http://www.jku.at/ifas/content/e108280/e146255/e146260/ifas_rp48.pdf&quot; target=&quot;_blank&quot;&gt;Data augmentation and MCMC for binary and multinomial logit models&lt;/a&gt;. In T. Kneib and G. Tutz, eds., &lt;i&gt;Statistical Modelling and Regression Structures – Festschrift in Honour of Ludwig Fahrmeir&lt;/i&gt;, 111–132. Physica-Verlag, Heidelberg. &lt;br /&gt;&lt;br /&gt;Frühwirth-Schnatter, S., Frühwirth, R., Held, L., and Rue, H. (2008). Improved auxiliary mixture sampling for hierarchical models of non-Gaussian data. &lt;i&gt;Statistics and Computing&lt;/i&gt; &lt;b&gt;19&lt;/b&gt;, 479.&lt;br /&gt;&lt;br /&gt;Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., and Rubin, D. B. (2014). &lt;i&gt;Bayesian Data Analysis&lt;/i&gt;. Chapman &amp;amp; Hall, 3rd edn.&lt;br /&gt;&lt;br /&gt;Gramacy, R. B. and Polson, N. G. (2012). Simulation-based regularized logistic regression. &lt;i&gt;Bayesian Analysis&lt;/i&gt; &lt;b&gt;7&lt;/b&gt;, 567–590.&lt;br /&gt;&lt;br /&gt;Holmes, C. C. and Held, L. (2006). Bayesian auxiliary variable models for binary and multinomial regression. &lt;i&gt;Bayesian Analysis&lt;/i&gt; &lt;b&gt;1&lt;/b&gt;, 145–168.&lt;br /&gt;&lt;br /&gt;Petris, G. (2010). An R package for dynamic linear models. &lt;i&gt;Journal of Statistical Software&lt;/i&gt; &lt;b&gt;36&lt;/b&gt;, 1–16.&lt;br /&gt;&lt;br /&gt;Petris, G., Petrone, S., and Campagnoli, P. (2009). &lt;i&gt;Dynamic Linear Models with R.&lt;/i&gt; useR! Springer-Verlag, New York.&lt;br /&gt;&lt;br /&gt;Polson, N. G. and Scott, S. L. (2011). Data augmentation for support vector machines. &lt;i&gt;Bayesian Analysis&lt;/i&gt; &lt;b&gt;6&lt;/b&gt;, 1–24. (with discussion).&lt;br /&gt;&lt;br /&gt;Scott, S. L. (2010). BoomSpikeSlab: MCMC for spike and slab regression. R package version 0.4.1.&lt;br /&gt;&lt;br /&gt;Scott, S. L. and Varian, H. R. (2014). Predicting the present with Bayesian structural time series. &lt;i&gt;International Journal of Mathematical Modelling and Numerical Optimisation&lt;/i&gt; &lt;b&gt;5&lt;/b&gt;, 4–23.&lt;br /&gt;&lt;br /&gt;Scott, S. L. and Varian, H. R. (2015). Bayesian variable selection for nowcasting economic time series. In A. Goldfarb, S. Greenstein, and C. Tucker, eds., &lt;i&gt;Economics of Digitization&lt;/i&gt;, 119 –136. NBER Press, London.&lt;br /&gt;&lt;br /&gt;Tassone, E. and Rohani, F. (2017). Our quest for robust time series forecasting at scale. &lt;a href=&quot;http://www.unofficialgoogledatascience.com/2017/04/our-quest-for-robust-time-series.html&quot;&gt;http://www.unofficialgoogledatascience.com/2017/04/our-quest-for-robust-time-series.html&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;Taylor, S. J. and Letham, B. (2017). Prophet: forecasting at scale. &lt;a href=&quot;https://research.fb.com/prophet-forecasting-at-scale/&quot; target=&quot;_blank&quot;&gt;https://research.fb.com/prophet-forecasting-at-scale/&lt;/a&gt;&lt;/div&gt;</content><link rel='replies' type='application/atom+xml' href='https://www.unofficialgoogledatascience.com/feeds/6693193164123748208/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='https://www.unofficialgoogledatascience.com/2017/07/fitting-bayesian-structural-time-series.html#comment-form' title='18 Comments'/><link rel='edit' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/6693193164123748208'/><link rel='self' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/6693193164123748208'/><link rel='alternate' type='text/html' href='https://www.unofficialgoogledatascience.com/2017/07/fitting-bayesian-structural-time-series.html' title='Fitting Bayesian structural time series with the bsts R package'/><author><name>Kay Brodersen</name><uri>http://www.blogger.com/profile/15877220456041691465</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://lh5.googleusercontent.com/e9pS3At33PjZ4yO5JID4JEv6bzIsdEs_XhOO2b_Cv4WrKKaZ2OrCqaoRxCLFEHJD3U-zt5X7MB-x7BNukQKPFzuYsQJYQk9wgFO52YA_2yDvgsy8iKaSPhF0d7FRKkiDa6ydj-H0=s72-c" height="72" width="72"/><thr:total>18</thr:total></entry><entry><id>tag:blogger.com,1999:blog-5825758052688213474.post-4732004365167765860</id><published>2017-04-17T17:02:00.001-07:00</published><updated>2017-04-19T13:39:40.083-07:00</updated><title type='text'>Our quest for robust time series forecasting at scale</title><content type='html'>by ERIC TASSONE, FARZAN ROHANI&lt;br /&gt;&lt;br /&gt;&lt;i&gt;We were part of a team of data scientists in Search Infrastructure at Google that took on the task of developing robust and automatic large-scale time series forecasting for our organization. In this post, we recount how we approached the task, describing initial stakeholder needs, the business and engineering contexts in which the challenge arose, and theoretical and pragmatic choices we made to implement our solution.&lt;/i&gt;&lt;br /&gt;&lt;h2&gt;Introduction&lt;/h2&gt;Time series forecasting enjoys a rich and luminous history, and today is an essential element of most any business operation. So it should come as no surprise that Google has compiled and forecast time series for a long time. For instance, the image below from the Google Visitors Center in Mountain View, California, shows hand-drawn time series of “Results Pages” (essentially search query volume) dating back nearly to the founding of the company on 04 September 1998.&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;img height=&quot;291&quot; src=&quot;https://lh6.googleusercontent.com/Vrd24K6K6UMR_QZuk1SV-PKSLG6PPLL3bbXaTsEDlmRSGFUrm2h9B5UlDsNbDT7z81hCr4kSmVoaq9W2PoNM2iyuHljlyQdkvqsN4BIEFVyJX3kJwQwJ1wa4MzuArXTFTEGqSC-n&quot; style=&quot;margin-left: auto; margin-right: auto;&quot; width=&quot;640&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot;&gt;&lt;br /&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;font-size: 12.8px; text-align: center;&quot;&gt;Hand-Drawn Time Series of Google “Results Pages”, November 1998 through July 2004. Due to multiple changes to the scale of the values depicted on the vertical axis, “Results Pages” values, which reflect search query volume, at the rightward end of the plot (corresponding to July 2004) are 2000 times larger than the values depicted at the leftward end (corresponding to November 1998).&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;font-size: 12.8px; text-align: center;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;The demand for time series forecasting at Google grew rapidly along with the company over its first decade. Various business and engineering needs led to a multitude of forecasting approaches, most reliant on direct analyst support. The volume and variety of the approaches, and in some cases their inconsistency, called out for an attempt to unify, automate, and extend forecasting methods, and to distribute the results via tools that could be deployed reliably across the company. That is, for an attempt to develop methods and tools that would facilitate accurate large-scale time series forecasting at Google. &lt;br /&gt;&lt;br /&gt;Our team of data scientists and software engineers in Search Infrastructure was already engaged in a particular type of forecasting. For us, demand for forecasts emerged from a determination to better understand business growth and health, more efficiently conduct day-to-day operations, and optimize longer-term resource planning and allocation decisions. Because our team was already forecasting a large number of time series for which direct analyst implementation and supervision were impractical, it was well positioned to attempt such a unification, automation, and distribution of forecasting methods.&lt;br /&gt;&lt;br /&gt;In the balance of this post, we will discuss and demonstrate aspects of our forecasting approach.&lt;br /&gt;&lt;ul&gt;&lt;li&gt;We begin by describing our general framework for thinking about our task, which entails carefully defining the overall forecasting problem -- what it is and what it is not -- and decomposing the problem into tractable subproblems whenever possible.&amp;nbsp;&lt;/li&gt;&lt;li&gt;We then detail subproblems resulting from the decomposition, and our approaches to solving them.&lt;/li&gt;&lt;ul&gt;&lt;li&gt;Adjustments to clean the data: missing values, anomalies, level changes, and transformations.&lt;/li&gt;&lt;li&gt;Adjustments for effects: holiday, seasonality, and day-of-week effects.&lt;/li&gt;&lt;li&gt;Disaggregation of the time series into subseries and reconciliation of the subseries forecasts.&lt;/li&gt;&lt;li&gt;Selection and aggregation of forecasts from an ensemble of models to produce a final forecast.&lt;/li&gt;&lt;li&gt;Quantification of forecast uncertainty via simulation-based prediction intervals.&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;We conclude with an example of our forecasting routine applied to publicly available Turkish Electricity data.&lt;/div&gt;&lt;br /&gt;&lt;h2&gt;GENERAL FRAMEWORK&lt;/h2&gt;A critical learning from our forecasting endeavor was the value of careful thought about the environment in which both the problem and potential solutions arise. The challenge, of course, is in the details: What problem are we trying to solve and -- just as importantly -- what problems are we not trying to solve? How do we carefully delineate and separate where possible the various subproblems that make up the problem we are trying to solve? Which subproblems should be decoupled and treated independently to better attack them or for ease of implementation?&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;h3&gt;Our Forecasting Problem&lt;/h3&gt;Our typical use case was to produce a time series forecast at the daily level for a 12-24 month forecast horizon based on a daily history two or more years long. On occasion, we might be interested in forecasting based on weekly totals or a move to a more refined temporal resolution, such as hours or minutes, but the norm was daily totals. Other times we might be asked to make a forecast based on a shorter history.&lt;br /&gt;&lt;br /&gt;In terms of scope, we sought to forecast a large number of time series, and to do so frequently. We wanted to forecast a variety of quantities: overall search query volume and particular types of queries; revenue; views and minutes of video watched on Google-owned YouTube (which recently &lt;a href=&quot;https://youtube.googleblog.com/2017/02/you-know-whats-cool-billion-hours.html&quot;&gt;reached a billion hours daily&lt;/a&gt;); usage of sundry internal resources; and more. Additionally, we often had an independent interest in subseries of those quantities, such as disaggregated by locale (e.g., into &amp;gt;100 countries or regions), device type (e.g., by desktop, mobile, and tablet), and operating system, as well as combinations of all these (e.g., locale-by-device type.) Given the assortment of quantities and their combinatorial explosion into subseries, our forecasting task was easily on the order of tens of thousands of forecasts. And then we wanted to forecast these quantities every week, or in some cases more often.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;The variety and frequency of forecasts demanded robust, automatic methods --- robust in the sense of dramatically reducing the chance of a poor forecast regardless of the particular characteristics of the time series being forecast (e.g., its growth profile) and automatic in the sense of not requiring human intervention before or after running the forecast.&lt;br /&gt;&lt;br /&gt;Another important domain constraint was that our time series were the result of some aggregate human phenomenon --- search queries, YouTube videos viewed, or (as in our example) electricity consumed in Turkey. Unlike physical phenomena such as earthquakes or the tides, the time series we forecast are shaped by the rhythms of human civilization and its weekly, annual, and at-times irregular and evolving cycles. Calendaring was therefore an explicit feature of models within our framework, and we made considerable investment in maintaining detailed regional calendars.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Non-Problems&lt;/h3&gt;&quot;Forecasting&quot; is in many ways an overloaded word, with many potential meanings. We certainly were not trying to handle all forecasting possibilities, which sometimes vexed our colleagues who had turned to us for assistance. So what did &quot;forecasting&quot; not mean to us?&lt;br /&gt;&lt;br /&gt;Our initial use case did not involve explanatory variables, instead relying only the time series histories of the variables being forecast. Our rationale was in accord with the views expressed in the online forecasting book by Hyndman and Athana­sopou­los [1], who after mentioning the potential utility of an &quot;explanatory model&quot; write:&lt;br /&gt;&lt;br /&gt;&lt;i&gt;However, there are several reasons a forecaster might select a time series model rather than an explanatory model. First, the system may not be understood, and even if it was understood it may be extremely difficult to measure the relationships that are assumed to govern its behavior. Second, it is necessary to know or forecast the various predictors in order to be able to forecast the variable of interest, and this may be too difficult. Third, the main concern may be only to predict what will happen, not to know why it happens. Finally, the time series model may give more accurate forecasts than an explanatory or mixed model.&lt;/i&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&quot;Forecasting&quot; for us also did not mean using time series in a causal inference setting. There are tools for this use case, such as Google-supported &lt;a href=&quot;http://www.unofficialgoogledatascience.com/2015/09/causal-attribution-in-era-of-big-time.html&quot;&gt;CausalImpact&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;CausalImpact is powered by &lt;a href=&quot;https://cran.r-project.org/web/packages/bsts/index.html&quot;&gt;bsts&lt;/a&gt; (“Bayesian Structural Time Series”), also from Google, which is a time series regression framework using dynamic linear models fit using Markov chain Monte Carlo techniques. The regression-based bsts framework can handle predictor variables, in contrast to our approach. Facebook in a recent blog post unveiled &lt;a href=&quot;https://research.fb.com/prophet-forecasting-at-scale/&quot;&gt;Prophet&lt;/a&gt;, which is also a regression-based forecasting tool. But like our approach, Prophet aims to be an automatic, robust forecasting tool.&lt;br /&gt;&lt;br /&gt;At lastly, &quot;forecasting&quot; for us did not mean anomaly detection. Tools such as Twitter’s &lt;a href=&quot;https://blog.twitter.com/2015/introducing-practical-and-robust-anomaly-detection-in-a-time-series&quot;&gt;AnomalyDetection&lt;/a&gt; or &lt;a href=&quot;https://github.com/Netflix/Surus&quot;&gt;RAD&lt;/a&gt; (&quot;Robust Anomaly Detection&quot;) from Netflix, as their names suggest, target this type of problem.&lt;/div&gt;&lt;br /&gt;&lt;h2&gt;Decomposing our problem&lt;/h2&gt;Our ultimate objective is to accurately forecast the expected growth trends of our time series. To do so, we have found it useful to make a variety of cleaning adjustments and effects adjustments to better isolate the underlying growth trend. Both cleaning and effects adjustments allow for better estimation of underlying trend. The difference is that cleaning is to remove unpredictable nuisance artifacts whereas the effects are regular patterns we wish to capture explicitly. Thus our forecasting routine decomposes our overall problem into subproblems along these very lines. &lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Cleaning Adjustments&lt;/h3&gt;At the start of our process, we make several cleaning adjustments to the observed time series. Four major cleaning adjustments that we treat as separate problems are (a) missing values, (b) presumed anomalies, (c) accounting for abrupt level changes in the time series history, and (d) any transformations likely to result in improved forecast performance. Pre-processing our time series with these cleaning adjustments helps them to better conform to the assumptions of forecasting models to be fit later, such as stationarity. &lt;br /&gt;&lt;br /&gt;Missing values and putative anomalous values may present as a dramatic spike or drop not explained by a launch, seasonality effect, or holiday impact. They can arise from data collection errors or other unlikely-to-repeat causes such as an outage somewhere on the Internet. If unaccounted for, these data points can have an adverse impact on forecast accuracy by disrupting seasonality, holiday, or trend estimation. &lt;br /&gt;&lt;br /&gt;Level changes are different in that they represent a more permanent change in the level of the time series, in contrast to anomalies, where the time series returns to its previous level at trend after a brief period. They may result from launches, logging changes, or external events. At a minimum, adjusting for level changes results in a longer effective time series, and often makes trend estimation easier when the models are finally fit.&lt;br /&gt;&lt;br /&gt;We also permit transformations, such as a Box-Cox transformation. These can be automatic (the default setting for some models) or user-specified. These too can help with meeting model assumptions and with eventual forecast accuracy (final forecasts must be re-transformed to the original scale, of course).&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Effect Adjustments&lt;/h3&gt;Typically we also adjust for several effects before fitting our models (an exception being seasonality in STL models which the models directly handled [2]). There are three major effects that we decouple and treat as subproblems: holiday effects, seasonality effects, and day-of-week effects. Whereas cleaning adjustments pre-process the data and are mostly targeted at one-off incidents, the other effects we attempt to adjust for are typically repeated patterns. &lt;br /&gt;&lt;br /&gt;For the effect adjustments we seek to: quantify the effect; remove the effect from the time series; forecast the time series without the effect; and re-apply the effect to that forecast post hoc. By contrast, for cleaning adjustments, we seek only to quantify, so as to remove. In addition, for the effect adjustments, we typically have business-knowledge or day-to-day-operations motivations for wanting to know the magnitude of the effect; this is not always the case for cleaning adjustments.&lt;br /&gt;&lt;br /&gt;Though not cleanly separated conceptually, holiday effects and seasonality effects are partially distinguished in that seasonality effects are defined as occurring during the same numerical week of the year, whereas holiday effects are specific to the day of the occurrence and adjacent &quot;shoulder&quot; days. That said, some holidays fall on the same calendar day but on a different day of the week each year, such as Independence Day falling on the 4th of July in the United States; some fall on the same day of the week and week of the year, but on a different calendar day, such as Thanksgiving Day falling on the fourth Thursday of November in the United States; and some vary with solar or lunar phenomena, such as &lt;a href=&quot;https://www.timeanddate.com/calendar/determining-easter-date.html&quot;&gt;the Easter Sunday holiday&lt;/a&gt; celebrated in many countries around the world, which can fall anywhere from late March to late April. Due to these differences, some holiday effects and seasonality effects may not be identifiable. But note the use of &quot;week&quot; as the germane unit of time for seasonality and &quot;day&quot; for holidays: We estimate holiday effects from the adjusted daily data, but then roll up daily values to weekly totals before estimating seasonality effects. &lt;br /&gt;&lt;br /&gt;Estimating holiday and seasonality effects is more art than science due to the paucity of relevant historical data. For instance, to estimate the effect of Independence Day when it falls on a weekday versus a weekend, you can accrue observations only so fast! And the effect may evolve as, say, smart phones proliferate and people begin doing more Google Maps searches the evening of Independence Day. Our experience with holiday adjustments is that ad hoc methods which work well and are easily interpretable (if only for debugging) are preferred to elegant, comprehensive mathematical models which may not deliver nor are amenable to a postmortem when something (inevitably) goes wrong.&lt;br /&gt;&lt;br /&gt;The plots below illustrate a toy example of cleaning and effects adjustments in action. In the first plot, the raw weekly actuals (in red) are adjusted for a level change in September 2011 and an anomalous spike near October 2012. The cleaned weekly actuals (in green) are much smoother, but still show strong seasonal and holiday effects. As noted above, these are estimated, removed, the long-term trend is forecast, and then the seasonal and holiday effects are added back, producing the weekly forecast (in blue), as shown in the second plot.&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;span id=&quot;docs-internal-guid-24f75ce0-7a8b-e38d-ca12-daa781a893b3&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;roboto&amp;quot;; font-size: 11pt; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;img alt=&quot;clean2.png&quot; height=&quot;316&quot; src=&quot;https://lh3.googleusercontent.com/3LtD75mI6oUE0DF45dpiHKXuC0EBfZ-seX6WyuZmp5NuNgvoyO5APIakOpN7JeG04k9TmHUcwC2sL7eDDwpPtZFHoo0H1kMIP7jlM1y2Bzw_9n5Gsp-n_JqWqXwMGJJKucYo1133&quot; style=&quot;border: none; transform: rotate(0rad);&quot; width=&quot;640&quot; /&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span id=&quot;docs-internal-guid-24f75ce0-7a8c-0d09-a254-b686abb5a363&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;roboto&amp;quot;; font-size: 11pt; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;img alt=&quot;forecast_2.png&quot; height=&quot;320&quot; src=&quot;https://lh6.googleusercontent.com/dvH-hGQf9feP95xgJV2DMM7svI-dWG5dhDN00oLnFnMSzGMkrIRoYytPFzBTxpZcNZpu41mevlEcBb-zZAYsdkbgRbW0MRg9Yg87zR72bvSXSF6ExUfpqlJ9_z_4qEoBPLDt6Bbj&quot; style=&quot;border: none; transform: rotate(0rad);&quot; width=&quot;640&quot; /&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;The block diagram below shows where these cleaning and effects adjustments occur in the overall sequence of our forecast procedure:&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;span id=&quot;docs-internal-guid-24f75ce0-7a8d-4867-28d1-1ba41a8c45e5&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;roboto&amp;quot;; font-size: 11pt; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;img alt=&quot;workflow.png&quot; height=&quot;298&quot; src=&quot;https://lh6.googleusercontent.com/Pgb1HVq9tvdKBsvNbfGGXnloDVy21YxGqhhcA00Rrl0ht4kMX0avZa0dBBqvn0iwuKvCr5rpjaCykCA_QIFxD5nZ5jYCLK19RVXYJkKnOxxNbxpM5ZnBAnyYE_5z43KZ-74ih9xM&quot; style=&quot;border: none; transform: rotate(0rad);&quot; width=&quot;640&quot; /&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;br /&gt;&lt;h3&gt;Day-of-Week Effects&lt;/h3&gt;By &quot;day-of-week effects&quot; we mean a contributor to the multiple seasonalities of daily data typically observed --- there is often one pattern with period seven and then one or more patterns with much larger periods corresponding to an annual cycle. There are often multiple calendars operating within any geography (to name just two possible calendars, Islamic and Gregorian calendars have different periods and often produce more than one annual cycle in certain locales). Again, we decompose our overall problem and treat this day-of-week effect as its own problem, on its own terms. That is, after the cleaning adjustments and after de-holidaying and de-seasoning the data, our initial output is a weekly forecast from rolled-up weekly totals. Thus for daily forecasts we are left with distributing weekly totals to daily values.&lt;br /&gt;&lt;div&gt;&lt;br /&gt;In our experience there are great dividends in treating the forecasting task, with its priority on growth trend estimation, separately from the day-of-week effect. By decomposing the overall problem, we can focus on methods that best solve the specific subproblem. For us, forecasting the trend from less noisy weekly data (again, and importantly, after cleaning and effect adjustments) results in better growth trend estimation compared to all-in-one methods. Methods that attempt to simultaneously estimate all relevant parameters in a unified framework may be vulnerable to omission, overfitting, confounding of effects, or something else going awry which subsequently impairs the method&#39;s ability to accurately estimate the growth trend.&lt;br /&gt;&lt;br /&gt;Likewise, by treating the day-of-week effect on its own we can deploy methods targeted to capturing its important aspects. For instance, in our data we often see secular trends as well as annual seasonality in the amplitude of the weekly cycle i.e., long-term changes to the proportions of the weekly total going to each day of the week and intra-year cycles in these proportions. A model general enough to accommodate such phenomena, and everything else, along with the data to identify all relevant parameters is for us too much to ask. Such a model risks conflating important aspects, notably the growth trend, with other less critical aspects. &lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Disaggregation And Reconciliation&lt;/h2&gt;Apart from the cleaning and effect adjustments, we may decompose our overall problem for other reasons.  Earlier we mentioned that we often had an independent interest in subseries of a parent time series, such as disaggregated by locale, device type, operating system, and combinations of these. Yet even when we do not have an independent interest in the subseries, forecasting the constituent parts (and indeed the entire &lt;a href=&quot;https://www.otexts.org/fpp/9/4&quot;&gt;hierarchy&lt;/a&gt; that results from this type of disaggregation) and then reconciling all those forecasts in some manner to produce a final forecast of the parent time series is typically more accurate and robust than only forecasting the parent time series itself directly. This type of decomposition is quite literal --- we turn the task of forecasting one time series into that of forecasting many subseries and then reconciling them.&lt;br /&gt;&lt;br /&gt;Consider, for example, Global search queries. The first step, disaggregation, is quite natural. Search queries are logged with many attributes, and we typically use at least geography and device type. This aspect is easy. Next, forecasting the many subseries: Again, this is easy --- our forecast methodology is designed to be robust and automatic, so dramatically increasing the number of forecasts is not overly risky, either. Lastly, however, we need a method to take this collection of forecasts and reconcile them. This is more challenging.&lt;br /&gt;&lt;br /&gt;A simple solution is to forecast only at the bottom of the hierarchy and simply sum the forecasts to produce an overall parent forecast (and indeed forecasts all throughout the hierarchy.) This can be quite effective, as the important differences in growth profiles tend to arise across the geography-by-device-type boundaries, and forecasting them independently as a first step frees them.&lt;br /&gt;&lt;br /&gt;An obvious requisite property of reconciliation is arithmetic coherence across the hierarchy (which is implicit in the sum-up-from-the-bottom possibility in the previous paragraph), but more sophisticated reconciliation may induce statistical stability of the constituent forecasts and improve forecast accuracy across the hierarchy. While we use reconciliation methods tailored to our specific context, similar techniques have been implemented in the R package &lt;a href=&quot;https://cran.r-project.org/web/packages/hts/index.html&quot;&gt;hts&lt;/a&gt;&amp;nbsp;and written about in the literature.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Ensemble Methods&lt;/h2&gt;In keeping with our goals of robustness and automation, we turned to &lt;a href=&quot;https://en.wikipedia.org/wiki/Ensemble_learning&quot;&gt;ensemble methods&lt;/a&gt; to forecast growth trends. In a time series context, ensemble methods generally fit multiple forecast models and derive a final forecast from the ensemble, perhaps via a weighted average, in an attempt to produce better forecast accuracy than might result from any individual model.&lt;br /&gt;&lt;br /&gt;Though more sophisticated, more mathematically alluring options are available for producing a final forecast from an ensemble, such as Bayesian Model Averaging, we opt for simple approaches. The confluence of our beliefs about the world, underlying theoretical results, and empirical performance compel us toward deriving a final forecast from the ensemble via a simple measure of central tendency (i.e., a mean or median) after some effort to remove outliers (which could be as simple as trimming or winsorizing.) Crucially, our approach does not rely on model performance on holdout samples. Details follow, but first an exploration of why such an embarrassingly simple and seemingly ad hoc approach might do so well in this setting.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;h3&gt;&lt;span style=&quot;text-align: justify; vertical-align: baseline;&quot;&gt;Why do simple methods perform well? &lt;/span&gt;&lt;/h3&gt;When it comes to obtaining a final forecast from an ensemble, the following quote from Clemen [3] gives us the lay of a land not entirely welcoming to the mathematical statistician (emphasis added):&lt;br /&gt;&lt;br /&gt;&lt;i&gt;In many studies, the average of the individual forecasts has performed best or almost best. Statisticians interested in modeling forecasting systems may find this state of affairs &lt;b&gt;&lt;u&gt;frustrating&lt;/u&gt;&lt;/b&gt;. The questions that need to be answered are (1) why does the simple average work so well, and (2) under what conditions do other specific methods work better?&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;Clemen then homes in on a contradiction inherent in the pursuit of sophisticated combination strategies (emphasis added):&lt;br /&gt;&lt;br /&gt;&lt;i&gt;From a conventional forecasting point of view, using a combination of forecasts amounts to an admission that the forecaster is unable to build a properly specified model. Trying ever more elaborate combining models &lt;b&gt;&lt;u&gt;seems only to add insult of injury&lt;/u&gt;&lt;/b&gt;, as the more complicated combinations do not generally perform all that well.&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;With that context, consider an example inspired by Bates and Granger [4] and by [3]. Define two independent random variables  $X_1 \sim N(0, 1)$ and $X_2 \sim N(0, 2)$. For this simple vignette, we might regard $X_1$ and $X_2$ as errors from a measuring scale and note that $X_2$ is not as precise an instrument as $X_1$. Define&lt;/div&gt;&lt;div&gt;$$&lt;br /&gt;X_A = \frac{1}{2} X_1 + \frac{1}{2} X_2&lt;br /&gt;$$From basic theory, $X_A \sim ~ N(0, \frac{3}{4})$. In other words, the simple, unweighted average of these particular $X_1$ and $X_2$ has smaller variance than $X_1$ even though we combined $X_1$ in equal measure with a less precise instrument, $X_2$. Now, if the variances of $X_1$ and&amp;nbsp;$X_2$ were truly known to be $1$ and $2$, respectively, [4] would suggest we form $$&lt;br /&gt;X_C = k X_1 + (1 - k) X_2&lt;br /&gt;$$with $k = 2/3$ to minimize the variance of $X_C$. Thus $X_C \sim N(0, \frac{2}{3})$, which would be a superior instrument compared to $X_A$ with its variance of $\frac{3}{4}$.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;However, consider another scenario. Suppose $X_1$ and $X_2$ are independent as before, but now suppose we only know that one of $X_1$ and $X_2$ is distributed as $N(0, 1)$ and the other as $N(0, 2)$, and we do not know which is which. Thus with equal chance we might have $X_1 \sim N(0, 1)$ and $X_2 \sim N(0, 2)$ or &amp;nbsp;$X_1 \sim N(0, 2)$ and $X_2 \sim N(0, 1)$. Let’s again form $X_A$ and $X_C$ as before. $X_A$ is unchanged, regardless of how $X_1$ and $X_2$ are actually distributed, with $X_A \sim N(0, \frac{3}{4})$. If $X_1$ has the lower variance we once again have $X_C \sim N(0, \frac{2}{3})$. But if it is the case that $X_1 \sim N(0, 2)$ and $X_2 \sim N(0, 1)$, then the variance of $X_C$ would be $1$. This would be an inferior instrument compared to $X_A$ with its variance of $\frac{3}{4}$.&lt;br /&gt;&lt;br /&gt;Overall, when the weights are placed correctly $X_C$ is a slightly better instrument compared to $X_A$, but when the weights are placed incorrectly, $X_C$ is substantially inferior to $X_A$. In other words, there is an asymmetry of risk-reward when there exists the possibility of misspecifying the weights in $X_C$. Consequently, our confidence in departing from a simple, unweighted average of models in favor of preferential weighting should be shaped by how much we believe (or can demonstrate) that we can know the &lt;b&gt;future&lt;/b&gt; precision of the instruments we deploy, as well as our risk tolerance.&lt;br /&gt;&lt;br /&gt;In our forecasting routine, we face a similar conundrum: Do we believe (or can we demonstrate) that the performance of the models in the ensemble on what is often a short holdout period is sufficiently predictive of relative future forecast accuracy of the ensemble’s models to warrant a departure from a simple average based on equal weights? Relatedly, what do we believe about the process that generates the data we see? &lt;br /&gt;&lt;br /&gt;In our view, the data we observe are not from a deterministic casino game with known probabilities or from physical laws embedded in the fabric of the Universe. Our data arise from a complex, human process; our data depend on &quot;fads and fashions&quot;; our data are infused with all the psychology and contradiction that is human existence; our data will eventually reflect -- and more likely, soon reflect! -- unknowable unknowns, future ideas and inventions yet to be developed, novel passions and preferences; in short, our data are far from stationary. It is worth quoting Makridakis and Hibon [5] extensively here, for we share their view:&lt;br /&gt;&lt;br /&gt;&lt;i&gt;The reason for the anomalies between the theory and practice of forecasting is that real-life time series are not stationary while many of them also contain structural changes as fads, and fashions can change established patterns and affect existing relationships. Moreover, the randomness in such series is high as competitive actions and reactions cannot be accurately predicted and as unforeseen events (e.g., extreme weather conditions) affecting the series involved can and do occur. Finally, many time series are influenced by strong cycles of varying duration and lengths whose turning points cannot be predicted, making them behave like a random walk.&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;When the context in which the data arises changes, even a well-executed cross validation provides inadequate protection against overfitting. Instead of overfitting to the observed data in the usual sense, the risk is in overfitting to a context that has changed. In our view, this is fundamentally what makes our forecasting problem different from other prediction problems.&lt;br /&gt;&lt;br /&gt;For such reasons, we place strong prior belief on simple methods to convert the ensemble into a final forecast. The decision not to rely on performance in a holdout sample was no mere implementation convenience. Rather, it best reflects our beliefs about the state of the world -- “that real-life time series are not stationary” -- while also best conforming to our overarching goal of an automatic, robust forecasting routine that minimizes the chance of any catastrophically bad forecast.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;What’s in our ensemble?&lt;/h3&gt;So, what models do we include in our ensemble? Pretty much any reasonable model we can get our hands on! Specific models include variants on many well-known approaches, such as the &lt;a href=&quot;https://en.wikipedia.org/wiki/Bass_diffusion_model&quot;&gt;Bass Diffusion Model&lt;/a&gt;, the &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0169207000000662&quot;&gt;Theta Model&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Logistic_function&quot;&gt;Logistic models&lt;/a&gt;, &lt;a href=&quot;https://cran.r-project.org/web/packages/bsts/index.html&quot;&gt;bsts&lt;/a&gt;, &lt;a href=&quot;http://search.proquest.com/openview/cc5001e8a0978a6c029ae9a41af00f21/1&quot;&gt;STL&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Exponential_smoothing#Triple_exponential_smoothing&quot;&gt;Holt-Winters&lt;/a&gt; and other &lt;a href=&quot;https://en.wikipedia.org/wiki/Exponential_smoothing&quot;&gt;Exponential Smoothing&lt;/a&gt; models, Seasonal and other other &lt;a href=&quot;https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average&quot;&gt;ARIMA&lt;/a&gt;-based models, Year-over-Year growth models, custom models, and more. Indeed, model diversity is a specific objective in creating our ensemble as it is essential to the success of model averaging. Our aspiration is that the models will produce something akin to a representative and not overly repetitive covering of the space of reasonable models. Further, by using well-known, well-vetted models, we attempt to create not merely a &quot;&lt;a href=&quot;https://en.wikipedia.org/wiki/The_Wisdom_of_Crowds&quot;&gt;wisdom of crowds&lt;/a&gt;&quot; but a &quot;wisdom of crowds of experts&quot; scenario, in the spirit of Mannes [6]. &lt;br /&gt;&lt;br /&gt;As an implementation detail, though, we provide flags in our code so that an analyst could abandon the default settings and force any particular model to be in or out of the ensemble. This imbues the forecasting routine with two attractive properties. First, an analyst who wishes to add a new model to the ensemble can do so with little risk of degrading the performance of the ensemble as a whole. In fact, the expectation is that a good-faith addition of a new model will do no harm and may improve the overall performance of a final forecast from the ensemble. Such future-proofing is a virtue. Second, if an analyst is insistent on running only their own new model it will now sit in the middle of the entire production pipeline that will handle all the adjustments previously discussed. In other words, their new model will not be at a disadvantage in terms of the adjustments.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h3&gt;Selection and aggregation: from ensemble to final forecast&lt;/h3&gt;For each week of the forecast horizon, we convert the models into a final forecast for that week. Though more sophisticated possibilities exist, in our initial approach we treat the collection of forecasts in the ensemble for a given week independently of other weeks. We then compute a simple trimmed mean of all the models passing some basic sanity checks. Since there is always the risk of an absurd forecast (e.g., going to zero or zooming off toward infinity) from any member of the ensemble, due to numerical problems or something else, we recommend an approach with some form of outlier removal. With such an approach, it is not uncommon for the final forecast to achieve predictive accuracy superior to any individual model. Over the many time series we forecast it is typically at or near the top -- delivering the type of robustness we desire.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Prediction Intervals&lt;/h2&gt;A statistical forecasting system should not lack uncertainty quantification. In addition, there is often interest in probabilistic estimates of tail events, i.e., how high could the forecast reasonably go? In our setup some of our previous choices, notably ensembling, break any notion of a closed-form prediction interval. Instead, we use simulation-based methods to produce prediction intervals. &lt;br /&gt;&lt;br /&gt;Our approach (illustrated below) uses historical k-week-out errors to drive the simulations. In a simple case the one-week-out errors suffice, and the simulation proceeds iteratively: forecast one week forward, perturb this forecast with a draw from the error distribution, and repeat 52 times for a one-year realization; then do this, say, $1000$ or $10,000$ times and take quantiles of the realizations as uncertainty estimates. The process is inherently parallel: Each realization can be produced independently by a different worker machine. Empirical coverage checks are recommended, of course, as is assessment of any autocorrelation in the k-week-out errors.&lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;img height=&quot;230&quot; src=&quot;https://lh3.googleusercontent.com/YqsFWE_Lgy750INBFz7hMRPCdyu2WraVWsYHb3F4ajadhkfo-AerrYg09SEDKyMs13XUAnPqlrAhd6FxdOKyHi0G49MVoX8TwBQMC7FNrv7uTHw-j593LBRheVbD7j9TB708vptb&quot; style=&quot;margin-left: auto; margin-right: auto;&quot; width=&quot;640&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Simulating prediction errors&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;While producing prediction intervals is computationally intensive, the Google environment features abundant, fast parallel computing. This shaped the evolution of our approach, making possible and selecting out as advantageous such a computationally intensive technique. Please see [7]&amp;nbsp;for additional details.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h2&gt;Example: Turkish Electricity data&lt;/h2&gt;To better illuminate our forecasting routine, we provide an example based on Turkish electricity data presented in De Livera [8] and available as a .csv file &lt;a href=&quot;http://robjhyndman.com/data/turkey_elec.csv&quot;&gt;here&lt;/a&gt;. Below are daily and weekly totals of electricity demand (in MV) in Turkey from the start of 2000 through the end of 2008. As noted by [8], these data exhibit complex, multiple, and non-nested seasonal patterns owing to two annual seasonalities induced by the Islamic and Gregorian calendars as well as a weekly cycle that differs during the year. There is also apparent long-term trend and holiday impacts, and in principle these and the aforementioned seasonal patterns could be changing over time.&lt;br /&gt;&lt;br /&gt;&lt;img height=&quot;426&quot; src=&quot;https://lh6.googleusercontent.com/Gr4xBQ6AgTOReou_izfUl1zeNuhcyl90KbCXUQcztSA7lKavk5Ucz9DnDuQ6o56xpQBHqFVlQlgqMDcKzfJQAZUciyDjn-ww_zV8kGVkDsc4TDo2m7X5DRSpDakmsKBqixeiTN2C&quot; width=&quot;640&quot; /&gt;&lt;br /&gt;&lt;br /&gt;&lt;img height=&quot;426&quot; src=&quot;https://lh6.googleusercontent.com/OSwjPVzOpl3Q6eKQLkLYFca1kaYe90CfdB0E8WLtDANpXTQUH3oq3hP6jYcC6YsRlK8LyhbZHNIexwZfsI61v5FQfy9GP4-JYN8wNaV3ikcB0OaJ-B9SPgZHSBDx4Chyv6X5FCA1&quot; width=&quot;640&quot; /&gt;&lt;br /&gt;&lt;br /&gt;We forecast this time series from the middle of 2006 through the end of the data, for a 30-month forecast horizon. Our procedure first cleans the daily actuals as described above and then estimates holiday effects (based on a human-curated list). The two plots below show daily and weekly cleaned and de-holidayed values. After that, we conclude our forecast preparation by switching to weekly totals and accounting for seasonality effects (quantified at the bottom of this post). &lt;br /&gt;&lt;br /&gt;&lt;img height=&quot;424&quot; src=&quot;https://lh3.googleusercontent.com/kp0i9z7R9PazY2KUFC8Ar_4iE4SFoFEiyv2l80Yp2q6QpkspYjOPusLWYqyALq_X7GUlyuJqsw-V6H3Dohay6leecm96W_HiHWj1gXA2iNRXhIS35lxqxBYeTZqxk1gglGq-thQg&quot; width=&quot;640&quot; /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;img height=&quot;418&quot; src=&quot;https://lh3.googleusercontent.com/kq1k95XyrdMu5f1RfbYTaKGgmkX4SPJqdyXWrbIKWGjr_BfqP7MCDmucUuE9UnkplBs-o5PZIZD8U73E2S_Z2xlrWjz3JX5CaGleYV9GO9fC_aKXnmAr9P3lqw_uPCNYBA8sZDKf&quot; width=&quot;640&quot; /&gt;&lt;br /&gt;&lt;br /&gt;We then fit the models in our ensemble to the cleaned, de-holidayed, de-seasoned weekly time series. Once the individual models in our ensemble are fit to the time series, we can display the resulting forecasts from every model in the ensemble in the spaghetti-like plot below. The thicker, red line is the final forecast based on our selection and aggregation method while the other lines are the forecasts from individual models in the ensemble. The hope is that the diverse array of models will &#39;bracket the truth&#39;, in the sense described by Larrick and Soll (2006), leading to a better and more robust final forecast.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;img height=&quot;426&quot; src=&quot;https://lh6.googleusercontent.com/st2LEgT_jVD8y1Fw16OOWSCVnhSftVqNM5kSOLPjwOlX2J4CKcn_I2hFTN6PsfkfS4UBllJClsxZ47rXkCaOItmAh8-g2j0kCpBxzcPB55gOKIvCsadQK3vxMpn8GlM3bXHDvtRp&quot; width=&quot;640&quot; /&gt;&lt;br /&gt;&lt;br /&gt;After converting our ensemble to a final forecast for each week of the forecast horizon, we re-season the data, distribute the weekly totals to the constituent days of the week for every week, and re-holiday the resulting daily values. From these we output final daily and weekly forecasts, as depicted below. &lt;br /&gt;&lt;br /&gt;The plot below shows the entire time series and weekly forecasts for a 30-month forecast horizon starting in mid-2006.&lt;br /&gt;&lt;br /&gt;&lt;img height=&quot;426&quot; src=&quot;https://lh6.googleusercontent.com/a4ntrWNn74MOuh_KIVAx5tIXGSZTlIbxLi1QrcXdq2OUtSBcQP-SdVuAmrxbu9W5jkWm6FTyI4T7cEQwO_ykLQFuIXhq-dv88N5z0JNb7pwoqPRh0HMYPOy7IXKNOp2A0LrNg2ov&quot; width=&quot;640&quot; /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;This next plot, also weekly, zooms in to focus on only the forecast period. This plot reveals reasonable forecast performance until about September 2008, ostensibly due to&amp;nbsp;&lt;a href=&quot;https://fred.stlouisfed.org/series/NAEXKP01TRQ652S&quot;&gt;the impact in Turkey&lt;/a&gt;&amp;nbsp;of the global&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/Great_Recession&quot;&gt;Great Recession of 2008&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;&lt;img height=&quot;426&quot; src=&quot;https://lh5.googleusercontent.com/7XWe3ZV-K1g4_zESZKKLQi23hi2cPj7s1HgGy_rg7Cn4p6LHcRQjAHhM8bJPDwrIoqJV5tlVdbNFjfZaw4LBC_jzbHuxTlIvaDYnKSOUE9vES3kWjWFKUReZlFOQfqFgFDlFLUXz&quot; width=&quot;640&quot; /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;Now we show the daily forecast and observed data for a three-month period in mid-2008, covering a period 21 to 24 months into the forecast horizon.&lt;br /&gt;&lt;br /&gt;&lt;img height=&quot;426&quot; src=&quot;https://lh3.googleusercontent.com/0dOPM4wOD1ZOo0GYBihr2nWBTBwEKf_LVfd0t2THzeMZXlKygRQWb3tvtH5jV-LCtwxcACFObH2BHtBpmcLOsc3q8UnakZX2UzsyQMxKXScLez_Er68rnbBP31CbWUPkXhkgJYxC&quot; width=&quot;640&quot; /&gt;&lt;br /&gt;&lt;br /&gt;Before the onset of the Great Recession’s impacts, the interquartile range of the percentage errors for the weekly forecast was 2.7%, from -1.2% to 1.5%. Corresponding numbers for the daily forecast were 3.2%, from -1.6% to 1.6%. The 10th and 90th percentiles were -3.5% and 2.8% for the weekly forecast, and -3.9% and 3.3% for the daily forecast. The median of the weekly percentage errors was 0.1% and was the same (0.1%) for the daily percentage errors. Absolute Percentage Errors told a similar story. Overall, the&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_absolute_percentage_error&quot;&gt;Mean Absolute Percentage Errors&lt;/a&gt;&amp;nbsp;were 1.9% for the weekly forecast and 2.3% for the daily forecast.&lt;br /&gt;&lt;br /&gt;Beyond the forecasts, our routine produces ancillary information with business relevance. For instance, immediately below we show the estimated holiday impacts for Eid al-Adha and Eid al-Fitr over a span of about 14 days surrounding the particular holiday (anchored about a &#39;zero&#39; day on the horizontal axis.) Farther below, we show annual estimated seasonality (weekly resolution), past and forecast. Both estimated holiday impacts and seasonal adjustment factors are expressed on relative scales. While important to business understanding in their own right, it is worth saying one last time that doing a good job estimating seasonal and holiday effects reduces the burden on the models in the ensemble, helping them to better identify long-term growth trends. &lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;img height=&quot;158&quot; src=&quot;https://lh5.googleusercontent.com/5Mes4_hqbvXHAmfxQdNHY3SoV4cUUqKolqlOUF5SJJQb4He1W8WrK87q61ANNhIhorffQCHdrMPJ2nldTJ_Jmb0kqdSIBtJspQu4HSnV-uO-1l9OaIB6YWplnAQo6v4gGCosXeuN&quot; width=&quot;640&quot; /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;img height=&quot;384&quot; src=&quot;https://lh5.googleusercontent.com/G8GKQbauH1XAee374ogqQf8sXnpT1Fr-L9I1I9AWoCwX1zP52g5lfws4hRzk37wfk0zX1-pOBxjZIiQToHfOqngJ--Q-v7VBycbTy0DqDQYcNEeCJQle736PA5a5qS42EbZOrTJu&quot; width=&quot;640&quot; /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Conclusion&lt;/h2&gt;&lt;/div&gt;&lt;div&gt;To summarize, key to our own attempt at a robust, automatic forecasting methodology was to divide and (hopefully) conquer where possible as well as to implement ensemble methods in accord with our risk tolerance and our beliefs about the data generating mechanism. But as anyone involved real-world forecasting knows, this is a complex space. We are eager to hear from practicing data scientists about their forecasting challenges --- how they might be similar to or differ from our problem. We hope this blog post is the start of such a dialogue.&lt;/div&gt;&lt;div&gt;&lt;span id=&quot;docs-internal-guid-24f75ce0-7abf-827b-0ce4-0df55867e529&quot;&gt;&lt;/span&gt;&lt;br /&gt;&lt;span id=&quot;docs-internal-guid-24f75ce0-7abf-827b-0ce4-0df55867e529&quot;&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;h3&gt;References&lt;/h3&gt;[1] Hyndman, Rob J., and George Athanasopoulos. Forecasting: principles and practice. OTexts, 2014. &lt;a href=&quot;http://otexts.org/fpp/&quot;&gt;http://otexts.org/fpp/&lt;/a&gt;. Accessed on 20 March 2017. Specifically, see &quot;1.4 Forecasting data and methods&quot;.&lt;br /&gt;&lt;br /&gt;[2] Cleveland, Robert B., William S. Cleveland, and Irma Terpenning. &quot;STL: A seasonal-trend decomposition procedure based on loess.&quot; Journal of Official Statistics 6.1 (1990): 3.&lt;br /&gt;&lt;br /&gt;[3] Clemen, Robert T. &quot;Combining forecasts: A review and annotated bibliography.&quot; International journal of forecasting 5.4 (1989): 559-583&lt;br /&gt;&lt;br /&gt;[4] Bates, John M., and Clive WJ Granger. &quot;The combination of forecasts.&quot; Journal of the Operational Research Society 20.4 (1969): 451-468.&lt;br /&gt;&lt;br /&gt;[5] Makridakis, Spyros, and Michele Hibon. &quot;The M3-Competition: results, conclusions and implications.&quot; International journal of forecasting 16.4 (2000): 451-476.&lt;br /&gt;&lt;br /&gt;[6] Mannes, Albert E., Jack B. Soll, and Richard P. Larrick. &quot;The wisdom of select crowds.&quot; Journal of personality and social psychology 107.2 (2014): 276.&lt;br /&gt;&lt;br /&gt;[7] Murray Stokely, Farzan Rohani, and Eric Tassone. “&lt;a href=&quot;https://research.google.com/pubs/pub37483.html&quot;&gt;Large-Scale Parallel Statistical Forecasting Computations in R&lt;/a&gt;”, Google Research report.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;[8] De Livera, Alysha M., Rob J. Hyndman, and Ralph D. Snyder. &quot;Forecasting time series with complex seasonal patterns using exponential smoothing.&quot; Journal of the American Statistical Association 106.496 (2011): 1513-1527.&lt;br /&gt;&lt;br /&gt;[9] Larrick, Richard P., and Jack B. Soll. &quot;Intuitions about combining opinions: Misappreciation of the averaging principle.&quot; Management science 52.1 (2006): 111-127. APA&lt;br /&gt;&lt;div&gt;&lt;span id=&quot;docs-internal-guid-24f75ce0-7a80-671c-7149-d5778c8fc95f&quot;&gt;&lt;/span&gt;&lt;br /&gt;&lt;div&gt;&lt;span id=&quot;docs-internal-guid-24f75ce0-7a80-671c-7149-d5778c8fc95f&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;roboto&amp;quot;; font-size: 10pt; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;span id=&quot;docs-internal-guid-24f75ce0-7a80-671c-7149-d5778c8fc95f&quot;&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;</content><link rel='replies' type='application/atom+xml' href='https://www.unofficialgoogledatascience.com/feeds/4732004365167765860/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='https://www.unofficialgoogledatascience.com/2017/04/our-quest-for-robust-time-series.html#comment-form' title='18 Comments'/><link rel='edit' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/4732004365167765860'/><link rel='self' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/4732004365167765860'/><link rel='alternate' type='text/html' href='https://www.unofficialgoogledatascience.com/2017/04/our-quest-for-robust-time-series.html' title='Our quest for robust time series forecasting at scale'/><author><name>Amir Najmi</name><uri>http://www.blogger.com/profile/18174523203317227640</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://lh6.googleusercontent.com/Vrd24K6K6UMR_QZuk1SV-PKSLG6PPLL3bbXaTsEDlmRSGFUrm2h9B5UlDsNbDT7z81hCr4kSmVoaq9W2PoNM2iyuHljlyQdkvqsN4BIEFVyJX3kJwQwJ1wa4MzuArXTFTEGqSC-n=s72-c" height="72" width="72"/><thr:total>18</thr:total></entry><entry><id>tag:blogger.com,1999:blog-5825758052688213474.post-3311282216595193329</id><published>2017-03-13T13:54:00.000-07:00</published><updated>2017-03-16T11:35:33.881-07:00</updated><title type='text'>Attributing a deep network’s prediction to its input features</title><content type='html'>By MUKUND SUNDARARAJAN, ANKUR TALY, QIQI YAN&lt;i&gt;&lt;br /&gt;&lt;br /&gt;&lt;/i&gt;&lt;br /&gt;&lt;div&gt;&lt;i&gt;&lt;i&gt;&lt;b&gt;&lt;u&gt;Editor&#39;s note:&lt;/u&gt; &lt;/b&gt;Causal inference is central to answering questions in science, engineering and business and hence the topic has received particular attention on this blog. Typically, causal inference in data science is framed in probabilistic terms, where there is statistical uncertainty in the outcomes as well as model uncertainty about the true causal mechanism connecting inputs and outputs. And yet even when the relationship between inputs and outputs is fully known and entirely deterministic, causal inference is far from obvious for a complex system. In this post, we explore causal inference in this setting via the problem of attribution in deep networks. This investigation has practical as well as philosophical implications for causal inference. On the other hand, if you just care about understanding what a deep network is doing, this post is for you too.&lt;/i&gt;&lt;/i&gt;&lt;br /&gt;&lt;i&gt;&lt;br /&gt;&lt;/i&gt;&lt;i&gt;&lt;br /&gt;&lt;/i&gt;Deep networks have had remarkable success in variety of tasks. For instance, they identify objects in images, perform language translation, enable web search — all with surprising accuracy. Can we improve our understanding of these methods? Deep networks are the latest instrument in our large toolbox of modeling techniques, and it is natural to wonder about their limits and capabilities. Based on&amp;nbsp;&lt;a href=&quot;https://arxiv.org/abs/1703.01365&quot;&gt;our paper&lt;/a&gt;&amp;nbsp;[4],&amp;nbsp;this post is motivated primarily by intellectual curiosity.&lt;br /&gt;&lt;div&gt;&lt;br /&gt;Of course, there is benefit to an improved understanding of deep networks beyond the satisfaction of curiosity — developers can use this to debug and improve models; end-users can understand the cause of a model’s prediction, and develop trust in the model. As an example of the latter, suppose that a deep network was used to predict an illness based on an image (from an X-ray, or an MRI, or some other imaging technology). It would be very helpful for a doctor to examine which pixels led to a positive prediction, and cross-check this with her intuition.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;We are all familiar with linear and logistic regression models. If we were curious about such a model’s prediction for a given input, we would simply inspect the weights (model coefficients) of the features present in the input. The top few features with the largest weight (i.e., coefficient times feature value) would be indicative of what the model deemed noteworthy.&lt;br /&gt;&lt;br /&gt;The goal of this post is to mimic this inspection process for deep models. &lt;b&gt;Can we identify what parts of the input the deep network finds noteworthy?&lt;/b&gt; As we soon discuss, the nonlinearity of deep networks makes this problem challenging. The outline of this post is as follows:&lt;/div&gt;&lt;div&gt;&lt;ul&gt;&lt;li&gt;we introduce a natural approach to attribution based on gradients&lt;/li&gt;&lt;li&gt;use failures of the gradient approach to guide the design of our method&lt;/li&gt;&lt;li&gt;present our method more formally and discuss its properties&lt;/li&gt;&lt;li&gt;describe applications to networks other than Inception/ImageNet (our running example)&amp;nbsp;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;div&gt;We&amp;nbsp;conclude&amp;nbsp;with some areas for future work.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;h2&gt;Feature Importance via Gradients&lt;/h2&gt;&lt;h4&gt;Feature attribution for (generalized) linear models&lt;/h4&gt;Suppose that our model is linear. Then, there is a simple, commonly followed practice to identify the importance of features — examine the coefficients of the features present in the input, weighted by the values of these features in the input. (One can think of categorical features as having values in $\{0,1\}$.) A summation of the resulting vector would equal the prediction score less the intercept term and so this process accounts for the entire prediction. If instead of summing, we sorted this vector in decreasing sequence of magnitude, we would identify the features that the model finds important. Occasionally, we may find that the coefficients don’t match our intuition of what is important. We may then check for overfitting, or for biases in the training data, and fix these issues. Or we find may that some of the features are correlated, and the strange coefficients are an artifact thereof. In either case, this process is integral to improving the network or trusting its prediction. Let us now attempt to mimic this process for deep networks.&lt;/div&gt;&lt;div&gt;&lt;h4&gt;The Inception architecture and ImageNet&lt;/h4&gt;For concreteness, let us focus on a network that performs object recognition. We consider a deep network using the &lt;a href=&quot;https://arxiv.org/abs/1602.07261&quot;&gt;Inception&lt;/a&gt;&amp;nbsp;[1] architecture trained on the &lt;a href=&quot;http://www.image-net.org/&quot;&gt;ImageNet&lt;/a&gt; dataset. It takes an image as input and assigns scores for 1000 different &lt;a href=&quot;http://image-net.org/challenges/LSVRC/2014/browse-synsets&quot;&gt;ImageNet categories&lt;/a&gt;. The input is specified via the R,G,B values of the pixels of the image. At the output, the network produces a score (probability) for each label using a multinomial logit (Softmax) function. The network “thinks” that objects with large output scores are probably present in the image. For instance, here is an image and its top few labels:&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;img height=&quot;289&quot; src=&quot;https://lh3.googleusercontent.com/OdmC9gir5TIFrk39uuxI92pK4FaXzLyAAi6PO1sLdTJTaR-mPdbdB6owk2IaTfCN3MWHhK7HVAIaVyXZ2Gyzm75bN8QukZFTEA7EZpb20oP9B-M4AeMTY1E3PWsuMDKnTPq3QkJ6&quot; width=&quot;640&quot; /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Notice that the score for the top label, “fireboat”, is very close to 1.0, indicating that the network is very sure that there is a “fireboat” somewhere in the image. The network is absolutely right in this case — a fireboat is a special boat used to fight fires on shorelines and aboard ships.&lt;/div&gt;&lt;div&gt;&lt;h4&gt;Applying Gradients to Inception&lt;/h4&gt;Which pixels made the network think of this as a fireboat? We cannot just examine the coefficients of the model as we do with linear models.&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/Deep_learning&quot;&gt;Deep networks&lt;/a&gt; have multiple layers of  logic and coefficients, combined using nonlinear &lt;a href=&quot;https://en.wikipedia.org/wiki/Activation_function&quot;&gt;activation functions&lt;/a&gt;. For instance, the Inception architecture has 22 layers. The coefficients of the input layer do not adequately cover the logic of the network. In contrast, the coefficients of the hidden layers aren’t in any human intelligible space. &lt;br /&gt;&lt;br /&gt;Instead, we could use the gradients of the output with respect to the input — if our deep network were linear, this would coincide exactly with the process for linear models, because the gradients correspond to the model coefficients. In effect, we are using a local linear approximation of the (nonlinear) deep network. This approach has been applied to deep networks in previous literature.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;Let us see how this does. We are going to inspect the  gradient of the score for the object “fireboat” with respect to the input, multiplied point-wise by the input itself (essentially, a Taylor approximation of the prediction function at the input). The result is a matrix that has three dimensions. Two of these correspond to the height and width of the image, and the third is for the primary color (R, G, or B).&lt;br /&gt;&lt;h4&gt;A note on visualization&lt;/h4&gt;The most convenient way to inspect our feature importances (attributions) is to visualize them. We do this by using the attributions as a (soft) window over the image itself. We construct the window by first removing the primary color dimension from the attributions, by taking the sum of  absolute value of the R, G, B values. To window the image, we take an element-wise product of the window with the pixel values and visualize the resulting image. The result is that unimportant pixels are dimmed. Our &lt;a href=&quot;https://github.com/ankurtaly/Attributions/blob/master/attributions.ipynb&quot;&gt;code&lt;/a&gt; has details (there are probably other reasonable visualization approaches that work just as well). The visualization of the gradients for the “fireboat” image looks like this:&lt;br /&gt;&lt;br /&gt;&lt;img height=&quot;320&quot; src=&quot;https://lh6.googleusercontent.com/J09LPNlCY3zp4cR3dTtKIf9YWv5honnvpu9GvZy7l6jodrTmLLm8Hg0WQBVqL-kOXRA-kTFXso_WqGE8Z1yWFn25A1dHWM0UFkzifOqYfIkkOuiEYJfg09z0scLuzDJKHhrzKiWp&quot; width=&quot;320&quot; /&gt;&lt;br /&gt;&lt;br /&gt;Unfortunately, gradients highlight pixels below the bridge which seem completely irrelevant to the “fireboat” prediction. This is unlikely to be a model bug — recall that the prediction was  correct. So what is happening?&lt;/div&gt;&lt;div&gt;&lt;br /&gt;It turns out that our local linear approximation does a poor job of indicating what the network thinks is important. The prediction function flattens in the vicinity of the input, and consequently, the gradient of the prediction function with respect to the input is tiny in the vicinity of the input vector. The dot product of the gradient with the image, which represents a first-order Taylor approximation of the prediction function at the input, adds up to only $4.6 \times 10^{-5}$ (while the actual value of the prediction is $0.999961$ — gradients aren’t accounting for a large portion of the score.&lt;br /&gt;&lt;br /&gt;A simple analysis substantiates this. We construct a series of images by scaling down the pixel intensities from the actual image to zero (black). Call this scaling parameter $\alpha$. One can see that the prediction function flattens after $\alpha$ crosses $0.2$.&lt;img height=&quot;275&quot; src=&quot;https://lh5.googleusercontent.com/faUu3mDx2KZqUVunkHrJSSpwD1koGOd6j9ll64gYCw3bGOyymufRpoXYXAJQtcjFBDumnJMBsq3LUTePHC-VynH_TAKTm__qFbTSI4tglOGZMA8aht7Kw_Y3Edw5bUKlgfgCIxkT&quot; width=&quot;400&quot; /&gt;&lt;br /&gt;&lt;br /&gt;This phenomenon of flattening is specific neither to this label (“fireboat”), this image, the output neuron, or nor even to this network. It has been observed by other work [2] and our previous paper [3].&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;h2&gt;Our method: Integrated Gradients&lt;/h2&gt;The same plot that demonstrates why gradients don’t work also tells us how to fix the issue. Notice that there is a large jump in the prediction score at low intensities. Perhaps it is useful to inspect the gradients of those images. The figure below visualizes these gradients, visualized with the same logic as in the previous section;  these are just the gradients of the original image at different levels of brightness.&lt;br /&gt;&lt;br /&gt;&lt;img height=&quot;212&quot; src=&quot;https://lh3.googleusercontent.com/zivMJ35RijWrGFQIjitWZo_TguhcoYxSVdhyB0gZpRE0R8x0MZ7-XSIEdz0lGiKdOcwM8Q9WgruqvxEfrwzNoYWvLBfN3LjIGkw6qBvaDk8QT1TrQvoVjCtXlPuRVvbKftoZU3Og&quot; width=&quot;640&quot; /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;img height=&quot;148&quot; src=&quot;https://lh3.googleusercontent.com/rUxCx7wNiVbcgLjLGl-4uZzwx41sKtMCpfNHuy7rT9PSOlhVEhjqjyWQ-JkV74YPPS8KZVJZgpGX0oP78Jss-LazUmf7KYiNuVWDV8XMi-9S8HlHlZ_wJvaThkXdXO-jIaoYwKwc&quot; width=&quot;640&quot; /&gt;&lt;br /&gt;&lt;img height=&quot;148&quot; src=&quot;https://lh4.googleusercontent.com/I7hEI-cCfB7Ck81tSUsN4itQeX3N06R1PoS_ziwYMCo6uxEfhrS_uu3leWv331McByaKoQfJPibT4KfiWuClkb6pelHH3lVZw_CfEH8qtdNCyjyQlRAvP4Kp3V3J4kWP-k7yKPef&quot; width=&quot;640&quot; /&gt;&lt;img height=&quot;149&quot; src=&quot;https://lh3.googleusercontent.com/W6hWPcQg-cgJRehxMPQldqa5wh57W5qHT_rSjNeCivdLnci-1kSvMYoV_1pFjlsMZiLr-aSDLH0rPzzcnWETx28jqBy13LBFIHLjyMbfjmAnS0eTLEk5KojVxZh4XX5xtGdnR0EC&quot; width=&quot;640&quot; /&gt;&lt;br /&gt;&lt;br /&gt;The visualizations show that at lower values of the scaling parameter $\alpha$, the pixels constituting the fireboat and spout of water are most important. But as $\alpha$ increases, the region around the fireboat (rather than the fireboat itself) gains relative importance. As the plot shows, the visualizations corresponding to lower values of the scale parameter are more important to the score because they have higher gradient magnitudes. This does not come through in our visualizations because they are each normalized for brightness; if they weren’t, the last few images would look nearly black. By summing the gradients across the images, and then visualizing this, we get a more realistic picture of what is going on.&lt;br /&gt;&lt;br /&gt;&lt;img height=&quot;320&quot; src=&quot;https://lh4.googleusercontent.com/QNKn83Z1Aa2-CSIUmklDdWxJvN56gMKGAz3v-FXoQbxZ1gc9Mq930vLwTjxiaAG6nEKRUbjSZpvxkFePTki3s9YMB96tgcG2uB31sikwOV0W3GyeZJxSv-pYRvxHxNVrszrgW4WM&quot; width=&quot;320&quot; /&gt;&lt;br /&gt;&lt;br /&gt;This is the essence of our method. We call this method &quot;integrated gradients&quot;. Informally, we average the gradients of the set of scaled images and then take the element-wise product of this with the original image. Formally, this approximates a certain integral as we will see later.&lt;br /&gt;&lt;br /&gt;The complete code for loading the Inception model and visualizing the attributions is available from&amp;nbsp;&lt;a href=&quot;https://github.com/ankurtaly/Attributions&quot;&gt;GitHub repository&lt;/a&gt;. The code is packaged as a single IPython notebook with less than 70 lines of Python TensorFlow code. Instructions for running the notebook are provided in our &lt;a href=&quot;https://github.com/ankurtaly/Attributions/blob/master/README.md&quot;&gt;README&lt;/a&gt;. Below is the key method for generating integrated gradients for a given image and label. It involves scaling the image and invoking the gradient operation on the scaled images:&lt;/div&gt;&lt;div&gt;&lt;i&gt;&lt;br /&gt;&lt;/i&gt;&lt;/div&gt;&lt;div&gt;&lt;i&gt;&lt;span id=&quot;docs-internal-guid-e64e2514-c8b3-5fd8-df5e-cc715b393176&quot;&gt;&lt;/span&gt;&lt;/i&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;i&gt;&lt;span id=&quot;docs-internal-guid-e64e2514-c8b3-5fd8-df5e-cc715b393176&quot;&gt;&lt;span style=&quot;background-color: transparent; color: #38761d; font-family: &amp;quot;courier new&amp;quot;; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;def&lt;/span&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;background-color: transparent; color: blue; font-family: &amp;quot;courier new&amp;quot;; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;integrated_gradients&lt;/span&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;(img, label, steps=50):&lt;/span&gt;&lt;/span&gt;&lt;/i&gt;&lt;/div&gt;&lt;span id=&quot;docs-internal-guid-e64e2514-c8b3-5fd8-df5e-cc715b393176&quot;&gt;&lt;/span&gt;&lt;div dir=&quot;ltr&quot; style=&quot;font-style: italic; line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span id=&quot;docs-internal-guid-e64e2514-c8b3-5fd8-df5e-cc715b393176&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&#39;&#39;&#39;&lt;/span&gt;&lt;span style=&quot;background-color: transparent; color: #cc0000; font-family: &amp;quot;courier new&amp;quot;; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;Returns attributions for the prediction label based&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;span id=&quot;docs-internal-guid-e64e2514-c8b3-5fd8-df5e-cc715b393176&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;font-style: italic; line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: #cc0000; font-family: &amp;quot;courier new&amp;quot;; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;on integrated gradients at the image.&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;font-style: italic; line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;b id=&quot;docs-internal-guid-e64e2514-c8b4-1971-8674-4c1aac33a82d&quot; style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;font-style: italic; line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: #cc0000; font-family: &amp;quot;courier new&amp;quot;; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Specifically, the method returns the dot product of the image&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;font-style: italic; line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: #cc0000; font-family: &amp;quot;courier new&amp;quot;; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;and the average of the gradients of the prediction label (w.r.t.&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;font-style: italic; line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: #cc0000; font-family: &amp;quot;courier new&amp;quot;; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;the image) at uniformly spaced scalings of the provided image.&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;font-style: italic; line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;b style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;font-style: italic; line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: #cc0000; font-family: &amp;quot;courier new&amp;quot;; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;The provided image must of shape (224, 224, 3), which is &lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;font-style: italic; line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: #cc0000; font-family: &amp;quot;courier new&amp;quot;; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;also the shape of the returned attributions tensor.&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;font-style: italic; line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&#39;&#39;&#39;&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;font-style: italic; line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;b style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;font-style: italic; line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: #e69138; font-family: &amp;quot;courier new&amp;quot;; font-size: 10pt; font-style: italic; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;# Obtain the tensor representing the softmax output of the provided label.&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;font-style: italic; line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;t_output = output_label_tensor(label) &lt;/span&gt;&lt;span style=&quot;background-color: transparent; color: #e69138; font-family: &amp;quot;courier new&amp;quot;; font-size: 10pt; font-style: italic; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;# shape: scalar&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;font-style: italic; line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;t_grad = tf.gradients(t_output, T(&#39;&lt;/span&gt;&lt;span style=&quot;background-color: transparent; color: #cc0000; font-family: &amp;quot;courier new&amp;quot;; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;input&lt;/span&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&#39;))[0]&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;font-style: italic; line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;b style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;font-style: italic; line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;scaled_images = [(float(i)/steps)*img &lt;/span&gt;&lt;span style=&quot;background-color: transparent; color: #38761d; font-family: &amp;quot;courier new&amp;quot;; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;for&lt;/span&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; i &lt;/span&gt;&lt;span style=&quot;background-color: transparent; color: #38761d; font-family: &amp;quot;courier new&amp;quot;; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;in&lt;/span&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; range(1, steps+1)]&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;font-style: italic; line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: #e69138; font-family: &amp;quot;courier new&amp;quot;; font-size: 10pt; font-style: italic; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;# Compute the gradients of the scaled images&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;font-style: italic; line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;grads = run_network(sess, t_grad, scaled_images)&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;font-style: italic; line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;b style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;font-style: italic; line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: #e69138; font-family: &amp;quot;courier new&amp;quot;; font-size: 10pt; font-style: italic; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;# Average the gradients of the scaled images and dot product with the original&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;font-style: italic; line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: #e69138; font-family: &amp;quot;courier new&amp;quot;; font-size: 10pt; font-style: italic; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;# image&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot;; font-size: 10pt; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&lt;/span&gt;&lt;span style=&quot;color: #38761d; font-family: &amp;quot;courier new&amp;quot;; font-size: 10pt; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;return&lt;/span&gt;&lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot;; font-size: 10pt; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; img*np.average(grads, axis=0)&lt;/span&gt;&lt;/div&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;The following figure shows some more visualizations of integrated gradients. Our visualization logic is identical to that of the gradient approach. For comparison, we also show the visualization for the gradient approach. From the visualizations, it is evident that the integrated gradients are better at capturing important features.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: white; color: #212121; font-family: &amp;quot;roboto&amp;quot;; font-size: 10pt; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;img height=&quot;19&quot; src=&quot;https://lh5.googleusercontent.com/no8tjBZXv-aC8vigd7tCuA8L5dp9EXAnQzzm3vZsfMRoetOa89ZOq9E1gYEqcx1DLGwJbzmPuU7oYVmBFzEJtiFFip6SykW1QnK7BVjaPaa99q-ftsrY5ZeIXzXs8vAS9nB5B7-2&quot; style=&quot;-webkit-transform: rotate(0.00rad); border: none; transform: rotate(0.00rad);&quot; width=&quot;624&quot; /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: white; color: #212121; font-family: &amp;quot;roboto&amp;quot;; font-size: 10pt; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;img height=&quot;140&quot; src=&quot;https://lh3.googleusercontent.com/6-GtG3lq4YRSX195jv8_Kh_Rj393iHNJhVdOUp5G7EfsN7gNvCYeAne1tnXjsNmbz82jEMU-KsM-YsoypfuQQ6o94DtE6ViJ4uw_cmvTzcXogWgWTHwAyQsqJRTo5kHvJUmC9vGl&quot; style=&quot;border: none; transform: rotate(0rad);&quot; width=&quot;640&quot; /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: white; color: #212121; font-family: &amp;quot;roboto&amp;quot;; font-size: 10pt; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;img height=&quot;140&quot; src=&quot;https://lh5.googleusercontent.com/y55rBOVNg9s0fD3J66yLyZWRtv4AdAqbWYJibD6rVY9YYJ1e3d8h_rco7OyEnIfvvkJaX9wtkXmskgMI89P-Vgzy4e7eOQqm4S3c8-dcROP-p6MQR19zxP9bgqKS3lGwAtLWZwyG&quot; style=&quot;border: none; transform: rotate(0rad);&quot; width=&quot;640&quot; /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: white; color: #212121; font-family: &amp;quot;roboto&amp;quot;; font-size: 10pt; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;img height=&quot;140&quot; src=&quot;https://lh3.googleusercontent.com/nzWquJGwQ4t2dyCROSUWc6srv4Z5SdLsWB8xt-1-dj7r4cMuaBnC2Eixm3WavO08N3YwWPvvsvOqRediQTSbQn8DL382hlJmavGY0POzz4TGXu_VZs67xDs9RiThMexhrahiA96m&quot; style=&quot;border: none; transform: rotate(0rad);&quot; width=&quot;640&quot; /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: white; color: #212121; font-family: &amp;quot;roboto&amp;quot;; font-size: 10pt; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;img height=&quot;140&quot; src=&quot;https://lh4.googleusercontent.com/5zKe8_4wRB1V-3uVTy-dCwhZD2iTJL1w-gAaOD7JxsShZRHCURuJPwBjsm46A-3oOpER1toCWApaYFTQEfnYK1-fbgOkl-nuWgTlBaKg9wfzxqb4_E1wMmLCJz_5RbAAGg78FXfm&quot; style=&quot;border: none; transform: rotate(0rad);&quot; width=&quot;640&quot; /&gt;&lt;/span&gt;&lt;/div&gt;&lt;br /&gt;&lt;h4&gt;Misclassifications&lt;/h4&gt;&lt;/div&gt;&lt;div&gt;We now turn to images that are misclassified by the Inception network, i.e., where the top five labels assigned by the Inception network are different from the ground truth label provided by the ImageNet dataset. The goal is understand &lt;b&gt;what made the network choose the wrong label&lt;/b&gt;? To understand this we visualize the integrated gradients with respect to the top label assigned by the Inception network (i.e., the wrong label).&lt;br /&gt;&lt;b style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;img alt=&quot;title.jpg&quot; height=&quot;25&quot; src=&quot;https://lh5.googleusercontent.com/RGxV4TwEzNySNJ3sAOKj0cA5_YVjSQrw5TLJ3V70Q38PrJ2eQZ0yAsGo-YiwQRTu4ywWUOqFM6M6TagwuxeUndDTdP4Aru6l-ma_ToEQouQrNiOK9Rd5tpaKr-b9vC-79GO_1y2o&quot; style=&quot;-webkit-transform: rotate(0.00rad); border: none; transform: rotate(0.00rad);&quot; width=&quot;624&quot; /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;arial&amp;quot;; font-size: 20pt; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;img alt=&quot;861e9130da1f0c34.jpg&quot; height=&quot;196&quot; src=&quot;https://lh5.googleusercontent.com/LQzvirMd2-MGT91nu-xuafHrZ2zg3z9ps99BSdoLTE2OG9F479Z7FcJDt6sNuPONVa1c7Pdg68oH4OkkMaVF2webiCK10dxmTGgr-ZyYghqpVAjEJoVPFXdC5J4yp6LamJvPQrDN&quot; style=&quot;border: none; transform: rotate(0rad);&quot; width=&quot;640&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;img alt=&quot;152ce04bb42aed36.jpg&quot; height=&quot;196&quot; src=&quot;https://lh6.googleusercontent.com/aNGC9YxtdWopPoMz9LpGBx1n3jG6CJ0LMNZi0TNYXpOdVJb5-fzzRpgaowoRXzPv5KMlFW2q1LNM-2_GRO5BqSH-_7W5UfnHVmGcdrMDfk7sIrePDDgzJ_76bWd_Go0Z-QGPZPX0&quot; style=&quot;border: none; transform: rotate(0rad);&quot; width=&quot;640&quot; /&gt;&lt;/span&gt;&lt;/div&gt;&lt;b style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: &amp;quot;arial&amp;quot;; font-size: 11pt; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;img alt=&quot;8cb79ab87da08e55.jpg&quot; height=&quot;196&quot; src=&quot;https://lh5.googleusercontent.com/_3ygmBdP2SoDFgTylEWfd3iIaXXDykKLjETHAXeVKZUkbSOxe8vYFDB1fIajbYmupnl4XhqNCr9bBfDeRt_KkQSWs8Uv7UicickYZWyhe9gg4RbirU8r7whIxZ_7jfM4bsvGqmcN&quot; style=&quot;border: none; transform: rotate(0rad);&quot; width=&quot;640&quot; /&gt;&lt;/span&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;With the first image, it is clear what went wrong even without examining the integrated gradients. The image does have a strainer, but the ground truth label is about a different object within the image (cabbage butterfly).  In contrast, the second and third images are more mysterious. Inspecting the images alone do not tell us anything about the source of the error. The integrated gradient visualization is clarifying — it identifies blurry shapes within the image that seem to resemble a walking stick and a vacuum cleaner. Perhaps a fix for these misclassifications is to feed these images as negative examples for the incorrect labels.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h2&gt;Properties of Integrated Gradients&lt;/h2&gt;We use this section to be precise about our problem statement, our method, and its properties. Part of the reason for the rigor is to argue why our method does not introduce artifacts into the attributions, and faithfully reflects the workings of the deep network.&lt;br /&gt;&lt;h4&gt;Attribution Problem&lt;/h4&gt;&lt;div&gt;Formally, suppose we have a function $F: \mathbb{R}^n \rightarrow [0,1]$ that represents a deep network, and an input $x = (x_1,\ldots,x_n) \in \mathbb{R}^n$.  An attribution of the prediction at input $x$ relative to a baseline input $x&#39;$ is a vector $A_F(x, x&#39;) = (a_1,\ldots,a_n) \in \mathbb{R}^n$ where $a_i$ is the contribution of $x_i$ to the function $F(x)$.&lt;/div&gt;&lt;br /&gt;In our ImageNet example, the function $F$ represents the Inception deep network (for a given output class). The input vector $x$ is simply the image — if one represents the image in grayscale, the indices of $x$ correspond to the pixels. The attribution vector $a$ is exactly what we visualize in the previous sections.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;Let us briefly examine the need for the baseline in the definition of the attribution problem.  A common way for humans to perform attribution relies on counterfactual intuition.  When we assign blame to a certain cause we implicitly consider the absence of the cause as a baseline — would the outcome change if the supposed cause were not present?&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;The attribution scheme for linear models that inspects the weights of the input features has an implicit baseline of an input with no features. The gradient based approach uses a baseline that is a slight perturbation of the original input. Of course, gradients, as we argued earlier, are a poor attribution scheme. Intuitively, the baseline is “too close” to the input. For integrated gradients, we will use baselines that are far enough away from the input that they don’t just focus on the flat region in the sense of the saturation plot shown in the earlier section. We will also ensure that baseline is fairly “neutral”, i.e., the predictions for this input are nearly zero. For instance, the black image for an object recognition network. This will allow us to interpret the attributions independent of the baseline as a property of the input alone.&lt;/div&gt;&lt;h4&gt;Integrated Gradients&lt;/h4&gt;We are now ready to define our method formally. The integrated gradient along the $i^{th}$ dimension for an input $x$ and baseline $x&#39;$ is defined as follows:&lt;br /&gt;&lt;div&gt;$$&lt;br /&gt;\mathrm{IntegratedGrads}_i(x) ::= (x-x&#39;)\times\int_{\alpha=0}^{1}&lt;br /&gt;\frac{\partial F(x&#39; + \alpha \times(x-x&#39;))}{\partial x_i&lt;br /&gt;}~d\alpha&lt;br /&gt;$$&lt;/div&gt;&lt;div&gt;where $\frac{\partial F(x)}{\partial x_i}$ is the gradient of $F$ along the $i^{th}$ dimension at $x$.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Below we list a number of properties that our method satisfies:&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;b&gt;Completeness:&lt;/b&gt; The attributions from integrated gradients sum to the difference between the prediction scores of the input and the baseline. The proof follows from the famous &lt;a href=&quot;https://en.wikipedia.org/wiki/Gradient_theorem&quot;&gt;gradient theorem&lt;/a&gt;. This property is desirable because we can be sure that the prediction is entirely accounted for. &lt;br /&gt;&lt;br /&gt;&lt;b&gt;Linearity preservation:&lt;/b&gt; If a network $F$ is a linear combination $a*F_1 + b*F_2$ of two networks $F_1$ and $F_2$, then a linear combination of the attributions for $F_1$ and $F_2$, with weights $a$ and $b$ respectively, is the attribution for the network $F$. The property is desirable because the attribution method preserves any linear logic present within a network. &lt;br /&gt;&lt;b&gt;&lt;br /&gt;Symmetry preservation:&lt;/b&gt; Integrated gradients preserve symmetry. That is, if the network behaves symmetrically with respect to two input features, then the attributions are symmetric as well. For instance, suppose $F$ is a function of three variables $x_1, x_2, x_3$, and $F(x_1,x_2, x_3) = F(x_2,x_1, x_3)$ for all values of $x_1, x_2, x_3$. Then $F$ is symmetric in the two variables $x_1$ and $x_2$. If the variables have identical values in the input and baseline, i.e., $x_1 = x_2$, and $x&#39;_1 =x&#39;_2$, then symmetry preservation requires that&amp;nbsp;$a_1 = a_2$. This property seems desirable because of the connotation of attributions as blame assignment. Why should two symmetric variables be blamed differently?&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;b&gt;Sensitivity: &lt;/b&gt;We define two aspects of sensitivity.&lt;br /&gt;&lt;ul&gt;&lt;li&gt;(A) If the baseline and the input differ only in one feature, but have different predictions, then this feature gets non-zero attribution.&lt;/li&gt;&lt;li&gt;(B) If a feature does not play any role in the network, it receives no attributions.&lt;/li&gt;&lt;/ul&gt;It is self-evident why we’d like Sensitivity to hold. Further, notice that the failure of gradients discussed earlier  was essentially a failure to satisfy Sensitivity(A). For instance, suppose we have a simple function $\min(x, 5)$. If the input is $x = 8$ and the baseline is $x&#39;=0$, then difference between the function value at the input and the baseline is $5$, but the gradient at $x=8$ is zero, and therefore the gradient-based attribution is zero. This is a one-variable caricature of what we saw with the object recognition network earlier.&lt;br /&gt;&lt;br /&gt;At first glance, these requirements above seem quite basic and we may expect that many methods ought to provably satisfy them. Unfortunately, other methods in literature fall into classes — they either violate Sensitivity(A) or  they violate an even more basic property, namely they depend on the implementation of the network in an undesirable way. That is, we can find examples where two networks have identical input-output behavior, but the method yields different attributions (due to a difference in the underlying structure of the two networks). In contrast, our method relies only on the functional representation of the network, and not its implementation, i.e., we say that it satisfies &quot;implementation invariance&quot;.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;In contrast, we can show that our method is essentially the unique method to satisfy all the properties listed above (up to certain convex combinations). We invite the interested reader to read &lt;a href=&quot;https://arxiv.org/abs/1703.01365&quot;&gt;our paper&lt;/a&gt;&amp;nbsp;[4], where we have formal descriptions of these properties, the uniqueness result, and comparisons with other methods.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h2&gt;Application to other networks&lt;/h2&gt;Our paper also includes application of integrated gradients to other networks (none of these networks were trained by us). One network is an image network that predicts diabetic retinopathy — we demonstrate the use of attributions in a user-facing context to help doctors gain some transparency into the network’s prediction. The second network is a chemistry network that performs virtual screening of drug molecules — we show how attributions help identify degenerate model features.  A third network categorizes queries in the context of a question-answering system — we demonstrate the use of attribution to extract human-intelligible rules.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;A quick checklist on applying our method to your favorite deep network. You will have to resolve three issues:&lt;br /&gt;&lt;ol&gt;&lt;li&gt;Identify a good baseline, i.e., the analog of the black image in our example. This should be treated as neutral by the network, i.e., the prediction score for this input should be nearly zero.&lt;/li&gt;&lt;li&gt;Identify the right variables to attribute to. This step is trivial for ImageNet. But in a text network the input is usually represented as embeddings. The attributions are then naturally produced in the space of embeddings and some simple processing is needed to map them to the space of terms.&lt;/li&gt;&lt;li&gt;Find a convenient visualization technique. Our paper&amp;nbsp;[4] has some ideas.&lt;/li&gt;&lt;/ol&gt;&lt;br /&gt;&lt;h2&gt;Concluding thoughts&lt;/h2&gt;This post discusses the problem of identifying input feature importance for a deep network. We present a very simple method, called &quot;integrated gradients&quot;, to do this. All it involves is a few calls to a gradient operator. It yields insightful results for a variety of deep networks.&lt;br /&gt;&lt;br /&gt;Of course, our problem formulation has limitations. It says nothing about the logic that is employed by the network to combine features. This is an interesting direction for future work. &lt;br /&gt;&lt;br /&gt;Our method and our problem statement are also restricted to providing insight into the behavior of the network on a single input. It does not directly offer any global understanding of the network. Other work has made progress in this direction via clustering inputs using the pattern of neuron activations, for instance, [5] or [6]. There is also work (such as &lt;a href=&quot;http://gabgoh.github.io/ThoughtVectors/&quot;&gt;this&lt;/a&gt;)&amp;nbsp;on architecting deep networks in ways that allow us to understand the internal representations of these networks. These are all very insightful. It is interesting to ask if there is a way to turn these insights into guarantees of some form as we do for the problem of feature attribution.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;Overall, we hope that deep networks lose their reputation for being impenetrable black-boxes which perform black magic. Though they are harder to debug than other models, there are ways to analyze them. And the process can be enlightening and fun!&lt;/div&gt;&lt;div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h3&gt;References&lt;/h3&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;[1] Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott E., Anguelov, Dragomir, Erhan, Dumitru, Vanhoucke, Vincent, and Rabinovich, Andrew. Going deeper with convolutions. CoRR, 2014.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;[2] Shrikumar, Avanti, Greenside, Peyton, Shcherbina, Anna, and Kundaje, Anshul. Not just a black box: Learning important features through propagating activation differences. CoRR, 2016.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;[3] Mukund Sundararajan, Ankur Taly, Qiqi Yan, 2016, &quot;Gradients of Counterfactuals&quot;, &amp;nbsp;&lt;a href=&quot;https://arxiv.org/abs/1611.02639&quot;&gt;arXiv:1611.02639&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;/div&gt;&lt;br /&gt;[4] Mukund Sundararajan, Ankur Taly, Qiqi Yan, 2017, &quot;Axiomatic Attribution for Deep Networks&quot;, &amp;nbsp;&lt;a href=&quot;https://arxiv.org/abs/1703.01365&quot;&gt;arXiv:1703.01365&lt;/a&gt;&lt;br /&gt;&lt;br /&gt;[5]&amp;nbsp;Ian J. Goodfellow, Quoc V. Le, Andrew M. Saxe, Honglak Lee, and Andrew Y. Ng. 2009, &quot;&lt;a href=&quot;http://ai.stanford.edu/~hllee/nips09-MeasuringInvariancesDeepNetworks.pdf&quot;&gt;Measuring invariances in deep networks&lt;/a&gt;&quot;. In Proceedings of the 22nd International Conference on Neural Information Processing Systems (NIPS&#39;09), USA, 646-654&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;[6] Erhan, Dumitru, Bengio, Yoshua, Courville, Aaron, and Vincent, Pascal. &quot;&lt;a href=&quot;http://igva2012.wikispaces.asu.edu/file/view/Erhan+2009+Visualizing+higher+layer+features+of+a+deep+network.pdf&quot;&gt;Visualizing higher-layer features of a deep network&lt;/a&gt;&quot;. Technical Report 1341, University of Montreal, 2009.&lt;/div&gt;&lt;div&gt;&lt;i&gt;&lt;br /&gt;&lt;/i&gt;&lt;/div&gt;&lt;div&gt;&lt;i&gt;&lt;br /&gt;&lt;/i&gt;&lt;/div&gt;&lt;div&gt;&lt;i&gt;&lt;br /&gt;&lt;/i&gt;&lt;/div&gt;&lt;/div&gt;</content><link rel='replies' type='application/atom+xml' href='https://www.unofficialgoogledatascience.com/feeds/3311282216595193329/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='https://www.unofficialgoogledatascience.com/2017/03/attributing-deep-networks-prediction-to.html#comment-form' title='3 Comments'/><link rel='edit' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/3311282216595193329'/><link rel='self' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/3311282216595193329'/><link rel='alternate' type='text/html' href='https://www.unofficialgoogledatascience.com/2017/03/attributing-deep-networks-prediction-to.html' title='Attributing a deep network’s prediction to its input features'/><author><name>Amir Najmi</name><uri>http://www.blogger.com/profile/18174523203317227640</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://lh3.googleusercontent.com/OdmC9gir5TIFrk39uuxI92pK4FaXzLyAAi6PO1sLdTJTaR-mPdbdB6owk2IaTfCN3MWHhK7HVAIaVyXZ2Gyzm75bN8QukZFTEA7EZpb20oP9B-M4AeMTY1E3PWsuMDKnTPq3QkJ6=s72-c" height="72" width="72"/><thr:total>3</thr:total></entry><entry><id>tag:blogger.com,1999:blog-5825758052688213474.post-5343098145846379837</id><published>2017-01-31T17:55:00.000-08:00</published><updated>2017-12-22T07:59:31.422-08:00</updated><title type='text'>Causality in machine learning</title><content type='html'>By OMKAR MURALIDHARAN, NIALL CARDIN, TODD PHILLIPS, AMIR NAJMI&lt;br /&gt;&lt;br /&gt;Given recent advances and interest in machine learning, those of us with traditional statistical training have had occasion to ponder the similarities and differences between the fields. Many of the distinctions are due to culture and tooling, but there are also differences in thinking which run deeper. Take, for instance, how each field views the provenance of the training data when building predictive models. For most of ML, the training data is a given, often presumed to be representative of the data against which the prediction model will be deployed, but not much else. With a few notable exceptions, ML abstracts away from the data generating mechanism, and hence sees the data as raw material from which predictions are to be extracted. Indeed, machine learning generally lacks the vocabulary to capture the distinction between observational data and randomized data that statistics finds crucial. To contrast machine learning with statistics is not the object of this post (we can do such a post if there is sufficient interest). Rather, the focus of this post is on combining observational data with randomized data in model training, especially in a machine learning setting. The method we describe is applicable to prediction systems employed to make decisions when choosing between uncertain alternatives.&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;h2&gt;Predicting and intervening&lt;/h2&gt;Most of the prediction literature assumes that predictions are made by a passive observer who has no influence on the phenomenon. On the other hand, most prediction systems are used to make decisions about how to intervene in a phenomenon. Often, the assumption of non-influence is quite reasonable —&amp;nbsp;say if we predict whether or not it will rain in order to determine if we should carry an umbrella. In this case, whether or not we decide to carry an umbrella clearly doesn&#39;t affect the weather. But at other times, matters are less clear. For instance, if the predictions are used to decide between uncertain alternative scenarios then we observe only the outcomes which were realized. In this framing, the decisions we make influence our future training data. Depending on how the model is structured, we typically use the information we gain from realized &lt;i&gt;factual&lt;/i&gt;&amp;nbsp;scenarios to assess probabilities associated with unrealized &lt;i&gt;counterfactual&lt;/i&gt;&amp;nbsp;scenarios. But this involves extrapolation and hence the counterfactual prediction might be less accurate. Some branches of machine learning (e.g. multi-arm bandits and reinforcement learning) adopt this framing of choice between alternative scenarios in order to study optimal tradeoffs between exploration and exploitation. Our goal here is specifically to evaluate and improve counterfactual predictions.&lt;br /&gt;&lt;br /&gt;Why would we care about the prediction accuracy of unrealized scenarios? There are a number of reasons. First is that our decision not to choose a particular scenario might be incorrect, but we might never learn this because we never generate data to contradict the prediction. Second, real-world prediction systems are constantly being updated and improved —&amp;nbsp;knowledge of errors would help us target model development efforts. Finally, a more niche reason is the use in auction mechanisms such as second-pricing where the winner (predicted highest) must pay what value the runner up is predicted to have realized.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;Let&#39;s start with a simple example to illustrate the problems of predicting and intervening. Suppose a mobile carrier builds a &quot;churn&quot; model to predict which of its customers are likely to discontinue their service in the next three months. The carrier offers a special renewal deal to those who were predicted by the model as most likely to churn. When we analyze the set of customers who have accepted the special deal (and hence not churned), we don&#39;t immediately know which customers would have continued their service anyway versus those who renewed &lt;i&gt;because&lt;/i&gt; of the special deal. This lack of information has some consequences:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;we cannot directly measure churn prediction accuracy on customers to whom we made offers&lt;/li&gt;&lt;li&gt;we cannot directly measure if the offer was effective (did its benefit exceeded its cost)&lt;/li&gt;&lt;li&gt;we must (somehow) account for the intervention when training future churn models&lt;/li&gt;&lt;/ul&gt;What we want to know is what would have happened had the carrier not acted on the predictions of its churn model.&lt;br /&gt;&lt;br /&gt;In this simple example, we could of course have run an experiment where we have a randomized control group to whom we would have made a special offer but did not (a &quot;holdback&quot; group). This gives us a way to answer each of the questions above. But what if we are faced with many different situations and many possible interventions, with the objective to select the best intervention in each case?&lt;br /&gt;&lt;br /&gt;Let&#39;s consider a more complex problem to serve as the running example in this post. Suppose an online newspaper provides a section on their front page called &lt;i&gt;Recommended for you&lt;/i&gt;&amp;nbsp;where they highlight news stories they believe a given user will find interesting (the NYTimes does this for its subscribers). We can imagine a sophisticated algorithm to maximize the number of stories a user will find valuable. It may predict which stories to highlight and in which order, based on the user&#39;s location, reading history and the topic of news stories.&lt;br /&gt;&lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://2.bp.blogspot.com/-SPMRpqxF5Tg/WJGHwp8ZqQI/AAAAAAAAcBE/V3nSX-o_U0UZ9RSSp8p8HPxsFxtxHQK0gCLcB/s1600/puppies.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;236&quot; src=&quot;https://2.bp.blogspot.com/-SPMRpqxF5Tg/WJGHwp8ZqQI/AAAAAAAAcBE/V3nSX-o_U0UZ9RSSp8p8HPxsFxtxHQK0gCLcB/s400/puppies.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Figure 1: A hypothetical example of personalized news recommendations&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;Those stories which are recommended are likely to be uptaken both because the algorithm works and because of sheer prominence of the recommendation. If the model is more complex (say, multiple levels of prominence, interaction effects), holdback experiments won&#39;t scale because they would leave little opportunity to take the optimal action. Randomized experiments are &quot;costly&quot; because we do something different from what we think is best. We want to bring a small amount of randomization into the machine learning process, but do it in manner that uses randomization effectively.&lt;br /&gt;&lt;br /&gt;Predicting well on counterfactuals is usually harder than predicting well on the observed data because the decision making process creates confounding associations in the data. Continuing our news story recommendation example, the correct decision rule will tend to make recommendations which are most likely to be uptaken. If we try to estimate the effect of recommendation prominence by comparing how often users read recommended stories against stories not recommended, the association between our prediction and prominence would probably dominate —&amp;nbsp;after all, the algorithm chooses to make prominent only those stories which appear likely to be of interest to the reader.&lt;br /&gt;&lt;br /&gt;If we want to use predictions to make good decisions, we have to answer the following questions:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;How do we measure accuracy on counterfactuals?&lt;/li&gt;&lt;li&gt;Are there counterfactual predictions we should avoid in decision making?&lt;/li&gt;&lt;li&gt;How can we construct our prediction system to do well on counterfactuals?&lt;/li&gt;&lt;/ul&gt;The rest of this post outlines an approach we have used to tackle these questions.&lt;/div&gt;&lt;br /&gt;&lt;h2&gt;A problem of counterfactual prediction&lt;/h2&gt;&lt;div&gt;&lt;div&gt;Before we describe solutions, it is important to be precise about the problem we are trying to address. First off, let&#39;s be clear that if the model we are training is the true model (i.e. it correctly specifies the generating mechanism), there is no problem. Likelihood theory guarantees that we will estimate the true model in an unbiased way and asymptotically converge upon it. The problem is that every real-world model is misspecified in some way, and this is what leads to poor counterfactual estimates.&lt;br /&gt;&lt;br /&gt;For illustration purposes, assume there is a single level of prominence and that the true model is binomial for the binary event of uptake $Y$. This true model is described by the GLM equation&lt;/div&gt;&lt;div&gt;$$\mathrm{logit}(EY) = \beta_1 X_1 + \beta_2 X_2 + \beta_{\pi} X_{\pi} \tag{Eq 1} &amp;nbsp; $$&lt;/div&gt;&lt;div&gt;where $X_1$ and $X_2$ are continuous features we use to estimate the relevance of the news story to the user, $X_{\pi}$ is the binary variable indicating whether the story was made prominent. Let&#39;s define $\beta_1 X_1 + \beta_2&amp;nbsp;X_2$ to be the &lt;i&gt;quality score&amp;nbsp;&lt;/i&gt;of the model. We wish to estimate $\beta_{\pi}$, the true log odds effect of prominence on uptake $Y$.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Now suppose we fit the following misspecified model using maximum likelihood&lt;/div&gt;&lt;div&gt;$$ &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; \mathrm{logit}(EY) = \beta_1 X_1 + \beta_{\pi} X_{\pi} &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; \tag{Eq 2} &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; $$&lt;/div&gt;&lt;div&gt;This model is misspecified because our quality score is missing $X_2$. Our estimate of $\beta_{\pi}$ will pick up the projection of $X_2$ onto $X_{\pi}$ (and onto $X_1$). In general, we will have misattribution to the extent that errors in our model are correlated with $X_{\pi}$. This isn&#39;t anything new. If all we care about is prediction on observed $Y$, we do fine, at least to the extent $X_2$ can be projected on the space spanned by $X_1$ and $X_{\pi}$. The fact that our estimate of $\beta_{\pi}$ is not unbiased isn&#39;t a concern because our predictions are unbiased (i.e. correct on average on the logit scale). The problem only arises when we use the model to predict on observations where the distribution of predictors is different from the training distribution —&amp;nbsp;this of course happens when we are deciding on which stories to administer prominence. Depending on the situation, this could be a big deal, and so it has been at Google.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;In theory, we could use a holdback experiment to estimate the effect of prominence where we randomly do not recommend stories which we would otherwise have recommended. We can estimate the causal effect of prominence as the difference in log odds of uptake between stories which were recommended and those which were eligible (i.e. would have been recommended) but were randomly not recommended. The value of $\beta_{\pi}$ in following GLM equation is the causal estimate we seek:&lt;br /&gt;$$&lt;br /&gt;\mathrm{logit}(EY) = \beta_{\pi} X_{\pi}&amp;nbsp;+ \beta_e X_e&amp;nbsp;&amp;nbsp; &amp;nbsp;\tag{Eq 3}&lt;br /&gt;&lt;br /&gt;$$ where $X_e$ is the binary variable denoting the story was eligible for recommendation and $\beta_e$ its associated coefficient. Since we only recommend eligible stories, $X_{\pi}=1$ implies $X_e=1$, and $X_{\pi} \neq X_e$ occurs only in our holdback.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Observe that $\beta_{\pi}$ is estimated as the difference in $\mathrm{logit}(EY)$ when $X_{\pi} = 1$, $X_e = 1$ and when $X_{\pi} = 0$, $X_e = 1$. Why we use this roundabout GLM model to express a simple odds ratio calculation will become clearer further on. The point is that this method works to estimate the causal effect of prominence because randomization breaks the correlation between $X_2$ and $X_{\pi}$ (&lt;a href=&quot;http://www.unofficialgoogledatascience.com/2016/06/to-balance-or-not-to-balance.html&quot;&gt;see an earlier post&lt;/a&gt; for a more detailed discussion on this point). As per Eq 2, we can apply this estimate of $\beta_{\pi}$ from our randomized holdback in estimating $\beta_1$ on observational data.&lt;/div&gt;&lt;br /&gt;&lt;h2&gt;Checking accuracy with randomization — realism vs. interpretability&lt;/h2&gt;The best and most obvious way to tell how well we are predicting the effects of counterfactual actions is to randomly take those counterfactual actions a fraction of the time and see what happens. In our news recommendation example, we can randomly decide to recommend or not some stories, and see if our decision-time prediction of the change in uptake rates is correct. As we saw, this works because randomization breaks the correlations between our chosen action (whether to recommend) and other decision inputs (the quality of the recommendation).&lt;br /&gt;&lt;br /&gt;In a complex system, randomization can still be surprisingly subtle because there are often multiple ways to randomize. For example, we can randomize inputs to the decision procedure, or directly randomize decisions. The former approach will tend to produce more realistic outcomes, but can be harder to understand, and may not give us adequate data to assess unlikely decisions. The latter approach is usually easier to understand, but can produce unrealistic outcomes.&lt;br /&gt;&lt;br /&gt;To show how subtle things can be, let&#39;s go back to our example earlier where we computed the causal effect of prominence by running a holdback experiment. What we did there was to randomly not recommend stories we would have recommended. But this is just one kind of random perturbation. This particular procedure allows us to estimate (and hence check on) what statisticians call &lt;i&gt;treatment on the treated&lt;/i&gt;. In other words, we estimate the average effect of prominence on the uptake of stories we recommend. This is different than the average effect of prominence across the population of stories.&amp;nbsp;What we miss is the effect of prominence on the kinds of stories we &lt;i&gt;never&lt;/i&gt; recommend. Suppose the effect of prominence is significantly lower for a news topic that no one finds interesting, say, news on the proceedings of the local chamber of commerce (PLCC). If we never recommend stories of the PLCC, they won&#39;t contribute to our holdback experiment and hence we will never learn that our estimates for such stories were too high. We could fix this problem by recommending random stories (as well as randomly suppressing recommendations) and hence directly measure the average effect of recommendation on all stories. But this might not be quite what we want either — it is unclear what we are learning from the unnatural scenario of recommending news of the intolerable PLCC. These recommendations might themselves cause users to react in an atypical manner, perhaps by not bothering to look at recommendations further down the list.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;Random expression and suppression of recommendation are examples of what we called &lt;i&gt;randomizing the decision&lt;/i&gt;. The alternative we mentioned was to &lt;i&gt;randomize inputs to the decision&lt;/i&gt;. We could achieve this by adding random noise to each news story&#39;s quality score and feeding it into the decision procedure. If the amount of random noise is commensurate with the variability in the quality score then we will truly generate a realistic set of perturbations. The data we collect from these randomized perturbations will tend to be near the quality threshold for recommendation, which is usually where data is most valuable. On the other hand, this data might be less interpretable — all sorts of decisions might change, not just the stories whose scores we randomized. The impact of each individual perturbation is not easily separated and this can make the data harder to use for modeling and analysis of prediction errors.&lt;br /&gt;&lt;br /&gt;There is no easy way to make the choice between realism and interpretability. We’ve often chosen artificial randomization that is easy to understand, since it is more likely to produce data useful for multiple applications, and subsequently checked the results against more realistic randomization. Happily, in our case, we found that answers between these two approaches were in good agreement.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;The No Fake Numbers Principle&lt;/h2&gt;&lt;div&gt;&lt;div&gt;Automated decision systems are often mission-critical, so it is important that everything which goes into them is accurate and checkable. We can’t reliably check counterfactual predictions for actions we can’t randomize. These facts lead to the &lt;i&gt;No Fake Numbers&lt;/i&gt; (NFN) principle:&lt;br /&gt;&lt;i&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp;Avoid decisions based on predictions for counterfactual actions you cannot take.&lt;/i&gt;&lt;/div&gt;&lt;div&gt;In other words, NFN says not to use predictions of unobservable quantities to make decisions.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;Now why would anyone ever place demands on a prediction system which run counter this seemingly reasonable principle? In our work, violations of this principle have arisen when we&#39;ve wanted to impose invariants on the decisions we make, usually with good intentions. The problem isn&#39;t the invariants themselves but rather the hypothetical nature of their premises.&lt;br /&gt;&lt;br /&gt;For example, suppose the online newspaper wishes to enforce a policy of &quot;platform neutrality&quot; whereby the quality of recommendations should be the same on iPhone and Android mobile phones. Perhaps the newspaper wishes to ensure that users would see the same recommendations regardless of the type of phone they use. However, this is a slippery notion. iPhone users might actually have different aggregate usage patterns and preferences from Android users, making a naive comparison inappropriate.&lt;br /&gt;&lt;br /&gt;One way to address this is to use techniques to derive causal inference from observational analysis to predict what an iPhone user would do if she were using Android. There is valuable literature on this (e.g.&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/Rubin_causal_model&quot;&gt;Rubin Causal Model&lt;/a&gt;) but, fundamentally, you really need to understand what you are doing with a causal model. That means the model needs careful, manual attention from an experienced modeler, and a nuanced understanding of its strengths and weaknesses. This is why observational analysis models are used only when there is no alternative, where the inference made by the model is carefully traded off against its assumptions. Such fine inspection is not feasible for production models, which are updated by many people (and automatically over time), require ongoing automated monitoring, and whose predictions are used for many applications.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;Why is NFN necessary? First, it is generally a better way to design decision systems. Actions violating NFN can never be realized. This usually means they are not directly important for decisions, and the system can be improved by thinking about its goals more carefully.&lt;br /&gt;&lt;br /&gt;Second, and more importantly, mission-critical systems have much stronger requirements than one-off analyses —&amp;nbsp;we need to be able to monitor them for correctness, to define clear notions of improvement, to check that their behavior is stable, and to debug problems. For example, suppose our system uses predictions for what iPhone users would do on Android. If those predictions drift over time, we have no way to tell if the system is working well or not. One-off analyses might be able to get away with using observational analysis techniques, but critical systems need ongoing, robust, direct validation with randomized data. Crucially, we can never check how accurate this prediction is, since we cannot randomly force the iPhone user to use Android.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;In summary, the NFN principle cautions us against imposing requirements whose solutions may have unintended consequences we cannot easily detect. As with any principle, we would override it with great caution.&lt;/div&gt;&lt;br /&gt;&lt;h2&gt;Using randomization in training&lt;/h2&gt;The previous sections described how to use randomization to check our prediction models and guide the design of our decision systems. This section describes how to go further, and directly incorporate randomized data into the systems.&lt;br /&gt;&lt;br /&gt;Large scale prediction systems are often bad at counterfactual predictions out of the box. This is because a large scale prediction system is almost certainly misspecified. Thus even very sophisticated ones suffer from a kind of overfitting. These systems don’t classically overfit —&amp;nbsp;they use cross-validation or progressive validation to avoid that —&amp;nbsp;but they tend to overfit to the observed distribution of the data. As we described earlier, this factual distribution doesn’t match the counterfactual distribution of data and hence the model can fail to generalize. When deciding which stories to recommend, we need predictions with and without the prominence of recommendation —&amp;nbsp;but that means we&#39;ll need good predictions for interesting stories which don&#39;t get recommended and uninteresting stories which are recommended, rare and strange parts of the factual data.&lt;br /&gt;&lt;br /&gt;An obvious attempt to fix this is to upweight randomized data in training, or even train the model solely on the randomized data. Unfortunately, such direct attempts perform poorly. Let&#39;s say 1% of the data are randomized. If we train on both randomized and observational data, the observational data will push estimates off course due to model misspecification. However, training solely on randomized data will suffer from data sparseness because you are training on 1% of the data. Nor is upweighting the randomized data of much of a solution —&amp;nbsp;we reduce the influence of observational data only to the extent we reduce its role in modeling. Thus, upweighting is tantamount to throwing away non-randomized data.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;The problem is that the model &lt;i&gt;doesn’t know&lt;/i&gt; the random data is random, so it uses it in exactly the same way as it uses any data. As we observed at the start of this post, standard machine learning techniques don’t distinguish between randomized and observational data the way statistical models do. To make better estimates, we need the randomized data to play a different role than the observational data in model training.&lt;br /&gt;&lt;br /&gt;What is the right role for randomized data? There is probably more than one good answer to that question. For instance, one could imagine shrinking the unbiased, high-variance estimates from randomized data towards the potentially-biased, low-variance observational estimates. This is not the approach we chose for our application but we nonetheless do use the observational estimates to reduce the variance of the estimates made from randomized data.&lt;br /&gt;&lt;br /&gt;Previously we used separate models to learn the effects of prominence and quality. The quality model (Eq 2) took the estimates of the prominence model (Eq 3) as an input. While this does achieve some of our goals it has its own problems. Firstly, this set-up is clunky, we have to maintain two models and changes in performance are the result of complex interactions between the two. Also, updating either is made harder by its relationship to the other. Secondly, the prominence model fails to take advantage of the information in the quality model.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;The approach we have found most effective is best motivated as a refinement of the simple model we used to estimate the causal effect of prominence from our holdback in Eq 3. Let the quality score for each story be the log odds prediction of uptake without prominence. For the model in Eq 2 it would simply be $\beta_1 X_1$ where $\beta_1$ is the coefficient of $X_1$ estimated by ML. Assume for a moment that the quality score component is given. An improved estimate of the causal effect of prominence is possible from estimating $\beta_{\pi}$ in the model&lt;/div&gt;&lt;div&gt;$$ \mathrm{logit}(EY) = \mathrm{offset}(\hat{\beta_1} X_1) +&amp;nbsp;\beta_{\pi} X_{\pi}&amp;nbsp;+ \beta_e X_e &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; \tag{Eq 4} $$&lt;/div&gt;&lt;div&gt;where $\mathrm{offset}$ is an abuse of R syntax to indicate that this component is given and not estimated. The estimate of $\beta_{\pi}$ in this model is still unbiased but by accounting for the (presumed) known quality effect, we reduce the variability of our estimate. In reality, the quality score is not known, and is estimated from the observational data. But regardless, randomization ensures that the estimate from this procedure will be unbiased. As long as we employ an estimate of quality score that is better than nothing, we account for some of the variability and hence reduce estimator variance. We have every reason to be optimistic of a decent-but-misspecified model.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;div&gt;&lt;span id=&quot;docs-internal-guid-03ce73af-f5d6-34b8-2a41-39070960a47a&quot;&gt;The procedure above involves first training a model entirely on observational data and then using the quality score thus derived to estimate a second model for prominence, trained on randomized data. The astute reader will note that the observational model itself estimates a prominence effect which we discard. It turns out we can do even better by co-training the quality score together with the prominence. Consider the following iterative updating procedure:&lt;/span&gt;&lt;br /&gt;&lt;ol&gt;&lt;span id=&quot;docs-internal-guid-03ce73af-f5d6-34b8-2a41-39070960a47a&quot;&gt;&lt;li&gt;On non-randomized data, use the model $$ \mathrm{logit}(EY) = \beta_1 X_1 + \mathrm{offset}(\hat{\beta_{\pi}} X_{\pi})&amp;nbsp;$$ and only update the quality score coefficients (here just $\beta_1$).&lt;/li&gt;&lt;li&gt;On randomized data, use the model $$ \mathrm{logit}(EY) = \mathrm{offset}(\hat{\beta_1} X_1) +&amp;nbsp;\beta_{\pi} X_{\pi}&amp;nbsp;+ \beta_e X_e$$ and only update the prominence coefficients (here $\beta_{\pi}$ and $\beta_e$).&lt;/li&gt;&lt;/span&gt;&lt;/ol&gt;&lt;span id=&quot;docs-internal-guid-03ce73af-f5d6-34b8-2a41-39070960a47a&quot;&gt;This co-training works better because it allows the causal estimate of $\beta_{\pi}$ to be used in estimating the quality score. There is a lot more to the mechanics of training a large model efficiently at scale, but the crux of our innovation is this.&lt;/span&gt;&lt;br /&gt;&lt;h2&gt;Conclusion&lt;/h2&gt;&lt;/div&gt;&lt;div&gt;In this post we described how some randomized data may be applied both to check and improve the accuracy of a machine learning system trained largely on observational data. We also shared some of the subtleties of randomization applied to causal modeling. While we&#39;ve spent years trying to understand and overcome issues arising from counterfactual (and &quot;counter-usual&quot;, atypical) predictions, there is much we have still to learn. And yet the ideas we describe here have already been deployed to solve some long-standing prediction problems at Google. We hope they will be useful to you as well.&lt;/div&gt;&lt;/div&gt;</content><link rel='replies' type='application/atom+xml' href='https://www.unofficialgoogledatascience.com/feeds/5343098145846379837/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='https://www.unofficialgoogledatascience.com/2017/01/causality-in-machine-learning.html#comment-form' title='6 Comments'/><link rel='edit' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/5343098145846379837'/><link rel='self' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/5343098145846379837'/><link rel='alternate' type='text/html' href='https://www.unofficialgoogledatascience.com/2017/01/causality-in-machine-learning.html' title='Causality in machine learning'/><author><name>Amir Najmi</name><uri>http://www.blogger.com/profile/18174523203317227640</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://2.bp.blogspot.com/-SPMRpqxF5Tg/WJGHwp8ZqQI/AAAAAAAAcBE/V3nSX-o_U0UZ9RSSp8p8HPxsFxtxHQK0gCLcB/s72-c/puppies.png" height="72" width="72"/><thr:total>6</thr:total></entry><entry><id>tag:blogger.com,1999:blog-5825758052688213474.post-1874103119442359898</id><published>2016-10-31T20:18:00.000-07:00</published><updated>2017-03-16T11:37:20.463-07:00</updated><title type='text'>Practical advice for analysis of large, complex data sets</title><content type='html'>By PATRICK RILEY&lt;br /&gt;&lt;br /&gt;&lt;div&gt;For a number of years, I led the data science team for Google Search logs. We were often asked to make sense of confusing results, measure new phenomena from logged behavior, validate analyses done by others, and interpret metrics of user behavior. Some people seemed to be naturally good at doing this kind of high quality data analysis. These engineers and analysts were often described as “careful” and “methodical”. But what do those adjectives actually mean? What actions earn you these labels?&lt;br /&gt;&lt;br /&gt;To answer those questions, I put together a document shared Google-wide which I optimistically and simply titled “Good Data Analysis.” To my surprise, this document has been read more than anything else I’ve done at Google over the last eleven years. Even four years after the last major update, I find that there are multiple Googlers with the document open any time I check.&lt;br /&gt;&lt;br /&gt;Why has this document resonated with so many people over time? I think the main reason is that it’s full of specific actions to take, not just abstract ideals. I’ve seen many engineers and analysts pick up these habits and do high quality work with them. I&#39;d like to share the contents of that document in this blog post.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;The advice is organized into three general areas:&lt;/div&gt;&lt;div&gt;&lt;ul&gt;&lt;li&gt;&lt;i&gt;Technical&lt;/i&gt;: Ideas and techniques for how to manipulate and examine your data.&lt;/li&gt;&lt;li&gt;&lt;i&gt;Process&lt;/i&gt;: Recommendations on how you approach your data, what questions to ask, and what things to check.&lt;/li&gt;&lt;li&gt;&lt;i&gt;Social&lt;/i&gt;: How to work with others and communicate about your data and insights.&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;a href=&quot;http://4.bp.blogspot.com/-CyVXj5pF5qM/WBgHAg3fXHI/AAAAAAAAb2k/A6ESjMJ8cBsgUf92Rts-_IPrGO01HriaQCK4B/s1600/blog.png&quot; imageanchor=&quot;1&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;398&quot; src=&quot;https://4.bp.blogspot.com/-CyVXj5pF5qM/WBgHAg3fXHI/AAAAAAAAb2k/A6ESjMJ8cBsgUf92Rts-_IPrGO01HriaQCK4B/s640/blog.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;h2&gt;Technical&lt;/h2&gt;&lt;h3&gt;Look at your distributions&lt;/h3&gt;While we typically use summary metrics (means, median, standard deviation, etc.) to communicate about distributions, you should usually be looking at a much richer representation of the distribution. Something like histograms, CDFs, Q-Q plots, etc. will allow you to see if there are important interesting features of the data such as multi-modal behavior or a significant class of outliers that you need to decide how to summarize.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;h3&gt;Consider the outliers&lt;/h3&gt;You should look at the outliers in your data. They can be canaries in the coal mine for more fundamental problems with your analysis. It’s fine to exclude them from your data or to lump them together into an “Unusual” category, but you should make sure you know why data ended up in that category. For example, looking at the queries with the lowest click-through rate (CTR) may reveal clicks on elements in the user interface that you are failing to count. Looking at queries with the highest CTR may reveal clicks you should not be counting. On the other hand, some outliers you will never be able to explain so you need to be careful in how much time you devote this.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;h3&gt;Report noise/confidence&lt;/h3&gt;First and foremost, we must be aware that randomness exists and will fool us. If you aren’t careful, you will find patterns in the noise. Every estimator that you produce should have a notion of your confidence in this estimate attached to it. Sometimes this will be more formal and precise (through techniques such as confidence intervals or credible intervals for estimators, and p-values or Bayes factors for conclusions) and other times you will be more loose. For example if a colleague asks you how many queries about frogs we get on Mondays, you might do a quick analysis looking and a couple of Mondays and report “usually something between 10 and 12 million” (not real numbers). &lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Look at examples&lt;/h3&gt;&lt;/div&gt;&lt;div&gt;Anytime you are producing new analysis code, you need to look at examples of the underlying data and how your code is interpreting those examples. It’s nearly impossible to produce working analysis code of any complexity without this. Your analysis is removing lots of features from the underlying data to produce useful summaries. By looking at the full complexity of individual examples, you can gain confidence that your summarization is reasonable.&lt;br /&gt;&lt;br /&gt;You should be doing stratified sampling to look at a good sample across the distribution of values so you are not too focussed on the most common cases.&lt;br /&gt;&lt;br /&gt;For example, if you are computing Time to Click, make sure you look at examples throughout your distribution, especially the extremes. If you don’t have the right tools/visualization to look at your data, you need to work on those first.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;Slice your data&lt;/h3&gt;&lt;div&gt;Slicing means to separate your data into subgroups and look at the values of your metrics in those subgroups separately. In analysis of web traffic, we commonly slice along dimensions like mobile vs. desktop, browser, locale, etc. If the underlying phenomenon is likely to work differently across subgroups, you must slice the data to see if it is. Even if you do not expect a slice to matter, looking at a few slices for internal consistency gives you greater confidence that you are measuring the right thing. In some cases, a particular slice may have bad data, a broken experience, or in some way be fundamentally different.&lt;br /&gt;&lt;br /&gt;Anytime you are slicing your data to compare two groups (like experiment/control, but even time A vs. time B comparisons), you need to be aware of mix shifts. A mix shift is when the amount of data in a slice is different across the groups you are comparing. &lt;a href=&quot;http://en.wikipedia.org/wiki/Simpson%27s_paradox&quot;&gt;Simpson’s paradox&lt;/a&gt; and other confusions can result. Generally, if the relative amount of data in a slice is the same across your two groups, you can safely make a comparison.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;Consider practical significance&lt;/h3&gt;&lt;div&gt;With a large volume of data, it can be tempting to focus solely on statistical significance or to hone in on the details of every bit of data. But you need to ask yourself, “Even if it is true that value X is 0.1% more than value Y, does it matter?” This can be especially important if you are unable to understand/categorize part of your data. If you are unable to make sense of some user agents strings in our logs, whether it’s 0.1% of 10% makes a big difference in how much you should investigate those cases.&lt;br /&gt;&lt;br /&gt;On the flip side, you sometimes have a small volume of data. Many changes will not look statistically significant but that is different than claiming it is “neutral”. You must ask yourself “How likely is it that there is still a practically significant change”?&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;Check for consistency over time&lt;/h3&gt;&lt;div&gt;One particular slicing you should almost always employ is to slice by units of time (we often use days, but other units may be useful also). This is because many disturbances to underlying data happen as our systems evolve over time. Typically the initial version of a feature or the initial data collection will be checked carefully, but it is not uncommon for something to break along the way.&lt;br /&gt;&lt;br /&gt;Just because a particular day or set of days is an outlier does not mean you should discard it. Use the data as a hook to find a causal reason for that day being different before you discard it.&lt;br /&gt;&lt;br /&gt;The other benefit of looking at day over day data is it gives you a sense of the variation in the data that would eventually lead to confidence intervals or claims of statistical significance. This should not generally replace rigorous confidence interval calculation, but often with large changes you can see they will be statistically significant just from the day-over-day graphs.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;h2&gt;Process&lt;/h2&gt;&lt;h3&gt;Separate Validation, Description, and Evaluation&lt;/h3&gt;&lt;/div&gt;&lt;div&gt;I think about about exploratory data analysis as having 3 interrelated stages:&lt;br /&gt;&lt;ol&gt;&lt;li&gt;&lt;i&gt;Validation or &lt;a href=&quot;http://en.wikipedia.org/wiki/Data_analysis&quot;&gt;Initial Data Analysis&lt;/a&gt;&lt;/i&gt;: Do I believe data is self-consistent, that the data was collected correctly, and that data represents what I think it does? This often goes under the name of “sanity checking”. For example, if manual testing of a feature was done, can I look at the logs of that manual testing? For a feature launched on mobile devices, do my logs claim the feature exists on desktops?&lt;/li&gt;&lt;li&gt;&lt;i&gt;Description&lt;/i&gt;: What’s the objective interpretation of this data? For example, “Users do fewer queries with 7 words in them?”, “The time page load to click (given there was a click) is larger by 1%”, and “A smaller percentage of users go to the next page of results.”&lt;/li&gt;&lt;li&gt;&lt;i&gt;Evaluation&lt;/i&gt;: Given the description, does the data tell us that something good is happening for the user, for Google, for the world? For example, “Users find results faster” or “The quality of the clicks is higher.”&lt;/li&gt;&lt;/ol&gt;By separating these phases, you can more easily reach agreement with others. Description should be things that everyone can agree on from the data. Evaluation is likely to have much more debate because you imbuing meaning and value to the data. If you do not separate Description and Evaluation, you are much more likely to only see the interpretation of the data that you are hoping to see. Further, Evaluation tends to be much harder because establishing the normative value of a metric, typically through rigorous comparisons with other features and metrics, takes significant investment.&lt;br /&gt;&lt;br /&gt;These stages do not progress linearly. As you explore the data, you may jump back and forth between the stages, but at any time you should be clear what stage you are in.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h3&gt;Confirm expt/data collection setup&lt;/h3&gt;Before looking at any data, make sure you understand the experiment and data collection setup. Communicating precisely between the experimentalist and the analyst is a big challenge. If you can look at experiment protocols or configurations directly, you should do it. Otherwise, write down your own understanding of the setup and make sure the people responsible for generating the data agree that it’s correct.&lt;br /&gt;&lt;br /&gt;You may spot unusual or bad configurations or population restrictions (such as valid data only for a particular browser). Anything notable here may help you build and verify theories later. Some things to consider:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;If it’s a features of a product, try it out yourself. If you can’t, at least look through screenshots/descriptions of behavior.&lt;/li&gt;&lt;li&gt;Look for anything unusual about the time range the experiment ran over (holidays, big launches, etc.)&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;&lt;h3&gt;Check vital signs&lt;/h3&gt;Before actually answering the question you are interested in (e.g. “Did users use my awesome new feature?”) you need to check for a lot of other things that may not be related to what you are interested in but may be useful in later analysis or indicate problems in the data. Did the number of users change? Did the right number of affected queries show up in all my subgroups? Did error rates changes? Just as your doctor always checks your height, weight, and blood pressure when you go in, check your data vital signs to potential catch big problems.&lt;/div&gt;&lt;div&gt;This is one important part of the “Validation” stage.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;Standard first, custom second&lt;/h3&gt;&lt;div&gt;This is a variant of checking for what shouldn’t change. Especially when looking at new features and new data, it’s tempting to jump right into the metrics that are novel or special for this new feature. But you should always look at standard metrics first, even if you expect them to change. For example, when adding a brand new UI feature to the search page, you should make sure you understand the impact on standard metrics like clicks on results before diving into the special metrics about this new UI feature. You do this because standard metrics are much better validated and more likely to be correct. If your new, custom metrics don’t make sense with your standard metrics, your new, custom metrics are likely wrong.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h3&gt;Measure twice, or more&lt;/h3&gt;Especially if you are trying to capture a new phenomenon, try to measure the same underlying thing in multiple ways. Then, check to see if these multiple measurements are consistent. By using multiple measurements, you can identify bugs in measurement or logging code, unexpected features of the underlying data, or filtering steps that are important. It’s even better if you can use different data sources for the measurements.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Check for reproducibility&lt;/h3&gt;Both slicing and consistency over time are particular examples of checking for reproducibility. If a phenomenon is important and meaningful, you should see it across different user populations and time. But reproducibility means more than this as well. If you are building models of the data, you want those models to be stable across small perturbations in the underlying data. Using different time ranges or random sub-samples of your data will tell you how reliable/reproducible this model is. If it is not reproducible, you are probably not capturing something fundamental about the underlying process that produced this data.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h3&gt;Check for consistency with past measurements&lt;/h3&gt;Often you will be calculating a metric that is similar to things that have been counted in the past. You should compare your metrics to metrics reported in the past, even if these measurements are on different user populations. For example, if you are looking at measuring search volume on a special population and you measure a much larger number than the commonly accepted number, then you need to investigate. Your number may be right on this population, but now you have to do more work to validate this. Are you measuring the same thing? Is there a rational reason to believe these populations are different? You do not need to get exact agreement, but you should be in the same ballpark. If you are not, assume that you are wrong until you can fully convince yourself. Most surprising data will turn out to be a error, not a fabulous new insight.&lt;br /&gt;New metrics should be applied to old data/features first&lt;br /&gt;&lt;br /&gt;If you gather completely new data and try to learn something new, you won’t know if you got it right. When you gather a new kind of data, you should first apply this data to a known feature or data. For example, if you have a new metric for user satisfaction, you should make sure it tells you your best features help satisfaction. Doing this provides validation for when you then go to learn something new.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h3&gt;Make hypotheses and look for evidence&lt;/h3&gt;Typically, exploratory data analysis for a complex problem is iterative. You will discover anomalies, trends, or other features of the data. Naturally, you will make hypotheses to explain this data. It’s essential that you don’t just make a hypothesis and proclaim it to be true. Look for evidence (inside or outside the data) to confirm/deny this theory. For example, If you believe an anomaly is due to the launch of some other feature or a holiday in Katmandu, make sure that the population the feature launched to is the only one affected by the anomaly. Alternatively, make sure that the magnitude of the change is consistent with the expectations of the launch.&lt;br /&gt;&lt;br /&gt;Good data analysis will have a story to tell. To make sure it’s the right story, you need to tell the story to yourself, predict what else you should see in the data if that hypothesis is true, then look for evidence that it’s wrong. One way of doing this is to ask yourself, “What experiments would I run that would validate/invalidate the story I am telling?” Even if you don’t/can’t do these experiments, it may give you ideas on how to validate with the data that you do have.&lt;br /&gt;&lt;br /&gt;The good news is that these hypotheses and possible experiments may lead to new lines of inquiry that transcend trying to learn about any particular feature or data. You then enter the realm of understanding not just this data, but deriving new metrics and techniques for all kinds of future analyses.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h3&gt;Exploratory analysis benefits from end to end iteration&lt;/h3&gt;When doing exploratory analysis, you should strive to get as many iterations of the whole analysis as possible. Typically you will have multiple steps of signal gathering, processing, modelling, etc. If you spend too long to get the very first stage of your initial signals perfect you are missing out on opportunities to get more iterations in the same amount of time. Further, when you finally look at your data at the end, you may make discoveries that change your direction. Therefore, your initial focus should not be on perfection but on getting something reasonable all the way through. Leave notes for yourself and acknowledge things like filtering steps and data records that you can’t parse/understand, but trying to get rid of all of them is a waste of time at the beginning of exploratory analysis.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h2&gt;Social&lt;/h2&gt;&lt;/div&gt;&lt;div&gt;&lt;h3&gt;Data analysis starts with questions, not data or a technique&lt;/h3&gt;There’s always a reason that you are doing some analysis. If you take the time to formulate your needs as questions or hypotheses, it will go a long way towards making sure that you are gathering the data you should be gathering and that you are thinking about the possible gaps in the data. Of course, the questions you ask can and should evolve as you look at the data. But analysis without a question will end up aimless.&lt;br /&gt;&lt;br /&gt;Further, you have to avoid the trap of finding some favorite technique and then only finding the parts of problems that this technique works on. Again, making sure you are clear what the questions are will help you avoid this.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h3&gt;Acknowledge and count your filtering&lt;/h3&gt;Almost every large data analysis starts by filtering the data in various stages. Maybe you want to consider only US users, or web searches, or searches with a result click. Whatever the case, you must&lt;br /&gt;&lt;ul&gt;&lt;li&gt;Acknowledge and clearly specify what filtering you are doing&lt;/li&gt;&lt;li&gt;Count how much is being filtered at each of your steps&lt;/li&gt;&lt;/ul&gt;Often the best way to do the latter is to actually compute all your metrics even for the population you are excluding. Then you can look at that data to answer questions like “What fraction of queries did my filtering remove?” &lt;br /&gt;&lt;br /&gt;Further, looking at examples of what is filtered is also essential for filtering steps that are novel for your analysis. It’s easy to accidentally include some “good” data when you make a simple rule of data to exclude.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h3&gt;Ratios should have clear numerator and denominators&lt;/h3&gt;Many interesting metrics are ratios of underlying measures. Unfortunately, there is often ambiguity of what your ratio is. For example, if I say click-through rate of a site on search results, is it:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;“# clicks on site’ / ‘# results for that site’&lt;/li&gt;&lt;li&gt;‘# search result pages with clicks to that site’ / ‘# search result pages with that site shown’&lt;/li&gt;&lt;/ul&gt;When you communicate results, you must be clear about this. Otherwise your audience (and you!) will have trouble comparing to past results and interpreting a metric correctly.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h3&gt;Educate your consumers&lt;/h3&gt;You will often be presenting your analysis and results to people who are not data experts. Part of your job is to educate them on how to interpret and draw conclusions from your data. This runs the gamut from making sure they understand confidence intervals to why certain measurements are unreliable in your domain to what typical effect sizes are for “good” and “bad” changes to understanding population bias effects.&lt;br /&gt;&lt;br /&gt;This is especially important when your data has a high risk of being misinterpreted or selectively cited. You are responsible for providing the context  and a full picture of the data and not just the number a consumer asked for.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h3&gt;Be both skeptic and champion&lt;/h3&gt;As you work with data, you must be both the champion of the insights you are gaining as well as a skeptic. You will hopefully find some interesting phenomena in the data you look at. When you have an interesting phenomenon you should ask both “What other data could I gather to show how awesome this is?” and “What could I find that would invalidate this?”. Especially in cases where you are doing analysis for someone who really wants a particular answer (e.g. “My feature is awesome”) you are going to have to play the skeptic to avoid making errors.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h3&gt;Share with peers first, external consumers second&lt;/h3&gt;A skilled peer reviewer can provide qualitatively different feedback and sanity-checking than the consumers of your data can, especially since consumers generally have an outcome they want to get. Ideally, you will have a peer that knows something about the data you are looking at, but even a peer with just experience looking at data in general is extremely valuable. The previous points suggested some ways to get yourself to do the right kinds of sanity checking and validation. But sharing with a peer is one of the best ways to force yourself to do all these things. Peers are useful at multiple points through the analysis. Early on you can find out about gotchas your peer knows about, suggestions for things to measure, and past research in this area. Near the end, peers are very good at pointing out oddities, inconsistencies, or other confusions.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h3&gt;Expect and accept ignorance and mistakes&lt;/h3&gt;There are many limits to what we can learn from data. Nate Silver makes a strong case in &lt;a href=&quot;http://www.amazon.com/dp/159420411X&quot;&gt;The Signal and the Noise&lt;/a&gt; that only by admitting the limits of our certainty can we make advances in better prediction. Admitting ignorance is a strength but it is not usually immediately rewarded. It feels bad at the time, but will ultimately earn you respect with colleagues and leaders who are data-wise. It feels even worse when you make a mistake and discover it later (or even too late!), but proactively owning up to your mistakes will translate into credibility. Credibility is the key social value for any data scientist.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Closing thoughts&lt;/h2&gt;&lt;br /&gt;No short list of advice can be complete even when we break through the barrier of the Top 10 List format (for those of you who weren’t counting, there are 24 here). As you apply these ideas to real problems, you’ll find the habits and techniques that are most important in your domain, the tools that help you do those analyses quickly and correctly, and the advice you wish were on this list. Make sure you share what you’ve learned so we can all be better data scientists.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;i&gt;I’d like to thank everyone who provided insight that went into this document, but especially Diane Tang, Rehan Khan, Elizabeth Tucker, Amir Najmi, Hilary Hutchinson, Joel Darnauer, Dale Neal, and Aner Ben-Artzi.&lt;/i&gt;&lt;/div&gt;</content><link rel='replies' type='application/atom+xml' href='https://www.unofficialgoogledatascience.com/feeds/1874103119442359898/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='https://www.unofficialgoogledatascience.com/2016/10/practical-advice-for-analysis-of-large.html#comment-form' title='7 Comments'/><link rel='edit' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/1874103119442359898'/><link rel='self' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/1874103119442359898'/><link rel='alternate' type='text/html' href='https://www.unofficialgoogledatascience.com/2016/10/practical-advice-for-analysis-of-large.html' title='Practical advice for analysis of large, complex data sets'/><author><name>Amir Najmi</name><uri>http://www.blogger.com/profile/18174523203317227640</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://4.bp.blogspot.com/-CyVXj5pF5qM/WBgHAg3fXHI/AAAAAAAAb2k/A6ESjMJ8cBsgUf92Rts-_IPrGO01HriaQCK4B/s72-c/blog.png" height="72" width="72"/><thr:total>7</thr:total></entry><entry><id>tag:blogger.com,1999:blog-5825758052688213474.post-4927176678689606760</id><published>2016-09-30T09:06:00.000-07:00</published><updated>2018-08-31T22:08:14.116-07:00</updated><title type='text'>Statistics for Google Sheets</title><content type='html'>&lt;i style=&quot;font-size: x-large;&quot;&gt;&lt;b&gt;Editor&#39;s note:&amp;nbsp;&lt;/b&gt;&lt;/i&gt;&lt;i style=&quot;font-size: x-large;&quot;&gt;The Google Sheets add-on described in this blog post is no longer supported externally by Google.&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;By STEVEN L. SCOTT&lt;br /&gt;&lt;br /&gt;&lt;i&gt;Big data is new and exciting, but there are still lots of small data problems in the world. Many people who are just becoming aware that they need to work with data are finding that they lack the tools to do so. The statistics app for Google Sheets hopes to change that.&lt;/i&gt;&lt;br /&gt;&lt;i&gt;&lt;br /&gt;&lt;/i&gt;&lt;i&gt;&lt;b&gt;Editor&#39;s note:&lt;/b&gt; We&#39;ve mostly portrayed data science as statistical methods and analysis approaches based on big data. But some of our readers have perfectly validly pointed out that this may be too narrow a view. While big data remains a focus of this blog, there are exciting innovations happening in other areas as well. Steve&#39;s post is an excellent example of this, and we are thrilled to see him contribute this month&#39;s article.&lt;/i&gt;&lt;br /&gt;&lt;i&gt;&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;&lt;div&gt;&lt;h2&gt;Introduction&lt;/h2&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;Statistics for Google Sheets is an add-on for Google Sheets that brings elementary statistical analysis tools to spreadsheet users. The app focuses on material commonly taught in introductory statistics and regression courses, with the intent that students who have taken these courses should be able to carry out the analyses that they learned when they move on to jobs in the work force.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Frequent readers of this blog probably don’t think of themselves as spreadsheet users, and may be wondering why anyone would want to carry out data analysis in a spreadsheet environment. After all, working with data in spreadsheets involves serious limitations, with scale being an obvious one. Google sheets is currently limited to 2 million cells, which is too small to hold a modern medium-to-large size data set. A spreadsheet-based app can’t (and shouldn’t!) hope to replace R, SAS, or similar packages designed by and for statistics experts.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Yet while the rise of “big data” problems has not reduced the number of “small data” problems in the world, the rise of “data science” has put data on the radar of many non-specialists. These are users who may have had (or may be taking) an introductory statistics course in college, but who have chosen to pursue skills other than data science. Spreadsheets are a natural choice for this audience both because spreadsheets are ubiquitous and because they provide an intuitive way to visually inspect the raw data.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;The goal of the Statistics app is to “democratize data science” by putting elementary statistics capabilities in the hands of anyone with a Google account. Because the app is merely an add-on to Google sheets, its learning curve is considerably gentler than that of an entirely separate software package. Because it runs in the browser it imposes no additional installation or maintenance requirements, which can be helpful for IT administrators as well as analysts and students. It is easy to underestimate the amount of work involved for an IT department to support individual software packages, which can make it hard for a young analyst in a medium to large sized corporation to gain access to statistical software, even a free system like R. Because most employees do not have administrator privileges on their computers, many of the ideas taught in elementary statistics courses go unimplemented in the workplace for lack of software.&lt;br /&gt;&lt;img height=&quot;1&quot; src=&quot;https://lh4.googleusercontent.com/WAp3SZL9ZXqXSQO6wLZuVoJTT0zBvQXpvkv9yKOiuuZ7cv_Yiyy_udyRuZVJCtBrTHwKNvN5423ZrVbwUtnfi_glwC4YGo5StyWe9Sij4THrCZpGjZN_bJX1Xnwh7IzQDwnWOmVw&quot; style=&quot;border: none; font-family: arial; font-size: 14.6667px; transform: rotate(0rad); white-space: pre-wrap;&quot; /&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;h2&gt;Example: Stock Market Returns&lt;/h2&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;To see the app in action, open this data set showing &lt;a href=&quot;https://docs.google.com/spreadsheets/d/1y0exLBlBqk3g4929etIxv2Bnc4jc_fuplukmvRSR19Q/edit?usp=sharing&quot; target=&quot;_blank&quot;&gt;S&amp;amp;P500 stock market returns&lt;/a&gt; in Google Sheets. This document is shared with the Internet, so make a personal copy that you can edit by selecting &lt;i&gt;File → Make a copy&lt;/i&gt; from the menu. If you don’t already have the &lt;a href=&quot;https://sites.google.com/site/statisticsforspreadsheets/&quot; target=&quot;_blank&quot;&gt;Statistics app&lt;/a&gt; installed, select &lt;i&gt;Add-ons → Get add-ons&lt;/i&gt; and search for the app named &lt;i&gt;Statistics&lt;/i&gt;.&lt;br /&gt;&lt;br /&gt;Of course, we’re not providing investment advice, and each person would need to evaluate the results for themselves. What we’re going to do is work through how the app might be used.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;One of the variables in the data (column H) is named &lt;i&gt;Return&lt;/i&gt;. It shows the percentage change relative to the previous day’s closing value. A related variable (in column J) is &lt;i&gt;UpDay?&lt;/i&gt;, which indicates whether &lt;i&gt;Return &amp;gt; 0&lt;/i&gt;. We can explore the values in these two columns by selecting &lt;i&gt;Add-ons → Statistics → Describe data&lt;/i&gt; and entering these two variables in the Variables section of the right hand configuration pane (using Ctrl+Click or Cmd+Click to select several values). Notice that &lt;i&gt;UpDay?&lt;/i&gt; is automatically treated as a factor because its values (the words “Up” and “Down”) are non-numeric.&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;span id=&quot;docs-internal-guid-e10de34f-6596-da21-981a-b8627b1840c8&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 14.6667px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;img height=&quot;191&quot; src=&quot;https://lh3.googleusercontent.com/FvEXKiNFD5fTw2HAQPKqNEtstXB9dVK3er-HrgjkGHvNfIgeTx6bdXeCFtCTDYaQABzxoCw_GO4eHkm5FidqBdWxGf8Uqu1P1L3K6exNNnNFOsx9uSpfAGh5v0S6IkiDGmSDN9h5&quot; style=&quot;-webkit-transform: rotate(0.00rad); border: none; transform: rotate(0.00rad);&quot; width=&quot;624&quot; /&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;Clicking the Create button at the bottom of the right hand pane creates a new sheet named &quot;Univariate 1&quot; filled with numerical and graphical summaries describing the data. Notice that the summaries are different for numeric and factor data. The histogram of &lt;i&gt;Return&lt;/i&gt; appears nearly symmetric, but the histogram of &lt;i&gt;UpDay?&lt;/i&gt; shows that there have been slightly more up days than down. We also see a third category named “missing”. Both &lt;i&gt;Return&lt;/i&gt; and &lt;i&gt;UpDay?&lt;/i&gt; were constructed from raw data in columns A-G using spreadsheet formulas, which are exposed in the spreadsheet’s formula bar when you click on the data in these columns. In particular, Return is the percent change from the previous day, so the formula for the very first day (the last row in the data set) returns an error, because there is no previous day to compare against. The app considers the cell containing the error to be missing data.&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;clear: left; float: left; font-family: &amp;quot;arial&amp;quot;; font-size: 14.6667px; margin-bottom: 1em; margin-right: 1em; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;img height=&quot;295&quot; src=&quot;https://lh5.googleusercontent.com/lEZzet8yi2gEkXPp_J836QDS_Zm4Xc8F3kct0rEJf8ySQbhyUwc3KIw9gxenw4xJAn5GQSnpYCvyQmcoUKnTtLd2apLxd70dSSJH1fxHamVydFzEjIM9jzsyQmCLr6YX2qssvWlH&quot; style=&quot;border: none; transform: rotate(0rad);&quot; width=&quot;293&quot; /&gt;  &lt;/span&gt;&lt;span id=&quot;docs-internal-guid-e10de34f-6597-0212-db10-46d11adc93ef&quot;&gt;&lt;/span&gt;&lt;img height=&quot;282&quot; src=&quot;https://lh4.googleusercontent.com/R-LEcSMdMvpeKNJ7Fn9deARMy5CqiHSu_f57AmGtXYSeAPhO41oJg2vNadMUjJa92X7B_xd_QunNOm1ym2aCW9BUEJiV5H5iS3Uhd94FUSWEem9ylYWaBggA9cjL9hCYWtC3Yiek&quot; style=&quot;border: none; font-family: Arial; font-size: 14.6667px; transform: rotate(0rad); white-space: pre-wrap;&quot; width=&quot;285&quot; /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;The summaries of &lt;i&gt;Return&lt;/i&gt; simply omit the missing observation (although the presence of missing data is reported on the “N missing” line, below the plots), but the factor variable &lt;i&gt;UpDay?&lt;/i&gt; counts the missing value as a separate category. We can manually exclude the missing values from the analysis by adding the variable &lt;i&gt;Use?&lt;/i&gt; (found in column L) as a filter. A filter is a variable filled with TRUE and FALSE values. Setting a filter instructs the app to use the rows where the filter is TRUE, and to omit the rows where it is FALSE. We have set the last two entries in &lt;i&gt;Use?&lt;/i&gt; to FALSE because we will soon care about the variables &lt;i&gt;LagReturn&lt;/i&gt; and &lt;i&gt;LagUpDay?&lt;/i&gt; which give the values of Return and &lt;i&gt;UpDay?&lt;/i&gt; on the preceding day. Each of these introduces an additional missing observation, which we also wish to exclude.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Repeating the analysis after setting the filter (and clicking &lt;i&gt;Create&lt;/i&gt; again) produces a summary of &lt;i&gt;Return&lt;/i&gt; that is essentially unchanged, but the missing category is no longer present in the histogram for &lt;i&gt;UpDay?&lt;/i&gt;.&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;clear: left; float: left; font-family: &amp;quot;arial&amp;quot;; font-size: 14.6667px; margin-bottom: 1em; margin-right: 1em; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;img height=&quot;320&quot; src=&quot;https://lh5.googleusercontent.com/lEZzet8yi2gEkXPp_J836QDS_Zm4Xc8F3kct0rEJf8ySQbhyUwc3KIw9gxenw4xJAn5GQSnpYCvyQmcoUKnTtLd2apLxd70dSSJH1fxHamVydFzEjIM9jzsyQmCLr6YX2qssvWlH&quot; style=&quot;border: none; transform: rotate(0rad);&quot; width=&quot;317&quot; /&gt;  &lt;/span&gt;&lt;span id=&quot;docs-internal-guid-e10de34f-6597-5275-bfd1-6283f3963a68&quot;&gt;&lt;/span&gt;&lt;img height=&quot;320&quot; src=&quot;https://lh5.googleusercontent.com/3aRcL3Lcqv40H9RfG_5WSIaqowjYTfWSN72oEL7Vckp7mom3ucyYG4GlNq45-6u51Ki5T7C2JY4no1mQT3lCmtAaycmRR-BB7iu38PjAEhIKopLzdlHJwXYR1ANMHYhVA-WijnGL&quot; style=&quot;border: none; font-family: Arial; font-size: 14.6667px; transform: rotate(0rad); white-space: pre-wrap;&quot; width=&quot;304&quot; /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;Beneath the graphs are numerical summaries. The mean daily return is zero to 3 decimal places, but clicking on the cell containing this number shows the mean to full precision. The standard deviation of daily returns is 1.3%. There have been 2230 up days and 1970 down days. Other charts include a normal quantile plot for judging whether returns are normally distributed (they have fatter tails than normal), and Pareto and pie charts for judging the relative proportion of up days.&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;span id=&quot;docs-internal-guid-e10de34f-6597-8cce-0a01-cf5a8abf55fe&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 14.6667px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;img height=&quot;109&quot; src=&quot;https://lh5.googleusercontent.com/4_ggkyBGiBFqTlI-O50-9tV_F_E7SnnrnuUJflJXO-bwBbJckiuWjS3pTfUbzewyLrWhJjSqBUSYobmL9kq5e9H6M4FxYSmjDjDSAa4pEzny5GcnSY64a1Dp6WyVOgqjyVM83gjG&quot; style=&quot;-webkit-transform: rotate(0.00rad); border: none; transform: rotate(0.00rad);&quot; width=&quot;190&quot; /&gt;  &lt;/span&gt;&lt;/span&gt;&lt;img height=&quot;244&quot; src=&quot;https://lh6.googleusercontent.com/dHBfpBnxh6SCeAWOAgLHtjQBQVaGzU1hB_akCO2oPCiUHpie4X-w5Bi-6Q0MNmh6FKbq3PYmYPWctI8ZvBXCcaeTiApyJO-7gPVbJNkfBSqbJXxdkklOUgyeqlcwEF_GxK_mlS_n&quot; style=&quot;border: none; font-family: Arial; font-size: 14.6667px; transform: rotate(0rad); white-space: pre-wrap;&quot; width=&quot;197&quot; /&gt;&amp;nbsp;&amp;nbsp;&lt;img height=&quot;67&quot; src=&quot;https://lh6.googleusercontent.com/N9ml5-kzp2R9YnkjboA5BDkoY5ECbtGd2l-XQqeSmVMv8ZcXeV0VhCfGl3u0UsAnXq-6-FVDtXzY5IGMpop-1labWL3nYgqUK8eRrQCuv1AERdlvYiAajI3H7rzUVzJmD7pZX-oH&quot; style=&quot;border: none; font-family: Arial; font-size: 14.6667px; transform: rotate(0rad); white-space: pre-wrap;&quot; width=&quot;209&quot; /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;Things get more interesting when we look for factors that potentially explain stock market returns. If we click the “By” button in the right hand pane we can see how the distribution of returns (and up/down days) varies with the return value the previous day (&lt;i&gt;LagReturn&lt;/i&gt;), or whether the previous day was up (&lt;i&gt;LagUpDay?&lt;/i&gt;). Let&#39;s choose the latter first.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;The boxplot below compares the distributions of returns on days following up and down days. The plot shows that the two days with the highest returns (which are substantially higher than other returns) occurred on days following down days.&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;span id=&quot;docs-internal-guid-e10de34f-6597-d14e-8ca1-65fb7edccd98&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 14.6667px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;img height=&quot;235&quot; src=&quot;https://lh5.googleusercontent.com/NKdZWrdQ5gZR-akWj3RBqd9dKbK-kQ567trzouRnlcGJQvtOEps34Br2ZaPEzRzQ9sye_pUHcckU02F1wkEw4v2TRp21J6LB5y45iW8ZkwfrotzecUvM5P8PvCm6jCfSMgOeHlid&quot; style=&quot;border: none; transform: rotate(0rad);&quot; width=&quot;528&quot; /&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;Setting the Date variable as a label (using the &lt;i&gt;Labels&lt;/i&gt; button in the right hand pane) allows us to click on the individual dots and find that both extreme points took place in October of 2008, during the financial crisis when the markets were extra volatile and prone to wild swings. On these two days the S&amp;amp;P 500 increased by more than 10% in a single day. The extreme losses on the left side of the plot also took place in October 2008, which was a bad month for the markets overall. If we wanted to, we could set some of these extreme observations aside in future analyses by setting their values to FALSE in the filter variable.&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;span id=&quot;docs-internal-guid-e10de34f-6598-1035-4862-7db5268a03d7&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 14.6667px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;img height=&quot;231&quot; src=&quot;https://lh4.googleusercontent.com/nJhz9ra_59p3JMbC9UGddxfFhxkhdc8DquLbmeHWERBzGOkpj2U_ZARtox6TWpVYfYwS5ANczOMUmwadAYjqjs0XhRRvJx9uYPjo9MZlBXdkXhX68GqmjhHnNgDQ8wLe_1Cr31py&quot; style=&quot;-webkit-transform: rotate(0.00rad); border: none; transform: rotate(0.00rad);&quot; width=&quot;522&quot; /&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;If you look closely at the boxplots you can see that returns following down days have slightly greater variation than returns following up days. The boundaries of the box are wider, and the stems extending from the box are slightly longer. The means and standard deviations of the two groups bear out the impression we obtained from the plots.&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;span id=&quot;docs-internal-guid-e10de34f-6598-4206-a4ba-871d10a49192&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 14.6667px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;img height=&quot;97&quot; src=&quot;https://lh3.googleusercontent.com/pdORM8vagsfEFuPgVWTgs9d92ztZrVxN0-D5R6U0zwx6bBZsT5z9u5P-85rPpqQeVP0f3u1X_vK1w2XuArACBCy1uyKySRFif6P0V9ix8c-I_AouFP70D1qDOrlg2L5kMVLJ4Dj6&quot; style=&quot;-webkit-transform: rotate(0.00rad); border: none; transform: rotate(0.00rad);&quot; width=&quot;411&quot; /&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;The mosaic plot for the relationship between consecutive up and down days shows a slight effect - following a down day, the probability of a down day is slightly lower than when following an up day. To see if this effect is statistically significant we can look at the chi-square test below the plot. The chi-square value of 18.026 is too large to be observed by chance with 1 degree of freedom, so it is probably safe to conclude that the distribution following up days really is different than the distribution following down days.&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;span id=&quot;docs-internal-guid-e10de34f-6598-67ec-a83f-696f228fd7f8&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 14.6667px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;img height=&quot;289&quot; src=&quot;https://lh6.googleusercontent.com/kCBZmYPq6ea9bqJJUpUSkmWjDKcZ6YAxFSquD862ElLE-L3WeUqzrwGZvEXvIiPt9pLKO8eKydSbtQx7BHlI7_iNcStBe6ObZnqnFPZRZCez5tsejnF7dKd519n31IQ6IsDA2KsT&quot; style=&quot;-webkit-transform: rotate(0.00rad); border: none; transform: rotate(0.00rad);&quot; width=&quot;370&quot; /&gt;  &lt;/span&gt;&lt;/span&gt;&lt;img height=&quot;54&quot; src=&quot;https://lh6.googleusercontent.com/rusfwpEC023sHa-IjwOWyjWTLBc3sTY-iVoZweBifXDb6gbdfjQBzh-wLBKISM-l6aPCJVgM3l8Kzo-_vuRoTStL0pOwOLIDgFkc334bkKO_ZNgDNy-IJhp0TIuqHsXi4tPiRy3B&quot; style=&quot;border: none; font-family: Arial; font-size: 14.6667px; transform: rotate(0rad); white-space: pre-wrap;&quot; width=&quot;219&quot; /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;Now change the variable in the “By” section of the configuration pane from &lt;i&gt;LagUpDay?&lt;/i&gt; to &lt;i&gt;LagReturn&lt;/i&gt;. Doing so allows us to look at how predictable daily returns are as a function of the previous day’s return. The scatterplot describing this relationship shows very little pattern, but there is a clearly negative slope to the trend line. Looking below this plot we see there is a small $R^2$ of .006, but the p-value for the slope of the regression line is highly significant with p &amp;lt; .0001. This is very strong evidence of a very small (but nonzero) effect. In other words, there is a real trend towards self correction in the market but it is not very strong.&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;span id=&quot;docs-internal-guid-e10de34f-6598-9dc6-f9de-836d987943c2&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 14.6667px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;img height=&quot;346&quot; src=&quot;https://lh4.googleusercontent.com/WAp3SZL9ZXqXSQO6wLZuVoJTT0zBvQXpvkv9yKOiuuZ7cv_Yiyy_udyRuZVJCtBrTHwKNvN5423ZrVbwUtnfi_glwC4YGo5StyWe9Sij4THrCZpGjZN_bJX1Xnwh7IzQDwnWOmVw&quot; style=&quot;border: none; transform: rotate(0rad);&quot; width=&quot;413&quot; /&gt; &lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span id=&quot;docs-internal-guid-e10de34f-6598-ae64-6b83-af488bafe115&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 14.6667px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;img height=&quot;233&quot; src=&quot;https://lh5.googleusercontent.com/o-CgBhRYKGsvKkOyn6fDv26Yb7XrQDIZrtSN26do9T7CrsSTtiQEoo--LBUzmTbZc7H2gb1ntpWoTkJpe13cYM7Ax-z9cIGkbAK1Z7pwnR6-631dU29bKx8LuBv4SF7oJq4vJr5c&quot; style=&quot;-webkit-transform: rotate(0.00rad); border: none; transform: rotate(0.00rad);&quot; width=&quot;409&quot; /&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;Finally, you can use logistic regression to see how a previous day’s return affects the probability of the next day’s return being positive. Here again we see a deviance $R^2$ of only 0.002, but the p-value for the logistic regression coefficient is .0013, which fits with our previous conclusion about strong evidence of a small effect.&amp;nbsp;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;span id=&quot;docs-internal-guid-e10de34f-6598-d2ec-db52-0e7f8bcd2097&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 14.6667px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;img height=&quot;219&quot; src=&quot;https://lh3.googleusercontent.com/4hUuOb-hT24zPH3Ef8AybOzrgn-T-EJ8hPvr6zRX-ttWA01MJJG99ey_K81Pajv-ujcCmlFm_lrnZd8mUtltXoRlL8WY-_LUu-y5LG9HyxKsM2LNIJwNosKCapajGR5ldlYpYjBV&quot; style=&quot;-webkit-transform: rotate(0.00rad); border: none; transform: rotate(0.00rad);&quot; width=&quot;423&quot; /&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;The weakness is evident when we look at the model fit plot, which plots the proportion of up days vs the average estimated probability of an up day in 10 equal-sample-size buckets, and the ROC curve, which is very nearly a straight line.&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;clear: left; float: left; font-family: &amp;quot;arial&amp;quot;; font-size: 14.6667px; margin-bottom: 1em; margin-right: 1em; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;img height=&quot;318&quot; src=&quot;https://lh5.googleusercontent.com/-ih0lMAmmD_ajGz6uQkc8jfV5nWXOvPrSh9P3suTGICfFFKyMlLokizGrpwtOw0_Hx-DgRtqsb2HsPbe-0ZH4TACCmWuxrCG2LqZ4De0N3IUMRGcNbhKZI1hjRcxus5_mluz1b5E&quot; style=&quot;border: none; transform: rotate(0rad);&quot; width=&quot;320&quot; /&gt;  &lt;/span&gt;&lt;span id=&quot;docs-internal-guid-e10de34f-6599-149a-b844-8221e1903384&quot;&gt;&lt;/span&gt;&lt;img height=&quot;293&quot; src=&quot;https://lh6.googleusercontent.com/40yzbG94zsQMk5_Ux3cA8Q5sYoPmDFfIDxR_M8zfScaEzDVYwIHMsBXm0XHM6QF9Q0pjB8-MMdmsOLMA_j9HKI3RdCCSCDIu9QlAGYqDdy7Xn2BBM6dWcKwhNDb3Z79mFYC5WI1g&quot; style=&quot;border: none; font-family: Arial; font-size: 14.6667px; transform: rotate(0rad); white-space: pre-wrap;&quot; width=&quot;290&quot; /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Where to go from here?&lt;/h2&gt;&lt;div&gt;&lt;div&gt;Statistics for Google Sheets gives analysts and students the tools to conduct elementary statistical analyses in a familiar spreadsheet environment. Keep in mind that everything shown above was based on one menu selection: &lt;i&gt;Add-ons → Statistics → Describe Data&lt;/i&gt;. More sophisticated tools for linear and logistic regression are available on the other (&lt;i&gt;Regression&lt;/i&gt;) menu selection. Taken together with native spreadsheet functions for elementary data management, probability, and hypothesis testing the app currently provides all the tools needed for a 1-2 semester introductory statistics course, in a format that students can take with them when they become analysts.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;We hope to add more capabilities to the app in the future. We’d like to scale the tool to handle bigger data (possibly handling data sources from outside the spreadsheet), improve coverage in the help system (to better educate the public), build in more elaborate modeling capabilities (methods for handling time series are an obvious choice), and provide more advanced statistical methods (e.g. Bayes and machine learning tools). With increased familiarity these tools can become more broadly understood and more broadly used.&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;</content><link rel='replies' type='application/atom+xml' href='https://www.unofficialgoogledatascience.com/feeds/4927176678689606760/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='https://www.unofficialgoogledatascience.com/2016/09/statistics-for-google-sheets.html#comment-form' title='7 Comments'/><link rel='edit' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/4927176678689606760'/><link rel='self' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/4927176678689606760'/><link rel='alternate' type='text/html' href='https://www.unofficialgoogledatascience.com/2016/09/statistics-for-google-sheets.html' title='Statistics for Google Sheets'/><author><name>Kay Brodersen</name><uri>http://www.blogger.com/profile/15877220456041691465</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://lh4.googleusercontent.com/WAp3SZL9ZXqXSQO6wLZuVoJTT0zBvQXpvkv9yKOiuuZ7cv_Yiyy_udyRuZVJCtBrTHwKNvN5423ZrVbwUtnfi_glwC4YGo5StyWe9Sij4THrCZpGjZN_bJX1Xnwh7IzQDwnWOmVw=s72-c" height="72" width="72"/><thr:total>7</thr:total></entry><entry><id>tag:blogger.com,1999:blog-5825758052688213474.post-5882522097717497514</id><published>2016-08-31T23:24:00.000-07:00</published><updated>2017-02-01T19:35:56.972-08:00</updated><title type='text'>Next generation tools for data science</title><content type='html'>By DAVID ADAMS&lt;br /&gt;&lt;br /&gt;&lt;i&gt;Since inception, &lt;a href=&quot;http://www.unofficialgoogledatascience.com/2015/08/welcome-to-unofficial-google-data.html&quot;&gt;this blog has defined “data science”&lt;/a&gt; as inference derived from data too big to fit on a single computer. Thus the ability to manipulate big data is essential to our notion of data science. While MapReduce remains a fundamental tool, many interesting analyses require more than it can offer. For instance, the well-known Mantel-Haenszel estimator cannot be implemented in a single MapReduce. &lt;b&gt;Apache Spark&lt;/b&gt; and &lt;b&gt;Google Cloud Dataflow&lt;/b&gt; represent two alternatives as “next generation” data processing frameworks. This post compares the two, relying on the author’s first-hand experience and subsequent background research.&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Introduction&lt;/h2&gt;&lt;br /&gt;&lt;div&gt;That MapReduce was &lt;b&gt;the&lt;/b&gt; solution to write data processing pipelines scalable to hundreds of terabytes (or more) is evidenced by the massive uptake. This was true within Google as well as outside of Google in the form of Hadoop/MapReduce (for some “Hadoop” and “data science” are synonyms). However, it didn’t take long for the pain of writing and running many-stage MapReduce programs to become apparent. MapReduce’s limitations motivated two different groups, AMPLab (Apache Spark) and Google (Cloud Dataflow), to write next-generation data processing frameworks. The two groups came at the problem from different vantage points and this can be seen in both obvious and subtle differences in the end-products. Dataflow was designed as a marriage of Google frameworks for expressing complex batch pipelines (FlumeJava) and streaming (MillWheel) pipelines.  Spark was developed &lt;a href=&quot;http://blog.madhukaraphatak.com/history-of-spark/&quot;&gt;at UC Berkeley to enable exploratory analysis and ML at scale&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;Six months ago, &lt;a href=&quot;https://cloud.google.com/blog/big-data/2016/08/cloud-dataflow-apache-beam-and-you&quot;&gt;Google donated the Cloud Dataflow programming model and SDKs to the Apache Software Foundation&lt;/a&gt;, resulting in the incubating project &lt;a href=&quot;http://beam.incubator.apache.org/&quot;&gt;&lt;i&gt;Apache Beam&lt;/i&gt;&lt;/a&gt;. Going forward, &lt;i&gt;Apache Beam&lt;/i&gt; will become the way to express data processing pipelines, while &lt;a href=&quot;https://cloud.google.com/dataflow/&quot;&gt;&lt;i&gt;Cloud Dataflow&lt;/i&gt;&lt;/a&gt; remains as a fully managed service for executing these pipelines. Similarly, I’ll refer to the programming model and API as &lt;i&gt;Beam&lt;/i&gt; and the service that runs on GCP as &lt;i&gt;Dataflow&lt;/i&gt;. Example code in this post uses the current Dataflow SDK, but similar concepts and programming patterns hold in Apache Beam.&lt;br /&gt;&lt;br /&gt;In this post, I’m going to shed light on the similarities and differences of Spark and Beam/Dataflow. For framing purposes, &lt;b&gt;Spark’s sweet spot is quickly developing exploratory/interactive analysis and iterative algorithms&lt;/b&gt;, e.g., gradient descent and MCMC, whereas &lt;b&gt;Dataflow’s sweet spot is processing streaming data and highly-optimized, robust, fixed pipelines&lt;/b&gt;.&lt;br /&gt;&lt;br /&gt;The rest of the post is organized as follows: the most important difference between the two, a motivating example with implementations in Spark and Beam/Dataflow, an example of an clustering algorithm written in Spark, examples of streaming processing in Spark and Dataflow, and finally, other major differences between the frameworks.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;When MapReduce is not enough&lt;/h2&gt;MapReduce is a great tool for single-stage parallel data processing. A few pain points emerge when trying to use MapReduce beyond small pipelines:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;Expressing complex pipelines requires significant boilerplate, separate programs, and the “interface” between stages to be files.&lt;/li&gt;&lt;li&gt;Writing intermediate results to disk between stages of pipelines is a serious bottleneck and forces the user to hand-optimize the placement of these divisions&lt;/li&gt;&lt;li&gt;Performing exploratory analysis requires reading and writing to disk, which is slow&lt;/li&gt;&lt;li&gt;Expressing streaming pipelines (low-latency and infinite data sources) is not supported,&lt;/li&gt;&lt;li&gt;Writing multi-stage pipelines are easily to stumble upon, take for example trying to measure the effects of a randomized experiment on ratios of metrics using Mantel-Haenszel (see more below).&lt;/li&gt;&lt;/ul&gt;Both Beam and Spark solve most of these problems storing results of MapReduce steps in by optimizing away many steps and storing necessary intermediate results in memory, &lt;i&gt;PCollections&lt;/i&gt; for Dataflow and &lt;i&gt;resilient distributed dataset&lt;/i&gt; (RDD) for Spark, instead of writing to disk just to load back into memory again.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h2&gt;The core difference: graph evaluation&lt;/h2&gt;The fundamental and enduring difference between Spark and Beam is that Spark only builds as much of the computation graph as needed (for lazy evaluation on actions) whereas Dataflow builds the entire computation graph before it is optimized and sent to the service or cluster to be executed.  Many significant differences between the two are a consequence of this distinction. Thus it is easy to use Spark for interactive analysis (from a python or scala shell) and to prototype ML algorithms but harder to do so in Dataflow. To be clear, it isn’t that one cannot do ML with Dataflow — single pass, streaming algorithms are fine. But many ML algorithms, such as basic gradient descent and MCMC, make data-driven decisions of how many iterations they need to run until convergence. Spark provides the user with greater flexibility.&lt;br /&gt;&lt;br /&gt;On the other hand, the benefits of requiring a complete computation graph are that it allows systems that execute Beam pipelines, like Dataflow to optimize &lt;b&gt;the complete graph&lt;/b&gt;. More importantly, decoupling graph construction from execution lets Beam create a language-agnostic representation of the computation graph. Thus, Dataflow graphs can be executed on other distributed processing back-ends, even including Spark. This property is what enabled the creation of the &lt;a href=&quot;http://beam.incubator.apache.org/&quot;&gt;Apache Beam project&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;Another consequence of Beam’s decision to require specification of the full computation graph is that Dataflow may be offered as a service. This &lt;i&gt;running-a-pipeline-job-as-a-service&lt;/i&gt; takes a graph specification and data inputs and produces outputs. The &lt;a href=&quot;https://cloud.google.com/dataflow/&quot;&gt;Cloud Dataflow Service&lt;/a&gt; can be thought of as a black box where details such as the number of VMs to use, overall or per stage, or distribution of work can all be left up to the service. Of course, most of these details can be specified if desired, &lt;a href=&quot;https://cloud.google.com/blog/big-data/2016/03/comparing-cloud-dataflow-autoscaling-to-spark-and-hadoop&quot;&gt;see here&lt;/a&gt; for a more expansive discussion. This is in stark comparison to Spark, which is cluster-oriented instead of job-oriented. To run a fixed pipeline in Spark requires spinning up a Spark cluster, running the pipeline and tearing the cluster down. Even though there are tools to make this process easier (see &lt;a href=&quot;https://cloud.google.com/dataproc/&quot;&gt;Cloud Dataproc&lt;/a&gt; on GCP or &lt;a href=&quot;https://databricks.com/&quot;&gt;Databricks&lt;/a&gt; on AWS) cluster management remains the responsibility of the user.&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;h2&gt;A motivating example for going beyond Hadoop/MapReduce&lt;/h2&gt;To compare the two frameworks, I’m going to solve the same problem twice then highlight key differences. Suppose we have on an online auction site with a wide variety of products for sale. Users browse the images and specifications of products in the inventory and may choose to bid on them. Because it is an auction site, prices for any item vary all the time. Products range in value from a few dollars (emoji eraser kits) to thousands (nitro coffee kits) so an important way we track them is by product category.&lt;br /&gt;&lt;br /&gt;We want to run a randomized control experiment to determine the effect of high-resolution images on our sales. Say, we wish to determine the impact of the experiment on average sell price. The naive thing to do would be simply to compare the average selling price of treatment to that of control. However this could be misleading — this measure will show a difference even if the price for each individual item remained constant and only the mix of products sold was affected. To be concrete, imagine the experiment caused us to sell relatively more nitro coffee kits than emoji kits. The &lt;b&gt;Mantel-Haenszel estimator&lt;/b&gt; (also known as Cochran-Mantel-Haenszel) is meant precisely to address this problem by adjusting for the mix of items being compared between treatment and control. The basic idea is first to compute the average price separately in treatment and control for each individual stratum (aka &quot;slice&quot;) and then to take the ratio of weighted combination of prices in treatment to that of control. The key is that the combination weights are the same for treatment and control, and these are chosen in some way to minimize estimator variance.&lt;br /&gt;&lt;br /&gt;Widely used in medicine for count data, the MH estimator and its generalizations are ubiquitous within data science at Google. In a future post we will cover applications and extensions of MH to analytic problems at Google. For now, we&#39;ll just go ahead and compute point MH estimates for this (non-count) price data, using product categories as our slices. Let&#39;s say in product category $i$ we sell $n_{t,i}$ and $n_{c,i}$ items in treatment and control respectively for a total sale value of $X_{t,i}$ and $X_{c,i}$. The MH estimate for the (mix-adjusted) ratio of prices between treatment and control is&lt;br /&gt;$$&lt;br /&gt;MH(t,c)=\frac{\sum_i w_i \frac{X_{t,i}}{n_{t,i}}} {\sum_i w_i \frac{X_{c,i}}{n_{c,i}}}&lt;br /&gt;$$ where the weights $w_i$ is the harmonic mean of $n_{t,i}$ and $n_{c,i}$. To facilitate computation, we will rewrite the formula as&lt;br /&gt;$$&lt;br /&gt;MH(t,c)=\frac{\sum_i&amp;nbsp;X_{t,i} \left(\frac{n_{c,i}}{n_{t,i} + n_{c,i}}\right)}&lt;br /&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;{\sum_i&amp;nbsp;X_{c,i} \left(\frac{n_{t,i}}{n_{t,i}&amp;nbsp;+ n_{c,i}}\right)}&lt;br /&gt;$$ taking care to omit terms where $n_{t,i}&amp;nbsp;+ n_{c,i} = 0$. Let&#39;s start with a csv of the following schema:&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;&lt;experiment_id&gt;&lt;product_id&gt;&lt;selling_price&gt;&amp;nbsp;exp_id, product_id, price, sale_count &lt;count&gt;&lt;/count&gt;&lt;/selling_price&gt;&lt;/product_id&gt;&lt;/experiment_id&gt;&lt;/span&gt;&lt;br /&gt;If I wrote a MapReduce pipeline to calculate the MH ratio of prices, it would require three MapReduce programs:&lt;br /&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;Map&lt;/b&gt; (if header skip else &lt;product_id exp_id=&quot;&quot;&gt; value &lt;price clicks=&quot;&quot;&gt;) → Reduce (sum reducer)&lt;/price&gt;&lt;/product_id&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;Map&lt;/b&gt; (key &lt;product_id&gt; value &lt;exp_id clicks=&quot;&quot; price=&quot;&quot;&gt;) → Reduce (no-op -- value is iter&lt;exp_id clicks=&quot;&quot; price=&quot;&quot;&gt;)&lt;/exp_id&gt;&lt;/exp_id&gt;&lt;/product_id&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;Map&lt;/b&gt; (key &amp;lt;&amp;gt; value &lt;numerator denominator=&quot;&quot;&gt;) → Reduce (sum reducer)&lt;/numerator&gt;&lt;/li&gt;&lt;/ol&gt;The net result would be a significant amount of code, considering a trivial word-count MapReduce requires roughly &lt;a href=&quot;https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html&quot;&gt;60 lines of mostly boilerplate Java code&lt;/a&gt;. That’s a lot of code that I’m not going to write. For the curious reader, much has been written about Hadoop/Mapreduce versus Spark, such as&amp;nbsp;&lt;a href=&quot;http://www.dattamsha.com/2014/09/hadoop-mr-vs-spark-rdd-wordcount-program/&quot;&gt;this post at Dattamsha&lt;/a&gt;.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;img alt=&quot;Compare Spark vs. Google Dataflow/Beam&quot; height=&quot;329&quot; src=&quot;https://lh4.googleusercontent.com/OlvW6SskdqiFWmJeucVj-ESXH7c_GR_0af4Rdo0vDi78eVKVbusCTlbs7UouymD-cfxszDarNsIUexlyuInZ15Wgm6hTGpPVzo7spMvv1mNsf95qlxP5kEKjGMBtPfeyUB-E1qYU&quot; style=&quot;border: none; margin-left: auto; margin-right: auto; transform: rotate(0rad);&quot; title=&quot;&quot; width=&quot;640&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot;&gt;A mashup of Apache Spark and Dataflow/Beam&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;div&gt;&lt;h2&gt;MH in python&lt;/h2&gt;Readers who prefer Python to prose can see the full Jupyter notebook solution in my &lt;a href=&quot;https://github.com/davidadamsphd/spark-vs-dataflow&quot;&gt;Spark vs Dataflow&lt;/a&gt; github project. To make the comparison easier, I separate the business logic from parallelized code. There are two business logic functions, &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;calc_numerator&lt;/span&gt; and &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;calc_denominator&lt;/span&gt;, that take all of count and price data for a particular product and calculate the numerator (and denominator) of the above formula.&lt;/div&gt;&lt;div&gt;&lt;h4&gt;MH in Apache Spark&lt;/h4&gt;The first thing to do for Spark is start the Spark shell, which involves adding the a few things to the PATH and running execfile on pyspark/shell.py (see the notebook for details).&amp;nbsp;Next comes the Spark code:&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.4571514285714287; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: white;&quot;&gt;&lt;span style=&quot;color: green; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/span&gt;&lt;span id=&quot;docs-internal-guid-70f625c1-e499-b463-fa8e-c4933f8d3367&quot; style=&quot;background-color: white;&quot;&gt;&lt;span style=&quot;color: green; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  from&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: blue; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;operator&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: green; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;import&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; add&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #408080; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: italic; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  # We want to calculate MH(v_{t,i},n_{t,i},v_{c,i},n_{c,i}), where t and c are treatment &lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #408080; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: italic; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  # &lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;background-color: white; color: #408080; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: italic; line-height: 20.4001px; white-space: pre-wrap;&quot;&gt;and control.&lt;/span&gt;&lt;span style=&quot;background-color: white; color: #408080; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: italic; line-height: 1.45715; white-space: pre-wrap;&quot;&gt; v and n in our cases are value of the sale prices and sale_count.&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.4571514285714287; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: white;&quot;&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  input_rdd &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; sc&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;textFile(&lt;/span&gt;&lt;span style=&quot;color: #ba2121; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&#39;sim_data_{0}_{1}.csv&#39;&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;format(NUM_LE, NUM_HE))&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  header &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; input_rdd&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;first() &lt;/span&gt;&lt;span style=&quot;color: #408080; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: italic; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;# Remove the first line.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  parsed_input_rdd &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; input_rdd&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;filter(&lt;/span&gt;&lt;span style=&quot;color: green; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;lambda&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; x: x &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;!=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;header)&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.4571514285714287; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: white;&quot;&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;                              .&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;map(&lt;/span&gt;&lt;span style=&quot;color: green; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;lambda&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; x: convert_line(x&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;split(&lt;/span&gt;&lt;span style=&quot;color: #ba2121; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&#39;,&#39;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;)))&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  transformed &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; parsed_input_rdd&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;map(&lt;/span&gt;&lt;span style=&quot;color: green; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;lambda&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; x: ((x[exp], x[prod]),&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.4571514285714287; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: white;&quot;&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;                                                (x[sale_count]&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;*&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;x[price], x[sale_count])))&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.4571514285714287; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: white;&quot;&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  (sp, clks) &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; (&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;, &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;1&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;) &lt;/span&gt;&lt;span style=&quot;color: #408080; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: italic; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;# sale price and sale_count&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  (ep, spc) &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; (&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;, &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;1&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;) &lt;/span&gt;&lt;span style=&quot;color: #408080; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: italic; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;# exp_id&amp;amp;product_id, sp&amp;amp;sale_count&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  (exp2, prod2) &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; (&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;, &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;1&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;) &lt;/span&gt;&lt;span style=&quot;color: #408080; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: italic; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;# exp_id, product_id&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;background-color: white;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;color: #408080;&quot;&gt;&lt;i&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/i&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;color: #408080; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: italic; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  # For each product cross exp_id, sum the sale prices and sale_count&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  grouped_result &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; transformed&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;reduceByKey(&lt;/span&gt;&lt;span style=&quot;color: green; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;lambda&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; x,y: (x[sp]&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;+&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;y[sp], x[clks]&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;+&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;y[clks]))&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  grouped_by_product &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; grouped_result&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;map(&lt;/span&gt;&lt;span style=&quot;color: green; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;lambda&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; x: ((x[ep][prod2]), (x[ep][exp2], x[spc][sp], x[spc][clks])))&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;groupByKey()&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;/span&gt;&lt;span id=&quot;docs-internal-guid-70f625c1-e644-8f9c-7969-e79933367c86&quot;&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;background-color: white;&quot;&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.4571514285714287; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: white;&quot;&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  numerator_sum &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; grouped_by_product&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;map(&lt;/span&gt;&lt;span style=&quot;color: green; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;lambda&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; x: calc_numerator(x))&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;reduce(add)&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  denominator_sum &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; grouped_by_product&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;map(&lt;/span&gt;&lt;span style=&quot;color: green; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;lambda&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; x: calc_denominator(x))&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;reduce(add)&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  effect &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; numerator_sum &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;/&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; denominator_sum&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: green; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  print&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;(numerator_sum, denominator_sum, effect)&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.4571514285714287; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: #f7f7f7; color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;Spark functions fall into two categories: &lt;b&gt;transformations&lt;/b&gt; that manipulate the data record by record (e.g., map and filter), and &lt;b&gt;actions&lt;/b&gt; that cause the data to be reorganized (e.g., &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;groupByKey&lt;/span&gt;, &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;reduceByKey&lt;/span&gt;). The significance of the distinction is that transformations have no immediate effect and are only acted upon when an action is reached, that is, Spark does lazy evaluation.&amp;nbsp;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;br /&gt;Excluding business-logic, the program is only a handful of lines of code.&lt;/div&gt;&lt;ul&gt;&lt;li&gt;&lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;sc.textFile(...)&lt;/span&gt; transforms a file on disk into an RDD (the distributed data structure in Spark)&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;input_rdd.first()&amp;nbsp;&lt;/span&gt;acts on the RDD returning first, header, element to the driver (my notebook).&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;input_rdd.filter(...).map(...)&lt;/span&gt; transforms &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;input_rdd&lt;/span&gt; removing the header then converts each csv line into floats and ints.&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;parsed_input_rdd.map(...)&lt;/span&gt; transforms records into key-value tuples &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;((exp_id, product_id), (cost, clicks))&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;transformed.reduceByKey(...)&lt;/span&gt; acts on transformed causing &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;input_rdd.filter(...).map(...)&lt;/span&gt; and &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;parsed_input_rdd.map(...)&lt;/span&gt; to be executed and produces the total clicks and cost by &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;(exp_id, product_id)&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;grouped_result.map(...).groupByKey()&amp;nbsp;&lt;/span&gt;acts to produce the same data, only grouped by &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;product_id&lt;/span&gt; instead of &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;product_id&lt;/span&gt; and &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;experiment_id&lt;/span&gt;.&amp;nbsp;&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;grouped_by_product.map(...).reduce(add)&lt;/span&gt; transforms the data per &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;product_id&lt;/span&gt; into the numerator and denominator of the MH calculation and then performs the action of summing the results using the &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;add&lt;/span&gt; function.&lt;/li&gt;&lt;/ul&gt;&lt;div dir=&quot;ltr&quot; style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;/div&gt;&lt;h4&gt;&lt;span style=&quot;vertical-align: baseline;&quot;&gt;MH in Apache Beam&lt;/span&gt;&lt;/h4&gt;&lt;span style=&quot;vertical-align: baseline;&quot;&gt;The organization of the Dataflow code is rather similar to Spark overall, with a handful of subtle, but important distinctions. One of the less interesting differences is that Dataflow doesn’t yet have a sum function for the reduce, so I have to write my own (&lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;t_sum&lt;/span&gt;). Note that this code is using the Dataflow SDK (not the new &lt;a href=&quot;https://github.com/apache/incubator-beam/tree/python-sdk/sdks/python&quot;&gt;Beam SDK&lt;/a&gt;).&lt;/span&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.4571514285714287; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span id=&quot;docs-internal-guid-70f625c1-e651-fd73-b71c-3e6ecef5983d&quot; style=&quot;background-color: white;&quot;&gt;&lt;/span&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.4571514285714287; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: white;&quot;&gt;&lt;span style=&quot;color: green; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  import&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: blue; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;apache_beam&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: green; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;as&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: blue; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;beam&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: green; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  def&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: blue; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;t_sum&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;(values):&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;result &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; [&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;,&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;]&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;color: green; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;for&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; v &lt;/span&gt;&lt;span style=&quot;color: #aa22ff; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;in&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; values:&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;result[&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;] &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;+=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; v[&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;]&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;result[&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;1&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;] &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;+=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; v[&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;1&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;]&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;color: green; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;return&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; (result[&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;], result[&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;1&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;])&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #408080; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: italic; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  # Create a pipeline executing on a direct runner (local, non-cloud).&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #408080; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: italic; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  # DirectPipelineRunner is the default runner, I&#39;m setting it here to show how one&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #408080; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: italic; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  # would change it to run on the Dataflow Service.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  pipeline_options &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; beam&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;utils&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;options&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;PipelineOptions([&lt;/span&gt;&lt;span style=&quot;color: #ba2121; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&#39;--runner=DirectPipelineRunner&#39;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;])&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  p &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; beam&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;Pipeline(options&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;pipeline_options)&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  parsed_input_rdd &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; (p&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;   &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;|&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: #ba2121; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&#39;load records&#39;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.4571514285714287; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: white;&quot;&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;     beam&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;io&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;Read(beam&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;io&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;TextFileSource(&lt;/span&gt;&lt;span style=&quot;color: #ba2121; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&#39;sim_data_{0}_{1}.csv&#39;&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;format(NUM_LE, NUM_HE)))&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;   &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;|&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: #ba2121; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&#39;filter header&#39;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; beam&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;Filter(&lt;/span&gt;&lt;span style=&quot;color: green; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;lambda&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; x: x[&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;] &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;!=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: #ba2121; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&#39;#&#39;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;)&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;   &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;|&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: #ba2121; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&#39;split line&#39;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; beam&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;Map(&lt;/span&gt;&lt;span style=&quot;color: green; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;lambda&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; x: convert_line(x&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;split(&lt;/span&gt;&lt;span style=&quot;color: #ba2121; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&#39;,&#39;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;))))&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  transformed &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; (parsed_input_rdd&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;   &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;|&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: #ba2121; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&#39;reshape&#39;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; beam&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;Map((&lt;/span&gt;&lt;span style=&quot;color: green; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;lambda&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; x: ((x[exp], x[prod]),&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.4571514285714287; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: white;&quot;&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;                                       (x[price]&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;*&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;x[sale_count], x[sale_count])))))&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  (sp, clks) &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; (&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;, &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;1&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;) &lt;/span&gt;&lt;span style=&quot;color: #408080; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: italic; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;# sale price and sale_count&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  (ep, spc) &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; (&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;, &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;1&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;) &lt;/span&gt;&lt;span style=&quot;color: #408080; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: italic; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;# exp_id&amp;amp;product_id, sp&amp;amp;sale_count&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  (exp2, prod2) &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; (&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;, &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;1&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;) &lt;/span&gt;&lt;span style=&quot;color: #408080; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: italic; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;# exp_id, product_id&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #408080; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: italic; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  # For each product cross exp_id, sum the sale prices and sale_count&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  grouped_result &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; (transformed&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;   &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;|&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: #ba2121; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&#39;combine per product/id&#39;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; beam&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;CombinePerKey(t_sum))&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  grouped_by_product &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; (grouped_result&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;   &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;|&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: #ba2121; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&#39;keyByExpProduct&#39;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; beam&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;Map(&lt;/span&gt;&lt;span style=&quot;color: green; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;lambda&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; x: ((x[ep][prod2]),&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.4571514285714287; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: white;&quot;&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;                                              (x[ep][exp2], x[spc][sp], x[spc][clks])))&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;   &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;|&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: #ba2121; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&#39;group&#39;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; beam&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;GroupByKey())&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  numerator_sum &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; (grouped_by_product&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;   &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;|&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: #ba2121; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&#39;MapForNum&#39;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; beam&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;Map(&lt;/span&gt;&lt;span style=&quot;color: green; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;lambda&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; x: calc_numerator(x))&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;   &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;|&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: #ba2121; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&#39;CombineNum&#39;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; beam&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;CombineGlobally(&lt;/span&gt;&lt;span style=&quot;color: green; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;sum&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;))&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  numerator_sum &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;|&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: #ba2121; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&#39;save numerator&#39;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; beam&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;io&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;Write(beam&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;io&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;TextFileSink(&lt;/span&gt;&lt;span style=&quot;color: #ba2121; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&#39;./numerator_sum&#39;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;))&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  denominator_sum &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; (grouped_by_product&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;   &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;|&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: #ba2121; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&#39;MapForDenom&#39;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; beam&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;Map(&lt;/span&gt;&lt;span style=&quot;color: green; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;lambda&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; x: calc_denominator(x))&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;   &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;|&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: #ba2121; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&#39;CombineDenom&#39;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; beam&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;CombineGlobally(&lt;/span&gt;&lt;span style=&quot;color: green; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;sum&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;))&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  denominator_sum &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;|&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: #ba2121; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&#39;save denominator&#39;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.4571514285714287; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: white;&quot;&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;     beam&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;io&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;Write(beam&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;io&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;TextFileSink(&lt;/span&gt;&lt;span style=&quot;color: #ba2121; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&#39;./denominator_sum&#39;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;))&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  p&lt;/span&gt;&lt;span style=&quot;color: #666666; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;courier new&amp;quot;; font-size: 14px; font-style: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;run()&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;br /&gt;Beam is conceptually two pieces: pipeline construction and pipeline execution. &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;beam.Pipeline()&lt;/span&gt; returns a pipeline, say &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;p&lt;/span&gt;, on which to build (using &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;beam.Map&lt;/span&gt;, &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;beam.GroupByKey,&lt;/span&gt; etc.) and &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;p.run()&lt;/span&gt; executes the pipeline on a cluster, by the Dataflow Service, or in our case, locally.&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;beam.Pipeline(options=pipeline_options)&lt;/span&gt; begins constructing a pipeline to run locally.&amp;nbsp;&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;p | beam.io.Read(...) | beam.Filter(...) | beam.Map(...) &lt;/span&gt;add reading the file, filtering lines that look like the header (starting with ‘#’), converting each line into floats and ints to the graph.&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;parsed_input_rdd | beam.Map(...)&lt;/span&gt; adds mapping each record to be keyed by &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;exp_id, product_id&lt;/span&gt; to the graph&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;transformed | beam.CombinePerKey(...) | beam.Map(...) | beam.GroupByKey()&amp;nbsp;&lt;/span&gt;adds summing clicks and cost by &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;exp_id, product_id&lt;/span&gt; and regrouping by &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;product_id&lt;/span&gt; to the graph&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;grouped_by_product | beam.Map(...) | beam.CombineGlobally(...)&lt;/span&gt; adds calculating the numerator/denominator values and the global sum to the graph&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;numerator_sum | beam.Write(...)&lt;/span&gt; adds a sync for the numerator (there is a matching output for the denominator).&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;p.run()&lt;/span&gt; optimizes constructed graph and ships the result to be executed (in our case the local machine)&lt;/li&gt;&lt;/ul&gt;&lt;h4&gt;Comparison of implementations&lt;/h4&gt;At a high-level, both implementations look similar and use the same basic parallel operations. The first difference to point out is that Spark &lt;b&gt;executes&lt;/b&gt; the graph only on &lt;b&gt;actions&lt;/b&gt;, e.g., &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;reduceByKey&lt;/span&gt;, whereas the runner executing Beam, e.g., Cloud Dataflow, executes the complete graph (when run is invoked). Another difference is that Dataflow graphs require &lt;b&gt;sources&lt;/b&gt; and &lt;b&gt;sinks&lt;/b&gt;, which means results must be &lt;b&gt;piped&lt;/b&gt; to files and cannot be returned to the calling program (with the exception of using the local runner).&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;h2&gt;Spark’s sweet-spot: iterative algorithms&lt;/h2&gt;Spark started as a &lt;a href=&quot;http://blog.madhukaraphatak.com/history-of-spark/&quot;&gt;solution for doing exploratory analysis and Machine Learning&lt;/a&gt;. One of the simplest ways to show this off is the clustering technique &lt;a href=&quot;https://en.wikipedia.org/wiki/K-means_clustering&quot;&gt;k-means clustering&lt;/a&gt;. K-means works by repeatedly applying two steps, assigning points to clusters and updating cluster centers, until the location of the cluster centers have stabilized. Below is the core of the algorithm written in Spark.&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.4571514285714287; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: #f7f7f7; color: #408080; font-family: &amp;quot;arial&amp;quot;; font-size: 14px; font-style: italic; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.4571514285714287; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: white; font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;&lt;span style=&quot;color: #408080; font-size: 14px; font-style: italic; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  # Load the data, remove the first line, and pick initial locations for the clusters.&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.4571514285714287; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: white; font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;&lt;span style=&quot;color: #408080; font-size: 14px; font-style: italic; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  # We put the x,y points in pt_rdd, and RDD of PtAgg (a class containing x, y,&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.4571514285714287; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: white; font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;&lt;span style=&quot;color: #408080; font-size: 14px; font-style: italic; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  # and count).&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  pt_rdd &lt;/span&gt;&lt;span style=&quot;color: #666666; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; parsed_input_rdd&lt;/span&gt;&lt;span style=&quot;color: #666666; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;map(&lt;/span&gt;&lt;span style=&quot;color: green; font-size: 14px; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;lambda&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; x: PtAgg(x[&lt;/span&gt;&lt;span style=&quot;color: #666666; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;], x[&lt;/span&gt;&lt;span style=&quot;color: #666666; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;1&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;], &lt;/span&gt;&lt;span style=&quot;color: #666666; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;1&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;))&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  MAX_STEPS &lt;/span&gt;&lt;span style=&quot;color: #666666; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: #666666; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;100; &lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&amp;nbsp;MIN_DELTA &lt;/span&gt;&lt;span style=&quot;color: #666666; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: #666666; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0.001&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;; delta &lt;/span&gt;&lt;span style=&quot;color: #666666; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: #666666; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;1.0&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;; step &lt;/span&gt;&lt;span style=&quot;color: #666666; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: #666666; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: green; font-size: 14px; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  while&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; delta &lt;/span&gt;&lt;span style=&quot;color: #666666; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; MIN_DELTA &lt;/span&gt;&lt;span style=&quot;color: #aa22ff; font-size: 14px; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;and&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; step &lt;/span&gt;&lt;span style=&quot;color: #666666; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; MAX_STEPS:&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;step &lt;/span&gt;&lt;span style=&quot;color: #666666; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;+=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: #666666; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;1&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;c_centers_old &lt;/span&gt;&lt;span style=&quot;color: #666666; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; copy&lt;/span&gt;&lt;span style=&quot;color: #666666; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;deepcopy(c_centers)&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;b_c_centers &lt;/span&gt;&lt;span style=&quot;color: #666666; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; sc&lt;/span&gt;&lt;span style=&quot;color: #666666; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;broadcast(c_centers_old)&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;color: #408080; font-size: 14px; font-style: italic; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;# For every point, find the cluster its closer to and add to its total x, y, and count&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;totals &lt;/span&gt;&lt;span style=&quot;color: #666666; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; pt_rdd&lt;/span&gt;&lt;span style=&quot;color: #666666; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;map(&lt;/span&gt;&lt;span style=&quot;color: green; font-size: 14px; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;lambda&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; x: pick_closest_center(x, b_c_centers&lt;/span&gt;&lt;span style=&quot;color: #666666; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;value))&lt;/span&gt;&lt;span style=&quot;color: #666666; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;reduce(&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;color: green; font-size: 14px; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;lambda&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; a,b: center_reduce(a,b))&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;color: #408080; font-size: 14px; font-style: italic; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;# Now update the location of the centers as the mean of all of the points closest to it&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;color: #408080; font-size: 14px; font-style: italic; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;# (unless there are none, in which case pick a new random spot).&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;c_centers &lt;/span&gt;&lt;span style=&quot;color: #666666; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; [t&lt;/span&gt;&lt;span style=&quot;color: #666666; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;Normalize() &lt;/span&gt;&lt;span style=&quot;color: green; font-size: 14px; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;if&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; t&lt;/span&gt;&lt;span style=&quot;color: #666666; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;cnt &lt;/span&gt;&lt;span style=&quot;color: #666666; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;!=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: #666666; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: green; font-size: 14px; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;else&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; random_point_location() &lt;/span&gt;&lt;span style=&quot;color: green; font-size: 14px; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;for&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; t &lt;/span&gt;&lt;span style=&quot;color: #aa22ff; font-size: 14px; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;in&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; totals]&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;color: #408080; font-size: 14px; font-style: italic; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;# compute the distance that each cluster center moves, the set the max of those as&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;color: #408080; font-size: 14px; font-style: italic; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;# the delta used to the stop condition.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;deltas &lt;/span&gt;&lt;span style=&quot;color: #666666; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; [math&lt;/span&gt;&lt;span style=&quot;color: #666666; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;sqrt(c&lt;/span&gt;&lt;span style=&quot;color: #666666; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;DistSqr(c_old)) &lt;/span&gt;&lt;span style=&quot;color: green; font-size: 14px; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;for&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; c, c_old &lt;/span&gt;&lt;span style=&quot;color: #aa22ff; font-size: 14px; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;in&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: green; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;zip&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;(c_centers, c_centers_old)]&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;delta &lt;/span&gt;&lt;span style=&quot;color: #666666; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: green; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;max&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;(deltas)&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  s &lt;/span&gt;&lt;span style=&quot;color: #666666; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: #ba2121; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&#39; &#39;&lt;/span&gt;&lt;span style=&quot;color: #666666; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;join([&lt;/span&gt;&lt;span style=&quot;color: green; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;str&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;(x) &lt;/span&gt;&lt;span style=&quot;color: green; font-size: 14px; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;for&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; x &lt;/span&gt;&lt;span style=&quot;color: #aa22ff; font-size: 14px; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;in&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; c_centers])&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br class=&quot;kix-line-break&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;color: green; font-size: 14px; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;  print&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;(&lt;/span&gt;&lt;span style=&quot;color: #ba2121; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&#39;final centers: {0}&#39;&lt;/span&gt;&lt;span style=&quot;color: #666666; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color: #333333; font-size: 14px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;format(s))&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;ul&gt;&lt;li&gt;&lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;parsed_input_rdd.map(...)&lt;/span&gt; maps the raw into into the &lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;PtAgg&lt;/span&gt; class we use to represent points&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;sc.broadcast(c_centers_old)&lt;/span&gt; sends the locations of the cluster centers to all machines holding RDDs&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;pt_rdd.map(...).reduce(...)&lt;/span&gt; maps each point to the center it’s closest to and then reduces to produce the average location of the points that are closest to each center (the result is in the driver script)&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;&quot;&gt;[t.Normalize() if t.cnt != 0 else random_point_location() for t in totals]&lt;/span&gt; updates the new cluster locations.&lt;/li&gt;&lt;/ul&gt;&lt;div dir=&quot;ltr&quot; style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;As it happens, I didn’t actually need to write that code because Spark has an ML library (mllib) containing many algorithms including k-means.  In practice, k-means is a just a few functions calls, &lt;a href=&quot;http://spark.apache.org/docs/latest/mllib-clustering.html&quot;&gt;details are here&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;Here is where the difference between Spark and Beam is most apparent&amp;nbsp;—&amp;nbsp;&lt;b&gt;no iterative algorithm that can stop early, including k-means, can be expressed in Beam&lt;/b&gt;. This is because Beam builds the computation graph, then optimizes and ships it to be executed. With iterative algorithms, the complete graph structure cannot be known before hand (don’t know how many loops will be done), so it can’t be expressed in Beam. It is possible to express “loops” in Dataflow, but only a fixed number of loops can be added.&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Beam/Dataflow’s sweet spot: streaming processing&lt;/h2&gt;Streaming processing is an ever-increasingly important topic for data science. After all, who among us really wants to wait for a daily batch pipeline to tell us how our live traffic experiments are doing? For a great introduction to streaming processing, I highly encourage reading Tyler Akidau’s two blog posts on the topic, &lt;a href=&quot;https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101&quot;&gt;The world beyond batch: Streaming 101&lt;/a&gt; and &lt;a href=&quot;https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-102&quot;&gt;102&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;When Dataflow was released, its strongest selling point was streaming (just read the &lt;a href=&quot;http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43864.pdf&quot;&gt;paper&lt;/a&gt;). That’s not to say that Spark doesn’t support streaming process or that it lacks a unified engine for batch and streaming. The significant step Dataflow initially made was (1) a truly unified batch and streaming API and (2) support for event time processing of data, that is, analysis of the results windowed by &lt;b&gt;when they happened not when they reached the analysis machine&lt;/b&gt;, (3) and a focus on watermarks (a method for tracking collection progress on unbounded data) and window completeness (has all of the data for this time period been collected). More practically, Dataflow, now Beam, has a streaming API that cleanly separates the important questions of streaming processing: what, where, when, how). It’s worth noting that different runners currently have different levels of support, e.g., Dataflow supports streaming for Java, but not yet for Python.&lt;br /&gt;&lt;br /&gt;All this is easiest to show using the examples the Dataflow team used in their&lt;a href=&quot;https://cloud.google.com/dataflow/blog/dataflow-beam-and-spark-comparison&quot;&gt; recent blog post&lt;/a&gt;, which is also a good read. The starkest contrast between the two is shown by their example of calculating hourly team scores for an online game.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h2&gt;Other differences in framework “philosophy”&lt;/h2&gt;There are several design decisions to show that Spark favors fast development by default and requires users to opt-in to performance whereas Dataflow favors high performance by default with the cost of slower development time.&lt;/div&gt;&lt;div&gt;&lt;h4&gt;Caching&lt;/h4&gt;Dataflow avoids the need for caching results by “fusing” sibling stages that would need the same input. Fusion comes with the limitation that mutable types should not be used. This is because the values are shared among fused sibling and modification would cause invalid results (this is verified against at run-time). This means writing correct Beam code with mutable types requires significantly more care. Spark re-computes by default, which can be much slower but doesn’t come with correctness issues. Spark does allow for many different caching methods, &lt;a href=&quot;http://spark.apache.org/docs/latest/programming-guide.html#which-storage-level-to-choose&quot;&gt;should caching be desired&lt;/a&gt;.&lt;br /&gt;&lt;h4&gt;Equality&lt;/h4&gt;Dataflow uses a fast, language agnostic, serialized byte-level equality for comparing classes in GroupByKey and other steps. This becomes a huge issue if the program uses grouping operations on classes that contain hashmaps, sets, or anything that causes semantic equality to differ from byte equality. Spark deserializes classes and uses the class’s comparison operator for grouping by default and allows user to opt-into byte-level equality making the opposite tradeoff.&lt;br /&gt;&lt;h4&gt;Overall performance&lt;/h4&gt;Unfortunately, we currently can’t make any definitive statements about which framework has better overall performance. The &lt;a href=&quot;http://mammothdata.com/wp-content/uploads/2016/04/GoogleCloudDataflow.Benchmark.pdf&quot;&gt;only study&lt;/a&gt; that compared Beam on the Dataflow Service to Spark used an older version of Spark (1.3). Given the significant performance increases with Spark 2.0, it’s unclear which framework has better performance at present.&lt;/div&gt;&lt;br /&gt;&lt;h2&gt;Conclusion&lt;/h2&gt;&lt;div&gt;Spark and Dataflow both solve the limitations of MapReduce with different viewpoints. Which tool is most appropriate will depend on the project. If the focus is interactivity, data exploration or algorithm development, Spark is probably the better option. If the focus is complex streaming processing (especially event time) then Beam would seem a better fit. For running scalable production pipelines, Beam on Cloud Dataflow is likely the better choice.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;</content><link rel='edit' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/5882522097717497514'/><link rel='self' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/5882522097717497514'/><link rel='alternate' type='text/html' href='https://www.unofficialgoogledatascience.com/2016/08/next-generation-tools-for-data-science.html' title='Next generation tools for data science'/><author><name>Amir Najmi</name><uri>http://www.blogger.com/profile/18174523203317227640</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://lh4.googleusercontent.com/OlvW6SskdqiFWmJeucVj-ESXH7c_GR_0af4Rdo0vDi78eVKVbusCTlbs7UouymD-cfxszDarNsIUexlyuInZ15Wgm6hTGpPVzo7spMvv1mNsf95qlxP5kEKjGMBtPfeyUB-E1qYU=s72-c" height="72" width="72"/></entry><entry><id>tag:blogger.com,1999:blog-5825758052688213474.post-6869788315354330264</id><published>2016-07-31T10:05:00.000-07:00</published><updated>2016-08-02T10:19:03.357-07:00</updated><title type='text'>Mind Your Units</title><content type='html'>By JEAN STEINER&lt;br /&gt;&lt;br /&gt;Randomized A/B experiments are the gold standard for estimating causal effects. The analysis can be straightforward, especially when it&#39;s safe to assume that individual observations of an outcome measure are independent. However, this is not always the case. When observations are not independent, an analysis that assumes independence can lead us to believe that effects are significant when they actually aren&#39;t. This blog post explores how this problem arises in applications at Google, and how we can &#39;mind our units&#39; through analysis and logging.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;A tale of two units&lt;/h2&gt;Imagine you are interested in the effectiveness of a new mathematics curriculum. You might run an experiment in which some students are exposed to the new curriculum while others are taught based on the existing curriculum. At the end of the semester, you administer a proficiency test to the students and compare test performance across the groups. For example, you might compare the average test score among students exposed to the new curriculum to the average score among the controls.&lt;br /&gt;&lt;div&gt;&lt;br /&gt;Sounds straightforward, right? So, why is this a tale about &#39;two units&#39;? The first unit is the unit of observation. The outcome measure we care about is an average of the students&#39; test scores, and so the unit of observation is a student. There is one test score per student, and each student was exposed to either the new or the old curriculum. But, can we treat all of the test scores from all of the students as&amp;nbsp;independent observations?&lt;br /&gt;&lt;br /&gt;To figure this out, let&#39;s consider an appropriate experimental design. To evaluate a curriculum, we will need many teachers to use it, and we will also want many students to be exposed to each curriculum.  Since teachers can only teach from a single curriculum for each semester, we will need to ensure that each teacher is assigned to either the new curriculum or the old curriculum. This means that we&#39;ll need to assign treatment status, or &lt;i&gt;randomize&lt;/i&gt;, at the level of the teacher. In other words, the teacher is our second kind of unit, the unit of experimentation.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;This type of experimental design is known as a group-randomized or &lt;a href=&quot;https://en.wikipedia.org/wiki/Cluster_randomised_controlled_trial&quot; target=&quot;_blank&quot;&gt;cluster-randomized&lt;/a&gt; trial. As the name suggests, in a group-randomized trial randomization occurs at the group level. In the example above, randomization occurs at the level of a teacher rather than at the student level. Since each teacher works with a group of students, the entire group of a teacher&#39;s students is exposed to either the new or the old curriculum.&lt;br /&gt;&lt;br /&gt;With this experimental design in mind, let&#39;s revisit this question: can we treat all of the students&#39; test scores as independent observations? The answer is no -- there are at least two features of classroom dynamics that prevent us from treating test scores from students in the same classroom as independent observations.&lt;br /&gt;&lt;br /&gt;First, there will generally be a teacher effect: when a teacher is really good, their students will likely do a bit better than when a teacher performs less well, and this should hold regardless of the curriculum. The second source of dependence comes from student interactions. Students within the same classroom will likely talk to each other and work with each other: some students might understand the material better and then teach their classmates, or, in the negative scenario, a classroom with a real trouble-maker might impede the progress of several classmates by distracting them.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;In summary, there is likely dependence among observations that come from students with the same teacher. When analyzing the outcome measure (e.g., student test scores) in a group-randomized trial, it is critical to account for the group structure (or the experimental unit). Ignoring the group structure would underestimate the variance and could lead to false positive findings.&lt;/div&gt;&lt;br /&gt;One typical modelling approach to account for the group structure is a mixed-level model with random effects. This is a comprehensive approach, and we often employ it at Google. However, we also employ alternative approaches whose conceptual simplicity can be a practical advantage.&lt;br /&gt;&lt;br /&gt;Let&#39;s begin by considering a typical online experiment with a group-randomized structure. Suppose we want to evaluate a proposed change to the user interface for web search. We want an experimental set-up that is consistent with what would happen if we actually launched the new interface so that our experiment will lead us to make relevant conclusions. Additionally, we want to ensure users have a great search experience even as we&#39;re trying out new features, so we don&#39;t want users to keep seeing new interfaces every time they do a new search. To satisfy both of these experimental design requirements, the best experimental unit is the user. Randomizing at the level of user (or cookie, which is often used as a proxy for a user) will ensure that each user sees the same interface every time she comes to the search page. For those familiar with causal inference, our design is said to satisfy the stable unit treatment value assumption, or &lt;i&gt;SUTVA&lt;/i&gt;. This assumption says that the potential outcomes in one user are independent of the treatment status of all other users and that there are no hidden variations in treatment.&lt;br /&gt;&lt;br /&gt;Examples of outcome measures might include the click-through-rate or the time-to-first-action, an indication of how quickly users found the information they wanted. This means the unit of observation is an individual search result page. Some users will do many searches, some will do only a handful. The observations that come from the same user are clearly not independent. For example, some users will be faster readers or &#39;more clicky&#39; than others.&lt;br /&gt;&lt;br /&gt;At this point, the group-randomized framework should be evident in the example of evaluating a new search interface: the &#39;group&#39; is the user, and the &#39;unit of observation&#39; is at the level of each search that a user conducts.&lt;br /&gt;&lt;br /&gt;This example also hints at a key difference between policy evaluations and online experiments; namely, the size of the study. Typically, in a policy evaluation setting there are relatively few groups, often on the order of hundreds or fewer per trial. By contrast, in an online setting, the number of groups can range from thousands to billions. As we will see, this informs the way we account for the group structure when performing causal inference at Google.&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;div&gt;&lt;h2&gt;The perils of incorrect units&lt;/h2&gt;Is the idea of &#39;minding our units&#39; just some esoteric issue, or can this actually hurt us in practice? Let&#39;s use a simple synthetic dataset to look at the effects of accounting for (or ignoring!) an existing group structure. We generate observations $Y_{ij}$ that correspond to the $i^\textrm{th}$ observation for the $j^\textrm{th}$ group.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;The simulation set-up is as follows: We fix the number of groups to be $N = 50,\!000$, then we generate observations for each of the groups, with group sizes governed by the Poisson parameter $\lambda$:&lt;br /&gt;&lt;div&gt;&lt;ul&gt;&lt;li&gt;Draw the group mean, $m_j$, for the $j^\textrm{th}$ group with $m_j \sim N(0, 1)$.&lt;/li&gt;&lt;li&gt;For $\lambda$ in (0.0, 0.2, ..., 1.2):&lt;/li&gt;&lt;ul&gt;&lt;li&gt;Determine the size of the $j^\textrm{th}$ group, $n_j$, by drawing from $\textrm{Poiss}(\lambda) + 1$.&lt;/li&gt;&lt;li&gt;Generate observations for the $j^\textrm{th}$ group: for each group $j$ in 1, ..., $N$, generate $n_j$ individual observations $Y_{ij} \sim N(m_j, 0.25)$, for $i = 1,..., n_j$ (where $n_j$ was the group size, and $m_j$ was the group mean).&lt;/li&gt;&lt;li&gt;Calculate 95% bootstrap confidence intervals with two alternative resampling schemes:&lt;/li&gt;&lt;ul&gt;&lt;li&gt;Ignoring the group structure by resampling each observation $Y_{ij}$.&lt;/li&gt;&lt;li&gt;Accounting for the group structure by resampling each $j^\textrm{th}$ group.&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;/ul&gt;It is worth noting that $\lambda$ is the only parameter that governs the overall sample size (which is $\Sigma n_j$ and will grow as $\lambda$ grows). However, the total number of independent groups is fixed at $N = 50,\!000$ throughout. Note also that in this simulation the within-group standard deviation (0.25) is much smaller than the between-group standard deviation (1.0), so we should not expect the overall sample standard deviation to respond directly to changes in the overall sample size.&lt;br /&gt;&lt;br /&gt;We&#39;ve implemented the above simulation in a simple R script&amp;nbsp;included at the bottom of this post. We ran the simulation once to illustrate what&#39;s going on (Table 1) and then repeated the process 1,000 times to demonstrate how coverage is affected (Figure 1).&lt;br /&gt;&lt;br /&gt;In Table 1, the first column gives the Poisson parameter $\lambda$, which governs the group sizes. The second column gives the percent of groups that have more than one observation per group, and the 3rd column gives the total number of observations across the 50,000 groups. The 4th and 5th columns give the half-width of the confidence interval (CI) by ignoring and accounting for the group structure.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;margin-left: 0pt;&quot;&gt;&lt;table style=&quot;border-collapse: collapse; border: none;&quot;&gt;&lt;colgroup&gt;&lt;col width=&quot;100&quot;&gt;&lt;/col&gt;&lt;col width=&quot;100&quot;&gt;&lt;/col&gt;&lt;col width=&quot;100&quot;&gt;&lt;/col&gt;&lt;col width=&quot;100&quot;&gt;&lt;/col&gt;&lt;col width=&quot;100&quot;&gt;&lt;/col&gt;&lt;/colgroup&gt;&lt;tbody&gt;&lt;tr style=&quot;height: 0px;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #cccccc 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 14.6667px; font-style: italic; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;𝜆&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 14.6667px; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;% groups with more than one observation&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;# observations&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;background-color: white; border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;CI half-width (a)&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;background-color: white; border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;CI half-width (b)&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0px;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #cccccc 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0.0&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0%&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;50,000&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0.009 &lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0.009&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0px;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #cccccc 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0.2&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;18%&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;59,887&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0.008 &amp;nbsp;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0.010&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0px;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #cccccc 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0.4&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;33%&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;70,154&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0.007 &lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0.010 &lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0px;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #cccccc 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0.6&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;45%&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;79,920&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0.007&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0.010&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0px;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #cccccc 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0.8&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;55%&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;89,699&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0.006&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0.010&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0px;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #cccccc 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;1.0&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;63%&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;100,070&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0.006&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0.010&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0px;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #cccccc 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;1.2&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;70%&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;109,838&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0.006&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0.010&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;b&gt;Table 1:&lt;/b&gt; Output of the simulation showing CIs that either (a) ignore or (b) account for within-group dependence.&lt;br /&gt;&lt;br /&gt;You can see that as you read down the table, $\lambda$ increases and the groups become increasingly heterogeneous (more groups contribute multiple observations per group). Of course, when $\lambda = 0$, each group contains only one observation, so there is no dependence to account for, but it is included for reference. By comparing the 4th and 5th column it is clear that as the groups become more heterogeneous, the discrepancy between the two CIs increases.&lt;br /&gt;&lt;br /&gt;For example, in the third to last row, when $\lambda = 0.8$ and 55% of the groups have more than one observation, the CI when we (mistakenly) treat each observation as independent is 40% narrower than the CI when we (correctly) account for the true group structure (compare 0.006 to 0.010). Even when we have a less extreme example such as the second row, with $\lambda = 0.2$ and a mere 18% of groups having multiple observations per group, there is already a noticeable difference in the CI widths.&lt;br /&gt;&lt;br /&gt;While Table 1 indicates why ignoring the group structure can give incorrect and excessively narrow CIs, it is even more instructive to look at the actual coverage of the two estimators. Coverage is the probability the CI computed by a particular method contains (or &#39;covers&#39;) the true value of the estimand, which is an effect of 0 in this simulation. The figure below shows empirical coverage rates based on 1,000 simulations. The red bars show how often the CI contained the true value of the estimand when we ignore the group structure. Contrast this with the cyan-colored bars, which show coverage of the CIs in which we properly account for the group structure. Even at values as low as $\lambda = 0.2$, the coverage of the CI that ignores the group structure drops to 90%, while the CI that accounts for the group structure keeps its nominal coverage of 95% (blue dotted line), in line with the desired false-positive rate of 5%. By the time $\lambda$ is 1.2, ignoring the group structure reduces coverage to 77%, showing how ignoring the group structure leads to optimistic inferences (i.e., CIs which are too narrow).&lt;/div&gt;&lt;div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://4.bp.blogspot.com/-F_KYHtECeMY/V6Cmc1Wju8I/AAAAAAAAD7g/XJfvDajK9NQFkUQqxsfdAiuVFnR7DjdCgCLcB/s1600/mind_your_units_coverage.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;https://4.bp.blogspot.com/-F_KYHtECeMY/V6Cmc1Wju8I/AAAAAAAAD7g/XJfvDajK9NQFkUQqxsfdAiuVFnR7DjdCgCLcB/s1600/mind_your_units_coverage.png&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;b&gt;Figure 1:&lt;/b&gt; Coverage based on 1,000 simulations in which we either ignore (red) or account for (cyan) within-group dependence. The x-axis shows the average group size minus 1. The plot shows how ignoring the group structure (red) leads to increasingly optimistic inferences (i.e., too narrow CIs) while accounting for the group structure leads to nominal coverage regardless of group size (cyan).&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h2&gt; How do we mind our units in analyses at Google? &lt;/h2&gt;The above simulation already hints at one of our approaches to incorporating the group structure in some analyses at Google. Since we often want to compute effects on hundreds of outcome measures and since their underlying distributions can be funky, we frequently resort to non-parametric methods such as the bootstrap. A common approach, then, to minding our units is to use the bootstrap as in the above simulation: we re-sample at the group level in order to account for the group structure.&lt;br /&gt;&lt;br /&gt;Another non-parametric method frequently used at Google is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Jackknife_resampling&quot; style=&quot;vertical-align: baseline;&quot; target=&quot;_blank&quot;&gt;jackknife&lt;/a&gt;. In order to account for the group structure, we use a blocked version of the jackknife in order to ensure that all observations from the same group are either left-out or included together.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;When we&#39;re feeling parametric, we may also use the mixed-level model approach as in the policy evaluation setting. An &lt;a href=&quot;http://www.unofficialgoogledatascience.com/2016/03/using-random-effects-models-in.html&quot;&gt;earlier blogpost&lt;/a&gt; described applications and implementation details of mixed-level modeling in the &#39;big data&#39; setting at Google.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;h2&gt;What if we don&#39;t (or can&#39;t) know our groups? &lt;/h2&gt;The blocked resampling strategies described above, as well as random-effects modelling, require that we have access to the independent unit or experimental unit, that is, that we know the group_id at analysis time. However, we might not be able to store all observations for each group_id. In cases like this, we employ an alternative solution: we use logging and intermediate data storage to capture the necessary information about the group &lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 14.6667px; white-space: pre-wrap;&quot;&gt;structure.&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;br /&gt;When possible, we begin by including the group_id in our stored data. We then store the full dataset in a table like the one shown below. &lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;margin-left: 0pt;&quot;&gt;&lt;table style=&quot;border-collapse: collapse; border: none;&quot;&gt;&lt;colgroup&gt;&lt;col width=&quot;100&quot;&gt;&lt;/col&gt;&lt;col width=&quot;100&quot;&gt;&lt;/col&gt;&lt;/colgroup&gt;&lt;tbody&gt;&lt;tr style=&quot;height: 0px;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 14.6667px; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;group_id&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;value&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0px;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 14.6667px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;1325&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0.1&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0px;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 14.6667px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;1325&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;1.4&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0px;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 14.6667px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;2347&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0.5&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0px;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 14.6667px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;2347&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0.4&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0px;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 14.6667px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;7825&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0.9&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0px;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;...&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;...&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;&lt;br /&gt;&lt;b&gt;Table 2:&lt;/b&gt; Individual observations for each group_id.&lt;br /&gt;&lt;br /&gt;If we have the full data of Table 2, we can go ahead and apply the analysis methods described in the preceding section, e.g., bootstrap re-sampling the group_ids. However, when we can&#39;t store the group_id, we store partly aggregated data, which we call &lt;i&gt;bucketed data&lt;/i&gt;. To generate bucketed data, all of the observations associated with the same group_id will go into the same bucket. To illustrate a simple bucketing scheme, compute the buckets by: &lt;br /&gt;&lt;br /&gt;&lt;div style=&quot;text-align: center;&quot;&gt;bucket = (group_id mod 100).&amp;nbsp;&lt;/div&gt;&lt;br /&gt;Now store data for each bucket:&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;margin-left: 0pt;&quot;&gt;&lt;table style=&quot;border-collapse: collapse; border: none;&quot;&gt;&lt;colgroup&gt;&lt;col width=&quot;100&quot;&gt;&lt;/col&gt;&lt;col width=&quot;100&quot;&gt;&lt;/col&gt;&lt;col width=&quot;160&quot;&gt;&lt;/col&gt;&lt;/colgroup&gt;&lt;tbody&gt;&lt;tr style=&quot;height: 0px;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;bucket&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;SUM(value)&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;# data points in bucket&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0px;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;25&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;2.4&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;3&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0px;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;47&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0.9&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;2&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0px;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;...&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;...&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;...&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;b&gt;Table 3:&lt;/b&gt; Bucketed observations (after associating each group_id with a bucket).&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;This bucketed data is no longer amenable to the group_id level bootstrap because we have aggregated observations and can no longer generate a full sampling distribution. However, we can still use methods that work on the bucketed data (including bootstrapping the bucket level data to get an estimate of the variability). One approach used at Google is the streaming bucket approach described in section 2.1 of &lt;a href=&quot;http://research.google.com/pubs/pub43157.html&quot; target=&quot;_blank&quot;&gt;this report&lt;/a&gt; by Nicholas Chamandy and colleagues, in which the variability of each bucket&#39;s estimate is used to approximate the overall variability of the estimator.&lt;br /&gt;&lt;br /&gt;Bucketed data is also amenable to the jackknife, which we can use in a leave-one-bucket-out fashion. The bucketed implementation of the jackknife is popular at Google because it provides a good trade-off between storage needs and statistical coverage: we can slice and deliver results on the fly across many dimensions. At the same time, compared to the streaming bucket approach and other alternatives considered, we have found that the bucketed jackknife has better coverage properties for ill-behaved metrics such as rare rate metrics.&lt;br /&gt;&lt;br /&gt;It is worth taking a small tangent to make a note of caution on how you bucket the data.  The simple bucketing scheme illustrated above, (group_id mod 100), is a bad idea in practice. If there is anything systematic about how the group_id&#39;s are generated, however subtle, then a simple modulus by itself fails to break this systematic behavior. This means we would retain systematic differences across buckets. For example, if the id&#39;s had been generated in a way to account for load balancing of some database, you can end up with simple buckets having strange periodicity to them. In other words, it&#39;s important to ensure that the group_id&#39;s are randomly distributed to buckets. We also want the bucket assignment to be consistent (say, across time), so that we can aggregate the data in various ways. One approach to having consistent and randomly distributed assignment is to use a salt (such as the experiment name) together with a hash of the group_id prior to taking the modulus. That is, if you wanted 100 buckets you could compute &lt;br /&gt;&lt;br /&gt;&lt;div style=&quot;text-align: center;&quot;&gt;bucket = (hash(group_id + salt) mod 100).&lt;/div&gt;&lt;br /&gt;It is also worth noting that for many of our applications at Google we only use 20 buckets, which we have found to give good coverage while allowing for storage and processing efficiency. It is important to use enough buckets to ensure that your CIs have the right coverage properties, and remember that applications with less data will likely need to use more buckets.&lt;br /&gt;&lt;h2&gt; Not all observations created equally &lt;/h2&gt;The basic bucketing method assumes that we care equally about the contribution from each observation. However, from a product perspective we sometimes care about some observations more than others. Suppose for example we have an onboarding flow in which users create new accounts.  Most of the time, each user will only create one account, but some power users might open up multiple accounts. From a product standpoint, we may want to focus only on analyzing the first account created by each user because the onboarding is really designed to onboard the new users, and when a user is returning to make a subsequent account, they likely don&#39;t need the onboarding. We also often see that power users exhibit different behaviors from the regular one-time users.&lt;br /&gt;&lt;br /&gt;In light of such considerations, instead of fully accounting for the group structure of these power users, we might want to separately analyze the first observation per group versus any subsequent observations. One approach might be to only log data for the first observation, but it&#39;s dangerous to lose visibility into a sizable chunk of data, and we generally would still want to be able to summarize all of the data and use the totals to sanity check our results and product health. In this type of situation, we would implement a counter that will log whether the group_id is seen for the first time or returning. In other words, we are adding another dimension to the data which we can filter by later.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Once the data is tagged with this additional dimension, we can store a combination of the bucket as well as whether the observations occur on the first instance or subsequent.  For example, assuming the data in Table 2 was sorted by time, we would store:&lt;/div&gt;&lt;div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;margin-left: 0pt;&quot;&gt;&lt;table style=&quot;border-collapse: collapse; border: none;&quot;&gt;&lt;colgroup&gt;&lt;col width=&quot;90&quot;&gt;&lt;/col&gt;&lt;col width=&quot;100&quot;&gt;&lt;/col&gt;&lt;col width=&quot;90&quot;&gt;&lt;/col&gt;&lt;col width=&quot;164&quot;&gt;&lt;/col&gt;&lt;/colgroup&gt;&lt;tbody&gt;&lt;tr style=&quot;height: 0px;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;bucket&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;counter&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;SUM(value)&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;# data points in bucket&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0px;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;25&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;first&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;1&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;2&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0px;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;25&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;subsequent&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;1.4&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;1&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0px;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;47&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;first&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0.5&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;1&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0px;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;47&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;subsequent&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;0.4&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;1&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0px;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;...&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;...&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;...&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 3px 3px 3px 3px; vertical-align: bottom;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;...&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;br /&gt;&lt;b&gt;Table 4:&lt;/b&gt; Bucketed data that enables separate analysis for first observations.&lt;br /&gt;&lt;br /&gt;While this approach requires more data storage than the straight bucketed data approach, it allows us to compute the metrics that are most relevant for evaluating the product.  Again, we can apply the &amp;nbsp;jackknife, but now to the subset of the data where the counter value is first or subsequent. In other words, we have filtered our data by a dimension value. Similarly, you can see that you can add any filter into the bucketed data approach (as long as the filtering field is available in the raw data logs). &lt;br /&gt;&lt;br /&gt;In summary, there are many different ways to account for the group structure when the experimental unit differs from the unit of observation. Regardless of how you do it, do remember to mind your units. &lt;br /&gt;&lt;br /&gt;&lt;h4&gt;Acknowledgments&lt;/h4&gt;&lt;i&gt;Many thanks to Dancsi Percival for his work on the impact of group-size heterogeneity on confidence intervals and to the many colleagues at Google who have evolved many of the ideas and tools described in this post. &lt;/i&gt;&lt;br /&gt;&lt;i&gt;&lt;br /&gt;&lt;/i&gt;&lt;br /&gt;&lt;h2&gt;Appendix: R code&lt;/h2&gt;&lt;div&gt;&lt;span style=&quot;font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; line-height: 1.2; white-space: pre-wrap;&quot;&gt;ComputeBootstrapMeanCI &amp;lt;- function(data, data.grouping, num.replicates = 2000) {&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;# Function to calculate confidence intervals for the mean via bootstrap.&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;# Must pass in a grouping variable to do the sampling on the grouping units.&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;# For bootstrap, we sample with replacement and each replicate is same size&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;# as original data; 95% CI obtained empirically by taking percentiles.&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;#&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;# Args:&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;# &amp;nbsp;&amp;nbsp;data: &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Numeric vector of data.&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;# &amp;nbsp;&amp;nbsp;data.grouping: &amp;nbsp;Numeric vector of grouping variable for data. Must be same&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;# &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;the same length as data.&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;# &amp;nbsp;&amp;nbsp;num.replicates: Number of replicates.&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;#&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;# Returns:&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;# &amp;nbsp;&amp;nbsp;Half-width of a 95% CI.&lt;/span&gt;&lt;/div&gt;&lt;span id=&quot;docs-internal-guid-aa550ea3-2f4e-a4b6-9383-80409dda6dae&quot; style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;# Split the data by the grouping variable.&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;assertthat::assert_that(length(data) == length(data.grouping))&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;data.split &amp;lt;- split(data, data.grouping)&lt;/span&gt;&lt;/div&gt;&lt;span style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;# Create replicates.&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;n.groups &amp;lt;- length(unique(data.grouping))&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;data.samples &amp;lt;-&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;replicate(num.replicates,&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;unlist(sample(data.split, replace = TRUE, size = n.groups)),&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;simplify = FALSE)&lt;/span&gt;&lt;/div&gt;&lt;span style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;# Get sampling distribution and summarize empirically.&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;bootstrap.means &amp;lt;- vapply(data.samples, mean, 1)&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;half.width &amp;lt;- unname(diff(quantile(bootstrap.means, c(0.025, 0.975))) / 2)&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;return(half.width)&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;}&lt;/span&gt;&lt;/div&gt;&lt;span style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;set.seed(1237)&lt;/span&gt;&lt;/div&gt;&lt;span style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;kNumGroups &amp;lt;- 50000 &amp;nbsp;# Total number of groups for all simulations.&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;kGroupSd &amp;lt;- 1 &amp;nbsp;# This is the standard deviation for group level means.&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;kNumReplicates &amp;lt;- 2000 &amp;nbsp;# Number of replicates for bootstrap.&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;kWithinGroupShrink &amp;lt;- 0.25 &amp;nbsp;# Shrinks standard dev. of w/in group observations.&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;kLambdaVector &amp;lt;- seq(0, 1.2, by = 0.2) &amp;nbsp;# Poisson parameter for group size.&lt;/span&gt;&lt;/div&gt;&lt;span style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;# Generate the mean for each group and use this for all runs of simulation.&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;group.means &amp;lt;- rnorm(kNumGroups, 0, kGroupSd)&lt;/span&gt;&lt;/div&gt;&lt;span style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;# Loop over simulation parameters and store results in a summary data frame.&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;summary.df &amp;lt;- data.frame()&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;for (param in kLambdaVector) {&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;group.sizes &amp;lt;- rpois(kNumGroups, param) + 1&lt;/span&gt;&lt;/div&gt;&lt;span style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;# Make data frame with group size &amp;amp; group mean for ease of computation.&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;group.df &amp;lt;- data.frame(id = 1:kNumGroups, group.means, group.sizes)&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;group.info &amp;lt;- split(group.df, group.df$id)&lt;/span&gt;&lt;/div&gt;&lt;span style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;# Helper function to generate per-group observations.&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;SimulateGroupObservations &amp;lt;- function(data) {&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;rnorm(data$group.sizes, data$group.means, kWithinGroupShrink * kGroupSd)&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;}&lt;/span&gt;&lt;/div&gt;&lt;span style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;# Generate vector of per-group observations for all groups.&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;full.obs &amp;lt;- unlist(sapply(group.info, SimulateGroupObservations))&lt;/span&gt;&lt;/div&gt;&lt;span style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;# Compute CI pretending that each observation is independent.&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;ignore.group.est.half.width &amp;lt;-&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;ComputeBootstrapMeanCI(full.obs, 1:length(full.obs), kNumReplicates)&lt;/span&gt;&lt;/div&gt;&lt;span style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;# Get a vector of group id&#39;s to incorporate dependence structure.&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;group.id &amp;lt;- unlist(sapply(group.info, function(x) rep(x$id, x$group.sizes)))&lt;/span&gt;&lt;/div&gt;&lt;span style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;# Compute CI accounting for the group structure.&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;include.group.est.half.width &amp;lt;-&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;ComputeBootstrapMeanCI(full.obs, group.id, kNumReplicates)&lt;/span&gt;&lt;/div&gt;&lt;span style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;# Formulate data frame to summarize results.&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;my.row &amp;lt;- data.frame(lambda = param,&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;n.multigroups = sum(group.sizes &amp;gt; 1) / kNumGroups,&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;n.observations = sum(group.sizes),&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;ignore.group.est.half.width,&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;include.group.est.half.width)&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt; &amp;nbsp;summary.df &amp;lt;- rbind(summary.df, my.row)&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;}&lt;/span&gt;&lt;br /&gt;&lt;div&gt;&lt;span style=&quot;color: black; font-family: &amp;quot;courier new&amp;quot;; font-size: 13.3333px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content><link rel='replies' type='application/atom+xml' href='https://www.unofficialgoogledatascience.com/feeds/6869788315354330264/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='https://www.unofficialgoogledatascience.com/2016/08/mind-your-units.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/6869788315354330264'/><link rel='self' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/6869788315354330264'/><link rel='alternate' type='text/html' href='https://www.unofficialgoogledatascience.com/2016/08/mind-your-units.html' title='Mind Your Units'/><author><name>Kay Brodersen</name><uri>http://www.blogger.com/profile/15877220456041691465</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://4.bp.blogspot.com/-F_KYHtECeMY/V6Cmc1Wju8I/AAAAAAAAD7g/XJfvDajK9NQFkUQqxsfdAiuVFnR7DjdCgCLcB/s72-c/mind_your_units_coverage.png" height="72" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-5825758052688213474.post-4158360322014638408</id><published>2016-06-30T00:18:00.000-07:00</published><updated>2016-06-30T02:04:20.433-07:00</updated><title type='text'>To Balance or Not to Balance?</title><content type='html'>By IVAN DIAZ &amp;amp; JOSEPH KELLY&lt;br /&gt;&lt;br /&gt;Determining the causal effects of an action—which we call treatment—on an outcome of interest is at the heart of many data analysis efforts. In an ideal world, experimentation through randomization of the treatment assignment allows the identification and consistent estimation of causal effects. In observational studies treatment is assigned by nature, therefore its mechanism is unknown and needs to be estimated. This can be done through estimation of a quantity known as the propensity score, defined as the probability of receiving treatment within strata of the observed covariates.&lt;br /&gt;&lt;br /&gt;There are two types of estimation method for propensity scores. &amp;nbsp;The first tries to predict treatment as accurately as possible. &amp;nbsp;The second tries to balance the distribution of predictors evenly between the treatment and control groups. The two approaches are related, because different predictor values among treated and control units could be used to better predict treatment status. In this post we discuss issues related to these goals, specification of loss functions for the two objectives, and compare both methods via simulation.&lt;br /&gt;&lt;br /&gt;We focus on an inverse propensity score weighted estimator for the causal effect. For this estimator, predicting the propensity score accurately is much more important than achieving the balancing property. This is because the estimator is not constructed utilizing the balancing property but wholly relies on reweighting using the propensity score. Other estimators, such as those based on matching and subclassification, may benefit from the balancing property, but the discussion of those estimators is postponed to a later post.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;The Fundamental Problem of Causal Inference&lt;/h2&gt;The primary goal of many data analysis efforts is to estimate the effect of an intervention on the distribution of some outcome. For example, imagine that you are working for a car manufacturer. Your company has recently launched a new pickup truck, along with the corresponding online advertisement campaign. You are in charge of assessing whether the campaign had an impact on sales. To do this, you have a data set at the person level containing, among other variables, an indicator of ad exposure, and whether the person bought the truck. A naïve way to solve this problem would be to compare the proportion of buyers between the exposed and unexposed groups, using a simple test for equality of means. Although it may seem sensible at first, this solution can be&lt;br /&gt;wrong if the data suffer from &lt;i&gt;selection bias&lt;/i&gt;.&lt;br /&gt;&lt;br /&gt;To illustrate the concept of selection bias, consider a group of users who have searched for pickup trucks through Google and a group who has not. Individuals in the former group are more likely to be exposed to an ad for pickup trucks. However, they are also more likely to buy a pickup truck regardless of ad exposure, because we know already that they are interested in pick up trucks given that they had done online research on them. A naïve comparison of the exposed and unexposed groups would produce an overly optimistic measurement of the effect of the ad, since the exposed group has a higher baseline likelihood of purchasing a pickup truck.&lt;br /&gt;&lt;br /&gt;There are two ways of getting around the selection bias problem: (i) &lt;i&gt;randomize&lt;/i&gt; ad exposure, and (ii) use an analysis for &lt;i&gt;observational&lt;/i&gt; data. In a randomized trial, we start with a random sample of the population of target individuals, and then assign them to one of two treatment arms &#39;randomly&#39; (that is, independent of all other observed or unobserved factors). Randomized studies are considered the gold standard for answering causal inference questions, since they guarantee the two groups only differ in their treatment assignment and have the same distribution in all observed and unobserved pre-treatment variables. Unfortunately, randomization is not possible in many situations. For example, in clinical trials, randomization may be unethical; in economics studies randomization is unfeasible in practice; and in marketing studies the potential cost of a lost business opportunity can make randomization unattractive.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Identification&lt;/h3&gt;We now discuss formally the statistical problem of causal inference. We start by describing the problem using standard statistical notation. For a random sample of units, indexed by $i = 1. \ldots n$, we have&lt;br /&gt;&lt;ul&gt;&lt;li&gt;$T_i$ as the binary treatment assignment variable,&lt;/li&gt;&lt;li&gt;$Y_{i1}$ and $Y_{i0}$ as the potential outcomes, and&lt;/li&gt;&lt;li&gt;$X_i$ as the vector of $p$ confounders&lt;/li&gt;&lt;/ul&gt;To simplify notation, since the data are assumed i.i.d., we drop the $i$ index. For $t=0,1$, the potential outcome $Y_t$ is defined as the outcome observed in a hypothetical world in which $P(T=t)=1$ (e.g., $Y_1$ is the outcome that would have been observed if everyone was exposed to the ad). We are interested in estimating the &lt;i&gt;average effect of treatment on the treated&lt;/i&gt;, or ATT, defined as $\delta = E(Y_{1}-Y_{0}|T=1)$. This quantity is interpreted as the expected change in the outcome caused by treatment, where the expectation is taken over the distribution of the potential outcomes among treated units.&lt;br /&gt;&lt;br /&gt;Because potential outcomes are unobserved, we need to link the distribution of $Y_t$ to the distribution of the observed data $(X, T, Y)$. A formula linking these distributions is known as an &lt;i&gt;identifiability&lt;/i&gt; result. Identifiability allows us to estimate the moments of $Y_t$ as functionals of the distribution of the observed data.&lt;br /&gt;&lt;br /&gt;The ATT can be estimated from the observed data if all three of the following&lt;br /&gt;&lt;i&gt;identifiability assumptions&lt;/i&gt; hold:&lt;br /&gt;&lt;ol&gt;&lt;li&gt;$Y_t=Y$ in the event $T=t$, stating that the observed outcome is congruent with the potential outcomes $(Y_0,Y_1)$, and&lt;/li&gt;&lt;li&gt;the assignment mechanism is *strongly ignorable*, that is $Y_0 \perp T \mid X$, which roughly states that all common causes of $T$ and $Y$ are measured and contained in $X,$ and&lt;/li&gt;&lt;li&gt;$P(T=0|X=x) &amp;gt; 0$ for all $x$ where $P(X=x)&amp;gt;0$,&lt;/li&gt;&lt;/ol&gt;stating there was &#39;enough experimentation&#39; in the observational study. This is often referred to as the &lt;i&gt;positivity&lt;/i&gt; assumption.&lt;br /&gt;&lt;br /&gt;The first assumption makes clear the fundamental problem of causal inference: for any given unit we observe at most one of the potential outcomes $Y=T\times Y_1 +(1-T)\times Y_0$. A violation to the second assumption formalizes the concept of selection bias. In our pickup truck example, $T$ indicates exposure to the ad, and $Y$ indicates purchase of a pick-up truck. Failure to include an indicator of search for the term &quot;pickup truck&quot; in $X$ would be a violation of strong ignorability. Identifiability assumptions are met by design in a randomized trial for every $X$, including $X=\emptyset$.&lt;br /&gt;&lt;br /&gt;These assumptions allow the identifiability of $E(Y_1|T=1)$ and $E(Y_0|T=1)$, both quantities necessary in estimating $\delta$. Identifiability of $E(Y_1|T=1)$ is simple: $E(Y_1|T=1) = E(Y|T=1)$ because $Y=Y_1$ in the event $T=1$. Identifiability of $\psi = E(Y_0|T=1)$ is somewhat harder. We start by noting that&lt;br /&gt;$$E(Y_0 \mid T=1, X)=E(Y_0 \mid T=0, X),$$&lt;br /&gt;due to strong ignorability. Then, $E(Y_0 \mid T=0, X) = E(Y\mid T=0, X)$, due to congruence of the potential and observed outcomes. The latter conditional expectation is well defined due to the positivity assumption. We can now use the law of iterated expectation to see that&lt;br /&gt;&lt;br /&gt;$\psi = E(Y_0|T=1)$&lt;br /&gt;&amp;nbsp; $= E\{E(Y_0 \mid T=1, X)\mid T=1\}$&lt;br /&gt;&amp;nbsp; $= E\{E(Y_0 \mid T=0, X)\mid T=1\}$&lt;br /&gt;&amp;nbsp; $= E\{E(Y \mid T=0, X)\mid T=1\}$&lt;br /&gt;&amp;nbsp; $= E\{\mu_0(X)\mid T=1\},$&lt;br /&gt;&lt;br /&gt;where $\mu_0(x)$ is a function of $x$ denoting the expected value of the outcome under $T=0$ and $X=x$. This formula is the identifiability result that we talked about earlier, and allows us to estimate $\delta$ solely from the observed data $(X, T, Y)$. Note that $\mu_0$ is the outcome expectation&lt;br /&gt;among the control units. This formula uses $\mu_0$ to predict the (unobserved) outcomes of the treated group, had they, contrary to the fact, been in the control group. The outer expectation takes the average of those predicted values for all treated units.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Identification Using The Balancing Property&lt;/h3&gt;Let us use $e(x)$ to denote the propensity score $P(T=1\mid X=x)$, following the convention in the propensity score literature.&lt;br /&gt;&lt;br /&gt;A balancing score is any function $b(x)$ satisfying $X\perp T\mid b(X)$. Using this property, simple algebra shows an equivalent identification result:&lt;br /&gt;$$\psi = E\{E(Y \mid T=0, b(X))\mid T=1\}.$$&lt;br /&gt;This and more features of balancing scores are discussed in a seminal paper by&lt;br /&gt;Rosenbaum &amp;amp; Rubin (1983).&lt;br /&gt;&lt;br /&gt;Clearly, $b(x)=x$ is a balancing score. However, $b(x)=x$ is not a very useful balancing score since it does not help alleviate the curse of dimensionality. A more useful balancing score is the propensity score $e(x)$. Since $e(x)$ is univariate, $\psi$ may be easily estimated by matching, subclassification, and other non-parametric estimation methods that adjust for an estimate of $e(x)$. Estimators that use this idea are said to be using the balancing property of the propensity score.&lt;br /&gt;&lt;br /&gt;A very important observation arising from this is that, for estimators of $\delta$ using the balancing property, we do not require consistent estimation of the propensity score but only of a balancing score. On the other hand, the reweighted estimator we describe next, does require consistent estimation of the propensity score. This apparently trivial observation is often overlooked and&lt;br /&gt;can be a source of confusion for data analysts working on causal inference analyses.&lt;br /&gt;&lt;br /&gt;Note that $\delta=E(Y\mid T=1)-\psi$. The expectation $E(Y\mid T=1)$ can be easily estimated with the sample mean of the outcome among the exposed units. In the simulation section we use this empirical mean estimator. Below we discuss estimators of $\psi$, which can be done via the propensity score with the methods we describe below.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;The Propensity Score Weighted Estimator&lt;/h2&gt;Writing the expectations as integrals plus some additional algebra shows that&lt;br /&gt;$$\psi = E\{E(Y \mid T=0, X)\mid T=1\} = \frac{E\{W Y\mid T=0\}}{E(W|T=0)},$$&lt;br /&gt;with $W=e(X)/(1-e(X)).$ A natural estimator for $\psi$ is then given by&lt;br /&gt;$$\hat \psi = \frac{\sum_{i:T_i=0} \hat W_i Y_i}{\sum_{i:T_i=0} \hat W_i},$$&lt;br /&gt;where $\hat W_i = \hat e(X_i)/(1-\hat e(X_i))$, and $\hat e(x)$ denotes an estimator of the propensity score.&lt;br /&gt;&lt;br /&gt;It can be easily checked that if $\hat e$ is consistent, so is $\hat \psi$. Then, because $E(Y\mid T=1)$ can be estimated consistently, the estimator of $\delta=E(Y\mid T=1)-\psi$ will be consistent as well. It is then very important to place all efforts on obtaining consistency of $\hat e$. In this post we review and compare two types of method for estimating the propensity score: methods based on predictive accuracy and methods based on balancing the covariate distribution between treated and control units. Based on our results and the above discussions we argue that predictive accuracy, rather than balance, should be the criterion guiding the choice of method.&lt;br /&gt;&lt;br /&gt;It should be noted that inverse probability weighting is not generally optimal (i.e., efficient) and doubly robust estimators such as the augmented IPW and the TMLE provide an opportunity to achieve the non-parametric efficiency bound (Hahn, 1998). Since our interest is to evaluate estimators for the propensity score as they relate to estimation of the causal effect, we focus on the reweighting method described above.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Estimating the Propensity Score&lt;/h2&gt;We now introduce more formally the main dichotomy of this post: predictive accuracy vs covariate balance. We do this by describing the methods in terms of loss functions whose expectation is optimized at the true value of the propensity score.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Methods Based on Predictive Power&lt;/h3&gt;Let $\cal F$ be the space of all functions of $x$ bounded between zero and one, and let $f$ be a generic function in that space. We say that a loss function $L$ is valid for the propensity score if the following holds:&lt;br /&gt;$$ \arg\min_{f\in \cal F} E(L(f; M)) = e, $$&lt;br /&gt;where $M = (X, T)$. That is, a loss function $L$ is valid if the expected loss is minimized at the true propensity score $e$. Examples of valid loss functions are the $L^2$ and negative log-likelihood loss functions given by&lt;br /&gt;&lt;br /&gt;&amp;nbsp; $L(f;m) = (t-f(x))^2,$ and&lt;br /&gt;&amp;nbsp; $L(f;m) = -t\log(f(x))-(1-t)\log(1-f(x)),$&lt;br /&gt;&lt;br /&gt;respectively. Though theoretically valid, the $L^2$ loss function is known to underperform compared to the log-likelihood.&lt;br /&gt;&lt;br /&gt;The choice of space $\cal F$ (sometimes called the &lt;i&gt;model&lt;/i&gt;) and loss function $L$ explicitly defines the estimation problem. For example, logistic regression is based on the negative log-likelihood loss function and the space of logistic functions&lt;br /&gt;$$ \mathcal{F}_\textrm{logistic} = \bigg\{f(x) = \frac{1}{1 + \exp(-x&#39;\beta)}:&lt;br /&gt;\beta \in \mathbb{R}^p \bigg\}. &amp;nbsp;$$&lt;br /&gt;A model like $\mathcal{F}_\textrm{logistic}$, indexed by a Euclidean parameter, is often referred to as a &lt;i&gt;parametric model&lt;/i&gt;. Although desirable, optimization in the complete space $\cal F$ is not possible in practice when $x$ contains continuous variables, or its dimension is large (cf. the curse of dimensionality). On the other hand, restriction to more tractable spaces such as $\cal{F}_\textrm{logistic}$ is well known to lead to the issue of &lt;i&gt;model misspecification&lt;/i&gt;, which occurs when the space considered does not contain the propensity score. In the presence of model misspecification, the estimator $\hat\psi$ is inconsistent.&lt;br /&gt;&lt;br /&gt;The field of statistical machine learning provides a solution to this problem, allowing exploration of larger spaces. For example, a tree regression algorithm uses&lt;br /&gt;$$\mathcal{F}_\textrm{tree} = \bigg\{\sum_j^J c_j I(x \in D_j): c_j \in&lt;br /&gt;\mathbb{R}, D_j\bigg\},$$&lt;br /&gt;where $c_j$ are constants, $I(A)$ is the indicator function returning one if $A$ is true and zero otherwise, and $D_j$ are disjoint partitions of the covariate space. Another example, given by multivariate adaptive regression splines (MARS), uses&lt;br /&gt;$$\mathcal{F}_\textrm{mars} = \bigg\{f(x) = \sum_j^J\beta_jB_j(x): \beta \in&lt;br /&gt;\mathbb{R}^p; B_j\bigg\},$$&lt;br /&gt;where $B_j$ are basis functions of the form of hinge functions $\max(0, x_j - c_j)$, and $c_j$ are tuning parameters. These spaces are larger than $\cal{F}_\textrm{logistic}$ above. Under lack of domain-specific scientific knowledge supporting the use of a parametric model, these data-adaptive methods have a better chance of consistently estimating the propensity score. Choosing the tuning parameters for data-adaptive methods such as regression trees and MARS is the subject of a large number of research articles and books.&lt;br /&gt;&lt;br /&gt;We chose the tree regression and MARS estimators only for illustration purposes, other possible choices of off-the-shelf prediction method include random forests, support vector machines, generalized boosted regression models, neural networks, $k$ nearest neighbors, regularized regression, and many more. An excellent review of statistical learning methods may be found in Friedman et. al. (2001).&lt;br /&gt;&lt;h4&gt;Model Stacking - Super Learner (SL)&lt;/h4&gt;When using predictive power as a criterion, the question arises of how to select among the many prediction methods available in the statistical learning literature. We approach this question with a data-adaptive mindset that involves the following steps (see van der Laan et al., 2007):&lt;br /&gt;&lt;br /&gt;&lt;ol&gt;&lt;li&gt;Propose a finite collection $\mathcal L=\{\hat e_k:k=1,\ldots,K\}$ of estimation algorithms. An estimation algorithm is a procedure that takes a training data set $\mathcal T=\{M_i,i=1,\ldots, n\}$ and outputs a function $\hat e_k(x)$.&lt;/li&gt;&lt;li&gt;Consider an ensemble learner of the type $\hat e_\alpha(x) = \sum_{k=1}^K \alpha_k \hat e_k(x);\quad\text{for}\quad 0 \leq \alpha_k\leq 1, \quad \sum_{k=1}^K\alpha_k=1.$&lt;/li&gt;&lt;li&gt;Let $\mathcal V_1, \ldots, \mathcal V_J$ denote a partition of the index set $\{1,\ldots, n\}$ of approximately the same size. In addition, for each $j$, let the associated training set be defined as $\mathcal{T}_j = \{1,\ldots, n\}\backslash \mathcal V_j$. For fixed weights $\alpha$, denote $\hat e_{\alpha, \mathcal T_j}$ the ensemble trained using only data in $\mathcal T_j$. Choose the weights $\alpha$ that minimize the cross-validated risk: $\hat\alpha =\arg\min_{\alpha} \frac{1}{J}\sum_{j=1}^J\frac{1}{|\mathcal V_j|}\sum_{i\in \mathcal V_j} L(M_i, \hat e_{\alpha, \mathcal T_j})$ subject to $\quad 0 \leq \alpha_k\leq 1, \sum_{k=1}^K\alpha_k=1,$ and define the final estimator as $\hat e_{\hat\alpha}(x)$. This algorithm is implemented in the &lt;a href=&quot;https://cran.r-project.org/web/packages/SuperLearner/index.html&quot;&gt;SuperLearner&lt;/a&gt;&amp;nbsp;R package&amp;nbsp;(Polley &amp;amp; van der Laan, 2014).&lt;/li&gt;&lt;/ol&gt;It is clear that this Super Learner explores a much larger space than any $\mathcal F_k$ explored by each of the independent learners. As a consequence, it has more chances of containing the propensity score than any given learner, and we expect it to perform better asymptotically. In fact, it has been shown that this cross-validation scheme will pick the best estimator as the sample size increases (van der Laan et al., 2007).&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Methods Based on The Balancing Property&lt;/h3&gt;Via &lt;a href=&quot;https://en.wikipedia.org/wiki/Law_of_total_expectation&quot;&gt;Adam&#39;s Law&lt;/a&gt; it can be shown that&lt;br /&gt;$$ E\bigg\{(1-T)\frac{e(X)}{1-e(X)}c(X)\bigg\} =&lt;br /&gt;E\{Tc(X)\}\, \text{ for all functions } c(x).$$&lt;br /&gt;Here $c(x)$ is any function of $x$. Estimation methods that use this property are referred to as &lt;i&gt;covariate balancing&lt;/i&gt;, since they assure that properly reweighting leads to the same distribution in the treated and control groups.&lt;br /&gt;&lt;br /&gt;As a result of the balancing property, any estimator $\hat e$ satisfying&lt;br /&gt;$$\sum_{i:T_i=0}\frac{\hat e(X_i)}{1-\hat e(X_i)}c(X_i) = \sum_{i:T_i=1}c(X_i)\,&lt;br /&gt;\text{ for all functions } c(x)$$&lt;br /&gt;can be expected to be a consistent estimator of $e(x)$. Because it has to hold for all functions $c(x)$, this balance condition is analogous to performing a search in the full space $\mathcal F$, and is impossible to achieve in practice. Methods that aim to achieve balance focus on a user-given set of functions $c_1,\ldots, c_J$. The task of choosing the correct functions $c_j$ is akin to the task of specifying the correct functional form in a parametric model. As a result, estimators that focus on covariate balancing are also susceptible to being inconsistent due to model misspecification.&lt;br /&gt;&lt;br /&gt;Now let&#39;s take a look at two methods that use covariate balancing to estimate the propensity score: entropy balancing and covariate balancing propensity score.&lt;br /&gt;&lt;h4&gt;Entropy Balancing (EB)&lt;/h4&gt;Entropy balancing (Hainmueller, 2012) is a method that directly estimates the weights $W_i$, rather than the propensity score, by solving the following optimization problem:&lt;br /&gt;$$ \hat W = \arg\min_W \sum_{i:T_i=0}W_i\log W_i $$&lt;br /&gt;subject to&lt;br /&gt;$$\frac{1}{n_0}\sum_{i:T_i=0}W_ic_j(X_i) = \frac{1}{n_1}\sum_{i:T_i=1}c_j(X_i)\,&lt;br /&gt;\text{ for a set of functions } c_j:j\in\{1,\ldots J\}.$$&lt;br /&gt;The above constrained optimization problem minimizes the entropy of $W_i$ to obtain obtain weights that satisfy the balance conditions for the user-specified covariate functions $c_j$. These functions are chosen by the user, and are generally given by lower order polynomials. Because covariate balance can only be achieved for a finite (generally small) number of functions $c_j$, this method will be inconsistent unless the correct functions $c_j$ are specified. In fact, Hainmueller (2012) show that entropy balancing is equivalent to estimating the weights as a log-linear model of the covariate functions $c_j(X)$.&lt;br /&gt;&lt;br /&gt;See Hainmueller (2012), and the work of Zhao &amp;amp; Percival (2015) for more details on how this optimization problem is solved, and for further discussion.&lt;br /&gt;&lt;h4&gt;Covariate Balancing Propensity Score (CBPS)&lt;/h4&gt;This method proceeds by specifying a parametric form for the propensity score, and then optimizing the negative log-likelihood loss, constrained to covariate balance on a set of functions $c_j(x)$. For example, consider the logistic model $\mathcal{F}_\textrm{logistic}$ defined above. CBPS proceeds by solving the following optimization problem:&lt;br /&gt;$$ e = \arg\min_{f\in \mathcal{F}_\textrm{logistic}} E(L(f;M)), $$&lt;br /&gt;subject to&lt;br /&gt;$$\sum_{i:T_i=0}\frac{\hat f(X_i)}{1-\hat f(X_i)}c_j(X_i) =&lt;br /&gt;\sum_{i:T_i=1}c_j(X_i)\, \text{ for a set of functions } c_j:j\in\{1,\ldots&lt;br /&gt;J\},$$&lt;br /&gt;where $L$ is the negative log-likelihood loss $L(f;w)=-t\log(f(x))-(1-t)\log(1-f(x))$. The constraint set of this method can be seen to be equivalent to the constraint of EB, only the loss function optimized changes. This problem may be solved using empirical likelihood (Qin &amp;amp; Lawless, 1994) or other methods. For a more detailed discussion the interested reader is referred to the original research article by Imai &amp;amp; Ratkovic (2014).&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Simulation Study&lt;/h2&gt;We compare the various methods for estimating the propensity score in a simulation study. We use performance metrics such as bias and mean squared error for the estimation of $\delta$, our causal estimand of interest, defined as the average effect of treatment on the treated. The methods used are:&lt;br /&gt;&lt;br /&gt;Covariate balance:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;EB.&lt;/li&gt;&lt;li&gt;CBPS (exact and over-parameterized).&lt;/li&gt;&lt;/ul&gt;Predictive accuracy:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;Multivariate adaptive regression splines (MARS) (Friedman, 1991).&lt;/li&gt;&lt;li&gt;Random forest with default R tuning parameters (Breiman, 2001).&lt;/li&gt;&lt;li&gt;Support vector machines with linear kernel (Cortes &amp;amp; Vapnik, 1995).&lt;/li&gt;&lt;li&gt;The MLE in a logistic regression model.&lt;/li&gt;&lt;li&gt;Bayesian logistic regression assuming a diffuse normal prior with mean zero.&lt;/li&gt;&lt;li&gt;$L^1$ regularized logistic regression (Lasso) (Tibshirani, 1996).&lt;/li&gt;&lt;li&gt;SL (discrete and full) where the full method is described above and the discrete method does not weight each algorithm but chooses the best one under cross-validation for predictions (van der Laan et al., 2007).&lt;/li&gt;&lt;/ul&gt;We use the data generating mechanism described below. This simulation scheme was first used in an influential paper by Kang &amp;amp; Schafer (2007), and became a standard for comparing estimators for causal estimands. Kang &amp;amp; Schafer use this data generating mechanism to illustrate bias arising in estimation of an outcome mean under informative nonresponse. The issues arising in their problem are identical in nature to issues arising in estimation of causal effects, therefore their setup is very appropriate to illustrate our points. You can read the original research paper to find out more.&lt;br /&gt;&lt;br /&gt;The true set of covariates is generated independently and identically distributed, from the following distribution:&lt;br /&gt;$$(Z_{1}, Z_{2}, Z_{3}, Z_{4}) \sim N_4(0, I_4)$$&lt;br /&gt;where $I_4$ is the four dimensional identity matrix. The outcome is generated as&lt;br /&gt;$$ Y = 210 + 27.4 Z_{1} + 13.7 Z_{2} + 13.7Z_{3} + 13.7 Z_{4} + \epsilon $$&lt;br /&gt;where $\epsilon \sim N(0, 1)$. The propensity score is defined as&lt;br /&gt;$$ P(T = 1 \mid Z = z) = \text{expit}(-z_{1} + 0.5 z_{2} - 0.25z_{3} - 0.1 z_{4}). $$&lt;br /&gt;where expit is the inverse-logit transformation. Note that $\delta = 0$, whereas $E(Y|T=1)=200$ and $E(Y|T=0)=220$, demonstrating the selection bias.&lt;br /&gt;&lt;br /&gt;In the simulation we will examine the performance of the above algorithms under the correctly specified propensity model from the data generation process and under a misspecified model. Misspecification occurs when the following transformations are observed in place of the true covariates:&lt;br /&gt;&lt;br /&gt;&amp;nbsp; $x_{1} = \exp(z_1/2),$&lt;br /&gt;&amp;nbsp; $x_{2} = z_{2}/(1 + \exp(z_{1})),$&lt;br /&gt;&amp;nbsp; $x_{3} = (z_{1} z_{3}/25 + 0.6)^3,$&lt;br /&gt;$x_{4} = (z_{2} + z_{4} + 20)^2.$&lt;br /&gt;&lt;br /&gt;These transformations were proposed in the original paper in order to reflect a real-life problem. It is very unlikely a researcher observing $(X_{1}, X_{2}, X_{3}, X_{4})$ would specify the correct model reflecting these transformations. However, $X$ contains all the relevant information and strong ignorability holds. Without the correct functional form, a researcher using parametric methods is guaranteed to fit a misspecified model. This fact is well documented by Kang &amp;amp; Schafer (2007). The case where only the $X$s are observed is the most interesting since it exemplifies most, if not all, real data analysis problems.&lt;br /&gt;&lt;br /&gt;To compare methods we conduct a simulation study by generating $10,000$ datasets according to the above data generation process. Then, for each dataset we estimate the propensity score using the true covariates $Z$ and the transformed covariates $X$ according to the various estimators we are examining. Then we estimate $\delta$ and aggregate across simulations to examine the estimated absolute bias&lt;br /&gt;$$&lt;br /&gt;\frac{1}{10^4} \bigg| \sum_{j = 1}^{10^4} (\hat\delta_j - \delta) \bigg|&lt;br /&gt;$$&lt;br /&gt;and root mean square error (RMSE)&lt;br /&gt;$$&lt;br /&gt;\sqrt{\frac{1}{10^4} \sum_{j = 1}^{10^4} (\hat \delta_j - \delta)^2}&lt;br /&gt;$$&lt;br /&gt;of the different methods. These formulas represent Monte Carlo integrals that aim to approximate the true bias and RMSE of the estimators. The largest estimated error in these integrals was around 0.3 which is small relative to the bias and MSE sizes we see.&lt;br /&gt;&lt;br /&gt;The algorithms in the plot are ordered according to the sum of the respective metrics weighted by the square root of the sample size considered. By doing this, we favor algorithms that perform better in larger datasets, in accordance with the statistical notion of consistency. Algorithms aimed at achieving balance in the covariates are shown in boldface.&lt;br /&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-4mKqjuC_tMU/V3TghohcUpI/AAAAAAAAD0I/pklkjNf6T0Ibdir85xlHqq8dik3GWmfyACLcB/s1600/BiasRMSE.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;496&quot; src=&quot;https://1.bp.blogspot.com/-4mKqjuC_tMU/V3TghohcUpI/AAAAAAAAD0I/pklkjNf6T0Ibdir85xlHqq8dik3GWmfyACLcB/s640/BiasRMSE.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;The similarity between the RMSE and bias plots teaches us that most of the poor performance is driven by bias rather than variance.&lt;br /&gt;&lt;br /&gt;As we stated before, the most interesting case for practitioners is when the transformed covariates are observed. In this case, MARS is the best estimator. The two versions of the Super Learner follow closely. This should not be taken as an argument in favor of using MARS in every practical problem. Rather, it is proof of the importance of using a principled model selection tool, since no one algorithm should be expected to outperform all others uniformly across datasets and problems (cf. no free lunch&lt;br /&gt;theorem). Likewise, methods based on random forests and SVM perform quite poorly. This may be surprising to some readers who know that these methods are data-adaptive and have seen them perform well in several applications. This is another demonstration of our claim that no method should be blindly trusted and all methods should be tested against the data for the specific problem at hand. We have seen applications in which the Lasso, Random Forests, or even a simple logistic regression outperforms all competitors in terms of predictive power.&lt;br /&gt;&lt;br /&gt;Regarding covariate balance, we see that the two versions of the CBPS perform relatively well. CBPS is a GLM with an extra optimization constraint to satisfy covariate balance. In our simulation, this extra constraint did improve the bias and MSE compared to a standard GLM.&lt;br /&gt;&lt;br /&gt;The entropy balanced estimator performs quite poorly. Note that the EB is an estimator that achieves balance in the sample covariate means, and as such would always pass the routine validation tests advocated in the balancing score literature. In the case of EB it appears that the predictive accuracy of the propensity score is sacrificed to ensure covariate balance. As our reweighting estimator for $\psi$ is constructed on the basis of consistent propensity score estimation (rather than the balancing property), it is not surprising to see the poor performance of EB.&lt;br /&gt;&lt;br /&gt;Interestingly, covariate balancing methods outperform all other methods—including a correct logistic regression model—when the correct covariates are observed. This is because the response is linear in the correct covariates and thus an estimator that ensures complete balance on the true covariates automatically reduces all of the bias. However, this is merely a theoretical curiosity, as almost every practical situation belongs in the first panel of the plot rather than the second one.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Conclusion&lt;/h2&gt;For our reweighting estimator we see that objective, flexible data-adaptive estimators for the propensity score typically perform best in the case of a misspecified model. This is because these algorithms are able to better explore the covariate space than their parametric counterparts and better recover, or at least approximate, the correct functional form.&lt;br /&gt;&lt;br /&gt;EB and CBPS both require specifying functions of the covariates that need to be balanced. This can be very useful if the researcher has prior domain knowledge and knows what functions of the covariates affect the response and hence need to be balanced. This sort of a-priori knowledge imposes a structure on the propensity score and thus is akin to knowing that the propensity score belongs&lt;br /&gt;to a parametric family of distributions. Unfortunately, objective knowledge of this type is absent in most practical situations, particularly in large dimensional problems.&lt;br /&gt;&lt;br /&gt;To conclude, in the absence of subject-matter knowledge supporting the use of parametric functional forms for the propensity score and the balancing conditions, predictive accuracy should be used to select an estimator among a collection of candidates. This collection may include covariate balanced&lt;br /&gt;estimators, and should contain flexible data-adaptive methods capable of unveiling complex patterns in the data. In particular, we advocate for the use of model stacking methods such as the Super Learner algorithm implemented in the SuperLearner R package.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Bibliography&lt;/h2&gt;&lt;a href=&quot;https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf&quot;&gt;Breiman, Leo. &quot;Random forests.&quot; Machine learning 45.1 (2001): 5-32.&lt;/a&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href=&quot;http://image.diku.dk/imagecanon/material/cortes_vapnik95.pdf&quot;&gt;Cortes, Corinna, and Vladimir Vapnik. &quot;Support-vector networks.&quot; Machine learning 20.3 (1995): 273-297.&lt;/a&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href=&quot;https://projecteuclid.org/euclid.aos/1176347963&quot;&gt;Friedman, Jerome H. &quot;Multivariate adaptive regression splines.&quot; The annals of statistics (1991): 1-67.&lt;/a&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href=&quot;http://statweb.stanford.edu/~tibs/ElemStatLearn/&quot;&gt;Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. &quot;The elements of statistical learning.&quot; Vol. 1. Springer, Berlin: Springer series in statistics, (2001)&lt;/a&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href=&quot;http://www.jstor.org/stable/2998560&quot;&gt;Hahn, Jinyong. &quot;On the role of the propensity score in efficient semiparametric estimation of average treatment effects.&quot; Econometrica (1998): 315-331.&lt;/a&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href=&quot;http://pan.oxfordjournals.org/content/20/1/25.abstract&quot;&gt;Hainmueller, Jens. &quot;Entropy balancing for causal effects: A multivariate reweighting method to produce balanced samples in observational studies.&quot; Political Analysis 20.1 (2012): 25-46.&lt;/a&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href=&quot;http://imai.princeton.edu/research/files/CBPS.pdf&quot;&gt;Imai, Kosuke, and Marc Ratkovic. &quot;Covariate balancing propensity score.&quot; Journal of the Royal Statistical Society: Series B (Statistical Methodology) 76.1 (2014): 243-263.&lt;/a&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href=&quot;http://www.jstor.org/stable/27645858&quot;&gt;Kang, Joseph DY, and Joseph L. Schafer. &quot;Demystifying double robustness: A comparison of alternative strategies for estimating a population mean from incomplete data.&quot; Statistical science (2007): 523-539.&lt;/a&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href=&quot;http://www.jstor.org/stable/2242455&quot;&gt;Qin, Jin, and Jerry Lawless. &quot;Empirical likelihood and general estimating equations.&quot; The Annals of Statistics (1994): 300-325.&lt;/a&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href=&quot;http://biomet.oxfordjournals.org/content/70/1/41.short&quot;&gt;Rosenbaum, Paul R., and Donald B. Rubin. &quot;The central role of the propensityscore in observational studies for causal effects.&quot; Biometrika 70.1 (1983): 41-55.&lt;/a&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href=&quot;http://www.jstor.org/stable/2346178&quot;&gt;Tibshirani, Robert. &quot;Regression shrinkage and selection via the lasso.&quot; Journal of the Royal Statistical Society. Series B (Methodological) (1996): 267-288.&lt;/a&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href=&quot;http://www.degruyter.com/view/j/sagmb.2007.6.issue-1/sagmb.2007.6.1.1309/sagmb.2007.6.1.1309.xml&quot;&gt;van der Laan, Mark J., and Polley, Eric C., and Hubbard, Alan E. &quot;Super learner.&quot; Statistical applications in genetics and molecular biology (2007): Vol 6.1&lt;/a&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/SuperLearner/index.html&quot;&gt;Eric Polley and Mark van der Laan (2014). SuperLearner: Super Learner Prediction. R package version 2.0-15.&lt;/a&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href=&quot;http://arxiv.org/abs/1501.03571&quot;&gt;Zhao, Qingyuan, and Daniel Percival. &quot;Primal-dual Covariate Balance and Minimal Double Robustness via Entropy Balancing.&quot; arXiv preprint arXiv:1501.03571 (2015).&lt;/a&gt;&lt;br /&gt;&lt;br /&gt;</content><link rel='replies' type='application/atom+xml' href='https://www.unofficialgoogledatascience.com/feeds/4158360322014638408/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='https://www.unofficialgoogledatascience.com/2016/06/to-balance-or-not-to-balance.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/4158360322014638408'/><link rel='self' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/4158360322014638408'/><link rel='alternate' type='text/html' href='https://www.unofficialgoogledatascience.com/2016/06/to-balance-or-not-to-balance.html' title='To Balance or Not to Balance?'/><author><name>Kay Brodersen</name><uri>http://www.blogger.com/profile/15877220456041691465</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-4mKqjuC_tMU/V3TghohcUpI/AAAAAAAAD0I/pklkjNf6T0Ibdir85xlHqq8dik3GWmfyACLcB/s72-c/BiasRMSE.png" height="72" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-5825758052688213474.post-3214828694374915136</id><published>2016-05-31T04:22:00.000-07:00</published><updated>2017-02-01T19:37:01.164-08:00</updated><title type='text'>Estimating causal effects using geo experiments</title><content type='html'>by JOUNI KERMAN, JON VAVER, and JIM KOEHLER&lt;br/&gt;&lt;br /&gt;&lt;div&gt;&lt;i&gt;Randomized experiments represent the gold standard for determining the causal effects of app or website design decisions on user behavior. We might be interested in comparing, for example, different subscription offers, different versions of terms and conditions, or different user interfaces. When it comes to online ads, there is also a fundamental need to estimate the return on investment. Observational data such as paid clicks, website visits, or sales can be stored and analyzed easily. However, it is generally not possible to determine the incremental impact of advertising by merely observing such data across time. One approach that Google has long used to obtain causal estimates of the impact of advertising is geo experiments.&lt;/i&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;What does it take to estimate the impact of online exposure on user behavior? Consider, for example, an &lt;i&gt;A/B&lt;/i&gt; &lt;i&gt;experiment&lt;/i&gt;, where one or the other version (&lt;i&gt;A&lt;/i&gt; or &lt;i&gt;B&lt;/i&gt;) of a web page is shown at random to a user. The analysis could then proceed with comparing the probabilities of clicking on a certain link on the page shown. The version of the web page that has a significantly higher estimated probability of click (&lt;i&gt;click-through rate&lt;/i&gt;, or &lt;i&gt;CTR&lt;/i&gt;) would be deemed the more effective one.&lt;br /&gt;&lt;br /&gt;Similarly, we could test the effectiveness of a search ad compared to showing only organic search results. Whenever a user types in a specific search query, the system makes a split-second decision of whether or not to show a particular ad next to the organic results. Click-through rates can then be compared to determine the relative effectiveness of the presence of ads.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Traffic experiments&lt;/b&gt;&amp;nbsp;like the ones above can randomize queries, but they cannot randomize users. The same user may be shown the ad whenever they perform the same search the second time. Traffic experiments, therefore, don&#39;t allow us to determine the longer term effect of the ad on the behavior of users.&lt;br /&gt;&lt;br /&gt;We could approximate users by &lt;a href=&quot;https://en.wikipedia.org/wiki/HTTP_cookie&quot;&gt;cookies&lt;/a&gt;. However, one user may have several devices (desktop, laptop, tablet, smartphone), each with its own cookie space. Moreover, cookies get deleted and regenerated frequently. &lt;i&gt;Cookie churn&lt;/i&gt; increases the chances that a user may end up receiving a mixture of both the active treatment and the control treatment.&lt;br /&gt;&lt;br /&gt;Even if  we were able to keep a record of all searches and ad clicks and associate them with online conversions, we would still not be able to observe any long-term behavior. A conversion might happen days after the ad was seen, perhaps at a regular brick-and-mortar store. This makes it difficult to attribute the effect of an online ad on offline purchases.&lt;br /&gt;&lt;br /&gt;It is important that we can measure the effect of these offline conversions as well. How can we connect an event of purchase to the event of perceiving the ad if the purchase does not happen immediately? And, from the perspective of the experiment set-up, how can we ensure that a user whom we assigned to the control group won&#39;t ever see the ad during the experiment?&lt;br /&gt;&lt;br /&gt;Another possibility is to run a &lt;b&gt;panel study&lt;/b&gt;, an experiment with a recruited set of users who allow us to analyze their web and app usage, and their purchase behavior. Panel studies make it possible to measure user behavior along with the exposure to ads and other online elements. However, meaningful insights require a representative panel or an analysis that corrects for the sampling bias that may be present. In addition, panel studies are expensive.&amp;nbsp;Wouldn&#39;t it be great if we didn&#39;t require individual data to estimate an aggregate effect? Let&#39;s take a look at larger groups of individuals whose aggregate behavior we can measure.&lt;br /&gt;&lt;br /&gt;A &lt;b&gt;geo experiment&lt;/b&gt; is an experiment where the experimental units are defined by geographic regions. Such regions are often referred to as &lt;i&gt;Generalized Market Areas&lt;/i&gt; (GMAs) or simply geos. They are non-overlapping &lt;a href=&quot;https://developers.google.com/adwords/api/docs/appendix/geotargeting&quot;&gt;geo-targetable regions&lt;/a&gt;. This means it is possible to specify exactly in which geos an ad campaign will be served&amp;nbsp;&lt;span id=&quot;docs-internal-guid-610870cd-24e8-7526-188c-4b2cc2bd5bdd&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 14.6667px; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;–&lt;/span&gt;&lt;/span&gt;&amp;nbsp;and to observe the ad spend and the response metric at the geo level. We&amp;nbsp;can then form treatment and control groups by randomizing a set of geos.&lt;br /&gt;&lt;br /&gt;Consider, as an example, the partition of the United States into &lt;a href=&quot;https://developers.google.com/adwords/api/docs/appendix/cities-DMAregions&quot;&gt;210 GMAs defined by Nielsen Media&lt;/a&gt;. The regions were originally formed based on television viewing behavior of their residents, clustering together “exclusive geographic area of counties in which the home market television stations hold a dominance of total hours viewed.” These geos can be targeted individually in Google AdWords. Here is an example of a randomized assignment:&lt;br /&gt;&lt;div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;div style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;http://2.bp.blogspot.com/-G6EoofJH1Iw/V1U2cA_qORI/AAAAAAAADtc/qxRTUzLogGI8SCRElmW8aKUNMfNcU1cFwCK4B/s1600/usdma.png&quot; imageanchor=&quot;1&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;217&quot; src=&quot;https://2.bp.blogspot.com/-G6EoofJH1Iw/V1U2cA_qORI/AAAAAAAADtc/qxRTUzLogGI8SCRElmW8aKUNMfNcU1cFwCK4B/s400/usdma.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;In contrast to the US, France doesn&#39;t currently have an analogous set of geos. So we created a set of geos using our own clustering algorithms. The figure below shows  an example of a set of geos partitioning mainland France into 29 GMAs:&lt;br /&gt;&lt;br /&gt;&lt;div style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;http://2.bp.blogspot.com/-4R_cIUcRNoI/V1U20eVxW6I/AAAAAAAADtk/YruerrmAMAQozvU_lFTCVRHReFwX3EP2gCK4B/s1600/france.png&quot; imageanchor=&quot;1&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;320&quot; src=&quot;https://2.bp.blogspot.com/-4R_cIUcRNoI/V1U20eVxW6I/AAAAAAAADtk/YruerrmAMAQozvU_lFTCVRHReFwX3EP2gCK4B/s320/france.png&quot; width=&quot;319&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;Suppose that users in control regions are served ads with a total spend intensity of $C$ dollars per week, while users in treatment regions are served ads with a cost of $T = C + A$ dollars per week ($A &amp;gt; 0$). The key assumption in geo experiments is that users in each region contribute to sales only in their respective region. This assumption allows us to estimate the effect of the ad spend on sales.&amp;nbsp;What makes geo experiments so simple and powerful is that they allow us to capture the full effects of advertising, including offline sales and conversions over longer periods of time (e.g., days or weeks).&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;h2&gt;Measuring the effectiveness of online ad campaigns&lt;/h2&gt;Estimating the causal effects of an advertising campaign, and the value of advertising in general, is what each of Google’s advertising clients would like to do for each of their products. There are many methods for estimating causal effects, and yet getting it right remains a challenging problem in practice.&lt;br /&gt;&lt;br /&gt;The quantity that we aim to estimate, in particular, is a specific type of &lt;i&gt;return on investment&lt;/i&gt; where the &lt;i&gt;investment&lt;/i&gt; is the cost of advertising. We often refer to this as the &lt;i&gt;Return On Ad Spend&lt;/i&gt; (ROAS). Even more specifically, we are typically interested in the &lt;i&gt;change&lt;/i&gt; in sales (or website visits, conversions, etc.) when we change the ad spend: the &lt;i&gt;incremental&lt;/i&gt; ROAS, or iROAS. When response is expressed in terms of the same currency as the investment, iROAS is just a scalar. For example, an iROAS of 3 means that each extra dollar invested in advertising leads to 3 incremental dollars in revenue. Alternatively, when the response is the number of conversions, iROAS can be given in terms of number of conversions per additional unit of currency invested, say “1000 incremental conversions per 10,000 additional dollars spent.” In other words, iROAS is the slope of a curve of the response metric plotted against the underlying advertising spend.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;h2&gt;Structure of a geo experiment&lt;/h2&gt;A typical geo experiment consists of two distinct time periods: &lt;i&gt;pretest&lt;/i&gt; and &lt;i&gt;test&lt;/i&gt;. During the pretest period, there are no differences in the ad campaign structure across geos. All geos operate at the same baseline level; the incremental difference between the control and treatment geos is zero in expectation. The pretest period is typically 4 to 8 weeks long.&lt;br /&gt;&lt;br /&gt;During the test period (which is typically 3 to 5 weeks long), geos in the treatment group are targeted with modified advertising campaigns. We know that this modification, by design, causes an incremental effect on ad spend. What we don&#39;t know is whether it also causes an incremental effect in the response metric. It is worth noting that modified campaigns may cause the ad spend to increase (e.g., by adding keywords or increasing bids in the AdWords auction) or decrease (e.g., by turning campaigns off). Either way, we typically expect the response metric to be affected in the same direction as spend. As a result, the iROAS is typically positive.&lt;br /&gt;&lt;br /&gt;After the test period finishes, the campaigns in the treatment group are reset to their original configurations. This doesn&#39;t always mean their causal effects will cease instantly. Incremental offline sales, for example, may well be delayed by days or even weeks. When studying delayed metrics, we may therefore want to include in the analysis data from an additional &lt;i&gt;cool-down&lt;/i&gt; period to capture delayed effects.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Designing a geo experiment: power analysis&lt;/h2&gt;As with any experiment, it is essential that a geo experiment is designed to have a high probability of being successful. This is what we mean when we estimate the power of the experiment: the probability of detecting an effect if an effect of a particular magnitude is truly present.&lt;br /&gt;&lt;br /&gt;Statistical power is traditionally given in terms of a probability function, but often a more intuitive way of describing power is by stating the expected &lt;i&gt;precision&lt;/i&gt; of our estimates. We define this as the standard error of the estimate times the multiplier to obtain the bounds of a confidence interval. This could, for example, be stated as the “iROAS point estimate +/- 1.0 for a 95% confidence interval”. This is a quantity that is easily interpretable and summarizes nicely the statistical power of the experiment.&lt;br /&gt;&lt;br /&gt;The expected precision of our inferences can be computed by simulating possible experimental outcomes. We also check that the false positive rate (i.e., the probability of obtaining a statistically significant result if the true iROAS is in fact zero) is acceptable, such as 5% or 10%. This power analysis is absolutely essential in the design phase as the amount of proposed ad spend change directly contributes to the precision of the outcome. We can therefore determine whether a suggested ad spend change is sufficient for the experiment to be feasible.&lt;br /&gt;&lt;br /&gt;One of the factors determining the standard error (and therefore, the precision) of our causal estimators is the amount of noise in the response variable. The noisier the data, the higher the standard error. On the other hand, for the models that we use, the standard error of the iROAS estimate is inversely proportional to the ad spend difference in the treatment group. That is, we can buy ourselves shorter confidence intervals (and a  “higher precision”) by increasing the ad spend difference.&lt;br /&gt;&lt;br /&gt;In practice, however, increasing precision is not always as easy as increasing spend. There might simply not be enough available &lt;i&gt;inventory&lt;/i&gt; (such as ad impressions, clicks, or YouTube views) to increase spend. Further, there is the risk that the increased ad spend will be less productive due to diminishing returns (e.g., the first 100 keywords in a campaign will be more efficient than the next 100 keywords.)&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h2&gt;A model for assessing incremental return on ad spend&lt;/h2&gt;&lt;/div&gt;&lt;div&gt;We are interested in estimating the iROAS, which for each geo is the ratio between the incremental (causal) revenue divided by the incremental change in expenditure in that geo. The incremental effect is defined as the difference between the expected potential outcome under treatment and the potential outcome under no treatment.&lt;br /&gt;&lt;br /&gt;We estimate the causal treatment effect using linear regression (see for example [3], chapter 9).  The model regresses the outcomes $y_{1,i}$ on the incremental change in ad spend $\delta_i$. We can however increase the precision of the estimates (and therefore the statistical power) by also including the pre-test response $y_{0,i}$ as a covariate:&lt;br /&gt;&lt;br /&gt;&lt;div style=&quot;text-align: center;&quot;&gt;$y_{1,i} = \beta_0 + \beta_1 y_{0,i} + \beta_2 \delta_i + \epsilon_i$ &amp;nbsp;for geo $i$&lt;/div&gt;&lt;br /&gt;For all control geos, we have $\delta_i = 0$ by design. For a treatment geo $i$, $\delta_i$ is the observed ad spend minus the counterfactual, that is, the ad spend that would have been observed in the absence of the treatment. This counterfactual is estimated using a linear regression model applied to control geos only (see [1] for details). If there was no previous ad expenditure (for this particular channel), the counterfactual estimate would simply be zero, and so i would be the observed spend in treatment geo $i$.&lt;br /&gt;&lt;br /&gt;The modelled incremental response caused by the change in ad spend $\delta_i$ in geo $i$ is $\beta_2 \delta_i$. The model parameter $\beta_2$ is our main quantity of interest, the incremental ROAS. For example, $\beta_2 = 3.1$ would indicate that each unit of currency invested caused an extra 3.1 units of currency generated.&lt;br /&gt;&lt;br /&gt;The term $\beta_1 y_{0,i}$ controls for seasonality and other factors common to all geos (e.g., a nationwide sales event), as it is plausible that the pretest and test periods experience a trend or a temporary increase that is unrelated to the effect we are measuring. The interpretation of the coefficient $\beta_1$, of course, depends on the length of the pretest and test periods.&lt;br /&gt;&lt;br /&gt;The geos are invariably of different sizes and therefore the data show considerable heteroscedasticity. Since each geo can be thought of as being constructed from small individual contributions, we assume that the variance of a geo is proportional to its mean. This is a plausible assumption, since the variance of a sum of independent variables is equal to the sum of their individual variances. A variance stabilizing transformation (square root) is an option here to equalize the variances, but we prefer to work on the original scale (as opposed to square-root units), as this is more convenient and flexible; for example, the interpretation of the coefficients such as that of $\beta_2$ (iROAS) is straightforward. We have seen in practice that fitting the model using weighted regression with weights $1 / y_{0,i}$ (inverse of the sum of the response variable in the pretest period) controls heteroskedasticity. Without weighted regression we would obtain a biased estimate of the variance. Caution is needed, however, to use the weights: when the pre-test period volume of a geo are close to zero, the weights may be large (this usually reflects an issue with data reporting). A quick remedy is to combine the smallest geos to form a new larger geo.&lt;br /&gt;&lt;br /&gt;The paper [1] gives more details on the model; the follow-up paper [2] describes an extension of the methodology to multi-period geo experiments.&lt;/div&gt;&lt;div&gt;&lt;h2&gt;Example&lt;/h2&gt;Here is a set of daily time series data of sales by geo. As we will see further below, this period will form our pre-period for a geo experiment. There are 100 geos; the largest one has a volume that is 342 times that of the smallest one. Such differences in scale are not unusual. The time series shows significant weekly level seasonality, with the lowest volumes occurring during weekends.&lt;br /&gt;&lt;br /&gt;&lt;div style=&quot;text-align: center;&quot;&gt;&lt;img height=&quot;320&quot; src=&quot;https://lh4.googleusercontent.com/WI_fmiPkskgdUBoptjaBLqm59pICYevDP3hszam45UpCxq3cnFf-lCs1zFEFFzRg1UXdgWa-SFxzIUD6UCZ0QDstzI1G954_mn0alPg0xVPuVU18YbM9Tu35PvDE8Ucz1emDplxj&quot; width=&quot;640&quot; /&gt;&lt;/div&gt;&lt;br /&gt;It is usually helpful to look at these response variables on a log scale. This really shows how similarly the geos behave, the only difference being their size.&lt;br /&gt;&lt;br /&gt;&lt;div style=&quot;text-align: center;&quot;&gt;&lt;img height=&quot;320&quot; src=&quot;https://lh4.googleusercontent.com/AUs1YQ4Rfw-KuE2SN1IEgkFlbPSwxmKoGKRZ5gepmvUmhMPqtG_uB49Az3VmDYbhqJQ8f81NieYZANz_5WhyQQEtyrIaptbb-FSuM2YC3PpRoyLpCOr8s2eyjAlBmos0WYCZ5zyW&quot; width=&quot;640&quot; /&gt;&lt;/div&gt;&lt;br /&gt;The goal of the study was to estimate the incremental return in sales on an additional ad spend change. The 100 geos were randomly assigned to control and treatment groups, and a geo experiment test period was set up for February 16&amp;nbsp;&lt;span style=&quot;font-family: &amp;quot;arial&amp;quot;; font-size: 14.6667px; white-space: pre-wrap;&quot;&gt;–&lt;/span&gt;&amp;nbsp;March 15, 2015, with the 6 previous weeks serving as the pre-period. During this test period, advertising spend was increased in the treatment group.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;div style=&quot;text-align: center;&quot;&gt;&lt;img height=&quot;320&quot; src=&quot;https://lh5.googleusercontent.com/m8_WAvXI28N6oI30U6XXNKC0jbhIIOiQpwkzGkOiPJHUL6neWTtbvs9r9zRHl4I9rjLTMvmUDFl3uxlKoCqqmpGayEjM1VAa3OKYgXt0kqsnz16TCZDs0SGqi1cObKPkY-KEHyN5&quot; width=&quot;640&quot; /&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;After the experiment was finished, the incremental return of ad spend was estimated to be $\beta_2 = 3.1$. The other parameters were $\beta_0 = 74$, and $\beta_1= 0.85$ with a residual standard deviation of 4.8.&amp;nbsp;Since the pre-test period is 6 weeks long and the test period is 4 weeks long, a value of $\beta_1 = 0.85$ corresponds to a weekly trend of $(6/4) \times 0.85 = 1.27$, that is, on average each week the response volumes of the geos tend to increase by 27% (in the absence of any intervention).&amp;nbsp;In practice, the focus of the team is however on the estimate of $\beta_2$, not to forget about the uncertainty around this estimate: the confidence interval half-width was estimated to be 0.27.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;h2&gt;Caveats&lt;/h2&gt;&lt;h3&gt;Model&lt;/h3&gt;As with any statistical model, the model is designed to operate under certain assumptions. It is important that the technical assumptions of a linear regression model are satisfied: linearity, additivity, independence of errors, normality, and equal variance of errors (after taking into account the weighting). If the response data indeed consists of sums of small independent contributions (say, sales of products) that are grouped into geos, the differences between geos should be normally distributed. A more important assumption is that the relative volume of the geos will not be changing during the geo experiment. &lt;br /&gt;&lt;br /&gt;It is critical to apply appropriate diagnostics to the data and to the model fit before and after the test. In practice, anomalies often indicate issues with data reporting: untidy data and programming errors are a frequent source for headaches that are best addressed by developing tools that verify the integrity of the data and that check all model assumptions. For example, we have developed a few diagnostics tools of our own to catch unexpected features in the data such as outliers.&lt;br /&gt;&lt;br /&gt;The model that we presented here is quite simple. It is then worth asking whether the model accounts for all of the factors that could influence the response. As usual, the honest answer is: probably not. One obvious omission is that the model presented ignores other possible predictors such as pricing and promotion information. And such information may not even be available. In the case of consumer packaged goods such as soft drinks or shampoo, for example, even manufacturers may not be aware of all the promotions going in each of the stores selling their products. Thanks to randomization, this omission will not bias our iROAS estimate. However, it will typically increase its variance.&lt;/div&gt;&lt;h3&gt;Forming treatment groups&lt;/h3&gt;&lt;div&gt;By randomizing, we aim to generate the treatment and control groups with comparable baseline characteristics. By complete randomization, we may well end up with groups that are not as balanced as we would prefer. By default, we use stratified randomization with each stratum containing geos of similar size. It may also be a good idea to consider forming strata by other characteristics such as geographical location.&lt;br /&gt;&lt;br /&gt;Even stratified randomization may not be enough: the problem of balance may be especially accentuated when we have few geos to work with (for example, in small countries). Highly heterogeneous countries often have their own set of challenges. For example, the metropolitan areas of London and Paris dominate entire countries like the UK or France. This makes it virtually impossible to partition such countries into homogeneous, well-balanced groups.&lt;br /&gt;&lt;br /&gt;For this reason, in these particular situations we need an algorithm that matches geos to form control and treatment groups that are predictive of each other, justifying a causal interpretation of the analysis.&lt;/div&gt;&lt;div&gt;&lt;h3&gt;Outliers&lt;/h3&gt;A particular nuisance is a geo that –&amp;nbsp;for a known or unknown reason –&amp;nbsp;experiences a shock (unexpected drop or surge) in the response variable during the test. For example, if a large geo in the control group suddenly experiences a heat wave, a geo experiment to measure the incremental sales of a particular ice cream brand might be affected, diluting the effect that we are aiming to measure. If the shock is balanced by another geo in the other group, this problem is fortunately mitigated (as it should be in a randomized experiment). Otherwise we will have to deal with the outlier geo(s) after the experiment. Hence we need to be aware of these problems and develop complementary methodologies to deal with them. &lt;br /&gt;&lt;br /&gt;It is a good idea to do a sensitivity analysis to check if there is any cause for concern. For example, one useful diagnostic to detect a single outlier geo is to look at the effect of each geo on the iROAS point estimate. In this &lt;i&gt;leave-one-out analysis&lt;/i&gt; we repeat the analysis as many times as there are geos, each time dropping one geo from the analysis. By looking at a histogram of the estimates, we may see that dropping a particular geo may have a clear effect on the iROAS estimate.&lt;/div&gt;&lt;div&gt;&lt;h3&gt;Small market regions&lt;/h3&gt;Since the statistical power of this model depends on the number of geos, it is not obviously suitable for situations in which the number of geos is very small. In the U.S., for national level experiments we have no problem as we have over 210 geos. In Europe, use of this methodology may not be always advisable; as a rough rule of thumb, we prefer to apply this approach to experiments with 30, or more, geos. If this is not the case, alternative approaches relying on time-series models, such as &lt;a href=&quot;http://www.unofficialgoogledatascience.com/2015/09/causal-attribution-in-era-of-big-time.html&quot;&gt;CausalImpact&lt;/a&gt;, may be a better choice.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h2&gt;What’s next?&lt;/h2&gt;We mentioned several challenges: ensuring the model is appropriate, lack of useful data, forming comparable treatment and control groups, dominant geos, outliers, and running experiments in small countries. For a data scientist, all this means more interesting and applicable research opportunities.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h2&gt;References&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;Jon Vaver and Jim Koehler, &lt;a href=&quot;http://services.google.com/fh/files/blogs/geo_experiments_final_version.pdf&quot;&gt;Measuring Ad Effectiveness Using Geo Experiments&lt;/a&gt;, 2011.&lt;/li&gt;&lt;li&gt;Jon Vaver and Jim Koehler, &lt;a href=&quot;http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/38356.pdf&quot;&gt;Periodic Measurement of Advertising Effectiveness Using Multiple-Test-Period Geo Experiments&lt;/a&gt;, 2012.&lt;/li&gt;&lt;li&gt;Andrew Gelman and Jennifer Hill, Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge, 2007.&lt;/li&gt;&lt;/ol&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content><link rel='edit' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/3214828694374915136'/><link rel='self' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/3214828694374915136'/><link rel='alternate' type='text/html' href='https://www.unofficialgoogledatascience.com/2016/06/estimating-causal-effects-using-geo.html' title='Estimating causal effects using geo experiments'/><author><name>Kay Brodersen</name><uri>http://www.blogger.com/profile/15877220456041691465</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://2.bp.blogspot.com/-G6EoofJH1Iw/V1U2cA_qORI/AAAAAAAADtc/qxRTUzLogGI8SCRElmW8aKUNMfNcU1cFwCK4B/s72-c/usdma.png" height="72" width="72"/></entry><entry><id>tag:blogger.com,1999:blog-5825758052688213474.post-7237562292052207230</id><published>2016-03-31T22:57:00.000-07:00</published><updated>2016-06-06T01:28:41.675-07:00</updated><title type='text'>Using random effects models in prediction problems</title><content type='html'>by NICHOLAS A. JOHNSON, ALAN ZHAO, KAI YANG, SHENG WU, FRANK O. KUEHNEL, and ALI NASIRI AMINI&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;i&gt;In this post, we give a brief introduction to random effects models, and discuss some of their uses. Through simulation we illustrate issues with model fitting techniques that depend on matrix factorization. Far from hypothetical, we have encountered these issues in our experiences with &quot;big data&quot; prediction problems. Finally, through a case study of a real-world prediction problem, we also argue that Random Effect models should be considered alongside penalized GLM&#39;s even for pure prediction problems.&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;Random effects models are a useful tool for both exploratory analyses and prediction problems. We often use statistical models to summarize the variation in our data, and random effects models are well suited for this — they are a form of ANOVA after all. In prediction problems these models can summarize the variation in the response, and in the process produce a form of adaptive regularization through the learned prior distributions. Random effects can be viewed as an application of empirical Bayes, and if we broaden our view to that parent technique, we can find theoretical evidence of their superiority over say fixed effects models. For example, the James-Stein estimator has an empirical Bayes interpretation — it was famously proven to dominate the MLE (an analogue of fixed effects) for the problem of estimating the mean of a multivariate normal distribution (see [3] and [4]).&lt;br /&gt;&lt;br /&gt;In the context of prediction problems, another benefit is that the models produce an estimate of the uncertainty in their predictions: the predictive posterior distribution. These predictive posterior distributions have many uses such as in &lt;a href=&quot;https://en.wikipedia.org/wiki/Multi-armed_bandit&quot;&gt;multi-armed bandit&lt;/a&gt; problems. &lt;a href=&quot;https://en.wikipedia.org/wiki/Thompson_sampling&quot;&gt;Thompson sampling&lt;/a&gt; is an &quot;explore/exploit&quot; algorithm which was designed for Bayesian models, and its strategy is to randomly select a choice (called an &quot;arm&quot;) with probability proportional to the posterior of that choice being the best option. See [9], [10] for a discussion of this approach.&lt;br /&gt;&lt;img border=&quot;0&quot; height=&quot;1&quot; src=&quot;https://4.bp.blogspot.com/-Ej4soocavso/Vv8Lj-3baJI/AAAAAAAAa9w/AHwu7IliFkY0xcd5UJErFd_GsGPejAp9g/s200/vs_lme4_timing_20160401_v2.png&quot; width=&quot;1&quot; /&gt; &lt;br /&gt;In the next two sections we give a brief introduction to random effects models and then discuss a case study in click-through-rate modeling.  This example shows that the prediction accuracy can be competitive with popular alternatives such as penalized GLM&#39;s — in fact, much better in this example.  Furthermore the algorithms to fit these models are sufficiently scalable to handle many problems of interest.&lt;br /&gt;&lt;br /&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;/div&gt;&lt;br /&gt;&lt;h2&gt;Random Effect Models&lt;/h2&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;We will start by describing a Gaussian regression model with known residual variance $\sigma_j^2$ of the $j$th training record&#39;s response, $y_j$.  Often our data can be stored or visualized as a table like the one shown below.  In this example we have three features/columns named &quot;a&quot;, &quot;b&quot;, and &quot;c&quot;.  Column &quot;a&quot; is an advertiser id, &quot;b&quot; is a web site, and &quot;c&quot; is the &#39;interaction&#39; of columns &quot;a&quot; and &quot;b&quot;.&lt;/div&gt;&lt;div&gt;&lt;span id=&quot;docs-internal-guid-ff67289e-cf3b-b48d-6f75-817bdeb67a58&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;margin-left: 0pt;&quot;&gt;&lt;table style=&quot;border-collapse: collapse; border: none; width: 624px;&quot;&gt;&lt;colgroup&gt;&lt;col width=&quot;80&quot;&gt;&lt;/col&gt;&lt;col width=&quot;147&quot;&gt;&lt;/col&gt;&lt;col width=&quot;107&quot;&gt;&lt;/col&gt;&lt;col width=&quot;164&quot;&gt;&lt;/col&gt;&lt;col width=&quot;*&quot;&gt;&lt;/col&gt;&lt;/colgroup&gt;&lt;tbody&gt;&lt;tr style=&quot;height: 0px;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif;&quot;&gt;&lt;span style=&quot;background-color: white; white-space: pre-wrap;&quot;&gt;$y$&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif;&quot;&gt;feature (a)&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif;&quot;&gt;feature (b)&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif;&quot;&gt;feature (c)&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.68; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;times&amp;quot; , &amp;quot;times new roman&amp;quot; , serif;&quot;&gt;$\sigma^2$&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0px;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif;&quot;&gt;1.2&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; color: #222222; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif;&quot;&gt;hi-fly-airlines&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif;&quot;&gt;123.com&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif; font-size: x-small;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&quot;&lt;/span&gt;&lt;span style=&quot;background-color: white; color: #222222; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;hi-fly-airlines&lt;/span&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;__123.com&quot;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif;&quot;&gt;1.10&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0px;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif;&quot;&gt;0.1&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif; font-size: x-small;&quot;&gt;erudite-bookstore&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif;&quot;&gt;123.com&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif; font-size: x-small;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&quot;erudite-bookstore&lt;/span&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;__123&lt;/span&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;.com&quot;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif;&quot;&gt;0.15&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0px;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif;&quot;&gt;0.3&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; color: #222222; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif;&quot;&gt;hi-fly-airlines&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif;&quot;&gt;xyz.com&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif; font-size: x-small;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&quot;&lt;/span&gt;&lt;span style=&quot;background-color: white; color: #222222; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;hi-fly-airlines&lt;/span&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;__xyz.com&quot;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif;&quot;&gt;0.20&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0px;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif;&quot;&gt;0.3&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; color: #222222; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif; font-size: x-small;&quot;&gt;lightspeed-brokerage&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif;&quot;&gt;www.com&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif; font-size: x-small;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&quot;&lt;/span&gt;&lt;span style=&quot;background-color: white; color: #222222; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;lightspeed-brokerage&lt;/span&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;__www.com&quot;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif;&quot;&gt;0.10&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;&lt;span id=&quot;docs-internal-guid-ff67289e-cf3b-b48d-6f75-817bdeb67a58&quot;&gt;&lt;/span&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;margin-left: 0pt;&quot;&gt;&lt;span id=&quot;docs-internal-guid-ff67289e-cf3b-b48d-6f75-817bdeb67a58&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;span id=&quot;docs-internal-guid-ff67289e-cf3b-b48d-6f75-817bdeb67a58&quot;&gt;To make mathematical notation simpler we suppose that string values in each column have been enumerated, and we deal with their integer index instead:&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;margin-left: 0pt;&quot;&gt;&lt;span id=&quot;docs-internal-guid-ff67289e-cf40-5c5c-e937-f5ba6c33a11b&quot;&gt;&lt;/span&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;margin-left: 0pt;&quot;&gt;&lt;table style=&quot;border-collapse: collapse; border: none; width: 624px;&quot;&gt;&lt;colgroup&gt;&lt;col width=&quot;80&quot;&gt;&lt;/col&gt;&lt;col width=&quot;143&quot;&gt;&lt;/col&gt;&lt;col width=&quot;129&quot;&gt;&lt;/col&gt;&lt;col width=&quot;146&quot;&gt;&lt;/col&gt;&lt;col width=&quot;*&quot;&gt;&lt;/col&gt;&lt;/colgroup&gt;&lt;tbody&gt;&lt;tr style=&quot;height: 0px;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;times&amp;quot; , &amp;quot;times new roman&amp;quot; , serif;&quot;&gt;$y$&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;times&amp;quot; , &amp;quot;times new roman&amp;quot; , serif;&quot;&gt;$a$&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;times&amp;quot; , &amp;quot;times new roman&amp;quot; , serif;&quot;&gt;$b$&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;times&amp;quot; , &amp;quot;times new roman&amp;quot; , serif;&quot;&gt;$c$&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.68; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;times&amp;quot; , &amp;quot;times new roman&amp;quot; , serif;&quot;&gt;$\sigma^2$&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0px;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif;&quot;&gt;1.2&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif;&quot;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif;&quot;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif;&quot;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif;&quot;&gt;1.10&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0px;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif;&quot;&gt;0.1&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif;&quot;&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif;&quot;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif;&quot;&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif;&quot;&gt;0.15&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0px;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif;&quot;&gt;0.3&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif;&quot;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif;&quot;&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif;&quot;&gt;3&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif;&quot;&gt;0.20&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;height: 0px;&quot;&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif;&quot;&gt;0.3&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif;&quot;&gt;3&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif;&quot;&gt;3&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif;&quot;&gt;4&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=&quot;border-bottom: solid #000000 1px; border-left: solid #000000 1px; border-right: solid #000000 1px; border-top: solid #000000 1px; padding: 7px 7px 7px 7px; vertical-align: top;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;&quot;&gt;&lt;span style=&quot;background-color: white; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span style=&quot;font-family: &amp;quot;trebuchet ms&amp;quot; , sans-serif;&quot;&gt;0.10&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;&lt;span id=&quot;docs-internal-guid-ff67289e-cf40-5c5c-e937-f5ba6c33a11b&quot;&gt;&lt;br /&gt;Now for the $j$th row of data we define the index of the feature &quot;a&quot; as&amp;nbsp;&lt;/span&gt;$I_a(j) \in \{1, 2, ..., \mbox{#advertisers} \}$. In our example we have \begin{align*}&lt;br /&gt;I_a(1) &amp;amp;= 1\\&lt;br /&gt;I_a(2) &amp;amp;= 2 \\&lt;br /&gt;I_a(3) &amp;amp;= 1 \\&lt;br /&gt;I_a(4) &amp;amp;=3 \\&lt;br /&gt;\cdots&lt;br /&gt;\end{align*}Similarly $I_b(j) = \{1, 2, ..., \mbox{#websites}\}$ could be the index of the site on which the ad was shown and $I_c(j)$ could index the (advertiser, site) pair.&lt;/div&gt;&lt;br /&gt;Finally we get to our model of the data: \[&lt;br /&gt;\{y_j | I_a(j), I_b(j), I_c(j), \sigma_j^2 \} \sim&lt;br /&gt;N(\beta + \mu_{I_a(j)}^a +  \mu_{I_b(j)}^b +  \mu_{I_c(j)}^c, \sigma_j^2)&lt;br /&gt;\]The curly-bracketed left-hand side, $\{y_j| \cdots \}$, denotes a conditional distribution; the parameter $\beta$ represents the overall mean of the data (whose fitted value depends on the structure of the model); and the $\mu^a$&#39;s, $\mu^b$&#39;s, and $\mu^c$&#39;s represent random effects i.e. unobserved random variables.&lt;br /&gt;&lt;br /&gt;The model is complete once we specify a prior distribution on each of the $\mu$&#39;s:&amp;nbsp;\begin{align*}&lt;br /&gt;\mu_k^a &amp;amp;\sim N(0, \tau_a^{-1}) \\&lt;br /&gt;\mu_k^b &amp;amp;\sim N(0, \tau_b^{-1}) \\&lt;br /&gt;\mu_k^c &amp;amp;\sim N(0, \tau_c^{-1})&lt;br /&gt;\end{align*}In the formulas above the parameter $\tau_x$ is the inverse-variance of the prior, and all coefficients associated with the same column are drawn from a common prior.  Rather than just specifying the $\tau$&#39;s, we will try to infer them from the data: they can be fitted by a Monte Carlo Expectation Maximization (MCEM) algorithm or they can be sampled in a Bayesian model.&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;margin-left: 0pt;&quot;&gt;&lt;br /&gt;These inferred prior distributions give a decomposition of the variance of the data\[&lt;br /&gt;&amp;nbsp;\mathrm{var\ }y|\sigma = \sigma^2 + \tau_a^{-1}&amp;nbsp;+ \tau_b^{-1}&amp;nbsp;+ \tau_c^{-1}&lt;br /&gt;\]The variance decomposition formula also gives breakdowns such as\begin{align}&lt;br /&gt;\mathrm{var\ }y|\sigma &amp;amp;= E_\sigma[[\mathrm{var\ } y|\sigma, I_b, I_c]| \sigma]&lt;br /&gt;&amp;nbsp;+ \mathrm{var_\sigma}[E[y|\sigma, I_b, I_c]| \sigma] \\&lt;br /&gt;&amp;amp;= (\sigma^2+ \tau_a^{-1}) + (\tau_b^{-1} + \tau_c^{-1})&lt;br /&gt;\end{align}Their interpretation is most intuitive in hierarchical models.  More complex models can involve elements such as &quot;random slopes&quot;, and for these it can be informative to calculate a version of $R^2$ (fraction of variance explained) which was developed for random effects models in the textbook by Gelman and Hill [1].&lt;br /&gt;&lt;br /&gt;The prior variance decomposition or $R^2$ are useful summaries of the explanatory power of each random effect column, but when making predictions we need the posterior variances of individual random effects.  At prediction time suppose we have features (a=3, b=7, c=2) and we need the linear combination $\beta + \mu_3^a + \mu_7^b + \mu_2^c$.  By considering several MCMC samples (indexed $t=1,2, \dots, T$) we can compute statistics based on the sample approximation to the posterior distribution of the prediction i.e. $\{\beta_t + \mu_{3,t}^a + \mu_{7,t}^b + \mu_{2,t}^c \}_{t=1}^T$.  If we only need the posterior mean then we can simply pre-compute the mean of each $\mu$.&lt;br /&gt;&lt;br /&gt;Although Gaussian models are a good way to introduce random effects, in practice we must often model discrete data — usually in the form of counts of events.  Using similar algorithms, we can fit the following Gamma-Poisson model:\begin{align}&lt;br /&gt;\{ y_j | I_a(j), I_b(j), I_c(j), \lambda_j \}&lt;br /&gt;&amp;amp;\sim \mathrm{Pois}(\lambda_j \mathrm{e}^&lt;br /&gt;{\beta +\mu_{I_a(j)}^a + \mu_{I_b(j)}^b + \mu_{I_c(j)}^c}) \\&lt;br /&gt;\mathrm{e}^{\mu_k^x} &amp;amp;\sim \Gamma(\tau_x, \tau_x)&lt;br /&gt;\end{align}where the prior has mean of $1.0$ and variance $\tau_x^{-1}$.&lt;br /&gt;&lt;br /&gt;Binomial models are also of interest but logistic regression in particular is similar to the Poisson regression when the rate of positive labels is low.  In many applications the positive rate is indeed very low, and we then prefer the Poisson model for the computational efficiency of being able to use sums as sufficient statistics [7].&lt;br /&gt;&lt;br /&gt;When we only need\[&lt;br /&gt;\log E[y|\mu, \beta, I_a, I_b, I_c, \sigma] =&lt;br /&gt;&amp;nbsp; &amp;nbsp; \beta +\mu_{I_a(j)}^a + \mu_{I_b(j)}^b + \mu_{I_c(j)}^c&lt;br /&gt;\]in the Poisson model, then it is simple to pre-calculate the mean of each parameter — just as in the Gaussian model.  On the other hand, if we required the posterior mean of $\mathrm{exp}(\beta +\mu_{I_a(j)}^a + \mu_{I_b(j)}^b + \mu_{I_c(j)}^c)$ (i.e. the mean of $y_j$), or if we need a variance estimate, then we must store multiple MCMC samples of every $\mu$.  This can represent a challenge since we must now store and manipulate many times more parameters; however, there is recent work to develop more compact summaries of the posterior mean [2], [5].  We look forward to further developments in this area.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;A Case Study Click-Through-Rate Prediction&lt;/h2&gt;&lt;br /&gt;In this section we evaluate the prediction accuracy of random effects models in click-through-rate modeling on several data sets — each corresponding to 28 days of traffic belonging to single display ads segment.&lt;br /&gt;&lt;br /&gt;We selected 10 segments and chose a subset of training data such that the number of training examples varied from between 20 million to 90 million for each segment.  We compared the output of a random effects model to a penalized GLM solver with &quot;Elastic Net&quot; regularization (i.e. both L1 and L2 penalties; see [8]) which were tuned for test set accuracy (log likelihood).&lt;br /&gt;&lt;br /&gt;On each of the ten segments the random effects model yielded higher test-set log likelihoods and AUCs, and we display the results in the figure below.  In that figure we have taken single future day of data as the test set i.e. we trained on data from days 1, ..., 28 and evaluated on day 29.  We saw similar results when looking further ahead and evaluating on days 29 through 34.&lt;br /&gt;&lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;http://2.bp.blogspot.com/-5Z-sCWwu2TU/Vv7NXfEUr7I/AAAAAAAAa9c/O1fOiR5dxlIm8YuNkpgGWLsfy0lfQmY3w/s1600/per_segment_acc_20160401.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;254&quot; src=&quot;https://2.bp.blogspot.com/-5Z-sCWwu2TU/Vv7NXfEUr7I/AAAAAAAAa9c/O1fOiR5dxlIm8YuNkpgGWLsfy0lfQmY3w/s640/per_segment_acc_20160401.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Figure 1: Comparing Random Effects vs. Penalized GLM on AUC and log likelihood.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;Through this case study we support the argument that practitioners should evaluate random effects models when they encounter a new problem.  Often one tries several different techniques and then either combines the outputs or selects the most accurate, and we believe random effects models are a valuable addition to the usual suite of approaches which includes penalized GLMs, decision trees, etc.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Scalability studies&lt;/h2&gt;&lt;br /&gt;First we evaluate the popular &lt;b&gt;lme4 R package&lt;/b&gt; and compare against a specialized Gibbs sampler which we describe in [7].  The lme4 software is an excellent choice for many datasets; however, to scale to large, multi-factor datasets we found it necessary to turn to alternative algorithms such as the Gibbs sampler.  These algorithms can be distributed and deal with problems which would be RAM-limited for lme4.&lt;br /&gt;&lt;br /&gt;This comparison is meant to show that the relative run times greatly depend on the structure of the random effect model matrix.  There are designs for which lme4 scales linearly in the number of columns, and there are also non-pathological designs for which it appears to be quadratic or worse.&lt;br /&gt;&lt;br /&gt;The figure below shows the run time for a Poisson lme4 model (dashed) and for 1000 iterations of a Gibbs sampler in the case of nested feature columns (left) and non-nested (right).  By &#39;nested&#39; we mean that there is hierarchical relationship so the level of a parent column is a function of the level of the child column.  For example, consider a domain and the url&#39;s within it.&lt;br /&gt;&lt;br /&gt;The left figure shows that the run time of both methods grows linearly with the number of input records and random effects (we simulated such that the number of random effects at the finest level was approximately equal to the number of input records divided by five).  The right figure shows that the run time of one scan of the Gibbs sampler has about the same cost per iteration whether the structure is nested or not (as expected); however, for lme4 the cost of fitting the model increases dramatically as we add random effect columns.&lt;br /&gt;&lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;http://4.bp.blogspot.com/-Ej4soocavso/Vv8Lj-3baJI/AAAAAAAAa9w/AHwu7IliFkY0xcd5UJErFd_GsGPejAp9g/s1600/vs_lme4_timing_20160401_v2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;262&quot; src=&quot;https://4.bp.blogspot.com/-Ej4soocavso/Vv8Lj-3baJI/AAAAAAAAa9w/AHwu7IliFkY0xcd5UJErFd_GsGPejAp9g/s640/vs_lme4_timing_20160401_v2.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Figure 2: Comparing custom Gibbs sampler vs. lmer running times.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;We have many routine analyses for which the sparsity pattern is closer to the nested case and lme4 scales very well; however, our prediction models tend to have input data that looks like the simulation on the right.  For example, we may have features that describe or identify an advertiser, other features for the web site on which an ad shows, and yet more features describing the device e.g. the type and model of device: computer, phone, tablet, iPhone5, iPhone6, ...; the operating system; the browser.&lt;br /&gt;&lt;br /&gt;In the simulations we plotted the run time for 1,000 scans of the Gibbs sampler.  In these simulations this number is far more than necessary — based on manual checks of the convergence of prior parameters and log likelihood.  We chose 1,000 iterations in order to put the run time on comparable scale with lme4.  This makes it easier to contrast how the two approaches handle more complex model structures i.e. more random effect columns.&lt;br /&gt;&lt;br /&gt;One of the known challenges of using MCMC methods is deciding how many iterations to run the algorithm for.  We recommend monitoring the prior variance parameters and the log likelihood of the data.  More sophisticated checks have been proposed, and a good review of convergence diagnostics is found in [6].  Many of these techniques appear infeasible for the large regression models we are interested in, and we are still searching for checks that are easy to implement and reliable for high-dimensional models.&lt;br /&gt;&lt;br /&gt;In the previous section we described per-segment models, and we now use the same datasets to illustrate the scalability of the algorithm.  We selected two segments as a case study and varied the number of machines used in inference.  The chosen segments had between 30 million and 90 million training records and a positive rate of about 1%.  The models contained approximately 190 feature columns and the number of levels per column varied from between just two at the smallest to 5 million at the largest (with a median of about 15,000 levels per column).  Finally, we should note that a training record typically represents a single ad-view but we can collapse those with identical features (all 190 of them in this case).  In these examples aggregation only reduced the size of the data set by about 10%. &lt;br /&gt;&lt;br /&gt;In the figure below we show how the run time per iteration (updating all 190 feature columns) varied with the number of machines used.  For this timing test we also evaluated the run time of the model when applied to two combined datasets from all ten segments and another from thirty segments.  These large timing tests had roughly 500 million and 800 million training examples respectively.  The combined raw training data for the smaller one was 834 GB in protocol buffer format and after integerization and column formatting (roughly how the data is stored in RAM) the training data was roughly 170 GB in size on disk.  The larger dataset was around 300 GB after integerization and column-formatting.&lt;br /&gt;&lt;br /&gt;In the right panel of the figure below we can see that the run time is growing sub-linearly in the amount of training data i.e. doubling the number of training instances does not double the time per iteration.&lt;br /&gt;&lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;http://1.bp.blogspot.com/-dpe131BRhS0/VwNQQSjC1rI/AAAAAAAAa-U/xNUSQOA9xY8qjMC19nvEjflXFvHH3Ae-w/s1600/per_segment_timing_20160401_v2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;251&quot; src=&quot;https://1.bp.blogspot.com/-dpe131BRhS0/VwNQQSjC1rI/AAAAAAAAa-U/xNUSQOA9xY8qjMC19nvEjflXFvHH3Ae-w/s640/per_segment_timing_20160401_v2.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;&lt;span style=&quot;font-size: 12.8px;&quot;&gt;Figure 3: Performance scalability of custom Gibbs sampler.&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Conclusion and Discussion&lt;/h2&gt;&lt;br /&gt;We have found random effects models to be useful tools for both exploratory analyses and prediction problems.  They provide an interpretable decomposition of variance, and in prediction problems they can supply predictive posterior distributions that can be used in stochastic optimization when uncertainty estimates are a critical component (e.g. bandit problems).&lt;br /&gt;&lt;br /&gt;Our study of per-segment click-through-rate models demonstrated that random effects models can deliver superior prediction accuracy.  Although this may not hold in every application, we encourage others to evaluate random effects models when selecting a methodology for a new prediction problem.  In our case study we considered models with a couple hundred feature columns and hundreds of millions of inputs.  This is for illustration and does not represent an upper bound on the problem size; we have applied these models to data sets with billions of inputs (not shown), and we are confident that the techniques are scalable enough to cover many problems of interest.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;References&lt;/h2&gt;&lt;br /&gt;[1] Andrew Gelman, and Jennifer Hill. &quot;Data analysis using regression and multilevel/hierarchical models.&quot; Cambridge University Press, (2006).&lt;br /&gt;&lt;br /&gt;[2] Edward Snelson and Zoubin Ghahramani. &quot;Compact approximations to bayesian predictive distributions.&quot; ICML, (2005).&lt;br /&gt;&lt;br /&gt;[3] Bradley Efron. &quot;Large-scale inference: empirical Bayes methods for estimation, testing, and prediction.&quot;  Vol. 1. Cambridge University Press, (2012).&lt;br /&gt;&lt;br /&gt;[4] Bradley Efron, and Carl Morris. &quot;Stein&#39;s estimation rule and its competitors—an empirical Bayes approach.&quot; Journal of the American Statistical Association 68.341 (1973): 117-130.&lt;br /&gt;&lt;br /&gt;[5] Anoop Korattikara, et al. &quot;Bayesian dark knowledge.&quot; arXiv preprint arXiv:1506.04416 (2015).&lt;br /&gt;&lt;br /&gt;[6] Mary Kathryn Cowles and Bradley P. Carlin. &quot;Markov Chain Monte Carlo Convergence Diagnostics: A Comparative Review&quot;. Journal of the American Statistical Association, Vol. 91, No. 434 (1996): 883-904.&lt;br /&gt;&lt;br /&gt;[7] Nicholas A. Johnson, Frank O. Kuehnel, Ali Nasiri Amini. &quot;A Scalable Blocked Gibbs Sampling Algorithm For Gaussian And Poisson Regression Models.&quot; arXiv preprint arXiv:1602.00047, (2016).&lt;br /&gt;&lt;br /&gt;[8] Hui Zou, and Trevor Hastie. &quot;Regularization and variable selection via the elastic net.&quot; Journal of the Royal Statistical Society: Series B (Statistical Methodology) 67.2 (2005): 301-320.&lt;br /&gt;&lt;br /&gt;[9] Steven L. Scott. &quot;A modern Bayesian look at the multi-armed bandit.&quot; Applied Stochastic Models in Business and Industry,  26 (2010): 639-658.&lt;br /&gt;&lt;br /&gt;[10] Steven L. Scott. &quot;Multi-armed bandit experiments in the online service economy.&quot; Applied Stochastic Models in Business and Industry, 31 (2015): 37-49.&lt;/div&gt;&lt;/div&gt;</content><link rel='replies' type='application/atom+xml' href='https://www.unofficialgoogledatascience.com/feeds/7237562292052207230/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='https://www.unofficialgoogledatascience.com/2016/03/using-random-effects-models-in.html#comment-form' title='10 Comments'/><link rel='edit' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/7237562292052207230'/><link rel='self' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/7237562292052207230'/><link rel='alternate' type='text/html' href='https://www.unofficialgoogledatascience.com/2016/03/using-random-effects-models-in.html' title='Using random effects models in prediction problems'/><author><name>Amir Najmi</name><uri>http://www.blogger.com/profile/18174523203317227640</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://4.bp.blogspot.com/-Ej4soocavso/Vv8Lj-3baJI/AAAAAAAAa9w/AHwu7IliFkY0xcd5UJErFd_GsGPejAp9g/s72-c/vs_lme4_timing_20160401_v2.png" height="72" width="72"/><thr:total>10</thr:total></entry><entry><id>tag:blogger.com,1999:blog-5825758052688213474.post-2103339658865337794</id><published>2016-02-29T23:27:00.002-08:00</published><updated>2016-03-02T09:10:19.674-08:00</updated><title type='text'>LSOS experiments: how I learned to stop worrying and love the variability</title><content type='html'>&lt;div dir=&quot;ltr&quot; style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;by AMIR NAJMI&lt;span style=&quot;vertical-align: baseline;&quot;&gt;&lt;i style=&quot;vertical-align: baseline;&quot;&gt;&lt;br /&gt;&lt;br /&gt;In the previous post we looked at how large scale online services (LSOS) must contend with the high coefficient of variation (CV) of the observations of particular interest to them. In this post we explore why some standard statistical techniques to reduce variance are often ineffective in this “data-rich, information-poor” realm.&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;Despite a very large number of experimental units, the experiments conducted by LSOS cannot presume statistical significance of all effects they deem practically significant. We previously &lt;a href=&quot;http://www.unofficialgoogledatascience.com/2016/01/variance-and-significance-in-large.html&quot; style=&quot;vertical-align: baseline;&quot;&gt;went into some detail&lt;/a&gt; as to why observations in an LSOS have particularly high coefficient of variation (CV). The result is that experimenters can’t afford to be sloppy about quantifying uncertainty. Estimating confidence intervals with precision and at scale was one of the early wins for statisticians at Google. It has remained an important area of investment for us over the years.&lt;br /&gt;&lt;br /&gt;Given the role played by the variability of the underlying observations, the first instinct of any statistician would be to apply statistical techniques to reduce varia&lt;/span&gt;bility. “We have just emerged from a terrible recession called &lt;i&gt;New Year&#39;s Day&lt;/i&gt; where there were very few market transactions,” joked Hal Varian, Google’s Chief Economist, recently. He was making the point that economists are used to separating the predictable effects of seasonality from the actual signals they’re interested in. Doing so makes it easier to study the effects of an intervention, say, a new marketing campaign, on the sales of a product. When analyzing experiments, statisticians can do something similar to go beyond overall comparisons between treatment and control. In this post we discuss a few approaches to removing the effects of known or predictable variation. These typically result in smaller estimation uncertainty and tighter interval estimates. Surprisingly, however, removing predictable variation isn’t always effective in an LSOS world where observations can have a high CV.&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Variance reduction through conditioning&lt;/h2&gt;&lt;br /&gt;Suppose, as an LSOS experimenter, you find that your key metric varies a lot by country and time of day. There are pretty clear structural reasons for why this could be the case. For instance, the product offering of a personal finance website might be very different in different countries and hence conversion rates (acceptances per offer) might differ considerably by country. Or perhaps the kinds of music requested from an online music service vary a lot with hour of day, in which case the average number of songs downloaded per user session might vary greatly too. Thus any randomized experiment will observe variation in metric difference between treatment and control groups due merely to sampling variation in the number of experimental units in each of these segments of traffic. In statistics, such segments are often called “blocks” or&amp;nbsp;“strata”. At Google, we tend to refer to them as&amp;nbsp;&lt;b&gt;slices&lt;/b&gt;.&lt;br /&gt;&lt;br /&gt;One approach statisticians use to mitigate between-slices variability is to employ&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/Stratified_sampling&quot;&gt;stratified sampling&lt;/a&gt;&amp;nbsp;when assigning units to the treatment and the control group. Stratified sampling means drawing a completely randomized sample from within each slice, ensuring greater overall balance. Figure 1 illustrates how this would work in two dimensions.&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;http://2.bp.blogspot.com/-MNQbK_A8gno/VtZG-PpVpLI/AAAAAAAAa18/wqvYlWgT8C4/s1600/random.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;280&quot; src=&quot;https://2.bp.blogspot.com/-MNQbK_A8gno/VtZG-PpVpLI/AAAAAAAAa18/wqvYlWgT8C4/s640/random.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot;&gt;Figure 1: Completely randomized versus stratified randomized sampling.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;In LSOS, stratified sampling is doable but less convenient than in, say, a clinical trial. This is because assignment to treatment and control arms is made in real time and thus the number of experimental units in each slice are not known in advance. An easier approach is to account for imbalances after the fact in analysis.&lt;br /&gt;&lt;div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;vertical-align: baseline;&quot;&gt;&lt;br /&gt;To illustrate how this works, let’s take the music service as our example. Let the outcome variable $Y_i$ represent the number of songs downloaded in a user session $i$, assuming each user session is independent. To simplify the analysis, let’s further assume that the treatment does not affect the variance of $Y_i$.&amp;nbsp;&lt;/span&gt;The simplest estimator for the average treatment effect (ATE) is given by the difference of sample averages for treatment and control:&amp;nbsp;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;vertical-align: baseline;&quot;&gt;\[&lt;br /&gt;\mathcal{E}_1=\frac{1}{|T|}\sum_{i \in T}Y_i - \frac{1}{|C|}\sum_{i \in C}Y_i &lt;br /&gt;\]&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;vertical-align: baseline;&quot;&gt;Here, $T$ and $C$ represent sets of indices of user sessions in treatment and control and $| \cdot |$ denotes the size of&amp;nbsp;&lt;/span&gt;a set. The variance of this estimator is&lt;br /&gt;\[&lt;br /&gt;\mathrm{var\ } \mathcal{E}_1 = \left(\frac{1}{|T|} + \frac{1}{|C|} \right )\mathrm{var\ }Y &lt;br /&gt;\]&lt;br /&gt;Estimator $\mathcal{E}_1$ accounts for the fact that the number of treatment and control units may vary each time we run the experiment. However, it doesn&#39;t take into account the variation in the fraction of treatment and control units assigned to each hour of day. Conditioned on $|T|$ and $|C|$, the number of units assigned to treatment or control in each hour of day will vary according to a multinomial distribution. And since the metric average is different in each hour of day, this is a source of variation in measuring the experimental effect.&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;vertical-align: baseline;&quot;&gt;&lt;br /&gt;We can easily quantify this effect for the case when there is a constant experimental effect and variance in each slice. In other words, assume that in Slice $k$,&amp;nbsp;$Y_i \sim N(\mu_k,\sigma^2)$ under&amp;nbsp;&lt;/span&gt;control and $Y_i \sim N(\mu_k+\delta,\sigma^2)$ under treatment.&amp;nbsp;To compute $\mathrm{var\ }Y$ we condition on slices using the &lt;a href=&quot;https://en.wikipedia.org/wiki/Law_of_total_variance&quot;&gt;law of total variance&lt;/a&gt; which says that \[&lt;br /&gt;\mathrm{var\ }Y = E_Z[\mathrm{var\ }Y|Z] + \mathrm{var_Z}[EY|Z]&lt;br /&gt;\]Let $Z$ be the random variable representing hour of day for the user session. $Z$ is our slicing variable, and takes on integer values $0$ through $23$. The first component, $E_Z[\mathrm{var\ }Y|Z]$, is the expected within-slice variance, in this case $\sigma^2$. The second component, $\mathrm{var_Z}[EY|Z]$, is the variance of the per-slice means. It&#39;s that part of&amp;nbsp;the variance which is due to the slices having different means. We’ll call it $\tau^2$:&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;\begin{align}&lt;br /&gt;\tau^2 &amp;amp;= \mathrm{var}_Z[EY|Z] \\&lt;br /&gt;&amp;amp;= E_Z[(EY|Z)^2] - (E_Z[EY|Z])^2 \\&lt;br /&gt;&amp;amp;= \sum_k w_k \mu_k^2 - \left( \sum_k w_k\mu_k \right )^2&lt;br /&gt;\end{align}&lt;br /&gt;where $w_k$ is the fraction of units in Slice $k$. In this situation, the variance of estimator $\mathcal{E}_1$ is&lt;/div&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;\[&lt;br /&gt;\mathrm{var\ } \mathcal{E}_1 = \left(\frac{1}{|T|} + \frac{1}{|C|} \right )(\sigma^2+\tau^2)&lt;br /&gt;\]&lt;br /&gt;Quantity $\tau^2$ is nonzero because the slices have different means. We can remove its effect if we employ an estimator $\mathcal{E}_2$ that takes into account the fact that the data are sliced:&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;\[&lt;br /&gt;\mathcal{E}_2=\sum_k \frac{|T_k|+|C_k|}{|T|+ |C|}\left( \frac{1}{|T_k|}\sum_{i \in T_k}Y_i - \frac{1}{|C_k|}\sum_{i \in C_k}Y_i \right)&lt;br /&gt;\]&lt;br /&gt;Here, $T_k$ and $C_k$ are the subsets of treatment and control indices in Slice $k$. This estimator improves on $\mathcal{E}_1$ by combining the per-slice average difference between treatment and control. Both estimators are unbiased. But the latter estimator has less variance. To see this, we can compute its variance:&lt;br /&gt;\[&lt;br /&gt;\mathrm{var\ } \mathcal{E}_2 =&lt;br /&gt;\sum_k \left(\frac{|T_k|+|C_k|}{|T|+ |C|} \right )^2\left( \frac{1}{|T_k|} + \frac{1}{|C_k|}  \right)&amp;nbsp;\sigma^2&lt;br /&gt;\]&lt;br /&gt;Given that $|T_k| \approx w_k |T|$ and $|C_k| \approx w_k|C|$,&lt;br /&gt;\begin{align}&lt;br /&gt;\mathrm{var\ } \mathcal{E}_2 &amp;amp;\approx&lt;br /&gt;\sum_k w_k^2 \left(\frac{1}{w_k|T|} + \frac{1}{w_k|C|} \right ) \sigma^2 \\&lt;br /&gt;&amp;amp;=&lt;br /&gt;\left(\frac{1}{|T|} + \frac{1}{|C|} \right ) \sigma^2 &lt;br /&gt;\end{align}&lt;br /&gt;Intuitively, we have increased the precision of our estimator by using information in $Z$ about which treatment units happened to be in each slice and likewise for control. In other words, this estimator is conditioned on the sums and counts (sufficient statistics) &lt;b&gt;in each slice&lt;/b&gt; rather than just overall sums and&amp;nbsp;counts. &lt;br /&gt;&lt;br /&gt;In general, we can expect conditioning to remove only effects due to the second component of the law of total variation. This is because $\mathrm{var}_Z [EY|Z]$ is the part of $\mathrm{var\ }Y$ explained by $Z$ while $E_Z[\mathrm{var\ }Y|Z]$ is the variance remaining even after we know $Z$. Thus, if an estimator is based on the average value of $Y$, we can bring its variance down to the fraction&lt;br /&gt;\[&lt;br /&gt;\frac{E_Z[\mathrm{var\ }Y|Z]}{\mathrm{var\ }Y}&lt;br /&gt;\]&lt;br /&gt;But that’s just the theory. As discussed in the last post, LSOS often face the situation that $Y$ has a large CV. But if the CV of $Y$ is large, it is often because the variance within each slice is large, i.e. $E_Z[\mathrm{var\ }Y|Z]$ is large. If we track a single metric across the slices, we do so because we believe more or less the same phenomenon is at work in each slice of the data. Obviously, this doesn’t have to be true. For example, an online music service may care about the fraction of songs listened to from playlists in each user session but this playlist feature may only be available to premium users. In this case, conditioning on user type could make a big difference to the variability. But then you have to wonder why the music service doesn’t just track this metric for premium user sessions as opposed to all user sessions!&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;span style=&quot;vertical-align: baseline;&quot;&gt;Another way to think about this is that the purpose of stratification is to use side information to group observations which are more similar within strata than between strata. We have ways to remove the variability due to differences between strata (between-stratum variance). But if between-stratum variance is small compared to within-stratum variance, then conditioning is not going to be very effective.&lt;/span&gt;&lt;/div&gt;&lt;span style=&quot;vertical-align: baseline;&quot;&gt;&lt;/span&gt;&lt;br /&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;Rare binary event example&lt;/h2&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;In the &lt;a href=&quot;http://www.unofficialgoogledatascience.com/2016/01/variance-and-significance-in-large.html&quot;&gt;previous post&lt;/a&gt;, we discussed how rare binary events can be fundamental to the LSOS business model. To that end, it is worth studying them in more detail. Let’s make this concrete with actual values. Say we have track the metric fraction of user sessions which result in a purchase. We care about the effect of experiments on this important metric.&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;Let $Y$ be the Bernoulli random variable representing the purchase event in a user session. Suppose that the purchase probability varies by hour of day $Z$. In particular, say $E[Y|Z]=\theta$ follows a sinusoidal function of hour of day varying by a factor of 3. In other words, $\theta$ varies from $\theta_{min}$ to $\theta_{max}=3 \theta_{min}$ with average $\bar{\theta} = (\theta_{min} + \theta_{max})/2$. In the plot below, $\theta_{min}=1\%$ and $\theta_{max}=3\%$.&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;img height=&quot;512&quot; src=&quot;https://lh3.googleusercontent.com/f-91n4Nbia0tnZMZqtjQjn8oZM0412okqrQJlJYE3wz76E98QIW4rzUec_w92b3brk7NM4w9mzfwxFZHiwE8qxIk8djR7zmg2sSlXYtui493F7kakUj_26qmRaHugRbb7y3p5jdR&quot; style=&quot;margin-left: auto; margin-right: auto;&quot; width=&quot;640&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Figure 2: Fictitious hourly variation in mean rate of sessions with purchase.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;For simplicity, assume there is no variation in frequency of user sessions in each hour of day. A variation in the hourly rate by a factor of 3 would seem worth addressing through conditioning. But we shall see that the improvement is small.&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;As discussed earlier, the portion of $\mathrm{var\ }Y$ we can reduce is $\mathrm{var_Z}[EY|Z]$. Here&lt;br /&gt;\[&lt;br /&gt;\mathrm{var\ }Y = \bar{\theta}(1-\bar{\theta}) \\&lt;br /&gt;\mathrm{var_Z}[EY|Z] = \mathrm{var\ }\theta&lt;br /&gt;\]&lt;br /&gt;We can approximate $\mathrm{var\ }\theta$ by the variance of a sine wave of amplitude $(\theta_{max} - \theta_{min})/2$. This gives us $\mathrm{var\ }\theta \approx (\theta_{max} - \theta_{min})^2/8$&amp;nbsp;which reduces to $\bar{\theta}^2/8$ when $\theta_{max}=3 \theta_{min}$. Thus&amp;nbsp;the fraction of reducible variance is&lt;/div&gt;&lt;div&gt;\[&lt;br /&gt;\frac{\mathrm{var\ }\theta}{\mathrm{var\ }Y} = \frac{\bar{\theta}}{8(1-\bar{\theta})}&lt;br /&gt;\]&lt;/div&gt;&lt;div&gt;which is less than $0.3\%$ when $\bar{\theta}=2\%$. Even with $\bar{\theta}=20\%$, the reduction in variance is still only about $3\%$. Since the fractional reduction in confidence interval width is about half the reduction in variance, there isn’t much to be gained here.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;For intuition as to why variance reduction did not work let’s plot on the same graph the unconditional standard deviation $\sqrt{\bar{\theta}(1-\bar{\theta})}$ of $14\%$ and the hourly standard deviation $\sqrt{\mathrm{var}\ Y|Z}= \sqrt{\theta(1-\theta)}$ which ranges from $10\%$ to $17\%$. We can see clearly that the predictable hourly variation of $\theta$ (the red line in the plot below) is small compared to the purple line representing variation in $Y$.&lt;/div&gt;&lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;img height=&quot;512&quot; src=&quot;https://lh4.googleusercontent.com/kkLHGrHO86VPWlmiJtUzuUjDUhdDLwtQ2gA5j_TFNGq1mYdn-Ucke9A2P1nNSll1pOwok_KWN2gNra1moqs_42Z0A_76Ua_MVk0fYr3pRGQ9PlCOU2e_PNVR-5va2XSfEN0EWLG0&quot; style=&quot;margin-left: auto; margin-right: auto;&quot; width=&quot;640&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;&lt;span style=&quot;font-size: 12.8px;&quot;&gt;Figure 3: Mean and standard deviation of hourly rate of sessions with purchase&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;Variance reduction through prediction&lt;/h2&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;In our previous example, conditioning on hour-of-day slicing did not meaningfully reduce variability. This was a case where our experiment metric was based on a rare binary event, the fraction of user sessions with a purchase. One could argue that we should have included all relevant slicing variables such as country, day of week, customer dimensions (e.g. premium or freemium) etc. Perhaps the combined effect of these factors would be worth addressing? If we were simply to posit the cartesian product of these factors, we would end up with insufficient data in certain cells. A better approach would be to create an explicit prediction of the probability that the user will purchase within a session. We could then define slices corresponding to intervals of the classification probability. To simplify the discussion, assume that all experimental effects are small compared with the prediction probabilities. Of course the classifier cannot use any attributes of the user session that are potentially affected by treatment (e.g. session duration) but that still leaves open a rich set of predictors.&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;We’d like to get a sense for how the accuracy of this classifier translates into variance reduction. For each user session, say the classifier produces a prediction $\theta$ which is its estimate of the probability of a purchase during that session. $Y$ is the binary event of a purchase. Let’s also assume the prediction is “calibrated” in the sense that $EY|\theta=\theta$, i.e. of the set of user sessions for which the classifier predicts $\theta=0.3$, exactly $30\%$ have a purchase. &amp;nbsp;If $EY=E\theta=\bar{\theta}$ is the overall purchase rate per session, the law of total variance used earlier says that the fraction of variance we cannot reduce by conditioning is&lt;br /&gt;\[&lt;br /&gt;\frac{E_{\theta}[\mathrm{var\ }Y|\theta]}{\bar{\theta}(1-\bar{\theta})}&lt;br /&gt;\]&lt;br /&gt;The mean squared error (MSE) of this (calibrated) classifier is&lt;/div&gt;&lt;div&gt;\[&lt;br /&gt;E(Y-\theta)^2=E_\theta [\mathrm{var\ }Y|\theta] + E_\theta(EY|\theta - \theta)^2&lt;br /&gt;= E_\theta[\mathrm{var\ }Y|\theta]&lt;br /&gt;\]&lt;br /&gt;In other words, &lt;b&gt;the MSE of the classifier is the residual variance after conditioning&lt;/b&gt;. Furthermore, an uninformative calibrated classifier (one which always predicts $\bar{\theta}$) has MSE of $\bar{\theta}(1-\bar{\theta})$. This is equal to $\mathrm{var\ }Y$ so, unsurprisingly, such a classifier gives us no variance reduction (how could it if it gives us a single slice?). Thus, the better the classifier for $Y$ we can build (i.e. the lower its MSE) the greater the variance reduction possible if we condition on its prediction. But since it is generally difficult predict the probability rare events accurately, this is unlikely to be an easy route to variance reduction.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;In the previous example, we tried to predict small probabilities. Another way to build a classifier for variance reduction is to address the rare event problem directly&amp;nbsp;—&amp;nbsp;what if we could predict a subset of&amp;nbsp;instances in which the event of interest will definitely not occur? This would make the event more likely in the complementary set and hence mitigate the variance problem. Indeed, I was motivated to take such an approach several years ago when looking for ways to reduce the variability of Google’s search ad engagement metrics. While it is hard to predict in general how a user will react to the ads on any given search query, there are many queries where we can be sure how the user will react. For instance, if we don’t show any ads on a query then there can be no engagement. Since we did not show ads on a very large fraction of queries, I thought this approach appealing and did the following kind of analysis.&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;Let’s go back to our example of measuring the fraction of user sessions with purchase. Say we build a classifier to classify user sessions into two groups which we will call “dead” and “undead” to emphasize the importance of the rare purchase event to our business model. Suppose the classifier correctly assigns the label “undead” to $100\%$ of the sessions with purchase while correctly predicting “dead” for a fraction $\alpha$ of sessions without purchase. In other words, it has no false negatives and a false positive rate of $1-\alpha$. The idea is to condition on the two classes output by the classifier (again assume we are studying small experimental effects). Let $\theta$ be the fraction of sessions resulting in purchase. The question is how high must $\alpha$ be (as a function of $\theta$) in order to make a significant dent in the variance.&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;If $w$ is the fraction of sessions for which the classifier outputs “undead”, $w=\theta + (1-\alpha)(1- \theta)$. Meanwhile the conditional variance for the classes “dead” and “undead” are respectively $0$ and $\theta/w \cdot (1-\theta/w)$, leading to an average conditional variance of $w\cdot \theta/w \cdot (1-\theta/w)$. This is the variance remaining of the unconditioned variance $\theta (1-\theta)$. The fractional variance remaining is given by&lt;/div&gt;\[&lt;br /&gt;\frac{w \cdot \theta/w \cdot (1 - \theta/w)}{ \theta \cdot (1-\theta)}  = \frac{1-\alpha}{1-\alpha+\alpha \theta}&lt;br /&gt;\]&lt;/div&gt;&lt;div&gt;&lt;div&gt;If the probability of a user session resulting in purchase is 2%, a classifier predicting 95% of non-purchase sessions (and all purchase sessions) would reduce variance by 28% (and hence CI widths by 15%). To get a 50% reduction in variance (29% smaller CIs) requires $\alpha \approx 1-\theta$, in this case 98%. Such accuracy seems difficult to achieve if the event of interest is rare.&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;img height=&quot;512&quot; src=&quot;https://lh4.googleusercontent.com/O_ZiGa-554ON2C_HyqyxXp3PVMfCOxFZZT07XswNn9jQYsXTyStymbxo-xZH7l-RBwnLVULsCIUEPfDpIG3LS-hTw0izx2N_5TsBBbn5dpTegqvLxQ_DW_n2ov3KS6CFJzbiIjNH&quot; style=&quot;margin-left: auto; margin-right: auto;&quot; width=&quot;640&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Figure 4: Residual fraction of variance as a function of $\alpha$ when $\theta=2\%$.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;Overlapping (a.k.a factorial) experiments&lt;/h2&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;For one last instance of variance reduction in live experiments, consider the case of overlapping experiments as described in [1]. The idea is that experiments are conducted in “layers” such that a treatment arm and its control will inhabit a single layer. Assignments to treatment and control across layers are independent, leading to what&#39;s known as a full factorial experiment. Take the simplest case of two experiments, one in each layer. Every experimental unit (every user session in our example) goes through either Treatment 1 or Control 1 and either Treatment 2 or Control 2. Assuming that the experimental effects are strictly additive, we can get a simple unbiased estimate of the effect of each experiment by ignoring what happens in the other experiment. This relies on the fact that units subject to Treatment 1 and Control 1 on average receive the same treatment in Layer 2.&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;The effects of Layer 2 on the experiment in Layer 1 (and vice versa) do indeed cancel on average but not in any specific instance. Thus multiple experiment layers introduce additional sources of variability. A more sophisticated analysis could try to take into account the four possible combinations of treatment each user session actually received. Let $Y_i$ be the response measured on the $i$th user session. A standard statistical way to solve for the effects in a full factorial experiment design is via regression. The data set would have a row for each observation $Y_i$ and a binary predictor indicating whether the observation went through the treatment arm of each experiment. When solved with an intercept term, regression coefficients for the binary predictors are maximum likelihood estimates for the experiment effects under assumption of additivity. We could do this but in our big data world, we would avoid materializing such an inefficient structure by reducing the regression to its sufficient statistics. Let&amp;nbsp;&lt;/div&gt;&lt;br /&gt;\begin{align}&lt;br /&gt;S_{00} &amp;amp;= \sum_{i \in C_1 \cap C_2} Y_i \\&lt;br /&gt;S_{01} &amp;amp;= \sum_{i \in T_1 \cap C_2} Y_i \\&lt;br /&gt;S_{10} &amp;amp;= \sum_{i \in C_1 \cap T_2} Y_i \\&lt;br /&gt;S_{11} &amp;amp;= \sum_{i \in T_1 \cap T_2} Y_i&lt;br /&gt;\end{align}&lt;br /&gt;&lt;div&gt;Similarly, let $N_{00}=| C_1 \cap C_2 |$ etc. The “regression” estimator for the effect of each experiment are the solutions for $\beta_1$ and $\beta_2$ in the matrix equation&lt;/div&gt;\[&lt;br /&gt;\begin{bmatrix}&lt;br /&gt;S_{00}\\ &lt;br /&gt;S_{01}\\ &lt;br /&gt;S_{10}\\ &lt;br /&gt;S_{11}&lt;br /&gt;\end{bmatrix}&lt;br /&gt;=\begin{bmatrix}&lt;br /&gt;N_{00} &amp;amp; 0 &amp;amp; 0 \\ &lt;br /&gt;N_{01} &amp;amp; N_{01} &amp;amp; 0\\ &lt;br /&gt;N_{10} &amp;amp; 0 &amp;amp; N_{10}\\ &lt;br /&gt;N_{11} &amp;amp; N_{11} &amp;amp; N_{11} &lt;br /&gt;\end{bmatrix}&lt;br /&gt;\begin{bmatrix}&lt;br /&gt;\beta_0\\ &lt;br /&gt;\beta_1\\ &lt;br /&gt;\beta_2&lt;br /&gt;\end{bmatrix}&lt;br /&gt;\]&lt;br /&gt;In contrast, the simple estimator which ignores the other layers has estimates for Experiment 1&lt;/div&gt;&lt;div&gt;\[&lt;br /&gt;\frac{S_{01} + S_{11}}{N_{01}+N_{11}} - \frac{S_{00} + S_{10}}{N_{00}+N_{10}}&lt;br /&gt;\]&lt;br /&gt;and Experiment 2&lt;/div&gt;&lt;div&gt;\[&lt;br /&gt;\frac{S_{10} + S_{11}}{N_{10}+N_{11}} - \frac{S_{00} + S_{01}}{N_{00}+N_{01}}&lt;br /&gt;\]&lt;/div&gt;&lt;div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;In this example, the regression estimator isn’t very hard to compute, but with several layers and tens of different arms in each layer, the combinatorics grow rapidly. All still doable but we’d like to know if this additional complexity is warranted.&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;At first blush this problem doesn’t resemble the variance reduction problems in the preceding sections. But if we assume for a moment that Experiment 2’s effect is known, we see that in estimating the effect of Experiment 1, Layer 2 is merely adding variance by having different means in each of its arms. Of course, the effect of Experiment 2 is not known and hence we need to solve simultaneous equations. But even if the effects of Experiment 2 were known, the additional variance due to Layer 2 would be very small unless Experiment 2 has a large effect. And by supposition, we are operating in the LSOS world of very small effects, certainly much smaller than the predictable effects within various slices (such as the 3x effect of hour of day we modeled earlier). Another way to say this is that knowing the effect of Experiment 2 doesn’t help us much in improving our estimate for Experiment 1 and vice versa. Thus we can analyze each experiment in isolation without much loss.&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;Conclusion&lt;/h2&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;In this post, we continued our discussion from the previous post about experiment analysis in large scale online systems (LSOS). It is often the case that LSOS have metrics of interest based on observations with high coefficients of variation (CV). On the one hand, this means very small effect sizes may be of practical significance. On the other hand, this same high CV makes ineffective some statistical techniques to remove variability due to known or predictable effects.&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;A different way to frame this is that conditioning reduces measurement variability by removing the effects of imbalance in assigning experimental units within individual slices to treatment and control. This happens because sampling noise causes empirical fractions within slices to deviate from their expectation. At the same time, if the experiment metric is based on observations of high CV, we need to run experiments with a very large number of experimental units in order to obtain statistically significant results. The experiment size is therefore large enough to ensure that the empirical fractions within slices are unlikely to be far from their expectation. In other words, the law of large numbers leaves little imbalance to be corrected by conditioning. All this is just a consequence of conducting sufficiently powerful experiments in a data-rich, information-poor setting.&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;We conclude this post with two important caveats. First, not all LSOS metrics rely on observations with high CV. For instance, an online music streaming site may be interested in tracking the average listening time per session as an important metric of user engagement. There is no reason a priori to think that listening time per session will have high CV. &lt;b&gt;If the CV is small, the very techniques discussed in this post may improve experiment analysis&amp;nbsp;&lt;/b&gt;&lt;b&gt;significantly&lt;/b&gt;. Second, this post has explored only a certain class of approaches to reduce measurement variability. Even in the high CV regime, there are techniques which can prove effective though they may require changes to the experiment itself. In a future post we hope to cover the powerful idea of what at Google we call “experiment counterfactuals”.&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;Until then, if we are operating in a regime where clever experiment analysis doesn’t help, we are freed from having to worry about it. A burden has been lifted. In a way, I find that comforting!&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;References&lt;/h2&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;[1] Diane Tang, Ashish Agarwal, Deirdre O’Brien, Mike Meyer, “&lt;a href=&quot;http://research.google.com/pubs/pub36500.html&quot;&gt;Overlapping Experiment Infrastructure: More, Better, Faster Experimentation&lt;/a&gt;”, Proceedings 16th Conference on Knowledge Discovery and Data Mining, Washington, DC&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;br /&gt;&lt;/div&gt;</content><link rel='replies' type='application/atom+xml' href='https://www.unofficialgoogledatascience.com/feeds/2103339658865337794/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='https://www.unofficialgoogledatascience.com/2016/02/lsos-experiments-how-i-learned-to-stop.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/2103339658865337794'/><link rel='self' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/2103339658865337794'/><link rel='alternate' type='text/html' href='https://www.unofficialgoogledatascience.com/2016/02/lsos-experiments-how-i-learned-to-stop.html' title='LSOS experiments: how I learned to stop worrying and love the variability'/><author><name>Amir Najmi</name><uri>http://www.blogger.com/profile/18174523203317227640</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://2.bp.blogspot.com/-MNQbK_A8gno/VtZG-PpVpLI/AAAAAAAAa18/wqvYlWgT8C4/s72-c/random.png" height="72" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-5825758052688213474.post-3104261807924866209</id><published>2016-01-14T18:27:00.003-08:00</published><updated>2021-02-11T11:34:46.023-08:00</updated><title type='text'>Variance and significance in large-scale online services</title><content type='html'>by AMIR NAJMI&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;i&gt;Running live experiments on large-scale online services (LSOS) is an important aspect of data science. Unlike&amp;nbsp;&lt;/i&gt;&lt;i&gt;experimentation in&amp;nbsp;&lt;/i&gt;&lt;i&gt;some other areas, LSOS experiments present a surprising challenge to statisticians — even though we operate in the realm of “big data”, the statistical uncertainty in our experiments can be substantial. Because individual observations have so little information, statistical significance remains important to assess. We must therefore maintain statistical rigor in quantifying experimental uncertainty.&amp;nbsp;&lt;/i&gt;&lt;i&gt;In this post we explore how and why we can be&amp;nbsp;&lt;/i&gt;&lt;i&gt;“&lt;/i&gt;&lt;i&gt;data-rich but information-poor&lt;/i&gt;&lt;i&gt;”&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;There are many reasons for the recent explosion of data and the resulting rise of data science. One big factor in putting data science on the map has been what we might call Large Scale Online Services (LSOS). These are sites and services which rely both on ubiquitous user access to the internet as well as advances in technology to scale to millions of simultaneous users. There are commercial sites which allow users to search for and purchase goods or book rooms they desire. There are music and video streaming sites where users decide which content to consume, and apps, be they for ride-sharing or dating. In each case, users engage with the service at will and the service makes available a rich set of possible interactions. Which action a user takes depends on many factors — her intent, her needs, her tastes, the perceived quality of choices available to her, the presentation of those choices, the ease of selection, the performance of the website, and so on. Indeed, understanding and facilitating user choices through improvements in the service offering is much of what LSOS data science teams do.&lt;br /&gt;&lt;br /&gt;As with any enterprise, the goal of the service provider is to better satisfy its users and further its business objectives. But the fact that a service could have millions of users and billions of interactions gives rise to both big data and methods which are effective with big data. Of particular interest to LSOS data scientists are modeling and prediction techniques which keep improving with more data. If these are a &lt;b&gt;simple matter of data&lt;/b&gt; (call it SMOD in analogy to &lt;a href=&quot;https://en.wikipedia.org/wiki/Small_matter_of_programming&quot;&gt;SMOP&lt;/a&gt;), they will improve automatically as the LSOS itself grows.&lt;br /&gt;&lt;br /&gt;A particularly attractive approach to understanding user behavior in online services is live experimentation. Randomized experiments are invaluable because they represent the gold standard for drawing causal inferences. And because the service is online and large scale, it may be feasible to experiment with each of many parameters of the service. For example, an LSOS experiment may answer the question of whether a new design for the main page is better for the user. The LSOS may do this by exposing a random group of users to the new design and compare them to a control group, and then analyze the effect on important user engagement metrics, such as bounce rate, time to first action, or number of experiences deemed positive. Indeed, such live experiments (so-called “A/B” experiments) have become a staple in the LSOS world [1].&lt;br /&gt;&lt;br /&gt;Since an LSOS experiment has orders of magnitude larger sample size than the typical social science experiment, it is tempting to believe that any meaningful experimental effect would automatically be statistically significant. It is certainly true that for any given effect, statistical significance is an SMOD. And an LSOS is awash in data, right? Well, it turns out that depending on what it cares to measure, an LSOS might not have enough data. Surprisingly, outcomes of interest to an LSOS often have very high coefficient of variation compared, say, to social science experiments. This means that each observation has little information, and we need a lot of observations to make reliable statements. The practical consequence of this is that we can’t afford to be sloppy about measuring statistical significance and confidence intervals. At Google, we have invested heavily in making our estimates of uncertainty evermore accurate (see &lt;a href=&quot;http://www.unofficialgoogledatascience.com/2015/08/an-introduction-to-poisson-bootstrap_26.html&quot;&gt;our blog post on Poisson Bootstrap&lt;/a&gt; for an example).&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Statistical Significance vs. Practical Significance&lt;/h2&gt;&lt;div&gt;Suppose we are running an LSOS with lots of “traffic” (pageviews, user sessions, requests, the like). Ours is a sophisticated outfit, doing lots of live experiments to determine which features will best serve our users’ needs. No doubt we have metrics which we track to determine which experimental change is worth launching. These metrics embody some important aspect of our business objectives, such as click-through rates on content, watch times on video, likes on a social networking site. In addition to a suitable metric, we must also choose our experimental unit. This is the unit being treated and whose response we assume to be independent of the treatment administered to other units (also known as the stable unit treatment value assumption, or &lt;a href=&quot;https://en.wikipedia.org/wiki/Rubin_causal_model#Stable_unit_treatment_value_assumption_.28SUTVA.29&quot;&gt;SUTVA&lt;/a&gt;, in the causal inference literature). Each experiment is conducted by treating some randomly sampled units and comparing against other randomly sampled untreated units. Choice of &lt;b&gt;experimental unit&lt;/b&gt; isn’t trivial either, since we want to define them to be as numerous as possible but still largely independent. For instance, we probably don’t want to posit pageviews to our website as our experimental unit because it is hard to argue that treatment received by a user on one page will not affect her behavior on another page in that session — perhaps user sessions or even whole users might be necessary as experimental units. In any event, let’s say we have an appropriate choice of experimental unit.&lt;br /&gt;&lt;br /&gt;At its simplest, we will run our randomized experiment and compare the average metric effect on treatment against that of control. Typically, we would require the results of the experiment to be both statistically significant and practically significant in order to launch. &lt;b&gt;Statistical significance&lt;/b&gt; ensures that the results of the experiment are unlikely to be due to chance. For this purpose, let’s assume we use a t-test for difference between group means. As noted, if there is any effect at all, statistical significance is “SMOD”. On the other hand, &lt;b&gt;practical significance&lt;/b&gt; is about whether the effect itself is large enough to be worthwhile. That’s more of a business question about the value of the underlying effect than about one’s ability to measure the effect. If the experiment gives us 95% confidence of a 0.01 +/- 0.002% change in our metric, we have enough measurement accuracy but may well not care to launch such a small effect.&lt;br /&gt;&lt;br /&gt;All this is old hat to statisticians and experimental social scientists even if they aren’t involved in data science. Indeed, a Google search for [&lt;a href=&quot;https://www.google.com/#q=statistical+significance+vs+practical+significance&quot;&gt;statistical significance vs practical significance&lt;/a&gt;] turns up lots of discussion. The surprise is that the effect sizes of practical significance are often extremely small from a traditional statistical perspective. To understand this better we need a few definitions.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Effect size&lt;/h2&gt;Let our metric be $Y_i$ on the $i$th experimental unit. Further assume $Y_i \sim N(\mu,\sigma^2)$ under control and $Y_i \sim N(\mu+\delta,\sigma^2)$ under treatment (i.e. known, equal variances). The statistical effect size is often defined as \[ e=\frac{\delta}{\sigma}&lt;br /&gt; \]which is the difference in group means as a fraction of the (pooled) standard deviation (sometimes referred to as &lt;a href=&quot;https://www.blogger.com/#&quot;&gt;“Cohen’s d”&lt;/a&gt;). An effect size of 0.2 in this situation is traditionally considered small (say, on Cohen’s scale). For a traditional (i.e. non-LSOS) example, let&#39;s say the height of men and women in the US each follows a normal distribution with means 69&quot; and 64&quot; and standard deviation of 3&quot; (&lt;a href=&quot;https://www.blogger.com/#&quot;&gt;close enough to reality&lt;/a&gt;). Then the effect size of gender difference on height is 1.67 (a large effect size).&lt;br /&gt; &lt;br /&gt;&lt;div&gt;Effect size thus defined is useful because the statistical power of a classical test for $\delta$ being non-zero depends on $e/\sqrt{\tilde{n}}$, where $\tilde{n}$ is the harmonic mean of sample sizes of the two groups being compared. To observe this, let $W$ be the sample average differences between groups (our test statistic). Since $W$ is the difference of two independent normal random variables,\[&lt;br /&gt;W \sim N\left(\delta, (\frac{1}{n_1}+\frac{1}{n_2})\sigma^2\right)&lt;br /&gt;\]where $n_1$and $n_2$ are the sample sizes of the two groups. If we define\[&lt;br /&gt;\frac{2}{\tilde{n}} = \frac{1}{n_t} +\frac{1}{n_c}\]then $W \sim N(\delta, 2\sigma^2/\tilde{n})$. A two-sided classical hypothesis test with Type I and Type II errors no greater than $\alpha$ and $\beta$ respectively requires that\[&lt;br /&gt;\frac{\delta}{\sqrt{2\sigma^2/\tilde{n}}} &amp;gt; \Phi^{-1}(1-\frac{\alpha}{2}) + \Phi^{-1}(1-\beta)&lt;br /&gt;\\&lt;br /&gt;\Rightarrow e \sqrt{\tilde{n}} &amp;gt; \sqrt{2}\left(\Phi^{-1}(1-\frac{\alpha}{2}) + \Phi^{-1}(1-\beta)\right)&lt;br /&gt;\] where $\Phi$ is the cumulative distribution function of the standard normal distribution. To obtain a sufficiently powered test, we therefore need\[&lt;br /&gt;\tilde{n} &amp;gt;\frac{K(\alpha, \beta)}{e^2}&lt;br /&gt;\] where $K(\alpha, \beta) = 2 \left(\Phi^{-1}(1-\frac{\alpha}{2}) + \Phi^{-1}(1-\beta) \right)^2$.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;For typical values of $\alpha=0.05$ and $\beta=0.1$ we have $K(\alpha,\beta)=21.01$. So continuing our traditional example, imagine we wish to test the hypothesis that the average height of men is different from the average height of women in the US. Using the effect size of $1.67$ (standard deviation assumed known), we obtain a minimal required sample size of $\tilde{n}&amp;gt;7.57$. This means we would need at least 16 people (8 men and 8 women) to get the desired statistical power.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;br /&gt;&lt;h2&gt;Effect fraction&lt;/h2&gt;In contrast to traditional analysis, the quantity typically of interest to an LSOS business is what we might call the &lt;b&gt;effect fraction&lt;/b&gt;, \[&lt;br /&gt; f = \frac{\delta}{\mu}&lt;br /&gt; \]namely, the difference in group means as a fraction of the mean. For instance, when running an experiment, we would want to know the change in downloads per user session on our music site as a fraction of current downloads per session (this is just the percent change in downloads). The business is probably much less interested in the change in downloads per session as a fraction of the standard deviation of downloads per session (effect size). “Effect fraction” isn’t a standard term but I find it useful to distinguish the concept from “effect size”. &lt;br /&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Often, a mature LSOS would consider changes of the order of 1% in effect fraction to be practically significant. Several improvements over the year, each of the order of 2% or 3%, would result in substantial annual improvement due to product changes alone. It’s a great recipe for steady product development but requires the ability to run many experiments while being able to measure effect fractions of this size quickly and reliably.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;div&gt;&lt;span id=&quot;docs-internal-guid-91165aa1-4254-f75b-c88e-1595523b33a6&quot;&gt;&lt;/span&gt;&lt;br /&gt;&lt;h2&gt;&lt;span id=&quot;docs-internal-guid-91165aa1-4254-f75b-c88e-1595523b33a6&quot;&gt;Coefficient of variation&lt;/span&gt;&lt;/h2&gt;Now let’s look at the ratio of effect fraction to effect size\[  \frac{f}{e} = \frac{\delta/\mu}{\delta/\sigma} = \frac{\sigma}{\mu}&lt;br /&gt;\]This ratio is just the coefficient of variation (CV) of a random variable, defined as its standard deviation over its mean. For our LSOS experiment above, CV of $Y_i$ in control is $c=\sigma/\mu$, and treatment CV approximately the same. Being dimensionless, it is a simple measure of the variability of a (non-negative) random variable. Furthermore, the fractional mean squared error when estimating $\mu$ from $n$ samples is $(\sigma/\sqrt{n})/\mu=c/\sqrt{n}$. Thus CV can be seen as a measure of the amount of information each sample from a distribution provides towards estimating its mean. In signal processing, CV is simply the reciprocal of the signal-to-noise ratio. We could call observations from a distribution “information-poor” if their distribution has large CV. It shouldn’t surprise, then, that the larger the CV, the more observations it takes to run useful experiments. And because $e=f/c$, the larger the CV for a given effect fraction, the smaller the resulting effect size. If we control for Type I and Type II errors as before, the required sample size is\[&lt;br /&gt; n &amp;gt; K(\alpha, \beta)\frac{c^2}{f^2}&lt;br /&gt; \]&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h2&gt;LSOS metrics can have large CV, small effect fraction, hence require large sample size&lt;/h2&gt;&lt;div&gt;If a metric is based on the average rate of rare occurrences, its underlying observations will have high CV. In the world of online services, this is rather common. For instance, a news site might care about the average number of comments per user session as a measure of user engagement, even though the vast majority of user sessions do not result in a comment. CV for a binary $\mathrm{Bernoulli}(p)$ random variable is $\sqrt{(1-p)/p}$. As the event becomes rarer, this grows as $1/\sqrt{p}$. Sometimes, the metric of interest is not the average rate of a rare binary event, per se, but is gated by such an event. For instance, the metric could be the price of goods purchased in the average user session. But if a small fraction of user sessions have any purchase at all, then the coefficient of variation for the metric (sale price per session) will necessarily be even larger than that of the binary event (sessions with a sale). In any case, suppose that on average 5% of user sessions to a news site result in comments. CV of the binary random variable “session has a comment” is $\sqrt{(1-0.05)/0.05}$ $=4.36$. Compare this to our non-LSOS example of adult heights in the US, CV of women’s heights is $0.047$.&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;While our focus has been on CV, we would be remiss not to point out the surprisingly small effect fractions of interest. As noted earlier, effect fractions of 1% or 2% can have practical significance to an LSOS. These are very small when compared with the kinds of effect fractions of interest in, say, medicine. Medicine uses the term “relative risk” to describe effect fraction when referring to the fractional change in incidence of some (bad) outcome like mortality or disease. To see what effect fractions are interesting in medicine, I looked at a &lt;a href=&quot;http://www.thelancet.com/pdfs/journals/lancet/PIIS0140-6736(15)01087-9.pdf&quot;&gt;recent Lancet paper&lt;/a&gt; which claims to prove that happiness doesn’t directly affect mortality. The paper gained much attention because, having conducted the largest study of its kind, it was understood to debunk the idea &lt;b&gt;definitively&lt;/b&gt;. However, their abstract presents relative risk of death comparing the unhappy group to the happy group with CIs we’d consider quite large — death from all causes -6% to&amp;nbsp;+1%, from ischemic heart disease -13% to&amp;nbsp;+10%, from cancer -7% to&amp;nbsp;+2%. A typical LSOS experiment with effect fraction CIs of several percent would be considered too underpowered to establish absence of meaningful effect.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h2&gt;A consequence of the LSOS business model?&lt;/h2&gt;&lt;div&gt;Large CV and small effect fraction of practical significance means that an LSOS &lt;b&gt;requires&lt;/b&gt; very large sample size compared to traditional experiments. It seems worth wondering why an LSOS should end up with large CV and also care about small effect fractions. One line of explanation I find plausible has to do with the low variable costs enabled by scalable web architectures — if the LSOS doesn’t involve physical goods and services (think Facebook, Spotify, Tinder as opposed to Amazon, Shutterfly, Apple) the marginal cost to support one more user request is almost zero. This is very different than bricks-and-mortar companies where there are many marginal costs, such as sales, manufacturing, transportation.&lt;/div&gt;&lt;/div&gt;&lt;br /&gt;Very low variable costs have two implications for the business model of these online services. First, low marginal cost of serving users allows the LSOS to pursue a business model in which it only monetizes through rare events while making the bulk of user interactions free. For instance, a free personal finance service may make its living through the rare sale of lucrative financial products while a free photo storage site may monetize through rare order for prints and photobooks which the LSOS refers to its physical business partners. Thus, important metrics for an LSOS often involve large CV precisely because they are based on aggregating these rare and vital events. Second, very low variable costs mean any growth essentially adds to the bottom line. In this light, a 1% effect fraction in activity metrics might be important because it could represent a much larger percentage of operating profit.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h2&gt;Conclusion&lt;/h2&gt;&lt;div&gt;LSOS experiments often measure metrics involving observations with high coefficient of variation. They also tend to care about small effect fractions. I speculated that both of these may be due to a common LSOS business model of “making it up on volume” (old joke, see &lt;a href=&quot;http://www.barrypopik.com/index.php/new_york_city/entry/we_lose_money_on_every_sale_but_make_it_up_on_volume&quot;&gt;here&lt;/a&gt; for its history). Whatever the reason, this means effect sizes of interest are orders of magnitude smaller than what a traditional statistical experiment would find practically significant. To detect such tiny effect sizes an LSOS needs to run experiments with a very large number of experimental units. Perhaps it is fitting that if scalable online services create a difficult estimation problem (small effect size) they also possess the means (many experimental units) to solve it.&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;h2&gt;References&lt;/h2&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;[1] Diane Tang, Ashish Agarwal, Deirdre O&#39;Brien, Mike Meyer, “&lt;a href=&quot;http://research.google.com/pubs/pub36500.html&quot;&gt;Overlapping Experiment Infrastructure: More, Better, Faster Experimentation&lt;/a&gt;”, Proceedings 16th Conference on Knowledge Discovery and Data Mining, Washington, DC&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content><link rel='replies' type='application/atom+xml' href='https://www.unofficialgoogledatascience.com/feeds/3104261807924866209/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='https://www.unofficialgoogledatascience.com/2016/01/variance-and-significance-in-large.html#comment-form' title='3 Comments'/><link rel='edit' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/3104261807924866209'/><link rel='self' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/3104261807924866209'/><link rel='alternate' type='text/html' href='https://www.unofficialgoogledatascience.com/2016/01/variance-and-significance-in-large.html' title='Variance and significance in large-scale online services'/><author><name>Amir Najmi</name><uri>http://www.blogger.com/profile/18174523203317227640</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><thr:total>3</thr:total></entry><entry><id>tag:blogger.com,1999:blog-5825758052688213474.post-4651288390164769572</id><published>2015-12-04T16:37:00.000-08:00</published><updated>2017-03-16T11:38:50.685-07:00</updated><title type='text'>Replacing Sawzall — a case study in domain-specific language migration</title><content type='html'>by AARON BECKER&lt;br/&gt;&lt;br/&gt;&lt;div&gt;&lt;i&gt;In a previous post, we described how data scientists at Google used Sawzall to perform powerful, scalable analysis. However, over the last three years we’ve eliminated almost all our Sawzall code, and now the niche that Sawzall occupied in our software ecosystem is mostly filled by Go. In this post, we’ll describe Sawzall’s role in Google’s analysis ecosystem, explain some of the problems we encountered as Sawzall use increased which motivated our migration, and detail the techniques we applied to achieve language-agnostic analysis while maintaining strong access controls and the ability to write fast, scalable analyses.&lt;/i&gt;&lt;/div&gt;&lt;b id=&quot;docs-internal-guid-275a6a72-6ebc-d603-cb56-2598afef4179&quot; style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;Any successful programming language has its own evolutionary niche, a set of problems that it solves unusually well. Sometimes this niche is created by language features. For example, &lt;a href=&quot;http://www.erlang.org/&quot;&gt;Erlang&lt;/a&gt; has strong tools for constructing distributed systems built into the language. In other cases, features such as standard libraries and a language’s community of users are more important — the main reason that R is a great language for statistics is that it’s widely used by statisticians and has a huge variety of useful statistics libraries. In order to understand the reason for Sawzall’s decline, we have to first understand the niche that it occupied in Google’s software ecosystem.&lt;br /&gt;&lt;br /&gt;&lt;a href=&quot;http://www.unofficialgoogledatascience.com/2015/09/on-procedural-and-declarative.html&quot;&gt;Our previous discussion of Sawzall&lt;/a&gt; focused on one of Sawzall’s biggest strengths —&amp;nbsp;it makes it easy to write powerful analysis scripts quickly for tasks like computing statistical aggregates or computing a &lt;a href=&quot;http://www.unofficialgoogledatascience.com/2015/08/an-introduction-to-poisson-bootstrap_26.html&quot;&gt;Poisson bootstrap&lt;/a&gt;. As such, it’s great for writing quick one-off analysis code and iterating on it as we come to a better understanding of the data. The name of the language is suggestive —&amp;nbsp;the actual physical&amp;nbsp;&lt;a href=&quot;https://www.milwaukeetool.com/power-tools/corded/6519-31&quot;&gt;Sawzall&lt;/a&gt;®&amp;nbsp;(trademark Milwaukee Tool) that the language is named after is a versatile hand tool that can make quick work of logs.&lt;br /&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;http://2.bp.blogspot.com/-rK08SSPyGuk/VmIkGErJemI/AAAAAAAAamQ/ZWA403fdQyQ/s1600/Sawzall-Reciprocating-Saw.jpg&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;http://2.bp.blogspot.com/-rK08SSPyGuk/VmIkGErJemI/AAAAAAAAamQ/ZWA403fdQyQ/s400/Sawzall-Reciprocating-Saw.jpg&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Figure 1: A physical Sawzall sawing physical logs.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;Sawzall also has important strengths in another critical area —&amp;nbsp;access control and auditing. The input to analysis jobs often includes personally identifiable information like IP addresses, and there are strict rules that limit what analysts can do with this data. We need to be able to answer several questions about any analysis before it runs:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;Should this analyst have access to this data at all?&amp;nbsp;&lt;/li&gt;&lt;li&gt;If they should have access, which fields should they be able to read? Our input records are &lt;a href=&quot;https://developers.google.com/protocol-buffers/?hl=en&quot;&gt;protocol buffers&lt;/a&gt;, and we’ve annotated the fields of our logged protos to indicate which ones may contain sensitive data (e.g. a user’s IP address) and which ones are innocuous (e.g. the amount of time it took to process a request). Reading sensitive fields requires a strong justification.&lt;/li&gt;&lt;li&gt;If they’re reading sensitive fields, what code are they actually running? We want to be able to audit the actual code that’s being used to do any sensitive analysis.&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;In short, we want fine-grained control over who has access to data, and visibility into what they’re doing with it. Sawzall provided a good solution to all these issues. We ran a centralized service called Sawmill that managed all Sawzall analysis on our logs. &lt;br /&gt;&lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;http://4.bp.blogspot.com/-AyEeYtpw9b0/VmIdtsY1KsI/AAAAAAAAalQ/pfkDvpRTW7M/s1600/sawzall.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;196&quot; src=&quot;http://4.bp.blogspot.com/-AyEeYtpw9b0/VmIdtsY1KsI/AAAAAAAAalQ/pfkDvpRTW7M/s640/sawzall.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot;&gt;Figure 2: In the Sawmill execution environment, users send their Sawzall analysis scripts to Sawmill Server, which performs authorization, applies access filters, and launches a MapReduce job on the user’s behalf in a restricted execution zone where the user isn’t allowed to run arbitrary binaries.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;vertical-align: baseline;&quot;&gt;You could send your Sawzall code to Sawmill, and it would make sure that you have access to the data that you want to analyze. If you do, it would add some code to the beginning of your script to filter out any fields that you don’t have access to and record your script for auditing purposes. Then it would start a MapReduce which runs your Sawzall code on each worker. Since your Sawzall code runs inside a sandbox, it cannot get access to the raw, unfiltered logs data. It only sees filtered input.&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;vertical-align: baseline;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;h2&gt;&lt;span style=&quot;vertical-align: baseline;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/h2&gt;&lt;h2&gt;&lt;span style=&quot;vertical-align: baseline;&quot;&gt;Problems with Sawzall&lt;/span&gt;&lt;/h2&gt;&lt;br /&gt;This setup is great for access control and auditing, but it also creates some problems. Since we’re relying on the Sawzall sandbox to enforce our access policies, we have to make sure that un-sandboxed code doesn’t run alongside our Sawzall analysis. If the analysis could call unsafe code (e.g. user-controlled C++ functions), it could bypass our sandbox and read sensitive fields before they’re filtered. Sawzall does provide a way of calling functions written in other languages as though they were Sawzall functions. These functions are called intrinsics, and they provide a bridge between Sawzall and the rest of the world.&lt;br /&gt;&lt;br /&gt;At Google, intrinsics were commonly used to provide an interface to large, complex C++ libraries and to interact with external services via RPC. However, since intrinsics provide a way to break out of the Sawzall sandbox, each one needed to be carefully vetted for safety before it could be whitelisted for use. As more and more people started using Sawzall, the demand for new intrinsics grew quickly and became a common point of friction for interoperability with services or libraries from other teams within Google.&lt;br /&gt;&lt;br /&gt;The need to prevent arbitrary un-sandboxed code from interacting with Sawzall analysis also put strong constraints on the execution environment where analysis runs. For example, if a user could run arbitrary programs alongside their sandboxed analysis, they would be able to inspect the memory of their Sawzall program and extract unfiltered data that they shouldn’t have access to. To avoid this scenario, we had to reserve compute resources for logs analysis with restrictions on what kinds of programs can be run and who can launch them, making our analysis infrastructure much less flexible.&lt;br /&gt;&lt;br /&gt;These problems were manageable when Sawzall occupied a small, well-contained niche. But as the community using Sawzall became larger and more diverse, the problems became more acute and the limitations of a domain-specific language became more important. &lt;br /&gt;&lt;br /&gt;Sawzall may be an excellent hand tool, but many teams at Google came to need something more akin to heavy industrial machinery. Sawzall is at its best for small, focused analyses. While Sawmill itself is large, sophisticated infrastructure that allows Sawzall analysis to scale up and process vast amounts of data efficiently, Sawzall is not well-suited for building large integrated pipelines with sophisticated testing and release management. Teams built their core business logic in Sawzall, but without an object system or any support for user-defined interfaces it became very hard to manage a large codebase. These problems aren’t unique to Google —&amp;nbsp;other companies that have adopted Sawzall for their analytics needs have reported &lt;a href=&quot;https://www.quantcast.com/blog/language-wrangling-running-googles-sawzall-on-quantcasts-mapreduce-cluster/&quot;&gt;similar difficulties&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;Sawzall likely could have continued as a small, niche language, but it was sufficiently useful that people wanted much more out of it, and those needs grew beyond what the language and its associated access control and execution model could provide.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;&lt;br /&gt;&lt;/h2&gt;&lt;h2&gt;Language-Agnostic Analysis&lt;/h2&gt;&lt;br /&gt;The first step toward solving these problems was removing the tight link between access controls on logs data and the Sawzall execution model. By placing these controls outside of the Sawzall sandbox, we can open the door for analysis written in any language without weakening our ability to control access to sensitive data.&lt;br /&gt;&lt;br /&gt;If we allow users to run arbitrary un-sandboxed code on the data, we have to change the model for how we filter out sensitive fields. Once the data gets to the user’s binary, it’s too late for filtering. We therefore need a separate service that proxies access to the raw data and enforces our access control policies before the data ever makes its way to analysts.&lt;br /&gt;&lt;br /&gt;We’ve built just such a system, called the &lt;b&gt;logs proxy&lt;/b&gt;. It provides a language-agnostic interface for reading logs data, and it applies all the necessary filtering logic before sending the data along to clients.  There are a few interesting wrinkles to this process (for example, what if I want to do a join that’s keyed by a field that will be filtered out?), and we’ve had to solve some tough performance optimization problems to handle the scale of analysis at Google, but the fundamental idea is very simple.&lt;br /&gt;&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;http://4.bp.blogspot.com/-5wq6FNPUAa4/VmIetDDj6AI/AAAAAAAAal0/IGWCB4oGfXE/s1600/lingo.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;180&quot; src=&quot;http://4.bp.blogspot.com/-5wq6FNPUAa4/VmIetDDj6AI/AAAAAAAAal0/IGWCB4oGfXE/s640/lingo.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot;&gt;Figure 3: In the logs proxy execution environment, user analysis code never has direct access to logs data. No restricted zone is necessary, because the logs proxy filters out sensitive fields before they’re available to analysis code.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Since the logs proxy decouples our data access policy from the programming language used for analysis, individual teams now have more freedom to choose the language that best fits their needs. However, since analysis libraries can often get very complicated, and multiple teams often share common data sources, there is an economy of scale in choosing a common language for most analysis.&lt;br /&gt;&lt;br /&gt;At Google, most Sawzall analysis has been replaced by &lt;a href=&quot;https://golang.org/&quot;&gt;Go&lt;/a&gt;. Go has the advantage of being a relatively small language which is easy to learn and integrates well with Google’s production infrastructure. Fast compile times and garbage collection make Go a natural fit for iterative development. To ease the process of migrating from Sawzall, we’ve developed a set of Go libraries that we call &lt;b&gt;Lingo&lt;/b&gt; (for Logs in Go). Lingo includes a table aggregation library that brings the powerful features of Sawzall aggregation tables to Go, using reflection to support user-defined types for table keys and values. It also provides default behavior for setting up and running a MapReduce that reads data from the logs proxy. The result is that Lingo analysis code is often as concise and simple as (and sometimes simpler than) the Sawzall equivalent.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;As an example, consider the spam classification task from an &lt;a href=&quot;http://www.unofficialgoogledatascience.com/2015/09/on-procedural-and-declarative.html&quot;&gt;earlier post on Sawzall&lt;/a&gt; on this site, where the goal is to measure the impact of two versions of a spam classifier on different websites. Here’s how that code looks in Lingo:&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;span style=&quot;color: magenta; font-family: Courier New, Courier, monospace;&quot;&gt;package&lt;/span&gt;&lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt; spamcount&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;&lt;br /&gt;&lt;span style=&quot;color: magenta;&quot;&gt;import&lt;/span&gt; (&lt;br /&gt;&lt;span style=&quot;color: #a64d79;&quot;&gt;&amp;nbsp; &quot;google/spam&quot;&lt;br /&gt;&amp;nbsp; &quot;google/table&quot;&lt;br /&gt;&amp;nbsp; &quot;google/webpage&quot;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;font-family: &#39;Courier New&#39;, Courier, monospace;&quot;&gt;)&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;&lt;br /&gt;&lt;span style=&quot;color: #e06666;&quot;&gt;// For each site, track whether or not it’s spam according to&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;&lt;span style=&quot;color: #e06666;&quot;&gt;// the old&amp;nbsp;and new spam scores.&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;color: magenta; font-family: &#39;Courier New&#39;, Courier, monospace;&quot;&gt;type&lt;/span&gt;&lt;span style=&quot;font-family: &#39;Courier New&#39;, Courier, monospace;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: #6aa84f; font-family: &#39;Courier New&#39;, Courier, monospace;&quot;&gt;SpamCount&lt;/span&gt;&lt;span style=&quot;font-family: &#39;Courier New&#39;, Courier, monospace;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color: magenta; font-family: &#39;Courier New&#39;, Courier, monospace;&quot;&gt;struct&lt;/span&gt;&lt;span style=&quot;font-family: &#39;Courier New&#39;, Courier, monospace;&quot;&gt; {&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;&amp;nbsp; Old int&lt;br /&gt;&amp;nbsp; New int&lt;br /&gt;&amp;nbsp; URLs int&lt;br /&gt;}&lt;br /&gt;&lt;br /&gt;&lt;span style=&quot;color: magenta;&quot;&gt;func&lt;/span&gt; &lt;span style=&quot;color: #0b5394;&quot;&gt;spamCount&lt;/span&gt;(score &lt;span style=&quot;color: #6aa84f;&quot;&gt;float&lt;/span&gt;) &lt;span style=&quot;color: #6aa84f;&quot;&gt;int&lt;/span&gt; {&lt;br /&gt;&lt;span style=&quot;color: #e06666;&quot;&gt;&amp;nbsp; // A record with a spam score above 0.5 counts as spam.&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;&amp;nbsp;&amp;nbsp;&lt;span style=&quot;color: magenta;&quot;&gt;if&lt;/span&gt; score &amp;gt; 0.5 {&lt;br /&gt;&amp;nbsp; &amp;nbsp; &lt;span style=&quot;color: magenta;&quot;&gt;return&lt;/span&gt; 1&lt;br /&gt;&amp;nbsp; }&lt;br /&gt;&amp;nbsp; &lt;span style=&quot;color: magenta;&quot;&gt;return&lt;/span&gt; 0&lt;br /&gt;}&lt;br /&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;color: #e06666; font-family: Courier New, Courier, monospace;&quot;&gt;// stats is a sum table with string keys (site name), and&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;color: #e06666; font-family: Courier New, Courier, monospace;&quot;&gt;// SpamCount values (the old and new spam counts and total&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;&lt;span style=&quot;color: #e06666;&quot;&gt;// count of URLs).&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;&lt;span style=&quot;color: magenta;&quot;&gt;var&lt;/span&gt; stats = table.&lt;span style=&quot;color: #0b5394;&quot;&gt;Sum&lt;/span&gt;(&lt;span style=&quot;color: #a64d79;&quot;&gt;&quot;my_stats&quot;&lt;/span&gt;, &lt;span style=&quot;color: #a64d79;&quot;&gt;&quot;site&quot;&lt;/span&gt;, &lt;span style=&quot;color: #6aa84f;&quot;&gt;SpamCount&lt;/span&gt;{})&lt;br /&gt;&lt;br /&gt;&lt;span style=&quot;color: magenta;&quot;&gt;func&lt;/span&gt; &lt;span style=&quot;color: #0b5394;&quot;&gt;Mapper&lt;/span&gt;(w *&lt;span style=&quot;color: #6aa84f;&quot;&gt;webpage.WebPage&lt;/span&gt;) {&lt;br /&gt;&lt;span style=&quot;color: #e06666;&quot;&gt;&amp;nbsp; // Each record is a protocol buffer of type WebPage, which&lt;br /&gt;&amp;nbsp; // has a url field which the spam package can classify.&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;font-family: &#39;Courier New&#39;, Courier, monospace;&quot;&gt;&amp;nbsp;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;font-family: &#39;Courier New&#39;, Courier, monospace;&quot;&gt;&amp;nbsp; stats.&lt;/span&gt;&lt;span style=&quot;color: #0b5394; font-family: &#39;Courier New&#39;, Courier, monospace;&quot;&gt;Emit&lt;/span&gt;&lt;span style=&quot;font-family: &#39;Courier New&#39;, Courier, monospace;&quot;&gt;(sites.&lt;/span&gt;&lt;span style=&quot;color: #0b5394; font-family: &#39;Courier New&#39;, Courier, monospace;&quot;&gt;SiteFromURL&lt;/span&gt;&lt;span style=&quot;font-family: &#39;Courier New&#39;, Courier, monospace;&quot;&gt;(w.&lt;/span&gt;&lt;span style=&quot;color: #0b5394; font-family: &#39;Courier New&#39;, Courier, monospace;&quot;&gt;GetUrl&lt;/span&gt;&lt;span style=&quot;font-family: &#39;Courier New&#39;, Courier, monospace;&quot;&gt;()), &lt;/span&gt;&lt;span style=&quot;color: #6aa84f; font-family: &#39;Courier New&#39;, Courier, monospace;&quot;&gt;SpamCount&lt;/span&gt;&lt;span style=&quot;font-family: &#39;Courier New&#39;, Courier, monospace;&quot;&gt;{&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;&amp;nbsp; &amp;nbsp; &lt;span style=&quot;color: #45818e;&quot;&gt;Old&lt;/span&gt;: &lt;span style=&quot;color: #0b5394;&quot;&gt;spamCount&lt;/span&gt;(spam.&lt;span style=&quot;color: #0b5394;&quot;&gt;SpamScore&lt;/span&gt;(w.&lt;span style=&quot;color: #0b5394;&quot;&gt;GetUrl&lt;/span&gt;())),&lt;br /&gt;&amp;nbsp; &amp;nbsp; &lt;span style=&quot;color: #45818e;&quot;&gt;New&lt;/span&gt;: &lt;span style=&quot;color: #0b5394;&quot;&gt;spamCount&lt;/span&gt;(spam.&lt;span style=&quot;color: #0b5394;&quot;&gt;NewSpamScore&lt;/span&gt;(w.&lt;span style=&quot;color: #0b5394;&quot;&gt;GetUrl&lt;/span&gt;())),&lt;br /&gt;&amp;nbsp; &amp;nbsp; &lt;span style=&quot;color: #45818e;&quot;&gt;URLs&lt;/span&gt;: 1,&lt;br /&gt;&amp;nbsp; })&lt;br /&gt;}&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;The structure of this Lingo program is very similar to &lt;a href=&quot;http://www.unofficialgoogledatascience.com/2015/09/on-procedural-and-declarative.html&quot;&gt;its Sawzall equivalent&lt;/a&gt;, thanks to the table library. It outputs a table of summed spam scores, keyed by site names. The table library uses the same output encoding as Sawzall, so the output of this program is byte-for-byte identical to its Sawzall equivalent. This greatly simplifies the process of migrating away from Sawzall for interested teams.&lt;br /&gt;&lt;br /&gt;The benefit of this work is that logs analysis is now much more flexible and better integrated into Google’s broader software ecosystem. The logs proxy has decoupled the choice of language from the execution and access control model for analysis, which gives teams the freedom to make their own determination about what language best suits their needs. &lt;br /&gt;&lt;br /&gt;&lt;h2&gt;&lt;br /&gt;&lt;/h2&gt;&lt;h2&gt;Conclusion&lt;/h2&gt;&lt;br /&gt;Moving away from Sawzall has been a huge job. In part that’s because Sawzall was quite successful at its original goal —&amp;nbsp;make it easy for analysts to write quick, powerful analysis programs. As a result there was a lot of Sawzall code to be migrated. However, Sawzall was in some ways a victim of its own success. There’s a natural tension for any domain-specific language between staying highly focused on its problem domain and growing to accommodate the needs of users who want to stretch the language in new directions. Sawzall’s development was shaped by this tension from the very beginning —&amp;nbsp;early designs didn’t even include the ability to define functions, but functions were quickly added when it became apparent that the language couldn’t meet users’ needs without them. Over time, many more features have been added. But as the language grows, the rationale for using a domain specific language rather than a general purpose language becomes more and more diluted.&lt;br /&gt;&lt;br /&gt;Fortunately, we’ve found that with carefully designed libraries we can get most of the benefits of Sawzall in Go while gaining the advantages of a powerful general-purpose language. The overall response of analysts to these changes has been extremely positive. Today, logs analysis is one of the most intensive users of Go at Google, and Go is the most-used language for reading logs through the logs proxy. And for users who prefer a different language, the logs proxy provides a language-agnostic way to read logs data while complying with our access policies. Looking forward, we can’t predict exactly what direction logs analysis at Google will go next, but we do know that its path won’t be constrained by our choice of programming language.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;</content><link rel='replies' type='application/atom+xml' href='https://www.unofficialgoogledatascience.com/feeds/4651288390164769572/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='https://www.unofficialgoogledatascience.com/2015/12/replacing-sawzall-case-study-in-domain.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/4651288390164769572'/><link rel='self' type='application/atom+xml' href='https://www.blogger.com/feeds/5825758052688213474/posts/default/4651288390164769572'/><link rel='alternate' type='text/html' href='https://www.unofficialgoogledatascience.com/2015/12/replacing-sawzall-case-study-in-domain.html' title='Replacing Sawzall — a case study in domain-specific language migration'/><author><name>Amir Najmi</name><uri>http://www.blogger.com/profile/18174523203317227640</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://2.bp.blogspot.com/-rK08SSPyGuk/VmIkGErJemI/AAAAAAAAamQ/ZWA403fdQyQ/s72-c/Sawzall-Reciprocating-Saw.jpg" height="72" width="72"/><thr:total>0</thr:total></entry></feed>