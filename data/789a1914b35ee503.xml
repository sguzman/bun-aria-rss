<?xml version='1.0' encoding='UTF-8'?><?xml-stylesheet href="http://www.blogger.com/styles/atom.css" type="text/css"?><feed xmlns='http://www.w3.org/2005/Atom' xmlns:openSearch='http://a9.com/-/spec/opensearchrss/1.0/' xmlns:blogger='http://schemas.google.com/blogger/2008' xmlns:georss='http://www.georss.org/georss' xmlns:gd="http://schemas.google.com/g/2005" xmlns:thr='http://purl.org/syndication/thread/1.0'><id>tag:blogger.com,1999:blog-9149402429183581490</id><updated>2022-10-18T04:43:58.535-07:00</updated><title type='text'>The Angry Statistician</title><subtitle type='html'></subtitle><link rel='http://schemas.google.com/g/2005#feed' type='application/atom+xml' href='http://angrystatistician.blogspot.com/feeds/posts/default'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default'/><link rel='alternate' type='text/html' href='http://angrystatistician.blogspot.com/'/><link rel='hub' href='http://pubsubhubbub.appspot.com/'/><link rel='next' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default?start-index=26&amp;max-results=25'/><author><name>Christopher D. Long</name><uri>http://www.blogger.com/profile/13687149457345266350</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><generator version='7.00' uri='http://www.blogger.com'>Blogger</generator><openSearch:totalResults>79</openSearch:totalResults><openSearch:startIndex>1</openSearch:startIndex><openSearch:itemsPerPage>25</openSearch:itemsPerPage><entry><id>tag:blogger.com,1999:blog-9149402429183581490.post-8245799372002363637</id><published>2021-09-10T23:53:00.003-07:00</published><updated>2021-09-11T01:04:25.687-07:00</updated><title type='text'>Probability and Cumulative Dice Sums</title><content type='html'>&lt;p&gt;Let a die be labeled with increasing positive integers \(a_1,\ldots , a_n\), and let the probability of getting \(a_i\) be \(p_i&amp;gt;0\). We start at 0 and roll the die, adding whatever number we get to the current total. If \({\rm Pr}(N)\) is the probability that at some point we achieve the sum \(N\), then \(\lim_{N \to \infty} {\rm Pr}(N)\) exists and equals \(1/\rm{E}(X)\) iff \((a_1, \ldots, a_n) = 1\).&lt;/p&gt;&lt;p&gt;The direction \(\implies\) is obvious. Now, if the limit exists it must equal \(1/{\rm E}(X)\) by Chebyshev&#39;s inequality, so we only need to show that the limit exists assuming that \((a_1, \ldots, a_n) = 1\).&lt;/p&gt;&lt;p&gt;We have the recursive relationship \[{\rm Pr}(N) = p_1 {\rm Pr}(N-a_1) + \ldots + p_n {\rm Pr}(N-a_n);\] the characteristic polynomial is therefore \[f(x) = x^{a_n} - \left(p_1 x^{(a_n-a_1)} + \ldots + p_n\right).\]&lt;/p&gt;&lt;p&gt;This clearly has the root \(x=1\). Next note \[ f&#39;(1) = a_n - \sum_{i=1}^{n} p_i a_n + \sum_{i=1}^{n} p_i a_i =&amp;nbsp;\rm{E}(X) &amp;gt; 0 ,\] hence this root is also unique.&lt;/p&gt;&lt;p&gt;&amp;nbsp;I&#39;ll now show that all other roots have absolute value \(&amp;lt; 1\).&lt;/p&gt;&lt;p&gt;We have \[ x^{a_n} - \left( p_1 x^{(a_n-a_1)} + \ldots + p_n \right) = 0 \iff x^{a_1} = p_1 + \ldots&amp;nbsp; + \frac{p_n}{x^{(a_n-a_1)}} .\]&lt;/p&gt;&lt;p&gt;If \(|x|&amp;gt;1\) then \[ |x^{a_1}| = \left|p_1 + \ldots + \frac{p_n}{x^{(a_n-a_1)}}\right| \leq |p_1| + \ldots + \left|\frac{p_n}{x^{(a_n-a_1)}}\right| \leq p_1 + \ldots + p_n = 1;\] contradiction.&lt;/p&gt;&lt;p&gt;If \(|x|=1\) \[ |x^{a_1}| = \left|p_1 + \ldots + \frac{p_n}{x^{(a_n-a_1)}}\right| &amp;lt; 1\] unless \( x^{(a_1 - a_i)} = 1 \) for all \( i \implies x^d = 1 \) where \( d = (a_1 - a_2, \ldots, a_1 - a_n) \).&lt;/p&gt;&lt;p&gt;Assume \(x^d = 1\); then \[ x^{a_1} = p_1 + \ldots + \frac{p_n}{x^{(a_n-a_1)}} = p_1 + \ldots + p_n = 1 \implies x^{a_1} = 1 \implies x^e = 1\] where \( e = (d, a_1) = (a_1, \ldots, a_n) = 1\) by assumption, hence \( x^1 = 1 \implies x = 1 \) is the only root of \( f(x) \) such that \( |x| \geq 1 \implies \lim_{N \to \infty} {\rm Pr}(N) \) exists.&lt;/p&gt;</content><link rel='replies' type='application/atom+xml' href='http://angrystatistician.blogspot.com/feeds/8245799372002363637/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://angrystatistician.blogspot.com/2021/09/probability-and-cumulative-dice-sums.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/8245799372002363637'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/8245799372002363637'/><link rel='alternate' type='text/html' href='http://angrystatistician.blogspot.com/2021/09/probability-and-cumulative-dice-sums.html' title='Probability and Cumulative Dice Sums'/><author><name>Christopher D. Long</name><uri>http://www.blogger.com/profile/13687149457345266350</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-9149402429183581490.post-656654543676154183</id><published>2019-09-04T02:14:00.002-07:00</published><updated>2019-09-04T03:05:04.780-07:00</updated><title type='text'>Gambling to Optimize Expected Median Bankroll</title><content type='html'>Gambling to optimize your expected bankroll mean is extremely risky, as you wager your entire bankroll for any favorable gamble, making ruin almost inevitable. But what if, instead, we gambled not to maximize the expected bankroll mean, but the expected bankroll median?&lt;br /&gt;&lt;br /&gt;Let the probability of winning a favorable bet be \(p\), and the net odds be \(b\). That is, if we wager \(1\) unit and win, we get back \(b\) units (in addition to our wager). Assume our betting strategy is to wager some fraction \(f\) of our bankroll, hence \(0 \leq f \leq 1\). By our assumption, our betting strategy is invariant with respect to the actual size of our bankroll, and so if we were to repeat this gamble \(n\) times with the same \(p\) and \(b\), the strategy wouldn&#39;t change. It follows we may assume an initial bankroll of size \(1\).&lt;br /&gt;&lt;br /&gt;Let \( q = 1-p \). Now, after \(n\)&amp;nbsp; such gambles our bankroll would have a binomial distribution with probability mass function \[ \Pr(k,n,p) = \binom{n}{k} p^k q^{n-k}, \] where \(k\) is the number of wins, \(n-k\) the number of losses. Note the median occurs at \( k=n p \), corresponding to a bankroll of \[ \left(1+f\cdot b\right)^{n p} \left(1-f\right)^{n q} .\] Now, maximizing this value is equivalent to maximizing its \(\log\), which is&amp;nbsp;\[ n p \log\left(1+f\cdot b\right) + n q\log\left(1-f\right) .\] But this is maximized when&amp;nbsp;\[ p \log\left(1+f\cdot b\right) + q\log\left(1-f\right)\] is maximized, and this is precisely the condition for a Kelly optimal bet! It follows that if we gamble to optimize our expected median, this is equivalent to Kelly optimal betting, and hence maximizing expected log wealth.&lt;br /&gt;&lt;br /&gt;With a little more work, we can show that the same conclusion holds if we gamble to optimize any expected quantile \(x\), with \( 0 &amp;lt; x &amp;lt; 1\). Maximizing the expected quantile \( 0 \) corresponds to &quot;riskless&quot; gambling, i.e. only gambling when there&#39;s no chance of a loss.&amp;nbsp;Maximizing the expected quantile \( 1 \) corresponds to maximizing the expected bankroll mean, which we can refer to as the &quot;reckless&quot; strategy. Thus, under our assumptions, there are only three quantile maximization strategies - riskless, Kelly and reckless.</content><link rel='replies' type='application/atom+xml' href='http://angrystatistician.blogspot.com/feeds/656654543676154183/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://angrystatistician.blogspot.com/2019/09/gambling-to-optimize-expected-median.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/656654543676154183'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/656654543676154183'/><link rel='alternate' type='text/html' href='http://angrystatistician.blogspot.com/2019/09/gambling-to-optimize-expected-median.html' title='Gambling to Optimize Expected Median Bankroll'/><author><name>Christopher D. Long</name><uri>http://www.blogger.com/profile/13687149457345266350</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-9149402429183581490.post-2713056191292143675</id><published>2018-04-08T16:59:00.000-07:00</published><updated>2018-04-08T20:45:10.782-07:00</updated><title type='text'>An Island of Liars is an Ensemble of Experts</title><content type='html'>In my previous post I looked at how a group of experts may be combined into a single, more powerful, classifier which I call &lt;a href=&quot;https://angrystatistician.blogspot.com/2018/04/combining-expert-opinions-naiveboost.html&quot; target=&quot;_blank&quot;&gt;NaiveBoost&lt;/a&gt;&amp;nbsp;after the related &lt;a href=&quot;https://en.wikipedia.org/wiki/AdaBoost&quot; target=&quot;_blank&quot;&gt;AdaBoost&lt;/a&gt;. I&#39;ll illustrate how it can be used with a few examples.&lt;br /&gt;&lt;br /&gt;As before, we&#39;re faced with making a binary decision, which we can view as an unknown label \( L \in \{ +1, -1 \}\). Furthermore, the prior distribution on \( L \) is assumed to be uniform. Let our experts&#39; independent probabilities be \( p_1 = 0.8, p_2 = 0.7, p_3 = 0.6\) and \(p_4 = 0.5\). Our combined NaiveBoost classifier is&amp;nbsp;\[ C(S) = \sum_i \frac{L_i}{2}\log{\left( \frac{p_i}{1-p_i}\right)},\] where \( S = \{ L_i \} \). A few things to note are that \( \log{\left( \frac{p_i}{1-p_i}\right)} \) is \( {\rm logit}( p_i )\), and an expert with \( p = 0.5 \) contributes 0 to our classifier. This latter observation is what we&#39;d expect, as \( p = 0.5 \) is random guessing. Also, experts with probabilities \( p_i \) and \( p_j \) such that \( p_i = 1 - p_j \) are equivalent if we replace the stated label \( L_j \) with \( -L_j \).&lt;br /&gt;&lt;br /&gt;Ignoring the last (uninformative) expert, we end up with the combined classifier \[ C(S) = \frac{L_1}{2} \log\left(4\right) +&amp;nbsp;\frac{L_1}{2} \log\left(\frac{7}{3}\right) + \frac{L_3}{2} \log\left( \frac{3}{2} \right).\] If the overall value is positive, the classifier&#39;s label is \( L = +1\); if it&#39;s negative, the classifier&#39;s label is \( L = -1 \). Note the base of the logarithm doesn&#39;t matter and we could also ignore the factor of \( \frac{1}{2} \), as these don&#39;t change the sign of \( C(S) \). However, the factor of \( \frac{1}{2} \) must be left in if we want the ability to properly recover the actual combined probability via normalization.&lt;br /&gt;&lt;br /&gt;Now say \( L_1 = -1, L_2 = +1, L_3 = +1 \). What&#39;s our decision? Doing the math, we get \( C(S) = \frac{1}{2} \log{ \left( \frac{7}{8} \right) } \), and as \( 7 &amp;lt; 8 \), \( C(S) &amp;lt; 0 \) and our combined classifier says \( L = -1\). If we wanted to recover the probability, note \[ \exp\left( \frac{1}{2} \log \left( \frac{7}{8} \right) \right) = {\left( \frac{7}{8} \right)}^{1/2},\] hence our classifier states \[ {\rm Pr}(L = +1 | S ) = \frac{ {\left( \frac{7}{8} \right)}^{1/2} }{ {\left( \frac{7}{8} \right)}^{1/2} + {\left( \frac{7}{8} \right)}^{-1/2} } = \frac{ \frac{7}{8} }{ \frac{7}{8} + 1 } = \frac{7}{15}, \] and of course \( {\rm Pr}(L = -1 | S ) =&amp;nbsp;\frac{8}{15}\).&lt;br /&gt;&lt;br /&gt;As a second example, consider the&amp;nbsp;&lt;a href=&quot;https://twitter.com/CutTheKnotMath&quot; target=&quot;_blank&quot;&gt;@CutTheKnotMath&lt;/a&gt;&amp;nbsp;puzzle of &lt;a href=&quot;https://twitter.com/CutTheKnotMath/status/982652920385634304&quot; target=&quot;_blank&quot;&gt;two liars on an island&lt;/a&gt;. Here we have A and B, each of which lies with probability 2/3 and tells the truth with probability 1/3. A makes a statement and B confirms that it&#39;s true. What&#39;s the probability that A&#39;s statement is actually truthful? We can solve this in a complicated way by observing that this is equivalent to an ensemble of experts, where \( L \in \{ +1, -1 \} \), the prior on \( L \) is uniform and \( L_1 = L_2 = +1\). The probability that \( L = +1 \) is precisely the probability that A is telling the truth.&lt;br /&gt;&lt;br /&gt;Following the first example, \[ L_{+1} = \frac{L_1}{2}\log\left( \frac{1}{2} \right) +&amp;nbsp;\frac{L_2}{2}\log\left( \frac{1}{2} \right) = \log\left( \frac{1}{2} \right).\] Continuing as before, we get&amp;nbsp;\[ {\rm Pr}(L = +1 | S ) = \frac{ \frac{1}{2} }{ \frac{1}{2} + 2 } = \frac{1}{5}.\]</content><link rel='replies' type='application/atom+xml' href='http://angrystatistician.blogspot.com/feeds/2713056191292143675/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://angrystatistician.blogspot.com/2018/04/an-island-of-liars-is-ensemble-of.html#comment-form' title='3 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/2713056191292143675'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/2713056191292143675'/><link rel='alternate' type='text/html' href='http://angrystatistician.blogspot.com/2018/04/an-island-of-liars-is-ensemble-of.html' title='An Island of Liars is an Ensemble of Experts'/><author><name>Christopher D. Long</name><uri>http://www.blogger.com/profile/13687149457345266350</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><thr:total>3</thr:total></entry><entry><id>tag:blogger.com,1999:blog-9149402429183581490.post-933839510170584773</id><published>2018-04-07T22:18:00.000-07:00</published><updated>2018-04-08T11:43:28.169-07:00</updated><title type='text'>Combining Expert Opinions: NaiveBoost</title><content type='html'>In many situations we&#39;re faced with multiple expert opinions. How should we combine them together into one opinion, hopefully better than any single opinion? I&#39;ll demonstrate the derivation of a classifier I&#39;ll call NaiveBoost.&lt;br /&gt;&lt;br /&gt;We&#39;ll start with a simple situation, and later gradually introduce more complexity. Let each expert state a yes or no opinion in response to a yes/no question (binary classifiers), each expert be independent of the other experts and assume expert \(i\) is correct with probability \(p_i\). We&#39;ll also assume that the prior distribution on whether the correct answer is yes or no to be uniform, i.e. each occurs with probability 0.5.&lt;br /&gt;&lt;br /&gt;Label a &quot;yes&quot; as +1, and &quot;no&quot; as -1. We ask our question, which has some unknown +1/-1 answer \(L\), and get back a set of responses (labels) \(S = \{L_i \}\), where \(L_i\) is the response from expert \(i\). Observe we have \[ \Pr(S | L=+1) = \prod_{i} {p_i}^{\frac{L_i+1}{2}} \cdot {(1-p_i)}^\frac{-L_i+1}{2}\] and also \[ \Pr(S | L=-1) = \prod_{i} {(1-p_i)}^{\frac{L_i+1}{2}} \cdot {p_i}^\frac{-L_i+1}{2}. \] As \( \Pr(L=+1 | S) = \frac{\Pr(S | L=+1)\cdot \Pr(L=+1)}{\Pr(S)}\) and&amp;nbsp;\( \Pr(L=-1 | S) = \frac{\Pr(S | L=-1)\cdot \Pr(L=-1)}{\Pr(S)}\), and given our assumption that \( \Pr(L=+1) =&amp;nbsp;\Pr(L=-1) \), we need only compute \(&amp;nbsp;\Pr(S | L=+1) \),&amp;nbsp;\( \Pr(S | L=-1) \) and normalize.&lt;br /&gt;&lt;br /&gt;We&#39;ll now take logs and derive a form similar to &lt;a href=&quot;https://en.wikipedia.org/wiki/AdaBoost&quot; target=&quot;_blank&quot;&gt;AdaBoost&lt;/a&gt;. Note for \( L_{+1} = \log\left( \Pr(S | L=+1) \right) \) this gives us \[ L_{+1} = \sum_i \frac{L_i+1}{2}\log{(p_i)} +&amp;nbsp;\frac{-L_i+1}{2}\log{(1-p_i)}.\] Rearranging, we get \[ L_{+1} = \sum_i \frac{L_i}{2}\log{\left( \frac{p_i}{1-p_i}\right)} +&amp;nbsp;\frac{1}{2}\log{\left( p_i(1-p_i)\right)}.\] Similarly, for&amp;nbsp;\( L_{-1} = \log\left( \Pr(S | L=-1) \right) \) we get&amp;nbsp;\[ L_{-1} = \sum_i -\frac{L_i}{2}\log{\left( \frac{p_i}{1-p_i}\right)} + \frac{1}{2}\log{\left( p_i(1-p_i)\right)}.\] Note that each of these includes the same terms \( \sum_i&amp;nbsp;\frac{1}{2}\log{\left( p_i(1-p_i)\right)}\). Upon exponentiation these would multiply \( \Pr(S | L=+1) \) and \( \Pr(S | L=-1) \) by the same factor, so we can ignore them as to recover the probabilities we&#39;ll need to normalize anyway. Thus we end up with a linear classifier with the AdaBoost form \[ C(S) = \sum_i \frac{L_i}{2}\log{\left( \frac{p_i}{1-p_i}\right)}. \] If \( C(S) \) is positive, the classifier&#39;s label is +1; if \( C(S) \) is negative, the classifier&#39;s label is -1. Furthermore, we may recover the classifier&#39;s probabilities by&amp;nbsp;exponentiating and normalizing.</content><link rel='replies' type='application/atom+xml' href='http://angrystatistician.blogspot.com/feeds/933839510170584773/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://angrystatistician.blogspot.com/2018/04/combining-expert-opinions-naiveboost.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/933839510170584773'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/933839510170584773'/><link rel='alternate' type='text/html' href='http://angrystatistician.blogspot.com/2018/04/combining-expert-opinions-naiveboost.html' title='Combining Expert Opinions: NaiveBoost'/><author><name>Christopher D. Long</name><uri>http://www.blogger.com/profile/13687149457345266350</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-9149402429183581490.post-8133679730007651996</id><published>2018-04-01T00:14:00.001-07:00</published><updated>2018-04-01T00:14:09.990-07:00</updated><title type='text'>Simplified Multinomial Kelly</title><content type='html'>Here&#39;s a simplified version for optimal Kelly bets when you have multiple outcomes (e.g. horse races).&lt;br /&gt;&lt;br /&gt;The Smoczynski &amp;amp; Tomkins algorithm, which is explained here (or in the original paper):&lt;br /&gt;&lt;br /&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Kelly_criterion#Multiple_horses&quot;&gt;https://en.wikipedia.org/wiki/Kelly_criterion#Multiple_horses&lt;/a&gt;&lt;br /&gt;&lt;br /&gt;Let&#39;s say there&#39;s a wager that, for every $1 you bet, will return a profit of $b if you win. Let the probability of winning be \(p\), and losing be \(q=1-p\).&lt;br /&gt;&lt;br /&gt;The original Kelly criterion says to wager only if \(b\cdot p-q &amp;gt; 0\) (the expected value is positive), and in this case to wager a fraction \( \frac{b\cdot p-q}{b} \) of your bankroll.&lt;br /&gt;&lt;br /&gt;But in a horse race, how do you decide which set of outcomes are favorable to bet on? It&#39;s tricky, because these wagers are mutually exclusive i.e. you can win at most one.&lt;br /&gt;&lt;br /&gt;It turns out there&#39;s a simple and intuitive method to find which bets are favorable:&lt;br /&gt;&lt;br /&gt;1) Look at \( b\cdot p-q\) for every horse.&lt;br /&gt;2) Pick any horse for which \( b\cdot p-q &amp;gt; 0\) and mark &quot;bet&quot;.&lt;br /&gt;3) Adjust the probabilities for the remaining horses by dividing all win probabilities by \( \frac{1}{1-p} \) so they add up to 1 again (&quot;renormalize&quot;).&lt;br /&gt;4) Repeat!&lt;br /&gt;&lt;br /&gt;That&#39;s it.&lt;br /&gt;&lt;br /&gt;This should be substantially easier to understand than the exposition in Smoczynski &amp;amp; Tomkins.&lt;br /&gt;&lt;br /&gt;The intuitive reasoning for why this should work is that you only need betting on a horse to be conditionally favorable assuming the other horses you&#39;ve bet on don&#39;t win. That is, it must be a positive hedge.&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;</content><link rel='replies' type='application/atom+xml' href='http://angrystatistician.blogspot.com/feeds/8133679730007651996/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://angrystatistician.blogspot.com/2018/04/simplified-multinomial-kelly.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/8133679730007651996'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/8133679730007651996'/><link rel='alternate' type='text/html' href='http://angrystatistician.blogspot.com/2018/04/simplified-multinomial-kelly.html' title='Simplified Multinomial Kelly'/><author><name>Christopher D. Long</name><uri>http://www.blogger.com/profile/13687149457345266350</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-9149402429183581490.post-6047131242088369324</id><published>2017-12-24T00:27:00.001-08:00</published><updated>2017-12-24T00:32:52.103-08:00</updated><title type='text'>Notes on Setting up a Titan V under Ubuntu 17.04</title><content type='html'>I recently purchased a Titan V GPU to use for machine and deep learning, and in the process of installing the latest Nvidia driver&#39;s hosed my Ubuntu 16.04 install. I was overdue for a fresh install of Linux, anyway, so I decided to upgrade some of my drives at the same time. Here are some of my notes for the process I went through to get the Titan V working perfectly with TensorFlow 1.5 under Ubuntu 17.04.&lt;br /&gt;&lt;br /&gt;Old install:&lt;br /&gt;Ubuntu 16.04&lt;br /&gt;EVGA GeForce GTX Titan SuperClocked 6GB&lt;br /&gt;2TB Seagate NAS HDD&lt;br /&gt;+ additional drives&lt;br /&gt;&lt;br /&gt;New install:&lt;br /&gt;Ubuntu 17.04&lt;br /&gt;Titan V 12GB&lt;br /&gt;/ partition on a 250GB Samsung 840 Pro SSD (had an extra around)&lt;br /&gt;/home partition on a new 1TB Crucial MX500 SSD&lt;br /&gt;New WD Blue 4TB HDD&lt;br /&gt;+ additional drives&lt;br /&gt;&lt;br /&gt;You&#39;ll need to install Linux in legacy mode, not UEFI, in order to use Nvidia&#39;s proprietary drivers for the Titan V. Note that Linux will cheerfully boot in UEFI mode, but will not load any proprietary drivers (including Nvidia&#39;s). You&#39;ll need proprietary drivers for TensorFlow.&lt;br /&gt;&lt;br /&gt;You may also need to disable fast boot.&lt;br /&gt;&lt;br /&gt;Keep a wired mouse handy, as your wireless mouse may decide to stop working until Linux is installed and updated. This occurred with my Logitech MX Master.&lt;br /&gt;&lt;br /&gt;Create an Ubuntu 17.04 live install USB - &lt;a href=&quot;https://help.ubuntu.com/community/Installation/FromUSBStick&quot;&gt;https://help.ubuntu.com/community/Installation/FromUSBStick&lt;/a&gt;&lt;br /&gt;&lt;br /&gt;Boot from your live Ubuntu USB with the BIOS in legacy mode.&lt;br /&gt;&lt;br /&gt;I selected the Samsung 850 Pro as my / and the Crucial MX500 as /home. You&#39;ll need to &quot;create&quot; and &quot;add&quot; if they&#39;re unformatted.&lt;br /&gt;&lt;br /&gt;Allow 3rd party/proprietary drivers.&lt;br /&gt;&lt;br /&gt;Install!&lt;br /&gt;&lt;br /&gt;Reboot, login to default Ubuntu (logging in to Unity may hang; it did on my system). The Titan V is not configured yet.&lt;br /&gt;&lt;br /&gt;Update Ubuntu:&lt;br /&gt;sudo apt-get update&lt;br /&gt;sudo apt-get upgrade&lt;br /&gt;&lt;br /&gt;I recommend installing the latest kernel.&lt;br /&gt;apt-get install linux-generic linux-headers-generic linux-image-generic&lt;br /&gt;&lt;br /&gt;I recommend installing the KDE/Plasma desktop, as I could not get the Unity desktop to work.&lt;br /&gt;apt-get install plasma-desktop dolphin konsole&lt;br /&gt;&lt;br /&gt;Download and install Nvidia&#39;s 387.34_1.0-1 driver - &lt;a href=&quot;http://www.nvidia.com/download/driverResults.aspx/128019/en-us&quot;&gt;http://www.nvidia.com/download/driverResults.aspx/128019/en-us&lt;/a&gt;&lt;br /&gt;sudo dpkg -i nvidia-driver-local-repo-ubuntu1704-387.34_1.0-1_amd64.deb&lt;br /&gt;&lt;br /&gt;The Titan V is still not configured, but should be after the next step.&lt;br /&gt;&lt;br /&gt;Download and install CUDA 9.0 - &lt;a href=&quot;https://developer.nvidia.com/cuda-90-download-archive?target_os=Linux&amp;amp;target_arch=x86_64&amp;amp;target_distro=Ubuntu&amp;amp;target_version=1704&amp;amp;target_type=deblocal&quot;&gt;https://developer.nvidia.com/cuda-90-download-archive?target_os=Linux&amp;amp;target_arch=x86_64&amp;amp;target_distro=Ubuntu&amp;amp;target_version=1704&amp;amp;target_type=deblocal&lt;/a&gt;&lt;br /&gt;sudo dpkg -i uda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64.deb&lt;br /&gt;apt-get install cuda-9-0&lt;br /&gt;&lt;br /&gt;Add the following two lines to your .bash_profile:&lt;br /&gt;&lt;br /&gt;export LD_LIBRARY_PATH=&quot;$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64&quot;&lt;br /&gt;export CUDA_HOME=/usr/local/cuda&lt;br /&gt;&lt;br /&gt;Reboot; the Titan V should be working and configured.&lt;br /&gt;&lt;br /&gt;I recommend installing git.&lt;br /&gt;&lt;br /&gt;apt-get install git&lt;br /&gt;&lt;br /&gt;You&#39;ll need to install pip for Python for TensorFlow.&lt;br /&gt;&lt;br /&gt;apt-get install python-pip (for Python 2)&lt;br /&gt;apt-get install python3-pip (for Python 3)&lt;br /&gt;&lt;br /&gt;We&#39;ll need cuDNN for TensorFlow.&lt;br /&gt;&lt;br /&gt;Download cuDNN v7.0.5 (Dec 5, 2017), for CUDA 9.0&lt;br /&gt;&lt;br /&gt;You&#39;ll need to create a (free) Nvidia developer account.&lt;br /&gt;&lt;br /&gt;&lt;a href=&quot;https://developer.nvidia.com/rdp/cudnn-download&quot;&gt;https://developer.nvidia.com/rdp/cudnn-download&lt;/a&gt;&lt;br /&gt;&lt;br /&gt;This is how I installed cuDNN.&lt;br /&gt;tar xzf cudnn-9.0-linux-x64-v7.tgz&lt;br /&gt;cd cuda&lt;br /&gt;sudo cp NVIDIA_SLA_cuDNN_Support.txt /usr/local/cuda-9.0&lt;br /&gt;sudo cp include/cudnn.h /usr/local/cuda-9.0/targets/x86_64-linux/include&lt;br /&gt;sudo cp lib64/libcudnn_static.a /usr/local/cuda-9.0/targets/x86_64-linux/lib&lt;br /&gt;sudo cp lib64/libcudnn.so.7.0.5 /usr/local/cuda-9.0/targets/x86_64-linux/lib&lt;br /&gt;cd /usr/local/cuda-9.0/targets/x86_64-linux/lib&lt;br /&gt;sudo ln -s libcudnn.so.7.0.5 libcudnn.so.7&lt;br /&gt;sudo ln -s libcudnn.so.7 libcudnn.so&lt;br /&gt;cd&lt;br /&gt;&lt;br /&gt;Let&#39;s install TensorFlow! We&#39;ll want the nightly.&lt;br /&gt;&lt;br /&gt;sudo -H pip install tf-nightly-gpu (Python 2)&lt;br /&gt;sudo -H pip3 install tf-nightly-gpu (Python 3)&lt;br /&gt;&lt;br /&gt;TensorFlow should now be working!&lt;br /&gt;&lt;br /&gt;$ python (or python3)&lt;br /&gt;...&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; import tensorflow as tf&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; hello = tf.constant(&#39;Hello, TensorFlow!&#39;)&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; sess = tf.Session()&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; print(sess.run(hello))&lt;br /&gt;Hello, TensorFlow!&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; a = tf.constant(10)&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; b = tf.constant(32)&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; print(sess.run(a + b))&lt;br /&gt;42&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt;&lt;br /&gt;&lt;br /&gt;See also - &lt;a href=&quot;https://www.tensorflow.org/versions/r0.12/get_started/os_setup&quot;&gt;https://www.tensorflow.org/versions/r0.12/get_started/os_setup&lt;/a&gt;&lt;br /&gt;&lt;br /&gt;If you also want to install the latest Julia.&lt;br /&gt;git clone https://github.com/JuliaLang/julia&lt;br /&gt;cd julia&lt;br /&gt;sudo apt-get install m4 cmake gfortran clang libopenblas-base libopenblas-dev&lt;br /&gt;make -j 4&lt;br /&gt;&lt;br /&gt;Enjoy!</content><link rel='replies' type='application/atom+xml' href='http://angrystatistician.blogspot.com/feeds/6047131242088369324/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://angrystatistician.blogspot.com/2017/12/notes-on-setting-up-titan-v-under.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/6047131242088369324'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/6047131242088369324'/><link rel='alternate' type='text/html' href='http://angrystatistician.blogspot.com/2017/12/notes-on-setting-up-titan-v-under.html' title='Notes on Setting up a Titan V under Ubuntu 17.04'/><author><name>Christopher D. Long</name><uri>http://www.blogger.com/profile/13687149457345266350</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-9149402429183581490.post-710723072524852659</id><published>2017-11-26T18:18:00.000-08:00</published><updated>2017-11-26T18:18:18.819-08:00</updated><title type='text'>Solving IMO 1989 #6 using Probability and Expectation</title><content type='html'>&lt;b&gt;IMO 1989 #6:&lt;/b&gt; A permutation \(\{x_1, x_2, \ldots , x_m\}\) of the set \(\{1, 2, \ldots , 2n\}\), where \(n\) is a positive integer, is said to have property \(P\) if \( | x_i - x_{i+1} | = n\) for at least one \(i\) in \(\{1, 2, ... , 2n-1\}\). Show that for each \(n\) there are more permutations with property \(P\) than without.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Solution:&lt;/b&gt; We first observe that the expected number of pairs \(\{i, i+1\}\) for which \( | x_i - x_{i+1} | = n\) is \(E = 1\). To see this note if \(j\), \( 1 \leq j \leq n\), appears in position \(1\) or \(2n\) it&#39;s adjacent to one number, otherwise two. Thus the probability it&#39;s adjacent to its partner \(j+n\) in a random permutation is \[\begin{equation}&lt;br /&gt;\eqalign{&lt;br /&gt;e_j &amp;amp;= \frac{2}{2n}\cdot \frac{1}{2n-1} + \frac{2n-2}{2n}\cdot \frac{2}{2n-1} \\&lt;br /&gt;&amp;amp;= \frac{2(2n-1)}{2n(2n-1)} \\&lt;br /&gt;&amp;amp;= \frac{1}{n}.&lt;br /&gt;}&lt;br /&gt;\end{equation}\] By linearity of expectation we overall have the expected number of \(j\) adjacent to its partner \(j+n\) is \(\sum_{j=1}^{n} e_j = n\cdot\frac{1}{n} = 1\).&lt;br /&gt;&lt;br /&gt;More is true. By the same argument, if we remove any partner pair \(\{j,j+n\}\), the expected number of partner pairs in a random permutation of the remaining integers is still 1. This is the critical observation.&lt;br /&gt;&lt;br /&gt;Conditional on the partner pair \(\{j,j+n\}\) appearing in a random permutation, what is the expected number of partner pairs \(e\)? Observe that if \(n&amp;gt;1\) it must be less than 2, since as before the expected number of partner pairs ignoring \(\{j,j+n\}\) is 1, and the probability the \(\{j,j+n\}\) pair where they appear has separated another partner pair is greater than 0.&lt;br /&gt;&lt;br /&gt;Putting this together, if \(n=1\) the property \(P\) obviously holds. For \(n&amp;gt;1\), note the expected number of partner pairs \(E = p\cdot e\), where \(p\) is the probability that a random permutation has property \(P\) and \(e\) is as before. But we already know \(E=1\), and by the previous argument if \(n&amp;gt;1\) we have \(e&amp;lt;2\), hence \( 1 = p\cdot e &amp;lt; 2p \) and we conclude \( p &amp;gt; \frac{1}{2}\).</content><link rel='replies' type='application/atom+xml' href='http://angrystatistician.blogspot.com/feeds/710723072524852659/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://angrystatistician.blogspot.com/2017/11/solving-imo-1989-6-using-probability.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/710723072524852659'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/710723072524852659'/><link rel='alternate' type='text/html' href='http://angrystatistician.blogspot.com/2017/11/solving-imo-1989-6-using-probability.html' title='Solving IMO 1989 #6 using Probability and Expectation'/><author><name>Christopher D. Long</name><uri>http://www.blogger.com/profile/13687149457345266350</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-9149402429183581490.post-1943698674891911964</id><published>2017-05-13T20:02:00.002-07:00</published><updated>2017-05-13T21:32:42.221-07:00</updated><title type='text'>Poisson Games and Sudden-Death Overtime</title><content type='html'>Let&#39;s say we have a game that can be reasonably modeled as two independent Poisson processes with team \(i\) having parameter \(\lambda_i\). If one team wins in regulation with team \(i\) scoring \(n_i\), then it&#39;s well-known we have the MLE estimate \(\hat{\lambda_i}=n_i\). But what if the game ends in a tie in regulation with each team scoring \(n\) goals and we have sudden-death overtime? How does this affect the MLE estimate for the winning and losing teams?&lt;br /&gt;&lt;br /&gt;Assuming without loss of generality that team \(1\) is the winner in sudden-death overtime. As we have two independent Poisson processes, the probability of this occurring is \(\frac{\lambda_1}{\lambda_1&amp;nbsp;+ \lambda_2}\). Thus, the overall likelihood we&#39;d like to maximize is \[L = e^{-\lambda_1} \frac{{\lambda_1}^n}{n!} e^{-\lambda_2} \frac{{\lambda_2}^n}{n!} \frac{\lambda_1}{\lambda_1 + \lambda_2}.\] Letting \(l = \log{L}\) we get \[l = -{\lambda_1} + n \log{\lambda_1} - {\lambda_2} + n \log{\lambda_2} - 2 \log{n!} + \log{\lambda_1}-\log({\lambda_1 + \lambda_2}).\] This gives \[\begin{equation}&lt;br /&gt;\eqalign{&lt;br /&gt;\frac{\partial l}{\partial \lambda_1} &amp;amp;= -1+\frac{n}{\lambda_1}+\frac{1}{\lambda_1}+\frac{1}{\lambda_1 + \lambda_2}\\&lt;br /&gt;\frac{\partial l}{\partial \lambda_2} &amp;amp;= -1+\frac{n}{\lambda_2}+\frac{1}{\lambda_1 + \lambda_2}.&lt;br /&gt;}&lt;br /&gt;\end{equation}\] Setting both partials equal to \(0\) and solving, we get \[\begin{equation}&lt;br /&gt;\eqalign{&lt;br /&gt;(n-\hat{\lambda_1})(\hat{\lambda_1}+\hat{\lambda_2})+\hat{\lambda_2} &amp;amp;= 0\\&lt;br /&gt;(n-\hat{\lambda_2})(\hat{\lambda_1}+\hat{\lambda_2})-\hat{\lambda_2} &amp;amp;= 0,&lt;br /&gt;}&lt;br /&gt;\end{equation}\] and so \[\begin{equation}&lt;br /&gt;\eqalign{&lt;br /&gt;\hat{\lambda_1} &amp;amp;= (n+1) \frac{2n}{2n+1}\\&lt;br /&gt;\hat{\lambda_2} &amp;amp;= n \frac{2n}{2n+1}.&lt;br /&gt;}&lt;br /&gt;\end{equation}\] For example, if both teams score \(3\) goals in regulation and team \(1\) wins in sudden-death overtime, our MLE estimates are \(\hat{\lambda_1} = 3\frac{3}{7},&amp;nbsp;\hat{\lambda_2} = 2\frac{4}{7}\).&lt;br /&gt;&lt;br /&gt;Intuitively this makes sense, because \(2n\) goals were scored in regulation time, hence we &quot;expect&quot; that the overtime goal occurred around a fraction \(\frac{1}{2n}\) of regulation, so team \(1\) scored \(n+1\) goals in about \(\frac{2n+1}{2n}\) regulation periods and team \(2\) scored&amp;nbsp;\(n\) goals in about \(\frac{2n+1}{2n}\) regulation periods. The standard Poisson process MLE estimates here coincide with the estimates we derived above.&lt;br /&gt;&lt;br /&gt;Does this work in practice? Yes! I tested it on my NCAA men&#39;s lacrosse model, and it increased the out-of-sample testing accuracy by 0.5%. Surprisingly large for such a small change!</content><link rel='replies' type='application/atom+xml' href='http://angrystatistician.blogspot.com/feeds/1943698674891911964/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://angrystatistician.blogspot.com/2017/05/poisson-games-and-sudden-death-overtime.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/1943698674891911964'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/1943698674891911964'/><link rel='alternate' type='text/html' href='http://angrystatistician.blogspot.com/2017/05/poisson-games-and-sudden-death-overtime.html' title='Poisson Games and Sudden-Death Overtime'/><author><name>Christopher D. Long</name><uri>http://www.blogger.com/profile/13687149457345266350</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-9149402429183581490.post-7242256171179291742</id><published>2017-04-01T21:59:00.000-07:00</published><updated>2018-04-10T09:50:14.276-07:00</updated><title type='text'>Why does Kaggle use Log-loss?</title><content type='html'>If you&#39;re not familiar with &lt;a href=&quot;https://www.kaggle.com/&quot; target=&quot;_blank&quot;&gt;Kaggle&lt;/a&gt;, it&#39;s an organization dedicated to data science competitions to both provide ways for companies to potentially do analytics at less cost, as well as to identify talented data scientists.&lt;br /&gt;&lt;br /&gt;Competitions are scored using a variety of functions, and the most common for binary classification tasks with confidence is something called log-loss, which is essentially \(\sum_{i=1}^{n} \log(p_i)\), where \(p_i\) is your model&#39;s claimed confidence for test data point \(i\)&#39;s correct label. Why does Kaggle use this scoring function? Here I&#39;ll follow &lt;a href=&quot;https://terrytao.wordpress.com/2016/06/01/how-to-assign-partial-credit-on-an-exam-of-true-false-questions/&quot; target=&quot;_blank&quot;&gt;Terry Tao&#39;s argument&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;Ideally what we&#39;d like is a scoring function \(f(x)\) that yields the maximum expected score precisely when the claimed confidence \(x_i\) in the correct label for \(i\) is actually what the submitter believes is the true probability (or frequency) of that outcome. This means that we want \[L(x)=p\cdot f(x)&amp;nbsp;+ (1-p)\cdot f(1-x)\] for fixed \(p\) to be maximized when \(x=p\). Differentiating, this means \[L&#39;(x) = p\cdot f&#39;(x) - (1-p)\cdot f&#39;(1-x) = 0\] when \(x=p\), hence \(p\cdot f&#39;(p) = (1-p)\cdot f&#39;(1-p)\) for all \(p\). This will be satisfied by any admissible \(f(x)\) with \(x\cdot f&#39;(x)\) symmetric around \(x=\frac{1}{2}\), but if we extend our analysis to multinomial outcomes we get the stronger conclusion that in fact \(x\cdot f&#39;(x) = c_0\) for some constant \(c_0\). This in turn implies \(f(x)=c_0\cdot \log(x)+c_1\). If we want \(f(1/2)=0\) and \(f(1)=1\), we end up with \(f(x)={\log}_2(2x)\) and the expected score is \[L(x)=x\cdot {\log}_2(2x) + (1-x)\cdot {\log}_2(2(1-x)).\]</content><link rel='replies' type='application/atom+xml' href='http://angrystatistician.blogspot.com/feeds/7242256171179291742/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://angrystatistician.blogspot.com/2017/04/why-does-kaggle-use-log-loss.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/7242256171179291742'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/7242256171179291742'/><link rel='alternate' type='text/html' href='http://angrystatistician.blogspot.com/2017/04/why-does-kaggle-use-log-loss.html' title='Why does Kaggle use Log-loss?'/><author><name>Christopher D. Long</name><uri>http://www.blogger.com/profile/13687149457345266350</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-9149402429183581490.post-513160205354158210</id><published>2017-03-31T19:43:00.000-07:00</published><updated>2017-03-31T19:45:41.026-07:00</updated><title type='text'>The Kelly Criterion and a Sure Thing</title><content type='html'>The &lt;a href=&quot;https://en.wikipedia.org/wiki/Kelly_criterion&quot; target=&quot;_blank&quot;&gt;Kelly Criterion&lt;/a&gt; is an alternative to standard utility theory, which seeks to maximize expected utility. Instead, the Kelly Criterion seeks to maximize expected &lt;i&gt;growth&lt;/i&gt;. That is, if we start out with an initial bankroll \(B_0\), we seek to maximize \(\mathrm{E}[g(t)]\), where \(B_t = B_0\cdot e^{g(t)}\).&lt;br /&gt;As a simple example, consider the following choice. We can have a sure $3000, or we can take the gamble of a \(\frac{4}{5}\) chance of $4000 and a \(\frac{1}{5}\) chance of $0. What does Kelly say?&lt;br /&gt;Assume we have a current bankroll of \(B_0\). After the first choice we have \(B_1 = B_0+3000\), which we can write as \[\mathrm{E}[g(1)] = \log\left(\frac{B_0+3000}{B_0}\right);\]for the second choice we have \[\mathrm{E}[g(1)] = \frac{4}{5} \log\left(\frac{B_0+4000}{B_0}\right).\]And so we want to compare \(\log\left(\frac{B_0+3000}{B_0}\right)\) and \(\frac{4}{5} \log\left(\frac{B_0+4000}{B_0}\right)\).&lt;br /&gt;Exponentiating, we&#39;re looking for the positive root of&amp;nbsp;\[{\left({B_0+3000}\right)}^5 - {B_0}\cdot {\left({B_0+4000}\right)}^4=0.\]&lt;a href=&quot;https://www.wolframalpha.com/input/?i=solve+(b_0%2B3000)%5E5-(b_0)*(b_0%2B4000)%5E4%3D0&quot; target=&quot;_blank&quot;&gt;Wolfram Alpha&lt;/a&gt;&amp;nbsp;now tells us that we should go with the sure thing if \(B_0 &amp;lt; $4942.92\), and take the gamble otherwise.</content><link rel='replies' type='application/atom+xml' href='http://angrystatistician.blogspot.com/feeds/513160205354158210/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://angrystatistician.blogspot.com/2017/03/the-kelly-criterion-and-sure-thing.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/513160205354158210'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/513160205354158210'/><link rel='alternate' type='text/html' href='http://angrystatistician.blogspot.com/2017/03/the-kelly-criterion-and-sure-thing.html' title='The Kelly Criterion and a Sure Thing'/><author><name>Christopher D. Long</name><uri>http://www.blogger.com/profile/13687149457345266350</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-9149402429183581490.post-7287905824139329105</id><published>2017-03-31T18:33:00.000-07:00</published><updated>2017-03-31T18:39:12.164-07:00</updated><title type='text'>Prime Divisors of \(3^{32}-2^{32}\)</title><content type='html'>Find four prime divisors &amp;lt; 100 for \(3^{32}-2^{32}\).&lt;br /&gt;Source: British Math Olympiad, 2006.&lt;br /&gt;&lt;br /&gt;This factors nicely as \(3^{32}-2^{32} = \left(3^{16}+2^{16}\right)\left(3^{16}-2^{16}\right)\), and we can continue factoring in this way to get \[3^{32}-2^{32} = \left(3^{16}+2^{16}\right)\left(3^8+2^8\right)\left(3^4+2^4\right)\left(3^2+2^2\right)\left(3^2-2^2\right).\]The final three terms are \(5, 13, 97\), so we have three of the four required primes. For another prime divisor, consider \(3^{16}-2^{16}\). By &lt;a href=&quot;https://en.wikipedia.org/wiki/Fermat%27s_little_theorem&quot;&gt;Fermat&#39;s Little Theorem&lt;/a&gt; \(a^{16}-1\equiv 0 \bmod 17\) for all \(a\) with \((a,17)=1\), and so it follows that \(3^{16}-2^{16}\equiv 0 \bmod 17\), and we therefore have \(17\) as a fourth such prime divisor.&lt;br /&gt;Alternatively, note \( \left(\dfrac{3}{17}\right)=-1, \left(\dfrac{2}{17}\right)=1\), hence by &lt;a href=&quot;https://en.wikipedia.org/wiki/Euler%27s_criterion&quot;&gt;Euler&#39;s Criterion&lt;/a&gt; \(3^8\equiv -1 \bmod 17\) and \(2^8\equiv 1 \bmod 17\), giving \(3^8+2^8\equiv 0\bmod 17\).</content><link rel='replies' type='application/atom+xml' href='http://angrystatistician.blogspot.com/feeds/7287905824139329105/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://angrystatistician.blogspot.com/2017/03/prime-divisors-of-332-232.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/7287905824139329105'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/7287905824139329105'/><link rel='alternate' type='text/html' href='http://angrystatistician.blogspot.com/2017/03/prime-divisors-of-332-232.html' title='Prime Divisors of \(3^{32}-2^{32}\)'/><author><name>Christopher D. Long</name><uri>http://www.blogger.com/profile/13687149457345266350</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-9149402429183581490.post-909880114773204542</id><published>2017-03-30T17:48:00.000-07:00</published><updated>2017-03-30T17:48:03.201-07:00</updated><title type='text'>Highest Powers of 3 and \(\left(1+\sqrt{2}\right)^n\)</title><content type='html'>Let \(\left(1+\sqrt{2}\right)^{2012}=a+b\sqrt{2}\), where \(a\) and \(b\) are integers. What is the greatest common divisor of \(b\) and \(81\)?&lt;br /&gt;Source: 2011-2012 SDML High School 2a, problem 15.&lt;br /&gt;&lt;br /&gt;Let \((1+\sqrt{2})^n = a_n + b_n \sqrt{2}\). I&#39;ve thought about this some more, and there&#39;s a nice way to describe the highest power of \(3\) that divides \(b_n\). This is probably outside of the scope of the intended solution, however.&lt;br /&gt;&lt;br /&gt;First note that \((1-\sqrt{2})^n = a_n - b_n \sqrt{2}\), and so from \((1+\sqrt{2})(1-\sqrt{2})=-1\) we get \((1+\sqrt{2})^n (1-\sqrt{2})^n = {(-1)}^n\). This gives \[{a_n}^2 - 2 {b_n}^2 = {(-1)}^n.\] Now define the highest power of a prime \(p\) that divides \(n\) to be \(\operatorname{\nu}_p(n)\).&lt;br /&gt;From cubing and using the above result it&#39;s straightforward to prove that if \(\operatorname{\nu}_3(b_n) = k &gt; 0\) then \(\operatorname{\nu}_3(b_{3n}) = k+1\).&lt;br /&gt;Note \((1+\sqrt{2})^4 = 17 + 12\sqrt{2} \equiv -1+3\sqrt{2} \pmod{3^2}\). Cubing and using the first formula as before, we can in fact show that \[(1+\sqrt{2})^{4\cdot 3^n} \equiv -1 + 3^{n+1}\sqrt{2} \pmod{3^{n+2}},\] and squaring we also have \[(1+\sqrt{2})^{8\cdot 3^n} \equiv 1 + 3^{n+1}\sqrt{2} \pmod{3^{n+2}}.\] Now assume \(\operatorname{\nu}_3(b_m) = k, \operatorname{\nu}_3(b_n) = l\) and \(k\neq l\). From the top formula if \(3 | b_i\) then \(3 \not{|} a_i\), and it follows that \[\operatorname{\nu}_3(b_{m+n}) = \min(k,l).\]Putting this all together, write \(n = 4\cdot m +k\), where \(0\leq k &lt;4\). If \(k\neq 0\), then \(\operatorname{\nu}_3(b_{n}) = 0\). If \(k=0\), let the base-3 expansion of \(m\) be \(a_i \cdot 3^i + \ldots + a_0\). Then \[\operatorname{\nu}_3(b_{n}) = \min_{a_j \neq 0} j+1 .\]&lt;br /&gt;For \(n=2012\), we have \(2012 = 4\cdot 503 = 4\cdot(2\cdot 3^5 + 3^2 + 2\cdot 3 + 2)\) and so \(\operatorname{\nu}_3(b_{2012})=1\). We don&#39;t actually need to compute the entire base-3 expansion for 503, of course; we only need to observe that it&#39;s not divisible by 3.&lt;br /&gt;&lt;br /&gt;For \(n=2016\), we have \(2016 = 4\cdot 504 = 4\cdot(2\cdot 3^5 + 2\cdot 3^2)\) and so \(\operatorname{\nu}_3(b_{2016})=3\).&lt;br /&gt;</content><link rel='replies' type='application/atom+xml' href='http://angrystatistician.blogspot.com/feeds/909880114773204542/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://angrystatistician.blogspot.com/2017/03/highest-powers-of-3-and-left1sqrt2rightn.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/909880114773204542'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/909880114773204542'/><link rel='alternate' type='text/html' href='http://angrystatistician.blogspot.com/2017/03/highest-powers-of-3-and-left1sqrt2rightn.html' title='Highest Powers of 3 and \(\left(1+\sqrt{2}\right)^n\)'/><author><name>Christopher D. Long</name><uri>http://www.blogger.com/profile/13687149457345266350</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-9149402429183581490.post-7042197233498512178</id><published>2017-03-30T12:24:00.000-07:00</published><updated>2017-03-30T12:28:18.843-07:00</updated><title type='text'>Sum of Two Odd Composite Numbers</title><content type='html'>What is the largest even integer that cannot be written as the sum of two odd composite numbers? Source: &lt;a href=&quot;https://artofproblemsolving.com/wiki/index.php/1984_AIME_Problems&quot; target=&quot;_blank&quot;&gt;AIME 1984&lt;/a&gt;, problem 14.&lt;br /&gt;&lt;br /&gt;Note \(24 = 3\cdot 3 + 3\cdot 5\), and so if \(2k\) has a representation as the sum of even multiples of 3 and 5, say \(2k = e_3\cdot 3 + e_5\cdot 5\), we get a representation of \(2k+24\) as a sum of odd composites via \(2k+24 = (3+e_3)\cdot 3 + (5+e_5)\cdot 5\). But by the &lt;a href=&quot;https://en.wikipedia.org/wiki/Coin_problem&quot; target=&quot;_blank&quot;&gt;Frobenius coin problem&lt;/a&gt; every number \(k &amp;gt; 3\cdot 5 -3-5 = 7\) has such a representation, hence every number \(2k &amp;gt; 14\) has a representation as the sum of even multiples of 3 and 5. Thus every number \(n &amp;gt; 24+14=38\) has a representation as the sum of odd composites. Checking, we see that \(\boxed{38}\) has no representation as a sum of odd composites.</content><link rel='replies' type='application/atom+xml' href='http://angrystatistician.blogspot.com/feeds/7042197233498512178/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://angrystatistician.blogspot.com/2017/03/sum-of-two-odd-composite-numbers.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/7042197233498512178'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/7042197233498512178'/><link rel='alternate' type='text/html' href='http://angrystatistician.blogspot.com/2017/03/sum-of-two-odd-composite-numbers.html' title='Sum of Two Odd Composite Numbers'/><author><name>Christopher D. Long</name><uri>http://www.blogger.com/profile/13687149457345266350</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-9149402429183581490.post-4898052062903412786</id><published>2016-06-18T23:30:00.000-07:00</published><updated>2016-06-19T03:54:04.610-07:00</updated><title type='text'>What&#39;s the Value of a Win?</title><content type='html'>In a previous entry I demonstrated &lt;a href=&quot;http://angrystatistician.blogspot.com/2016/06/a-simple-estimate-for-pythagorean.html&quot;&gt;one simple way to estimate an exponent for the Pythagorean win expectation&lt;/a&gt;. Another nice consequence of a Pythagorean win expectation formula is that it also makes it simple to estimate the run value of a win in baseball, the point value of a win in basketball, the goal value of a win in hockey etc.&lt;br /&gt;&lt;br /&gt;Let our Pythagorean win expectation formula be \[ w=\frac{P^e}{P^e+1},\] where \(w\) is the win fraction expectation, \(P\) is runs/allowed (or similar) and \(e\) is the Pythagorean exponent. How do we get an estimate for the run value of a win? The expected number of games won in a season with \(g\) games is \[W = g\cdot w = g\cdot \frac{P^e}{P^e+1},\] so for one estimate we only need to compute the value of the partial derivative \(\frac{\partial W}{\partial P}\) at \(P=1\). Note that \[ W = g\left( 1-\frac{1}{P^e+1}\right), \] and so \[ \frac{\partial W}{\partial P} = g\frac{eP^{e-1}}{(P^e+1)^2}\] and it follows \[ \frac{\partial W}{\partial P}(P=1) = \frac{ge}{4}.\] Our estimate for the run value of a win now follows by setting \[\frac{\Delta W}{\Delta P} = \frac{ge}{4} \] giving \[ \Delta W = 1 = \frac{ge}{4} \Delta P.\] What is \(\Delta P\)? Well \(P = R/A\), where \(R\) is runs scored over the season and \(A\) is runs allowed over the season. We&#39;re assuming this is a league average team and asking how many more runs they&#39;d need to score to win an additional game, so \(A\) is actually fixed at \(L\), the league average number of runs scored (or allowed). This gives us \[1 = \frac{ge}{4} \Delta P = \frac{ge\Delta R}{4L}.\] Now \(L/g = l\), the league average runs per game, so we arrive at the estimate \[\Delta R = \frac{4l}{e}.\] In the specific case of MLB, we have \(e = 1.8\) and \(l = 4.3\), giving that a win is approximately \(\Delta R = 9.56\) runs.&lt;br /&gt;&lt;br /&gt;Bill James originally used the exponent \(e=2\); in this case the formula simplifies to \(\Delta R = 2l\), i.e. we get the particularly simple result that a win is equal to approximately twice the average number of runs scored per game.&lt;br /&gt;&lt;br /&gt;Applying this estimate to the NBA, a win is approximately \( \Delta R = \frac{4\cdot 101}{16.4} = 24.6\) points. Similarly, we get the estimates for a win of 4.5 goals for the NHL and 5.1 goals for the Premier League.</content><link rel='replies' type='application/atom+xml' href='http://angrystatistician.blogspot.com/feeds/4898052062903412786/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://angrystatistician.blogspot.com/2016/06/whats-value-of-win.html#comment-form' title='2 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/4898052062903412786'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/4898052062903412786'/><link rel='alternate' type='text/html' href='http://angrystatistician.blogspot.com/2016/06/whats-value-of-win.html' title='What&#39;s the Value of a Win?'/><author><name>Christopher D. Long</name><uri>http://www.blogger.com/profile/13687149457345266350</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><thr:total>2</thr:total></entry><entry><id>tag:blogger.com,1999:blog-9149402429183581490.post-7009974848766785430</id><published>2016-06-08T16:05:00.000-07:00</published><updated>2016-06-08T16:05:06.375-07:00</updated><title type='text'>A Simple Estimate for Pythagorean Exponents</title><content type='html'>Given the number of runs scored and runs allowed by a baseball team, what&#39;s a good estimate for that team&#39;s win fraction? Bill James famously came up with what he called the &quot;&lt;a href=&quot;https://en.wikipedia.org/wiki/Pythagorean_expectation&quot;&gt;Pythagorean expectation&lt;/a&gt;&quot; \[w = \frac{R^2}{R^2 + A^2},\] which can also be written as \[w = \frac{{(R/A)}^2}{{(R/A)}^2 + 1}.\] More generally, if team \(i\) scores \(R_i\) and allows \(A_i\) runs, the Pythagorean estimate for the probability of team \(1\) beating team \(2\) is \[w = \frac{{(R_1/A_1)}^2}{{(R_1/A_1)}^2 + (R_2/A_2)^2}.\] We can see that the estimate of the team&#39;s win fraction is a consequence of this, as an average team would by definition have \(R_2 = A_2\). Now, there&#39;s nothing magical about the exponent being 2; it&#39;s a coincidence, and in fact is not even the &quot;best&quot; exponent. But what&#39;s a good way to estimate the exponent? Note the structural similarity of this win probability estimator and the Bradley-Terry estimator \[ w = \frac{P_1}{P_1+P_2}.\] Here the \(P_i\) are what we could call the &quot;Bradley-Terry power&quot; of the team. This immediately suggests one way to estimate the expectation model&#39;s exponent - fit a Bradley-Terry model, then fit the log-linear regression \(\log(P_i)\) vs \(\log(R_i/A_i)\). The slope of this regression will be one estimate for the expectation exponent.&lt;br /&gt;&lt;br /&gt;How well does this work? I get &lt;a href=&quot;https://github.com/octonion/lunchtime/blob/master/pythagorean/mlb_btl.txt&quot;&gt;1.727 for MLB in 2014&lt;/a&gt;. The R code and data files for MLB and other sports may be found in my &lt;a href=&quot;https://github.com/octonion/lunchtime/tree/master/pythagorean&quot;&gt;GitHub repo&lt;/a&gt;.</content><link rel='replies' type='application/atom+xml' href='http://angrystatistician.blogspot.com/feeds/7009974848766785430/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://angrystatistician.blogspot.com/2016/06/a-simple-estimate-for-pythagorean.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/7009974848766785430'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/7009974848766785430'/><link rel='alternate' type='text/html' href='http://angrystatistician.blogspot.com/2016/06/a-simple-estimate-for-pythagorean.html' title='A Simple Estimate for Pythagorean Exponents'/><author><name>Christopher D. Long</name><uri>http://www.blogger.com/profile/13687149457345266350</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-9149402429183581490.post-3609641838826143688</id><published>2016-05-02T22:40:00.000-07:00</published><updated>2016-05-02T22:40:14.420-07:00</updated><title type='text'>Behind the Speadsheet</title><content type='html'>In the book &lt;a href=&quot;http://www.amazon.com/Only-Rule-Has-Work-Experiment-ebook/dp/B016IBVN6Y&quot;&gt;&quot;The Only Rule Is It Has to Work: Our Wild Experiment Building a New Kind of Baseball Team&quot;&lt;/a&gt;, Ben Lindbergh and Sam Miller recount a grand adventure to take command of an independent league baseball team, with the vision of trying every idea, sane or crazy, in an attempt to achieve a winning edge. Five infielders, four outfielders, defensive shifts, optimizing lineups - everything.&lt;br /&gt;&lt;br /&gt;It was really an impossible task. Professional sports at every level are filled with highly accomplished and competitive athletes, with real lives and real egos. Now imagine walking in one day and suddenly trying to convince them that they should be doing things differently. Who do you think you are?&lt;br /&gt;&lt;br /&gt;I was one of the analysts who helped Ben and Sam in this quest, and I wanted to write some thoughts down from my own perspective, not as one of the main characters, but as someone more behind the scenes. These are some very short initial thoughts only, but I&#39;d like to followup with some more ideas on where things went wrong from my perspective, and also how independent league teams can better identify roster talent from some non-traditional sources.&lt;br /&gt;&lt;br /&gt;My focus was on attempting to identify talent overlooked in the MLB draft. This is extremely challenging; there are 30 teams, 40 standards rounds plus other picks. Furthermore, among those players left, many sign as amateur free agents post-draft. You&#39;re left with players from lower divisions, very small schools, 23-year-old seniors, bad bodies, soft tossers, poor defenders, etc. But, still, there may be players who aren&#39;t good MLB prospects, but who could still perform well as part of an independent league team.&lt;br /&gt;&lt;br /&gt;Looking at top framing college catchers was a bust; this is a premium defensive position and very little is overlooked.&lt;br /&gt;&lt;br /&gt;Among the undrafted senior hitters and pitchers there were several potential prospects, many of whom you&#39;ll read about in the book. The most important fact to keep in mind is that these are real people with real lives, real families and real hopes and dreams, and playing independent ball isn&#39;t nearly lucrative enough to pay the bills. Harsh reality will limit your pool even more, and those who choose to pursue it will face the additional stress of financial strain.&lt;br /&gt;&lt;br /&gt;That being said, was Ben and Sam&#39;s experiment a success? You&#39;ll have to read the book, but absolutely, some talent was found.</content><link rel='replies' type='application/atom+xml' href='http://angrystatistician.blogspot.com/feeds/3609641838826143688/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://angrystatistician.blogspot.com/2016/05/behind-speadsheet.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/3609641838826143688'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/3609641838826143688'/><link rel='alternate' type='text/html' href='http://angrystatistician.blogspot.com/2016/05/behind-speadsheet.html' title='Behind the Speadsheet'/><author><name>Christopher D. Long</name><uri>http://www.blogger.com/profile/13687149457345266350</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-9149402429183581490.post-5461723911114364192</id><published>2016-02-09T20:16:00.000-08:00</published><updated>2016-02-09T20:16:05.287-08:00</updated><title type='text'>When is a Lead Safe in the NBA?</title><content type='html'>Assuming two NBA teams of equal strength with \(t\) seconds remaining, what is a safe lead at a prescribed confidence level? Bill James has a &lt;a href=&quot;http://www.slate.com/articles/sports/sports_nut/2008/03/the_lead_is_safe.html&quot;&gt;safe lead formula for NCAA basketball&lt;/a&gt;, and the topic has been addressed by other researchers at various levels of complexity, e.g. &lt;a href=&quot;http://arxiv.org/abs/1503.03509&quot;&gt;Clauset, Kogan and Redner&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;I&#39;ll present a simple derivation. Start by observing there are about 50 scoring groups per team per game (scoring groups include all baskets and free throws that occur at the same time), with each scoring group worth about two points. Assume scoring events by team are Poisson distributed with parameter \(\lambda = \frac{50\cdot t}{48\cdot 60} = \frac{t}{57.6}\). Using a normal approximation, the difference of these two distributions is normal with mean 0 and variance \(\sqrt{2}\lambda\), giving a standard deviation of \(0.1863\sqrt{t}\).&lt;br /&gt;&lt;br /&gt;Using this approximation, what is a 90% safe lead? A 90% tail is 1.28 standard deviations, \(1.28\cdot 0.1863\sqrt{t} = 0.2385\sqrt{t}\) scoring groups. As a scoring group is about two points, this means a 90% safe lead, assuming a jump ball, is about \(0.477\sqrt{t}\) points (Clauset et. al. obtained \(0.4602\sqrt{t}\)). For example, a safe lead at halftime is approximately \(0.477 \sqrt{24\cdot 60} = 18.1\) points.&lt;br /&gt;&lt;br /&gt;Next - adjustments for possession arrow and shot clock time; validity of approximation; adjusting for team strengths.</content><link rel='replies' type='application/atom+xml' href='http://angrystatistician.blogspot.com/feeds/5461723911114364192/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://angrystatistician.blogspot.com/2016/02/when-is-lead-safe-in-nba.html#comment-form' title='1 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/5461723911114364192'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/5461723911114364192'/><link rel='alternate' type='text/html' href='http://angrystatistician.blogspot.com/2016/02/when-is-lead-safe-in-nba.html' title='When is a Lead Safe in the NBA?'/><author><name>Christopher D. Long</name><uri>http://www.blogger.com/profile/13687149457345266350</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><thr:total>1</thr:total></entry><entry><id>tag:blogger.com,1999:blog-9149402429183581490.post-2344669656897553681</id><published>2015-10-15T01:48:00.002-07:00</published><updated>2015-10-15T03:40:15.810-07:00</updated><title type='text'>An Enormous Number of Kilograms</title><content type='html'>For years the kilogram has been defined with respect to a platinum and iridium cylinder, but this is now &lt;a href=&quot;http://www.nature.com/news/kilogram-conflict-resolved-at-last-1.18550&quot;&gt;no longer the case&lt;/a&gt;. Here&#39;s a puzzle about kilograms that&#39;s easy to state and understand, but the answer is very, very surprising.&lt;br /&gt;&lt;br /&gt;I&#39;ve always had a fascination with really large numbers. First 100 when I was really little, and as I got older and more sophisticated numbers like a &lt;a href=&quot;https://en.wikipedia.org/wiki/Googol&quot;&gt;googol&lt;/a&gt; and the smallest number that satisfies the conditions of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Archimedes%27_cattle_problem&quot;&gt;Archimedes cattle problem&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;When I was an undergraduate I interviewed for a summer internship with an insurance company as an actuarial student. They gave me the following puzzle - what&#39;s the smallest number that when you move the last digit to the front it multiplies by 2? I calculated for a little while, then said &quot;This can&#39;t be right, my answer has 18 digits!&quot;. It turns out that the smallest solution does, indeed, have 18 digits.&lt;br /&gt;&lt;br /&gt;We can see this by letting our \((n+1)\)-digit number \( x = 10 m + a\), where \(m\) is an \(n\)-digit number and \(0\leq a &lt; 10\). Moving \(a\) to the front we get \(y = 10^n a + m\), and our requirement is \(y = 2x\). This gives: \begin{align} 20 m + 2 &amp;= 10^n a + m \\ 19 m &amp;= a(10^n-2) \\ m &amp;= \frac{2a(5\cdot 10^{n-1} - 1)}{19} \end{align} The smallest \(m\), if one exists, requires \(a,n\) such that 19 divides \(5\cdot 10^{n-1}-1\) (as 19 can&#39;t divide \(2 a\)) and the result has \(n\)-digits. It&#39;s easy to check that the smallest value of \(n\) that satisfies the first condition is \(n=17\). To get the smallest solution we try \(a=1\), but this yields a value with only 16 digits. Setting \(a=2\), however, yields the 17-digit \(m = 10526315789473684\). The smallest solution to our puzzle is therefore the 18-digit number \(105263157894736842\); that&#39;s surprisingly large. &lt;p\&gt;&lt;br /&gt;&lt;br /&gt;Numbers with this type of property are known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Parasitic_number&quot;&gt;parasitic numbers&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;Later, I wondered if there were numbers with the slightly different, but equally interesting property, that moving the last digit to the front converted (&quot;autoconverts&quot;) it from a value under one unit of measurement to an equivalent value under a different unit of measurement.&lt;br /&gt;&lt;br /&gt;The first one I tried was moving the last digit to the front converts from Celsius to Fahrenheit. This is a fun puzzle that eventually made its way into the &lt;a href=&quot;http://tierneylab.blogs.nytimes.com/2009/11/16/monday-puzzle-conversion-factors/&quot;&gt;New York Times&lt;/a&gt;. The smallest such value is 275 C, which exactly equals 527 F. What&#39;s the next smallest temperature?&lt;br /&gt;&lt;br /&gt;How about moving the first digit to the end? We&#39;ll need to use the little-known fact that, legally, a &lt;a href=&quot;https://en.wikipedia.org/wiki/Pound_(mass)&quot;&gt;pound is exactly equal to 0.45359237 kilograms&lt;/a&gt;. Given this, does there exist a number such that moving the first digit to the end converts from pounds to kilograms? The answer is yes, but the smallest solution has 108,437,840 digits! The solution is similar to the above, but as it&#39;s computationally more involved I&#39;ve written &lt;a href=&quot;https://en.wikipedia.org/wiki/SageMath&quot;&gt;Sage&lt;/a&gt; code to solve it, which you can find in my &lt;a href=&quot;https://github.com/octonion/puzzles/tree/master/parasitic&quot;&gt;GitHub puzzles repository&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;The smallest number that autoconverts from gallons to liters, incidentally, is even bigger at 382,614,539 digits!</content><link rel='replies' type='application/atom+xml' href='http://angrystatistician.blogspot.com/feeds/2344669656897553681/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://angrystatistician.blogspot.com/2015/10/an-enormous-number-of-kilograms.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/2344669656897553681'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/2344669656897553681'/><link rel='alternate' type='text/html' href='http://angrystatistician.blogspot.com/2015/10/an-enormous-number-of-kilograms.html' title='An Enormous Number of Kilograms'/><author><name>Christopher D. Long</name><uri>http://www.blogger.com/profile/13687149457345266350</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-9149402429183581490.post-1759380959914215633</id><published>2015-10-10T17:25:00.000-07:00</published><updated>2015-10-10T17:25:28.315-07:00</updated><title type='text'>Solving a Math Puzzle using Physics</title><content type='html'>The following math problem, which appeared on a Scottish maths paper, has been &lt;a href=&quot;http://gizmodo.com/can-you-solve-the-math-problem-that-stumped-most-scotti-1735604246&quot;&gt;making the internet rounds&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;http://2.bp.blogspot.com/-1Q63s68Tj0Q/VhmsVliOXmI/AAAAAAAADTE/tGn2BHhRuVQ/s1600/1466229790313874095.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;http://2.bp.blogspot.com/-1Q63s68Tj0Q/VhmsVliOXmI/AAAAAAAADTE/tGn2BHhRuVQ/s400/1466229790313874095.png&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;The first two parts require students to interpret the meaning of the components of the formula \(T(x) = 5 \sqrt{36+x^2} + 4(20-x) \), and the final &quot;challenge&quot; component involves finding the minimum of \( T(x) \) over \( 0 \leq x \leq 20 \). Usually this would require a differentiation, but if you know &lt;a href=&quot;https://en.wikipedia.org/wiki/Snell%27s_law&quot;&gt;Snell&#39;s law&lt;/a&gt; you can write down the solution almost immediately. People normally think of Snell&#39;s law in the context of light and optics, but it&#39;s really a statement about least time across media permitting different velocities.&lt;br /&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;http://3.bp.blogspot.com/-JDqJpTkDKdE/VhmmQcVJT8I/AAAAAAAADS0/po9hMnRXoJ8/s1600/Picture0.jpg&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;http://3.bp.blogspot.com/-JDqJpTkDKdE/VhmmQcVJT8I/AAAAAAAADS0/po9hMnRXoJ8/s320/Picture0.jpg&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;One way to phrase Snell&#39;s law is that least travel time is achieved when \[ \frac{\sin{\theta_1}}{\sin{\theta_2}} = \frac{v_1}{v_2},\] where \( \theta_1, \theta_2\) are the angles to the normal and \(v_1, v_2\) are the travel velocities in the two media.&lt;br /&gt;&lt;br /&gt;In our puzzle the crocodile has an implied travel velocity of 1/5 in the water and 1/4 on land. Furthermore, the crocodile travels along the riverbank once it hits land, so \( \theta_2 = 90^{\circ}\) and \(\sin{\theta_2} = 1\). Snell&#39;s law now says that the path of least time satisfies \[ \sin{\theta_1} = \frac{x}{\sqrt{36+x^2}} = \frac{4}{5},\] giving us \( 25x^2 = 16x^2 + 24^2\). Solving, \( 3^2 x^2 = 24^2, x^2 = 8^2\) and the solution is \(x = 8\).</content><link rel='replies' type='application/atom+xml' href='http://angrystatistician.blogspot.com/feeds/1759380959914215633/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://angrystatistician.blogspot.com/2015/10/solving-math-puzzle-using-physics.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/1759380959914215633'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/1759380959914215633'/><link rel='alternate' type='text/html' href='http://angrystatistician.blogspot.com/2015/10/solving-math-puzzle-using-physics.html' title='Solving a Math Puzzle using Physics'/><author><name>Christopher D. Long</name><uri>http://www.blogger.com/profile/13687149457345266350</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://2.bp.blogspot.com/-1Q63s68Tj0Q/VhmsVliOXmI/AAAAAAAADTE/tGn2BHhRuVQ/s72-c/1466229790313874095.png" height="72" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-9149402429183581490.post-8224978452592947571</id><published>2015-10-04T19:31:00.000-07:00</published><updated>2015-10-05T20:23:03.074-07:00</updated><title type='text'>Mixed Models in R - Bigger, Faster, Stronger</title><content type='html'>When you start doing more advanced sports analytics you&#39;ll eventually starting working with what are known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Mixed_model&quot;&gt;hierarchical, nested or mixed effects models&lt;/a&gt;. These are models that contain both &lt;a href=&quot;https://en.wikipedia.org/wiki/Fixed_effects_model&quot;&gt;fixed&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Random_effects_model&quot;&gt;random effects&lt;/a&gt;. There are &lt;a href=&quot;http://andrewgelman.com/2005/01/25/why_i_dont_use/&quot;&gt;multiple ways of defining fixed vs random random effects&lt;/a&gt;, but one way I find particularly useful is that random effects are being &quot;predicted&quot; rather than &quot;estimated&quot;, and this in turn involves some &quot;shrinkage&quot; towards the mean.&lt;br /&gt;&lt;br /&gt;Here&#39;s some R code for NCAA ice hockey power rankings using a nested Poisson model (which can be found in my &lt;a href=&quot;https://github.com/octonion/hockey&quot; target=&quot;_blank&quot;&gt;hockey GitHub repository&lt;/a&gt;):&lt;br /&gt;&lt;pre&gt;model &lt;- gs ~ year+field+d_div+o_div+game_length+(1|offense)+(1|defense)+(1|game_id)&lt;br /&gt;fit &lt;- glmer(model,&lt;br /&gt;             data=g,&lt;br /&gt;      verbose=TRUE,&lt;br /&gt;      family=poisson(link=log)&lt;br /&gt;      )&lt;br /&gt;&lt;/pre&gt;The fixed effects are &lt;b&gt;year&lt;/b&gt;, &lt;b&gt;field&lt;/b&gt; (home/away/neutral), &lt;b&gt;d_div&lt;/b&gt; (NCAA division of the defense), &lt;b&gt;o_div&lt;/b&gt; (NCAA division of the offense) and &lt;b&gt;game_length&lt;/b&gt; (number of overtime periods); &lt;b&gt;offense&lt;/b&gt; (strength of offense), &lt;b&gt;defense&lt;/b&gt; (strength of defense) and &lt;b&gt;game_id&lt;/b&gt; are all random effects. The reason for modeling team offenses and defenses as random vs fixed effects is that I view them as random samples from the same distribution. As mentioned above, this results in &lt;a href=&quot;https://en.wikipedia.org/wiki/Shrinkage_(statistics)&quot;&gt;statistical shrinkage&lt;/a&gt; or &quot;regression to the mean&quot; for these values, which is particularly useful for partially completed seasons. &lt;p/&gt;One of the problems with large mixed models is that they can be very slow to fit. For example, the model above takes several hours on a 12-core workstation, which makes it very difficult to test model changes and tweaks. Is there any way to speed up the fitting process? Certainly! One way is to add two options to the above code: &lt;pre&gt;fit &lt;- glmer(model,&lt;br /&gt;             data=g,&lt;br /&gt;      verbose=TRUE,&lt;br /&gt;      family=poisson(link=log),&lt;br /&gt;      nAGQ=0,&lt;br /&gt;      control=glmerControl(optimizer = &quot;nloptwrap&quot;)&lt;br /&gt;      )&lt;br /&gt;&lt;/pre&gt;What do these do? Model fitting is an optimization process. Part of that process involves the estimation of particular integrals, which can be very slow; the option &quot;nAGQ=0&quot; tells glmer to ignore estimating those integrals. For some models this has minimal impact on parameter estimates, and this NCAA hockey model is one of those. The second option tells glmer to fit using the &quot;nloptwrap&quot; optimizer (there are several other optimizers available, too), which tends to be faster than the default optimization method.&lt;br /&gt;&lt;br /&gt;The impact can be rather startling. With the default options the above model takes about 3 hours to fit. Add these two options, and the model fitting takes 30 seconds with minimal impact on the parameter estimates, or approximately 400 times faster.</content><link rel='replies' type='application/atom+xml' href='http://angrystatistician.blogspot.com/feeds/8224978452592947571/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://angrystatistician.blogspot.com/2015/10/mixed-models-in-r-bigger-faster-stronger.html#comment-form' title='4 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/8224978452592947571'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/8224978452592947571'/><link rel='alternate' type='text/html' href='http://angrystatistician.blogspot.com/2015/10/mixed-models-in-r-bigger-faster-stronger.html' title='Mixed Models in R - Bigger, Faster, Stronger'/><author><name>Christopher D. Long</name><uri>http://www.blogger.com/profile/13687149457345266350</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><thr:total>4</thr:total></entry><entry><id>tag:blogger.com,1999:blog-9149402429183581490.post-7371474520122350280</id><published>2015-10-01T23:14:00.000-07:00</published><updated>2015-10-02T14:18:15.530-07:00</updated><title type='text'>Elo&#39;s Rating System as a Forgetful Logistic Model</title><content type='html'>&lt;a href=&quot;https://en.wikipedia.org/wiki/Elo_rating_system&quot; target=&quot;_blank&quot;&gt;Elo&#39;s rating system&lt;/a&gt; became famous from its use in chess, but it and variations are now used in &lt;a href=&quot;http://fivethirtyeight.com/datalab/introducing-nfl-elo-ratings/&quot; target=&quot;_blank&quot;&gt;sports like the NFL&lt;/a&gt; to &lt;a href=&quot;http://leagueoflegends.wikia.com/wiki/Elo_rating_system&quot; target=&quot;_blank&quot;&gt;eSports like League of Legends&lt;/a&gt;. It also was infamously used on various &quot;Hot or Not&quot; type websites, as shown in this scene from the movie &quot;Social Network&quot;:&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;iframe allowfullscreen=&quot;&quot; class=&quot;YOUTUBE-iframe-video&quot; data-thumbnail-src=&quot;https://i.ytimg.com/vi/BzZRr4KV59I/0.jpg&quot; frameborder=&quot;0&quot; height=&quot;266&quot; src=&quot;https://www.youtube.com/embed/BzZRr4KV59I?feature=player_embedded&quot; width=&quot;320&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Of course, there&#39;s a mistake in the formula in the movie!&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;What is the Elo rating system? As originally proposed, it presumes that if two players A and B have ratings \(R_A\) and \(R_B\), then the expected score of player A is \[\frac{1}{1+10^{\frac{R_B-R_A}{400}}}.\] Furthermore, if A has a current rating of \(R_A\) and plays some more games, then the updated rating \({R_A}&#39;\) is given by \({R_A}&#39; = R_A + K(S_A-E_A)\), where \(K\) is an adjustment factor, \(S_A\) is the number of points scored by A and \(E_A\) was the expected number of points scored by A based on the rating \(R_A\).&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Now, the expected score formula given above has the same form as a &lt;a href=&quot;https://en.wikipedia.org/wiki/Logistic_regression&quot; target=&quot;_blank&quot;&gt;logistic regression model&lt;/a&gt;. What&#39;s the connection between the two? One answer is that Elo&#39;s rating system is a type of&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/Online_algorithm&quot; target=&quot;_blank&quot;&gt;online&lt;/a&gt; version of a logistic model. An online algorithm is an algorithm that only sees each piece of data once. As applied to a statistical model, it&#39;s a model with parameter estimates that are updated as new data comes in, but not refitting on the entire data set. It can also be considered a&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/Memorylessness&quot; target=&quot;_blank&quot;&gt;memoryless&lt;/a&gt; model; it has &quot;forgotten&quot; the old data and only knows the current parameter estimates. The appeal of such models is that they&#39;re extremely efficient, can operate on enormous data sets and parameter estimates can be updated in real-time.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Okay, let&#39;s say we have a &quot;forgetful&quot; logistic model. Can we derive an updating rule, and does it look like Elo&#39;s? I&#39;m going to give one possible derivation under the simplifying assumption that games are won or lost, with no ties.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;We don&#39;t know how many games A and B had previously played, so let&#39;s assume they both had previously played \(n\) games and have just played \(m\) additional games between them, with A scoring \(S_A\) points. That means they&#39;ve both played \(n+m\) games, but we&#39;re just going to forget this again, so let&#39;s adjust everything so that they end up with \(n\) games. One way to do this is to &lt;a href=&quot;https://en.wikipedia.org/wiki/Normalization_(statistics)&quot; target=&quot;_blank&quot;&gt;normalize&lt;/a&gt; \(n\) and \(m\) so that they sum to \(n\), thus \(n\) becomes \(n\frac{n}{n+m}\) and \(m\) becomes \(m\frac{n}{n+m}\).&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;We&#39;re now assuming they had each played&amp;nbsp;\(n\frac{n}{n+m}\) games in the past, have just played&amp;nbsp;\(m\frac{n}{n+m}\) additional games and A scored \(S_A \frac{n}{n+m}\) points (it has to be adjusted, too!) in those games.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Again, we&#39;re memoryless, so we don&#39;t know how strong the opponents were that each had played in the past, so we&#39;re going to assume that they had both played opponents that were as strong as themselves and had won half and lost half of those games. After all, people generally prefer to play competitive games.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;Define \(d\) by \({R_A}&#39; = R_A + d\) and let \(c = R_A - R_B\); we also require that \({R_B}&#39; = R_B - d\). The &lt;a href=&quot;https://en.wikipedia.org/wiki/Likelihood_function#Log-likelihood&quot; target=&quot;_blank&quot;&gt;log-likelihood&lt;/a&gt;&amp;nbsp;\(L\) of A having scored&amp;nbsp;\(S_A \frac{n}{n+m}\) points is \[ \frac{-2 n^2}{n+m}\log(1+e^{-d}) -\frac{n^2}{n+m}d-\frac{m n}{n+m}\log(1+e^{-c-2d}) - \frac{(m-S_A)n(c+2d)}{n+m}.\] Factoring out the constant term \(n/(n+m)\) simplifies this to&amp;nbsp;\[ L = -2 n\log(1+e^{-d}) - n d - m \log(1+e^{-c-2d}) - (m-S_A)(c+2d).\] Taking the partial derivative of \(L\) with respect to \(d\) we get&lt;br /&gt;\begin{align}&lt;br /&gt;\frac{\partial L}{\partial d} &amp;amp;= 2n \frac{e^{-d}}{1+e^{-d}} -n + 2m \frac{e^{-c-2d}}{1+e^{-c-2d}}-2(m-S_A) \\&lt;br /&gt;&amp;amp;= -n\frac{1-e^{-d}}{1+e^{-d}} + 2 S_A - 2m\frac{1}{1+e^{-c-2d}} \\&lt;br /&gt;&amp;amp;= -n\tanh(d/2) + 2 S_A - 2m\frac{1}{1+e^{-c-2d}}.&lt;br /&gt;\end{align} What is \( m\frac{1}{1+e^{-c-2d}} \)? This is actually just \( {E_A}&#39; \), the expected score for A when playing B for \(m\) games, but assuming the updated ratings for both players. Finally, setting \(\frac{\partial L}{\partial d} = 0\), we get \[ n\tanh(d/2) = 2(S_A - {E_A}&#39;)\] and hence \[ \tanh(d/2) = \frac{2}{n} (S_A - {E_A}&#39;).\] Assuming \(n\) is large relative to \(S_A - {E_A}&#39;\), we have \( \tanh(d/2) \approx d/2\) and \( {E_A}&#39; \approx E_A \). This is Elo&#39;s updating rule in the form \[ d = \frac{4}{n} (S_A - E_A ).\] If we rescale with the constant \( \sigma \), the updating rule becomes \[ d = \frac{4\sigma }{n} (S_A - E_A ).\] We also now see that the adjustment factor \( K = \frac{4\sigma }{n}\).</content><link rel='replies' type='application/atom+xml' href='http://angrystatistician.blogspot.com/feeds/7371474520122350280/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://angrystatistician.blogspot.com/2015/10/elos-rating-system-as-forgetful.html#comment-form' title='2 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/7371474520122350280'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/7371474520122350280'/><link rel='alternate' type='text/html' href='http://angrystatistician.blogspot.com/2015/10/elos-rating-system-as-forgetful.html' title='Elo&#39;s Rating System as a Forgetful Logistic Model'/><author><name>Christopher D. Long</name><uri>http://www.blogger.com/profile/13687149457345266350</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://img.youtube.com/vi/BzZRr4KV59I/default.jpg" height="72" width="72"/><thr:total>2</thr:total></entry><entry><id>tag:blogger.com,1999:blog-9149402429183581490.post-3689344049108158170</id><published>2015-07-11T19:54:00.000-07:00</published><updated>2015-07-11T20:08:13.781-07:00</updated><title type='text'>Power Rankings: Looking at a Very Simple Method</title><content type='html'>One of the simplest and most common power ranking models is known as the &lt;a href=&quot;https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model&quot; target=&quot;_blank&quot;&gt;Bradley-Terry-Luce model&lt;/a&gt;, which is &lt;a href=&quot;http://angrystatistician.blogspot.com/2013/03/baseball-chess-psychology-and.html&quot; target=&quot;_blank&quot;&gt;equivalent to other famous models&lt;/a&gt; such the &lt;a href=&quot;https://en.wikipedia.org/wiki/Logistic_regression&quot; target=&quot;_blank&quot;&gt;logistic model&lt;/a&gt; and the &lt;a href=&quot;https://en.wikipedia.org/wiki/Elo_rating_system&quot; target=&quot;_blank&quot;&gt;Elo rating system&lt;/a&gt;. I&#39;ll be referring to &quot;teams&quot; here, but of course the same ideas apply to any two-participant game.&lt;br /&gt;&lt;br /&gt;Let me clarify what I mean when I use the term &quot;power ranking&quot;. A power ranking supplies not only a ranking of teams, but also provides numbers that may be used to estimate the probabilities of various outcomes were two particular teams to play a match.&lt;br /&gt;&lt;br /&gt;In the BTL power ranking system we assume the teams have some latent (hidden/unknown) &quot;strength&quot; \(R_i\), and that the probability of \(i\) beating \(j\) is \( \frac{R_i}{R_i+R_j} \). Note that each \(R_i\) is assumed to be strictly positive. Where does this model structure come from?&lt;br /&gt;&lt;br /&gt;Here are three reasonable constraints for a power ranking model:&lt;br /&gt;&lt;ol&gt;&lt;li&gt;&amp;nbsp;If \(R_i\) and \(R_j\) have equal strength, the probability of one beating the other should be \( \frac{1}{2}\).&lt;/li&gt;&lt;li&gt;As the strength of one team strictly approaches 0 (infinitely weak) with the other team fixed, the probability of the other team winning strictly increases to 1.&lt;/li&gt;&lt;li&gt;As the strength of one team strictly approaches 1 (infinitely strong) with the other team fixed, the probability of the other team winning strictly decreases to 0.&lt;/li&gt;&lt;/ol&gt;&lt;div&gt;Note that our model structure satisfies all three constraints. Can you think of other simple model structures that satisfy all three constraints?&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Given this model and a set of teams and match results, how can we estimate the \(R_i\). The &lt;a href=&quot;https://en.wikipedia.org/wiki/Maximum_likelihood&quot; target=&quot;_blank&quot;&gt;maximum-likelihood estimators&lt;/a&gt;&amp;nbsp;are the set of \( R_i \) that maximizes the probability of the observed outcomes actually happening. For any given match this probability of team \( i \) beating team \( j \) is&amp;nbsp;\( \frac{R_i}{R_i+R_j} \), so the overall probability of the observed outcomes of the matches \( M \) occurring is \[ \mathcal{L} = \prod_{m\in M} \frac{R_{w(m)}}{R_{w(m)}+R_{l(m)}},\] where \( w(m) \) is then winner and \( l(m) \) the loser of match \( m \). We can transform this into a sum by taking logarithms; \[ \log\left( \mathcal{L} \right) = \log\left(R_{w(m)}\right) - \log\left(R_{w(m)}+R_{l(m)}\right).\] Before going further, let&#39;s make a useful reparameterization by setting \( e^{r_i} = R_i \); this makes sense as we&#39;re requiring the \( R_i \) to be strictly positive. We then get \[&amp;nbsp;\log\left( \mathcal{L} \right) = r_{w(m)} - \log\left(e^{r_{w(m)}}+e^{r_{l(m)}}\right).\] Taking partial derivatives we get \begin{eqnarray*}&lt;br /&gt;\frac{\partial \log\left( \mathcal{L} \right)}{\partial r_i} &amp;amp;=&amp;amp; \sum_{w(m)=i} 1 - \frac{e^{r_{w(m)}}}{e^{r_{w(m)}}+e^{r_{l(m)}}} + \sum_{l(m)=i} - \frac{e^{r_{l(m)}}}{e^{r_{w(m)}}+e^{r_{l(m)}}}\\&lt;br /&gt;&amp;amp;=&amp;amp; \sum_{w(m)=i} 1 - \frac{e^{r_i}}{e^{r_i}+e^{r_{l(m)}}} + \sum_{l(m)=i} - \frac{e^{r_i}}{e^{r_{w(m)}}+e^{r_i}}\\&lt;br /&gt;&amp;amp;=&amp;amp;0.&lt;br /&gt;\end{eqnarray*} But this is just the number of actual wins minus the expected wins! Thus, the maximum likelihood estimators for the \( r_i \) satisfy \( O_i = E_i \) for all teams \( i \), where \( O_i \) is the actual (observed) number of wins for team \( i \), and \( E_i \) is the expected number of wins for team \( i \) based on our model. That&#39;s a nice property!&lt;br /&gt;&lt;br /&gt;If you&#39;d like to experiment with some actual data, and to see that the resulting fit does indeed satisfy this property, here&#39;s an &lt;a href=&quot;https://github.com/octonion/hockey/tree/master/lunchtime&quot; target=&quot;_blank&quot;&gt;example BTL model using NCAA men&#39;s ice hockey scores&lt;/a&gt;. You can, of course, actually use this property to iteratively solve for the MLE estimators \( R_i \). Note that you&#39;ll have to fix one of the \( R_i \) to be a particular value (or add some other constraint), as the model probabilities are invariant with respect to multiplication of the \( R_i \) by the same positive scalar.&lt;/div&gt;</content><link rel='replies' type='application/atom+xml' href='http://angrystatistician.blogspot.com/feeds/3689344049108158170/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://angrystatistician.blogspot.com/2015/07/power-rankings-looking-at-very-simple.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/3689344049108158170'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/3689344049108158170'/><link rel='alternate' type='text/html' href='http://angrystatistician.blogspot.com/2015/07/power-rankings-looking-at-very-simple.html' title='Power Rankings: Looking at a Very Simple Method'/><author><name>Christopher D. Long</name><uri>http://www.blogger.com/profile/13687149457345266350</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-9149402429183581490.post-8665826341097287811</id><published>2015-07-11T15:00:00.001-07:00</published><updated>2015-07-11T15:00:38.036-07:00</updated><title type='text'>Getting Started Doing Baseball Analysis without Coding</title><content type='html'>There&#39;s lot of confusion about how best to get started doing baseball analysis. &lt;a href=&quot;http://www.dataphoric.com/2015/06/27/learn_data_science_the_hard_way/&quot; target=&quot;_blank&quot;&gt;It doesn&#39;t have to be difficult!&lt;/a&gt; You can start doing it right away, even if you don&#39;t know anything about R, Python, Ruby, SQL or machine learning (most GMs can&#39;t code). Learning these and other tools makes it easier and faster to do analysis, but they&#39;re only part of the process of constructing a well-reasoned argument. They&#39;re important, of course, because they can turn 2 months of hard work into 10 minutes of typing. Even if you don&#39;t like mathematics, statistics, coding or databases, they&#39;re mundane necessities that can make your life much easier and your analysis more powerful.&lt;br /&gt;&lt;br /&gt;Here are two example problems. You don&#39;t have to do these specifically, but they illustrate the general idea. Write up your solutions, then publish them for other people to make some (hopefully) helpful comments and suggestions. This can be on a blog or through a versioning control platform like &lt;a href=&quot;https://github.com/&quot; target=&quot;_blank&quot;&gt;GitHub&lt;/a&gt; (which is also great for versioning any code or data your use). Try to write well! A great argument, but poorly written and poorly presented isn&#39;t going to be very convincing. Once it&#39;s finished, review and revise, review and revise, review and revise. When a team you follow makes a move, treat it as a puzzle for you to solve. Why did they do it, and was it a good idea?&lt;br /&gt;&lt;ol&gt;&lt;li&gt;Pick a recent baseball trade. For example, the Padres traded catcher Yasmani Grandal for Dodgers outfielder Matt Kemp. It&#39;s never that simple of course; the Padres aren&#39;t paying all of Matt Kemp&#39;s salary. Find out what the salary obligations were for each club in this trade. Using your favorite public projection system, where were the projected surplus values for each player at the time of the trade? Of course, there were &lt;a href=&quot;http://www.si.com/mlb/2014/12/18/matt-kemp-arthritic-hips-dodgers-padres-trade&quot; target=&quot;_blank&quot;&gt;other players&lt;/a&gt;&amp;nbsp;involved in that trade, too. What were the expected surplus values of those players? From the perspective of surplus values, who won or lost this trade? Finally, why do you think each team made this trade, especially considering that they were division rivals? Do you think one or both teams made any mistakes in reasoning; if so, what were they, and did the other team take advantage of those mistakes?&lt;/li&gt;&lt;li&gt;Pick any MLB team and review the &lt;a href=&quot;http://mlb.mlb.com/mlb/events/draft/y2015/drafttracker.jsp&quot; target=&quot;_blank&quot;&gt;draft picks they made in the 2015 draft&lt;/a&gt; for the first 10 rounds. Do you notice any trends or &lt;a href=&quot;http://mlb.mlb.com/mlb/events/draft/y2014/drafttracker.jsp&quot; target=&quot;_blank&quot;&gt;changes from the 2014 draft&lt;/a&gt;? Do these picks agree or disagree with the various public pre-draft player rankings? Which picks were designed to save money to help sign other picks? Identify those tough signs. Was the team actually able to sign them, and were the picks to save money still reasonably good picks? Do you best to identify which picks you thought were good and bad, write them down in a notebook with your reasoning, then check back in 6 months and a year. Was your reasoning correct? If not, what were your mistakes and how can you avoid making them in the future?&lt;/li&gt;&lt;/ol&gt;</content><link rel='replies' type='application/atom+xml' href='http://angrystatistician.blogspot.com/feeds/8665826341097287811/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://angrystatistician.blogspot.com/2015/07/getting-started-doing-baseball-analysis.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/8665826341097287811'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/8665826341097287811'/><link rel='alternate' type='text/html' href='http://angrystatistician.blogspot.com/2015/07/getting-started-doing-baseball-analysis.html' title='Getting Started Doing Baseball Analysis without Coding'/><author><name>Christopher D. Long</name><uri>http://www.blogger.com/profile/13687149457345266350</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-9149402429183581490.post-8962656961286759513</id><published>2015-03-08T17:27:00.001-07:00</published><updated>2015-03-08T17:27:27.572-07:00</updated><title type='text'>Some Potentially Useful SQL Resources</title><content type='html'>&lt;br /&gt;Some potentially useful SQL resources - explanations, visualizations, exercises, games, classes.&lt;br /&gt;&lt;ol&gt;&lt;li&gt;&lt;a href=&quot;http://blog.codinghorror.com/a-visual-explanation-of-sql-joins/&quot; target=&quot;_blank&quot;&gt;A Visual Explanation of SQL Joins&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;http://datamonkey.pro/&quot; target=&quot;_blank&quot;&gt;Datamonkey&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://my.vanderbilt.edu/cs265/&quot; target=&quot;_blank&quot;&gt;Introduction to Database Management Systems&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;http://wwwlgis.informatik.uni-kl.de/cms/courses/informationssysteme/sqlisland/&quot; target=&quot;_blank&quot;&gt;SQL Island Adventure Game&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;http://pgexercises.com/&quot; target=&quot;_blank&quot;&gt;PostgreSQL Exercises&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;http://www.padjo.org/&quot; target=&quot;_blank&quot;&gt;Public Affairs Data Journalism&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/rhc2104/sqlteaching&quot; target=&quot;_blank&quot;&gt;SQL Teaching&#39;s GitHub repo (if you&#39;re curious)&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://class.stanford.edu/courses/DB/2014/SelfPaced/about&quot; target=&quot;_blank&quot;&gt;Stanford&#39;s Self-Paced Database MOOC&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;http://hackr.io/tutorials/sql&quot; target=&quot;_blank&quot;&gt;Hackr.io&#39;s SQL Section (good to check occasionally)&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;http://www.sql-ex.ru/&quot; target=&quot;_blank&quot;&gt;Practical skills of SQL language&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;http://www.sqlteaching.com/&quot; target=&quot;_blank&quot;&gt;SQL Teaching (learn SQL in your browser)&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;http://sqlzoo.net/wiki/Main_Page&quot; target=&quot;_blank&quot;&gt;SQLZOO - Interactive SQL Tutorial&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://schemaverse.com/&quot; target=&quot;_blank&quot;&gt;The Schemaverse: a space-based strategy game implemented entirely within a PostgreSQL database&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;http://blog.treasuredata.com/blog/2014/12/05/learn-sql-by-calculating-customer-lifetime-value-part-1/&quot; target=&quot;_blank&quot;&gt;Treasure Data: Learn SQL by Calculating Customer Lifetime Value&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</content><link rel='replies' type='application/atom+xml' href='http://angrystatistician.blogspot.com/feeds/8962656961286759513/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://angrystatistician.blogspot.com/2015/03/some-potentially-useful-sql-resources.html#comment-form' title='2 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/8962656961286759513'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/8962656961286759513'/><link rel='alternate' type='text/html' href='http://angrystatistician.blogspot.com/2015/03/some-potentially-useful-sql-resources.html' title='Some Potentially Useful SQL Resources'/><author><name>Christopher D. Long</name><uri>http://www.blogger.com/profile/13687149457345266350</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><thr:total>2</thr:total></entry><entry><id>tag:blogger.com,1999:blog-9149402429183581490.post-7862524207431434181</id><published>2015-03-03T00:56:00.002-08:00</published><updated>2015-03-03T00:56:53.621-08:00</updated><title type='text'>Who Controls the Pace in Basketball, Offense or Defense?</title><content type='html'>During a recent chat with basketball analyst &lt;a href=&quot;https://twitter.com/sethpartnow&quot; target=&quot;_blank&quot;&gt;Seth Partnow&lt;/a&gt;, he mentioned a topic that came up during a discussion at the recent &lt;a href=&quot;http://www.sloansportsconference.com/&quot; target=&quot;_blank&quot;&gt;MIT Sloan Sports Analytics Conference&lt;/a&gt;. Who &amp;nbsp;controls the pace of a game more, the offense or defense? And what is the percentage of pace responsibility for each side? The analysts came up with a rough consensus opinion, but is there a way to answer this question analytically? I came up with one approach that examines the variations in possession times, but it suddenly occurred to me that this question could also be answered immediately by looking at the offense-defense asymmetry of the home court advantage.&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;As you can see in the &lt;a href=&quot;https://github.com/octonion/basketball-m/blob/master/ncaa/diagnostics/lmer.txt&quot; target=&quot;_blank&quot;&gt;R output of my NCAA team model code&lt;/a&gt; in one of my &lt;a href=&quot;https://github.com/octonion/basketball-m&quot; target=&quot;_blank&quot;&gt;public basketball repositories&lt;/a&gt;, the offense at home scores points at a rate about \( e^{0.0302} =&amp;nbsp;1.031 \) times the rate on a neutral court, everything else the same. Likewise, the defense at home allows points at a rate about \( e^{-0.0165} =&amp;nbsp;0.984\) times the rate on a neutral court; in both cases the neutral court rate is the reference level. Notice the geometric asymmetry; \(&amp;nbsp;1.031\cdot 0.984 = 1.015 &amp;gt; 1\). The implication is that the offense is responsible for about the fraction \[ \frac{(1.031-1)}{(1.031-1)+(1-0.984)} = 0.66 \] of the scoring pace. That is, offensive controls 2/3 of the pace, defense 1/3 of the pace. The consensus opinion the analysts came up with at Sloan? It was 2/3 offense, 1/3 defense! It&#39;s nice when things work out, isn&#39;t it?&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;I&#39;ve used NCAA basketball because there are plenty of neutral court games; to examine the NBA directly we&#39;ll have to use a more sophisticated (but perhaps&amp;nbsp;&lt;a href=&quot;http://en.wikipedia.org/wiki/Mathematical_beauty&quot; target=&quot;_blank&quot;&gt;less beautiful&lt;/a&gt;) approach involving the variation in possession times. I&#39;ll do that next, and I&#39;ll also show you how to apply this new information to make better &lt;a href=&quot;https://github.com/octonion/basketball-m/blob/master/ncaa/sos/predict_daily.txt&quot; target=&quot;_blank&quot;&gt;game predictions&lt;/a&gt;. Finally, there&#39;s a nice connection to some recent work on &lt;a href=&quot;http://qz.com/316826/mathematicians-have-finally-figured-out-how-to-tell-correlation-from-causation/&quot; target=&quot;_blank&quot;&gt;inferring causality&lt;/a&gt; that I&#39;ll outline.&lt;/div&gt;</content><link rel='replies' type='application/atom+xml' href='http://angrystatistician.blogspot.com/feeds/7862524207431434181/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://angrystatistician.blogspot.com/2015/03/who-controls-pace-in-basketball-offense.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/7862524207431434181'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/9149402429183581490/posts/default/7862524207431434181'/><link rel='alternate' type='text/html' href='http://angrystatistician.blogspot.com/2015/03/who-controls-pace-in-basketball-offense.html' title='Who Controls the Pace in Basketball, Offense or Defense?'/><author><name>Christopher D. Long</name><uri>http://www.blogger.com/profile/13687149457345266350</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><thr:total>0</thr:total></entry></feed>