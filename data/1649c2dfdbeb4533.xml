<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Building Babylon</title>
	<atom:link href="http://building-babylon.net/feed/" rel="self" type="application/rss+xml" />
	<link>https://building-babylon.net</link>
	<description>Notes on Machine Learning &#38; Mathematics</description>
	<lastBuildDate>Tue, 08 May 2018 14:44:51 +0000</lastBuildDate>
	<language>en-AU</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.0.3</generator>
	<item>
		<title>Siegelmann &#038; Sontag’s “On the Computational Power of Neural Nets”</title>
		<link>https://building-babylon.net/2018/05/08/siegelmann-sontags-on-the-computational-power-of-neural-nets/</link>
					<comments>https://building-babylon.net/2018/05/08/siegelmann-sontags-on-the-computational-power-of-neural-nets/#respond</comments>
		
		<dc:creator><![CDATA[Benjamin]]></dc:creator>
		<pubDate>Tue, 08 May 2018 14:41:28 +0000</pubDate>
				<category><![CDATA[Talks]]></category>
		<category><![CDATA[recurrent neural nets]]></category>
		<category><![CDATA[turing machines]]></category>
		<guid isPermaLink="false">http://building-babylon.net/?p=11168</guid>

					<description><![CDATA[Here are the slides from a talk I gave the Sydney machine learning meetup on Siegelmann and Sontag&#8217;s paper from 1995 “On the Computational Power of Neural Nets”, showing that recurrent neural networks are Turing complete. It is a fantastic paper, though it is a lot to present in a single talk. I spent some &#8230; <p class="link-more"><a href="https://building-babylon.net/2018/05/08/siegelmann-sontags-on-the-computational-power-of-neural-nets/" class="more-link">Continue reading<span class="screen-reader-text"> "Siegelmann &#038; Sontag’s “On the Computational Power of Neural Nets”"</span></a></p>]]></description>
		
					<wfw:commentRss>https://building-babylon.net/2018/05/08/siegelmann-sontags-on-the-computational-power-of-neural-nets/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>Graph embeddings in Hyperbolic Space</title>
		<link>https://building-babylon.net/2018/04/10/graph-embeddings-in-hyperbolic-space/</link>
					<comments>https://building-babylon.net/2018/04/10/graph-embeddings-in-hyperbolic-space/#comments</comments>
		
		<dc:creator><![CDATA[Benjamin]]></dc:creator>
		<pubDate>Tue, 10 Apr 2018 15:20:21 +0000</pubDate>
				<category><![CDATA[Geometry]]></category>
		<category><![CDATA[Optimisation]]></category>
		<category><![CDATA[Talks]]></category>
		<category><![CDATA[gradient descent]]></category>
		<category><![CDATA[hyperbolic space]]></category>
		<category><![CDATA[talk]]></category>
		<guid isPermaLink="false">http://building-babylon.net/?p=11163</guid>

					<description><![CDATA[I gave a talk last night at the Berlin machine learning meetup on learning graph embeddings in hyperbolic space, featuring the recent NIPS 2017 paper of Nickel &#038; Kiela. Covered are: An illustration of why the Euclidean plane is not a good place to embed trees (since circle circumference grows only linearly in the radius); &#8230; <p class="link-more"><a href="https://building-babylon.net/2018/04/10/graph-embeddings-in-hyperbolic-space/" class="more-link">Continue reading<span class="screen-reader-text"> "Graph embeddings in Hyperbolic Space"</span></a></p>]]></description>
		
					<wfw:commentRss>https://building-babylon.net/2018/04/10/graph-embeddings-in-hyperbolic-space/feed/</wfw:commentRss>
			<slash:comments>1</slash:comments>
		
		
			</item>
		<item>
		<title>Gradient optimisation on the Poincaré disc</title>
		<link>https://building-babylon.net/2018/04/10/gradient-optimisation-on-the-poincare-disc/</link>
					<comments>https://building-babylon.net/2018/04/10/gradient-optimisation-on-the-poincare-disc/#respond</comments>
		
		<dc:creator><![CDATA[Benjamin]]></dc:creator>
		<pubDate>Tue, 10 Apr 2018 14:58:13 +0000</pubDate>
				<category><![CDATA[Geometry]]></category>
		<category><![CDATA[gradient descent]]></category>
		<category><![CDATA[Optimisation]]></category>
		<category><![CDATA[hyperbolic space]]></category>
		<category><![CDATA[poincare disc]]></category>
		<guid isPermaLink="false">http://building-babylon.net/?p=11156</guid>

					<description><![CDATA[Nickel &#038; Kiela had a great paper on embedding graphs in hyperbolic space at NIPS 2017. They work with the Poincaré ball model of hyperbolic space. This is just the interior of the unit ball, equipped with an appropriate Riemannian metric. This metric is conformal, meaning that the inner product on the tangent spaces on &#8230; <p class="link-more"><a href="https://building-babylon.net/2018/04/10/gradient-optimisation-on-the-poincare-disc/" class="more-link">Continue reading<span class="screen-reader-text"> "Gradient optimisation on the Poincaré disc"</span></a></p>]]></description>
		
					<wfw:commentRss>https://building-babylon.net/2018/04/10/gradient-optimisation-on-the-poincare-disc/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>Circle circumference in the hyperbolic plane is exponential in the radius: proof by computer game</title>
		<link>https://building-babylon.net/2018/04/10/circle-circumference-in-the-hyperbolic-plane-is-exponential-in-the-radius-a-proof-by-computer-game/</link>
					<comments>https://building-babylon.net/2018/04/10/circle-circumference-in-the-hyperbolic-plane-is-exponential-in-the-radius-a-proof-by-computer-game/#respond</comments>
		
		<dc:creator><![CDATA[Benjamin]]></dc:creator>
		<pubDate>Tue, 10 Apr 2018 13:25:58 +0000</pubDate>
				<category><![CDATA[Geometry]]></category>
		<category><![CDATA[Teaching]]></category>
		<category><![CDATA[hyperbolic space]]></category>
		<category><![CDATA[poincare disc]]></category>
		<guid isPermaLink="false">http://building-babylon.net/?p=11150</guid>

					<description><![CDATA[I recently needed to demonstrate this fact to an audience that I could not assume would be familiar with Riemannian geometry, and it took some time to find a way to do it! You can use the HyperRogue game, which takes place on a tiling of the Poincaré disc. The avatar moves across the Poincaré &#8230; <p class="link-more"><a href="https://building-babylon.net/2018/04/10/circle-circumference-in-the-hyperbolic-plane-is-exponential-in-the-radius-a-proof-by-computer-game/" class="more-link">Continue reading<span class="screen-reader-text"> "Circle circumference in the hyperbolic plane is exponential in the radius: proof by computer game"</span></a></p>]]></description>
		
					<wfw:commentRss>https://building-babylon.net/2018/04/10/circle-circumference-in-the-hyperbolic-plane-is-exponential-in-the-radius-a-proof-by-computer-game/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>Hierarchical Softmax</title>
		<link>https://building-babylon.net/2017/08/01/hierarchical-softmax/</link>
					<comments>https://building-babylon.net/2017/08/01/hierarchical-softmax/#comments</comments>
		
		<dc:creator><![CDATA[Benjamin]]></dc:creator>
		<pubDate>Tue, 01 Aug 2017 08:29:58 +0000</pubDate>
				<category><![CDATA[Optimisation]]></category>
		<category><![CDATA[hierarchical softmax]]></category>
		<category><![CDATA[word classes]]></category>
		<category><![CDATA[word2vec]]></category>
		<guid isPermaLink="false">http://building-babylon.net/?p=11122</guid>

					<description><![CDATA[[These are the notes from a talk I gave at the seminar] Hierarchical softmax is an alternative to the softmax in which the probability of any one outcome depends on a number of model parameters that is only logarithmic in the total number of outcomes. In &#8220;vanilla&#8221; softmax, on the other hand, the number of &#8230; <p class="link-more"><a href="https://building-babylon.net/2017/08/01/hierarchical-softmax/" class="more-link">Continue reading<span class="screen-reader-text"> "Hierarchical Softmax"</span></a></p>]]></description>
		
					<wfw:commentRss>https://building-babylon.net/2017/08/01/hierarchical-softmax/feed/</wfw:commentRss>
			<slash:comments>9</slash:comments>
		
		
			</item>
		<item>
		<title>Minsky &#038; Papert&#8217;s &#8220;Perceptrons&#8221;</title>
		<link>https://building-babylon.net/2017/06/08/minsky-paperts-perceptrons/</link>
					<comments>https://building-babylon.net/2017/06/08/minsky-paperts-perceptrons/#comments</comments>
		
		<dc:creator><![CDATA[Benjamin]]></dc:creator>
		<pubDate>Thu, 08 Jun 2017 10:25:51 +0000</pubDate>
				<category><![CDATA[History]]></category>
		<category><![CDATA[history]]></category>
		<category><![CDATA[impossibility]]></category>
		<category><![CDATA[perceptrons]]></category>
		<guid isPermaLink="false">http://building-babylon.net/?p=11114</guid>

					<description><![CDATA[In their book &#8220;Perceptrons&#8221; (1969), Minsky and Papert demonstrate that a simplified version of Rosenblatt&#8217;s perceptron can not perform certain natural binary classification tasks, unless it uses an unmanageably large number of input predicates. It is easy to show that with sufficiently many input predicates, a perceptron (even on this type) can perform any classification &#8230; <p class="link-more"><a href="https://building-babylon.net/2017/06/08/minsky-paperts-perceptrons/" class="more-link">Continue reading<span class="screen-reader-text"> "Minsky &#038; Papert&#8217;s &#8220;Perceptrons&#8221;"</span></a></p>]]></description>
		
					<wfw:commentRss>https://building-babylon.net/2017/06/08/minsky-paperts-perceptrons/feed/</wfw:commentRss>
			<slash:comments>1</slash:comments>
		
		
			</item>
		<item>
		<title>Re-parameterising for non-negativity yields multiplicative updates</title>
		<link>https://building-babylon.net/2017/04/24/re-parameterising-for-non-negativity-yields-multiplicative-updates/</link>
					<comments>https://building-babylon.net/2017/04/24/re-parameterising-for-non-negativity-yields-multiplicative-updates/#respond</comments>
		
		<dc:creator><![CDATA[Benjamin]]></dc:creator>
		<pubDate>Mon, 24 Apr 2017 12:19:10 +0000</pubDate>
				<category><![CDATA[Optimisation]]></category>
		<category><![CDATA[gradient descent]]></category>
		<category><![CDATA[non-negativity]]></category>
		<guid isPermaLink="false">http://building-babylon.net/?p=11101</guid>

					<description><![CDATA[Suppose you have a model that depends on real-valued parameters, and that you would like to constrain these parameters to be non-negative. For simplicity, suppose the model has a single parameter . Let denote the error function. To constrain to be non-negative, parameterise as the square of a real-valued parameter : &#160; &#160; We can &#8230; <p class="link-more"><a href="https://building-babylon.net/2017/04/24/re-parameterising-for-non-negativity-yields-multiplicative-updates/" class="more-link">Continue reading<span class="screen-reader-text"> "Re-parameterising for non-negativity yields multiplicative updates"</span></a></p>]]></description>
		
					<wfw:commentRss>https://building-babylon.net/2017/04/24/re-parameterising-for-non-negativity-yields-multiplicative-updates/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>Factorisation of stochastic matrices</title>
		<link>https://building-babylon.net/2017/04/23/factorisation-of-stochastic-matrices/</link>
					<comments>https://building-babylon.net/2017/04/23/factorisation-of-stochastic-matrices/#respond</comments>
		
		<dc:creator><![CDATA[Benjamin]]></dc:creator>
		<pubDate>Sun, 23 Apr 2017 20:02:26 +0000</pubDate>
				<category><![CDATA[Uncategorised]]></category>
		<guid isPermaLink="false">http://building-babylon.net/?p=11099</guid>

					<description><![CDATA[Here we derive updates rules for the approximation of a row stochastic matrix by the product of two lower-rank row stochastic matrices using gradient descent. Such a factorisation corresponds to a decomposition &#160; &#160; Both the sum of squares and row-wise cross-entropy functions are considered.]]></description>
		
					<wfw:commentRss>https://building-babylon.net/2017/04/23/factorisation-of-stochastic-matrices/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>Heider and Simmel misinterpreted</title>
		<link>https://building-babylon.net/2017/01/18/heider-and-simmel-misinterpreted/</link>
					<comments>https://building-babylon.net/2017/01/18/heider-and-simmel-misinterpreted/#respond</comments>
		
		<dc:creator><![CDATA[Benjamin]]></dc:creator>
		<pubDate>Wed, 18 Jan 2017 00:00:00 +0000</pubDate>
				<category><![CDATA[Uncategorised]]></category>
		<guid isPermaLink="false">http://building-babylon.net/?p=11076</guid>

					<description><![CDATA[I learnt of the 1944 experiment of Heider and Simmel in the Machine Intelligence workshop at NIPS 2016. The experiment involved showing subjects the video below, and asking them to describe what they saw. If you’ve watched the video, you’ll not be surprised to learn that most of the subjects anthropomorphised the geometric objects (i.e. &#8230; <p class="link-more"><a href="https://building-babylon.net/2017/01/18/heider-and-simmel-misinterpreted/" class="more-link">Continue reading<span class="screen-reader-text"> "Heider and Simmel misinterpreted"</span></a></p>]]></description>
		
					<wfw:commentRss>https://building-babylon.net/2017/01/18/heider-and-simmel-misinterpreted/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>Wine dataset demonstrates importance of feature scaling</title>
		<link>https://building-babylon.net/2017/01/17/wine-dataset-demonstrates-importance-of-feature-scaling/</link>
					<comments>https://building-babylon.net/2017/01/17/wine-dataset-demonstrates-importance-of-feature-scaling/#respond</comments>
		
		<dc:creator><![CDATA[Benjamin]]></dc:creator>
		<pubDate>Tue, 17 Jan 2017 07:52:03 +0000</pubDate>
				<category><![CDATA[Teaching]]></category>
		<guid isPermaLink="false">http://building-babylon.net/?p=11057</guid>

					<description><![CDATA[The UCI hosts a dataset of wine measurements that is fantastic for demonstrating the importance of feature scaling in unsupervised learning . There are a bunch of real-valued measurements (of e.g. chemical composition) for three varieties of wine: Barolo, Grignolino and Barbera. I am using this dataset to introduce feature scaling my course on DataCamp. &#8230; <p class="link-more"><a href="https://building-babylon.net/2017/01/17/wine-dataset-demonstrates-importance-of-feature-scaling/" class="more-link">Continue reading<span class="screen-reader-text"> "Wine dataset demonstrates importance of feature scaling"</span></a></p>]]></description>
		
					<wfw:commentRss>https://building-babylon.net/2017/01/17/wine-dataset-demonstrates-importance-of-feature-scaling/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
	</channel>
</rss>
