<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/"><channel><title>IEEE Spectrum</title><link>https://spectrum.ieee.org/</link><description>IEEE Spectrum</description><atom:link href="https://spectrum.ieee.org/feeds/topic/semiconductors.rss" rel="self"></atom:link><language>en-us</language><lastBuildDate>Sat, 05 Nov 2022 16:00:00 -0000</lastBuildDate><image><url>https://spectrum.ieee.org/media-library/eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy8yNjg4NDUyMC9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTY5OTk5OTQzOX0.aimbeagNFKGtififsLPFvztNYGr1_NMvLOOT1mPOjEU/image.png?width=210</url><link>https://spectrum.ieee.org/</link><title>IEEE Spectrum</title></image><item><title>Electric Play-Doh Is Mysterious Conductor</title><link>https://spectrum.ieee.org/conductive-polymers</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-black-disc-with-gold-nodes-on-the-corners-against-a-gray-background.jpg?id=32045677&width=2000&height=1500&coordinates=0%2C135%2C0%2C136"/><br/><br/><p>"Electrically conductive Play-Doh" is how scientists describe new materials they can make like plastic but conduct like metal. It remains a mystery why they conduct electricity so well, since their doing so flies in the face of much of what is known about conductivity. Indeed, the stretchy conductor's surprising capabilities may open up new directions and technological applications in electronics, researchers say.<br/></p><p>The largest group of conductors are metals such as copper, gold and aluminum. Then, about 50 years ago, <u><a href="https://en.wikipedia.org/wiki/Conductive_polymer" target="_blank">conductive organic polymers</a></u> such as <u><a href="https://en.wikipedia.org/wiki/Polyacetylene" rel="noopener noreferrer" target="_blank">polyacetylene</a></u> entered the scene that proved more flexible and easier to process than metals. However, conductive organic polymers are less stable than metals, typically losing their conductivity if exposed to moisture or too much heat.</p><p>The electrical conductivity of both metals and conductive organic polymers depend on straight, closely packed rows of atoms or molecules through which electrons can easily flow, much like cars on a highway. Scientists had thought a material had to have these orderly rows to conduct electricity well.</p><p>Now, in a new study, chemists reveal they have unexpectedly developed new materials that are conductors even though they possess disorderly structures. "The exact reason for the high conductivity of the materials is still a mystery," says study senior author John Anderson, a chemist at the University of Chicago.</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="boxes of different intensity of lights" class="rm-shortcode" data-rm-shortcode-id="2b697a47f475e45c7b54bf5ca08324fc" data-rm-shortcode-name="rebelmouse-image" id="6fb97" loading="lazy" src="https://spectrum.ieee.org/media-library/boxes-of-different-intensity-of-lights.jpg?id=32045704&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">The brightness of yellow LED bulbs while they touch different conductive materials. Left to right: a pellet of the conductive organic polymer PEDOT:PSS; a NiTTFtt pellet; a NiTTFtt pellet with gold nodes; a copper plate.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Frank Wegloski</small></p><p>The scientists experimented with polymers made using an electron-rich sulfur-based molecule named tetrathiafulvalene tetrathiolate (TTFtt). Much remained uncertain about these materials because they were difficult to synthesize. Recently, the researchers developed new techniques for synthesizing these polymers and began exploring their properties.</p><p>The polymer the scientists created was consisted of molecular chains of nickel atoms and TTFtt arranged together like pearls in a necklace. The resulting material "is a powder, which we can press into a pellet," Anderson says. The powder is "basically black," but "when it's pressed into a pellet or smoothed out, it's shiny and iridescent, a bit like a metal."</p><p>The researchers found the new material has an electrical conductivity of 120,000 <a href="https://en.wikipedia.org/wiki/Siemens_(unit)" target="_blank">siemens per meter</a> at room temperature. This makes it nearly as <u><a href="https://en.wikipedia.org/wiki/Electrical_resistivity_and_conductivity#Resistivity_and_conductivity_of_various_materials" rel="noopener noreferrer" target="_blank">conductive</a></u> as graphite. Moreover, its maximum conductivity may actually be roughly three times greater, Anderson says.</p><p>Moreover, the material proved very stable, keeping electrically conductive even when exposed to air, humidity, acids, and bases ranging in pH from 0 to 14, and temperatures up to 140 degrees C. "We think this could be really important for applications, and also processability," Anderson says.</p><p>The researchers suspect the high conductivity of this material results from how it forms layers, much like sheets in lasagna. Even if the sheets no longer form an orderly stack, electrons can still flow in them as long as the sheets overlap. The result is a material that can get mashed like clay yet still conduct electricity.</p><p>Given how fundamentally important conductors are to electronics, the scientists expect a new kind of conductor may open up new technological applications. For example, metals usually have to be melted in order to be made into the right shape for a device, which limits what one can make with them, since other components of the device have to be able to withstand the heat to process those materials. In comparison, the new material can be made at room temperatures. In addition, the new conductor may prove robust in everyday conditions where conductive organic polymers may fail.</p><p>Currently, "we're particularly interested in developing paintable or sprayable conducting inks for different applications," Anderson says. In addition, "the pellets are brittle, but we're looking into making thin films now, which we might be able to put down onto flexible substrates."</p><p>In the future, the researchers seek to investigate similar polymers. For example, they may replace nickel with other metals such as copper, cobalt, palladium or platinum, or TTFtt with other sulfur-based molecules, Anderson says. "It's a really exciting area," Anderson notes.</p><p>The scientists detailed <u><a href="https://www.nature.com/articles/s41586-022-05261-4" rel="noopener noreferrer" target="_blank">their findings</a></u> 26 October in the journal <em>Nature</em>.</p>]]></description><pubDate>Sat, 05 Nov 2022 13:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/conductive-polymers</guid><category>Pedot</category><category>Conductive polymers</category><category>Conductor</category><dc:creator>Charles Q. Choi</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-black-disc-with-gold-nodes-on-the-corners-against-a-gray-background.jpg?id=32045677&amp;width=980"></media:content></item><item><title>DRAM’s Moore’s Law Is Still Going Strong</title><link>https://spectrum.ieee.org/micron-dram</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-golden-disc-etched-with-fine-lines-and-with-light-splayed-across-it-at-intersecting-angles.jpg?id=32025123&width=2000&height=1500&coordinates=0%2C0%2C0%2C1"/><br/><br/><p>Memory and storage chip maker <a href="https://www.micron.com/" target="_blank">Micron Technology</a> says it is shipping samples of the most bit-dense DRAM memory chips yet. Compared with its own previous generation, the 16-gigabit DRAM chip is 15 percent more power efficient and 35 percent more dense. Notably, Micron achieved the improvement without resorting to the most advanced chip-making technology, extreme ultraviolet lithography. The features that make up DRAM cells are not nearly as tiny as those on logic chips, but this advance shows that DRAM density could still shrink further in the future.<br/></p><p>Micron says it is shipping samples of LPDDR5X chips, memory made for power-constrained systems such as smartphones. (LPDDR5X, unpacked: a revved-up twist on the low-power version of the fifth generation of the double-data-rate memory communications standard, capable of transferring 8.5 gigabits per second.) It’s the first chip made using Micron’s new manufacturing process, called 1-beta, which the company says maintains the <a href="https://investors.micron.com/news-releases/news-release-details/micron-delivers-industrys-first-1a-dram-technology" target="_blank">lead it took a year ago</a> over rivals, including <a href="https://semiconductor.samsung.com/dram/" target="_blank">Samsung</a> and <a href="https://www.skhynix.com/" target="_blank">SK Hynix</a>.</p><p>Manufacturing processes for DRAM and logic chips diverged decades ago, with logic chips shrinking transistors much more aggressively as the years went by, explains <a href="https://objective-analysis.com/jim-handy/" target="_blank">Jim Handy</a>, a memory and storage analyst at <a href="https://objective-analysis.com/" rel="noopener noreferrer" target="_blank">Objective Analysis</a>, in Los Gatos, Calif. The reason for the difference has to do with DRAM’s structure. DRAM stores a bit as charge in a capacitor. Access to each capacitor is gated by a transistor. But the transistor is an imperfect barrier, and the charge will eventually leak away. So DRAM must be periodically refreshed, restoring its bits before they drain away. In order to keep that refresh period reasonable while still increasing the density of memory, DRAM makers had to make some pretty radical changes to the makeup of the capacitor. For Micron and other major manufacturers, it now resembles a tall pillar and is made using materials not found in logic chips.</p><p>Nevertheless, memory makers have been investing in the latest key manufacturing tool that logic-chip companies have: <a href="https://spectrum.ieee.org/euv-lithography-finally-ready-for-chip-manufacturing" target="_blank">extreme ultraviolet lithography</a>. But Micron didn’t need it to achieve its latest chip. Instead, the company used a proprietary version of “multipatterning” technology while sticking with the long-established <a href="https://spectrum.ieee.org/chip-makings-wet-new-world" target="_blank">193-nanometer immersion lithography</a>. Multipatterning involves <a href="https://www.micron.com/about/blog/2021/january/inside-1a-the-worlds-most-advanced-dram-process-technology" target="_blank">cycles of projecting a pattern, etching and depositing material, then projecting another pattern</a>—done in such a way that the interaction produces finer structures than any single pattern could. This version of multipatterning was adapted from one used in Micron’s NAND flash business, according to Thy Tran, vice president of DRAM process integration at Micron. “We have taken that and extended it aggressively,” she says. “It’s extremely valuable to be able to leverage both DRAM and NAND [flash].”</p><p>Not needing to use EUV is “a real coup,” says Handy. But it follows a trend. “Micron, for over a decade, has been able to use older process technology and equipment smarter than everybody else.”</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="A gold rectangle with intersecting paralle pale lines and darker sopts." class="rm-shortcode rm-resized-image" data-rm-shortcode-id="5d2693732af78ecc2942b08030783717" data-rm-shortcode-name="rebelmouse-image" id="4873a" loading="lazy" src="https://spectrum.ieee.org/media-library/a-gold-rectangle-with-intersecting-paralle-pale-lines-and-darker-sopts.jpg?id=32025124&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">A 16-gigabit DRAM die made with Micron’s 1-beta technology.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Micron Technology</small></p><p>Tran says the company currently believes it will begin using EUV with the next process, 1-gamma.</p><p>The initial product is meant for mobile systems, and so it makes power saving a priority. Part of that comes from the use of “enhanced” dynamic voltage and frequency scaling. This is technology that allows a chip to run with a slower clock and at lower voltage to conserve energy and then ramp up to higher frequency and voltage to get more work done. Micron’s previous manufacturing technology, 1-alpha, could transfer data at 1,600 megabits per second when in the power-saving mode, explains Ross Dermott, vice president of mobile product line management. The LPDDR5X DRAM, made using the 1-beta process, can operate at 3,200 Mb/s in the low-power condition. Handset makers that “have features or applications that are running at that faster speed,  can go into [the low-power] mode and essentially further reduce the power consumption,” he says.<br/></p><p>Tran says that Micron will later use 1-beta to make other types of DRAM, including the high-bandwidth memory that powers data-center processors and AI accelerators.</p><p><em>This post was corrected on 3 November. Micron's vice president of mobile product line management is Ross Dermott, not Ross McDermott.</em></p>]]></description><pubDate>Tue, 01 Nov 2022 13:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/micron-dram</guid><category>Dram</category><category>Moore's law</category><category>Micron</category><category>Samsung</category><category>Sk hynix</category><category>Extreme ultraviolet lithography</category><category>Euv</category><category>Memory</category><dc:creator>Samuel K. Moore</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-golden-disc-etched-with-fine-lines-and-with-light-splayed-across-it-at-intersecting-angles.jpg?id=32025123&amp;width=980"></media:content></item><item><title>How Ted Hoff Invented the First Microprocessor</title><link>https://spectrum.ieee.org/ted-hoff</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/man-in-white-striped-shirt-with-pocket-protector-seated-in-front-of-oscilloscope-and-other-electronics-equipment.jpg?id=31848732&width=2000&height=1500&coordinates=0%2C0%2C0%2C376"/><br/><br/><p><strong>The rays of the rising sun</strong> have barely reached the foothills of Silicon Valley, but Marcian E. (Ted) Hoff Jr. is already up to his elbows in electronic parts, digging through stacks of dusty circuit boards. This is the monthly flea market at Foothill College, and he rarely misses it.</p><p>Ted Hoff is part of electronics industry legend. While a research manager at Intel Corp., then based in Mountain View, he realized that silicon technology had advanced to the point that, with careful engineering, a complete central processor could fit on a chip. Teaming up with Stanley Mazor and Federico Faggin, he created the first commercial microprocessor, <a href="https://spectrum.ieee.org/chip-hall-of-fame-intel-4004-microprocessor" target="_self">the Intel 4004</a>.</p><h3></h3><br/><p>This article was first published as “Marcian E Hoff.” It appeared in the February 1994 issue of <em>IEEE Spectrum. </em>A <a href="https://ieeexplore.ieee.org/document/259493" target="_blank">PDF version</a> is available on IEEE Xplore. The photographs appeared in the original print version.</p><h3></h3><br/><p>But for Hoff, the microprocessor was merely one blip among many along the tracing of his long fascination with electronics. His passion for the field led him from New York City’s used electronics stores to elite university laboratories, through the intense early years of the microprocessor revolution and the tumult of the video game industry, and ultimately to his job today: high-tech private eye.</p><p>Fairly early in his childhood Hoff figured out that the best way to feel less like a kid—and a little more powerful—was to understand how things work. He started his explorations with chemistry. By the age of 12 he had moved on to electronics, building things with parts ordered from an Allied Radio Catalog, a shortwave radio kit, and surplus relays and motors salvaged from the garbage at his father’s employer, General Railway Signal Co., in Rochester, NY. Then in high school, working mostly with second­hand components, he built an oscilloscope, an achievement he parlayed into a technician’s job at General Railway Signal.</p><p>Hoff returned to that job during breaks from his undergraduate studies at <a href="https://www.rpi.edu/" target="_blank">Rensselaer Polytechnic Institute</a>, Troy, N.Y. Several summers began with Hoff entering the General Railway laboratory to find the researchers’ two best oscilloscopes broken. He would repair the state-of-the-art Tektronix 545s, then move on to more interesting stuff, like inventing an audio frequency railroad­train tracking circuit and a lightning protection unit that gave him two patents before he was out of his teens.</p><p>The best thing about the job, Hoff recalled, was the access it gave him to components that were beyond the budgets of most engineering students in the l950s—transistors, for instance, and even the just-introduced power transistor. He did an undergraduate thesis on transistors used as switches, and the cash prize he won for it quickly went for a Heathkit scope of his own.</p><h2>Early Neural Networks</h2><p>Hoff liked the engineering courses at Rensselaer, but not the narrow focus of the college itself. He wanted to broaden his perspective, both intellectually and geographically (he had never been more than a few miles west of Niagara Falls), so chose California’s <a href="https://www.stanford.edu/" rel="noopener noreferrer" target="_blank">Stanford University</a> for graduate school. While working toward his Ph.D. there, he did research in adaptive systems (which today are called neural networks) and, with his thesis advisor Bernard Widrow, racked up two more patents.</p><p class="pull-quote">“He had a toy train moving back and forth under computer control, balancing a broom­ stick. I saw him as a kooky inventor, a mad scientist.”<br/>—Stanley Mazor<br/></p><p>His Intel colleague Mazor, now training manager at Synopsys Inc., Mountain View, Calif., recalled meeting Hoff in his Stanford laboratory.</p><p>“He had a toy train moving back and forth under computer control, balancing a broomstick,” Mazor said. “I saw him as a kooky inventor, a mad scientist.”</p><p>After getting his degree, Hoff stayed at Stanford for six more years as a postdoctoral researcher, continuing the work on neural networks. At first, his group made the networks trainable by using a device whose resistance changed with the amount and direction of current applied. It consisted of a pencil lead and a piece of copper wire sitting in a copper sulfate and sulfuric acid solution, and they called it a memistor.</p><p class="pull-quote">“One result of all our work on microprocessors that has always pleased me is that we got computers away from those [computer center] people.”<br/>—Ted Hoff</p><p>The group soon acquired an IBM 1620 computer, and Hoff had his first experience in programming—and in bucking the system. He had to deal with officials at the campus computer center who thought all computers should be in one place, run by specialists who handled the boxes of punched cards delivered by researchers. The idea that a researcher should program computer systems interactively was anathema to them.</p><h3>Ted Hoff: Vital Stats</h3><br><p><strong>Name </strong></p><p><strong></strong>Marcian E. (Ted) Hoff Jr.<br/></p><p><strong>Date of birth </strong></p><p>Oct. 28, 1937</p><p><strong>Family </strong></p><p>Wife, Judy; three daughters, Carolyn, Lisa, and Jill</p><p><strong>Education</strong></p><p>BS, 1958, Rensselaer Polytechnic Institute, Troy, N.Y.; MS, 1959, Ph.D., 1962, Stanford University, California, all in electrical engineering</p><p><strong>First job</strong></p><p>Planting cabbages</p><p><strong>First electronics job</strong></p><p>Technician, General Railway Signal Co., Rochester, N.Y.</p><p><strong>Biggest surprise in career</strong></p><p>Media hysteria over the microprocessor</p><p><strong>Patents</strong></p><p>17</p><p><strong>Books recently read</strong></p><p><em>Introduction to Nuclear Reactor Theory</em> by John R. Lamarsh; A Compiler Generator by William M. McKeeman, James J. Horning, and David B. Wortman</p><p><strong>People most respected</strong></p><p>Intel Corp. founders Robert Noyce and Gordon Moore, Intel chief executive officer Andrew Grove</p><p><strong>Favorite restaurants</strong></p><p>Postrio and Bella Voce in San Francisco, Beausejour in Los Altos, Calif.</p><p><strong>Favorite movies</strong></p><p><em>2001</em>,<em> Dr. Strangelove</em></p><p><strong>Motto</strong></p><p>“If it works, it’s aesthetic”</p><p><strong>Leisure activities</strong></p><p>Playing with electronics; attending operas and concerts; going to the theater, body surfing in Hawaii; walking his Alaskan malamutes</p><p><strong>Car</strong></p><p>Porsche 944</p><p><strong>Management creed</strong></p><p>“The best motivation is self-motivation”</p><p><strong>Organizational memberships</strong></p><p>IEEE, Sigma Xi</p><p><strong>Major awards</strong></p><p>Stuart Balantine Medal of the Franklin Institute, IEEE Cledo Brunetti Award, IEEE Centennial Medal, IEEE Fellow</p><h3></h3><br><p>“One result of all our work on microprocessors that has always pleased me,” Hoff told <em>IEEE Spectrum</em>, “is that we got computers away from those people.”<br/></p><p>By 1968 student hostility to the government over the Vietnam War was growing and life for researchers on campus who, like Hoff, relied on government funding was looking as if it might get uncomfortable. Hoff had already been contemplating the possibilities of industrial jobs when he received a telephone call from Robert Noyce, who told him he was starting a new company, <a href="https://www.intel.com/content/www/us/en/homepage.html" target="_blank">Intel Corp</a>., and had heard Hoff might be interested in a job. He asked Hoff where the semiconductor integrated circuit business would find its next growth area. “Memories,” Hoff replied.</p><p>That was the answer Noyce had in mind (Intel was launched as a memory manufacturer), and that year he hired Hoff as a member of the technical staff, Intel’s 12th employee. Working on memory technology, Hoff soon received a patent for a cell for use in MOS random-access integrated circuit memory. Moving on to become manager of applications research, he had the first customer contact of his career.</p><p class="pull-quote">“Engineering people tend to have a very haughty attitude toward marketing, but I discovered you learn a tremendous amount if you keep your eyes and ears open in the field.” <br/>—Hoff</p><p>“Engineering people tend to have a very haughty attitude toward marketing,” Hoff said, “but I discovered you learn a tremendous amount if you keep your eyes and ears open in the field. Trying to understand what problems people are trying to solve is very helpful. People back in the lab who don’t have that contact are working at a disadvantage.”</p><h2>From 12 Chips to One Microprocessor</h2><p>One group of customers with whom Hoff made contact were from Busicom Corp., Tokyo. Busicom had hired Intel to develop a set of custom chips for a low-cost calculator and had sent three engineers to Santa Clara to work on the chip designs. Hoff was assigned to look after them, getting them pencils and paper, showing them where the lunchroom was—nothing technical.</p><p>But the technical part of Hoff’s mind has no off-switch, and he quickly concluded that the engineers were going in the wrong direction. Twelve chips, each with more than 3000 transistors and 36 leads, were to handle different elements of the calculator logic and controls, and he surmised the packaging alone would cost more than the targeted retail price of the calculator. Hoff was struck by the complexity of this tiny calculator, compared with the simplicity of the PDP-8 minicomputer he was currently using in another project, and he concluded that a simple computer that could handle the functions of a calculator could be designed with about 1900 transistors. Given Intel’s advanced MOS process, all these, he felt, could fit on a single chip.</p><h3></h3><br><div class="rblad-ieee_in_content"></div><h3></h3><br><img alt="Man sitting at patio table with large dog seated next to him. Laptop and coffee mug on table." class="rm-shortcode" data-rm-shortcode-id="6d1418ca3ba18981624e1c2d8a5f483b" data-rm-shortcode-name="rebelmouse-image" id="9b51c" loading="lazy" src="https://spectrum.ieee.org/media-library/man-sitting-at-patio-table-with-large-dog-seated-next-to-him-laptop-and-coffee-mug-on-table.jpg?id=31784380&width=980"/><h3></h3><br><p>The Busicom engineers had no interest in dumping their design in favor of Hoff’s unproved proposal. But Hoff, with Noyce’s blessing, started working on the project. Soon Mazor, then a research engineer at Intel, joined him, and the two pursued Hoff’s ideas, developing a simple instruction set that could be implemented with about 2000 transistors. They showed that the one set of instructions could handle decimal addition, scan a keyboard, maintain a display, and perform other functions that were allocated to separate chips in the Busicom design.</p><p>In October 1969, Hoff, Mazor, and the three Japanese engineers met with Busicom management, visiting from Japan, and described their divergent approaches. Busicom’s managers chose Hoff’s approach, partly, Hoff said, because they understood that the chip could have varied applications beyond that of a calculator. The project was given the internal moniker “4004.”</p><p>Federico Faggin, now president and chief executive officer of Synaptics Inc., San Jose, Calif., was assigned to design the chip, and in nine months came up with working prototypes of a 4-bit, 2300-transistor “microprogrammable computer on a chip.” Busicom received its first shipment of the devices in February 1971.</p><p>Faggin recalled that when he began implementing the microprocessor, Hoff seemed to have lost interest in the project, and rarely interacted with him. Hoff was already working on his next project, the preliminary design of an 8-bit microprogrammable computer for Computer Terminals Corp., San Antonio, Texas, which, architected by Computer Terminals, was named the 8008. Hoff always “had to do very cutting-edge work,” Faggin told <em>Spectrum</em>. “I could see a tension in him to always be at the forefront of what was happening.”</p><p>In those early Intel days, Mazor recalled that Hoff had a number of ideas for projects, many of which, though not commercially successful, proved prescient: a RAM chip that would act like a digital camera and capture an image in memory, a video game with moving spaceships, a device for programming erasable programmable ROMs, and computer-aided design tools intended for logic simulation.</p><p class="pull-quote">The Intel marketing department they estimated that sales [of microprocessors] might total only 2000 chips a year.</p><p>Meanwhile, the microprocessor revolution was gearing up, albeit slowly. Hoff joined Faggin as a microprocessor evangelist, trying to convince people that general-purpose one chip computers made sense. Hoff said his toughest sell was to the Intel marketing department.</p><p>“They were rather hostile to the idea,” he recalled, for several reasons. First, they felt that all the chips Intel could make would go for several years to one company, so there was little point in marketing them to others. Second, they told Hoff, ‘‘We have diode salesman out there struggling like crazy to sell memories, and you want them to sell computers? You’re crazy.” And finally, they estimated that sales might total only 2000 chips a year.</p><p>But word went out. In May 1971 an article in <em>Datamation</em> magazine mentioned the product, and the following November Intel produced its first ad for the 4004 CPU and placed it in <em>Electronic News</em>. By 1972 stories about the miracle of what began being called the microprocessor started appearing regularly in the press, and Intel’s competitors followed its lead by launching microprocessor products of their own.</p><p class="pull-quote">Hoff never even considered patenting the microprocessor. To him the invention seemed to be obvious.</p><p>One step Hoff did not take at that time was apply for a patent, even though he had already successfully patented several inventions. (Later, with Mazor and Faggin he filed for and was granted a patent for a “memory system for a multi-chip digital computer.”)</p><p>Looking back, Hoff recalled that he never even considered patenting the microprocessor in those days. To him the invention seemed to be obvious, and obviousness was considered grounds for rejecting a patent application (though, Hoff said bitterly, the patent office currently seems to ignore that rule). It was obvious to Hoff that if in one year a computer could be built with 1000 circuits on100 chips, and if in the following year those 1000 circuits could be put onto10 chips, eventually those 1000 circuits could be con­ structed on one chip.</p><p>Instead of patenting, Hoff in March 1970 published an article in the proceedings of the 1970 IEEE International Convention that stated: “An entirely new approach to design of very small computers is made possible by the vast circuit complexity possible with MOS technology. With from 1000 to 6000 MOS devices per chip, an entire central processor may be fabricated on a single chip.”</p><p>But in December 1970, an independent inventor outside the cliquish semiconductor industry, Gilbert Hyatt, filed for a patent on a processor and mentioned that it was to be made on a single chip. In 1990, after numerous appeals and extensions, Hyatt was granted that patent and began collecting royalties from many microprocessor manufacturers. Currently, though history traces today’s microprocessor back to Hoff, Mazor, and Faggin, the legal rights to the invention belong to Hyatt.</p><h2>The Invention of the Codec</h2><p>While the microprocessor has proved to be his most celebrated achievement, Hoff does not view it as his biggest technical breakthrough. That designation he reserves for the single-chip analog-to-digital/ digital-to-analog coder/decoder (codec).</p><p>“Now that work was an exciting technical challenge,” Hoff recollected with some glee, “because there were so many who said it couldn’t be done.”</p><p>The project was kicked off by Noyce, who spotted the telephone industry as ripe for new technology, and urged Hoff to find an important product for that market. Studying telephone communications, Hoff and several other researchers saw that digitized voice transmission, then being used between central offices, depended on the use of complex expensive codecs that tied into electromechanical switches.</p><p>”We thought,” Hoff told <em>Spectrum</em>, “we could integrate this, the analog-to-digital conversion, on a chip, and then use these circuits as the basis for switching.”</p><p>Besides reducing the cost of the systems to the telephone company, such chips would enable companies to build small branch exchanges that handled switching electronically.</p><p>Hoff and his group developed a multiplexed approach to conversion in which a single converter is shared by the transmit and receive channels. They also established a number of other techniques for conversion and decoding that Hoff saw as not being obvious and for which he received patents.</p><p>With that project’s completion in 1980, after six years of effort, and its transfer to Intel’s manufacturing facility in Chandler, Ariz., Hoff became an Intel Fellow, free to pursue whatever technology interested him. What interested him was returning to his work on adaptive structures, combining the concepts he had wrestled with at Stanford with the power of the microprocessor in the service of speech recognition. After a year he built a recognition system that Intel marketed for several years.</p><p>A prime customer for the system was the automotive industry. Its inspectors used the systems to help them check out a car as it finally left the assembly line. When an inspector noted out loud various problems that needed fixing, the system would prompt him for further information, and log his responses in a computer.</p><h3></h3><br><div class="rblad-ieee_in_content"></div><h3></h3><br/><h2>From Intel to Atari</h2><p>
	Though his position as an Intel Fellow gave Hoff a fair amount of freedom, he found himself getting bored. Intel’s success in microprocessors by 1983 had turned it into a chip supplier, and other companies were designing the chips into systems.
</p><p>
	“I had always been more interested in systems than in chips,” Hoff said, “and I had been at Intel for 14 years, at a time when the average stay at a company in Silicon Valley was three years. I was overdue for a move.”
</p><p>
	Again, Hoff had not gone beyond thinking about leaving Intel when a new job came to him. <a href="https://spectrum.ieee.org/pong" target="_self">Atari Inc</a>., Sunnyvale, Calif., then a booming video game company owned by Warner Communications Inc. and a major user of microprocessors, was looking for a vice president of corporate technology. In February 1983, after discussing the scope of the ideas that Atari researchers were pursuing, Hoff latched onto the opportunity.
</p><p>
	Intel from the start had a structured, highly controlled culture. At Atari, chaos reigned.
</p><p>
	Intel from the start had a structured, highly controlled culture. At Atari, chaos reigned. Under Hoff were research laboratories in Sunnyvale, Los Angeles, and Grass Valley, Calif.; Cambridge, Mass.; and New York City. Researchers were working on picture telephones, electronic aids for joggers, computer controls that gave tactile feedback, graphical environments akin to today’s virtual reality, digital sound synthesis, advanced personal computers, and software distribution via FM sidebands.
</p><p>
	But Hoff had barely had time to learn about all the research projects under way before the video game business took a well-publicized plunge. Without solid internal controls, Atari was unable to determine how well its games were selling at the retail point, and distributors were returning hundreds of thousands of cartridges and game machines. Hoff began receiving orders for staff cuts monthly.
</p><p>
	“It would have been one thing if I had known I had to cut back to, say, one-quarter the size of my group,” he told <em>Spectrum</em>. “But when every month you find you have to cut another chunk, morale really drops.”
</p><p>
	In July 1984, while Hoff was at his 30th high school reunion, Warner sold Atari to Jack Tramiel. Hoff then had to choose between convincing Tramiel that he could play a role in a narrowly focused company uninterested in funding futuristic research, and allowing Warner to buy out his contract. He chose the latter.
</p><p>
	Looking back, most of the people who were at Atari in those days now view them darkly. But Hoff recalls his year there as an enjoyable and ultimately useful experience. “Maybe I look at it more positively than I should,” he said, “but it turned out to be a good transition for me, and the life I have now is a very nice one.”
</p><p class="pull-quote">
	“Whenever you are working on one problem, there is always another problem over here that seems more interesting.”<br/>
	—Hoff
</p><p>
	He now spends half his time as a consultant and half pursuing technical projects of his own devising—a read­out device for machine tools, various types of frame grabbers, pattern recognition, and techniques for analog-to-digital conversion. This variegated schedule is perfect for him. He has always felt himself to be a generalist, and has had trouble focusing on just one technology.
</p><p>
	“It’s easy for me to get distracted,” he said. “Whenever you are working on one problem, there is always another problem over here that seems more interesting. But now it is more likely that my own projects get delayed, rather than things critical to other people and their employment.”
</p><p>
	Faggin for one is not surprised that such independent work appeals to Hoff. “He never was the gregarious type,” Faggin said. “He liked introverted work, the thinking, the figuring out of new things. That is what he is good at. I always was impressed how he was able to visualize an architecture for a new IC, practically on the spot.”
</p><p class="pull-quote">
	“He comes up with idea after idea, situation after situation. I think if he wanted to, Ted could sit down and crank out a patent a month.”<br/>
	—Gary Summers
</p><p>
	Said Gary Summers, president and chief executive officer of Teklicon Inc., Mountain View, the consulting firm that employs Hoff today: “He comes up with idea after idea, situation after situation. I think if he wanted to, Ted could sit down and crank out a patent a month.”
</p><p>
	“There is no doubt in my mind that he is a genius,” Mazor stated. Summers readily concurred.
</p><p>
	Hoff’s first project after Atari was a voice­controlled music synthesizer, which gave off the sound of a selected instrument when someone sang into it. Hoff’s biggest contribution to the project was a system that ensured that the emerging notes would be in tune, or at least harmonically complement the tune, even when the singer strayed off key. He scored another patent for this system, and the gadget was sold briefly through the Sharper Image catalog, but never became a big success.
</p><p>
	Hoff still contributes occasionally to product designs. At Teklicon, however, where he is vice president and chief technical officer, most of his consulting is done for lawyers. Hoff has a unique combination of long experience with electronic design and long-standing pack rat habits. His home workshop contains about eight personal computers of different makes and vintages, five oscilloscopes, including a vintage Tektronix 545 scope, 15000 ICs inventoried and filed, and shelves loaded with IC data books dating right back to the 1960s.
</p><p class="pull-quote">
	“If my washing machine breaks down, I call the repairman. Most clever engineers would buy the replacement gear and install it. Ted is capable of analyzing the reason the gear failed in the first place, redesigning a better gear from basic principles, carving it out of wood, casting it at his home, and dynamically balancing it on his lathe before installing it.”<br/>
	—Mazor
</p><p>
	When a lawyer shows him a patent disclosure, even one decades old, he can determine whether or not it could then have been “reduced to practice” and whether it provided sufficient information to allow “one of ordinary skill in the art” to practice the invention. Then he can build a model proving his conclusion, using vintage components from his collection, and demonstrate the model in court as an expert witness. This model-building can get very basic. On <em>Spectrum</em>’s visit, Rochelle salt crystals that Hoff attempted to grow for a recent court demonstration littered his workshop floor, next to metal-working equipment that he uses to build cases for his models.
</p><p>
	Hoff sees this ability to get down to basics as one of his strengths. “I relate things to fundamental principles,” he said. “People who don’t question the assumptions made going into a problem often end up solving the wrong problem.”
</p><p>
	Mazor said, “If my washing machine breaks down, I call the repairman. Most clever engineers would buy the replacement gear and install it. Ted is capable of analyzing the reason the gear failed in the first place, redesigning a better gear from basic principles, carving it out of wood, casting it at his home, and dynamically balancing it on his lathe before installing it.”
</p><p>
	Doing legal detective work appeals to Hoff for another reason: it gives him an excuse to hunt for interesting “antique” components at flea markets and electronics stores.
</p><p>
	Hoff cannot discuss the specifics of patent cases he has been involved with. Several recently were in the video game area; others have involved various IC companies. In a number of cases, Hoff was confident that his side was right, and his side still lost, so he felt little surprise when the microprocessor patent was granted to Hyatt. (After the award was made, though, he did sit down with Hyatt’s patent application and attempted to design a working microprocessor based on Hyatt’s disclosures. He found several incongruities—like a clock rate only suited to bipolar technology with logic that could only be rendered in MOS technology, and logic that required far too many transistors to put on a chip, proving in his mind that the award was incorrect.)
</p><p>
	Seeing someone else get credit for the microprocessor, particularly in recent media reports, “is irritating,” Hoff told <em>Spectrum</em>, “but I’m not going to let it bother me, because I know what I did, I know what all the other people on our project did, and I know what kind of company Intel is. And I know that I was where the action was.”<span class="ieee-end-mark"></span>
</p><p>
<em>Editor’s note: Hoff retired from Teklicon in 2007. He currently serves as a judge for the </em><a href="https://www.invent.org/collegiate-inventors/judging" rel="noopener noreferrer" target="_blank"><em>Collegiate Inventors Competition</em></a><em>, held annually by </em><a href="https://www.invent.org/" rel="noopener noreferrer" target="_blank"><em>the National Inventors Hall of Fame</em></a><em>. These days, his main technical interests surround energy, water, and climate change.</em>
</p></br></br></br></br></br></br>]]></description><pubDate>Sat, 08 Oct 2022 15:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/ted-hoff</guid><category>Intel 4004</category><category>Ted hoff</category><category>Microprocessor</category><dc:creator>Tekla S. Perry</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/man-in-white-striped-shirt-with-pocket-protector-seated-in-front-of-oscilloscope-and-other-electronics-equipment.jpg?id=31848732&amp;width=980"></media:content></item><item><title>Modeling Thermal Management Systems for Electronics</title><link>https://event.on24.com/wcc/r/3959064/F4D5B0E14D03698DE8F5FB556899D62C</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/image.png?id=26851526&width=980"/><br/><br/><p>
	The ability to dissipate heat is one of the most important features of modern electronic devices and is usually a limiting factor in the miniaturization of these devices.
</p>
<p>
<span></span>COMSOL Multiphysics includes functionality for heat transfer through conduction, convection, and radiation. Its ability to treat conjugate heat transfer, including laminar and turbulent flow as well as surface-to-surface radiation, has proven to be of great importance for the design and optimization of thermal management systems in electronics. Its multiphysics modeling capabilities also enable the study of thermoelectric effects as well as thermal–structural effects, such as thermal expansion.<u></u><u></u>
</p>
<p>
	In this webinar, we will demonstrate how to create models and apps for conjugate heat transfer in electronic devices. We will also give a general overview of the software’s capabilities for multiphysics modeling, including heat transfer as one of the modeled phenomena.
</p>
<p>
<a href="https://event.on24.com/wcc/r/3959064/F4D5B0E14D03698DE8F5FB556899D62C" target="_blank">Register now for this free webinar!</a>
</p>
<hr/>
<p>
	Speaker<br/>
</p>
<p>
<span></span><span style="background-color: initial;"><strong>Mranal Jain<br/>
</strong></span>Senior Applications Engineer, COMSOL<br/>
	Mranal Jain has been with COMSOL since 2013 and currently leads the transport team in the Burlington, MA office. He studied microfluidics and electrokinetic transport, while pursuing his PhD in chemical engineering at the University of Alberta, Edmonton.
</p>]]></description><pubDate>Mon, 03 Oct 2022 12:00:00 +0000</pubDate><guid>https://event.on24.com/wcc/r/3959064/F4D5B0E14D03698DE8F5FB556899D62C</guid><category>Comsol</category><category>Multiphysics</category><category>Simulation</category><category>Thermal management</category><category>Type:webinar</category><dc:creator>COMSOL</dc:creator><media:content medium="image" type="image/png" url="https://assets.rbl.ms/26851526/origin.png"></media:content></item><item><title>Disentangling the Facts From the Hype of Quantum Computing</title><link>https://spectrum.ieee.org/ieee-quantum-week</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/two-men-hold-a-large-gold-cylinder-that-is-suspended-by-several-narrow-gold-pipes.jpg?id=31720293&width=2000&height=1500&coordinates=0%2C0%2C404%2C0"/><br/><br/><div></div><p><em>This is a guest post in recognition of <a href="https://qce.quantum.ieee.org/2022/" target="_blank">IEEE Quantum Week 2022</a>. The views expressed here are solely those of the author and do not represent positions of </em>IEEE Spectrum<em> or the IEEE.</em></p><p>Few fields invite as much unbridled hype as quantum computing. Most people’s understanding of quantum physics extends to the fact that it is unpredictable, powerful, and almost existentially strange. A few years ago, I provided <em>IEEE Spectrum </em>an update on the <a href="https://spectrum.ieee.org/an-optimists-view-of-the-4-challenges-to-quantum-computing" target="_self">state of quantum computing</a> and looked at both the positive and negative claims across the industry. And just as back in 2019, I remain enthusiastically optimistic today. Even though the hype is real and has outpaced the actual results, much has been accomplished over the past few years.<br/></p><p>First, let’s address the hype. </p><p>Over the past five years, there has been undeniable hype around quantum computing—hype around approaches, timelines, applications, and more. As far back as 2017, vendors were claiming the commercialization of the technology was just a couple of years away. There was even what I’d call <a href="https://spectrum.ieee.org/the-case-against-quantum-computing" target="_self">antihype</a>, with some questioning if quantum computers would materialize at all. I hope they end up being wrong.</p><p>More recently, companies have shifted their timelines from a few years to a decade, but they continue to release road maps showing <a href="https://www.theverge.com/2021/5/19/22443453/google-quantum-computer-2029-decade-commercial-useful-qubits-quantum-transistor" rel="noopener noreferrer" target="_blank">commercially viable systems as early as 2029</a>. And these hype-fueled expectations are becoming institutionalized: The <a href="https://www.dhs.gov/" target="_blank">Department of Homeland Security</a> even <a href="https://www.nextgov.com/emerging-tech/2021/10/dhs-issues-roadmap-help-organizations-prepare-quantum-computing-threat/185844/" target="_blank">released a road map</a> to protect against the threats of quantum computing, in an effort to help institutions transition to new security systems. This creates an “adopt or you’ll fall behind” mentality for both quantum-computing applications and postquantum cryptography security.</p><p>Market research firm <a href="https://www.gartner.com/en" target="_blank">Gartner</a> (of the <a href="https://www.gartner.com/en/research/methodologies/gartner-hype-cycle" target="_blank">“Hype Cycle”</a> fame) believes quantum computing may have already reached peak hype, or phase two of its five-phase growth model. This means the industry is about to enter a phase called “the trough of disillusionment." According to <a href="https://www.mckinsey.com/" target="_blank">McKinsey & Company</a>, “fault tolerant quantum computing <a href="https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/quantum-computing-use-cases-are-getting-real-what-you-need-to-know" target="_blank">is expected between 2025 and 2030</a> based on announced hardware roadmaps for gate-based quantum computing players.” I believe this is not entirely realistic, as we still have a long journey to achieve quantum practicality—the point at which quantum computers can do something unique to change our lives. </p><p>In my opinion, quantum practicality is likely still 10 to 15 years away. However, progress toward that goal is not just steady; it’s accelerating. That’s the same thing we saw with Moore’s Law and semiconductor evolution: The more we discover, the faster we go. Semiconductor technology has taken decades to progress to its current state, accelerating at each turn. We expect similar advancement with quantum computing.</p><p>In fact, we are discovering that what we have learned while engineering transistors at <a href="https://www.intel.com/content/www/us/en/homepage.html" target="_blank">Intel</a> is also helping to speed our quantum-computing development work today. For example, when developing <a href="https://spectrum.ieee.org/intels-quantum-computing-plans-hot-qubits-cold-control-chips-and-rapid-testing" target="_self">silicon spin qubits</a>, we’re able to leverage existing transistor-manufacturing infrastructure to ensure quality and to speed up fabrication. We’ve started the mass production of qubits on a 300-millimeter silicon wafer in a high-volume fab facility, which allows us to fit an array of more than 10,000 quantum dots on a single wafer. We’re also leveraging our experience with semiconductors to create a cryogenic quantum control chip, called <a href="https://www.intel.com/content/www/us/en/research/quantum-computing.html" rel="noopener noreferrer" target="_blank">Horse Ridge</a>, which is helping to solve the interconnect challenges associated with quantum computing by eliminating much of the cabling that today crowds the dilution refrigerator. And our experience with testing semiconductors has led to the development of the <a href="https://www.intel.com/content/www/us/en/newsroom/news/intels-cryoprober-quantum-research.html" rel="noopener noreferrer" target="_blank">cryoprober</a>, which enables our team to get testing results from quantum devices in hours instead of the days or weeks it used to take.</p><p>Others are likely benefiting from their own prior research and experience, as well. For example, <a href="https://www.quantinuum.com/" target="_blank">Quantinuum’s</a> recent research <a href="https://www.quantinuum.com/pressrelease/logical-qubits-start-outperforming-physical-qubits" target="_blank">showed the entanglement of logical qubits</a> in a fault-tolerant circuit using real-time quantum error correction. While still primitive, it’s an example of the type of progress needed in this critical field. For its part, Google has a new open-source library called <a href="https://quantumai.google/cirq" rel="noopener noreferrer" target="_blank">Cirq</a> for programming quantum computers. Along with similar libraries from IBM, Intel, and others, Cirq is helping drive development of improved quantum algorithms. And, as a final example, IBM’s 127-qubit processor, called <a href="https://research.ibm.com/blog/127-qubit-quantum-processor-eagle" rel="noopener noreferrer" target="_blank">Quantum Eagle</a>, shows steady progress toward upping the qubit count.</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-right" data-rm-resized-container="25%" style="float: right;">
<img alt="A man holds a bumpy gold rectangle in front of a spindly machine." class="rm-shortcode rm-resized-image" data-rm-shortcode-id="2e57de474e52ab8607c49a0a860eeadf" data-rm-shortcode-name="rebelmouse-image" id="50104" loading="lazy" src="https://spectrum.ieee.org/media-library/a-man-holds-a-bumpy-gold-rectangle-in-front-of-a-spindly-machine.jpg?id=31720324&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">The author shows Intel quantum-computing prototypes.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Intel</small></p><p>There are also some key challenges that remain.</p><ul><li>First, we still need better devices and high-quality qubits. While the very best one- and two-qubit gates meet the needed threshold for fault tolerance, the community has yet to accomplish that on a much larger system.</li><li>Second, we’ve yet to see anyone propose an interconnect technology for quantum computers that is as elegant as how we wire up microprocessors today. Right now, each qubit requires multiple control wires. This approach is untenable as we strive to create a large-scale quantum computer.</li><li>Third, we need fast qubit control and feedback loops. Horse Ridge is a precursor for this, because we would expect latency to improve by having the control chip in the fridge and therefore closer to the qubit chip.</li><li>And finally, error correction. While there have been some recent indications of progress to correction and mitigation, no one has yet run an error-correction algorithm on a large group of qubits.</li></ul><p>With new research regularly showing novel approaches and advances, these are challenges we will overcome. For example, many in the industry are looking at how to integrate qubits and the controller on the same die to create quantum system-on-chips (SoCs).</p><p>But we’re still quite a way off from having a fault tolerant quantum computer. Over the next 10 years, Intel expects to be competitive (or pull ahead) of others in terms of qubit count and performance, but as I stated before, a system large enough to deliver compelling value won’t be realized for 10 to 15 years, by anyone. The industry needs to continue its evolution of qubit counts and quality improvement. After that, the next milestone should be the production of thousands of quality qubits (still several years away), and then scaling that to millions.</p><p>Let’s remember that it took Google 53 qubits to create an application that could accomplish a supercomputer function. If we want to explore new applications that go beyond today’s supercomputers, we’ll need to see system sizes that are orders of magnitude larger.</p><p>Quantum computing has come a long way in the past five years, but we still have a long way to go, and investors will need to fund it for the long term. Significant developments are happening in the lab, and they show immense promise for what could be possible in the future. For now, it’s important that we don’t get caught up in the hype but focus on real outcomes.</p><p><br/></p><p><em><strong>Correction 21 Sept. 2022: </strong>A previous version of this post stated incorrectly that the release of an announced 5,000-qubit quantum computer in 2020 did not happen. <a href="https://en.wikipedia.org/wiki/D-Wave_Systems#Pegasus" target="_blank">It did</a>. </em>Spectrum<em> regrets the error. </em><br/></p>]]></description><pubDate>Mon, 19 Sep 2022 14:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/ieee-quantum-week</guid><category>Quantum computers</category><category>Quantum processors</category><category>Quantum programming</category><category>Cryogenic ic</category><category>Qubits</category><category>Intel</category><category>Quantum computing</category><dc:creator>James S. Clarke</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/two-men-hold-a-large-gold-cylinder-that-is-suspended-by-several-narrow-gold-pipes.jpg?id=31720293&amp;width=980"></media:content></item><item><title>Exploring the Value of Power Modules</title><link>https://ad.doubleclick.net/ddm/trackclk/N5744.125581.6383549968321/B28005468.338910302;dc_trk_aid=537092994;dc_trk_cid=177580565;dc_lat=;dc_rdid=;tag_for_child_directed_treatment=;tfua=;ltd=</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/image.png?id=31699487&width=980"/><br/><br/><p>In this training series, we will discuss the high level of integration of DC/DC power modules and the significant implications that this has on power supply design. </p><p><a href="https://ad.doubleclick.net/ddm/trackclk/N5744.125581.6383549968321/B28005468.338910302;dc_trk_aid=537092994;dc_trk_cid=177580565;dc_lat=;dc_rdid=;tag_for_child_directed_treatment=;tfua=;ltd=" rel="noopener noreferrer nofollow" target="_blank">Watch this free webinar now!</a></p><p>In addition to high power density and small solution size, modules can also simplify EMI mitigation and reduce power supply design time. And thanks to improved process and packaging technology, a power module may even provide all of these benefits with a lower overall solution cost as well.</p><hr/><p>In addition to these core topics, this training series also touches on some of the lesser-known aspects of power module design like inductor withstand voltage and high-temperature storage testing.</p>]]></description><pubDate>Wed, 14 Sep 2022 12:15:00 +0000</pubDate><guid>https://ad.doubleclick.net/ddm/trackclk/N5744.125581.6383549968321/B28005468.338910302;dc_trk_aid=537092994;dc_trk_cid=177580565;dc_lat=;dc_rdid=;tag_for_child_directed_treatment=;tfua=;ltd=</guid><category>Texas instruments</category><category>Power electronics</category><category>Power supplies</category><category>Type:webinar</category><dc:creator>Texas Instruments</dc:creator><media:content medium="image" type="image/png" url="https://assets.rbl.ms/31699487/origin.png"></media:content></item><item><title>Domain-Wall Discovery Points Toward Self-Healing Circuits</title><link>https://spectrum.ieee.org/domain-wall</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/conical-ferroelectric-domain-walls-in-lithium-niobate.jpg?id=31639003&width=2000&height=1500&coordinates=0%2C0%2C1%2C0"/><br/><br/><p>Atomically thin materials such as <a data-linked-post="2650278756" href="https://spectrum.ieee.org/is-graphene-by-any-other-name-still-graphene" target="_blank">graphene</a> have drawn attention for how electrons can race in them at exceptionally quick speeds, leading to visions of advanced new electronics. Now scientists find that similar behavior can exist within two-dimensional sheets, known as domain walls, that are embedded within unusual crystalline materials. Moreover, unlike other atomically thin sheets, domain walls can easily be created, moved, and destroyed, which may lead the way for novel circuits that can instantly transform or be repaired on command.</p><p>In the new study, researchers investigated crystalline lithium niobate <a href="https://spectrum.ieee.org/antiferroelectric" target="_self">ferroelectric</a> film just 500 nanometers thick. Electric charges within materials separate into positive and negative poles, and ferroelectrics are materials in which these electric dipoles are generally oriented in the same direction. The electric dipoles in ferroelectrics are clustered in regions known as domains. These are separated by two-dimensional layers known as domain walls.</p><p>The amazing <a href="https://spectrum.ieee.org/memristor-6g-switches" target="_self">electronic properties</a> of two-dimensional materials such as <a href="https://spectrum.ieee.org/atomically-thin-circuits-made-from-graphene-and-molybdenite" target="_self">graphene and molybdenum disulfide</a> have led researchers to hope <a href="https://spectrum.ieee.org/the-most-complex-2d-microchip-yet" target="_self">they may allow Moore’s Law to continue</a> once it becomes impossible to make further progress using silicon. Researchers have also investigated similarly attractive behavior in exceptionally thin electrically conducting <a href="https://spectrum.ieee.org/the-etchasketchtm-of-microscopy-creates-single-electron-transistors" target="_self">heterointerfaces</a> between two different insulating materials, such as lanthanum aluminate and strontium titanate.</p><p>Domain walls are essentially homointerfaces between chemically identical regions of the same material. However, unlike any other 2D electronic material, applied electric or magnetic fields can readily create, move, and annihilate domain walls inside materials.</p><p>This unique quality of domain walls may potentially lead to novel “<a href="https://www.nature.com/articles/s41578-021-00375-z" target="_blank">domain wall electronics</a>” far more flexible and adaptable than current devices that rely on static components. One might imagine entire circuits “created in one instant, for one purpose, only to be wiped clean and rewritten in a different form, for a different purpose, in the next instant,” says study lead author <a href="https://pure.qub.ac.uk/en/persons/conor-mccluskey" target="_blank">Conor McCluskey</a>, a physicist at Queen’s University Belfast, in the United Kingdom. “Malleable domain-wall network architecture that can continually metamorphose could represent a kind of technological genie, granting wishes on demand for radical moment-to-moment changes in electronic function.”</p><p>However, scientists have found it difficult to examine domain walls in detail. The fact that domain walls are both very thin and buried under the surfaces of crystals makes them less easy to analyze “than regular 3D or even 2D materials,” McCluskey says.</p><p>In the new study, McCluskey and his colleagues focused on how the domain walls in the crystals they were investigating are shaped like cones. This geometry let them analyze the behavior of the domain walls using a relatively simple probe design.</p><p class="pull-quote">“Malleable domain-wall network architecture that can continually metamorphose could represent a kind of technological genie, granting wishes on demand for radical moment-to-moment changes in electronic function.” —<em>Conor McCluskey</em></p><p>The scientists found that electric-charge mobility was exceptionally fast at room temperature on average. These speeds may be “the highest room-temperature value in any oxide” and “at least comparable to that seen in graphene,” McCluskey says. The researchers detailed <u><a href="https://onlinelibrary.wiley.com/doi/full/10.1002/adma.202204298" target="_blank">their findings</a></u> in the 11 August issue of the journal <em>Advanced Materials</em>.</p><p>Precise knowledge of such parameters “are needed for envisioning and building devices that work reliably,” McCluskey says. “The dream is that it could allow completely malleable or ephemeral nanocircuitry to be created, destroyed, and reformed from one moment to the next.”</p><p>One promising application for domain walls may be brain-mimicking <a href="https://spectrum.ieee.org/neuromorphic-computing-more-than-ai" target="_self">neuromorphic computing</a>, with neuromorphic devices playing the role of the synapses that link neurons together, McCluskey says.</p><p>“The brain works by forging pathways which have some memory about their history: If a particular synaptic pathway is used more frequently, it becomes stronger, making it easier for this pathway to be used in the future. The brain learns by forging these stronger pathways,” McCluskey says. “Some domain-wall systems can behave in the same way: If you apply a small voltage to walls in our particular system, they tilt and change slightly, increasing their conductivity and giving a higher current. The next pulse will produce a higher current, and so on and so on, as if they have some memory of their past.”</p><p>If domain walls can play the role of artificial synapses, “this could pave the way to a low-heat-production, low-power-consumption brainlike architecture for neuromorphic computing,” he adds.</p><p>However, although reconfigurable electronics based on domain walls are a tantalizing idea, in many ferroelectrics the domain walls conduct only marginally better than the rest of the material, and so they will likely not help support viable devices, McCluskey notes.</p><p>“This isn’t a problem for the system we have investigated, lithium niobate, as it has quite an astonishing ratio between the conductivity of the domain walls and the bulk material,” McCluskey says. However, lithium niobate does currently require large voltages to manipulate domain walls. Scaling these systems down in thickness for use with everyday voltages “is one major hurdle,” he notes. “We are working on it.”</p><p>Future experiments will explore why electric-charge mobility is so fast in domain walls. “Broadly speaking, the carrier mobility relies on two things—the number of times the charge carrier will scatter or bump into something on its journey through the material, and the so-called effective mass with which the carrier moves,” McCluskey says.</p><p>Electrons can deflect off defects in materials, as well as vibrations known as phonons. “It is possible the presence of a domain wall alters the defect or phonon concentrations locally, resulting in fewer scattering centers along the domain wall,” McCluskey says.</p><p>When it comes to the effective mass of a charge carrier such as an electron, “when we consider an electron moving through a crystal lattice, we need to consider it not as a free electron, such as one in vacuum, but as an electron moving through the solid crystalline environment,” McCluskey explains. “The electron feels the effect of the nearby atoms as it progresses, changing its energy as it moves closer or further away from any given atom.” This can essentially make an electron moving in a crystal lighter or heavier than a normal electron. The way in which domain walls disturb crystal lattices may in turn alter the electron’s effective mass, he says.</p><p>“Without further experiments, it’s impossible to say which of these contributions is more responsible for determining the carrier mobility in our system,” McCluskey says. “We hope that our study prompts a shift in focus toward characterizing the transport in domain-wall systems, which may be every bit as exciting as some of the other 2D functional materials systems at the forefront of research today.”</p>]]></description><pubDate>Sat, 10 Sep 2022 01:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/domain-wall</guid><category>Two-dimensional materials</category><category>Lithium niobate</category><category>Ferroelectric</category><category>Domain wall</category><dc:creator>Charles Q. Choi</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/conical-ferroelectric-domain-walls-in-lithium-niobate.jpg?id=31639003&amp;width=980"></media:content></item><item><title>Introduction to Peer-to-Peer Streaming and GPU Processing in Data Acquisition Systems</title><link>https://go.teledynelecroy.com/ieee_p2p_webinar</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/image.png?id=28133682&width=980"/><br/><br/><p>Real-time digital signal processing is a vital part of many of today's data acquisition systems, and affordable graphics processing units (GPUs) offer a great complement to onboard field-programmable gate arrays (FPGAs).</p><p><span></span>Join Teledyne SP Devices for an introductory webinar about the basics and benefits of peer-to-peer streaming and GPU post-processing in data acquisition systems.</p><p><a href="https://go.teledynelecroy.com/ieee_p2p_webinar" target="_blank">Register now for this free webinar!</a></p><hr/><p><strong>Selected topics covered in this webinar:</strong></p><ul><li>Comparison between different data transfer methodologies</li><li>Processing differences between FPGA and GPU</li><li>Overview of solutions offered by Teledyne SP Devices</li></ul><p><strong>Who should attend?</strong> </p><p>Engineers that would like to learn more about data processing capabilities in high-performance data acquisition systems.</p><p><strong>What attendees will learn?</strong> </p><p>System-level requirements and achievable performance when using different data transfer methodologies between digitizers and graphics processing units (GPUs). A brief introduction to processing properties in FPGAs versus GPUs. Digitizer capabilities, operating system support, supported GPU models, etc.</p><p><strong>Presenter:</strong> Thomas Elter, Senior Field Applications Engineer</p>]]></description><pubDate>Wed, 07 Sep 2022 13:14:00 +0000</pubDate><guid>https://go.teledynelecroy.com/ieee_p2p_webinar</guid><category>Teledyne</category><category>Digital signal processing</category><category>Fpga</category><category>Gpu</category><category>Graphics processing unit</category><category>Signal processing</category><category>Type:webinar</category><dc:creator>Teledyne</dc:creator><media:content medium="image" type="image/png" url="https://assets.rbl.ms/28133682/origin.png"></media:content></item><item><title>Introduction to High-Level Synthesis (HLS) in Data Acquisition Systems</title><link>https://go.teledynelecroy.com/hls_IEEE_webinar_invitation</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/image.png?id=28133682&width=980"/><br/><br/><p>High-level synthesis (HLS) is used to develop firmware for field-programmable gate arrays (FPGAs) in languages such as C. </p><p>Join Teledyne SP Devices for an introductory webinar on HLS in the context of high-performance digitizers.</p><p><a href="https://go.teledynelecroy.com/hls_IEEE_webinar_invitation" target="_blank">Register now for this free webinar!</a></p><hr/><p><span></span></p><p><strong>Topics covered in this webinar:</strong></p><ul><li>Benefits of onboard FPGA signal processing</li><li>FPGA architecture and development basics</li><li>Programming languages and development tools</li><li>Application areas and signal processing examples</li></ul><p><strong>Who should attend? </strong></p><p><strong></strong>Developers that want to learn more about the possibilities and benefits of onboard digital signal processing in high-performance digitizers.</p><p><strong>What attendees will learn? </strong></p><p><strong></strong>An introduction to FPGA development for high-performance digitizers.</p><p><span></span></p><p><strong>Presenter:</strong> Thomas Elter, Senior Field Applications Engineer</p>]]></description><pubDate>Tue, 06 Sep 2022 16:06:06 +0000</pubDate><guid>https://go.teledynelecroy.com/hls_IEEE_webinar_invitation</guid><category>Fpga</category><category>Data acquisition</category><category>Programming languages</category><category>Teledyne</category><category>Type:webinar</category><dc:creator>Teledyne</dc:creator><media:content medium="image" type="image/png" url="https://assets.rbl.ms/28133682/origin.png"></media:content></item><item><title>Parasitic Extraction of MIM/MOM Capacitor Devices in Analog/RF Designs</title><link>https://engineeringresources.spectrum.ieee.org/free/w_engx195/prgm.cgi</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/parasitic-extraction-of-mim-mom-capacitor-devices-in-analog-rf-designs.gif?id=31300601&width=2000&height=1500&coordinates=92%2C0%2C93%2C0"/><br/><br/><p>The extensive use of MIM/MOM capacitors in analog/RF designs presents parasitic extraction challenges to designers. Understanding best practices and recommended tools for extracting the complex geometries of capacitor devices, as well as the in-context coupling effects for those devices in sensitive analog/RF blocks, enables designers to accurately apply the appropriate extraction process to different parts of the design.</p>]]></description><pubDate>Tue, 30 Aug 2022 14:42:02 +0000</pubDate><guid>https://engineeringresources.spectrum.ieee.org/free/w_engx195/prgm.cgi</guid><category>Type:whitepaper</category><dc:creator>Siemens</dc:creator><media:content medium="image" type="image/gif" url="https://spectrum.ieee.org/media-library/parasitic-extraction-of-mim-mom-capacitor-devices-in-analog-rf-designs.gif?id=31300601&amp;width=980"></media:content></item><item><title>Proposed Standardization of Chiplet Models for Heterogeneous Integration</title><link>https://engineeringresources.spectrum.ieee.org/free/w_engx193/prgm.cgi</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/proposed-standardization-of-chiplet-models-for-heterogeneous-integration.gif?id=31255546&width=2000&height=1500&coordinates=92%2C0%2C93%2C0"/><br/><br/><p>As general purpose chiplet providers offer their devices for use in heterogeneous package designs, manufacturers need a standardized set of design models to ensure operability in electronic design automation (EDA) design workflows. For successful industry-wide 3D IC packaging, these models should be:</p> <ul> <li>Electronically readable for use in design workflows</li> <li>Leverage existing industry standards that are readily available</li> <li>Compatible across the industry, without regard for supply for an open ecosystem and supply chain</li> </ul>]]></description><pubDate>Fri, 26 Aug 2022 18:42:11 +0000</pubDate><guid>https://engineeringresources.spectrum.ieee.org/free/w_engx193/prgm.cgi</guid><category>Type:whitepaper</category><dc:creator>Siemens</dc:creator><media:content medium="image" type="image/gif" url="https://spectrum.ieee.org/media-library/proposed-standardization-of-chiplet-models-for-heterogeneous-integration.gif?id=31255546&amp;width=980"></media:content></item><item><title>Delivering 3D IC Innovations Faster</title><link>https://engineeringresources.spectrum.ieee.org/free/w_engx194/prgm.cgi</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/delivering-3d-ic-innovations-faster.gif?id=31255528&width=2000&height=1500&coordinates=92%2C0%2C93%2C0"/><br/><br/><p>One of the biggest semiconductor engineering challenges today is delivering best-in-class devices while dealing with the technology scaling and cost limitations of monolithic IC design processes. To overcome these challenges, more companies are turning to heterogeneous integration and the 3D stacking of ICs and specialized chiplets (implemented in different processes geometries) into 3D ICs. Chiplets are small ICs specifically designed and optimized for operation within a package in conjunction with other chiplets and full-sized ICs. In heterogeneous designs, chips and chiplets are stacked and interconnected with vertical wiring. Designers can also combine them with 3D memory stacks, such as high bandwidth memory, on a silicon interposer within the package of a device.</p> <p>Learn how to engineer a smarter future faster by downloading this ebook.</p>]]></description><pubDate>Fri, 26 Aug 2022 18:41:23 +0000</pubDate><guid>https://engineeringresources.spectrum.ieee.org/free/w_engx194/prgm.cgi</guid><category>Type:whitepaper</category><dc:creator>Siemens</dc:creator><media:content medium="image" type="image/gif" url="https://spectrum.ieee.org/media-library/delivering-3d-ic-innovations-faster.gif?id=31255528&amp;width=980"></media:content></item><item><title>Simulating for High-Speed Digital Insights</title><link>https://connectlp.keysight.com/LP=30540?elqCampaignId=22564&amp;cmpid=ASC-2105674&amp;utm_source=ADSC&amp;utm_medium=ASC&amp;utm_campaign=306</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/image.png?id=27151193&width=980"/><br/><br/><p>The latest technology for serial links and memory interfaces is getting into the multi-gigabit range. We see them adopting multi-level modulations and more advanced data recovery methods. As a result creating a stable and compliant design is more challenging than ever before and standard signal integrity analysis is no longer sufficient.</p><p><a href="https://connectlp.keysight.com/LP=30540?elqCampaignId=22564&cmpid=ASC-2105674&utm_source=ADSC&utm_medium=ASC&utm_campaign=306" rel="noopener noreferrer" target="_blank">Register now for this free webinar!</a></p><p>Keysight is offering a design flow, which gives you all the insights you need. In this webinar series, our experts will cover leading edge applications of Keysight's premier SerDes and Memory simulation platform, PathWave ADS, with respect to Signal Integrity, Power Integrity and EMI simulation and analysis.</p>]]></description><pubDate>Thu, 25 Aug 2022 19:25:56 +0000</pubDate><guid>https://connectlp.keysight.com/LP=30540?elqCampaignId=22564&amp;cmpid=ASC-2105674&amp;utm_source=ADSC&amp;utm_medium=ASC&amp;utm_campaign=306</guid><category>Keysight</category><category>Signal processing</category><category>Simulation</category><category>Type:webinar</category><dc:creator>Keysight</dc:creator><media:content medium="image" type="image/png" url="https://assets.rbl.ms/27151193/origin.png"></media:content></item><item><title>3D-Stacked CMOS Takes Moore’s Law to New Heights</title><link>https://spectrum.ieee.org/3d-cmos</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/an-image-of-stacked-squares-with-yellow-flat-bars-through-them.png?id=30541109&width=2000&height=1500&coordinates=124%2C0%2C125%2C0"/><br/><br/><p>
<strong>Perhaps the most</strong> far-reaching technological achievement over the last 50 years has been the steady <a href="https://spectrum.ieee.org/a-better-way-to-measure-progress-in-semiconductors" target="_self">march toward ever smaller transistors</a>, fitting them more tightly together, and reducing their power consumption. And yet, ever since the two of us started our careers at Intel more than 20 years ago, we’ve been hearing the alarms that the descent into the infinitesimal was about to end. Yet year after year, brilliant new innovations continue to propel the semiconductor industry further.
</p><p>
	Along this journey, we engineers had to change the transistor’s architecture as we continued to scale down area and power consumption while boosting performance. The “planar” transistor designs that took us through the last half of the 20th century gave way to 3D fin-shaped devices by the first half of the 2010s. Now, these too have an end date in sight, with a new gate-all-around (GAA) structure rolling into production soon. But we have to look even further ahead because our ability to scale down even this new transistor architecture, which we call RibbonFET, has its limits.
</p><p>
	So where will we turn for future scaling? We will continue to look to the third dimension. We’ve created experimental devices that stack atop each other, delivering logic that is 30 to 50 percent smaller. Crucially, the top and bottom devices are of the two complementary types, NMOS and PMOS, that are the foundation of all the logic circuits of the last several decades. We believe this 3D-stacked complementary metal-oxide semiconductor (CMOS), or CFET (complementary field-effect transistor), will be the key to extending Moore’s Law into the next decade.
</p><h2>The Evolution of the Transistor </h2><p>
<strong>Continuous innovation is an essential underpinning of Moore’s Law</strong>, but each improvement comes with trade-offs. To understand these trade-offs and how they’re leading us inevitably toward 3D-stacked CMOS, you need a bit of background on transistor operation.
</p><p>
	Every metal-oxide-semiconductor field-effect transistor, or MOSFET, has the same set of basic parts: the gate stack, the channel region, the source, and the drain. The source and drain are chemically doped to make them both either rich in mobile electrons (
	<em>n</em>-type) or deficient in them (<em>p</em>-type). The channel region has the opposite doping to the source and drain.
</p><p>
	In the planar version in use in advanced microprocessors up to 2011, the MOSFET’s gate stack is situated just above the channel region and is designed to project an electric field into the channel region. Applying a large enough voltage to the gate (relative to the source) creates a layer of mobile charge carriers in the channel region that allows current to flow between the source and drain.
</p><p>
	As we scaled down the classic planar transistors, what device physicists call short-channel effects took center stage. Basically, the distance between the source and drain became so small that current would leak across the channel when it wasn’t supposed to, because the gate electrode struggled to deplete the channel of charge carriers. To address this, the industry moved to an entirely different transistor architecture called a 
	<a href="https://spectrum.ieee.org/how-the-father-of-finfets-helped-save-moores-law" target="_self">FinFET</a>. It wrapped the gate around the channel on three sides to provide better electrostatic control.
</p><h3>Transistor Evolution</h3><br/><img alt="Blocks of grey, silver, and black with a stripe of gold dots on one and a fin-shaped structure of gold dots on the other." class="rm-shortcode" data-rm-shortcode-id="5efbe5a5bb1d156298d117a1372c0553" data-rm-shortcode-name="rebelmouse-image" id="dc39f" loading="lazy" src="https://spectrum.ieee.org/media-library/blocks-of-grey-silver-and-black-with-a-stripe-of-gold-dots-on-one-and-a-fin-shaped-structure-of-gold-dots-on-the-other.jpg?id=30799212&width=980"/><h3></h3><br/><img alt="Blocks of grey, silver, and black with a stripe of gold dots on one and a fin-shaped structure of gold dots on the other." class="rm-shortcode" data-rm-shortcode-id="19f1ce24c1d6d33cec903b159c775a2f" data-rm-shortcode-name="rebelmouse-image" id="e1679" loading="lazy" src="https://spectrum.ieee.org/media-library/blocks-of-grey-silver-and-black-with-a-stripe-of-gold-dots-on-one-and-a-fin-shaped-structure-of-gold-dots-on-the-other.jpg?id=30541824&width=980"/><h3></h3><br/><p class="caption">
	The shift from a planar transistor architecture [left] to the FinFET [right] provided greater control of the channel [covered by blue box], resulting in a reduction in power consumption of 50 percent and an increase in performance of 37 percent.
	<style class="photo-credit">
Emily Cooper
	</style>
</p><p>
<a href="https://spectrum.ieee.org/intels-new-transistors-enter-the-third-dimension">Intel introduced its FinFETs</a> in 2011, at the 22-nanometer node, with the third-generation Core processor, and the device architecture has been the workhorse of Moore’s Law ever since. With FinFETs, we could operate at a lower voltage and still have less leakage, reducing power consumption by some 50 percent at the same performance level as the previous-generation planar architecture. FinFETs also switched faster, boosting performance by 37 percent. And because conduction occurs on both vertical sides of the “fin,” the device can drive more current through a given area of silicon than can a planar device, which only conducts along one surface.<br/>
</p><p>
	However, we did lose something in moving to FinFETs. In planar devices, the width of a transistor was defined by lithography, and therefore it is a highly flexible parameter. But in FinFETs, the transistor width comes in the form of discrete increments—adding one fin at a time–a characteristic often referred to as fin quantization. As flexible as the FinFET may be, fin quantization remains a significant design constraint. The design rules around it and the desire to add more fins to boost performance increase the overall area of logic cells and complicate the stack of interconnects that turn individual transistors into complete logic circuits. It also increases the transistor’s capacitance, thereby sapping some of its switching speed. So, while the FinFET has served us well as the industry’s workhorse, a new, more refined approach is needed. And it’s that approach that led us to the 3D transistors we’re introducing soon.
</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="A blue block pierced by three gold-coated ribbons all atop a thicker grey block." class="rm-shortcode rm-resized-image" data-rm-shortcode-id="a88176fce5c0dee6795e05f2f4541a91" data-rm-shortcode-name="rebelmouse-image" id="b8771" loading="lazy" src="https://spectrum.ieee.org/media-library/a-blue-block-pierced-by-three-gold-coated-ribbons-all-atop-a-thicker-grey-block.jpg?id=30541900&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">In the RibbonFET, the gate wraps around the transistor channel region to enhance control of charge carriers. The new structure also enables better performance and more refined optimization.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;"> Emily Cooper</small>
</p><p>
	This advance, the RibbonFET, is our first new transistor architecture since the FinFET’s debut 11 years ago. In it, the gate fully surrounds the channel, providing even tighter control of charge carriers within channels that are now formed by nanometer-scale ribbons of silicon. With these nanoribbons (also called 
	<a href="https://spectrum.ieee.org/the-nanosheet-transistor-is-the-next-and-maybe-last-step-in-moores-law" target="_self">nanosheets)</a>, we can again vary the width of a transistor as needed using lithography.
</p><p>
	With the quantization constraint removed, we can produce the appropriately sized width for the application. That lets us balance power, performance, and cost. What’s more, with the ribbons stacked and operating in parallel, the device can drive more current, boosting performance without increasing the area of the device.
	<br/>
</p><p>
	We see RibbonFETs as the best option for higher performance at reasonable power, and we will be introducing them in 2024 along with other innovations, such as PowerVia, our version of 
	<a href="https://spectrum.ieee.org/next-gen-chips-will-be-powered-from-below" target="_self">backside power delivery</a>, with the Intel 20A fabrication process.
</p><h2>Stacked CMOS </h2><p>
<strong>One commonality of planar, </strong><strong>FinFET, and RibbonFET transistors</strong> is that they all use CMOS technology, which, as mentioned, consists of <em>n</em>-type (NMOS) and <em>p</em>-type (PMOS) transistors. CMOS logic became mainstream in the 1980s because it draws significantly less current than do the alternative technologies, notably NMOS-only circuits. Less current also led to greater operating frequencies and higher transistor densities.
</p><p>
	To date, all CMOS technologies place the standard NMOS and PMOS transistor pair side by side. But in a 
	<a href="https://ieeexplore.ieee.org/document/8993462/" target="_blank">keynote</a> at the <a href="https://ieeexplore.ieee.org/xpl/conhome/8971803/proceeding" target="_blank">IEEE International Electron Devices Meeting (IEDM) in 2019</a>, we introduced the concept of a 3D-stacked transistor that places the NMOS transistor on top of the PMOS transistor. The following year, at IEDM 2020, we presented <a href="https://ieeexplore.ieee.org/document/9372066" target="_blank">the design for the first logic circuit using this 3D technique</a>, an inverter. Combined with appropriate interconnects, the 3D-stacked CMOS approach effectively cuts the inverter footprint in half, doubling the area density and further pushing the limits of Moore’s Law.
</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="Two blue blocks stacked atop each other. Each is pierced through by gold coated ribbons." class="rm-shortcode rm-resized-image" data-rm-shortcode-id="89aed2b1f017bfea983406c5caac631d" data-rm-shortcode-name="rebelmouse-image" id="f2f30" loading="lazy" src="https://spectrum.ieee.org/media-library/two-blue-blocks-stacked-atop-each-other-each-is-pierced-through-by-gold-coated-ribbons.jpg?id=30566578&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">3D-stacked CMOS puts a PMOS device on top of an NMOS device in the same footprint a single RibbonFET would occupy. The NMOS and PMOS gates use different metals.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">Emily Cooper</small>
</p><p>
	Taking advantage of the potential benefits of 3D stacking means solving a number of process integration challenges, some of which will stretch the limits of CMOS fabrication.
	<br/>
</p><p>
	We built the 3D-stacked CMOS inverter using what is known as a self-aligned process, in which both transistors are constructed in one manufacturing step. This means constructing both 
	<em>n</em>-type and <em>p</em>-type sources and drains by epitaxy—crystal deposition—and adding different metal gates for the two transistors. By combining the source-drain and dual-metal-gate processes, we are able to create different conductive types of silicon nanoribbons (<em>p</em>-type and <em>n</em>-type) to make up the stacked CMOS transistor pairs. It also allows us to adjust the device’s threshold voltage—the voltage at which a transistor begins to switch—separately for the top and bottom nanoribbons.
</p><div class="ieee-sidebar-medium">
<p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Three pairs of two grey rectangles float above three more pairs of grey rectangles in a black-and-white micrograph." class="rm-shortcode" data-rm-shortcode-id="48b52a45c5fb5944b65e66f2eba06564" data-rm-shortcode-name="rebelmouse-image" id="be20b" loading="lazy" src="https://spectrum.ieee.org/media-library/three-pairs-of-two-grey-rectangles-float-above-three-more-pairs-of-grey-rectangles-in-a-black-and-white-micrograph.png?id=30542655&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">In CMOS logic, NMOS and PMOS devices usually sit side by side on chips. An early prototype has NMOS devices stacked on top of PMOS devices, compressing circuit sizes.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Intel</small>
</p>
</div><p>
	How do we do all that? The self-aligned 3D CMOS fabrication begins with a silicon wafer. On this wafer, we deposit repeating layers of silicon and silicon germanium, a structure called a superlattice. We then use lithographic patterning to cut away parts of the superlattice and leave a finlike structure. The superlattice crystal provides a strong support structure for what comes later.
</p><p>
	Next, we deposit a block of “dummy” polycrystalline silicon atop the part of the superlattice where the device gates will go, protecting them from the next step in the procedure. That step, called the vertically stacked dual source/drain process, grows phosphorous-doped silicon on both ends of the top nanoribbons (the future NMOS device) while also selectively growing boron-doped silicon germanium on the bottom nanoribbons (the future PMOS device). After this, we deposit dielectric around the sources and drains to electrically isolate them from one another. The latter step requires that we then polish the wafer down to perfect flatness.
</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="Gold columns are bridged by a purple polygon and a green one. A rectangle bisects the polygon. It's pink on top and yellow on the bottom." class="rm-shortcode rm-resized-image" data-rm-shortcode-id="525d94aae14048862c4adbed75287921" data-rm-shortcode-name="rebelmouse-image" id="ca890" loading="lazy" src="https://spectrum.ieee.org/media-library/gold-columns-are-bridged-by-a-purple-polygon-and-a-green-one-a-rectangle-bisects-the-polygon-it-s-pink-on-top-and-yellow-on-th.jpg?id=30567356&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">An edge-on view of the 3D stacked inverter shows how complicated its connections are. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">Emily Cooper</small>
</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="Blue, pink and green rectangles representing different parts of transistors are arranged in a larger circuit on the left and one half the size on the right." class="rm-shortcode rm-resized-image" data-rm-shortcode-id="2d4db470e635020e04ef79ad35941677" data-rm-shortcode-name="rebelmouse-image" id="17c9b" loading="lazy" src="https://spectrum.ieee.org/media-library/blue-pink-and-green-rectangles-representing-different-parts-of-transistors-are-arranged-in-a-larger-circuit-on-the-left-and-one.jpg?id=30542209&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">By stacking NMOS on top of PMOS transistors, 3D stacking effectively doubles CMOS transistor density per square millimeter, though the real density depends on the complexity of the logic cell involved. The inverter cells are shown from above indicating source and drain interconnects [red], gate interconnects [blue], and vertical connections [green].</small>
</p><p>
	Finally, we construct the gate. First, we remove that dummy gate we’d put in place earlier, exposing the silicon nanoribbons. We next etch away only the silicon germanium, releasing a stack of parallel silicon nanoribbons, which will be the channel regions of the transistors. We then coat the nanoribbons on all sides with a vanishingly thin layer of an insulator that has a high dielectric constant. The nanoribbon channels are so small and positioned in such a way that we can’t effectively dope them chemically as we would with a planar transistor. Instead, we use a property of the metal gates called the work function to impart the same effect. We surround the bottom nanoribbons with one metal to make a 
	<em>p</em>-doped channel and the top ones with another to form an <em>n</em>-doped channel. Thus, the gate stacks are finished off and the two transistors are complete.<br/>
</p><p>
	The process might seem complex, but it’s better than the alternative—a technology called sequential 3D-stacked CMOS. With that method, the NMOS devices and the PMOS devices are built on separate wafers, the two are bonded, and the PMOS layer is transferred to the NMOS wafer. In comparison, the self-aligned 3D process takes fewer manufacturing steps and keeps a tighter rein on manufacturing cost, something we demonstrated in research and reported at IEDM 2019.
</p><p>
	Importantly, the self-aligned method also circumvents the problem of misalignment that can occur when bonding two wafers. Still, sequential 3D stacking is being explored to facilitate integration of silicon with nonsilicon channel materials, such as germanium and III-V semiconductor materials. These approaches and materials may become relevant as we look to tightly integrate optoelectronics and other functions on a single chip.
</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="Orange elongated blocks connect to several narrower blocks of a variety of colors. " class="rm-shortcode rm-resized-image" data-rm-shortcode-id="6e210c3ed15eb6f5ff93e896603d33ca" data-rm-shortcode-name="rebelmouse-image" id="78c40" loading="lazy" src="https://spectrum.ieee.org/media-library/orange-elongated-blocks-connect-to-several-narrower-blocks-of-a-variety-of-colors.jpg?id=30542266&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">Making all the needed connections to 3D-stacked CMOS is a challenge. Power connections will need to be made from below the device stack. In this design, the NMOS device [top] and PMOS device [bottom] have separate source/drain contacts, but both devices have a gate in common.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">Emily Cooper</small>
</p><p>
	The new self-aligned CMOS process, and the 3D-stacked CMOS it creates, work well and appear to have substantial room for further miniaturization. At this early stage, that’s highly encouraging. Devices having a gate length of 75 nm demonstrated both the low leakage that comes with excellent device scalability and a high on-state current. Another promising sign: We’ve made wafers where the smallest distance between two sets of stacked devices is only 
	<a href="https://ieeexplore.ieee.org/document/9720633" target="_blank">55 nm</a>. While the device performance results we achieved are not records in and of themselves, they do compare well with individual nonstacked control devices built on the same wafer with the same processing.
</p><p>
	In parallel with the process integration and experimental work, we have many ongoing theoretical, simulation, and design studies underway looking to provide insight into how best to use 3D CMOS. Through these, we’ve found some of the key considerations in the design of our transistors. Notably, we now know that we need to optimize the vertical spacing between the NMOS and PMOS—if it’s too short it will increase parasitic capacitance, and if it’s too long it will increase the resistance of the interconnects between the two devices. Either extreme results in slower circuits that consume more power.
</p><p>
	Many design studies, such as one by 
	<a href="https://ieeexplore.ieee.org/document/9720577/" target="_blank">TEL Research Center America presented at IEDM 2021</a>, focus on providing all the necessary interconnects in the 3D CMOS’s limited space and doing so without significantly increasing the area of the logic cells they make up. The TEL research showed that there are many opportunities for innovation in finding the best interconnect options. That research also highlights that 3D-stacked CMOS will need to have interconnects both above and below the devices. This scheme, called <a href="https://spectrum.ieee.org/next-gen-chips-will-be-powered-from-below" target="_self">buried power rails</a>, takes the interconnects that provide power to logic cells but don’t carry data and removes them to the silicon below the transistors. Intel’s PowerVIA technology, which does just that and is scheduled for introduction in 2024, will therefore play a key role in making 3D-stacked CMOS a commercial reality.
</p><h2>The Future of Moore’s Law</h2><p>
<strong>With RibbonFETs and 3D CMOS, we have a clear path</strong> to extend Moore’s Law beyond 2024. In a <a href="http://large.stanford.edu/courses/2012/ph250/lee1/docs/Excepts_A_Conversation_with_Gordon_Moore.pdf" target="_blank">2005 interview</a> in which he was asked to reflect on what became his law, Gordon Moore admitted to being “periodically amazed at how we’re able to make progress. Several times along the way, I thought we reached the end of the line, things taper off, and our creative engineers come up with ways around them.”
</p><p>
		With the move to FinFETs, the ensuing optimizations, and now the development of RibbonFETs and eventually 3D-stacked CMOS, supported by the myriad packaging enhancements around them, we’d like to think Mr. Moore will be amazed yet again. 
	<span class="ieee-end-mark"></span>
</p>]]></description><pubDate>Thu, 11 Aug 2022 15:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/3d-cmos</guid><category>Cfet</category><category>Cmos</category><category>Stacked cmos</category><category>Intel</category><category>Nanosheet transistors</category><category>Nanoribbon</category><category>Moore's law</category><category>Buried power rails</category><dc:creator>Marko Radosavljevic</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/an-image-of-stacked-squares-with-yellow-flat-bars-through-them.png?id=30541109&amp;width=980"></media:content></item><item><title>Accelerate Time to Market with Calibre nmLVS Recon Technology: A New Paradigm for Circuit Verification</title><link>https://engineeringresources.spectrum.ieee.org/free/w_engx174/prgm.cgi</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/accelerate-time-to-market-with-calibre-nmlvs-recon-technology-a-new-paradigm-for-circuit-verification.gif?id=30822102&width=2000&height=1500&coordinates=92%2C0%2C93%2C0"/><br/><br/><p>One thing is clear…tapeouts are getting harder, and taking longer. As part of a growing suite of innovative early-stage design verification technologies, the Calibre nmLVS Recon tool enables design teams to rapidly examine dirty and immature designs to find and fix high-impact circuit errors earlier and faster, leading to an overall reduction in tapeout schedules and time to market.</p> <p>Learn more in this technical paper.</p>]]></description><pubDate>Thu, 11 Aug 2022 14:40:13 +0000</pubDate><guid>https://engineeringresources.spectrum.ieee.org/free/w_engx174/prgm.cgi</guid><category>Type:whitepaper</category><dc:creator>Siemens</dc:creator><media:content medium="image" type="image/gif" url="https://spectrum.ieee.org/media-library/accelerate-time-to-market-with-calibre-nmlvs-recon-technology-a-new-paradigm-for-circuit-verification.gif?id=30822102&amp;width=980"></media:content></item><item><title>How New Storage Technologies Enhance HPC Systems</title><link>https://engineeringresources.spectrum.ieee.org/free/w_defa2975/prgm.cgi</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/how-new-storage-technologies-enhance-hpc-systems.gif?id=30412557&width=2000&height=1500&coordinates=83%2C0%2C84%2C0"/><br/><br/><p>High-performance computing (HPC) has historically been available primarily to governments, research institutions, and a few very large corporations for modeling, simulation, and forecasting applications. As HPC platforms are being deployed in the cloud for shared services, high-performance computing is becoming much more accessible, and its use is benefiting organizations of all sizes. Increasing investment in the industrial internet of things (IIoT), artificial intelligence (AI), and electronic design automation (EDA) and silicon IP for engineering development are a few factors that are driving increased use of high-performance computing systems. In general, increasingly complex models for big data processing, simulation, and forecasting are driving a need for more compute power and greater storage capacity & performance.</p> <p>This white paper highlights how different storage technologies can maximize the efficiency and effectiveness of HPC systems while providing high capacity and low latency storage, and minimizing network bandwidth and power consumption.</p>]]></description><pubDate>Mon, 01 Aug 2022 18:06:13 +0000</pubDate><guid>https://engineeringresources.spectrum.ieee.org/free/w_defa2975/prgm.cgi</guid><category>Type:whitepaper</category><dc:creator>Synopsys</dc:creator><media:content medium="image" type="image/gif" url="https://spectrum.ieee.org/media-library/how-new-storage-technologies-enhance-hpc-systems.gif?id=30412557&amp;width=980"></media:content></item><item><title>U.S. Passes Landmark Act to Fund Semiconductor Manufacturing</title><link>https://spectrum.ieee.org/chips-act-of-2022</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-gold-hued-photo-of-a-man-in-clean-room-garb-walking-down-a-an-aisle-with-white-machines-on-either-side.jpg?id=30326057&width=2000&height=1500&coordinates=31%2C0%2C31%2C0"/><br/><br/><p>Legislation aimed at increasing semiconductor manufacturing in the United States has finally passed both houses of Congress, following a multiyear journey that saw many mutations and delays. The <a href="https://www.commerce.senate.gov/services/files/1201E1CA-73CB-44BB-ADEB-E69634DA9BB9#:~:text=DIVISION%20A%20%2D%20CHIPS%20ACT%20OF%202022,-Sec.%20101%E2%80%94Short&text=This%20Act%20may%20be%20cited,%E2%80%9CCHIPS%20Act%20of%202022.%E2%80%9D&text=implement%20the%20Commerce%20Department%20semiconductor,Sec.%209902%20%26%209906)." target="_blank">CHIPS and Science Act</a>, provides about US $52 billion over 5 years to grow semiconductor manufacturing and authorizes a 25 percent tax credit for new or expanded facilities that make semiconductors or chipmaking equipment. It’s part of a $280 billion package aimed at improving the United States’ ability to compete in future technologies. And it comes amidst efforts by other nations and regions to boost chip manufacturing, an industry increasingly seen as a key to economic and military security.<br/></p><p>“This is going to make a huge difference in how the U.S. does innovation,” says <a href="https://ieeeusa.org/about/staff/" target="_blank">Russell T. Harrison</a>, acting managing director of IEEE-USA, who has been involved in the legislation since its beginnings more than two years ago.</p><p>The bill’s $52 billion includes $39 billion in grants for new manufacturing, $11 billion for federal semiconductor research programs and workforce development, and $2 billion for Defense Department–related microelectronics activities.</p><p class="pull-quote">“Twenty-five percent [tax credit] means we’re in it to win.”<br/>—Ian Steff, former U.S. Assistant Secretary of Commerce</p><p>In addition, the bill directs $200 million over five years to the National Science Foundation to “promote growth of the semiconductor workforce.” The Commerce Department expects the United States will need 90,000 more workers in the semiconductor industry by 2025.</p><p>And there’s a further $500 million for “coordinating with foreign government partners to support international information and communications technology security and semiconductor supply-chain activities, including supporting the development and adoption of secure and trusted telecommunications technologies, semiconductors, and other emerging technologies.”</p><p>The 25 percent tax credit goes a long way toward making the building of new capacity in the United States comparable with building it offshore, according to <a href="https://www.linkedin.com/in/ian-steff-7a9a6088/" target="_blank">Ian Steff, former Assistant Secretary of Commerce</a>, and now a consultant advising Minnesota-based chip foundry <a href="https://www.skywatertechnology.com/" target="_blank">Skywater Technology</a>. “Twenty-five percent means we’re in it to win,” he says.</p><p>The legislation has been variously sold as an opportunity to create well-paid jobs, a chance to strengthen the semiconductor supply chain following the chip shortage of 2020, and as a national-defense imperative that would lessen the concern that China might strangle the supply of 90 percent of the most advanced logic by attacking Taiwan. It might be all of that.</p><p>Big chip manufacturers have been planning to add and expand fabs in anticipation of government incentives. GlobalFoundries is doing a <a href="https://spectrum.ieee.org/ignoring-intel-rumors-globalfoundries-will-do-1billion-expansion" target="_blank">$1 billion addition in Malta, N.Y.</a> <a href="https://www.datacenterdynamics.com/en/news/tsmc-starts-work-on-12bn-arizona-semiconductor-fab-gets-funding-for-japanese-chip-rd/" target="_blank">TSMC is already building</a> a $12 billion facility in Arizona. And Samsung plans a <a href="https://www.samsung.com/us/sas/News/Detail/202111230901001_News_2021FabExpansion" target="_blank">$17 billion fab</a> outside Austin, while dangling the possibility of nearly $200 billion in the future. Intel was probably the most explicit in its expectations. When it announced a plan for a <a href="https://spectrum.ieee.org/intel-ohio-fab" target="_self">$20 billion fab complex in Ohio</a>, <a href="https://www.intel.com/content/www/us/en/newsroom/biographies/biography-keyvan-esfarjani.html#gs.n3ifog" rel="noopener noreferrer" target="_blank">Keyvan Esfarjani</a>, Intel senior vice president of manufacturing, supply chain, and operations made the strings explicit: “The scope and pace of Intel’s expansion in Ohio...will depend heavily on funding from the CHIPS Act,” he said at the time. The company said its investment could reach $100 billion over ten years with the proper government backing.</p><p>Getting this far has been “an effort that has transcended administrations and gotten bipartisan support since its early inception,” says Steff. Still, the legislation was stalled for a long time. The bill that passed in Congress largely appropriates funds for things that were already authorized in a the <a href="https://spectrum.ieee.org/us-takes-strategic-step-to-onshore-electronics-manufacturing" target="_blank">National Defense Authorization Act of 2021</a>, which passed in January of that year.</p><p>Within the U.S. semiconductor industry much of the debate fell into what Harrison calls the “normal legislative process.” Companies or industry sectors not covered under the legislation fight to gain inclusion, while those already on the inside fight to keep it exclusive, concerned that the pool of funds will become diluted. Some initial outsiders succeeded: Chip packaging, which has grown increasingly important as advanced processor makers find they cannot get enough computing from a single sliver of silicon, was swiftly added. Efforts to expand the bill beyond its manufacturing scope continued nearly up until the end. According to <a href="https://www.cnbc.com/2022/07/18/us-chip-industry-split-over-chips-act-benefits-to-intel.html" rel="noopener noreferrer" target="_blank">reports</a>, chip designers whose processors are manufactured by others, including AMD, Nvidia, and Qualcomm, indicated their displeasure that they would not get in on the act.</p><p>Finding the balance of who’s in and who’s out meant making the terms broad enough to accomplish the goal of bringing chip manufacturing to the United States “without making it so broad that it becomes mush,” says Harrison. “They have now settled on something a little bigger than they had at first, but it’s focused on chips and their manufacture.”</p>]]></description><pubDate>Fri, 29 Jul 2022 14:53:55 +0000</pubDate><guid>https://spectrum.ieee.org/chips-act-of-2022</guid><category>Fabs</category><category>Foundries</category><category>Tsmc</category><category>Intel</category><category>Samsung</category><category>Globalfoundries</category><category>Skywater technology foundry</category><category>Congress</category><category>Chips</category><category>Semiconductor manufacturing</category><dc:creator>Samuel K. Moore</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-gold-hued-photo-of-a-man-in-clean-room-garb-walking-down-a-an-aisle-with-white-machines-on-either-side.jpg?id=30326057&amp;width=980"></media:content></item><item><title>Is This the Best Semiconductor Ever Found?</title><link>https://spectrum.ieee.org/best-semiconductor-boron-arsenide</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/figure-of-cubic-boron-arsenide-on-a-colorful-background.jpg?id=30296928&width=2000&height=1500&coordinates=0%2C0%2C0%2C0"/><br/><br/><p>Silicon is the foundation of the electronics industry. However, its performance as a semiconductor leaves much to be desired. Now scientists have discovered that an obscure material known as cubic boron arsenide (c-BAs) may perform much better than silicon. In fact, it may be the best semiconductor ever found, and potentially even the best possible one.<br/></p><p><a href="https://spectrum.ieee.org/silicon-valley-cares-about-chips-again" target="_blank">Silicon</a> is one of the most abundant elements on earth. In its pure form, silicon is key to much of modern technology, from microchips to solar cells. However, its properties as a semiconductor are far from ideal.</p><p class="pull-quote">“We demonstrated, for the first time, a new material with high carrier mobility and simultaneously high thermal conductivity.”<br/>—Zhifeng Ren, University of Houston</p><p>For one thing, silicon is not very good at conducting heat. As such, <a href="https://spectrum.ieee.org/carbon-nanotubes-could-solve-overheating-problem-for-nextgen-computer-chips" target="_blank">overheating</a> and expensive <a href="https://spectrum.ieee.org/meet-the-electronics-cooling-system-400-million-years-in-the-making" target="_blank">cooling systems</a> are common in computers. Furthermore, although silicon lets electrons race through its structure easily, it is much less obliging to the positively charged absences of electrons known as holes. These weaknesses reduce silicon’s overall efficiency as a semiconductor. (To be fair, most semiconductors offer high mobility only for either electrons or holes.)</p><p>In <u><a href="https://www.science.org/doi/10.1126/science.aat5522" target="_blank">2018</a></u>, experiments revealed that c-BAs—a crystal grown from boron and arsenic, two relatively common mineral elements—conducted heat nearly 10 times as well as silicon. This is the best known thermal conductivity of any semiconductor, and the third-best known thermal conductivity of any material, behind diamond and isotopically enriched cubic boron nitride.</p><p>In addition, <u><a href="https://journals.aps.org/prb/abstract/10.1103/PhysRevB.98.081203" rel="noopener noreferrer" target="_blank">theoretical predictions</a></u> suggested that c-BAs would also possess very high mobility for both electrons and holes. Now, in <u><a href="https://www.science.org/doi/10.1126/science.abn4290" rel="noopener noreferrer" target="_blank">two</a></u> <u><a href="https://www.science.org/doi/10.1126/science.abn4727" rel="noopener noreferrer" target="_blank">studies</a></u> in the 22 July issue of the journal <em>Science</em>, experiments confirm cubic boron arsenide's high electron and hole mobility.</p><p>“We demonstrated, for the first time, a new material with high carrier mobility and simultaneously high thermal conductivity,” says Zhifeng Ren, a physicist and materials scientist at the University of Houston and a coauthor on both studies. “The findings point out a new direction for semiconductors that could revolutionize the semiconductor industry in the near future.”</p><p>Analyzing electron and hole mobility in c-BAs was challenging because the crystals the researchers had were small. In addition, the crystals were riddled with impurities that scattered the electrons and holes. By probing the crystals with <a href="https://orensteinlab.berkeley.edu/optical-techniques/#transGrat" rel="noopener noreferrer" target="_blank">laser</a> <a href="https://www.nature.com/articles/s41563-019-0498-x" rel="noopener noreferrer" target="_blank">pulses</a>, the team of scientists (from the University of Houston as well as MIT, the University of Texas at Austin, and Boston College) found that electrons and electron holes had the highest mobility at locations on the lattice with the fewest impurities.</p><p>Electron and hole mobility is measured in units of square centimeters per volt-seconds (cm<sup>2</sup>/V•s). Silicon has an electron mobility of 1,400 cm<sup>2</sup>/V•s and a hole mobility of 450 cm<sup>2</sup>/V•s  at room temperature. By contrast, according to the new findings, c-BAs has a mobility of 1,600 cm<sup>2</sup>/V•s  for both electrons and holes moving together at room temperature.</p><p>Moreover, one of the two new studies in <em>Science</em> found that electron mobility in c-BAs could reach as high as 3,000 cm<sup>2</sup>/V•s. This feat may be due to “hot electrons,” which preserve the energy generated by laser pulses used to excite the charge carriers longer than they do in most other materials.</p><p>So far, scientists have made c-BAs only in small, lab-scale batches that are not uniform. Still, Ren thinks it very likely that it can be made in a practical and economic way, since boron, arsenic, and the crystal fabrication technique are all inexpensive. He says that in order to maintain quality control, the crystals may be scaled to much larger sizes only “when the growth process is fully understood.”</p><p>In addition, says Ren, “my group has always believed that even higher thermal conductivity and higher mobility should be achieved when the crystal quality is further improved, so the near-term goal is to improve their growth for higher-quality crystals.”</p>]]></description><pubDate>Wed, 27 Jul 2022 16:07:32 +0000</pubDate><guid>https://spectrum.ieee.org/best-semiconductor-boron-arsenide</guid><category>Semiconductor</category><category>Silicon</category><category>Electronics</category><category>Solar cell</category><category>Electrons</category><category>Holes</category><category>Charge carrier mobility</category><category>Cubic boron arsenide</category><dc:creator>Charles Q. Choi</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/figure-of-cubic-boron-arsenide-on-a-colorful-background.jpg?id=30296928&amp;width=980"></media:content></item><item><title>Micron Is First to Deliver 3D Flash Chips With More Than 200 Layers</title><link>https://spectrum.ieee.org/micron-is-first-to-deliver-3d-flash-chips-with-more-than-200-layers</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-gold-rectangle-resembling-a-tall-apartment-building.jpg?id=30263105&width=2000&height=1500&coordinates=0%2C0%2C0%2C0"/><br/><br/><p><sup></sup>Boise, Idaho-based memory manufacturer <a href="https://www.micron.com/" target="_blank">Micron Technology</a> says it has reached volume production of a 232-layer NAND flash-memory chip. It’s the first such chip to pass the 200-layer mark, and it’s been a tight race. Competitors are currently providing 176-layer technology, and some already <a href="https://news.samsung.com/global/editorial-extraordinary-innovation-for-a-more-unforgettable-world-the-story-behind-samsungs-pioneering-v-nand-memory-solution" target="_blank">have working chips</a> with 200+ layers <a href="https://news.skhynix.com/sk-hynix-develops-worlds-highest-238-layer-4d-nand-flash/" target="_blank">in hand</a>.<br/></p><p>The new Micron tech as much as doubles the density of bits stored per unit area versus competing chips, packing in 14.6 gigabits per square millimeter. Its 1-terabit chips are bundled into 2-terabyte packages, each of which is barely more than a centimeter on a side and can store about two weeks worth of 4K video.</p><p>With 81 trillion gigabytes (81 zettabytes) of data generated in 2021 and International Data Corp.  <br/>(<a href="https://www.idc.com/" target="_blank">IDC</a>) predicting 221 ZB in 2026, “storage has to innovate to keep up,” says Alvaro Toledo, Micron’s vice president of data-center storage.</p><p>The move to 223 layers is a combination and extension of many technologies Micron has already deployed. To get a handle on them, you need to know the basic structure and function of 3D NAND flash. The chip itself is made up of a bottom layer of CMOS logic and other circuitry that’s responsible for controlling reading and writing operations and getting data on and off the chip as quickly and efficiently as possible. Improvements to this layer, such as optimizing the path data travels and reducing the capacitance of the chip’s inputs and outputs, yielded a 50 percent improvement in the data transfer rate to 2.4 Gb/s.</p><p>Above the CMOS are layers upon layers of NAND flash cells. Unlike other devices, Flash-memory cells are built vertically. They start as a (relatively) deep, narrow hole etched through alternating layers of conductor and insulator. Then the holes are filled with material and processed to form the bit-storing part of the device. It’s the ability to reliably etch and fill the holes through all those layers that’s a key limit to the technology. Instead of etching through all 232 layers in one go, Micron’s process builds them in two parts and stacks one atop the other. Even so, “it’s an astounding engineering feat,” says Alvaro. “That was one of the biggest challenges we overcame.”</p><p>According to Toledo, there is a path toward even more layers in future NAND chips. “There are definitely challenges,” he says. But “we haven’t seen the end of that path.”</p><p>Competitors are hot on Micron's heels. <a href="https://news.skhynix.com/sk-hynix-develops-worlds-highest-238-layer-4d-nand-flash/" rel="noopener noreferrer" target="_blank">SK Hynix says it is shipping samples of a 238-layer TLC product </a>that will be in full production in 2023. Samsung says it has working chips with <a href="https://news.samsung.com/global/editorial-extraordinary-innovation-for-a-more-unforgettable-world-the-story-behind-samsungs-pioneering-v-nand-memory-solution" rel="noopener noreferrer" target="_blank">more than 200-layers</a>, but it hasn't detailed when these will go into full production.</p><p>In addition to adding more and more layers, NAND flash makers have been increasing the density of stored bits by packing multiple bits into a single device. Each of the Micron chip’s memory cells is capable of storing three bits per cell. That is, the charge stored in each cell produces a distinct enough effect to discern eight different states. Though 3-bit-per-cell products (called TLC) are the majority, <a href="https://www.micron.com/products/nand-flash/qlc-nand" rel="noopener noreferrer" target="_blank">four-bit products</a> (called QLC) are also available. One QLC chip presented by Western Digital researchers at the <a href="https://www.isscc.org/" rel="noopener noreferrer" target="_blank">IEEE International Solid State Circuits Conference</a> earlier this year achieved a <a href="https://ieeexplore.ieee.org/document/9731110" rel="noopener noreferrer" target="_blank">15 Gb/mm<sup>2 </sup>areal density in a 162-layer chip</a>. And Kioxia engineers reported <a href="https://ieeexplore.ieee.org/document/9830513" rel="noopener noreferrer" target="_blank">5-bit cells</a> last month at the <a href="https://www.vlsisymposium.org/" rel="noopener noreferrer" target="_blank">IEEE Symposium on VLSI Technology and Circuits</a>. There has even been a <a href="https://ieeexplore.ieee.org/document/9779301" rel="noopener noreferrer" target="_blank">7-bit cell demonstrated</a>, but it required dunking the chip in 77-kelvin liquid nitrogen.</p><p><em>This post was updated on 2 August 2022 to clarify the state of SK Hynix's and Samsung's plans.</em></p>]]></description><pubDate>Tue, 26 Jul 2022 18:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/micron-is-first-to-deliver-3d-flash-chips-with-more-than-200-layers</guid><category>Flash memory</category><category>Nand flash memory</category><category>Micron technologies</category><dc:creator>Samuel K. Moore</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-gold-rectangle-resembling-a-tall-apartment-building.jpg?id=30263105&amp;width=980"></media:content></item><item><title>Reliability Verification in the Cloud Delivers Significant Runtime Benefits</title><link>https://engineeringresources.spectrum.ieee.org/free/w_defa2930/prgm.cgi</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/reliability-verification-in-the-cloud-delivers-significant-runtime-benefits.gif?id=30193424&width=2000&height=1500&coordinates=92%2C0%2C93%2C0"/><br/><br/><p>Design complexities and time-to-market pressures compel companies to find innovative ways to leverage all available resources. Cloud compute environments provide a scalable and sustainable platform that can significantly improve runtimes in Calibre® PERC™ flows and other demanding EDA compute tasks. Running Calibre PERC verification flows on cloud resources to satisfy peak demand usage can increase productivity and expedite turnaround-times. Understanding the cost/benefit relationship of cloud computing helps companies determine the optimal configuration that provides the greatest returns.</p>]]></description><pubDate>Thu, 21 Jul 2022 14:06:42 +0000</pubDate><guid>https://engineeringresources.spectrum.ieee.org/free/w_defa2930/prgm.cgi</guid><category>Type:whitepaper</category><dc:creator>Siemens</dc:creator><media:content medium="image" type="image/gif" url="https://spectrum.ieee.org/media-library/reliability-verification-in-the-cloud-delivers-significant-runtime-benefits.gif?id=30193424&amp;width=980"></media:content></item><item><title>The First Million-Transistor Chip: the Engineers’ Story</title><link>https://spectrum.ieee.org/intel-i860</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/twenty-people-crowd-into-a-cubicle-the-man-in-the-center-seated-holding-a-silicon-wafer-full-of-chips.jpg?id=29972100&width=2000&height=1500&coordinates=0%2C0%2C0%2C0"/><br/><br/><p><em></em><em></em><strong>In San Francisco</strong> on Feb. 27, 1989, <a href="https://www.intel.com/content/www/us/en/homepage.html" rel="noopener noreferrer" target="_blank">Intel Corp</a>., Santa Clara, Calif., startled the world of high technology by presenting the first ever 1-million-transistor microprocessor, which was also the company’s first such chip to use a reduced instruction set.</p><p>The number of transistors alone marks a huge leap upward: Intel’s previous microprocessor, the 80386, has only 275,000 of them. But this long-deferred move into the booming market in reduced-instruction-set computing (RISC) was more of a shock, in part because it broke with Intel’s tradition of compatibility with earlier processors—and not least because after three well-guarded years in development the chip came as a complete surprise. Now designated the i860, it entered development in 1986 about the same time as the 80486, the yet-to-be-introduced successor to Intel’s highly regarded 80286 and 80386. The two chips have about the same area and use the same 1-micrometer CMOS technology then under development at the company’s systems production and manufacturing plant in Hillsboro, Ore. But with the i860, then code-named the N10, the company planned a revolution.</p><p>Freed from the limitations of compatibility with the 80X86 processor family, the secret N10 team started with nothing more than a virtually blank sheet of paper.</p><h3></h3><br/><p>This article was first published as "Intel's secret is out." It appeared in the April 1989 issue of <em>IEEE Spectrum. </em>A <a href="https://ieeexplore.ieee.org/document/24150/" rel="noopener noreferrer" target="_blank">PDF version</a> is available on IEEE Xplore. The diagrams and photographs appeared in the original print version.</p><h3></h3><br/><h2>One man’s crusade</h2><p>The paper was not to stay blank for long. Leslie Kohn, the project’s chief architect, had already earned the nickname of Mr. RISC. He had been hoping to get started on a RISC microprocessor design ever since joining Intel in 1982. One attempt went almost 18 months into development, but current silicon technology did not allow enough transistors on one chip to gain the desired performance. A later attempt was dropped when Intel decided not to invest in that particular process technology.</p><p>Jean-Claude Cornet, vice president and general manager of Intel’s Santa Clara Microcomputer Division, saw N10 as an opportunity to serve the high-performance microprocessor market. The chip, he predicted, would reach beyond the utilitarian line of microprocessors into equipment for the high-level engineering and scientific research communities.</p><p>“We are all engineers,” Cornet told <em>IEEE Spectrum</em>, “so this is the type of need we are most familiar with: a computation-intensive, simulation-intensive system for computer-aided design.”</p><p>Discussions with potential customers in the supercomputer, graphics workstation, and minicomputer industries contributed new requirements for the chip. Supercomputer makers wanted a floating-point unit able to process vectors and stressed avoiding a performance bottleneck, a need that led to the entire chip being designed in a 64-bit architecture made possible by the 1 million transistors. Graphics workstation vendors, for their part, urged the Intel designers to balance integer performance with floating-point performance, and to make the chip able to produce three-dimensional graphics. Minicomputer makers wanted speed, and confirmed the decision that RISC was the only way to go for high performance; they also stressed the high throughput needed for database applications.</p><p class="pull-quote">Freed from the limitations of compatibility with the 80X86 processor family, the secret N10 team started with nothing more than a virtually blank sheet of paper.</p><p>The Intel team also speculated over what its competitors—such as MIPS Computer Systems Inc., <a href="https://spectrum.ieee.org/after-the-sun-microsystems-sets-the-real-stories-come-out" target="_self">Sun Micro Systems Inc</a>., and <a href="https://www.motorola.com/us/" rel="noopener noreferrer" target="_blank">Motorola Inc</a>.—were up to. The engineers knew their chip would not be the first in RISC architecture on the market, but the 64-bit technology meant that they would leapfrog their competitor’s 32-bit designs. They were also already planning the more fully defined architecture, with memory management, cache, floating-point, and other features on the one chip, a versatility impossible with what they correctly assumed were the smaller transistor budgets of their competitors.</p><h3>Defining terms</h3><br/><p>
<strong>Cache memory</strong>
</p><p>
	Units on chip buffers that hold data and instructions from main memory where they can be more readily accessed.
</p><p>
<strong>Data path</strong>
</p><p>
	The section of the chip through which information to be manipulated passes, including parts of the buses, caches, and registers.
</p><p>
<strong>Integer core</strong>
</p><p>
	The part of the chip that performs integer calculations, separate from the area where floating-point operations are performed; this section also performs some control functions for the entire chip.
</p><p>
<strong>Page</strong>
</p><p>
	A portion of a program or data to be moved in and out of main memory as needed.
</p><p>
<strong>Reduced-instruction-set computer (RISC)</strong>
</p><p>
	One where seldom used instructions are left out of a processor so as to gain speed in executing those remaining. Vector processing: the ability to manipulate arrays of numbers simultaneously.
</p><h3></h3><br/><p>The final decision rested with Albert Y.C. Yu, vice president and general manager of the company’s Component Technology and Development Group. For several years, Yu had been intrigued by Kohn’s zeal for building a superfast RISC microprocessor, but he felt Intel lacked the resources to invest in such a project. And because this very novel idea came out of the engineering group, Yu told Spectrum, he found some Intel executives hesitant. However, toward the end of 1985 he decided that, despite his uncertainty, the RISC chip’s time had come. “A lot depends on gut feel,” he said. “You take chances at these things.”</p><p>The moment the decision was made, in January 1986, the heat was on. Intel’s RISC chip would have to reach its market before the competition was firmly entrenched, and with the project starting up alongside the 486 design, the two groups might have to compete both for computer time and for support staff. Kohn resolved that conflict by making sure that the N10 effort was continually well out in front of the 486. To cut down on bureaucracy and communications overhead, he determined that the N10 team would have as few engineers as possible.</p><h2>Staffing up</h2><p>As soon as Yu approved the project, Sai Wai Fu, an engineer at the Hillsboro operation, moved to Santa Clara and joined Kohn as the team’s comanager. Fu and Kohn had known each other as students at the California Institute of Technology in Pasadena, had been reunited at Intel, and had worked together on one of Kohn’s earlier RISC attempts. Fu was eager for another chance and took over the recruiting, scrambling to assemble a compatible group of talented engineers. He plugged not only the excitement of breaking the million-transistor barrier, but also his own philosophy of management: broadening the engineers’ outlook by challenging them outside their areas of expertise.</p><p class="pull-quote">To cut down on bureaucracy and communications overhead, [Leslie Kohn] determined that the N10 team would have as few engineers as possible.The project attracted a number of experienced engineers within the company. Piyush Patel, who had been head logic designer for the 80386, joined the N10 team rather than the 486 project.</p><p>“It was risky,” he said, “but it was more challenging.”</p><p>Hon P. Sit, a design engineer, also chose N10 over the 486 because, he said: “With the 486, I would be working on control logic, and I knew how to do that. I had done that before. N10 needed people to work on the floating-point unit, and I knew very little about floating-point, so I was interested to learn.”</p><p>In addition to luring “escapees,” as 486 team manager John Crawford called them, the N10 group pulled in three memory design specialists from Intel’s technology development groups, important because there was to be a great deal of on-chip memory. Finally, Kohn and Fu took on a number of engineers fresh out of college. The number of engineers grew to 20, eight more than they had at first thought would be needed, but less than two thirds the number on the 486 team.</p><h2>Getting it down on paper</h2><p>During the early months of 1986, when he was not tied up with Intel’s lawyers over the NEC copyright suit (Intel had sued NEC alleging copyright infringement of its microcode for the 8086), Kohn refined his ideas about what N10 would contain and how it would all fit together. Among those he consulted informally was Crawford.</p><p> “Both the N10 and the 486 were projected to be something above 400 mils, and I was a little nervous about the size,” Crawford said. “But [Kohn] said ‘Hey, if it’s not 450, we can forget it, because we won’t have enough functions on the die. So we should shoot for 450, and recognize that these things hardly ever shrink.’”</p><p>The chip, they realized, would probably turn out to be greater than 450 mils on the side. The actual i860 measures 396 by 602 mils.</p><p>Kohn started by calling for a RISC core with fast integer performance, large caches for instructions and data, and specialized circuitry for fast floating-point calculations. Where most microprocessors take from five to 10 clock cycles to perform a floating-point operation, Kohn’s goal was to cut that to one cycle by pipelining. He also wanted a 64-bit data bus overall, but with a 128-bit bus between data cache and floating-point section, so that the floating-point section would not encounter bottlenecks when accessing data. Like a supercomputer, the chip would have to perform vector operations, as well as execute different instructions in parallel.</p><h3></h3><br/><img alt="Two men lean over a circuit diagram and a silicon wafer" class="rm-shortcode" data-rm-shortcode-id="e7878362605ffd7e19e8c6f37b517d9d" data-rm-shortcode-name="rebelmouse-image" id="b25a2" loading="lazy" src="https://spectrum.ieee.org/media-library/two-men-lean-over-a-circuit-diagram-and-a-silicon-wafer.jpg?id=29971925&width=980"/><h3></h3><br/><p>
	Early that April, Fu took a pencil and an 8 1/2-by-11-inch piece of paper and sketched out a plan for the chip, divided into eight sections: RISC integer core, paging unit, instruction cache, data cache, floating-point adder, floating-point multiplier, floating point registers, and bus controller. As he drew, he made some choices: for example, a line size of 32 bytes for the cache area. (A line, of whatever length, is a set of memory cells, the smallest unit of memory that can be moved to and fro between cache and main memory.) Though a smaller line size would have improved performance slightly, it would have forced the cache into a different shape and rendered it more awkward to position on the chip. “So I chose the smallest line size we could have and still have a uniform shape,” Fu said.
</p><p>
	His sketch also effectively did away with one of Kohn’s ideas: a data cache divided into four 128-bit compartments to create four-way parallelism-called four-way set associative. But as he drew his plan, Fu realized that the four-way split would not work. With two compartments, the data could flow from the cache in a straight line to the floating-point unit. With four-way parallelism, hundreds of wires would have to bend. “The whole thing would just fall apart for physical layout reasons,” Fu said. Abandoning the four-way split, he saw, would cost only 5 percent in performance, so the two-way cache won the day.
</p><p class="pull-quote">
	“When I was adding these blocks together, I didn’t add them properly. I missed 250 microns.”<br/>— Sai Wai Fu
</p><p>
	When he finished his sketch, he had a block of empty space. “I’d learned you shouldn’t pack so tight up front when you don’t know the details, because things grow,” Fu said. That space was filled up, and more. Several sections of the design grew slightly as they were implemented. Then one day toward the end of the design process, Fu recalls, an engineer apologetically said: “When I was adding these blocks together, I didn’t add them properly. I missed 250 microns.”
</p><p>
	It was a simple mistake in adding. “But it is not something that you can fix easily,” Fu said. “You have to find room for the 250 microns, even though we know that because we are pushing the limits of the process technology, adding a hundred microns here or there risks sending the yield way down.
</p><p>
	“We tried every trick we could think of to compensate, but in the end,” he said, “we had to grow the chip.”
</p><h3></h3><br><img alt="A circuit diagram sketched in pencil on paper, handwritten notes identify FPU-Adder, 32-64bit, INST CACHE, D-Cache, and other features" class="rm-shortcode" data-rm-shortcode-id="7db1a43b5364573239c0f5fe49a9ba79" data-rm-shortcode-name="rebelmouse-image" id="66184" loading="lazy" src="https://spectrum.ieee.org/media-library/a-circuit-diagram-sketched-in-pencil-on-paper-handwritten-notes-identify-fpu-adder-32-64bit-inst-cache-d-cache-and-other-fe.jpg?id=29971970&width=980"/><h3></h3><br/><img alt="Extreme close-up photo of a chip, with blocks of circuitry outlined in orange and blue, and labels for Data cache, Instruction ache, clock, Integer RISC core, and other features" class="rm-shortcode" data-rm-shortcode-id="2539e7c21a3b467d8c1d28da1c2be4aa" data-rm-shortcode-name="rebelmouse-image" id="e279c" loading="lazy" src="https://spectrum.ieee.org/media-library/extreme-close-up-photo-of-a-chip-with-blocks-of-circuitry-outlined-in-orange-and-blue-and-labels-for-data-cache-instruction-a.jpg?id=29971981&width=980"/><h3></h3><br/><p>Since Fu’s sketch partitioned the chip into eight blocks, he and Kohn divided their team into eight groups of either two or three engineers, depending upon the block’s complexity. The groups began work on logic simulation and circuit design, while Kohn continued to flesh out the architectural specifications.</p><p>“You can’t work in a top-down fashion on a project like this,” Kohn said. “You start at several different levels and work in parallel.”</p><p>Said Fu: “If you want to push the limits of a technology, you have to do top-down, bottom-up, and inside-out iterations of everything.”</p><p>The power budget at first caused serious concern. Kohn and Fu had estimated that the chip should dissipate 4 watts at 33 megahertz.</p><p>Fu divided the power budget among the teams, allocating half a watt here, a watt there. “I told them go away, do your designs, then if you exceed your budget, come back and tell me.”</p><p>The wide buses were a particular worry. The designers found that one memory cell on the chip drove a long transmission line with 1 to 2 picofarads of capacitance; by the time it reached its destination, the signal was very weak and needed amplification. The cache memory needed about 500 amplifiers, about 10 times as many as a memory chip. Designed like most static RAMs, these amplifiers would burn 2.5 watts—more than half the chip’s power budget. Building the SRAMs using circuit-design techniques borrowed from dynamic RAM technology cut that to about 0.5 watt.</p><p>“It turned out that while some groups exceeded their budget, some didn’t need as much, even though I purposely underestimated to scare them a little so they wouldn’t go out and burn a lot of power,” Fu said. The actual chip’s data sheet claims 3 watts of dissipation.</p><h3></h3><br/><h2>One instruction, one clock</h2><p>In meeting their performance goal, the designers made executing each instruction in one clock cycle something of a religion—one that required quite a number of innovative twists. Using slightly less than two cycles per instruction is common for RISC processors, so the N10 team’s goal of one instruction per cycle appeared achievable, but such rates are uncommon for many of the chip’s other functions. New algorithms had to be developed to handle floating-point additions and multiplications in one cycle in pipeline mode. The floating-point algorithms are among the some 20 innovations on the chip for which Intel is seeking patents.</p><p>Floating-point divisions, however, take anything from 20 to 40 cycles, and the designers saw early on that they would not have enough space on the chip for the special circuitry needed for such an infrequent operation.</p><h3></h3><br/><p>[The designers] did discover a way to do the fast three-dimensional graphics demanded by engineers and scientists, without any painful tradeoffs.</p><h3></h3><br/><p>The designers of the floating-point adder and multiplier units made the logic for rounding off numbers conform to IEEE standards, which slowed performance. (Cray Research Inc.’s computers, for example, reject those standards to boost performance.) While some N10 engineers wanted the higher performance, they found customers preferred conformity.<br/></p><p>However, they did discover a way to do the fast three-dimensional graphics demanded by engineers and scientists, without any painful tradeoffs. The designers were able to add this function by piggybacking a small amount of extra circuitry onto the floating-point hardware, adding only 3 percent to the chip’s size but boosting the speed of handling graphics calculations by a factor of 10, to 16 million 16-bit picture elements per second.</p><p>With a RISC processor, performing loads from cache memory in one clock cycle typically requires an extra register write port, to prevent interference between the load information and the result returning from the arithmetic logic unit. The N10 team figured out a way to use the same port for both pieces of information in one cycle, and so saved circuitry without losing speed. Fast access to instructions and data is key for a RISC processor: because the instructions are simple, more of them maybe needed. The designers developed new circuit design techniques—for which they have filed patent applications—to allow one-cycle access to the large cache memory through very large buses drawing only 2.5 watts.</p><p>“Existing SRAM parts can access data in a comparable amount of time, but they burn up a lot of power,” Kohn said.</p><h2>No creeping elegance</h2><p>The million transistors meant that much of the 2 1/2 years of development was spent in designing circuitry. The eight groups working on different parts of the chip called for careful management to ensure that each part would work seamlessly with all the others after their assembly.</p><p>First of all, there was the N10 design philosophy: no creeping elegance. “Creeping elegance has killed many a chip,” said Roland Albers, the team’s circuit design manager. Circuit designers, he said, should avoid reinventing the wheel. If a typical cycle is 20 nanoseconds, and an established technique leads to a path that takes 15 ns, the engineer should accept this and move on to the next circuit.</p><p class="pull-quote">“If you let people just dive in and try anything they want, any trick they’ve read about in some magazine, you end up with a lot of circuits that are marginal and flaky”<br/>—Roland Albers</p><p>Path timings were documented in initial project specifications and updated at the weekly meetings Albers called once the actual designing of circuits was under way.</p><p>“If you let people just dive in and try anything they want, any trick they’ve read about in some magazine, you end up with a lot of circuits that are marginal and flaky,” said Albers. “Instead, we only pushed it where it had to be pushed. And that resulted in a manufacturable and reliable part instead of a test chip for a whole bunch of new circuitry.”</p><p>In addition to enhancing reliability, the ban on creeping elegance sped up the entire process.</p><p>To ensure that the circuitry of different blocks of the chip would mesh cleanly, Albers and his circuit designers wrote a handbook covering their work. With engineers from Intel’s CAD department, he developed a graphics-based circuit-simulation environment with which engineers entered simulation schematics including parasitic capacitance of devices and interconnections graphically rather than alphanumerically. The output was then examined on a workstation as graphic waveforms.</p><p>At the weekly meetings, each engineer who had completed a piece of the design would present his results. The others would make sure it took no unnecessary risks, that it adhered to the established methodology, and that its signals would integrate with the other parts of the chip.</p><p><span></span>Intel had tools for generating the layout design straight from the high-level language that simulated the chip’s logic. Should the team use them or not? Such tools save time and eliminate the bugs introduced by human designers, but tend not to generate very compact circuitry. Intel’s own autoplacement tools for layout design cut density about in half, and slowed things down by one-third, when compared with handcrafted circuit design. Commercially available tools, Intel’s engineers say, do even worse.</p><p>Deciding when and where to use these tools was simple enough: those parts of the floating-point logic and RISC core that manipulate data had to be designed manually, as did the caches, because they involved a lot of repetition. Some cells are repeated hundreds, even thousands, of times (the SRAM cell is repeated 100,000 times), so the space gained by hand-packing the circuits involved far more than a factor of two. With the control logic, however, where there are few or no repetitions, the saving in time was considered worth the extra silicon, particularly because automatic generation of the circuitry allowed last-minute changes to correct the chip’s operation.</p><p class="pull-quote">About 40,000 transistors out of the chip’s more than a million were laid out automatically, while about 10,000 were generated manually and replicated to produce the remaining 980,000.</p><p>About 40,000 transistors out of the chip’s more than a million were laid out automatically, while about 10,000 were generated manually and replicated to produce the remaining 980,000. “If we’d had to do those 40,000 manually, it would have added several months to the schedule and introduced more errors, so we might not have been able to sample first silicon,” said Robert G. Willoner, one of the engineers on the team.</p><p>These layout-generation tools had been used at Intel before, and the team was confident that they would work, but they were less sure how much space the automatically designed circuits would take up.</p><p>Said Albers: “It took a little more than we had thought, which caused some problems toward the end, so we had to grow the die size a little.”</p><h2>Unauthorized tool use</h2><p>Even with automated layout, one section of the control logic, the bus controller, started to fall behind schedule. Fearing the controller would become a bottleneck for the entire design, the team tried several new techniques. RISC processors are usually designed to interface to a fast SRAM system that acts as an external cache and interfaces in turn with the DRAM main memory. Here, however, the plan was to make it possible for users to bypass the SRAM and attach the processor directly to a DRAM, which would allow the chip to be designed into low-cost systems as well as to address very large data structures.</p><p>For this reason, the bus can pipeline as many as three cycles before it gets the first data back from the DRAM, and the data has the time to travel through a slow DRAM memory without holding up the processor. The bus also had to use the static column mode, a feature of the newest DRAMs that allows sequential addresses accessing the same page in memory to tell the system, through a separate pin, that the bit is located on the same page as the previous bit.</p><p>Both those features presented unexpected design complications, the first because the control logic had to keep track of various combinations of outstanding bus cycles. While the rest of the chip was already being laid out, the bus designers were still struggling with the logic simulation. There was no time even for manual circuit design, followed by automatic layout, followed by a check of design against layout.</p><p>One of the designers heard from a friend in Intel’s CAD department about a tool that would take a design from the logic simulation level, optimize the circuit design, and generate an optimized layout. The tool eliminated the time taken up by circuit schematics, as well as the checking for schematics errors. It was still under development, however, and while it was even then being tested and debugged by the 486 team (who had several more months before deadline than did the N10 team), it was not considered ready for use.</p><p>The N10 designer accessed the CAD department’s mainframe through the in-house computer network and copied the program. It worked, and the bus-control bottleneck was solved.</p><p>Said CAD manager Nave guardedly: “A tool at that stage definitely has problems. The specific engineer who took it was competent to overcome most of the problems himself, so it didn’t have any negative impact, which it could have. It may have worked well in the case of the N10, but we don’t condone that as a general practice.”</p><h2>Designing for testability</h2><p>The N10 designers were concerned from the start about how to test a chip with a million transistors. To ensure that the chip could be tested adequately, early in 1987 and about halfway into the project a product engineer was moved in with the N10 team. At first, Beth Schultz just worked on circuit designs alongside the others, familiarizing herself with the chip’s functions. Later, she wrote diagnostic programs, and now, back in the product engineering department, she is supervising the i860’s transfer to Intel’s manufacturing operations.</p><p>The first attempt to test the chip demonstrated the importance of that early involvement by product engineering. In the normal course of events, a small tester—a logic analyzer with a personal computer interface—in the design department is working on a new chip’s circuits long before the larger testers in product engineering get in on the act. The design department’s tester debugs in turn the test programs run by product engineering. This time, because a product engineer was already so familiar with the chip, her department’s testers were operating before the one in the design department.</p><p>The product engineer’s presence on the team also made the other designers more conscious of the testability question, and the i860 reflects this in several ways. The product engineer was consulted when logic designers set the bus’s pin timing, to make sure it would not overreach the tester’s capabilities. Production engineering constantly reminded the N10 team of the need to limit the number of signal pins to 128: even one over would require spending millions of dollars on new testers. (The i860 has 120 signal pins, along with 48 pins for power and grounding.)</p><p>The chip’s control logic was formed with level-sensitive scan design (LSSD). Pioneered by <a href="https://www.ibm.com/us-en/" target="_blank">IBM Corp</a>., this design-for-testability technique sends signals through dedicated pins to test individual circuits, rather than relying on instruction sequences. LSSD was not employed for the data-path circuitry, however, because the designers determined that it would take up too much space, as well as slow down the chip. Instead, a small amount of additional logic lets the instruction cache’s two 32-bit segments test each other. A boundary scan feature lets system designers check the chip’s input and output connections without having to run instructions.</p><p class="pull-quote">Ordinarily the design and process engineers “don’t speak the same language. So tying the technology so closely to the architecture was unique.”<br/>— Albert Y.C. Yu</p><p>Planning the i860’s burn-in called for much negotiation between the design team and the reliability engineers. The i860 normally uses 64-bit instructions; for burn-in, the reliability engineers wanted as few connections as possible: 64 was far too many.</p><p>“Initially,” said Fu, “they started out with zero wires. They wanted us to self-test. So we said, ‘How about 15 or 20?’”</p><p>They compromised with an 8-bit mode that was to be used only for the burn-in, but with this feature i860 users can boot up the system from an 8-bit wide erasable programmable ROM.</p><p>The designers also worked closely with the group developing the 1-μm manufacturing process first used on a compaction of the 80386 chip that appeared early in 1988. Ordinarily, Intel vice president Yu said, the design and process engineers “don’t speak the same language. So tying the technology so closely to the architecture was unique.”</p><p>Said William Siu, process development engineering manager at Intel’s Hillsboro plant: “This process is designed for very low parasitic capacitance, which allows circuits to be built that have high performance and consume less power. We had to work with the design people to show them our limitations.”</p><p> The process engineers had the most influence on the on-chip caches. “Initially,” said designer Patel, “we weren’t sure how big the caches could be. We thought that we couldn’t put in as big a cache as we wanted, but they told us the process was good enough to do that.”</p><h2>A matter of timing</h2><p>The i860’s most unique architectural feature is perhaps its on-chip parallelism. The instruction cache’s two 32-bit segments issue two simultaneous 32-bit instructions, one to the RISC core, the other to the floating-point section. Going one step further, certain floating-point instructions call upon adder and multiplier simultaneously. The result is a total of three operations acted upon in one clock cycle.</p><p>The architecture increases the chip’s speed, but because it complicated the timing, implementing it presented problems. For example, if two or three parallel operations request the same data, they must be served serially. Many bugs found in the chip’s design involved this kind of synchronization.</p><p>The logic that freezes a unit when needed data is for the moment unavailable presented one of the biggest timing headaches. Initially, designers thought this situation would not crop up too often, but the on-chip parallelism caused it more frequently than had been expected.</p><p>The freeze logic grew and grew until, said Patel, “it became so kludgy we decided to sit down and redesign the whole freeze logic.” That was not a trivial decision—the chip was about halfway through its design schedule and that one revision took four engineers more than a month.</p><p class="pull-quote">Even running on a large mainframe, the circuit simulations were bogging down. Engineers would set one to run over the weekend and find it incomplete when they came in on Monday.</p><p>As the number of transistors approached the 1 million mark, the CAD tools that had been so much help began to break down. Intel has developed CAD tools in-house, believing its own tools would be more tightly coupled with its process and design technology, and therefore more efficient. But the N10 represented a vast advance on the 80386, Intel’s biggest microprocessor up to now, and the CAD systems had never been applied to a project anywhere near the size of the new chip. Indeed, because the i860’s parallelism has resulted in huge numbers of possible combinations (tens of millions have been tested; the total is many times that), its complexity is staggering. </p><p>Even running on a large mainframe, the circuit simulations were bogging down. Engineers would set one to run over the weekend and find it incomplete when they came in on Monday. That was too long to wait, so they took to their CAD tools to change the simulation program. One tool that goes through a layout to localize short circuits ran for days, then gave up. “We had to go in and change the algorithm for that program,” Willoner said.</p><p>The team first planned to plot the entire chip layout as an aid in debugging, but found that it would take more than a week of running the plotters round the clock. They gave up, and instead examined on workstations the chip’s individual areas.</p><p>But now the mainframe running all these tools began to balk. The engineers took to setting their alarm clocks to ring several times during the night and logging on to the system through their terminals at home to restart any computer run that had crashed.</p><p class="pull-quote">The net-list software failed totally; the schematic was just too big.</p><p>Before a chip design is turned over to manufacturing for its first silicon run—a transfer called the tape-out—the computer performs full-chip verification, comparing the schematics with the layout. To do this, it needs a net list, an intermediate version of the schematic, in the form of alphanumerics. The net list is normally created only a few days before tape-out, when the design is final. But knowing the 486 team was on their heels and would soon be demanding—and, as a priority project, receiving—the manufacturing department’s resources, the N10 team did a full-chip-verification dry run two months early with an incomplete design.</p><p>And the net-list software failed totally; the schematic was just too big. “Here we were, approaching tape-out, and we suddenly discover we can’t net-list this thing,” said Albers. “In three days one of our engineers figured out a way around it, but it had us scared for a while.”</p><h2>Into silicon</h2><p>After mid-August, when the chip was turned over to the product engineering department to be prepared for manufacture, all the design team could do was wait, worry, and tweak their test programs in the hope that the first silicon run would prove functional enough to test completely. And six weeks later, when the first batch of wafers arrived, they were complete enough to be tested, but not enough to be packaged. Normally, design and product engineering teams wait until wafers are through the production process before testing them, but not this time.</p><p>Rajeev Bharadhwaj, a design engineer, flew to Oregon—on a Monday—to pick up the first wafers, hot off the line. By 9:30 p.m. he was back in Santa Clara, where the whole design team, as well as product engineers and marketing people, waited while the first test sequences ran—at no more than 10 MHz, far below the 33 MHz target. It looked like a disaster, but after the engineers spent 20 nervous minutes going over critical paths in the chips in search of the bottleneck, one noticed that the power-supply pin was not attached—the chip had been drawing power only from the clock signal and its I/O systems. Once the power pin was connected, the chip ran easily at 40 MHz.</p><p>By 3 a.m., some 8000 test vectors had been run through the chip—vectors that the product engineer had worked six months to create. This was enough for the team to pronounce confidently: “It works!”</p><p>The <a href="https://en.wikipedia.org/wiki/Intel_i860" target="_blank">i860 designation</a> was chosen to indicate that the new chip does bear a slight relationship to the 80486—because the chips structure their data with the same byte ordering and have compatible memory-management systems, they can work together in a system and exchange data.</p><h2>This little chip goes to market</h2><p>Intel expects to have the chip available—at $750 for the 33 MHz and $1037 for the 40 MHz version—in quantity by the fourth quarter of this year, and has already shipped samples to customers. (Peripheral chips for the 386 can be used with the i860 and are already on the market.) Because the i860 has the same data-storage structure as the 386, operating systems for the 386 can be easily adapted to the new production.</p><p>Intel has announced a joint effort toward developing a multiprocessing version of Unix for the i860 with <a href="https://www.att.com/" target="_blank">AT&T Co.</a> (Unix Software Operation, Morristown, N.J.), Olivetti Research Center (Menlo Park, Calif.), Prime Computer (Commercial Systems Group, Natick, Mass.), and Convergent Technologies (San Jose, Calif., a division of Unisys Corp.). <a href="https://www.tek.com/en/about-us?&utm_source=Google&utm_medium=ppc&utm_term=tektronix&utm_content=Evergreen&utm_campaign=Evergreen-Tektronix-Brand&_bt=586384014457&_bk=tektronix&_bm=p&_bn=g&_bg=135394538818&gclid=CjwKCAjwkYGVBhArEiwA4sZLuBkXnzonk59wC_7xp3-USfmctYc8SZOjogX6MNP66erXy_gSF5moSxoCP8UQAvD_BwE" target="_blank">Tektronix NC</a> and <a href="https://www.kontron.com/en" target="_blank">Kontron Elektronik GmbH</a> plan to manufacture debuggers (logic analyzers) for the chip.</p><p>For software developers, Intel has developed a basic tool kit (assemblers, simulators, debuggers, and the like) and Fortran and C compilers. In addition, Intel has a Fortran vectorizer, a tool that automatically restructures standard Fortran code into vector processes with a technology previously only available for supercomputers.</p><p> IBM plans to make the i860 available as an accelerator for the PS/2 series of personal computers, which would boost them to near supercomputer performance. Kontron, <a href="https://www.softwareag.com/en_corporate/company/connected-enterprise.html" target="_blank">SPEA Software AG</a>, and Number Nine Computer Corp. will be using the i860 in personal-computer graphics boards. <a href="https://www.microsoft.com/en-us/?ql=2" target="_blank">Microsoft Corp.</a> has endorsed the architecture but has not yet announced products.</p><p>Minicomputer vendors are excited about the chip because the integer performance is much higher than was expected when the project began.</p><p>“We have the Dhrystone record on a microprocessor today’’—85,000 at 40 MHz, said Kohn. (A Dhrystone is a synthetic benchmark representing an average integer program and is used to measure integer performance of a microprocessor or computer system.) Olivetti is one company that will be using the N10 in minicomputers, as will PCS Computer Systems Inc.</p><p>Megatek Corp. is the first company to announce plans to makei860-based workstations in a market where the chip will be competing with such other RISC microprocessors as SPARC from Sun, the 88000 from Motorola, Clipper from Integraph Corp., and R3000 from MIPS Computer Systems Inc.</p><p>Intel sees its chip as having leapfrogged the current 32-bit generation of microprocessors. The company’s engineers think the i860 has another advantage: whereas floating-point chips, graphics chips, and caches must be added to the other microprocessors to build a complete system, the i860 is fully integrated, and therefore eliminates communications overhead. Some critics see this as a disadvantage, however, because it limits the choices open to system designers. It remains to be seen if this feature can overcome the lead the other chips have in the market.</p><p>The i860 team expects other microprocessor manufacturers to follow with their own 64-bit products with other capabilities besides RISC integer processing integrated onto a single chip. As leader in the new RISC generations, however, Intel hopes the i860 will set a standard for workstations, just as the 8086 did for personal computers.</p><h2>To probe further</h2><p>Intel’s first paper describing the i860, by Leslie Kohn and SaiWai Fu—<a href="https://ieeexplore.ieee.org/document/48231" rel="noopener noreferrer" target="_blank">’’A 1,000,000 transistor microprocessor”</a>—was published in the 1989 International Solid-State Circuits Conference Digest of Technical Papers, February 1989, pp. 54-55.</p><p>The advantages of reduced-instruction-set computing (RISC) are discussed in <a href="https://ieeexplore.ieee.org/document/6370784" rel="noopener noreferrer" target="_blank">“Toward simpler, faster computers</a>,” by Paul Wallich (IEEE Spectrum, August 1985, pp. 38-45).</p><p><span></span><em><strong>Editor’s note June 2022:</strong> The i860 (N10) microprocessor did not exactly take the marketplace by storm. Though it handled graphics with impressive speed and found a niche as a graphics accelerator, its performance on general-purpose applications was disappointing. Intel discontinued the chip in the mid-1990s.</em></p><h3> Approaching Cray 1 performance—with a single chip</h3><br><img alt="Four rows of waveform diagrams with processing steps noted blue" class="rm-shortcode" data-rm-shortcode-id="eb3dfe11a84ab7b251496a867e89a3fb" data-rm-shortcode-name="rebelmouse-image" id="7ac8f" loading="lazy" src="https://spectrum.ieee.org/media-library/four-rows-of-waveform-diagrams-with-processing-steps-noted-blue.jpg?id=29972050&width=980"/><p>The Intel i860—called the N10 by its designers —is a 64-bit CMOS microprocessor measuring 488 square mils. It contains more than 1 million transistors.</p><p>The chip’s core is a reduced-instruction-set computing (RISC) processor that, for integer operations, has been benchmarked at 85,000 Dhrystones (a standard measurement of integer performance) at 40 megahertz, according to Intel. (A RISC microprocessor from MIPS Computer Systems Incl, one of the fastest available, hits 42,000 Dhrystones on this benchmark; the Intel 80386 hits 13,000 Dhrystones.)</p><p>One-third of the i860 is devoted to floating-point calculations and runs at 80 million floating-point operations per second for single-precision calculations, and at 60 MFLOPS for double-precision calculations, again at 40 MHz.</p><p>In addition to standard scalar operations, the i860 can, like a supercomputer, perform vector operations. This vector capability is not implemented by dedicated hardware; rather, it is handled by a software library, and an on-chip cache that stores data for the chip’s regular calculations also holds the intermediate results of the vector operations. The i860’s overall performance is for most applications about half the speed of a Cray-1.</p><p>The chip’s designers consider its most important feature to be its balance. There are no on-chip bottlenecks, they say. All execution units run at the same speed, and the data flow to those units is just as fast as they can handle it. Therefore, the designers ay, the full power of the RISC and floating-point units is constantly available. Performance measurements refer not just to peak running, but are sustainable for vector operations. In addition, the integer and floating-point addition and multiplication units can function simultaneously at up to three operations for each clock.</p></br></br>]]></description><pubDate>Sat, 02 Jul 2022 15:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/intel-i860</guid><category>History</category><category>Risc</category><category>Intel</category><category>Microprocessors</category><category>Chip design</category><category>History of technology</category><dc:creator>Tekla S. Perry</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/twenty-people-crowd-into-a-cubicle-the-man-in-the-center-seated-holding-a-silicon-wafer-full-of-chips.jpg?id=29972100&amp;width=980"></media:content></item><item><title>Why Graphyne Isn’t Graphene 2.0</title><link>https://spectrum.ieee.org/graphyne-wondering-over-the-new-wonder-material</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/illustration-of-graphyne-structure.jpg?id=30012828&width=2000&height=1500&coordinates=51%2C0%2C51%2C0"/><br/><br/><p>Since graphene’s discovery 18 years ago—leading to a Nobel Prize in Physics in 2010—the versatile material has been investigated for hundreds of applications. These include strong <a href="https://spectrum.ieee.org/graphene-composites-go-big" target="_self">composite materials</a>, high-capacity battery electrodes, transparent conductive coatings for displays and solar cells, <a href="https://spectrum.ieee.org/smallest-transistor-one-carbon-atom" target="_self">supersmall</a> and <a href="https://spectrum.ieee.org/ucla-builds-fastest-graphene-transistor-yet" target="_self">ultrafast transistors</a>, and <a href="https://spectrum.ieee.org/printed-electronics-graphene-emulsion" target="_self">printable electronics</a>.</p><p>While graphene is finding its way into sports equipment and car tires for its mechanical strength, though, its highly touted electronic applications have been slower to materialize. One reason is that bulk graphene is not a semiconductor. To make it semiconductive, which is crucial for transistors, it must be produced in the form of nanoribbons with the right dimensional ratios.</p><p>There’s another one-dimensional form of carbon related to graphene that scientists first predicted back in 1987, that is a semiconductor without needing to be cut into certain shapes and sizes. But this material, graphyne, has proven nearly impossible to make in more than microscopic quantities.</p><p>Now, researchers at the University of Colorado in Boulder have reported a method to produce graphyne in bulk. “By using our method we can make bulk powder samples,” says <a href="https://www.colorado.edu/chemistry/wei-zhang" target="_blank">Wei Zhang</a>, a professor of chemistry at University of Colorado Boulder. “We find multilayer sheets of graphyne made of 20 to 30 layers. We are pretty confident we can use different exfoliation methods to gather a few layers or even a single layer.”</p><p>Graphite, diamond, fullerenes, and graphene are all carbon allotropes, and their diverse properties arise from the combination and arrangement of multiple types of bonds between their carbon atoms. So while the 3D cubic lattice of carbon atoms in diamond make it exceptionally hard, graphene’s single layer of carbon atoms in a hexagonal lattice make it extremely conductive.</p><p>Graphyne is similar to graphene in that it’s an atom-thick sheet of carbon atoms. But instead of a hexagonal lattice, it can take on different structures of spaced-apart rings connected via triple bonds between carbon atoms.</p><p>The material’s unique conducting, semiconducting, and optical properties could make it even more exciting for electronic applications than graphene. Graphyne's intrinsic electron mobility could, in theory, be 50 percent higher than graphene. In some graphynes, electrons can be conducted only in one direction. And the material has other exciting properties such as ion mobility, which is important for battery electrodes. </p><p>Zhang, Yingjie Zhao of Qingdao University of Science and Technology, in China, and their colleagues made graphyne using a method called <a href="https://en.wikipedia.org/wiki/Alkyne_metathesis" target="_blank">alkyne metathesis</a>. This is a catalyst-triggered organic reaction in which chemical bonds between carbon atoms in hydrocarbon molecules can crack open and reform to reach a more stable structure.</p><p>The process is complicated and slow. But it produces enough graphyne for scientists to be able to study the material’s properties in depth and evaluate its uses for potential applications. “It will take at least a couple years to have some fundamental understanding of the material,” says Zhang. “Then it will be in good shape for people to take it to a higher level, which is targeting specific semiconducting or battery applications.”</p><p>He and his colleagues plan to investigate ways to produce the material in much larger quantities. Being able to use solution-based chemical reactions would be critical for making graphyne at industrially relevant scales, he says. </p><p>It’s just the beginning for graphyne though, and for now, just being able to make this long-hypothesized material in sufficient quantities is an exciting first step. “Fullerenes were discovered in the 1980s, then nanotubes in the early '90s, then graphene in 2004,” Zhang says. “From discovery of a new carbon allotrope to its intensive study to first application, the timeline is becoming shorter. I’m already receiving calls from venture capitalists around the world. But I tell them it’s a little bit early.”</p>]]></description><pubDate>Thu, 23 Jun 2022 18:20:00 +0000</pubDate><guid>https://spectrum.ieee.org/graphyne-wondering-over-the-new-wonder-material</guid><category>Graphene</category><category>Graphyne</category><category>Nanomaterials</category><dc:creator>Prachi Patel</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/illustration-of-graphyne-structure.jpg?id=30012828&amp;width=980"></media:content></item><item><title>This Amplifier Chip for Light Is Faint No More</title><link>https://spectrum.ieee.org/chip-scale-photonic-amplifier</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-series-of-disks-of-varying-sizes-on-a-flat-square-in-the-center-one-circle-glows-green-with-a-line-coming-from-top-and-from.jpg?id=30008472&width=2000&height=1500&coordinates=0%2C0%2C0%2C0"/><br/><br/><p>Photonics has long held the promise of microchips that operate faster and consume less energy than their electronic counterparts. However, developing such circuitry has proven challenging over the years. One of the primary difficulties involves providing enough output power to yield a strong enough signal. However, researchers have now developed a chip-scale power booster for light with performance roughly as good as those already seen in commercial telecommunications.<br/></p><p>The <a href="https://www.infrapedia.com/" target="_blank">ultrabroadband fiber-optic networks</a> that now connect the globe depend on <a href="https://www.rp-photonics.com/erbium_doped_fiber_amplifiers.html" target="_blank">erbium-doped fiber amplifiers</a> that enable ultrafast data rates worldwide. Optical signals must be amplified many times when transmitted over great distances due to signal losses from the optical fibers and other network components. First developed in the 1980s, erbium-doped fiber amplifiers help boost optical signals without the extra step of converting them into electrical signals beforehand. (Specifically, these devices can amplify light in the <a href="https://www.garlandtechnology.com/blog/fiber-facts-yes-you-do-need-to-read-this#:~:text=There%20are%20three%20main%20wavelengths,the%20glass%20and%20causes%20attenuation." target="_blank">1.55-micrometer or 1,550-nanometer wavelength region</a>, where optical fibers experience the fewest transmission losses.)</p><p class="pull-quote">“The most exciting part of this work is how well the amplifiers work and that they are on par with commercial amplifiers, despite just being a few hundred microns across in each dimension.”<br/>—Tobias Kippenberg, Swiss Federal Institute of Technology Lausanne</p><p>For decades, scientists have sought to invent similar amplifiers that can work onboard photonic microchips. However, attempts to develop such chip-scale amplifiers resulted in devices whose output power was typically less than a milliwatt—too weak for many practical applications—due mostly to losses from the waveguides used to route light inside the chips. These amplifiers typically also had large footprints, and their fabrication was not compatible with contemporary photonic integrated circuit manufacturing techniques.</p><p>Now researchers have developed a chip-scale version of an erbium-doped fiber amplifier. The new device has a record-high output power of more than 145 milliwatts given just 2.61 mW of input power with more than 30 decibels of small-signal gain. This resulted in a more than a thousandfold amplification in the telecommunication band in continuous operation, a performance already comparable to that of commercial high-end erbium-doped fiber amplifiers.</p><p>“The most exciting part of this work is how well the amplifiers work and that they are on par with commercial amplifiers, despite just being a few hundred microns across in each dimension,” says study senior author Tobias Kippenberg, an optical engineer at the Swiss Federal Institute of Technology Lausanne. “That it would be possible to realize such amplifiers seemed impossible just a few years ago.”</p><p>Moreover, the researchers packed this device’s erbium-doped waveguide of up to a half-meter long into a spiral whose footprint measured just 1.2 millimeters by 3.6 mm across. The device also operates with a high power-conversion efficiency of roughly 60 percent.</p><p>The key to this new device is an ultralow-loss chip-scale photonic waveguide based on <a href="https://en.wikipedia.org/wiki/Silicon_nitride" target="_blank">silicon nitride</a>, a material that has already widely used in the semiconductor industry. Recently, Kippenberg and his colleagues fabricated <a href="https://www.nature.com/articles/s41467-021-21973-z" target="_blank">ultralow-loss silicon nitride waveguides up to meters long</a>. This led them to investigate whether implanting erbium into such waveguides might result in optical amplifiers.</p><p>“Erbium ions can provide amplification of light but only very faintly,” Kippenberg says. “It is only when they are embedded into very-low-loss optical fibers, and when they interact with light for very long distances, typically meters in length, that one can achieve gain.”</p><p>In experiments, the researchers showed they could boost the output power of devices known as <a href="https://www.nature.com/articles/s41566-021-00901-z" target="_blank">soliton microcombs</a> by 100 times. Soliton microcombs, can be used in spectroscopy, metrology, and other applications, but their output power is limited to only tens to hundreds of microwatts, requiring amplification in almost every application.</p><p>The scientists also revealed their device could directly amplify more than 20 <a href="https://spectrum.ieee.org/optical-nets-brace-for-even-heavier-traffic" target="_self">wavelength-division multiplexing</a> channels for data transmission over a 1-kilometer-long optical fiber link. This suggests it could be used in chip-scale amplification for telecommunication networks.</p><p>The researchers noted their optical amplifier still needs a pump laser that dwells off the microchip. Therefore, the entire unit is still not integrated together yet. “This is a key deficiency we need to address in the future via hybrid integration,” Kippenberg says.</p><p>Ultimately, the scientists hope their optical amplifiers may help enable chip-scale, mode-locked lasers delivering bursts just quadrillionths of a second (a.k.a. <a href="https://en.wikipedia.org/wiki/Femtosecond" target="_blank">femtoseconds</a>) long. Such devices may have a wide variety of uses, such as lidar, says study lead author Yang Liu, an optical engineer at the Swiss Federal Institute of Technology Lausanne.</p><p>“The femtosecond mode-locked laser is clearly a holy grail, and what we now are setting our sights on,” Kippenberg says.</p><p>The scientists detailed <u><a href="http://www.science.org/doi/10.1126/science.abo2631" rel="noopener noreferrer" target="_blank">their findings</a></u> 16 June in the journal <em>Science</em>.</p>]]></description><pubDate>Wed, 22 Jun 2022 15:48:18 +0000</pubDate><guid>https://spectrum.ieee.org/chip-scale-photonic-amplifier</guid><category>Optical fiber</category><category>Amplifiers</category><category>Optical communications</category><category>Wavelength division multiplexing</category><category>Photonic integrated circuit</category><category>Photonics chip</category><category>Photonics</category><dc:creator>Charles Q. Choi</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-series-of-disks-of-varying-sizes-on-a-flat-square-in-the-center-one-circle-glows-green-with-a-line-coming-from-top-and-from.jpg?id=30008472&amp;width=980"></media:content></item><item><title>The First High-Yield, Sub-Penny Plastic Processor</title><link>https://spectrum.ieee.org/plastic-microprocessor</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/at-left-a-circle-with-repeated-texture-at-right-a-textured-square-with-sections-of-different-sizes-in-different-colors-with-t.png?id=29971002&width=2000&height=1500&coordinates=583%2C0%2C583%2C0"/><br/><br/><p>
	For decades, hopeful techies have been promising a world where absolutely every object you encounter—bandages, bottles, bananas—will have some kind of smarts thanks to supercheap programmable plastic processors. If you’ve been wondering why that hasn’t happened yet, it’s that nobody has built working processors that can be made in the billions for less than a penny each.
</p><p>
	It hasn’t been for want of trying; in 2021 Arm reproduced its simplest <a href="https://www.nature.com/articles/s41586-021-03625-w" rel="noopener noreferrer" target="_blank">32-bit microcontroller, the M0, in plastic</a>, but even this couldn’t hope to meet the mark. The problem, according to engineers at the <a href="https://illinois.edu/" rel="noopener noreferrer" target="_blank">University of Illinois Urbana-Champaign</a> and at British flexible-electronics manufacture <a href="https://www.pragmaticsemi.com/" rel="noopener noreferrer" target="_blank">PragmatIC Semiconductor</a>, is that even the simplest industry-standard microcontrollers are too complex to make on plastic in bulk.
</p><hr/><p>
	In research to be presented at the <a href="https://iscaconf.org/isca2022/" rel="noopener noreferrer" target="_blank">International Symposium on Computer Architecture</a> later this month, the transatlantic team presents a simple yet fully functional plastic processor that could be made at sub-penny prices. The Illinois team designed 4-bit and 8-bit processors specifically to minimize size and maximize the percentage of working integrated circuits produced. Eighty-one percent of the 4-bit version worked, and that’s a good enough yield, says team leader <a href="https://ece.illinois.edu/about/directory/faculty/rakeshk" rel="noopener noreferrer" target="_blank">Rakesh Kumar,</a> to breach the one-penny barrier.
</p><p>
	“Flexible electronics has been niche for decades,” says Kumar. He adds that this yield study shows “that they may be ready for the mainstream.”
</p><p>
	The processors his team built were made using the flexible thin-film semiconductor indium gallium zinc oxide (IGZO), which can be built on plastic and continues to work even when bent around a radius of millimeters. But while a reliable manufacturing process is a prerequisite, it was the design that made the difference.
</p><h3>Why Not Silicon?</h3><br/><p>You might be wondering why silicon processors can’t do the job of supercheap flexible computing. Kumar’s analysis suggest it won’t work. Compared to plastic, silicon is expensive and inflexible, but if you make the chip small enough, the plastic can just bend around it. However, silicon fails at the task for two reasons: One is that although the area of circuitry could be made supersmall, you still need to leave a comparatively large amount of space around the edges so that the chip can be cut out of the wafer. In the case of a microcontroller as simple as the Flexicore, there would be more space around the edge than there is area containing circuitry. What’s more, you’ll need still more room to fit enough I/O pads so data and power can get to the chip. Suddenly, you’ve got a large area of costly blank silicon, pushing up expenses past the critical US $0.01 mark.</p><p>
	Instead of adapting an existing microcontroller architecture to plastic, Kumar’s team started from scratch to create a design called Flexicore. “Yield goes down very quickly as you increase gate count,” says Kumar. Knowing that, they came up with a design meant to minimize the number of gates needed. Using 4-bit and 8-bit logic instead of 16-bit or 32-bit helped. As did separating the memory that stores instructions from the memory that stores data. But they also cut down on the number and complexity of the instructions the processor is capable of executing.
</p><p>
	The team further simplified, by designing the processor so it executes an instruction in a single clock cycle instead of the multistep pipelines of today’s CPUs. Then they designed logic that implements those instructions by reusing parts, further reducing the gate count. “In general, we were able to simplify the design of FlexiCores by tailoring them to the needs of flexible applications, which tend to be computationally simple,” says Nathaniel Bleier, Kumar’s student.
</p><p>
	All of this resulted in a 5.6-square-millimeter 4-bit FlexiCore made up of just 2,104 semiconductor devices (about the same as the number of transistors in an Intel 4004 from 1971) versus some 56,340 devices for PlasticARM. “It’s an order of magnitude less than the tiniest silicon microcontrollers in terms of gate count,” he says. The team also developed an 8-bit version of FlexiCore, but it did not yield as well.
</p><p>
	“This is exactly the kind of design innovation needed to support truly ubiquitous electronics,” says <a href="https://www.pragmaticsemi.com/about/management" rel="noopener noreferrer" target="_blank">Scott White</a>, CEO of PragmatIC Semiconductor.
</p><p>
	With PragmatIC, the Illinois team produced plastic-coated wafers full of 4-bit and 8-bit processors and tested them at a variety of voltages on multiple programs and bent them without mercy. The experiment seems basic, but according to Kumar, it’s groundbreaking. Most research processors built using nonsilicon technologies yield so poorly that results are reported from one or at best a few working chips. “This is the first work, to the best of our knowledge, where anyone reported data from multiple chips for any nonsilicon technology,” he says.
</p><p>
	Not satisfied with this success, Kumar’s team came up with a design tool to explore architectural optimizations for different applications. For example, the tool showed that power consumption could be reduced considerably by allowing the gate count to inch up a bit.
</p><p>
	The chip industry has been targeted toward “the metrics of power and performance and to some degree reliability,” observed Kumar. “We haven’t focused on cost, conformality, and thinness. Focusing on those allows us to build new computer architectures and target new applications.”
</p><p>
	Flexible electronics pioneer <a href="http://rogersgroup.northwestern.edu/" rel="noopener noreferrer" target="_blank">John A. Rogers</a>, at Northwestern University, called the work “very impressive.” He looks forward to experimental studies of the effects of bending on circuit performance.
</p><p>
<em>This article appears in the August 2022 print issue as “Sub-Penny Plastic Processors.”</em>
</p>]]></description><pubDate>Tue, 14 Jun 2022 14:45:25 +0000</pubDate><guid>https://spectrum.ieee.org/plastic-microprocessor</guid><category>Plastic electronics</category><category>Plastic arm</category><category>Computer architecture</category><category>Microcontrollers</category><category>Microprocessor design</category><category>Microprocessors</category><category>Flexible electronics</category><dc:creator>Samuel K. Moore</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/at-left-a-circle-with-repeated-texture-at-right-a-textured-square-with-sections-of-different-sizes-in-different-colors-with-t.png?id=29971002&amp;width=980"></media:content></item><item><title>Improved Dynamic Range for Pulse Detection</title><link>https://go.teledynelecroy.com/pdrx_webinar_ieee</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/image.png?id=28133682&width=980"/><br/><br/><p>Join Teledyne SP Devices for an introduction to our Pulse Detection Range eXtension (PDRX) technology. It achieves a dynamic range equivalent to 16-bit analog-to-digital converters (ADCs) while exceeding the sampling rate supported by commercially available devices. It is ideal for pulse capture in applications such as mass spectrometry. <a href="https://go.teledynelecroy.com/pdrx_webinar_ieee" target="_blank">Register now for this free webinar!</a><br/></p><hr/><p><strong>Topics covered in this webinar:</strong></p><ul><li>PDRX principles and architecture</li><li>Real-time digital signal processing overview</li><li>Challenges and solutions associated with increased dynamic range</li><li>ADQ32-PDRX – the first digitizer based on PDRX with 2.5 GSPS sampling rate</li></ul><p><strong>Who should attend?</strong> Developers working with high-speed time-domain data acquisition where dynamic range is important.</p><p><strong>What attendees will learn?</strong> The underlying principles of the PDRX technology and how it can help improve the dynamic range. Challenges associated with previously undetected signals becoming uncovered – such as reflections caused by impedance mismatch.</p><p><strong>Presenter:</strong> Jan-Erik Eklund, Product Manager</p>]]></description><pubDate>Sat, 11 Jun 2022 12:00:00 +0000</pubDate><guid>https://go.teledynelecroy.com/pdrx_webinar_ieee</guid><category>Data acquisition</category><category>Signal processing</category><category>Spectrometry</category><category>Teledyne</category><category>Type:webinar</category><dc:creator>Teledyne</dc:creator><media:content medium="image" type="image/png" url="https://assets.rbl.ms/28133682/origin.png"></media:content></item><item><title>“PACMAN” Hack Breaks Apple M1’s Last Line of Defense</title><link>https://spectrum.ieee.org/pacman-hack-can-break-apple-m1s-last-line-of-defense</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/product-photograph-of-a-microprocessor.jpg?id=29961290&width=2000&height=1500&coordinates=0%2C0%2C0%2C0"/><br/><br/><p><a href="https://www.apple.com/newsroom/2020/11/apple-unleashes-m1/" target="_blank">Apple’s M1 processor</a> made a big splash on its November 2020 release, noteworthy for its eye-popping performance and miserly power consumption. But the value of its security may not be as obvious at first blush. A lack of serious attacks since its launch nearly two years ago indicates that its security systems, among them a last line of defense called pointer authentication codes, are working well. But its honeymoon period could possibly be coming to an end.<br/></p><p>At the<a href="https://iscaconf.org/isca2022/" target="_blank"> International Symposium on Computer Architecture</a> later this month, researchers led by MIT’s <a href="https://people.csail.mit.edu/mengjia/" target="_blank">Mengjia Yan</a> will present a mode of attack that so weakens the pointer authentication code (PAC) defense that the core of a computer’s operating system is made vulnerable. And because PACs may be incorporated in future processors built from the 64-bit Arm architecture, the vulnerability could  become more widespread. It’s possible that other processors are already using PACs, but the M1 was the only one available to Yan’s lab.</p><p>“What we found is actually quite fundamental,” says Yan. “It’s a class of attack. Not one bug.”</p><p class="pull-quote">How PACMAN picks the lock goes to the heart of modern computing. </p><p>The vulnerability, called PACMAN, assumes that there is already a software bug in operation on the computer that can read and write to different memory addresses. It then exploits a detail of the M1 hardware architecture to give the bug the power to execute code and possibly take over the operating system. “We assume the bug is there and we make it into a more serious bug,” says <a href="https://www.csail.mit.edu/person/joseph-ravichandran" target="_blank">Joseph Ravichandran</a> a student of Yan’s who worked on the exploit with fellow students Weon Taek Na and Jay Lang.</p><p>To understand how the attack works you have to get a handle on what pointer authentication is and how a detail of processor architecture called speculative execution works. Pointer authentication is a way to guard against software attacks that try to corrupt data that holds memory addresses, or pointers. For example, malicious code might execute a buffer overflow attack, writing more data than expected  into a part of memory, with the excess spilling over into a pointer’s address and overwriting it. That might then mean that instead of the computer’s software executing code stored at the original address, it is diverted to malware stored at the new one.</p><p>Pointer authentication appends a cryptographic signature to the end of the pointer. If there’s any malicious manipulation of the pointer, the signature will no longer match up with it. PACs are used to guard the core of the system’s operating system, the kernel. If an attacker got so far as to manipulate a kernel pointer, the mismatch between the pointer and its authentication code would produce what’s called an “exception,” and the system would crash, ending the malware’s attack. Malware would have to be extremely lucky to guess the right code, about 1 in 65,000.</p><p>PACMAN finds a way for malware to keep guessing over and over without any wrong guesses triggering a crash. How it does this goes to the heart of modern computing. For decades now, computers have been speeding up processing using what’s called speculative execution. In a typical program, which instruction should follow the next often depends on the outcome of the previous instruction (think if/then). Rather than wait around for the answer, modern CPUs will speculate—make an educated guess—about what comes next and start executing instructions along those lines. If the CPU guessed right, this speculative execution has saved a bunch of clock cycles. If it turns out to have guessed wrong, all the work is thrown out, and the processor begins along the correct sequence of instructions. Importantly, the mistakenly computed values are never visible to the software. There is no program you could write that would simply output the results of speculative execution.</p><p class="pull-quote">Initial solutions to PACMAN only tended to increase the processor’s overall vulnerability.</p><p>However, over the past several years, researchers have discovered ways to exploit speculative execution to do things like sneak data out of CPUs. These are called side-channel attacks, because they acquire data by observing indirect signals, such as how much time it takes to access data. <a href="https://meltdownattack.com/" target="_blank">Spectre and Meltdown</a>, are perhaps the best known of these side-channel attacks.</p><p>Yan’s group came up with a way to trick the CPU into guessing pointer authentication codes in speculation so an exception never arises, and the OS doesn’t crash. Of course, the answer is still invisible to software. But a side-channel trick involving stuffing a particular buffer with data and using timing to uncover which part the successful speculation replaces, provides the answer. [A similar concept is explained in more detail in “<a href="https://spectrum.ieee.org/how-the-spectre-and-meltdown-hacks-really-worked" target="_self">How the Spectre and Meltdown Hacks Really Worked</a>,” <em>IEEE Spectrum</em>, 28 February 2019.]</p><p><strong>With regard to PACMAN, </strong>Apple’s product team provided this response to Yan’s group:</p><p style="margin-left: 20px;">“We want to thank the researchers for their collaboration as this proof-of-concept advances our understanding of these techniques. Based on our analysis, as well as the details shared with us by the researchers, we have concluded this issue does not pose an immediate risk to our users and is insufficient to bypass device protections on its own.”</p><p>Other researchers familiar with PACMAN say that how dangerous it really is remains to be seen. However, PACMAN “increases the number of things we have to worry about when designing new security solutions,” says <a href="https://www.cs.ucr.edu/~nael/" target="_blank">Nael Abu-Ghazaleh</a>, chair of computer engineering at University of California, Riverside, and an expert in architecture security, including speculative execution attacks. Processors makers have been adding new security solutions to their designs besides pointer authentication in recent years. He suspects that now that PACMAN has been revealed, other research will begin to find speculative attacks against these new solutions.</p><p>Yan’s group explored some naive solutions to PACMAN, but they tended to increase the processor’s overall vulnerability. “It’s always an arms race,” says <a href="https://www.linkedin.com/in/krebello/" target="_blank">Keith Rebello</a>, the former program manager of DARPA’s <a href="https://spectrum.ieee.org/hack-our-hardware" target="_blank">System Security Integrated Through Hardware and firmware (SSITH) </a>program and currently a senior technical fellow at the Boeing Company. PACs are there “to make it much harder to exploit a system, and they have made it a lot harder. But is it the complete solution? No.” He’s hopeful that tools developed through SSITH, such as <a href="https://spectrum.ieee.org/morpheus-turns-a-cpu-into-a-rubiks-cube-to-defeat-hackers" target="_self">rapid re-encryption</a>, could help.</p><p>Abu-Ghazaleh credits Yan’s group with opening a door to a new aspect of processor security.</p><p>“People used to think software attacks were standalone and separate from hardware attacks,” says Yan. “We are trying to look at the intersection between the two threat models. Many other mitigation mechanisms exist that are not well studied under this new compounding threat model, so we consider the PACMAN attack as a starting point.”</p>]]></description><pubDate>Fri, 10 Jun 2022 11:00:01 +0000</pubDate><guid>https://spectrum.ieee.org/pacman-hack-can-break-apple-m1s-last-line-of-defense</guid><category>Apple</category><category>Processors</category><category>Security</category><category>Side channel attacks</category><category>Speculative execution</category><category>Spectre</category><category>Meltdown</category><category>Computer architecture</category><category>Hacking</category><dc:creator>Samuel K. Moore</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/product-photograph-of-a-microprocessor.jpg?id=29961290&amp;width=980"></media:content></item><item><title>Hands-on Electrochemical Impedance Spectroscopy</title><link>https://zhinst.zoom.us/webinar/register/4016533236301/WN_K7ag35DHRqSAVBm1dc7wqg</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/image.png?id=29943736&width=980"/><br/><br/>Would you like to start impedance measurements, but don't know how? Or do you already use impedance spectroscopy but would like to know more about theory and practice, while also learning some practical tips and tricks? <a href="https://zhinst.zoom.us/webinar/register/4016533236301/WN_K7ag35DHRqSAVBm1dc7wqg" rel="noopener noreferrer" target="_blank">Register for this free webinar now!</a><hr/><p><u></u><u></u></p><p>Zurich Instruments will introduce the basics of impedance spectroscopy and related analysis, and give practical hints to avoid common errors. This webinar will consist of a tutorial lecture followed by hands-on demonstrations showing how to configure a fault-free measurement and how to optimize your setup.<u></u><u></u></p>]]></description><pubDate>Mon, 06 Jun 2022 21:06:28 +0000</pubDate><guid>https://zhinst.zoom.us/webinar/register/4016533236301/WN_K7ag35DHRqSAVBm1dc7wqg</guid><category>Spectroscopy</category><category>Zurich instruments</category><category>Type:webinar</category><dc:creator>Zurich Instruments</dc:creator><media:content medium="image" type="image/png" url="https://assets.rbl.ms/29943736/origin.png"></media:content></item><item><title>Pulse Detection in the Presence of Noise</title><link>https://go.teledynelecroy.com/dbs_on_demand_webinar_ieee</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/image.png?id=28133682&width=980"/><br/><br/>Join Teledyne SP Devices for an introductory webinar providing technical insight and practical advice on how to improve triggered acquisition of weak signals in the presence of noise. <a href="https://go.teledynelecroy.com/dbs_on_demand_webinar_ieee" rel="noopener noreferrer" target="_blank">Register now for this free webinar!</a><hr/><p><strong>Topics covered in this webinar:</strong></p><ul><li>What is pattern noise and why may it cause false trigger events?</li><li>Why baseline drift caused by temperature variations can result in false readings</li><li>How noise analysis can help optimize the acquisition of weak signals</li><li>Overview of digital signal processing solutions that address these challenges</li></ul><p><strong>Who should attend?</strong> Engineers that want to learn more about noise sources and how to better optimize the acquisition of weak signals.</p><p><strong>What attendees will learn?</strong> The origin of pattern noise in time-interleaved analog-to-digital converters (ADCs), its impact on the idle-channel noise level, and the resulting risk of false trigger events and signal distortion. Characteristics of temperature-dependent baseline drift and challenges associated with that. How to determine noise distribution in order to set an appropriate trigger level.</p><p><strong>Presenter:</strong> Thomas Elter, Senior Field Applications Engineer</p>]]></description><pubDate>Sun, 29 May 2022 16:26:00 +0000</pubDate><guid>https://go.teledynelecroy.com/dbs_on_demand_webinar_ieee</guid><category>Noise</category><category>Signal processing</category><category>Teledyne</category><category>Type:webinar</category><dc:creator>Teledyne</dc:creator><media:content medium="image" type="image/png" url="https://assets.rbl.ms/28133682/origin.png"></media:content></item><item><title>Writing UVM/SystemVerilog Testbenches for Analog/Mixed-Signal Verification</title><link>https://us02web.zoom.us/webinar/register/6316533764571/WN_WM_pftAcT7GG3oOzt4iP_w</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/image.png?id=29862956&width=980"/><br/><br/>Learn how to write reusable SystemVerilog testbenches for analog/mixed-signal IPs, using standardized UVM components and Scientific Analog's XMODEL! <a href="https://us02web.zoom.us/webinar/register/6316533764571/WN_WM_pftAcT7GG3oOzt4iP_w" rel="noopener noreferrer nofollow" target="_blank">Register for this free webinar now!</a><hr/><p>Join this webinar on how to write a UVM testbench for analog/mixed-signal circuits. UVM (Universal Verification Methodology) is a framework of standardized SystemVerilog classes to build reusable and scalable testbenches for digital designs, and it can be extended to verifying analog circuits simply by using a fixture module that generates analog stimuli and measures analog responses with Scientific Analog's XMODEL.<br/></p><p>Using a digitally-programmable audio bandpass filter as an example, we'll show how to write a UVM testbench that measures the filter's transfer gain at randomly-chosen frequencies, collects the results in a scoreboard until the desired coverage is met, and checks the supply current and bias voltages during power-down with assertions. The <a href="https://us02web.zoom.us/webinar/register/6316533764571/WN_WM_pftAcT7GG3oOzt4iP_w" target="_blank">webinar</a> will start with an intuitive yet systematic introduction to UVM.</p><h3>Speaker</h3><p><span style="background-color: initial;"><strong>Charles Dančak<br/></strong></span>Verification Instructor and Consultant<br/>Charles Dančak is a trainer and consultant based in Silicon Valley. He holds two MS degrees, one in electrical engineering and one in solid-state physics. Charles began his career as a technology engineer in one of Intel's wafer fabs and spent ten years at Synopsys developing hands-on courses on HDL-based design, simulation, and DFT. He introduced the first SystemVerilog workshop in the University of California Extension system in 2007 and still teaches SystemVerilog online, currently with UC San Diego Extension (ECE-40301). Recently, Charles presented a paper on UVM for analog/mixed-signal verification at DVCon U.S. 2022.</p>]]></description><pubDate>Fri, 27 May 2022 15:35:05 +0000</pubDate><guid>https://us02web.zoom.us/webinar/register/6316533764571/WN_WM_pftAcT7GG3oOzt4iP_w</guid><category>Analog</category><category>Circuit design</category><category>Type:webinar</category><category>Verilog</category><dc:creator>Scientific Analog, Inc.</dc:creator><media:content medium="image" type="image/png" url="https://assets.rbl.ms/29862956/origin.png"></media:content></item><item><title>Hydrogen Helps Make Topological Insulators Practical</title><link>https://spectrum.ieee.org/topological-insulator-hydrogen</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/rendering-shows-a-clear-beaker-with-rendering-suspended-in-liquid-of-atomic-scale-model-featuring-red-and-white-connected-dots-f.jpg?id=29820287&width=2000&height=1500&coordinates=0%2C0%2C0%2C0"/><br/><br/><p>Future microchips that require far less energy than present-day devices may rely on exotic materials known as <a href="https://spectrum.ieee.org/a-beginners-guide-to-topological-materials" target="_blank">topological insulators</a>, in which electricity flows across only surfaces and edges, with virtually no dissipation of energy. However, it can prove tricky developing such materials for real-world applications. Now a new study reveals that simply incorporating hydrogen into topological insulators may control their electronic properties to help make them useful.<br/></p><p>Topology is the branch of mathematics that investigates what features of shapes may survive deformation. Material science has emerged in recent decades as an unexpected but compelling application of topology. The insights from topological models, scientists have discovered, help to understand and predict some materials’ unusual properties. These include electromagnetic effects <a href="https://artscimedia.case.edu/wp-content/uploads/sites/176/2014/05/15022516/Sammon_Mathur.pdf" target="_blank">beyond those explained by Maxwell’s Equations</a> as well as <a href="https://www.annualreviews.org/doi/abs/10.1146/annurev-conmatphys-031016-025458" target="_blank">quantum particles</a> that could yield <a href="https://physics.aps.org/articles/v10/63" target="_blank">new kinds of electronic and optical devices</a>.        </p><p>Using insights from topology, scientists also developed the first electronic <a href="https://www.sciencedirect.com/science/article/pii/S1631070513001461" target="_blank">topological insulators</a> in 2007. Electricity flowing along the edges or surfaces of these materials is “topologically protected,” meaning that the patterns in which the electrons flow will stay unchanged in the face of any disturbances they might encounter, a discovery that helped win the Nobel Prize in Physics in 2016.</p><p>Topological insulators could lead to advances such as <a href="https://spectrum.ieee.org/a-beginners-guide-to-topological-materials" target="_self">ultralow-energy transistors</a>. One major class of topological insulators are known as <a href="https://www.nist.gov/publications/chalcogenide-topological-insulators" target="_blank">chalcogenides</a>, which contain elements known as chalcogens such as selenium and tellurium.</p><p>To make sure topological insulators manifest topological protection on their surfaces, researchers need to make sure the interiors of these materials really are insulating and not conducting. However, naturally occurring defects “can donate electrical charges to the bulk of the material and contribute to unwanted conduction in the bulk,” says study senior author <a href="https://www.ccny.cuny.edu/profiles/lia-krusin-elbaum" target="_blank">Lia Krusin-Elbaum</a>, a physicist at the City College of New York.</p><p>One way to control conduction in the interiors of topological insulators is to make them very thin. However, there are limits to this method, as below a certain thickness, the topologically protected states on the surfaces of these materials vanish. Another strategy involves lacing these materials with a variety of elements, but this can reduce the operating temperature of these materials.</p><p>Now Krusin-Elbaum and her colleagues have developed what they say is a remarkably simple, efficient way to tune the conductivity of the interior of these materials—the insertion or extraction of hydrogen. The scientists detailed <u><a href="https://www.nature.com/articles/s41467-022-29957-3" rel="noopener noreferrer" target="_blank">their findings</a></u> 28 April in the journal <em>Nature Communications</em>.</p><p>In experiments, the scientists used hydrochloric acid diluted in water to help weave hydrogen into chalcogenide topological insulators. The hydrogen can bind with atoms of tellurium or selenium, donating electrons to adjust the material’s electronic properties. This method was fully reversible using a low amount of heating to remove the hydrogen.</p><p>This hydrogen-tuning strategy may relax thickness limits for chalcogenide topological insulators and results in materials that are stable at room temperature. “We were much surprised by the effect and by the efficiency of the hydrogenation as a tuning technique,” Krusin-Elbaum says.</p><p>Krusin-Elbaum suggests hydrogen tuning may help lead to new kinds of topological superconductors, many of which can generate mysterious particles on their surfaces known as Majorana fermions, long-theorized particles that are their own antiparticles. Majorana fermions could find use as quantum bits, or qubits, which lie at the heart of most quantum computers—machines that can theoretically perform more computations in an instant than there are atoms in the universe.</p><p>Qubits are typically fragile, but the Majorana fermions of topological superconductors could prove topologically protected against disturbances. This feature may help lead to “error-free quantum computing,” Krusin-Elbaum says.</p>]]></description><pubDate>Wed, 18 May 2022 16:08:24 +0000</pubDate><guid>https://spectrum.ieee.org/topological-insulator-hydrogen</guid><category>Quantum computing</category><category>Hydrogen</category><category>Chalcogenide</category><category>Majorana fermions</category><category>Topological insulators</category><dc:creator>Charles Q. Choi</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/rendering-shows-a-clear-beaker-with-rendering-suspended-in-liquid-of-atomic-scale-model-featuring-red-and-white-connected-dots-f.jpg?id=29820287&amp;width=980"></media:content></item></channel></rss>