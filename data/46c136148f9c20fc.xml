<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Daniel Forsyth]]></title><description><![CDATA[Daniel Forsyth]]></description><link>https://www.danielforsyth.me/</link><image><url>https://www.danielforsyth.me/favicon.png</url><title>Daniel Forsyth</title><link>https://www.danielforsyth.me/</link></image><generator>Ghost 4.48</generator><lastBuildDate>Wed, 02 Nov 2022 15:12:29 GMT</lastBuildDate><atom:link href="https://www.danielforsyth.me/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Looking at 2019 NFC East Offensive Spending vs. Production with nflfastR]]></title><description><![CDATA[<p>I have been following <a href="https://github.com/maksimhorowitz/nflscrapR">nflscrapR</a> and NFL analytics community on Twitter for a while now and digging into the package has long been an item on my to-do list. With a lot more free time on my hands due to the pandemic and the recent release of <a href="https://github.com/mrcaseb/nflfastR">nflfastR</a>, a new</p>]]></description><link>https://www.danielforsyth.me/looking-at-2019-nfc-east-spending-and-production-with-nfl/</link><guid isPermaLink="false">5efbd731ee06a20039ef71df</guid><dc:creator><![CDATA[Daniel Forsyth]]></dc:creator><pubDate>Wed, 08 Jul 2020 16:55:25 GMT</pubDate><content:encoded><![CDATA[<p>I have been following <a href="https://github.com/maksimhorowitz/nflscrapR">nflscrapR</a> and NFL analytics community on Twitter for a while now and digging into the package has long been an item on my to-do list. With a lot more free time on my hands due to the pandemic and the recent release of <a href="https://github.com/mrcaseb/nflfastR">nflfastR</a>, a new package which builds on nflscrapr, I decided it was time to dive in. </p><p>If you are not familiar nflscrapR, and now nflfastR, are packages that pull play-by-play data from the NFL API and then parse the text description of each play to give you a cleaned up, analysis ready dataset. Open source sports data is hard to come by so the package(s) and community around them have become quite popular. On top of the play-by-play data they also includes built in models such as Expected Points (EP) and Win Probability (WP). </p><p>If you are not familiar with EP or WP the original paper is available <a href="https://arxiv.org/abs/1802.00998">here</a>. To summarize:</p><h3 id="expected-points">Expected Points </h3><p>At its core EP looks at the current in-game situation and, using a model trained on historical play-by-play data, says based on the current in-game situation what do teams typically go on to do. To accomplish this EP uses a multinomial logistic regression model to estimate the expected points for a given situation. It takes the following input features:</p><ul><li>Current down</li><li>Number of seconds remaining in the half</li><li>Yards from the endzone</li><li>A log transform of yards to go for a first down</li><li>A binary indicator for whether or not it is a goal down situation</li><li>Another binary indicator for whether or not there is less than two minutes remaining in the half </li></ul><p>The model uses these features, which describe the current in-game drive situation, to estimate the probabilities of the response variable which in this scenario is every possible drive outcome:</p><ul><li>Touchdown (7)</li><li>Field Goal (3)</li><li>Safety (2)</li><li>No Score (0)</li><li>&#x2212;Touchdown (-7)</li><li>&#x2212;Field Goal (-3)</li><li>&#x2212;Safety (-2) </li></ul><p>Finally it takes these predicted probabilities and multiplies them by their respective point values to get the Expected Points of that particular situation. </p><h3 id="win-probability">Win Probability</h3><p>WP follows the same framework but instead of asking what do teams go on to do during a drive in this situation it asks how often does a team in this situation <em>win the game.</em> WP also uses a different model, a generalized additive model, as well as a different set of features to describe the game state:</p><ul><li>Expected score differential</li><li>Number of seconds remaining in game</li><li>Expected score time ratio</li><li>Current half of the game (1st, 2nd, or overtime)</li><li>Number of seconds remaining in half</li><li>Indicator for whether or not time remaining in half is<br>under two minutes</li><li>Time outs remaining for offensive (possession) team</li><li>Time outs remaining for defensive team</li></ul><h3 id="epa-and-wpa">EPA and WPA</h3><p>With both EP and WP the core value comes when you look at the difference in values between two plays. If you consider the current play Vi and the following play Vf you can place either EP or WP in and calculate Vf-Vi to determine the <em>Expected Points Added</em> (EPA) and <em>WPA</em> (Win Probability Added) of that particular play. As an Eagles fan I particularly enjoyed this example from the paper:</p><blockquote>During Super Bowl LII the Philadelphia Eagles&#x2019; Nick Foles received a touchdown when facing fourth down on their opponent&#x2019;s one yard line with thirty-eight seconds remaining in the half. At the start of the play the Eagles&#x2019; expected points was V i &#x2248; 2.78, thus resulting in EPA &#x2248; 7 &#x2212; 2.78 = 4.22. In an analogous calculation, this famous play known as the &#x201C;Philly special&#x201D; resulted in WPA &#x2248; 0.1266 as the Eagles&#x2019; increased their lead before the end of the half.</blockquote><!--kg-card-begin: html--><iframe src="https://giphy.com/embed/JPTn4ykXUDuvu" width="480" height="270" frameborder="0" class="giphy-embed" allowfullscreen></iframe><p><a href="https://giphy.com/gifs/special-philly-JPTn4ykXUDuvu">via GIPHY</a></p><!--kg-card-end: html--><h3 id="2019-nfc-east-offensive-spending">2019 NFC East Offensive Spending</h3><p>For my own analysis I wanted to look into the 2019 NFC East offensive spending and see how offensive production compared. I was able to pull salary data from <a href="https://www.spotrac.com/nfl">spotrac</a> to put together the below chart on spending by team and position. </p><!--kg-card-begin: html--><img alt="Image Description" src="https://www.danielforsyth.me/content/images/2020/07/Rplot-1.png" style="width: 667px; height:386px"><!--kg-card-end: html--><p>Some quick takeaways from this:</p><ul><li>Dak was on the last year of his rookie contract so the Cowboys were spending very little on quarterbacks.</li><li>On the other end of the spectrum Eli Manning was also on the final year of his deal but took home 23M or almost 12% of the entire Giants cap space.</li><li>All four teams in the division spent roughly the same amount on running backs.</li><li>Washington spent nearly double the rest of division on tight ends with Jordan Reed and Vernon Davis both on lucrative deals, both of whom ended up on the IR.</li><li>Dallas spent the most out of the group in wide receivers - and for good reason.</li></ul><h3 id="spending-vs-production">Spending vs. Production</h3><p>Next I wanted to see how this spending compared to the offensive output of each position. To do this I looked at the average EPA per play by position and compared it to the cap % spent. I should state that this approach is rather crude and does have its limitations:</p><ol><li>EPA is based on the outcome of the entire play which consists of 11 offensive players. Theoretically the credit should be divided among these players and not given solely to the one key offensive player involved. </li><li>You need to consider certain players contracts in the context of their career progression like Dak and Eli.</li></ol><p>That being said EPA can work well as a proxy measure and I thought this would be an interesting way to get my feet wet with NFL data. </p><!--kg-card-begin: html--><img alt="Image Description" src="https://www.danielforsyth.me/content/images/2020/07/qb_plot.png" style="width: 766px; height:491px"><!--kg-card-end: html--><!--kg-card-begin: html--><img alt="Image Description" src="https://www.danielforsyth.me/content/images/2020/07/rb_plot_1.png" style="width: 766px; height:491px"><!--kg-card-end: html--><!--kg-card-begin: html--><img alt="Image Description" src="https://www.danielforsyth.me/content/images/2020/07/wr_plot.png" style="width: 766px; height:491px"><!--kg-card-end: html--><!--kg-card-begin: html--><img alt="Image Description" src="https://www.danielforsyth.me/content/images/2020/07/te_plot.png" style="width: 766px; height:491px"><!--kg-card-end: html--><p>Takeaways:</p><p><em>Quarterbacks</em> - Its hard to gauge too much here due to Dak and Eli&apos;s situations but you can see just how valuable Dak was to the Cowboys and that they&apos;re going to have to empty the bank to secure him long term. </p><p><em>Running Backs</em> - All four teams are pretty tightly clustered here which is in line with the NFL analytics communities darling mantra that <em>running backs don&apos;t matter.</em></p><p><em>Wide Receivers</em> - Washington, New York and Philadelphia paid between five to ten percent of cap on their receivers and all got roughly the same output while the Cowboys paid a bit more and got significantly higher output from their star receiving core.</p><p><em>Tight Ends</em> - Washington spent more than twice the rest of the division on tight ends and ended up with an average EPA per play less the -.1. This was mostly due to bad fortune with Jordan Reed and Vernon Davis both suffering concussions and missing all or most of the season respectively. </p><p>While this analysis was interesting I think a logical next step with would be to factor in offensive line ratings for each team as you can imagine a good line will: give the quarterback more time in the pocket, create more room for running backs, and give tight ends and receivers more time to run their routes. This would alleviate some of the credit distribution issues without getting into more complex modeling. It would also be interesting to look more into individual players and their value as well as how this changes throughout their career.</p><p>If you have any questions, feedback, advice, or corrections please get in touch with me on <a href="https://twitter.com/Daniel_Forsyth1">Twitter</a> or email me - link on sidebar. Also wanted to give a shoutout to the developers of both nflscrapR and nflfastR for creating such useful tools that allow anyone to get involved in the NFL data world. </p>]]></content:encoded></item><item><title><![CDATA[2019 StockX Data Contest Entry]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>Ever since I was a kid I have loved sneakers and been interested in the impact they have on everything from sports to popular culture. So a few months ago when I saw that StockX was holding a data contest I was very excited to participate. If you are not</p>]]></description><link>https://www.danielforsyth.me/2019-stockx-data-contest-entry/</link><guid isPermaLink="false">5d263c31f2e666003856dafb</guid><dc:creator><![CDATA[Daniel Forsyth]]></dc:creator><pubDate>Wed, 10 Jul 2019 20:58:15 GMT</pubDate><content:encoded><![CDATA[<!--kg-card-begin: markdown--><p>Ever since I was a kid I have loved sneakers and been interested in the impact they have on everything from sports to popular culture. So a few months ago when I saw that StockX was holding a data contest I was very excited to participate. If you are not familiar with <a href="https://stockx.com/how-it-works">StockX</a> they label themselves as &apos;The Stock Market of Things&apos; and feature a live &#x2018;bid/ask&#x2019; marketplace for sneakers, clothes, bags, and watches. They are mostly known for their sneakers and have emerged as one of the premier players in the secondary sneaker market among competitors such as <a href="https://www.goat.com/">GOAT</a>, <a href="https://www.grailed.com/">Grailed</a>, and <a href="https://www.stadiumgoods.com/">Stadium Goods</a>.</p>
<p>The data given for the <a href="https://stockx.com/news/the-2019-data-contest/">contest</a> consisted of:</p>
<blockquote>
<p>A random sample of all Off-White x Nike and Yeezy 350 sales from between 9/1/2017 (the month that Off-White first debuted &#x201C;The Ten&#x201D; collection) and the present. There are 99,956 total sales in the data set; 27,794 Off-White sales, and 72,162 Yeezy sales. The sample consists of U.S. sales only. To create this sample, we took a random, fixed percentage of StockX sales (X%) for each colorway, on each day, since September 2017. So, for each day the Off-White Jordan 1 was on the market, we randomly selected X% of its sale from each day. (It&#x2019;s not important to know what X is; all that matters is that it&#x2019;s a random sample, and that the same fixed X% of sales was selected from every day, for every sneaker). We&#x2019;ve included 8 variables for you to work with: Order Date, Brand, Sneaker Name, Sale Price ($), Retail Price ($), Release Date, Shoe Size, and Buyer State (the U.S. state the buyer shipped to).</p>
</blockquote>
<p>with the entry rules being wide open:</p>
<blockquote>
<p>We give you a bunch of original StockX sneaker data, then you crunch the numbers and come up with the coolest, smartest, most compelling story you can tell. It can be literally anything you want. A theory, an insight, even just a really original data visualization. It could be a novel hypothesis about resale prices you&#x2019;ve always wanted to test. Or maybe it&#x2019;s just a beautiful chart to visualize the data. It can be on any subject &#x2013; sneakers, brands, buyers, or even StockX itself. Whatever you find interesting, just follow your bliss.</p>
</blockquote>
<p>When trying to think of interesting ideas for a submission I recalled a class I had taken a few years ago called &quot;Machine Learning for Trading&quot; and thought it would be the perfect fit to combine some techniques I learned  in the class to the data contest, it is StockX afterall.</p>
<p>When analyzing securities (stocks, bonds, derivatives, <em>sneakers</em>) there are two primary methods used. The first is fundamental analysis where you are attempting to determine the intrinsic value of the company by looking at key numbers and economic indicators from things like financial statements and industry conditions. The second is technical analysis where you attempt to forecast the direction of prices by looking at historical data. I wanted to apply technical analysis to sneaker data for my entry.</p>
<p>I used a tool from technical analysis called Bollinger Bands to develop optimal buy and sell signals for a particular sneaker, in this case the Jordan 1 Retro High Off-White Chicago. A new take on Michael Jordan&apos;s first signature shoe and one of the most sought after sneakers in the past few years (Currently selling for $3650 in my size on StockX).</p>
<p><img src="https://www.danielforsyth.me/content/images/2019/07/801781_01.jpg" alt="801781_01" loading="lazy"></p>
<p>Bollinger Bands are a form of technical analysis used to evaluate price action and volatility of a stock. To create Bollinger Bands you first plot the 20-day moving average of the stock price. Next you create two &#x201C;Bands&#x201D; that are two standard deviations above and below the moving average respectively. These bands are measures of volatility, when the movement of the price is volatile the bands will expand, and when the price becomes less volatile they contract. When the price crosses down, from outside the upper Band, to inside towards the moving average we use this as a trigger to sell. Conversely, when the price crosses up through the bottom Band towards the moving average this signals a buy. Bollinger Bands are effective because when the price exits either of the Bands that means it is outside 2 standard deviations of the moving average. Therefore we would expect that the price will eventually return back in the direction of the moving average. This method attempts to follow the classic mantra &#x201C;buy low and sell high&#x201D; but using Bollinger Bands instead of intuition.</p>
<p>Below are my plotted Bollinger Bands for the Jordan 1 Retro High Off-White Chicago sales data. You can see early on the price dropped pretty steadily and there are several sell signals. Then as time goes on and the price slowly rises the buy signals begin to come in. Around August 2018 the prices start to become more volatile, the sales less frequent, and the bands widen which probably signals the number of sneakers available is beginning to dry up which will slowly drive the price even higher.</p>
<p><img src="https://www.danielforsyth.me/content/images/2019/07/Screen-Shot-2019-02-26-at-3.09.34-PM.png" alt="Screen-Shot-2019-02-26-at-3.09.34-PM" loading="lazy"></p>
<p>I think something like this would be a cool feature to implement as a &apos;Good Time to Buy&apos; banner on a particular sneakers sale page - <a href="https://stockx.com/air-jordan-1-retro-high-off-white-chicago">here</a> is the page for the Jordan 1.</p>
<p>I did not win the contest but I had a lot of fun exploring the dataset and look forward to next years. There were a ton of excellent submissions - you can see the winners <a href="https://stockx.com/news/2019-data-contest-winner/">here</a>.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Mapping NYC Taxi Data]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p><em>This post was inspired by HN user eck&apos;s top comment seen <a href="https://news.ycombinator.com/item?id=10003118">here</a></em>.</p>
<p>Earlier this week the New York City Taxi &amp; Limousine Commission officially released yellow and green taxi trip record data for <a href="http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml">all of 2014 and up to June of 2015</a>. This includes millions of records that</p>]]></description><link>https://www.danielforsyth.me/mapping-nyc-taxi-data/</link><guid isPermaLink="false">5d24af011e693a0017f5e5fa</guid><dc:creator><![CDATA[Daniel Forsyth]]></dc:creator><pubDate>Fri, 07 Aug 2015 11:18:52 GMT</pubDate><content:encoded><![CDATA[<!--kg-card-begin: markdown--><p><em>This post was inspired by HN user eck&apos;s top comment seen <a href="https://news.ycombinator.com/item?id=10003118">here</a></em>.</p>
<p>Earlier this week the New York City Taxi &amp; Limousine Commission officially released yellow and green taxi trip record data for <a href="http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml">all of 2014 and up to June of 2015</a>. This includes millions of records that include pick-up and drop-off dates and times, pick-up and drop-off locations, trip distances, itemized fares, rate types, payment types, and driver-reported passenger counts. The data, which was previously only available through submission of a formal Freedom of Information Law (FOIL) request, is available in CSV format as well as from <a href="https://cloud.google.com/bigquery/">Google&apos;s BigQuery tools</a> [1].</p>
<p>After seeing the aforementioned comment and its accompanying visualization I wanted to have a go at replicating it in python. The first step was getting the data into pandas. In this situation it was much easier to query the data from GBQ than to download the individual CSV files. (Note that you must have the <code>google-api-python-client</code> installed and and be logged into <a href="https://cloud.google.com/bigquery/">GBQ</a> and create a new project for this to work properly). Pandas has an <a href="http://pandas.pydata.org/pandas-docs/stable/io.html#io-bigquery">included</a> <code>pandas.io.gbq</code> module that allows you to parse the results of a GBQ query into a dataframe very easily. The following query creates a dataframe that includes the latitude and longitude of all pickup locations in 2015, this ends up  being around 750,000 records.</p>
<pre><code>import pandas as pd

df=pd.io.gbq.read_gbq(&quot;&quot;&quot;
SELECT ROUND(pickup_latitude, 4) as lat, ROUND(pickup_longitude, 4) as long, COUNT(*) as num_pickups 
FROM [nyc-tlc:yellow.trips_2015] 
WHERE (pickup_latitude BETWEEN 40.61 AND 40.91) AND (pickup_longitude BETWEEN -74.06 AND -73.77 )
GROUP BY lat, long
&quot;&quot;&quot;, project_id=&apos;taxi-1029&apos;)
</code></pre>
<p>Once you have the dataframe ready the next step is to plot it. I chose to use <code>matplotlib</code> which always entails fiddling with a bunch of different parameters. I ended up using the following code which simply plots the longitude and latitude on a two dimensional scatter plot.</p>
<pre><code>import matplotlib
import matplotlib.pyplot as plt
#Inline Plotting for Ipython Notebook 
%matplotlib inline 

pd.options.display.mpl_style = &apos;default&apos; #Better Styling 
new_style = {&apos;grid&apos;: False} #Remove grid
matplotlib.rc(&apos;axes&apos;, **new_style)
from matplotlib import rcParams
rcParams[&apos;figure.figsize&apos;] = (17.5, 17) #Size of figure
rcParams[&apos;figure.dpi&apos;] = 250

P.set_axis_bgcolor(&apos;black&apos;) #Background Color

P=df.plot(kind=&apos;scatter&apos;, x=&apos;long&apos;, y=&apos;lat&apos;,color=&apos;white&apos;,xlim=(-74.06,-73.77),ylim=(40.61, 40.91),s=.02,alpha=.6)

</code></pre>
<p>And the Result:</p>
<p><img src="https://www.danielforsyth.me/content/images/2015/08/Screen-Shot-2015-08-06-at-3-02-05-PM.png" alt loading="lazy"></p>
<p>I was quite pleased with the result, it is interesting to toy with the different parameters such as the size of the dataframe, axis scales, size, color, and opacity of the individual points to see how it affects the overall appearance of the plot. It seems to me the only limit of what is possible with this dataset is ones imagination, it is going to be exciting to see what others put together, especially since the press release stated all data back to 2009 when the program started is going to be released in the coming weeks.</p>
<p>If you have any questions, feedback, advice, or corrections please get in touch with me on <a href="https://twitter.com/Daniel_Forsyth1">Twitter</a> or email me at <a href="mailto:danforsyth1@gmail.com">danforsyth1@gmail.com</a>.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[NBA Twitter, Emojis, and Word Embeddings]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>A few weeks ago I read this <a href="http://instagram-engineering.tumblr.com/post/117889701472/emojineering-part-1-machine-learning-for-emoji">blog post</a> from the Instagram engineering team on machine learning and emoji trends. The post talked about general emoji usage over time on Instagram and then used <a href="http://arxiv.org/pdf/1309.4168.pdf">word2vec</a> an algorithm that uses a unsupervised learning process to read through a corpus of text</p>]]></description><link>https://www.danielforsyth.me/nba-twitter-emojis-and-word-embeddings/</link><guid isPermaLink="false">5d24af011e693a0017f5e5f9</guid><dc:creator><![CDATA[Daniel Forsyth]]></dc:creator><pubDate>Wed, 17 Jun 2015 11:59:01 GMT</pubDate><content:encoded><![CDATA[<!--kg-card-begin: markdown--><p>A few weeks ago I read this <a href="http://instagram-engineering.tumblr.com/post/117889701472/emojineering-part-1-machine-learning-for-emoji">blog post</a> from the Instagram engineering team on machine learning and emoji trends. The post talked about general emoji usage over time on Instagram and then used <a href="http://arxiv.org/pdf/1309.4168.pdf">word2vec</a> an algorithm that uses a unsupervised learning process to read through a corpus of text and is then able to predict the context around a given word or emoji. The famous example of this word embedding method is vector[&apos;king&apos;] - vector[&apos;man] + vector[&apos;woman&apos;] = [&apos;queen&apos;]. When I first read this post the NBA playoffs had recently started so I decided I would collect tweets pertaining to the playoffs and then try and use the same techniques Instagram used on the tweet dataset.</p>
<p><img src="https://www.danielforsyth.me/content/images/2015/06/curry.jpg" alt loading="lazy"></p>
<p>I used the same process I used to <a href="https://www.danielforsyth.me/analyzing-a-nhl-playoff-game-with-twitter/">collect tweets about an NHL game</a> for this project however for the search terms I used the name of every starting player on each team that remained in the playoffs on May 11th. Over the last thirty six days I was able to collect <strong>8,520,786</strong> tweets. This was up until the beginning of game six of the finals. Once I had all of the tweets on my local machine I was able to start analyzing them.</p>
<p>The first thing I wanted to do was create a pandas dataframe of all of the tweets and then drop all duplicate tweets, retweets, and tweets that contained a URL.</p>
<pre><code>import pandas as pd
pd.options.display.max_colwidth  = 0
df = pd.read_csv(&apos;/Users/danielforsyth/desktop/tweets.csv&apos;, encoding=&apos;utf-8&apos;)

tweets = df[&apos;text&apos;].drop_duplicates().values.tolist()

tweets  = [x for x in tweets if not x.startswith(&apos;RT&apos;)]

tweets = [i for i in tweets if not (&apos;http://&apos; in i or &apos;https://&apos; in i)]
</code></pre>
<p>Now that I had a filtered list of tweets I wanted to find all of the emojis used in the tweets. First I used a regex pattern to capture all of the emojis and then created a pandas series to find the top 25 most used emojis.</p>
<pre><code>import re
try:
    # UCS-4
    e = re.compile(u&apos;[\U00010000-\U0010ffff]&apos;)
except re.error:
    # UCS-2
    e = re.compile(u&apos;[\uD800-\uDBFF][\uDC00-\uDFFF]&apos;)

emojis = []
for x in tweets:
    match  = e.search(x)
    if match:
        emojis.append(match.group())

dfe =  pd.DataFrame(emojis,columns=[&apos;text&apos;])
pd.Series(&apos; &apos;.join(dfe[&apos;text&apos;]).lower().split()).value_counts()[:25]
</code></pre>
<p><img src="https://www.danielforsyth.me/content/images/2015/06/Screen-Shot-2015-06-16-at-10-16-07-PM-1.png" alt loading="lazy"></p>
<p>The &apos;face with tears of joy&apos; emoji came in first place being used 67,884 times. This is interesting because this emoji was also the most used in Instagram&apos;s results of more than 50 millions comments and captions.</p>
<p>The final thing I wanted to do was create a word2vec model using the tweets and see what type of results I could find. I used the <a href="https://radimrehurek.com/gensim/index.html">genism library</a> which makes setting up a model pretty trivial.</p>
<pre><code>wt = [list(x.split()) for x in tweets]

import logging
logging.basicConfig(format=&apos;%(asctime)s : %(levelname)s : %(message)s&apos;,\
    level=logging.INFO)

# Set values for various parameters
num_features = 400    # Word vector dimensionality                      
min_word_count = 5   # Minimum word count                        
num_workers = 4       # Number of threads to run in parallel
context = 10          # Context window size                                                                                    
downsampling = 1e-3   # Downsample setting for frequent words

from gensim.models import word2vec
print &quot;Training model...&quot;
model = word2vec.Word2Vec(wt, workers=num_workers, \
            size=num_features, min_count = min_word_count, \
            window = context, sample = downsampling)

model.init_sims(replace=True)
</code></pre>
<p>After creating the model, which could take some time depending on your machine and the size of the dataset, you can start performing some various syntactic/semantic NLP word tasks.</p>
<pre><code>model.most_similar(&apos;&#x1F3C6;&apos;.decode(&apos;utf-8&apos;),topn=15)
</code></pre>
<p><img src="https://www.danielforsyth.me/content/images/2015/06/Screen-Shot-2015-06-16-at-10-35-14-PM.png" alt loading="lazy"></p>
<pre><code>model.most_similar(&apos;dunk&apos;,topn =15)
</code></pre>
<p><img src="https://www.danielforsyth.me/content/images/2015/06/Screen-Shot-2015-06-16-at-10-40-41-PM.png" alt loading="lazy"></p>
<p>You can see there is quite a few references to this Marreese Speights missed dunk:</p>
<iframe src="http://streamable.com/e/ttq9" width="560" height="311" frameborder="0" allowfullscreen webkitallowfullscreen mozallowfullscreen scrolling="no"></iframe>
<pre><code>model.most_similar(&apos;chef&apos;,topn=15)
</code></pre>
<p><img src="https://www.danielforsyth.me/content/images/2015/06/Screen-Shot-2015-06-16-at-10-50-53-PM.png" alt loading="lazy"></p>
<blockquote>
<p>&quot;Been cookin&apos; with the sauce, chef, curry with the pot, boy 360 with the wrist, boy.&quot; -Drake</p>
</blockquote>
<p>What I found to be the most interesting was what happened when I ran the <code>similarity</code> method with both MVP candidates and the word &apos;MVP&apos;.</p>
<pre><code>model.similarity(&apos;james&apos;, &apos;mvp&apos;)
</code></pre>
<p><img src="https://www.danielforsyth.me/content/images/2015/06/Screen-Shot-2015-06-16-at-10-48-36-PM-1.png" alt loading="lazy"></p>
<pre><code>model.similarity(&apos;curry&apos;, &apos;mvp&apos;)
</code></pre>
<p><img src="https://www.danielforsyth.me/content/images/2015/06/Screen-Shot-2015-06-16-at-10-49-33-PM.png" alt loading="lazy"></p>
<p>It looks like twitter believes Stephen Curry is more deserving of the finals MVP trophy.</p>
<p>It was very interesting using word2vec for the first time as it was quite easy to setup and get results compared to other NLP techniques. In the future I would like to spend some more time working with word2vec particularly with a different and larger dataset.</p>
<p>If you have any questions, feedback, advice, or corrections please get in touch with me on <a href="https://twitter.com/Daniel_Forsyth1">Twitter</a> or email me at <a href="mailto:danforsyth1@gmail.com">danforsyth1@gmail.com</a>.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Pandas  &  Burritos - Analyzing Chipotle Order Data]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p><img src="https://www.danielforsyth.me/content/images/2015/04/pp-1.jpg" alt loading="lazy"></p>
<p>A few months back the New York Times ran an article titled <a href="http://www.nytimes.com/interactive/2015/02/17/upshot/what-do-people-actually-order-at-chipotle.html?abt=0002&amp;abg=0">&quot;At Chipotle, How Many Calories Do People Really Eat?&quot;</a> which took a look at the average amount of calories a typical order at Chipotle contained. They found that:</p>
<blockquote>
<p>The typical order at Chipotle has about 1,</p></blockquote>]]></description><link>https://www.danielforsyth.me/pandas-burritos-analyzing-chipotle-order-data-2/</link><guid isPermaLink="false">5d24af011e693a0017f5e5f8</guid><dc:creator><![CDATA[Daniel Forsyth]]></dc:creator><pubDate>Fri, 24 Apr 2015 11:02:20 GMT</pubDate><content:encoded><![CDATA[<!--kg-card-begin: markdown--><p><img src="https://www.danielforsyth.me/content/images/2015/04/pp-1.jpg" alt loading="lazy"></p>
<p>A few months back the New York Times ran an article titled <a href="http://www.nytimes.com/interactive/2015/02/17/upshot/what-do-people-actually-order-at-chipotle.html?abt=0002&amp;abg=0">&quot;At Chipotle, How Many Calories Do People Really Eat?&quot;</a> which took a look at the average amount of calories a typical order at Chipotle contained. They found that:</p>
<blockquote>
<p>The typical order at Chipotle has about 1,070 calories. That&#x2019;s more than half of the calories that most adults are supposed to eat in an entire day. The recommended range for most adults is between 1,600 and 2,400.</p>
</blockquote>
<p>Very surprising, Chipotle isn&apos;t the healthiest place to eat. What was more interesting to me was the fact that the  NYT released the data they used for the article on their <a href="https://github.com/TheUpshot/chipotle">Github page</a>. The article states that &quot;The data is based on about 3,000 meals in about 1,800 Grubhub orders from July to December 2012, almost all from two Chipotle restaurants: one in Washington, D.C., and another in East Lansing, Mich.&quot;</p>
<p>I had been meaning to mess around with this order file for a while but had forgotten about it. That was until today when today I saw Chipotle had <a href="http://bits.blogs.nytimes.com/2015/04/23/chipotle-signs-exclusive-deal-with-food-delivery-start-up-postmates/">announced</a> it will be offering delivery in select cities through a SF based delivery startup.</p>
<p>After starting up a new IPython notebook I loaded the TSV file into a pandas dataframe and ran the .head() method to make sure everything looked okay.</p>
<pre><code>import pandas as pd
df = pd.read_table(&apos;/Users/danielforsyth/Desktop/orders.tsv&apos;)
df.head()
</code></pre>
<p><img src="https://www.danielforsyth.me/content/images/2015/04/Screen-Shot-2015-04-23-at-8-55-04-PM.png" alt="df.head" loading="lazy"></p>
<p>The First thing I wanted to look at was what items were the most popular.</p>
<pre><code>items  = df.item_name.value_counts().plot(kind=&apos;bar&apos;)
</code></pre>
<p><img src="https://www.danielforsyth.me/content/images/2015/04/Screen-Shot-2015-04-23-at-9-01-03-PM.png" alt loading="lazy"></p>
<p>You can see that the chicken bowl dominates all of the items but there&apos;s obviously way too much going on here. How about just the top ten most popular items.</p>
<pre><code>items  = df.item_name.value_counts()[:10].plot(kind=&apos;bar&apos;)
</code></pre>
<p><img src="https://www.danielforsyth.me/content/images/2015/04/Screen-Shot-2015-04-23-at-9-02-43-PM.png" alt loading="lazy"></p>
<p>That&apos;s much easier to read and you can clearly see the chicken bowl and chicken burrito are the most popular items at the locations included in the data.</p>
<p>Next I wanted to look at the prices of the orders. This was a little trickier as the &apos;item price&apos; column contained string values and multiple items could be part of one order. The first step was to strip the $ sign from the string and then convert it to a float. Then you can just group by the order_id and add the sum method on the end.</p>
<pre><code>df[&apos;item_price&apos;] = df[&apos;item_price&apos;].str.replace(&apos;$&apos;,&apos;&apos;)
df[&apos;item_price&apos;] = df[&apos;item_price&apos;].astype(float)

orders = df.groupby(&apos;order_id&apos;).sum()
orders.head()
orders[&apos;item_price&apos;].describe()
</code></pre>
<p><img src="https://www.danielforsyth.me/content/images/2015/04/Screen-Shot-2015-04-23-at-9-24-57-PM-1.png" alt loading="lazy"></p>
<p><img src="https://www.danielforsyth.me/content/images/2015/04/Screen-Shot-2015-04-23-at-9-25-07-PM.png" alt loading="lazy"></p>
<p>From this we are able to see that 1834 meals were ordered with a total of 4622 different items. The mean price of a meal is $18 with the minimum being right around $10 (Obligatory mention of the term <a href="http://www.economist.com/news/business/21638120-why-slightly-more-upmarket-outlets-are-eating-fast-foods-lunch-better-burgers-choicer-chicken">fast casual</a>). Someone was also able to rack up a $205 dollar bill.</p>
<p>The last type of information I was interested in finding was the variety of different meals people ordered. The &apos;choice description&apos; column describes the item type. For example what kind of soda or what toppings on a burrito bowl.</p>
<pre><code>descriptions = df.groupby([&quot;item_name&quot;, &quot;choice_description&quot;])[&quot;order_id&quot;].count().reset_index(name=&quot;count&quot;)

</code></pre>
<p>Using this new dataframe we can easily see the most popular versions of each menu item. For example the different kinds of the most ordered item, the chicken bowl.</p>
<pre><code>descriptions = descriptions[descriptions[&apos;item_name&apos;].str.contains(&quot;Chicken Bowl&quot;)]
descriptions.sort([&apos;count&apos;], ascending=False)[:10]
</code></pre>
<p>There were actually 348 different varieties of chicken bowls ordered but I limited it to the top ten which you can see here.</p>
<p><img src="https://www.danielforsyth.me/content/images/2015/04/Screen-Shot-2015-04-23-at-10-09-48-PM.png" alt loading="lazy"></p>
<p>How about the most popular drinks?</p>
<pre><code>descriptions = descriptions[descriptions[&apos;item_name&apos;].str.contains(&quot;Canned Soda&quot;)]
descriptions.sort([&apos;count&apos;], ascending=False)
</code></pre>
<p><img src="https://www.danielforsyth.me/content/images/2015/04/Screen-Shot-2015-04-23-at-10-11-55-PM.png" alt loading="lazy"></p>
<p>It was very interesting playing around with this dataset, I would be curious to see the results if something like this were to be done at a much larger scale, to see things such as ordering trends in different stores and areas of the country. I am sure this data is very useful to Chipotle to see what items are doing well on top of optimizing inventory control on a location to location basis.</p>
<p>If you have any questions, feedback, advice, or corrections please get in touch with me on <a href="https://twitter.com/Daniel_Forsyth1">Twitter</a> or email me at <a href="mailto:danforsyth1@gmail.com">danforsyth1@gmail.com</a>.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Exploring NBA Data with Python]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>After a long weekend of NBA All-Star game festivities I stumbled upon <a href="http://www.gregreda.com/2015/02/15/web-scraping-finding-the-api/">Greg Reda&apos;s excellent blog post about web scraping</a> on Twitter. In it he goes over how to find and use API&apos;s to scrape data from webpages. The example he uses is the NBA&apos;</p>]]></description><link>https://www.danielforsyth.me/exploring_nba_data_in_python/</link><guid isPermaLink="false">5d24af011e693a0017f5e5f7</guid><category><![CDATA[Python]]></category><category><![CDATA[Pandas]]></category><category><![CDATA[SportsVu]]></category><dc:creator><![CDATA[Daniel Forsyth]]></dc:creator><pubDate>Wed, 18 Feb 2015 05:38:42 GMT</pubDate><content:encoded><![CDATA[<!--kg-card-begin: markdown--><p>After a long weekend of NBA All-Star game festivities I stumbled upon <a href="http://www.gregreda.com/2015/02/15/web-scraping-finding-the-api/">Greg Reda&apos;s excellent blog post about web scraping</a> on Twitter. In it he goes over how to find and use API&apos;s to scrape data from webpages. The example he uses is the NBA&apos;s very own stats website, which to my surprise provides a lot of very interesting data. I decided to dig a little deeper and see what I could find.</p>
<p><img src="https://31.media.tumblr.com/e78ff845b3838af04b145bbe105d6cb8/tumblr_njug3cyss71rqkpceo1_500.gif" alt="alt" loading="lazy"></p>
<p>The shot log API from NBA.com returns data about every shot a player took during a game. These data points include how much time was left in the game when the shot was taken, time on the shot clock when the shot was taken, dribbles taken before the shot, and even the closest defender when the shot was taken. The information I found the most interesting and focused on collecting were the distance the shot was taken from, the distance of the closest defender, the number of dribbles taken before the shot was taken, and the amount of time the player possessed the ball before shooting. The API takes a player ID and returns all of the data for each shot in every game this season unless specified otherwise. So my first step was creating a dictionary of all the players I wanted to collect data on. I chose to use every player who has played in at least seventy percent of his team&apos;s games as this is the minimum the NBA uses to qualify players as a scoring leader [1]. Below is an example of the Washington Wizard&apos;s dictionary with the player name as the key and the players id as the value, I wont show every team for the sake of space.</p>
<pre><code>teams ={&apos;wizards&apos;:{
                   &apos;garrett temple&apos;:&apos;202066&apos;,
                   &apos;andre miller&apos;:&apos;1889&apos;,
                   &apos;kevin seraphin&apos;:&apos;202338&apos;,
                   &apos;otto porter&apos;:&apos;203490&apos;,
                   &apos;rasual butler&apos;:&apos;2446&apos;,
                   &apos;kris humphries&apos;:&apos;2743&apos;,
                   &apos;nene hilario&apos;:&apos;2403&apos;,
                   &apos;paul pierce&apos;:&apos;1718&apos;,
                   &apos;marcin gortat&apos;:&apos;101162&apos;,
                   &apos;bradley beal&apos;:&apos;203078&apos;,
                   &apos;john wall&apos;:&apos;202322&apos;
                   }
</code></pre>
<p>After having all of the player ID&apos;s ready I wrote a function to take each players ID, get the data from the API, find the players averages for the year and then add all of this into a dictionary for each players.</p>
<pre><code>import requests
import json
import pandas as pd

players = []
player_stats = {&apos;name&apos;:None,&apos;avg_dribbles&apos;:None,&apos;avg_touch_time&apos;:None,&apos;avg_shot_distance&apos;:None,&apos;avg_defender_distance&apos;:None}

def find_stats(name,player_id):
    #NBA Stats API using selected player ID
    url = &apos;http://stats.nba.com/stats/playerdashptshotlog?&apos;+ \
    &apos;DateFrom=&amp;DateTo=&amp;GameSegment=&amp;LastNGames=0&amp;LeagueID=00&amp;&apos; + \
    &apos;Location=&amp;Month=0&amp;OpponentTeamID=0&amp;Outcome=&amp;Period=0&amp;&apos; + \
    &apos;PlayerID=&apos;+player_id+&apos;&amp;Season=2014-15&amp;SeasonSegment=&amp;&apos; + \
    &apos;SeasonType=Regular+Season&amp;TeamID=0&amp;VsConference=&amp;VsDivision=&apos;
    
    #Create Dict based on JSON response
    response = requests.get(url)
    shots = response.json()[&apos;resultSets&apos;][0][&apos;rowSet&apos;]
    data = json.loads(response.text)
    
    #Create df from data and find averages 
    headers = data[&apos;resultSets&apos;][0][&apos;headers&apos;]
    shot_data = data[&apos;resultSets&apos;][0][&apos;rowSet&apos;]
    df = pd.DataFrame(shot_data,columns=headers) 
    avg_def = df[&apos;CLOSE_DEF_DIST&apos;].mean(axis=1)
    avg_dribbles = df[&apos;DRIBBLES&apos;].mean(axis=1)
    avg_shot_distance = df[&apos;SHOT_DIST&apos;].mean(axis=1)
    avg_touch_time = df[&apos;TOUCH_TIME&apos;].mean(axis=1)
     
    #add Averages to dictionary then to list
    player_stats[&apos;name&apos;] = name
    player_stats[&apos;avg_defender_distance&apos;]=avg_def
    player_stats[&apos;avg_shot_distance&apos;] = avg_shot_distance
    player_stats[&apos;avg_touch_time&apos;] = avg_touch_time
    player_stats[&apos;avg_dribbles&apos;] = avg_dribbles
    players.append(player_stats.copy())
</code></pre>
<p>The next step was getting all of the data. I did this by iterating over every player in the teams dictionary and calling the <code>find_stats</code> function.</p>
<pre><code>for x in teams:
    for y in teams[x]:
        find_stats(y,teams[x][y])
</code></pre>
<p>Now that I had all of the data in place I created a pandas dataframe to make sorting through everything much easier.</p>
<pre><code>cols = [&apos;name&apos;,&apos;avg_defender_distance&apos;,&apos;avg_dribbles&apos;,&apos;avg_shot_distance&apos;,&apos;avg_touch_time&apos;]
df = pd.DataFrame(players,columns = cols)
</code></pre>
<p>Sanity Check</p>
<pre><code>df.head()
</code></pre>
<p><img src="https://www.danielforsyth.me/content/images/2015/02/Screen-Shot-2015-02-17-at-10-41-06-PM.png" alt="alt" loading="lazy"></p>
<p>With everything in place it was time to start answering some questions with the data. The first thing I was interested in finding was which player had the largest average defender distance when they shot.</p>
<pre><code>defender_distance = df[[&apos;name&apos;,&apos;avg_defender_distance&apos;]]
defender_distance.sort(&apos;avg_defender_distance&apos;,ascending=False)
</code></pre>
<p><img src="https://www.danielforsyth.me/content/images/2015/02/Screen-Shot-2015-02-17-at-11-49-50-PM.png" alt="alt" loading="lazy"></p>
<p>As you can see Mike Miller of the Cavaliers has the highest average distance of just over six feet. Defenders are basically daring Miller to take open shots, which makes sense given his .33 field goal percentage which is less than half that of league leader DeAndre Jordan who&apos;s .725 field goal percentage has defenders guarding him from an average of just under three feet away.</p>
<pre><code>defender_distance.loc[df[&apos;name&apos;] == &apos;deandre jordan&apos;]
</code></pre>
<p><img src="https://www.danielforsyth.me/content/images/2015/02/Screen-Shot-2015-02-18-at-12-09-15-AM.png" alt="alt" loading="lazy"></p>
<p>The next question I was interested in answering was which player is shooting from distance the most?</p>
<pre><code>shot_distance = df[[&apos;name&apos;,&apos;avg_shot_distance&apos;]]
shot_distance.sort(&apos;avg_shot_distance&apos;,ascending=False)
</code></pre>
<p><img src="https://www.danielforsyth.me/content/images/2015/02/Screen-Shot-2015-02-18-at-12-43-17-AM.png" alt="alt" loading="lazy"></p>
<p>Mike Miller is back again with an average shot distance of twenty three and a half feet, which makes his average shot a three-pointer.</p>
<p>The last question I was interested in answering was who was holding the ball the longest before they shot? I used the average dribbles before the shot and average possession time before the shot for this.</p>
<pre><code>dribbles = df[[&apos;name&apos;,&apos;avg_dribbles&apos;]]
dribbles.sort(&apos;avg_dribbles&apos;,ascending=False)
avg_time = df[[&apos;name&apos;,&apos;avg_touch_time&apos;]]
avg_time.sort(&apos;avg_touch_time&apos;,ascending=False)
</code></pre>
<p><img src="https://www.danielforsyth.me/content/images/2015/02/Untitled-3.jpg" alt="alt" loading="lazy"></p>
<p>As you can see average dribbles before shot and time of possession before shot are very closely correlated, with each dribble taking about one second. Almost all of the players on both lists are also point guards which makes sense because they&apos;re going to have the ball in their hand the most out of anyone on the court.</p>
<p>Looking at all of this data has been very interesting and there is still a lot left to explore. Some other things I would like to look into are which players are taking the big shots, shots taken with only a few seconds left on the play clock, and also take a look at these types of data points on a team level.</p>
<p>If you have any questions, feedback, advice, or corrections please get in touch with me on <a href="https://twitter.com/Daniel_Forsyth1">Twitter</a> or email me at <a href="mailto:danforsyth1@gmail.com">danforsyth1@gmail.com</a>.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Catching the bus to class with Python]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>For the past month I have been studying abroad in London. One of the first things I noticed when arriving here was how much better the public transportation system was compared to what I am used to in Philadelphia. Whether you are taking the tube or the bus it is</p>]]></description><link>https://www.danielforsyth.me/catching-the-bus-to-class-with-python/</link><guid isPermaLink="false">5d24af011e693a0017f5e5f6</guid><category><![CDATA[Python]]></category><category><![CDATA[Flask]]></category><category><![CDATA[TFL]]></category><dc:creator><![CDATA[Daniel Forsyth]]></dc:creator><pubDate>Tue, 07 Oct 2014 20:26:00 GMT</pubDate><content:encoded><![CDATA[<!--kg-card-begin: markdown--><p>For the past month I have been studying abroad in London. One of the first things I noticed when arriving here was how much better the public transportation system was compared to what I am used to in Philadelphia. Whether you are taking the tube or the bus it is clean, quick, and the easiest way to get around the city. It took a few weeks to become fully acclimated with all the stops and routes but when everything starts connecting it becomes very easy to get around.</p>
<p>Depending on traffic my commute to class is about a fifteen-minute bus ride from my dorm. A few days ago while waiting for the bus in the morning and looking at the LED board at the stop which displays the upcoming buses and when they are due to arrive I began to wonder where this information was coming from. Surely enough after some searching I came across the <a href="https://www.tfl.gov.uk/info-for/open-data-users/our-feeds">Transport for London API</a>. The API has quite a few feeds for the buses, tube, and even the Barclays bike share program. After some reading I set out to optimize my morning commute to class.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/10/FullSizeRender.jpg" alt loading="lazy"></p>
<p>After some fiddling around I was able to get a JSON response that included all of the data sent to the bus stop outside my building. The board at this stop has been offline quite a few times the past week so it was nice to see that its a problem with the board and not the API.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/10/Screen-Shot-2014-10-07-at-8-04-34-PM.png" alt loading="lazy"></p>
<p>Now that I had access to all of the data I needed to narrow it down to just the bus I was interested in taking to class in the morning. This is where python comes in, I wrote a quick function that takes in the JSON from the API and then parses out just the bus I take to class.</p>
<pre><code>
import requests

def getTime():
    r = requests.get(&apos;http://countdown.tfl.gov.uk/stopBoard/58984&apos;)
    json_result = r.json()
    all_stops = json_result[&apos;arrivals&apos;]
    my_stops = []
    for x in all_stops:
        if x[&apos;isRealTime&apos;]== True and x[&apos;destination&apos;] == &apos;White City&apos;:
            x = str(x[&apos;routeName&apos;]) + &apos; &apos; +           str(x[&apos;destination&apos;]) + &apos; &apos; + str(x[&apos;estimatedWait&apos;])
            my_stops.append(x)
    print my_stops
</code></pre>
<p><img src="https://www.danielforsyth.me/content/images/2014/10/Screen-Shot-2014-10-07-at-8-13-19-PM.png" alt loading="lazy"></p>
<p>It worked perfectly and only took ten lines of code. I decided to take the idea a step further and wrap it in a quick flask application. I came up with this single page app that displays all of the upcoming buses.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/10/Screen-Shot-2014-10-07-at-10-14-15-PM.png" alt loading="lazy"></p>
<p>For now it is still just local on my laptop but has been working well saving me some time waiting outside in the morning. This was a quick and fun learning experience that helped me learn quite a bit more about flask applications.</p>
<p>If you have any questions, feedback, advice, or corrections please get in touch with me on <a href="https://twitter.com/Daniel_Forsyth1">Twitter</a> or email me at <a href="mailto:danforsyth1@gmail.com">danforsyth1@gmail.com</a>.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[A quick look at the World Cup final through Instagram]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>The 2014 World Cup will go down as one of the best in recent history. It featured countless headlines, from Germany asserting themselves as the top team in the world by humiliating the entire nation of Brazil 7-1 on their own turf and eventually winning it all. To the end</p>]]></description><link>https://www.danielforsyth.me/instagram-and-the-world-cup/</link><guid isPermaLink="false">5d24af011e693a0017f5e5f5</guid><category><![CDATA[Python]]></category><category><![CDATA[Instagram]]></category><dc:creator><![CDATA[Daniel Forsyth]]></dc:creator><pubDate>Mon, 14 Jul 2014 22:55:05 GMT</pubDate><content:encoded><![CDATA[<!--kg-card-begin: markdown--><p>The 2014 World Cup will go down as one of the best in recent history. It featured countless headlines, from Germany asserting themselves as the top team in the world by humiliating the entire nation of Brazil 7-1 on their own turf and eventually winning it all. To the end of Spanish world soccer dominance and Luis Suarez continuing with his animalistic tendencies. And lets not forget James Rodriguez making sure he&#x2019;s a household name with this gem:<br>
<img src="https://www.danielforsyth.me/content/images/2014/Jul/YearlyFabulousCaiman.gif" alt loading="lazy"></p>
<p>I had wanted to do a project using the Instagram API for a while and thought the world cup final would be great place to start. I hacked together a quick python script to search the Instagram API for a specific term and save the images into a local directory.</p>
<pre><code>import os
import json
import urllib
import urllib2
from instagram.client import InstagramAPI


INSTAGRAM_CLIENT_ID = &apos;&apos;
INSTAGRAM_CLIENT_SECRET = &apos;&apos;

api = InstagramAPI(client_id=INSTAGRAM_CLIENT_ID,client_secret=INSTAGRAM_CLIENT_SECRET)

def find_images(query, path):
    recent = api.tag_recent_media(1,100,query)
    link = recent[1]
    response = urllib2.urlopen(link)
    ig = json.load(response) 
    status=ig[&apos;meta&apos;][&apos;code&apos;]
    
    while status == 200:
        data = ig[&apos;data&apos;]
        if not data:
            link = ig[&apos;pagination&apos;][&apos;next_url&apos;]
            response = urllib2.urlopen(link)
            ig = json.load(response)
        else:
            image = ig[&apos;data&apos;][0][&apos;images&apos;][&apos;standard_resolution&apos;][&apos;url&apos;]
            unique = str(ig[&apos;data&apos;][0][&apos;created_time&apos;])
            str(image)
            urllib.urlretrieve(image, os.path.join(path, &apos;%s.jpg&apos;) % unique )
            link = ig[&apos;pagination&apos;][&apos;next_url&apos;]
            response = urllib2.urlopen(link)
            ig = json.load(response)
</code></pre>
<p>After you enter your credentials and run the function you will see your directory begin to fill with images. I ran the script for a few minutes (you will get rate limited pretty quickly) during the game and was able to gather around six hundred images. I created a quick collage with a few below. As you can see it consists of mostly TVs, Messi and German flags.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Jul/ig.jpg" alt loading="lazy"></p>
<p>This was a quick and easy project which gave me quite a few ideas for the future. It would be interesting to go through each image individually and analyze them for say most prominent colors. It would also be neat to build a machine learning pipeline to autotag the pictures into categories such as people, landscapes, cars, etc. I would also like to take a look at the real time streaming API and build some sort of web front end on top of it.</p>
<p>In the end I learned quite a bit with this and look forward to using Instagrams API again. If you have any questions, feedback, advice, or corrections please get in touch with me on <a href="https://twitter.com/Daniel_Forsyth1">Twitter</a> or email me at <a href="mailto:danforsyth1@gmail.com">danforsyth1@gmail.com</a>.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Breakfast under Bill - A look at my morning on the front page of Hacker News]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>Tuesday night I wrote a short blog post about how I used <a href="https://www.danielforsyth.me/finding-the-best-ticket-price-simple-web-scraping-with-python/">python to find cheap tickets to a music festival</a>. I finished up pretty late so I decided to post it online the next morning. I woke up pretty early and posted the article on a few websites around</p>]]></description><link>https://www.danielforsyth.me/breakfast-under-bill-a-look-at-my-morning-on-the-front-page-of-hacker-news/</link><guid isPermaLink="false">5d24af011e693a0017f5e5f4</guid><category><![CDATA[Python]]></category><category><![CDATA[Pandas]]></category><category><![CDATA[Data]]></category><dc:creator><![CDATA[Daniel Forsyth]]></dc:creator><pubDate>Thu, 19 Jun 2014 02:29:15 GMT</pubDate><content:encoded><![CDATA[<!--kg-card-begin: markdown--><p>Tuesday night I wrote a short blog post about how I used <a href="https://www.danielforsyth.me/finding-the-best-ticket-price-simple-web-scraping-with-python/">python to find cheap tickets to a music festival</a>. I finished up pretty late so I decided to post it online the next morning. I woke up pretty early and posted the article on a few websites around seven. I started watching my google analytics page and the hits started coming in very fast, much faster than normal. First it was twenty, then thirty, and shortly after fifty people were reading within minutes of submitting. I looked at the map and most of the hits were from Europe. I must have lunch hour crowd. I navigated over to Hacker News planning to check the new section and see if I had gotten any upvotes and to my surprise I saw my article at number five on the front page. I couldn&apos;t believe my eyes and refreshed and it had already moved up a spot. At its peak it hit the number two spot, I couldn&apos;t believe short article on something pretty trivial had made the front page. It spent most of the morning fluctuating around the fifth spot, a good portion of the time under Bill Gates blog post on <a href=" http://www.gatesnotes.com/Development/Building-Better-Bananas">developing genetically modified bananas</a>.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Jun/Screen-Shot-2014-06-18-at-11-22-56-PM.png" alt loading="lazy"></p>
<p>By the end of the day my blog dashboard said I had hit fourteen thousand hits. However google analytics had a somewhat smaller number. I am not sure what caused this difference but I decided to export the data and take a look at what had happened throughout the day. The first thing I took a look at was views throughout the day.</p>
<pre><code>import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline

views = pd.read_csv(&apos;/Users/danielforsyth/Desktop/views_tues.csv&apos;)
views[&apos;hour&apos;] = pd.to_datetime(pd.Series(views[&apos;hour&apos;]))
views.set_index(&apos;hour&apos;, drop=False, inplace=True)

pd.options.display.mpl_style = &apos;default&apos;
from matplotlib import rcParams
rcParams[&apos;figure.figsize&apos;] = (18, 6)
rcParams[&apos;figure.dpi&apos;] = 150

views.plot(colormap = &apos;Spectral&apos;)</code></pre>
<div class="text-center">
![](/content/images/2014/Jun/Screen-Shot-2014-06-19-at-12-09-30-AM.png)
</div>
<div class="text-center">
![](/content/images/2014/Jun/Screen-Shot-2014-06-18-at-11-30-57-PM.png)
</div>
<p>As you can see the views increase dramatically early on and then after peaking around two thousand at nine slowly drop as the post moved further down the page.  To get an idea of how much traffic I recieved compared to normal I created a graph of all traffic since I created my blog.</p>
<pre><code>total = pd.read_csv(&apos;/Users/danielforsyth/Desktop/views_all.csv&apos;)
total[&apos;Day&apos;] = pd.to_datetime(pd.Series(total[&apos;Day&apos;]))
total.set_index(&apos;Day&apos;, drop=False, inplace=True)
total.plot()</code></pre>
<div class="text-center">
![](/content/images/2014/Jun/Screen-Shot-2014-06-18-at-11-49-21-PM.png)
</div>
<p>The spikes occur on the days I had posted articles. The most views I had gotten in a single day previously was two thousand, the same number I got within an hour while on the front page. Lastly I took a quick look at where people were viewing from and what browser they were using.</p>
<p>There were views from one hundred and fourteen countries. I listed a few of them below with a bar chart of the top ten, I did the same for browsers.</p>
<pre><code>countries = pd.read_csv(&apos;/Users/danielforsyth/Desktop/countries.csv&apos;)
countries.set_index(&apos;Country&apos;, drop=False, inplace=True)</code></pre>
<div class="text-center">
![](/content/images/2014/Jun/Screen-Shot-2014-06-18-at-11-55-20-PM.png)
</div>
<pre><code>top10 = countries.head(10)
top10.plot(kind=&apos;bar&apos;, rot=90)</code></pre>
<div class="text-center">
![](/content/images/2014/Jun/Screen-Shot-2014-06-18-at-11-56-36-PM.png)
</div>
<pre><code>browser = pd.read_csv(&apos;/Users/danielforsyth/Desktop/browsers.csv&apos;)
browser.set_index(&apos;Browser&apos;, drop=False, inplace=True)</code></pre>
<div class="text-center">
![](/content/images/2014/Jun/Screen-Shot-2014-06-18-at-11-58-38-PM.png)
</div>
<pre><code>btop10 = browser.head(10)
btop10.plot(kind=&apos;bar&apos;, rot=90)</code></pre>
<div class="text-center">
![](/content/images/2014/Jun/Screen-Shot-2014-06-18-at-11-59-39-PM.png)
</div>
<p>As you can see making the front page of HN provided an insane amount of traffic. I believe I mostly got lucky by posting at the right time but I recieved a lot of great feedback and messages which provided great motivation to keep working on cool things. If you have any questions, feedback, advice, or corrections please get in touch with me on <a href="https://twitter.com/Daniel_Forsyth1">Twitter</a> or email me at <a href="mailto:danforsyth1@gmail.com">danforsyth1@gmail.com</a>.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Finding the Best Ticket Price - Simple Web Scraping with Python]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>One of my favorite parts of the summer is attending music festivals. Most festivals offer &quot;early bird&quot; tickets for a significantly lower price than general admission, however they typically sell out well before the actual event. Whether it is laziness, lack of money, or just plain stupidity I</p>]]></description><link>https://www.danielforsyth.me/finding-the-best-ticket-price-simple-web-scraping-with-python/</link><guid isPermaLink="false">5d24af011e693a0017f5e5f3</guid><category><![CDATA[Python]]></category><category><![CDATA[Web Scraping]]></category><dc:creator><![CDATA[Daniel Forsyth]]></dc:creator><pubDate>Tue, 17 Jun 2014 01:51:09 GMT</pubDate><content:encoded><![CDATA[<!--kg-card-begin: markdown--><p>One of my favorite parts of the summer is attending music festivals. Most festivals offer &quot;early bird&quot; tickets for a significantly lower price than general admission, however they typically sell out well before the actual event. Whether it is laziness, lack of money, or just plain stupidity I never seem to purchase these early bird tickets on time and have to look to different options. In recent years I have found success using Craigslist last minute, around a week before the festival, and getting tickets around or even lower than the early bird/pre sale prices. This year instead of sitting on craigslist day after day refreshing I decided to try and automate the process.<br>
<br></p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Jun/90417_1680195.jpg" alt loading="lazy"></p>
<br>
<p>After looking at the structure of the Craigslist results page and messing around with <a href="http://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a> I came up with the following script.</p>
<br>
<div class="text-center">
![](/content/images/2014/Jun/Screen-Shot-2014-06-16-at-10-09-24-PM.png)
</div>
<br>
<pre><code>import requests
from bs4 import BeautifulSoup
from urlparse import urljoin

URL = &apos;http://philadelphia.craigslist.org/search/sss?sort=date&amp;query=firefly%20tickets&apos;
BASE = &apos;http://philadelphia.craigslist.org/cpg/&apos;

response = requests.get(URL)

soup = BeautifulSoup(response.content)
for listing in soup.find_all(&apos;p&apos;,{&apos;class&apos;:&apos;row&apos;}):
    if listing.find(&apos;span&apos;,{&apos;class&apos;:&apos;price&apos;}) != None:
        price = listing.text[2:6]
        price = int(price)
        if price <=250 and price> 100:
        	print listing.text
    		link_end = listing.a[&apos;href&apos;]
    		url = urljoin(BASE, link_end)
    		print url
    		print &quot;\n&quot;</=250></code></pre>
   <br>
<p>Requests is used to get all of the data from the webpage and then beautiful soup parses out everything I was interested in. Once the script is run it returns the most recently posted tickets between $100 and $250 with the price, listing title, location and link.</p>
 <br>
<div class="text-center">
![](/content/images/2014/Jun/Screen-Shot-2014-06-16-at-10-25-16-PM.png)
</div>
   <br>
<p>Using this script in conjunction with something like cron or osx&apos;s launchd you can have the script run a few times a day and the output emailed to you.</p>
<p>In the future I think it would be interesting to keep track of third party tickets sales as the event approaches on websites like ebay, stubhub, and craigslist and see when the best time to buy is. Something similar to this <a href="http://www.cheapair.com/blog/travel-tips/when-to-ignore-our-advice-and-book-your-flight-as-early-as-possible/">study</a> on when to book a flight. Also a web app that allows you to search all three at the same time could prove interesting.</p>
<p>Any feedback is appreciated, you can reach me on <a href="https://twitter.com/Daniel_Forsyth1">Twitter</a> or email me at <a href="mailto:danforsyth1@gmail.com">danforsyth1@gmail.com</a>.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Analyzing a NHL Playoff Game With Twitter]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>Since its inception eight years ago Jack Dorsey&apos;s Twitter has grown into one of the most popular websites on the internet. With over a billion registered users and an average of five hundred million tweets sent per day Twitter is creating incredible amounts of data. Many novel ideas</p>]]></description><link>https://www.danielforsyth.me/analyzing-a-nhl-playoff-game-with-twitter/</link><guid isPermaLink="false">5d24af011e693a0017f5e5f2</guid><category><![CDATA[Python]]></category><category><![CDATA[Pandas]]></category><category><![CDATA[Sports]]></category><category><![CDATA[Twitter]]></category><dc:creator><![CDATA[Daniel Forsyth]]></dc:creator><pubDate>Thu, 24 Apr 2014 20:52:55 GMT</pubDate><content:encoded><![CDATA[<!--kg-card-begin: markdown--><p>Since its inception eight years ago Jack Dorsey&apos;s Twitter has grown into one of the most popular websites on the internet. With over a billion registered users and an average of five hundred million tweets sent per day Twitter is creating incredible amounts of data. Many novel ideas have come out of the question of what to do with all of this data, from <a href="http://www.fastcoexist.com/1681873/twitter-can-predict-the-stock-market-if-youre-reading-the-right-tweets">predicting stocks</a> to more recently <a href="http://phys.org/news/2014-04-twitter-crime.html">predicting crime</a>.</p>
<p>When I was first learning python one of the first libraries I came across was <a href="https://github.com/tweepy/tweepy">Tweepy</a>, an open source, easy to use python library for accessing the Twitter API. There are three different ways to get access to Twitters enormous amounts of data. First is Twitters search API which allows you to gather tweets which were made in the past. You can collect tweets based on user, keywords, locations, its basically the same as when you are searching for tweets on Twitters website except you are limited to how much you can collect. Twitters streaming API allows you to compile tweets as they are happening in real time. You can use the same type parameters as you can with the search  API looking for tweets based on keywords or even geographic locations. The disadvantage of the streaming API is that you are not collecting all of the Tweets that are being sent in real time only a sample of ~1% of the current traffic. The only way to get the full stream of all tweets being sent that match your criteria is the Twitter Firehose. The only way to get access to the Firehose is to go through a third party such as <a href="http://datasift.com/">Datasift</a> or the recently acquired by twitter, <a href="http://gnip.com/">GNIP</a>.</p>
<p>Early on when I was experimenting with Tweepy I began thinking of interesting projects that could come out of all this data I had the the potential of collecting. One idea that always stuck was the thought of collecting tweets during sports games and seeing what could be done with the resulting data. Being a Philadelphia Flyers fan I chose to use Twitters streaming API to collect tweets sent during their last playoff game against the New York Rangers (SPOILER ALERT: they lost).</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Apr/591764.jpg" alt loading="lazy"></p>
<p>The following code takes all tweets with the keyword &apos;flyers&apos; and sends the time it was created, text of the tweet, location (if available), and the source of the tweet to a local MongoDB database. I have removed my consumer and secret keys, you can obtain your own by creating an app on <a href="https://dev.twitter.com/ ">twitters dev site</a>.</p>
<pre><code>import tweepy
import sys
import pymongo

consumer_key=&quot;&quot;
consumer_secret=&quot;&quot;

access_token=&quot;&quot;
access_token_secret=&quot;&quot;

auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)
api = tweepy.API(auth)

class CustomStreamListener(tweepy.StreamListener):
    def __init__(self, api):
        self.api = api
        super(tweepy.StreamListener, self).__init__()

        self.db = pymongo.MongoClient().Flyers

    def on_status(self, status):
        print status.text , &quot;\n&quot;

        data ={}
        data[&apos;text&apos;] = status.text
        data[&apos;created_at&apos;] = status.created_at
        data[&apos;geo&apos;] = status.geo
        data[&apos;source&apos;] = status.source
      
        self.db.Tweets.insert(data)

    def on_error(self, status_code):
        print &gt;&gt; sys.stderr, &apos;Encountered error with status code:&apos;, status_code
        return True # Don&apos;t kill the stream

    def on_timeout(self):
        print &gt;&gt; sys.stderr, &apos;Timeout...&apos;
        return True # Don&apos;t kill the stream

sapi = tweepy.streaming.Stream(auth, CustomStreamListener(api))
sapi.filter(track=[&apos;flyers&apos;])</code></pre>
<p>Once you run the script you will see the tweets appear on the terminal window and the MongoDB begin to fill. I use the Robomongo GUI to keep track of this. I ran the script fifteen minutes prior to the beginning of the game and ended it fifteen minutes after for the sake of consistency. By the end I had collected 35,443 tweets. For a little bit of context I was collecting around 7,000 tweets for regular season Flyers games and was able to gather 640,000 during the Super Bowl.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Apr/Screen-Shot-2014-04-24-at-6-04-40-PM.png" alt loading="lazy"></p>
<p>Once I had all of the data collected I exported a CSV of everything and began looking at it in iPython. The code below creates a pandas dataframe from the CSV file, makes the created_at column into the index and then converts it to a pandas time series. I also converted the time to EST 12 hour format for graph readability.</p>
<pre><code>import pandas as pd
from pandas.tseries.resample import TimeGrouper
from pandas.tseries.offsets import DateOffset
flyers = pd.read_csv(&apos;/Users/danielforsyth/Desktop/PHI_NYR_G3.csv&apos;)
flyers[&apos;created_at&apos;] = pd.to_datetime(pd.Series(flyers[&apos;created_at&apos;]))
flyers.set_index(&apos;created_at&apos;, drop=False, inplace=True)
flyers.index = flyers.index.tz_localize(&apos;GMT&apos;).tz_convert(&apos;EST&apos;)
flyers.index = flyers.index - DateOffset(hours = 12)
flyers.index
</code></pre>
<p><img src="https://www.danielforsyth.me/content/images/2014/Apr/Screen-Shot-2014-04-24-at-6-15-25-PM.png" alt loading="lazy"></p>
<p>Next I took a quick look at everything using the head and desrcibe methods built into pandas.</p>
<pre><code>flyers.head()</code></pre>
<p><img src="https://www.danielforsyth.me/content/images/2014/Apr/Screen-Shot-2014-04-24-at-6-17-07-PM.png" alt loading="lazy"></p>
<pre><code>flyers.describe()</code></pre>
<p><img src="https://www.danielforsyth.me/content/images/2014/Apr/Screen-Shot-2014-04-24-at-6-18-12-PM.png" alt loading="lazy"></p>
<p>Now it was time to get the data ready to graph. One quick line and the created_at timeseries is in a per minute minute format.</p>
<pre><code>flyers1m = flyers[&apos;created_at&apos;].resample(&apos;1t&apos;, how=&apos;count&apos;)
flyers1m.head()</code></pre>
<p><img src="https://www.danielforsyth.me/content/images/2014/Apr/Screen-Shot-2014-04-24-at-6-24-51-PM.png" alt loading="lazy"></p>
<p>You can also quickly find the average amount of tweets per minute. Which in this case was 187.</p>
<pre><code>avg = flyers1m.mean()</code></pre>
<p>Now that I had all of the data formatted properly I imported Vincent and created a graph.</p>
<pre><code>import vincent
vincent.core.initialize_notebook()
area = vincent.Area(flyers1m)
area.colors(brew=&apos;Spectral&apos;)
area.display()</code></pre>
<p><img src="https://www.danielforsyth.me/content/images/2014/Apr/Screen-Shot-2014-04-24-at-6-27-22-PM.png" alt loading="lazy"></p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Apr/new.jpg" alt loading="lazy"></p>
<p>As you can see because the search term used here was &apos;flyers&apos; the results are very biased towards them. The two highest peaks in tweet volume are during the first Flyers goal (700 tweets per minute) and the final Rangers goal by ex-Flyer Dan Carcillo (938 tweets per minute), who doesn&apos;t have an exceptionally great reputation. There are also two large peaks at the beginning and end of the game.</p>
<p>After I had graphed all of the data I turned to looking at the actual tweets themself. For this process I used <a href="http://www.nltk.org/">NLTK</a> which is a natural language processing library for python. The first steps were to import NLTK, create a list of all the text from the tweets and then create a new list after filtering out all of the stop words (high frequency words that are irrelevant i.e. to, the, also).</p>
<pre><code>import nltk
from nltk.corpus import stopwords
from nltk import FreqDist
stop = stopwords.words(&apos;english&apos;)
text = flyers[&apos;text&apos;]

tokens = []
for txt in text.values:
    tokens.extend([t.lower().strip(&quot;:,.&quot;) for t in txt.split()])
    
filtered_tokens = [w for w in tokens if not w in stop]</code></pre>
<p>After we have a list of filtered words we are able to look at the frequency ditribution of the words. Below are the 50 most used words and a frequency plot of the 25 most used words.</p>
<pre><code>freq_dist = nltk.FreqDist(filtered_tokens)

freq_dist</code></pre>
<p><img src="https://www.danielforsyth.me/content/images/2014/Apr/Screen-Shot-2014-04-24-at-9-00-37-PM.png" alt loading="lazy"></p>
<pre><code>freq_dist.keys()[:50]</code></pre>
<p><img src="https://www.danielforsyth.me/content/images/2014/Apr/Screen-Shot-2014-04-24-at-9-01-27-PM.png" alt loading="lazy"><br>
<img src="https://www.danielforsyth.me/content/images/2014/Apr/Screen-Shot-2014-04-24-at-9-03-08-PM.png" alt loading="lazy"></p>
<pre><code>freq_dist.plot(25)</code></pre>
<p><img src="https://www.danielforsyth.me/content/images/2014/Apr/Screen-Shot-2014-04-24-at-9-04-24-PM.png" alt loading="lazy"></p>
<p>Next I took a look at the source data that was collected with each tweet. Below are the top fifteen sources, as you can see iPhone users dominate the sample with four times as many tweets as both android and web users.</p>
<pre><code>flyers.source.value_counts()</code></pre>
<p><img src="https://www.danielforsyth.me/content/images/2014/Apr/Screen-Shot-2014-04-24-at-9-10-17-PM.png" alt loading="lazy"></p>
<p>The last piece of data I had to look at was the location. Since having location data attatched with your tweets is optional not every tweet includes this information. Out of the 35,443 tweets I had collected only 1,609 had location data. Below is a map of all the tweets I created using <a href="https://github.com/wrobstory/folium">Folium</a></p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Apr/Screen-Shot-2014-04-24-at-9-19-01-PM.png" alt loading="lazy"></p>
<p>You can see almost all of the tweets are concentrated in the Philadelphia area but there are also some areas in both Florida and California with some pretty heavy tweeting.</p>
<p>In the future I would like to try putting scripts on EC2 instances and pushing all of the tweets to something like Heroku to automate the process and keep from having everything local. I would also like to try searching for the opposing team simultaneously to see how the results differ from one another. Implementing some D3 visualizations is also somthing I would like to try as well as building a real time web application to watch the data come in as the game is being played.</p>
<p>This was a very interesting project with some pretty cool results especially considering I was only using around one percent of all the tweets being sent during the game. If you have any questions, feedback, advice, or corrections please get in touch with me on <a href="https://twitter.com/Daniel_Forsyth1">Twitter</a> or email me at <a href="mailto:danforsyth1@gmail.com">danforsyth1@gmail.com</a>.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Is Basketball A Weakest Link Sport?]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>I recently finished <a href="http://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&amp;field-keywords=the%20numebrs%20game">The Numbers Game: Why Everything You Know About Soccer Is Wrong</a> by Chris Anderson and David Sally, the book describes recent innovations in soccer analytics. In it Anderson and Sally talk about how they believe soccer is on the verge of its &quot;Moneyball&quot; moment, as</p>]]></description><link>https://www.danielforsyth.me/is-basketball-a-weakest-link-sport/</link><guid isPermaLink="false">5d24af011e693a0017f5e5f1</guid><category><![CDATA[Python]]></category><category><![CDATA[Pandas]]></category><category><![CDATA[Sports]]></category><dc:creator><![CDATA[Daniel Forsyth]]></dc:creator><pubDate>Thu, 10 Apr 2014 17:31:07 GMT</pubDate><content:encoded><![CDATA[<!--kg-card-begin: markdown--><p>I recently finished <a href="http://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&amp;field-keywords=the%20numebrs%20game">The Numbers Game: Why Everything You Know About Soccer Is Wrong</a> by Chris Anderson and David Sally, the book describes recent innovations in soccer analytics. In it Anderson and Sally talk about how they believe soccer is on the verge of its &quot;Moneyball&quot; moment, as more and more clubs are employing analysts to run the numbers trying to find that extra edge on the competition just as Bill James did for the Oakland A&apos;s over a decade ago.</p>
<p>One of the most intriguing takeaways I found in The Numbers Game was a chapter where the authors described soccer as a &quot;weakest link sport&quot;. They go on to say :</p>
<blockquote>
<p>&quot;If you want to build a team for success, you need to look less at your strongest links and more at your weakest ones. It is there that a teams destiny is determined, whether it will go down in history or be forevrer considered a failure.&quot;</p>
</blockquote>
<p>In the same chapter they made another point I found compelling. &quot;Soccer is vastly different from basketball, the most superstar driven sport&quot;. I thought this topic would be interesting to explore so I set out looking for some data.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Apr/durant-2.jpg" alt loading="lazy"></p>
<p>After acquiring some data from ESPN and basketball-reference.com I started putting it together and got to work.</p>
<p>The first thing I did was import Pandas and two csv&apos;s I had downloaded. One was all of the starters from every NBA team and their respective team rank. The other included every NBA players player efficiency rating(PER), a stat that combines everything a player does on the court into one number.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Apr/Screen-Shot-2014-04-08-at-4-32-17-PM.png" alt loading="lazy"></p>
<p>Next I tested to make sure everything was imported properly using pandas built in head method.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Apr/Screen-Shot-2014-04-08-at-4-33-40-PM.png" alt loading="lazy"></p>
<p>Players must play 6.09 minutes per game to be eligible for a PER rating so next I eliminated all players whose MPG fell under 6.09 minutes and then combined the starters dataframe with the PER dataframe so all of the data I was interested in was together.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Apr/Screen-Shot-2014-04-08-at-4-37-23-PM.png" alt loading="lazy"><br>
<img src="https://www.danielforsyth.me/content/images/2014/Apr/Screen-Shot-2014-04-08-at-4-38-24-PM-1.png" alt loading="lazy"></p>
<p>I needed a number to compare the weakest players on each team to so I found the mean PER of all starters.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Apr/Screen-Shot-2014-04-08-at-4-41-13-PM.png" alt loading="lazy"></p>
<p>As well as some other statistics on starters PER.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Apr/Screen-Shot-2014-04-08-at-4-41-58-PM.png" alt loading="lazy"></p>
<p>Next I created a new dataframe that included a column of the difference between the mean PER and the PER of the weakest player on each teams starting five.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Apr/Screen-Shot-2014-04-08-at-4-42-53-PM.png" alt loading="lazy"><br>
<img src="https://www.danielforsyth.me/content/images/2014/Apr/Screen-Shot-2014-04-08-at-4-44-10-PM.png" alt loading="lazy"></p>
<p>My last step in pandas involved creating a dataframe that included every team, their rank, and their PER gap.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Apr/Screen-Shot-2014-04-08-at-4-47-22-PM.png" alt loading="lazy"></p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Apr/Screen-Shot-2014-04-08-at-4-48-02-PM.png" alt loading="lazy"><br>
<img src="https://www.danielforsyth.me/content/images/2014/Apr/Screen-Shot-2014-04-08-at-4-48-47-PM.png" alt loading="lazy"></p>
<p>Now that I had all of my data formatted properly it was time to starts visualizing it.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Apr/Screen-Shot-2014-04-08-at-4-51-16-PM.png" alt loading="lazy"></p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Apr/skillgap.jpg" alt loading="lazy"><br>
<img src="https://www.danielforsyth.me/content/images/2014/Apr/Screen-Shot-2014-04-10-at-2-31-59-PM.png" alt loading="lazy"></p>
<p>As you can see there is a, albeit weak, correlation between the skill gap and team rank with some strong outliers. The four most prominent outliers are the Miami Heat, Oklahoma City Thunder, LA Clippers, Memphis Grizzlies and Sacremento Kings. These outliers can be explained pretty easily. The Thunder have Kevin Durant, rated first overall in PER with a score of 30.17 (Michael Jordans career PER was 27.91), as well as Russel Westbrook who comes in eighth for PER. The Miami Heat, of course, have the big three of Lebron James, Dwayne Wade, and Chris Bosh. Los Angelos has Chris Paul and Blake Griffin, both in the top ten for PER and excellent compliments to each other. Sacremento has Demarcaus Cousins ranked 5th in PER and the Grizzlies while not having a starter in the top 25 for PER have Zach Randolph and Marc Gasol two very imposing big men.</p>
<p>Once the outliers are removed the r value rises from a weak .117 to a substantially  higher .594.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Apr/Screen-Shot-2014-04-10-at-2-28-33-PM.png" alt loading="lazy"></p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Apr/skillgap2.jpg" alt loading="lazy"></p>
<p>You can see that Anderson and Sally were right, having a top player in the NBA is extremely important, especially when you have a weak starter. Having one, or two, heavy hitters can easily make up for having a large divide between the average NBA starter and the worst player on your starting five. I have only scraped the surface here and in the end Ithink I may have raised more questions than I have answered but it was a good learning experience and brought out some interesting points.</p>
<p>If you have any questions, feedback, advice, or corrections please get in touch on <a href="https://twitter.com/Daniel_Forsyth1">Twitter</a> or email me at <a href="mailto:danforsyth1@gmail.com">danforsyth1@gmail.com</a>.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Analyzing Philadelphia's Bike Thefts]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>While mulling over project ideas and browsing Twitter earlier today I ran across <a href="http://opendataphilly.org/">http://opendataphilly.org/</a>. Per their website OpenDataPhilly &quot;is a portal that provides access to over 175 data sets, applications, and APIs related to the Philadelphia region. Simply accessing data, however, is not the ultimate goal of</p>]]></description><link>https://www.danielforsyth.me/analyzing-philadelphias-bike-thefts/</link><guid isPermaLink="false">5d24af011e693a0017f5e5f0</guid><category><![CDATA[Python]]></category><category><![CDATA[Pandas]]></category><category><![CDATA[Data]]></category><dc:creator><![CDATA[Daniel Forsyth]]></dc:creator><pubDate>Fri, 28 Mar 2014 13:16:12 GMT</pubDate><content:encoded><![CDATA[<!--kg-card-begin: markdown--><p>While mulling over project ideas and browsing Twitter earlier today I ran across <a href="http://opendataphilly.org/">http://opendataphilly.org/</a>. Per their website OpenDataPhilly &quot;is a portal that provides access to over 175 data sets, applications, and APIs related to the Philadelphia region. Simply accessing data, however, is not the ultimate goal of OpenDataPhilly. By connecting people with data, we&#x2019;re hoping to encourage users to take the data and transform it into creative applications, projects, and visualizations that demonstrate the power that data can have in understanding and shaping our communities.&quot; This sounded intriguing enough, so I began sorting through the available data. After browsing for a while I ran across a set of all bicycle thefts reported to the Philadelphia police department from January 1st, 2010 to September 16th, 2013. I thought it would be interesting to see what type of questions this data set could answer, so I downloaded the CSV and opened up iPython.</p>
<div class="text-center">
![](/content/images/2014/Mar/bike.jpg)
</div>
<p>First I imported the appropriate libraries, in this case it was Pandas and Matplotlib. I then loaded the data and took a quick look at it using the .head() method which displays the the first 5 rows by default.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Mar/Screen-Shot-2014-03-27-at-9-48-47-PM.png" alt loading="lazy"></p>
<p>Next I took only the columns I was interested in using for this analysis and renamed them to make life easier during the rest of the process. The Value_Code column breaks the bike value into three categories. 615 is a bike over 200 dollars, 625 is a bike between 50 and 200 dollars and 635 is a bike valued under 50 dollars.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Mar/Screen-Shot-2014-03-27-at-9-50-03-PM.png" alt loading="lazy"></p>
<p>After I had the data imported and cleaned up I wanted to get a better feel for what I was looking at so I created a plot of the thefts over the entire time period.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Mar/Screen-Shot-2014-03-27-at-9-52-39-PM.png" alt loading="lazy"></p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Mar/bike_theft.png" alt loading="lazy"></p>
<p>Something that stood out immediately was how many more thefts occured in the warmer months than in the winter. This reminded me of a few articles I had read on this topic, such as <a href="http://online.wsj.com/news/articles/SB10001424052748703995104575389461974136120">this</a> from the WSJ.</p>
<p>After seeing this coorelation I downloaded some weather data for Philadelphia going back to January 1st, 2010(where the theft reporting begins) and loaded it in.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Mar/Screen-Shot-2014-03-27-at-10-01-52-PM.png" alt loading="lazy"></p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Mar/weather.png" alt loading="lazy"></p>
<p>Looks pretty familiary doesnt it?</p>
<p>After I had both dataframes loaded I then merged them together.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Mar/Screen-Shot-2014-03-27-at-10-13-49-PM.png" alt loading="lazy"></p>
<p>Next I started breaking the dates down to see when the most crimes occured. First by year, note that 2013 is significantly lower because the data set ended in September, leaving out the last three months of the year.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Mar/Screen-Shot-2014-03-27-at-10-17-12-PM.png" alt loading="lazy"></p>
<p>Then I broke it down by month, as you can see the two hottest months are on top with almost ten times the thefts compared to the colder months.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Mar/Screen-Shot-2014-03-27-at-10-18-23-PM.png" alt loading="lazy"></p>
<p>I then seperated the data by hour, looks like most thefts occur at the end of the work day into the evening.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Mar/Screen-Shot-2014-03-27-at-10-21-39-PM.png" alt loading="lazy"></p>
<p>After seperating everything by time I began working with the actual value of the bikes, seems that most bikes taken are in the two to five hundred dollar range.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Mar/Screen-Shot-2014-03-27-at-10-24-34-PM.png" alt loading="lazy"></p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Mar/Screen-Shot-2014-03-27-at-10-27-53-PM.png" alt loading="lazy"></p>
<p>Next up was seeing the most theft prone blocks and creating a simple map of every theft:</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Mar/Screen-Shot-2014-03-27-at-10-29-22-PM.png" alt loading="lazy"></p>
<iframe width="800" height="500" scrolling="no" frameborder="no" src="https://www.google.com/fusiontables/embedviz?q=select+col0+from+1edaVb92ffZ-ievRAtueRX0z_GPvTz02OYvHBQSeb&amp;viz=MAP&amp;h=false&amp;lat=39.98829932165471&amp;lng=-75.03087728771482&amp;t=1&amp;z=11&amp;l=col0&amp;y=2&amp;tmplt=2&amp;hml=TWO_COL_LAT_LNG"></iframe>
<p>This was by no means an exhaustive analysis but rather a quick experiment, It still brought up some intriguing points and strong correlations which would interesting to take a step further and test for causation. In the future I would like to add a machine learning aspect to this to estimate probabilites your bike will be stolen based on time of year, location, cost of bike etc. I would also like to create more advanced visualizations, possibily with D3.</p>
<p>The complete notebook is available <a href="http://nbviewer.ipython.org/github/danielforsyth/Bikes/blob/master/Bikes.ipynb">here</a> and all of the code is on my <a href="https://github.com/danielforsyth/Bikes">github</a>.</p>
<p>Any feedback, advice, or corrections are appreciated. You can find me on <a href="https://twitter.com/Daniel_Forsyth1">Twitter</a> or email me at <a href="mailto:danforsyth1@gmail.com">danforsyth1@gmail.com</a>.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[What If NCAA Basketball Players Were Paid? - An Analysis]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>Every year around this time, after the conclusion of the collegiate football season and the height of March Madness the question I always hear posed is, should NCAA athletes be paid? This got me thinking, if student athletes <i>were</i> to be paid how much would they be worth? Since we</p>]]></description><link>https://www.danielforsyth.me/what-if-ncaa-basketball-players-were-paid-an-analysis/</link><guid isPermaLink="false">5d24af011e693a0017f5e5ef</guid><category><![CDATA[Python]]></category><category><![CDATA[Machine Learning]]></category><category><![CDATA[Pandas]]></category><category><![CDATA[Sports]]></category><dc:creator><![CDATA[Daniel Forsyth]]></dc:creator><pubDate>Wed, 26 Mar 2014 12:39:39 GMT</pubDate><content:encoded><![CDATA[<!--kg-card-begin: markdown--><p>Every year around this time, after the conclusion of the collegiate football season and the height of March Madness the question I always hear posed is, should NCAA athletes be paid? This got me thinking, if student athletes <i>were</i> to be paid how much would they be worth? Since we are in the middle of the annual NCAA basketball tournament I chose to work with basketball data in an attempt to answer this question.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Mar/ncaa.jpg" alt loading="lazy"></p>
<br>
My first order of business was to find a statistic that expresses a players overall contributions, A tell all stat that pools everything a player does on the court into one number. After some reading I came across the Player Efficency Rating or PER, which is exactly what I was looking for. Developed by John Hollinger, current Vice President of basketball operations for the Memphis Grizzlies, the player efficeny rating measures a players per-minute performance on the court adjusted for pace. PER takes into account both positive and negative contributions by the player, it is than adjusted to a per-minute basis so that every player, whether it be a starter or a sixth-man, can be compared equally. At this point my game plan become to find current NBA data that included PER and Salary, create a predictive model using this data, then feed it in current NCAA PER stayts to predi players salary.
<p>After downloading some data from <a href="http://www.basketball-reference.com/"> basketball-reference.com</a> I opened up ipython and began looking at the data.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Mar/Screen_Shot_2014_03_25_at_11_51_26_PM.png" alt loading="lazy"></p>
<p>I was only interested in the Player Name and current year salary.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Mar/Screen_Shot_2014_03_25_at_11_55_49_PM.png" alt loading="lazy"></p>
<p>Load Data that includes player efficency rating.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Mar/Screen_Shot_2014_03_25_at_11_57_05_PM.png" alt loading="lazy"></p>
<p>Add a column for minutes per game, to qualify for PER you must have a minimum of 6.09 MPG.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Mar/Screen_Shot_2014_03_25_at_11_57_57_PM.png" alt loading="lazy"></p>
<p>Merge PER and salary data into one table, then display only qualified players and remove and duplicate entries.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Mar/Screen_Shot_2014_03_26_at_12_01_21_AM.png" alt loading="lazy"></p>
<p>Now that we have all of the data cleaned and in the proper format we can begin plotting it and working on making predictions. Below I have created a scatter plot of the PER and Salary data.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Mar/Screen_Shot_2014_03_26_at_12_07_42_AM.png" alt loading="lazy"></p>
<p>To further visualize the data I created a scatter plot matrix of the Salary, PER, and MPG.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Mar/Screen_Shot_2014_03_26_at_12_11_41_AM.png" alt loading="lazy"></p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Mar/Screen_Shot_2014_03_26_at_12_11_56_AM.png" alt loading="lazy"></p>
<p>Time to make some predictions on the data, the first step in this process was creating a model with the NBA data. I went with a simple Linear Regression classifier as we are looking to predict a continious target variable.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Mar/Screen_Shot_2014_03_26_at_12_19_15_AM.png" alt loading="lazy"></p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Mar/Screen_Shot_2014_03_26_at_12_19_39_AM.png" alt loading="lazy"></p>
<p>Testing the classifier on a random PER.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Mar/Screen_Shot_2014_03_26_at_12_20_29_AM.png" alt loading="lazy"></p>
<p>At this point it was time to show the model some data from NCAA players. I was able to scrape the PER ratings of the top 100 NCAA players from this current season off of ESPN.com. The following code displays this data.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Mar/Screen_Shot_2014_03_26_at_12_23_12_AM.png" alt loading="lazy"></p>
<p>I then created a list of predicted salary based on the players PER rating and then added it into the data frame.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Mar/Screen_Shot_2014_03_26_at_12_24_53_AM.png" alt loading="lazy"></p>
<p>The dataframe below shows the top 15 players based on PER and their predicted salary.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Mar/Screen_Shot_2014_03_26_at_12_27_06_AM.png" alt loading="lazy"></p>
<p>In the future it would be interesting to apply this approach to previous seasons data and then compare the predicted salary to the actual salary of players that were drafted into the NBA. I would also be curious to see how players with very high player efficeny ratings in college fared when they made the move to the pros, especially those playing in less competitive conferences.</p>
<p>The complete notebook is available <a href="http://nbviewer.ipython.org/github/danielforsyth/NCAA-Salary-Prediction/blob/master/NCAA.ipynb">here</a> and all of the code is on my <a href="https://github.com/danielforsyth/NCAA-Salary-Prediction">github</a>. As always I would appreciate any feedback, advice, or corrections. You can find me on <a href="https://twitter.com/Daniel_Forsyth1">Twitter</a>  or email me at <a href="mailto:danforsyth1@gmail.com">danforsyth1@gmail.com</a>.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Predicting Artists]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>As of late I have become very interested in both machine learning and computer vision. While thinking of a project to work on both of these skills I thought it would be interesting to build a program that would be able to predict an artist based on a painting. I</p>]]></description><link>https://www.danielforsyth.me/predicting-artists/</link><guid isPermaLink="false">5d24af011e693a0017f5e5ed</guid><category><![CDATA[Python]]></category><category><![CDATA[Computer Vision]]></category><category><![CDATA[Machine Learning]]></category><dc:creator><![CDATA[Daniel Forsyth]]></dc:creator><pubDate>Tue, 25 Mar 2014 03:27:11 GMT</pubDate><content:encoded><![CDATA[<!--kg-card-begin: markdown--><p>As of late I have become very interested in both machine learning and computer vision. While thinking of a project to work on both of these skills I thought it would be interesting to build a program that would be able to predict an artist based on a painting. I have become pretty comfortable using Sci-Kit Learn for machine learning, but wasnt sure the best route to go for the vision portion of this project. After some research I found OpenCV would be best for this type of work. Developed by Intel, OpenCV is a free, open source computer vision library written in C++. It has a huge online community and bindings for several languages, including Python, my language of choice.<br>
<br></p>
<p><img src="https://www.danielforsyth.me/content/images/2019/07/the-lovers-1928-1-1-.jpeg" alt="the-lovers-1928-1-1-" loading="lazy"></p>
<blockquote>
<p><quote>&#x201C;We are surrounded by curtains. We only perceive the world behind a curtain of semblance. At the same time, an object needs to be covered in order to be recognized at all.&#x201D;<br>
&#x2015; Ren&#xE9; Magritte</quote></p>
</blockquote>
<p>After I had my idea finalized and the appropriate tools ready the next step was getting some data. After writing a script to scrape images from the google image API I was able to get one hundred twenty images. Sixty paintings by Ren&#xE9; Magritte and sixty paintings by Salivador Dal&#xED;. One caveat of this approach is the fact that not all of these images are actual paintings by the artist but rendtions of them by others and photoshop jobs however they still maintain the overall style of the original artist so I chose to keep moving. One hundred twenty images is also a very small number for this type of problem but its enough for the scope of this project.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Mar/Screen_Shot_2014_03_24_at_10_05_03_PM.png" alt loading="lazy"></p>
<p>After I had all of my data I needed to start thinking about what features I was going to extract to construct the model. This problem is an example of classification, a branch of  supervised Machine Learning. Basically we take a bunch of data, in this case it is paintings, label them with the correct classes, Magritte or Dali, and then create a feature vector. This feature vector is completely dependent on domain you are working in, more on this in a minute. You then input this feature vector into a machine learning algorithm which creates a predictive model. At this point can show the model new unlabeled data and it will give you a prediction of the class. Here is a very helpful diagram of this process.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Mar/plot_ML_flow_chart_1.png" alt loading="lazy"></p>
<p>At this point the problem became what I was going to use for the feature vectors of all of my images. After some more research I decided to use the RGB values of each image for my features. Every pixel in an image is assigned three digits in the format (Red, Green, Blue) the values are of each are equal to the intensity of the color and range from 0 to 255. Below is a simple example of this.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Mar/Screen_Shot_2014_03_24_at_10_46_39_PM.png" alt loading="lazy"></p>
<p>This snippet is reading in an image from the desktop, using matplotlib to display it, and then displaying the shape of the image. Since the image is 3000x2258 the shape of the numpy array is 3000X2258X3. This introduces another problem, the fact that all the images are different sizes. I also noticed that when OpenCV reads an image it converts it into a numpy array in the format (Blue, Green, Red). To overcome these two problems I wrote a small function.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Mar/Screen_Shot_2014_03_24_at_10_52_46_PM.png" alt loading="lazy"></p>
<p>At this point we are ready to get all of the data ready for Sci-Kit Learn. In the code below I created a list of numpy arrays for all of the images, I also created a list called labels which designates which artist the painting belongs to.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Mar/Screen_Shot_2014_03_24_at_10_56_39_PM.png" alt loading="lazy"></p>
<p>Sci-Kit Learn takes a two dimensional array [n samples x n features] so we have to flatten our arrays before we input them. The below code creates a list of flattened arrays for each image.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Mar/Screen_Shot_2014_03_24_at_11_03_32_PM.png" alt loading="lazy"></p>
<p>We now have all our data in the correct format for inputting into Sci-Kit. But before we do this we must split our data to avoid overfitting. Sci-Kit learn has a handy method for this, called train test split. The below code splits the flattened array of images into two seperate sets. The training set which is 70% of the data and the testing set which is 30%. Each image has 750,000 features which comes from the fact that each image is 500x500x3. So we train on the 70% and then make predictions on the remaining 30%.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Mar/Screen_Shot_2014_03_24_at_11_06_41_PM.png" alt loading="lazy"></p>
<p>Finally we are ready to start making predictions.  After trying out a few different classifiers including SGD, SVM, SVC, and a few different variations of Naive Bayes I found I got the best results with the Multinomial Naive Bayes classifier. The code for this is below where I was able to achieve 80% prediction accuracy, not bad for 120 samples.</p>
<p><img src="https://www.danielforsyth.me/content/images/2014/Mar/Screen_Shot_2014_03_24_at_11_15_02_PM.png" alt loading="lazy"></p>
<p>For future work on this project I would like to:</p>
<ul>
<li>Scrape much more data, possibly add artists</li>
<li>Look into more options for feature extraction of images</li>
<li>Learn and implement grid seach for hyperameter optimaztion</li>
<li>Implement the entire project into a web front end</li>
</ul>
<p>I have added all the code for this project to my github available <a href="https://github.com/danielforsyth/Art-Prediction">here</a> as well as the full iPython notebook <a href="http://nbviewer.ipython.org/github/danielforsyth/Art-Prediction/blob/master/Art_Prediction.ipynb">here</a></p>
<p>I am still very much new to all of this for any typos, code errors, advice, questions, or anything else, please get in touch via <a href="https://twitter.com/Daniel_Forsyth1">Twitter</a> or email me at <a href="mailto:danforsyth1@gmail.com">danforsyth1@gmail.com</a>. I&apos;d love to hear from you!</p>
<!--kg-card-end: markdown-->]]></content:encoded></item></channel></rss>